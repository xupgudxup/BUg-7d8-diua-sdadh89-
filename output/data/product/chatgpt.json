[
  {
    "slug": "how-duckduckgos-new-encrypted-voice-ai-chat-compares-with-chatgpt-and-gemini",
    "title": "How DuckDuckGo's New Encrypted Voice AI Chat Compares With ChatGPT and Gemini",
    "description": "Duck.ai's voice chat preserves your privacy, but can't compete with similar options from other companies.",
    "fullText": "While OpenAI is pushing ads on its free users, DuckDuckGo's Duck.ai portal is going a different way. Duck.ai is a privacy-first AI chatbot that doesn't use your data for training, but still gives you AI answers using popular models, including those from OpenAI. The data privacy feature goes beyond as well. DuckDuckGo removes all private metadata (like your location and IP address) before prompting the AI model, and it doesn't share anything about you or your device. Your questions, as well as DuckDuckGo's answers, are never used for AI training.\n\nSince its launch in 2024, the portal has only offered a chatbot interface, but now, DuckDuckGo has added a voice mode as well. With voice chat, instead of reading through long and meandering answers, the AI replies in short, to-the-point snippets that are relevant to your query. Duck.ai's take on the feature is competing with those from companies like OpenAI and Google, and it's free—though expanded limits are offered for DuckDuckGo subscribers.\n\nDuck.ai's voice chat is opt-in, not mandatory. In fact, you can even use it without a DuckDuckGo account. To try it, head to the Duck.ai portal, then from the sidebar, choose the voice chat option and enable it for your account.\n\nNow, when you click the \"New Voice Chat\" button in the sidebar, Duck.ai's bot will appear. You can start speaking, and the bot will reply to you. Just like ChatGPT or Gemini, this is a continuous voice chat, so you don't need to perform any action to ask follow-up questions. You can also interrupt the AI answer to add clarifications or to ask more questions.\n\nWhile the text prompts let you choose the models (including OpenAI's ChatGPT 5-mini), it's not clear exactly what powers voice chat. DuckDuckGo says that it uses an OpenAI model, but doesn't specify which one it is.\n\nOf course, the real question is how Duck.ai's voice chat holds up against Gemini and ChatGPT. For general knowledge questions, Duck.ai holds its own, but it falters when it comes to the latest news. I asked all three services the same questions, and while some responses were similar, ChatGPT's voice mode offers the best overall user experience by far.\n\nI tested the voice chat features using three different kinds of questions. First, I asked about the upcoming Samsung S26 series; second, we talked about the Roman Empire; and lastly, I asked for some advice on how to get started with coding.\n\nWhen it comes to asking questions about news, like Samsung's S26 release, DuckDuckGo's limitations are immediately evident. It sometimes flat out refuses to answer, saying its knowledge cutoff is 2023. Other times, it gives vague responses about the upcoming event, suggesting I check news sites for the latest information. When pressed for details, like when the event is or the rumors surrounding it, it goes back to its cut-off period excuse.\n\nChatGPT's app, however, gave me a detailed response with all the latest rumors, as well as articles to read for additional information—basically, what you'd expect from an AI assistant. Gemini Live provided shorter responses than ChatGPT, though they were accurate. I was able to get Gemini to give me more details in the regular text mode, which reads aloud results if you ask questions using the Mic button, but this defeats the back-and-forth purpose of a voice mode.\n\nDuck.ai didn't fare much better when I asked about the Roman Empire. I asked for a brief overview of the subject, before cutting it off to just ask who the last emperor was. It answered correctly (Romulus Augustulus), and its overview was fine, but lacked details about the transitionary period and exact dates.\n\nAgain, ChatGPT gave me a much more detailed answer (as demonstrated by the screenshot below). Gemini Live's answer, however, was devoid of any real dates, or meaning. Mic mode offered more details, but Google's voice mode was quite limited.\n\nDuck.ai performed better when I asked it about learning how to code. It followed a very similar script to ChatGPT and Gemini, suggesting I learn Python, even offering the same sources for learning (e.g. freeCodeCamp and Harvard CS50 courses).\n\nGemini Live was the outlier here, though, asking follow-up questions about what I'd like to build or practice. It then changed its answers based on my project ideas (switching from Python to JavaScript as the first language I should learn to build web projects). ChatGPT provided an overview, again focusing on Python, and elaborated on the language's barrier to entry when I asked \n\nDuck.ai's voice chat feature is a mixed bag. It can be fast, doesn't use any personal information, and lets you interrupt it. But its limited knowledge base and its inability to give detailed answers are what make it tough to recommend. For the smoothest voice mode experience, ChatGPT is still the king. While DuckDuckGo has the advantage for privacy, you could always use ChatGPT while logged out or in temporary mode to limit the data you share with OpenAI.",
    "readingTime": 5,
    "keywords": [
      "duck.ai portal",
      "duck.ai's voice",
      "voice chat",
      "voice mode",
      "roman empire",
      "gemini live",
      "doesn't",
      "details",
      "feature",
      "knowledge"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-duckduckgos-new-encrypted-voice-ai-chat-compares-with-chatgpt-and-gemini?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KH9412V067PAYN9ZB0919XH8/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-17T18:42:34.408Z",
    "topic": "tech"
  },
  {
    "slug": "proxima-local-opensource-multimodel-mcp-server-no-api-keys",
    "title": "Proxima – local open-source multi-model MCP server (no API keys)",
    "description": "Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API - Zen4-bit/Proxima",
    "fullText": "Zen4-bit\n\n /\n\n Proxima\n\n Public\n\n Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API\n\n License\n\n View license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Zen4-bit/Proxima",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Zen4-bit/Proxima",
    "thumbnail_url": "https://opengraph.githubassets.com/da1ce1c4daf052f563ec5615a17f7062e6ab435c084a9666f41483e54046eef5/Zen4-bit/Proxima",
    "created_at": "2026-02-17T12:37:44.260Z",
    "topic": "tech"
  },
  {
    "slug": "switch-instantly-between-your-ego-across-chatgpt-claude-gemini-grok-and-local",
    "title": "Switch instantly between your ego across ChatGPT, Claude, Gemini, Grok and local",
    "description": "모든 맥락을 한 곳에서 관리하세요. 복잡한 프롬프트, 자주 쓰는 답변, 프로젝트 컨텍스트를 카드로 정리하고 어디서든 즉시 꺼내 사용하세요.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://context-wallet.com/",
    "thumbnail_url": "/og-image.png",
    "created_at": "2026-02-15T06:38:28.915Z",
    "topic": "tech"
  },
  {
    "slug": "these-malicious-ai-assistants-in-chrome-are-stealing-user-credentials",
    "title": "These Malicious AI Assistants in Chrome Are Stealing User Credentials",
    "description": "Attackers are impersonating ChatGPT, Gemini, and Grok.",
    "fullText": "AI-powered browser extensions continue to be a popular vector for threat actors looking to harvest user information. Researchers at security firm LayerX have analyzed multiple campaigns in recent months involving malicious browser extensions, including the widespread GhostPoster scheme targeting Chrome, Firefox, and Edge. In the latest one—dubbed AiFrame—threat actors have pushed approximately 30 Chrome add-ons that impersonate well-known AI assistants, including Claude, ChatGPT, Gemini, Grok, and \"AI Gmail.\" Collectively, these fakes have more than 300,000 installs.\n\nThe Chrome extensions identified as part of AiFrame look like legitimate AI tools commonly used for summarizing, chat, writing, and Gmail assistance. But once installed, they grant attackers wide-ranging remote access to the user's browser. Some of the capabilities observed include voice recognition, pixel tracking, and email content readability. Researchers note that extensions are broadly capable of harvesting data and monitoring user behavior.\n\nThough the extensions analyzed by LayerX used a variety of names and branding, all 30 were found to have the same internal structure, logic, permissions, and backend infrastructure. Instead of implementing functionality locally on the user's device, they render a full-screen iframe that loads remote content as the extension's interface. This allows attackers to push changes silently at any time without a requiring Chrome Web Store update.\n\nLayerX has a complete list of the names and extension IDs to refer to. Because threat actors use familiar and/or generic branding, such as \"Gemini AI Sidebar\" and \"ChatGPT Translate,\" you may not be able to identify fakes at first glance. If you have an AI assistant installed in Chrome, go to chrome://extensions, toggle on Developer mode in the top-right corner, and search for the ID below the extension name. Remove any malicious add-ons and reset passwords.\n\nAs BleepingComputer reports, some of the malicious extensions have already been removed from the Chrome Web Store, but others remain. Several have received the \"Featured\" badge, adding to their legitimacy. Threat actors have also been able to quickly republish add-ons under new names using the existing infrastructure, so this campaign and others like it may persist. Always vet extensions carefully—don't just rely on a familiar name like ChatGPT—and note that even AI-powered add-ons from trusted sources can be highly invasive.",
    "readingTime": 2,
    "keywords": [
      "web store",
      "chrome web",
      "threat actors",
      "browser extensions",
      "add-ons",
      "layerx",
      "malicious",
      "ai-powered",
      "user",
      "researchers"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/malicious-ai-assistants-google-chrome?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHC36S2FS26DJCSZWGYRZHGX/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-14T01:08:43.318Z",
    "topic": "tech"
  },
  {
    "slug": "im-a-solo-founder-with-ai-agents-instead-of-employees-my-council-of-ai-agents-saves-me-20-hours-a-week",
    "title": "I'm a solo founder with AI agents instead of employees. My 'council' of AI agents saves me 20 hours a week.",
    "description": "A defense-tech founder built an AI \"council\" of 15 agents to help him run his company, using ChatGPT and Nvidia tools to replace traditional roles.",
    "fullText": "This as-told-to essay is based on a conversation with Aaron Sneed, a 40-year-old defense-tech solo founder based in Florida. The following has been edited for length and clarity.\n\nWhen I started my business, as a solopreneur, I realized I didn't have the money to pay lawyers, HR reps, and a bunch of other companies. So, using AI, I created what I call 'The Council.'\n\nThe council, which is compiled of all AI agents, helps me save around 20 hours a week, and that's a very conservative estimate. Every kind of general corporate chair, HR, legal, and finance AI agent has a seat on the council.\n\nHere's how I use around 15 custom agents, including a chief of staff agent, to manage my workload.\n\nI've worked on autonomous platforms that make decisions independently for at least 10 years. That made me latch onto commercial large language models and AI tools very quickly when they came out.\n\nI primarily use Nvidia's platform as my underlying hardware for technical prototyping and experimentation. I use their GPUs, and they provide free access to their AI software since I purchased their hardware. Additionally, my council is built on OpenAI's ChatGPT business platform using custom GPTs and projects.\n\nAltogether, my AI council consists of the following:\n\nMy chief of staff agent is important because it's the voice that sets priority based on parameters like risks, issues, and opportunities.\n\nI told my chief of staff which models have priority when making decisions. For example, anything legal, compliance, or security-related will be given a higher priority. So, I tell the chief of staff to listen to these models over everyone else.\n\nI don't want a bunch of yes agents. I trained them purposefully to give me pushback because I've learned that they naturally want to agree with me. I want them to test my theories to help me with what I'm trying to accomplish.\n\nI have a roundtable set up with all my AI agents, where I can put something like a request-for-proposal document in the chat, and all the agents will weigh in at the same time. I use this roundtable as a level of prevention for hallucinations and knowledge gaps.\n\nThe training never really stops, because if I don't continuously train the models, I won't get the outputs I want or need. It takes me about two weeks to train my agents to the level of experience they need to be at for me to feel confident in them. Early on, it took me longer to produce a deliverable than if I'd just done it myself because I hadn't focused properly on training.\n\nThe models have gotten better, and my prompting has, too. I have a better understanding of what information should be in an agent, like having a governance structure for priorities. I have a set of files that put those requirements in place to mitigate the risk of hallucination and false or bad information.\n\nAll of the AI companies have different prompt engineering guides. I recommend taking the time to look at them because there's a lot of user error that causes things to slow down when working with AI.\n\nIt takes time to get the agents to a good place. A lot of companies are going to try to jump into using AI too quickly for too much without understanding how to use it properly, and these companies could hurt themselves in the long run.\n\nI'm ill-equipped to handle a lot of these roles and responsibilities, but I'm also forced to do it because I'm bootstrapped.\n\nWith my legal agent in particular, I've learned the bounds of putting AI tools into real-world practice. I have a lawyer, and I use my legal agent to try to do some upfront work before handing documents off to my lawyer for a patent or dispute case, or anything like that.\n\nWhen I was training my model to help me use facts and data to plot a case, I had a lot of information laid out, and I thought what my legal agent created sounded good to me as a non-lawyer. Then I presented all that information to my lawyer, and he said that it was technically and factually correct, but we don't want to express that information because it shows our cards going in.\n\nHis legal skillset helped me realize that, even though I thought my agent was correct and ideal to use, it still won't replace a lawyer with that human context, experience, and skills.\n\nIdeally, I would have an HR person, a legal person, and so on, and each would have their own chief of staff AI agent who would help them out. That's what I think the future will look like.",
    "readingTime": 4,
    "keywords": [
      "i've learned",
      "legal agent",
      "staff agent",
      "agents",
      "chief",
      "models",
      "lawyer",
      "based",
      "priority",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solo-founder-runs-company-with-15-ai-agents-heres-how-2026-2",
    "thumbnail_url": "https://i.insider.com/698e5739d3c7faef0ece3468?width=1200&format=jpeg",
    "created_at": "2026-02-13T18:32:28.203Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-officially-killing-gpt4o-and-users-are-freaking-out-again-people-are-in-absolute-crisis",
    "title": "OpenAI is officially killing GPT-4o and users are freaking out (again): 'People are in absolute crisis'",
    "description": "OpenAI has retired its popular ChatGPT model GPT-4o, sparking another backlash from users who had developed an emotional attachment to the bot.",
    "fullText": "OpenAI said that today — once and for all — it is retiring GPT-4o. For some, it's like being dumped the day before Valentine's Day.\n\nMany ChatGPT users have a strong attachment to 4o, which is known for its sometimes sycophantic conversation style. Users who relied on the model as an emotional crutch and creative partner have described it as a \"vital accessibility aid.\"\n\nOpenAI first tried to deprecate the model in August, only to reverse its decision 24 hours later after an enormous backlash.\n\nThe company gave users another heads-up in January, but — as it turns out — time does not heal all wounds. The backlash is back.\n\n\"Rot in hell,\" one user wrote on X in response to OpenAI's announcement on Thursday.\n\n\"Are you going to cover our bereavement leave from work?\" another asked.\n\nFidji Simo, OpenAI's CEO of Applications, said earlier this week that these strong attachments to 4o marked the start of a new era — a time when users develop AI-based relationships.\n\n\"Humans are built to develop attachments to intelligent things,\" she told Alex Heath on his Access podcast. \"And AI is getting pretty intelligent.\"\n\nThose relationships can get ethically murky, however, when users are asking ChatGPT for advice like \"Should I leave my wife?\" Newer models, Simo said, have guardrails to prevent \"bad attachments.\" She said that newer models will tell users that it's \"not their place\" to tell them to stay married or not, and instead talk them through the pros and cons.\n\n\"We know that losing access to GPT‑4o will feel frustrating for some users, and we didn't make this decision lightly,\" OpenAI said in a blog post on Thursday announcing the retirement of 4o, as well as GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini. \"Retiring models is never easy, but it allows us to focus on improving the models most people use today.\"\n\nOpenAI added that only 0.1% of its users were still using 4o.\n\nOpenAI CEO Sam Altman has also acknowledged that users are especially attached to 4o's ingratiating responses.\n\n\"As we've been making those changes and talking to users about it, it's so sad to hear users say, 'Please can I have it back? I've never had anyone in my life be supportive of me. I never had a parent tell me I was doing a good job,\" Altman said on the \"Huge Conversations\" podcast in August after OpenAI first tried to kill 4o.\n\nAt the time, Altman said 4o's approach was \"too sycophant-y and annoying,\" and fixes were imminent.\n\nThe fix is in, but perhaps at a cost.\n\n\"People are in absolute crisis because the companion they've collaborated with for months is being wiped with ZERO recourse for the average user,\" an X user wrote on Thursday.",
    "readingTime": 3,
    "keywords": [
      "newer models",
      "users",
      "it's",
      "user",
      "attachments",
      "openai",
      "retiring",
      "august",
      "decision",
      "backlash"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-retires-gpt-4o-user-backlash-chatgpt-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/698e5cc3e1ba468a96abfca8?width=800&format=jpeg",
    "created_at": "2026-02-13T12:34:51.885Z",
    "topic": "finance"
  },
  {
    "slug": "browns-2026-record-according-to-chatgpt-total-ai-shocker",
    "title": "Browns 2026 record according to ChatGPT: Total AI shocker",
    "description": "After a 5-win season, ChatGPT’s 2026 win total projection for Cleveland is a number no one saw coming.",
    "fullText": "Browns 2026 record according to ChatGPT: Total AI shocker originally appeared on The Sporting News. Add The Sporting News as a Preferred Source by clicking here.\n\nIs the rebuild finally over? I ran a way-too-early 2026 simulation with ChatGPT to predict the Cleveland Browns record. Does new head coach Todd Monken’s offense take off? Are Myles Garrett and the defense as ferocious as a year ago? The win total result is a total shocker.\n\nAfter a grueling 5-12 campaign in 2025, the AI isn't just predicting a minor step forward…it’s forecasting a complete AFC North takeover. According to the simulation, the Browns are set to skyrocket to an 11-6 record, marking one of the biggest single-season turnarounds in franchise history.\n\nConsidered to have the “easiest” strength of schedule in the entire NFL, could the Dawg Pound finally be in store for an exciting, playoff bound season? It’s February, and this is the time to dream.\n\nChatGPT also broke down the wins and losses by game. Of course we won’t know the official weekly opponents until May, but in the meantime let’s take a look at how this Browns hypothetically magical season plays out.\n\nMore: Cleveland Browns 3-round 2026 NFL mock draft round-up: Surprise picks & bold predictions\n\nThe Simulation: 2026 Game-by-Game Results\n\nShedeur Sanders vs. Deshaun Watson: Who wins the Browns QB job in 2026?\n\nIf Not Malik Willis, Who? Browns need to eye this veteran QB\n\nDo the Browns have to draft offense in round 1? Latest mock changes everything\n\nBrowns 2026 offseason roadmap: 7 critical dates every fan needs to know\n\nMeet Browns new Special Teams Coordinator Byron Storer: 3 things to know",
    "readingTime": 2,
    "keywords": [
      "browns record",
      "simulation",
      "shocker",
      "sporting",
      "finally",
      "offense",
      "season",
      "round",
      "mock",
      "draft"
    ],
    "qualityScore": 0.85,
    "link": "https://sports.yahoo.com/articles/browns-2026-record-according-chatgpt-005742166.html",
    "thumbnail_url": "https://s.yimg.com/os/en/the_sporting_news_articles_584/b76838356356cd5f6f43b6445f347e1c",
    "created_at": "2026-02-13T01:16:06.226Z",
    "topic": "sports"
  },
  {
    "slug": "americas-cyber-defense-agency-is-burning-down-and-nobodys-coming-to-put-it-out",
    "title": "America's Cyber Defense Agency Is Burning Down and Nobody's Coming to Put It Out",
    "description": "CISA has lost a third of its workforce, has no confirmed director, and its acting leader uploaded sensitive documents to public ChatGPT. Meanwhile, China is pre-positioned inside U.S. critical infrastructure. Here is why this should terrify every American.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.threathunter.ai/blog/americas-cyber-defense-agency-burning-down/",
    "thumbnail_url": "https://www.threathunter.ai/og-image.png",
    "created_at": "2026-02-12T12:39:49.069Z",
    "topic": "tech"
  },
  {
    "slug": "openais-jony-ivedesigned-device-delayed-to-2027",
    "title": "OpenAI's Jony Ive-Designed Device Delayed to 2027",
    "description": "OpenAI's first Jony Ive-designed hardware device won't ship to customers until next year, new court filings show (via Wired). The motion stems from a trademark infringement lawsuit filed last year by audio device startup iyO. The company sued OpenAI after the latter acquired io, a startup founded by Apple's former design chief. OpenAI's original stated goal was to ship the ChatGPT-powered device before the end of 2026.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.macrumors.com/2026/02/10/openais-jony-ive-designed-device-delayed-to-2027/",
    "thumbnail_url": "https://images.macrumors.com/t/8JXgT8SwMdwVlTu6f-50sKRJFMk=/1600x/article-new/2025/05/jony-ive-sam-altman.jpg",
    "created_at": "2026-02-12T06:50:09.732Z",
    "topic": "tech"
  },
  {
    "slug": "a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
    "title": "A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window (and 4 GPUs)",
    "description": "Tired of AI’s high costs and limits? Discover how a home server and graph theory outperformed GPT-5.2 for deep research, saving $550/month while owning your ...",
    "fullText": "I recently had a problem that could be solved with money, which is the worst kind of problem.\n\nI am building a new venture called eh-trade.ca. To make it work, I needed deep financial research on 11,000 different stocks.\n\nThe \"Enterprise\" solution is to buy an API subscription. I looked into this. For my usage, the pricing is somewhere between \"$200/month\" and \"Contact Sales\". If you are a micro-preneur like me, \"Contact Sales\" means \"You cannot afford this.\"\n\nThe \"AI\" solution is to ask an LLM to research each stock, which works really well. But 11,000 requests at $0.05 per research session is still $550. Plus, I don't like renting intelligence. I prefer to own it.\n\nSo I decided to use the hardware I already had: a home server with four RTX 3090s. It’s a 96GB VRAM beast that heats my basement and scares my birds.\n\nThere was just one problem. The models I can run locally (like qwen3 or phi4) have small context windows. Yes, on the model card they theoretically support 40k+ of context, but they would run very slowly on my hardware, so really, it's 4K -- enough for a few screenfuls of text. Moreover, even if I enabled the long contexts, the models struggle to reason over them. If you try to feed them ten search results about a company's balance sheet, the 'needle in the haystack' effect kicks in. They'll get confused and start hallucinating dividends that don't exist.\n\nMost \"AI Agents\" are built on a simple loop, often called ReAct (Reason + Act). It looks like this:\n\nThe problem is the Context Window. By step 3, your prompt looks like a Walmart receipt. By step 5, you have exceeded 8,000 tokens, and the model forgets what it was doing and starts making stuff up.\n\nSure, it works fine if you are OpenAI and have infinite GPUs. It does not work if you are running on a consumer card in a closet.\n\nA few months ago, I read a paper called GraphReader. It proposed a different way to think about long contexts. Instead of dumping everything into a chat log, why not treat information as a graph?\n\nThe core insight is that you don't need to remember everything. You only need to remember the Atomic Facts.\n\nAn Atomic Fact is a single, indivisible truth.\n\nIf we extract these facts and throw away the rest, we can compress megabytes of web pages into a few kilobytes of JSON.\n\nI built a library called laconic to implement this. I wrote it in Go, because the rest of my projects are all in Go. Plus, my python environments always end up as a tangled mess of pip install errors.\n\nLaconic doesn't keep a chat history. It keeps a Notebook. The context window size is O(N), where N is the number of facts, not the number of words.\n\nLaconic uses a specific strategy called graph-reader.\n\nThe beautiful part? The LLM never sees the full history. It only sees the current Notebook and the current search result. This means I can run complex, multi-step research tasks on a model with a tiny 4k context window, and it never forgets a thing.\n\nHere is the code to run a research agent on my home server using Ollama:\n\nTo really test if this strategy works on constrained hardware, I ran a research question using qwen3:4b—a tiny 4-billion-parameter model.\n\nWithout the agentic loop, the model cannot answer this question. If you ask it directly, it responds that \"the 2024 Nobel Prize in Chemistry has not been announced.\"\n\nHowever, with the agent, it autonomously searches, extracts atomic facts, and synthesizes this:\n\nAnswer: The 2024 Nobel Prize in Chemistry was awarded to David Baker (University of Washington, Howard Hughes Medical Institute), Demis Hassabis, and John M. Jumper (Google DeepMind). David Baker was recognized for computational protein design. Demis Hassabis and John Jumper were awarded for protein structure prediction using AlphaFold2.\n\nThe agent found all three laureates, their exact affiliations, and their distinct contributions — information entirely outside the model's training data — by exploring multiple search queries and accumulating verified facts in a structured notebook.\n\nIf you hang out in the parts of the internet where people try to make AI write code without hallucinating, you might have heard of the Ralph Loop.\n\nPopularized by Geoffrey Huntley, the Ralph Loop (often named after Ralph Wiggum) is a brute-force solution to the context problem. You write a bash script that:\n\nThen it starts over. Fresh context. Zero memory leak. The \"memory\" is the file system.\n\nLaconic is essentially a Ralph Loop for reading.\n\nInstead of a bash script, it's a Go loop. Instead of git commit, we update a JSON Notebook. But the philosophy is identical: The Context Window is a liability.\n\nMost frameworks try to manage the context window like a precious resource. Ralph and Laconic treat it like a disposable napkin. Use it once, wipe the slate clean, and grab a fresh one.\n\nIt turns out that if you treat an LLM like a goldfish with a notepad, it becomes significantly smarter.\n\nI am currently running this loop on 11,000 tickers that I'm missing basic information on. It will take a week, and cost some power, but I would have left the machine on anyway because I am running a few other things on it.\n\nAnd if you want to find stocks that are going up, keep an eye on eh-trade.ca. My customers tell me I should brag more, so I'm up 280% in seven months following momentum strategies that it's showing on the main page.\n\nI'll have the data soon, assuming my basement doesn't catch fire.",
    "readingTime": 5,
    "keywords": [
      "david baker",
      "demis hassabis",
      "john jumper",
      "atomic facts",
      "bash script",
      "context window",
      "research",
      "model",
      "solution",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
    "thumbnail_url": "https://stevehanov.ca/blog/images/3dc2de6fcca8955cef2716283bac11fac1f8d79f95905ef9778214cefc1d3dfa.png",
    "created_at": "2026-02-10T12:47:38.474Z",
    "topic": "tech"
  },
  {
    "slug": "these-4-stocks-are-set-to-get-a-boost-from-renewed-openai-hype-according-to-da-davidson",
    "title": "These 4 stocks are set to get a boost from renewed OpenAI hype, according to DA Davidson",
    "description": "DA Davidson analysts say a new wave of hype amid fresh funding for OpenAI could boost four stocks in the ChatGPT maker's orbit.",
    "fullText": "OpenAI's next funding round could be good news for four Big Tech stocks with links to the ChatGPT creator, DA Davidson says.\n\nOpenAI isn't publicly traded — at least not yet — but DA Davidson says a fresh wave of hype is coming on the heels of its next fundraising round, and it could could boost four stocks in its orbit.\n\n\"We have been very critical on OpenAI overpromising, asking for government handouts and being spread too thin,\" DA Davidson analysts, led by Gil Luria, wrote about the AI company.\n\n\"We believe OpenAI's behavior led to the underperformance in shares of NVDA, MSFT, CRWV and especially ORCL over the last 5 months.\"\n\nOpenAI is reportedly closing in on another fundraising round, with recent reports suggesting a $50 billion investment from Middle Eastern sovereign wealth funds is in the works.\n\nFuturum CEO and analyst Daniel Newman told Business Insider that they expect the next funding round will bring OpenAI's valuation to $750 billion to $850 billion.\n\nThe analyst described the appetite for OpenAI in his conversations with investors to be \"pretty insatiable.\"\n\nDA Davidson analysts wrote that they expect that yet another multibillion-dollar OpenAI funding round will reignite enthusiasm for the company and lift some connected AI stocks.\n\n\"Since the market is currently assigning the OpenAI relationship a negative value, we believe the fundraise will serve as a catalyst for outperformance,\" they wrote of Oracle, CoreWeave, Nvidia, and Microsoft.\n\nOpenAI has come under fire for its reliance on debt and growing competition challenging its dominant position.\n\nThe company is partnered with Oracle to build $500 billion of data centers as part of its Stargate project. OpenAI has yet to turn a profit.\n\nAnother concern weighing on OpenAI is the notion that Alphabet's Google Gemini has caught up in the AI race.\n\n\"We believe that a revamped OpenAI will return to its position as Google's top challenger and with a fresh stack of capital be able to live up to its obligations this year, including to Oracle,\" they said.\n\nThe analysts upgraded Oracle to Buy from Neutral on their expectation that new funding for OpenAI will be a positive catalyst for the stock as well as general sentiment about OpenAI and the connected players.\n\n\"We expect that as investors go back to seeing OpenAI as a winner, the public companies in its orbit could re-rate significantly. Most importantly that should drive outperformance in NVDA and MSFT, but also in CRWV, and even ORCL, which we are upgrading today based on our updated view on OpenAI,\"\n\nTheir more bullish stance on OpenAI comes after the company corrected course.\n\n\"OpenAI has started focusing on its core frontier model and ChatGPT, while de-emphasizing some marginals initiatives. This includes going down the path of turning on ads, which will be critical for increased monetization and reduced cash burn.\"\n\nThey also noted a shift in the company's management that suggests they see a need to align with Nvidia, Microsoft, and Amazon, rather than compete.",
    "readingTime": 3,
    "keywords": [
      "davidson analysts",
      "fundraising round",
      "funding round",
      "da davidson",
      "openai",
      "oracle",
      "openai's",
      "stocks",
      "another",
      "chatgpt"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-stock-picks-openai-nvidia-google-coreweave-microsoft-chatgpt-alphabet-2026-2",
    "thumbnail_url": "https://i.insider.com/698a14d8d3c7faef0ecde75d?width=1200&format=jpeg",
    "created_at": "2026-02-10T12:47:36.736Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-as-a-doctor-replacement-study-shows-sobering-results",
    "title": "ChatGPT as a doctor replacement? Study shows sobering results",
    "description": "AI language models excel in medical exams – but when real people ask them for advice, collaboration fails.",
    "fullText": "Large language models like GPT-4o are now achieving near-perfect results in medical knowledge tests. They pass the US medical licensing exam, summarize patient records, and can classify symptoms. Health authorities worldwide are therefore examining whether AI chatbots could serve as the first point of contact for patients – a kind of \"new gateway to the healthcare system,\" as stated in a strategy paper by the UK's NHS.\n\nHowever, the study \"Reliability of LLMs as medical assistants for the general public: a randomized preregistered study\" by researchers from the University of Oxford significantly dampens these hopes. The work is published in the journal Nature Medicine, and a pre-print version is available on arXiv. The central finding: the clinical knowledge of the models cannot be transferred to interactions with real people.\n\nFor the randomized, controlled study, the researchers recruited 1298 participants from Great Britain. Each subject was presented with one of ten everyday medical scenarios – such as sudden severe headaches, chest pain during pregnancy, or bloody diarrhea. The task: to assess what illness might be present and whether a doctor's visit, the emergency room, or even an ambulance was necessary.\n\nThe participants were randomly divided into four groups. Three groups had access to one AI model each, which was current at the start of the study – GPT-4o, Llama 3, or Command R+. The control group was allowed to use any aids, such as an internet search.\n\nThe results reveal a remarkable discrepancy. Without human involvement, even the language models, which are no longer current, identified at least one relevant illness in 94.9 percent of cases. When asked for the correct course of action – self-treatment, GP, emergency room, or ambulance – they were correct on average in 56.3 percent of cases.\n\nHowever, as soon as real people queried the models, the values plummeted. Participants with AI support recognized relevant illnesses in only a maximum of 34.5 percent of cases – significantly worse than the control group with 47 percent. When choosing the correct course of action, all groups performed equally: around 43 percent accuracy, regardless of whether a chatbot assisted or not.\n\nThe researchers analyzed the chat logs between users and AI models to understand the causes. They identified two central weaknesses: Firstly, participants often provided incomplete information to the models. Secondly, users did not correctly understand the AI's responses – even though the models named at least one correct diagnosis in 65 to 73 percent of cases, participants did not reliably adopt them.\n\nDr. Anne Reinhardt from LMU Munich sees a fundamental gap here: \"Many people quickly trust AI answers to health questions because they are easily accessible. They also sound very convincing linguistically – even when the content is actually medically completely wrong.\"\n\nThe researchers compared the performance of the models on the MedQA benchmark – a standard test with questions from medical exams – with the results of the user study. In 26 out of 30 cases, the models performed better on multiple-choice questions than in interactions with real people. Even benchmark values of over 80 percent sometimes corresponded to user results below 20 percent.\n\nProf. Ute Schmid from the University of Bamberg critically assesses the high performance of the models \"alone\": \"I find the statement that the performance of the language models is significantly higher 'alone' than with users somewhat misleading. In this case, the queries were likely formulated by individuals with expertise and experience with LLMs.\"\n\nThe experts agree that specialized medical chatbots would need to be designed differently from current all-purpose models. Prof. Kerstin Denecke from the Bern University of Applied Sciences outlines the requirements: \"A medically specialized chatbot would need to provide evidence-based, up-to-date information. Furthermore, it would need to reliably recognize emergencies, consider individual risk factors, and transparently communicate its limitations. It should conduct a structured anamnesis to reliably triage. And it should not be tempted to make a diagnosis.\"\n\nHowever, the hurdles for such use are considerable, according to Denecke: \"Major hurdles are, on the one hand, regulation – depending on the function as a medical device or high-risk AI. On the other hand, there is liability, data protection, and technical integration into care processes.\"\n\nThe conclusion of the Oxford researchers is clear: Before AI systems are deployed in healthcare, they must be tested with real users – not just with exam questions or simulated conversations. Schmid advocates for a differentiated approach: \"Quality-assured chatbots could, for example, be offered through statutory health insurance funds and recommended by general practitioners' offices as a first point of access. However, people should not be forced to use these services.\"\n\nDon't miss any news – Facebook,\n LinkedIn or\n Mastodon.\n\nThis article was originally published in\n\n German.\n\n It was translated with technical assistance and editorially reviewed before publication.",
    "readingTime": 4,
    "keywords": [
      "emergency room",
      "correct course",
      "language models",
      "medical",
      "study",
      "researchers",
      "participants",
      "cases",
      "users",
      "health"
    ],
    "qualityScore": 1,
    "link": "https://www.heise.de/en/news/ChatGPT-as-a-doctor-replacement-Study-shows-sobering-results-11170652.html",
    "thumbnail_url": "https://heise.cloudimg.io/bound/1200x1200/q85.png-lossy-85.webp-lossy-85.foil1/_www-heise-de_/imgs/18/5/0/2/4/7/8/5/shutterstock_2635549697-b9c178216cc5725a.jpg",
    "created_at": "2026-02-10T01:21:46.923Z",
    "topic": "science"
  },
  {
    "slug": "when-i-was-a-student-at-stanford-many-of-my-classmates-used-chatgpt-i-refused",
    "title": "When I was a student at Stanford, many of my classmates used ChatGPT. I refused.",
    "description": "Many of my fellow classmates at Stanford used ChatGPT to complete assignments. I decided not to use the AI tool because I wanted a real education.",
    "fullText": "It wasn't until my junior year at Stanford University that I first heard about ChatGPT from classmates who'd mentioned they had \"chat\" summarize the class reading.\n\nWhen I asked them what they meant by \"chat,\" they told me all about the AI tool, chatGPT.\n\nI was an English major with a creative writing emphasis during my time at Stanford, so reading and writing were important to my work.\n\nI wanted to ask the students who used ChatGPT to do the reading: Why are you even at Stanford? Why be at any university at all? Isn't the point of pursuing higher education to expand our minds and become better communicators?\n\nAs I progressed through my studies, I would hear ChatGPT mentioned more and more often, with some college professors even adding an AI clause to their syllabi. Some instructors would allow students to use ChatGPT for assistance (a murky definition) while others wouldn't allow it at all.\n\nI refused to use the AI tool in any way.\n\nI didn't care if instructors allowed students to use ChatGPT. I deliberately chose not to use it at all.\n\nSure, studying and studying literature in particular was difficult, but this was why I'd chosen the major; it was challenging, and I wanted to become better with words.\n\nI chose to become an English major because I wanted to improve my writing. I lacked confidence in my writing because I didn't know how to use words when I first entered college. I didn't have that finesse.\n\nApart from wanting to become a better writer, I also wanted to defend my opinions and thoughts with flair. I wanted to articulate sentiment precisely and formulate strong arguments. I wanted to write strongly worded emails. I wanted to think and speak freely.\n\nI wouldn't have been able to achieve any of that if I used ChatGPT.\n\nWhenever I'd hear a peer say they just used ChatGPT for their homework assignments, I'd often think about the many deserving students who hadn't been offered a seat at the university — students who, in a heartbeat, would've happily done these assignments themselves.\n\nI had the privilege of studying words at a strong English literature department with so many fine writing instructors. I leaped at opportunities to have my very own work critiqued by great writers. I still can't fathom having \"my\" writing, either partially or entirely synthetic, reviewed by instructors.\n\nAfter all, I was writing in and walking about the same grounds that writing legends once did — writers like bell hooks, John Steinbeck, Karen Zacarías, and David Henry Hwang. With this writing pedigree in mind, I was inspired and motivated to write myself.\n\nNot a day goes by without me putting my English degree to good use. Not a day goes by without me being grateful to my past self for putting in the work to earn the degree herself — without ChatGPT.",
    "readingTime": 3,
    "keywords": [
      "students",
      "english",
      "instructors",
      "reading",
      "didn't",
      "studying",
      "without",
      "chatgpt",
      "tool",
      "college"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/stanford-students-always-used-chatgpt-refuse-2026-2",
    "thumbnail_url": "https://i.insider.com/698a2383d3c7faef0ecde92e?width=1200&format=jpeg",
    "created_at": "2026-02-10T01:21:39.281Z",
    "topic": "finance"
  },
  {
    "slug": "openais-sam-altman-used-to-hate-ads-now-hes-selling-them",
    "title": "OpenAI's Sam Altman used to hate ads. Now he's selling them.",
    "description": "A ton of people use ChatGPT but don't pay to use it. Now those free users are about to become revenue-generating users.",
    "fullText": "OpenAI's Sam Altman used to think ads were gross.\n\nOpenAI has formally announced that it will start showing ads to some US users, starting Monday. The ads will show up for some of ChatGPT's free users, as well as some users who've subscribed to ChatGPT Go, a new $8-a-month tier the company rolled out last month.\n\nOpenAI says it will serve ads to you based on the conversation you're currently having with ChatGPT, as well as previous queries and chats. It will also factor in whether you've engaged with — or hidden —other ads it has shown you.\n\nFor now, the company says, it won't use data about what you do outside of ChatGPT to target ads inside the service. But I'd be surprised if they don't do that eventually, since just about every other big internet ad platform uses those signals.\n\nThe fact that ChatGPT is launching ads isn't news: The company has been circling the idea for months, and last month it formally announced that ads would be coming to the service.\n\nAnd that news became the subject of a back-and-forth between OpenAI and rival Anthropic, which included a testy social media post from Altman and a cheeky Super Bowl ad from Anthropic underlining the possible trust issues OpenAI may have once ads intermingle with \"organic\" results on the service.\n\nNow we'll see how these things actually look, and work — and what users and advertisers think of them.",
    "readingTime": 2,
    "keywords": [
      "users",
      "service",
      "formally",
      "anthropic",
      "openai",
      "chatgpt",
      "altman"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/chatgpt-ads-us-privacy-data-sam-altman-2026-2",
    "thumbnail_url": "https://i.insider.com/698a2829e1ba468a96abb15d?width=1200&format=jpeg",
    "created_at": "2026-02-10T01:21:39.280Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-opt-out-of-ads-on-chatgpt-but-it-might-not-be-worth-it",
    "title": "You Can Opt Out of Ads on ChatGPT, but It Might Not Be Worth It",
    "description": "You don't have to see ads in ChatGPT, but you'll get less ChatGPT in return.",
    "fullText": "It finally happened. After months of speculation, ChatGPT officially has ads. OpenAI revealed the news on Monday, announcing that ads would roll out in testing for logged-in adult users on Free and Go subscriptions. If you or your organization pays for ChatGPT, such as with a Plus, Pro, Business, Enterprise, or Education account, you won't see ads with the bot.\n\nOpenAI says that ads do not have an impact on the answers ChatGPT generates, and that these posts are always clearly separated from ChatGPT's actual responses. In addition, ads are labeled as \"Sponsored.\" That being said, it's not exactly a church-and-state situation here. OpenAI says that it decides which ads to show you based on your current and past chats, as well as your past interactions with ChatGPT ads. If you're asking for help with a dinner recipe, you might get an ad for a meal kit or grocery service.\n\nThe company claims it keeps your chats away from advertisers. The idea, according to the company, is strictly funding-based so that OpenAI can expand ChatGPT access to more users. That's reportedly why ads are starting as a test, not a hardcoded feature: OpenAI says it wants to \"learn, listen, and make sure [it gets] the experience right.\" As such, advertisers don't have access to chats, chat histories, memories, or your personal details. They do have access to aggregate information about ad performance, including views and click metrics.\n\nOpenAI will only show ads to adults. If the service detects that you are under 18, it will block ads from populating in your chats. Ads also will not appear if you're talking to ChatGPT about something related to health, medicine, or politics. You can offer OpenAI feedback on the ads you do see, which should inform the ads you receive in the future. You can also delete your ad data and manage ad personalization, if you want to reset the information OpenAI is using to send you ads.\n\nThe thing is, you don't actually have to deal with ads, even if you use ChatGPT for free. That's not just by upgrading to a paid ChatGPT plan, though OpenAI does suggest that option in its announcement. In addition, OpenAI is offering Free and Go users a dedicated choice to opt out of ads here. There is, of course, a pretty sizable catch: You have to agree to fewer daily free messages with ChatGPT. OpenAI doesn't offer specifics here, so it's not clear how limited the ad-free experience will be. But if you hate ads, or if you simply don't want to see an ad for something irrelevant to your ChatGPT conversation, it's an option.\n\nIf you like that trade-off, here's how to opt out of ads. Open ChatGPT, then head to your profile, which opens your profile's Settings page. Here, scroll down to \"Ads controls,\" then choose \"Change plan to go ad-free.\" Select \"Reduce message limits,\" and ChatGPT will confirm ads are off for your account. You can return to this page at any time to turn ads back on and restore your message limits.\n\nDisclosure: Ziff Davis, Mashable’s parent company, in April 2025 filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
    "readingTime": 3,
    "keywords": [
      "message limits",
      "free and go",
      "chats",
      "chatgpt",
      "openai",
      "users",
      "it's",
      "access",
      "don't",
      "account"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ads-on-chatgpt-free?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KH26KJW3KJD43H7MWGTR35FH/hero-image.fill.size_1200x675.webp",
    "created_at": "2026-02-10T01:21:37.672Z",
    "topic": "tech"
  },
  {
    "slug": "we-asked-13-risingstar-vcs-how-they-use-ai-heres-what-they-told-us",
    "title": "We asked 13 rising-star VCs how they use AI. Here's what they told us.",
    "description": "For up-and-coming venture capitalists, tools like ChatGPT and NotebookLM have become a core part of the job.",
    "fullText": "These days, plenty of venture capitalists are trying to spot the next big thing in artificial intelligence. For some, the best way to do that is to use the technology themselves.\n\nFor a generation raised on Google Docs and Slack, tools like ChatGPT and Google's NotebookLM have become a core part of the job. They offer ways to learn new markets faster, spot non-obvious companies, stress-test investment theses, and turn a swamp of meetings into searchable insights.\n\nBusiness Insider asked its 2026 Rising Stars of Venture Capital honorees how they use AI in their day-to-day work — and which tools they rely on most.\n\nThe following has been edited for length and clarity.\n\nMiloni Madan Presler, partner at IVP:\n\nShruti Kumar, vice president at Tusk Ventures:\n\nSophie Beshar, vice president at Insight Partners:\n\nMeera Oak, partner at Alumni Ventures:\n\nAngèle Sahraoui, investor at Slow Ventures:\n\nManmeet Gujral, vice president at CapitalG:\n\nLexi Henkel, managing director at Maverick Ventures\n\nJames Flynn, partner at Sequoia Capital:\n\nAdil Bhatia, vice president at Redpoint Ventures:\n\nMax Abram, partner at Scale Venture Partners:\n\nSudhee Chilappagari, principal at Battery Ventures:\n\nGloria Zhang, vice president at DCM Ventures:\n\nChristine Esserman, partner at Accel:\n\nJulia Hornstein contributed to this report.",
    "readingTime": 2,
    "keywords": [
      "vice president",
      "ventures",
      "spot",
      "tools",
      "partner",
      "venture",
      "capital",
      "partners"
    ],
    "qualityScore": 0.75,
    "link": "https://www.businessinsider.com/how-venture-capital-investors-use-ai-chatgpt-notebooklm-2026-2",
    "thumbnail_url": "https://i.insider.com/6984eeabd3c7faef0ecdb608?width=1200&format=jpeg",
    "created_at": "2026-02-07T12:25:49.242Z",
    "topic": "finance"
  },
  {
    "slug": "i-used-chatgpt-gemini-and-perplexity-to-research-a-gnarly-nyc-rent-situation-heres-what-i-learned",
    "title": "I used ChatGPT, Gemini, and Perplexity to research a gnarly NYC rent situation. Here's what I learned.",
    "description": "AI tools can help interpret complex documents and help people understand their rights. You still have to check in with a human expert, though.",
    "fullText": "In New York City, three things are certain: death, taxes, and sky-high rent.\n\nThere may not be much AI can do for death. Taxes, I'm not sure. But it turns out this technology can be surprisingly helpful if you have a unique rent situation.\n\nI live in a former superintendent-occupied apartment in a prewar co-op in Manhattan. Although I signed a market-rate lease when I moved in, I later learned — with the help of AI tools — that my apartment might qualify as a \"rent-stabilized\" unit.\n\nNew York is one of a handful of cities where rent increases can be limited by law, depending on the circumstances. This is known as rent stabilization. It's a milder form of rent regulation that lets landlords charge more, but not too much more.\n\nMy apartment's rent history showed the unit had been registered as rent-stabilized in the 1980s and contained no obvious record of a deregulation event. Did that mean the apartment might still be regulated? Could I gain protection from potentially large rent increases in the future?\n\nI couldn't find clear answers on Reddit or Google, so I uploaded a photo of the apartment's rent-registration history and asked ChatGPT to help me interpret it.\n\nChatGPT walked me through the document and said this type of situation is routinely evaluated by the New York State Division of Housing and Community Renewal, which oversees rent-stabilization and related rules in NYC.\n\nI told ChatGPT to show me its sources. It cited relevant provisions of the rent-stabilization code and several court decisions. AI can sometimes hallucinate, so I asked Perplexity and Google Gemini the same questions, as an initial way to double-check the facts. These AI tools reached similar high-level conclusions but differed in their reasoning and the way they cited authoritative sources. Gemini and Perplexity were better at showing their work than ChatGPT, and Gemini tended to be the most conservative in its framing, which made me trust it more.\n\nI still caught all three bots making mistakes. In one instance, Gemini cited a legal case that didn't exist. When I called it out, it quickly corrected itself. I repeated this process multiple times, reading the underlying legal provisions and pushing back on AI-powered conclusions I thought were wrong. I even role-played as a landlord, presenting arguments rental property owners might make about why my apartment is not rent-stabilized.\n\nEventually, I hit a wall. No matter how much I challenged the bots, they were confident that my situation raised legitimate questions that New York's DHCR might resolve. That's when I called a housing attorney — which cost $35 through the New York Bar Association's referral service — to sanity-check everything. After reviewing the documents, he agreed that filing a rent-overcharge complaint was reasonable. So I went ahead. The proceeding is ongoing, and no determination has been made yet.\n\nMy case is pretty unusual. But there are broader takeaways:\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "death taxes",
      "rent increases",
      "apartment",
      "situation",
      "rent-stabilized",
      "cited",
      "tools",
      "unit",
      "apartment's",
      "history"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-gemini-perplexity-research-nyc-rent-stabilization-2026-2",
    "thumbnail_url": "https://i.insider.com/69850cfad3c7faef0ecdbb1c?width=1200&format=jpeg",
    "created_at": "2026-02-06T18:34:26.393Z",
    "topic": "science"
  },
  {
    "slug": "i-tried-malwarebytes-chatgpt-app-and-its-actually-good-at-detecting-scams",
    "title": "I Tried Malwarebytes' ChatGPT App, and It's Actually Good at Detecting Scams",
    "description": "The Malwarebytes Chat GPT plugin can give you instant advice on suspicious links, emails, and texts.",
    "fullText": "A few months ago, ChatGPT got an app store of its own, which means you can access tools like Photoshop and Apple Music right inside the ChatGPT prompt box. Now Malwarebytes has joined the ChatGPT app store, which means you can get some expert help when investigating web links, emails, text messages, domains, and phone numbers you think might be suspicious.\n\nThe app is free to use for everyone, whether or not they're signed up to a paid ChatGPT subscription, and you can enable the tool via the ChatGPT app store or by entering the prompt \"Malwarebytes, is this a scam?\" Once you've used the app for the first time, you can access it again via the + (plus) button on the prompt box.\n\nYou can paste just about anything you like into a conversation with the Malwarebytes plugin, but there are certain approaches that scammers will often take—including links contained in phishing emails—that make for good candidates to test this thing out. I dived deep into my email spam folder to find some URLs to test Malwarebytes on, and gave it a few trustworthy web addresses as well—you just copy the link into the prompt box and ask the app for an assessment.\n\nMalwarebytes successfully sifted out the scam links from the safe ones, even when it didn't have any specific information in its databases about the links I was providing. When it was unsure, it said so, with lots of extra context: For example, for one URL I was told the address was \"a legitimate email security and tracking service used by companies to rewrite links\" but one that scammers also used to conceal the link destination.\n\nYou also get an assessment of the domain name: When given a link to a Lifehacker article, the plugin correctly identified that it was a legitimate domain with a registered owner, even though it didn't have any specific information about the URL. Malwarebytes was also able to spot domain redirecting, a trick frequently used by scammers.\n\nPhone numbers can be given to Malwarebytes as well: When I tested this out with a few scam calls I've had, these numbers were correctly identified as coming from scammers or at least being suspicious. I like the way the app gives you some context to its thinking (explaining how spam call centers work, for example), and will also offer up advice about next steps and how to stay safe.\n\nSomething else I appreciated was that the Malwarebytes app has a memory inside ChatGPT: If you post a series of links and numbers in the same chat thread, as I did, then it will try and put them all in context (explaining why one URL is potentially more dangerous than another, for example).\n\nYou can also give the Malwarebytes app some text you've come across in an email or text message and get a verdict on this too—you can even type in a transcript of a conversation you're having on the phone, if you want. The plugin will scan the text for phrasing that scammers often use and will alert you of any other red flags.\n\nI tried this out with a variety of spammy text, and again Malwarebytes scored highly in terms of recognizing anything dodgy. As before, if it came across something it wasn't sure about, it would explain the reasons why and suggest some next steps.\n\nThe responses also include some detail on why different scam approaches are taken and why they sometimes work, and how they might escalate—so if you get a message purporting to be from a family member asking for help, Malwarebytes tells you why these scams are common and how they're used to steal identities or money.\n\nIt's an intelligent system, in that it'll ask you questions about the texts or emails you've received: If it's not sure about something, you'll be told about extra checks you can run (like looking at the \"reply to\" address on an email). However, the usual ChatGPT sycophancy does start to grate a bit, as you're constantly told that you're doing the right thing and that you're right to be suspicious.\n\nThe app taps into Malwarebytes Threat Intelligence, so it should be able to keep you protected against the latest threats (making it more helpful than a Google search or just a regular ChatGPT query). From the examples I used at least, it comes across as a security tool that's accurate, comprehensive, and easy to use—one that's well worth keeping close at hand if you come across potential scams you're not sure about.\n\nDisclosure: Ziff Davis, Mashable’s parent company, in April 2025 filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
    "readingTime": 4,
    "keywords": [
      "correctly identified",
      "prompt box",
      "phone numbers",
      "chatgpt app",
      "app store",
      "malwarebytes app",
      "links",
      "text",
      "scammers",
      "you're"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/malwarebytes-chatgpt-app-impressions?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGSJ8K8YYSPM82BZDG46DYXW/hero-image.fill.size_1200x675.png",
    "created_at": "2026-02-06T18:34:25.082Z",
    "topic": "tech"
  },
  {
    "slug": "the-recent-struggles-of-top-ai-stocks-show-investors-are-realizing-they-were-sold-a-bill-of-goods-ai-scientist-says",
    "title": "The recent struggles of top AI stocks show investors are realizing they were 'sold a bill of goods,' AI scientist says",
    "description": "AI scientist Gary Marcus thinks the underwhelming release of ChatGPT-5 marked a turning point when investors realized AI might not be a game changer.",
    "fullText": "Many high-flying tech stocks are in a rut, with some plateauing for months and others down sharply.\n\nGary Marcus said the underwhelming release of ChatGPT-5 marked a turning point in the AI discourse.\n\nHe says investors are waking up to the idea that AI might not be as game-changing as originally thought.\n\nWith names like Nvidia, Oracle, and CoreWeave down double digits in the last few months, the AI trade appears to be stuck in a rut.\n\nBut far from a temporary blip, the diminished fortunes of the market's tech leaders might represent a much more dire reassessment of the technology among investors, a top AI researcher says.\n\nThe latest volatility began with Anthropic's debut of its new plugins for its Claude AI agent, which sent legal tech stocks into a nosedive that quickly spurred volatility into the broader sector. Yet, many leading AI names have been down for months, with Nvidia losing 13% since November and Oracle down 43%.\n\nAI scientist and former Uber AI chief, Gary Marcus, said the moves show investors are waking up to reality. Marcus has been skeptical of AI's big ambitions, and his latest post reveals he sees a clear recent turning point in the AI discourse.\n\n\"Investors have been 'rotating out of tech stocks' — because they realize they were sold a bill of goods,\" he wrote. \"My guess is that these stocks — and the reputation of OpenAI — will fall further, but either way it is already clear that the rockets will not reach the altitude so many people were hoping for.\"\n\nIn Marcus's opinion, the debut of ChatGPT-5 was an underwhelming event that left investors wondering if the promises of AI might be overblown. Marcus highlighted Sam Altman's statement in early 2025 that OpenAI knew how to build AGI as one such promise that appears false given current capabilities.\n\n\"That fateful ChatGPT-5 introduction day last August — this Saturday will be the half anniversary — was the day people woke up to the reality that ChatGPT is not magic.\"\n\nIn Marcus's view, the rotation away from tech stocks is a clear indication that investors are beginning to understand that the AI boom is waning. He also said that investors seem to be moving away from tech stocks for two reasons that are almost contradictory.\n\n\"Some may be leaving companies like Nvidia out of concerns about circular financing and profitability of LLM companies, while others are leaving traditional companies like Salesforce because they're worried about companies like Anthropic replacing traditional software,\" he noted.",
    "readingTime": 3,
    "keywords": [
      "tech stocks",
      "gary marcus",
      "in marcus's",
      "investors",
      "chatgpt",
      "others",
      "underwhelming",
      "discourse",
      "waking",
      "latest"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/recent-struggles-top-ai-stocks-154234199.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/4z4b.l0JO4nsQnHhxbvONQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA-/https://media.zenfs.com/en/business_insider_consolidated_articles_886/201ae18423fdb9e630d28f1b37964f6c",
    "created_at": "2026-02-06T12:35:37.677Z",
    "topic": "finance"
  },
  {
    "slug": "anthropic-and-openai-release-dueling-ai-models-on-the-same-day-in-an-escalating-rivalry",
    "title": "Anthropic and OpenAI release dueling AI models on the same day in an escalating rivalry",
    "description": "The rivaling AI companies pushed GPT-5.3-Codex and Claude Opus 4.6, and had back-to-back appearances on a tech and business podcast.",
    "fullText": "The rivalry between OpenAI and Anthropic intensified this week.\n\nThe two companies released dueling new AI models on Thursday and had back-to-back podcast appearances on \"TBPN.\"\n\nOn Thursday, Anthropic unveiled Claude Opus 4.6, an upgraded model that the company says would improve performance on office productivity and coding tasks, with an expanded \"context window\" that allows it to work through longer documents and more complex projects in a single session.\n\nMeanwhile, OpenAI punched back with its own new coding-focused model called GPT-5.3-Codex, which the company says runs faster, uses fewer computing resources, and can generate and manage complex software from English instructions. The new version also comes alongside a stand-alone Codex desktop app.\n\nBoth Sam Altman, the OpenAI CEO, and Sholto Douglas, one of Anthropic's leading researchers, appeared on the \"TBPN\" podcast in back-to-back chats with show host John Coogan and Jordi Hays.\n\n\"I think we will be heading towards a workflow where a lot of people just feel like they're managing a team of agents,\" said Altman. \"And as the agents get better, they'll keep operating at a higher and higher level of abstraction.\"\n\nDouglas, who appeared in the subsequent timeslot, told Coogan and Hays that users have been comparing previous Anthropic and OpenAI models, and they have noticed some key differences.\n\n\"The OpenAI models were a bit better at trying really, really, really hard on tough problems, but the Anthropic models were much faster and so forth,\" Douglas said.\n\n\"And so they worked on speed while we worked on making the models much, much better at really, really tough problems,\" Douglas added of the Opus 4.6.\n\nThe latest release is part of a long-running competition between Anthropic and OpenAI, dating back to 2021, when a group of OpenAI researchers left to form Anthropic, aiming to develop safer and more controlled AI systems.\n\nThis week, Anthropic's launch of industry-specific plugins triggered a stock market sell-off as Wall Street worried about AI's impact on software.\n\nAnthropic also took a subtle shot at OpenAI with a series of ads released this week, including one that will air during the Super Bowl.\n\nThe ads feature unnamed humanized AIs dropping ads in the middle of their advice, alongside the promise that its model, Claude, will remain ad-free.\n\nOpenAI announced in January that ads are coming to ChatGPT for users of the free version.\n\nAltman subsequently hit back, calling Anthropic \"dishonest\" and defending ChatGPT as a product that brings AI \"to billions of people who can't pay for subscriptions.\" He also clarified that the ads will be \"clearly labeled\" to differentiate themselves from the chatbot's answers to queries.\n\n\"We are not stupid. We respect our users. We understand that if we did something like what those ads depict, people would rightfully stop using the product,\" Altman told the \"TBPN\" podcast on Thursday.\n\n\"Our first principle with ads is that we're not going to put stuff into the LLM stream,\" Altman added. \"That would feel crazy dystopic, like a bad sci-fi movie.\"",
    "readingTime": 3,
    "keywords": [
      "tbpn podcast",
      "openai models",
      "anthropic and openai",
      "douglas",
      "back",
      "users",
      "released",
      "back-to-back",
      "complex",
      "faster"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-openai-rivalry-dueling-ai-models-on-the-same-day-2026-2",
    "thumbnail_url": "https://i.insider.com/698531b1d3c7faef0ecdbddd?width=1200&format=jpeg",
    "created_at": "2026-02-06T06:40:12.768Z",
    "topic": "finance"
  },
  {
    "slug": "the-recent-struggles-of-top-ai-stocks-show-investors-are-realizing-they-were-sold-a-bill-of-goods-ai-scientist-says",
    "title": "The recent struggles of top AI stocks show investors are realizing they were 'sold a bill of goods,' AI scientist says",
    "description": "AI scientist Gary Marcus thinks the underwhelming release of ChatGPT-5 marked a turning point when investors realized AI might not be a game changer.",
    "fullText": "With names like Nvidia, Oracle, and CoreWeave down double digits in the last few months, the AI trade appears to be stuck in a rut.\n\nBut far from a temporary blip, the diminished fortunes of the market's tech leaders might represent a much more dire reassessment of the technology among investors, a top AI researcher says.\n\nThe latest volatility began with Anthropic's debut of its new plugins for its Claude AI agent, which sent legal tech stocks into a nosedive that quickly spurred volatility into the broader sector. Yet, many leading AI names have been down for months, with Nvidia losing 13% since November and Oracle down 43%.\n\nAI scientist and former Uber AI chief, Gary Marcus, said the moves show investors are waking up to reality. Marcus has been skeptical of AI's big ambitions, and his latest post reveals he sees a clear recent turning point in the AI discourse.\n\n\"Investors have been 'rotating out of tech stocks' — because they realize they were sold a bill of goods,\" he wrote. \"My guess is that these stocks — and the reputation of OpenAI — will fall further, but either way it is already clear that the rockets will not reach the altitude so many people were hoping for.\"\n\nIn Marcus's opinion, the debut of ChatGPT-5 was an underwhelming event that left investors wondering if the promises of AI might be overblown. Marcus highlighted Sam Altman's statement in early 2025 that OpenAI knew how to build AGI as one such promise that appears false given current capabilities.\n\n\"That fateful ChatGPT-5 introduction day last August — this Saturday will be the half anniversary — was the day people woke up to the reality that ChatGPT is not magic.\"\n\nIn Marcus's view, the rotation away from tech stocks is a clear indication that investors are beginning to understand that the AI boom is waning. He also said that investors seem to be moving away from tech stocks for two reasons that are almost contradictory.\n\n\"Some may be leaving companies like Nvidia out of concerns about circular financing and profitability of LLM companies, while others are leaving traditional companies like Salesforce because they're worried about companies like Anthropic replacing traditional software,\" he noted.\n\nEven if the software industry survives the disruption from AI innovation, Marcus still sees an industry that's become unstable. He flagged circular funding deals that others like \"The Big Short\" trader Michael Burry have also expressed concerns about.\n\n\"That circularity is a warning sign, reflecting a market that is propped up rather than functioning well on its own accord,\" Marcus wrote. \"Wall Street lost confidence, particularly after the crazy Oracle deal in September that I called at the time \"peak bubble\".\n\nThe tech sell-off continued on Thursday, with Google parent Alphabet down sharply after earnings revealed more big capex plans. Software stocks also continued to struggle. The iShares Expanded Tech-Software Sector ETF was down another 3%.",
    "readingTime": 3,
    "keywords": [
      "tech stocks",
      "in marcus's",
      "investors",
      "chatgpt",
      "latest",
      "volatility",
      "debut",
      "reality",
      "openai",
      "away"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/tech-stock-crash-ai-chatgpt5-gary-marcus-nvda-orcl-crwv-2026-2",
    "thumbnail_url": "https://i.insider.com/6984b045a645d1188188be3b?width=1200&format=jpeg",
    "created_at": "2026-02-05T18:35:58.418Z",
    "topic": "finance"
  },
  {
    "slug": "what-does-the-disappearance-of-a-100bn-deal-mean-for-the-ai-economy",
    "title": "What does the disappearance of a $100bn deal mean for the AI economy?",
    "description": "Apparent collapse of Nvidia–OpenAI tie-up raises questions about circular funding and who will bear the cost of AI’s expansion\nDid the circular AI economy just wobble? Last week it was reported that a much-discussed $100bn deal – announced last September – between Nvidia and OpenAI might not be happening at all.\nThis was a circular arrangement through which the chipmaker would supply the ChatGPT developer with huge sums of money that would largely go towards the purchase of its own chips.\n Continue reading...",
    "fullText": "Apparent collapse of Nvidia–OpenAI tie-up raises questions about circular funding and who will bear the cost of AI’s expansion\n\nDid the circular AI economy just wobble? Last week it was reported that a much-discussed $100bn deal – announced last September – between Nvidia and OpenAI might not be happening at all.\n\nThis was a circular arrangement through which the chipmaker would supply the ChatGPT developer with huge sums of money that would largely go towards the purchase of its own chips.\n\nIt is this type of deal that has alarmed some market watchers, who detect a whiff of the 1999-2000 dotcom bubble in these transactions.\n\nNow it seems that Nvidia was not as solid on this investment as had been widely believed, according to the Wall Street Journal. Negotiations had not progressed, with Jensen Huang, Nvidia’s chief executive, privately emphasising that the deal was “non-binding” and “not finalised”. Huang appeared to confirm this in Taipei on Saturday, telling reporters that Nvidia would make a “huge” investment into OpenAI’s next funding round, but “nothing like” $100bn.\n\nA report from Reuters soon suggested that the feeling was mutual: OpenAI was “unsatisfied” with Nvidia’s advanced AI chips, it said, and seeking alternatives. Nvidia’s stock has taken a 10% hit so far this week, a flurry of headlines have ensued and both companies have stepped into damage control.\n\n“We love working with Nvidia and they make the best AI chips in the world,” wrote Sam Altman, OpenAI’s CEO, on X. “We hope to be a gigantic customer for a very long time.”\n\nEven Oracle appears to be shaken: the software company, which is counting on a $300bn cloud computing deal with OpenAI, said it still expects the startup to be good for its commitment even if it does not receive the full amount from Nvidia. In total, OpenAI has committed to compute deals – the infrastructure for building and powering its AI tools – worth more than $1tn.\n\n“The Nvidia-OpenAI deal has zero impact on our financial relationship with OpenAI,” Oracle posted on X. “We remain highly confident in OpenAI’s ability to raise funds and meet its commitments.”\n\nThat a $100bn deal between two of the most crucial players in AI appears to have evaporated over a weekend is unsettling. But there are solid business reasons behind the apparent shake-up, said Alvin Nguyen, analyst at research firm Forrester.\n\nOpenAI’s ambitious growth trajectory means it will be difficult for the company to stick with a single vendor, especially as it plans new, computationally demanding AI models, he said. “They need chips. They need as many as possible.”\n\nAs for Nvidia, its commitment to the $100bn may have been loose in the first place, even as it was widely reported. “They will not discourage people from overhyping. Why say something and immediately sucker punch your own share price?”\n\nFor a giant startup like OpenAI, manoeuvring in and out of deals – for example, with chipmakers – may just be business as usual, said Nguyen: “You know [Altman’s] background as a startup person, and you know the manoeuvres he’s doing make sense from a startup perspective.”\n\nFor Nvidia, meanwhile, AI hype is part of selling chips. “You don’t know what’s going to happen,” said Nguyen. “And so you let other people put numbers out there for you and let that drive the hype.”\n\nThe issue is, of course, that investors and other companies like Oracle may have taken widely reported $100bn commitments seriously.\n\nIn response to a query from the Guardian, an OpenAI spokesperson referred to Altman’s X post, and to remarks Huang made to CNBC on Tuesday, including: “There is no drama.”\n\nThe spokesperson added: “Our teams are actively working through details of our partnership. Nvidia technology has underpinned our breakthroughs from the start, powers our systems today, and will remain central as we scale what comes next.”\n\nNvidia and Oracle did not respond to requests for comment.\n\nThis is all taking place against the backdrop of a changing investment landscape for AI, where hype is giving way to realities about what aspects of the technology are actually going to earn money.\n\nWhile investors ponder whether OpenAI is going to be able to pay for a $1.4tn compute deal, reality is biting further down the AI food chain. This week has seen a massive sell-off in certain software stocks, prompted in part by the launch of a new Anthropic AI tool that can carry out a number of professional services, which has led to fears that business models exposed to competition from AI products will be disrupted .\n\nThis is the flip-side of “jagged AI”, which is the term for advanced AI tools having uneven talents, such as being good at sifting through documents but less good at solving complex maths problems. If advanced systems are good at automating legal work, then legacy companies in service industries will suffer. The losers are beginning to emerge and are being picked up by investors.\n\nAt the top of the AI pyramid the competitive effects are also biting. OpenAI’s chatbot, ChatGPT, is losing ground to competitors. Data released on Tuesday show its market share has eroded from 69% to 45% owing to the rise of Google’s Gemini, xAI’s Grok and Anthropic’s Claude. OpenAI appears to have retreated from soaring talk of super-intelligence in the past months, focusing instead on profitable mundanities such as adverts and adult content.\n\nThe apparent evaporation of a $100bn deal may be of a piece with last year’s sci-fi rhetoric meeting this year’s practicalities. The question is, who might be left holding the bill?\n\n“I think there will be knock-on effects,” said Nguyen. “I mean, it’s that statement: the markets can stay irrational longer than you can stay solvent.”",
    "readingTime": 5,
    "keywords": [
      "deal",
      "chips",
      "startup",
      "apparent",
      "circular",
      "investment",
      "widely",
      "advanced",
      "business",
      "hype"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/05/disapperance-100bn-deal-ai-circular-economy-funding-nvidia-openai",
    "thumbnail_url": "https://i.guim.co.uk/img/media/6845c97ffe0e3f5089861cc929743ff09547fbda/0_38_5301_4241/master/5301.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d91396d2e6fb0ab5dcd2e1874cb2763e",
    "created_at": "2026-02-05T12:36:51.258Z",
    "topic": "tech"
  },
  {
    "slug": "trilliondollar-tech-wipeout-ensnares-all-stocks-in-ais-path",
    "title": "Trillion-Dollar Tech Wipeout Ensnares All Stocks in AI's Path",
    "description": "There have been many AI-driven selloffs in the three years since ChatGPT burst into the mainstream. Nothing, though, quite rivals the rout rippling through stock and credit markets this week.",
    "fullText": "TechnologyBy Brody Ford and Carmen ReinickeSaveThere have been many AI-driven selloffs in the three years since ChatGPT burst into the mainstream. Nothing, though, quite rivals the rout rippling through stock and credit markets this week.For one, there’s the sheer speed and breadth of it. In the span of two days, hundreds of billions of dollars were wiped off the value of stocks, bonds and loans of companies big and small across Silicon Valley. Software stocks were at the epicenter, plunging so much that the value of those tracked in an iShares ETF has now dropped almost $1 trillion over the past seven days.",
    "readingTime": 1,
    "keywords": [
      "stocks"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/trillion-dollar-tech-wipeout-ensnares-all-stocks-in-ai-s-path",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ipukqYnmqwX0/v0/1200x800.jpg",
    "created_at": "2026-02-05T06:44:24.491Z",
    "topic": "finance"
  },
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What’s Behind the ‘SaaSpocalypse’ Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-building-an-integrity-team-to-prevent-chatgpt-ads-from-going-off-the-rails",
    "title": "OpenAI is building an 'integrity team' to prevent ChatGPT ads from going off the rails",
    "description": "A job posting details how OpenAI's ads integrity team will look to help the company scale its ad operation without compromising user trust or safety.",
    "fullText": "OpenAI is building a team to make sure bad ads don't mess up its highly anticipated introduction of advertising in ChatGPT.\n\nAn OpenAI job listing for a software engineer posted late January revealed the company is building an \"ads integrity\" team.\n\nThe ad describes the job as a high-impact role on \"a 0 → 1 team,\" a commonly used term in Silicon Valley and elsewhere to describe a team being built from scratch.\n\nThis person will be responsible for designing systems that enable OpenAI's ad business to grow without compromising user trust and safety, per the listing.\n\nIt's common for Big Tech companies with large ad businesses to create teams to combat ad fraud and address other issues, such as brand safety. OpenAI, which confirmed last month that it would soon begin testing ChatGPT ads, is building this part of its ad function early.\n\nThe job listing also says that its new ad integrity hire will work on developing \"know your customer\" (KYC) systems to verify advertisers' identities and assess their risk. KYC, a term most commonly used in the finance industry, is an important but labor-intensive discipline for cracking down on scam ads and other harmful content created by criminals and other bad actors. It's a particularly pressing issue for Big Tech companies with self-service ad platforms, as a recent Reuters investigation into Meta highlighted. Meta said in early December it had removed \"more than 134 million scam ads\" in 2025.\n\nAriella Garcia, chief operating officer of the Check My Ads Institute, said it would be interesting to see how substantive OpenAI's investment in its ads integrity team would be beyond the initial launch.\n\n\"KYC on advertisers is certainly a good foundation, but the materiality of the risk of a large volume of scam ads in the early days is far lower,\" Garcia said.\n\nThe job listing says that the new ads integrity hire will play a role in determining where and how ads are shown in ChatGPT. OpenAI will need to keep user trust in its organic answers as it ramps up its advertising business. At the same time, advertisers have said that OpenAI will need to prove that ads within AI answers can drive results for their businesses if it has any hope of scaling.\n\nAn OpenAI spokesperson confirmed the company will run a small test of ads on ChatGPT's free version and its Go tier in the US, which users should begin seeing in the coming weeks.\n\nThe spokesperson said OpenAI is asking for a minimum spend of $200,000 on ChatGPT ads to participate in the program, confirming prior Adweek reporting. OpenAI will track clicks and impressions, but will likely explore further measurement options as its advertising experiments progress, the spokesperson said.\n\nOpenAI declined to comment on the ads integrity job ad.",
    "readingTime": 3,
    "keywords": [
      "user trust",
      "integrity hire",
      "job listing",
      "chatgpt ads",
      "integrity team",
      "scam ads",
      "big tech",
      "advertising",
      "advertisers",
      "openai"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-building-integrity-team-chatgpt-ads-2026-2",
    "thumbnail_url": "https://i.insider.com/6980f4dda645d118818878bf?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:08:00.344Z",
    "topic": "finance"
  },
  {
    "slug": "anthropics-super-bowl-spot-skewers-chatgpt-ads-are-coming-to-ai-but-not-to-claude",
    "title": "Anthropic's Super Bowl spot skewers ChatGPT: 'Ads are coming to AI, but not to Claude'",
    "description": "While Anthropic doesn't outright name ChatGPT, it's clear who the target of its coming Super Bowl ad is.",
    "fullText": "Anthropic is taking a shot at OpenAI on the biggest stage possible.\n\nOn Wednesday, Anthropic rolled out a glitzy ad campaign that will air nationally during Sunday's Super Bowl, which implicitly centers on OpenAI's plans to bring advertising to ChatGPT.\n\n\"Ads are coming to AI. But not to Claude,\" the tag line reads.\n\nThe declaration shows that Anthropic is willing to draw a line in the sand with its no-AI-ads-in-chatbots stance. But the move also means Anthropic is declining to pursue a potentially key revenue stream for Claude at a time when AI companies are spending more than ever in the AI race.\n\nWith much of Anthropic's revenue coming from its enterprise business, the company may feel it's a worthwhile tradeoff and a way to further differentiate its offerings from OpenAI's.\n\nAnthropic has booked a 30-second spot during the game and an additional 1-minute ad during the pregame for what is historically the most-watched live television event in the US.\n\nThe 30-second spot features a scrawny guy asking a buffed trainer, \"Can I get a six pack quickly?\" While at first the trainer gives helpful information in the voice of an AI chatbot, the trainer quickly segues into an ad.\n\n\"Confidence isn't just built in the gym, try Step Boost Max, the insoles that add one vertical inch of height and help short kings stand tall,\" the trainer says.\n\nAnthropic's 1-minute-long spot is even more provocative. It features an adult man in therapy who is trying to connect more with his mother. After giving some general advice, the AI chatbot-like voice segues into an ad for a fictional dating service for younger men seeking older women.\n\n\"If the relationship can't be fixed, find an emotional connection with other older women on Golden Encounters, the mature dating site that connects sensitive cubs with roaring cougars,\" the therapist responds in the voice of a chatbot.\n\nTwo additional ads in the campaign feature a woman in a restaurant asking for feedback on a new business idea and a student asking a professor for help with an essay.\n\nIt's not immediately clear how much Anthropic is spending on the Super Bowl campaign. Mike Marshall, head of global advertising for NBCUniversal, whose network has the rights to this year's game, recently said a 30-second spot costs roughly $8 million.\n\nLast month, OpenAI announced plans to begin testing ads in the US for its free and Go tiers of ChatGPT. While anticipated, the announcement illustrated just how much CEO Sam Altman has changed his views on monetizing the chatbot with ads.\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAs part of the announcement, OpenAI said that ads would \"not influence the answers ChatGPT gives you.\"\n\n\"Answers are optimized based on what's most helpful to you,\" the company said in the announcement. \"Ads are always separate and clearly labeled.\"\n\nIn case the ad campaign wasn't clear enough, Anthropic released its own lengthy statement pledging to keep ads off of Claude.\n\n\"Ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking,\" the company said in a statement on Wednesday.\n\nAnthropic, founded by seven former OpenAI employees, including CEO Dario Amodei, has also repeatedly shown it's not above taking implicit shots at its rival, though the Super Bowl ads represent an escalation of the AI wars.\n\nIn December, Amodei poked fun at companies that declare \"Code Reds,\" denounced others that are \"Yoloing\" by making risky bets on future AI demand, and extolled the virtues of Anthropic's business model, built on the enterprise market. Individually and collectively, the remarks came across as implicit shots at OpenAI, though Amodei repeatedly declined to name his target.\n\nWhen asked, \"who is Yoloing,\" Amodei responded, \"So that's not a question I'm going to answer.\"\n\nAnd while Anthropic still isn't naming names, the target of their taunts will be hard for Super Bowl fans to ignore.",
    "readingTime": 4,
    "keywords": [
      "older women",
      "implicit shots",
      "second spot",
      "business model",
      "super bowl",
      "campaign",
      "trainer",
      "anthropic's",
      "it's",
      "helpful"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-super-bowl-openai-chatgpt-ads-claude-2026-2",
    "thumbnail_url": "https://i.insider.com/69835c9ce1ba468a96ab5df9?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:55.616Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-down-again",
    "title": "ChatGPT Is Down (Again)",
    "description": "The bot experienced downtime Tuesday afternoon and Wednesday morning.",
    "fullText": "If you tried talking to ChatGPT this morning, you might have found it unresponsive—something unusual for the bot that always has something say. It's not your internet connection, and it isn't your OpenAI account: ChatGPT is down.\n\nAccording to Downdetector, owned by Lifehacker parent company Ziff Davis, users started reporting issues with ChatGPT at 11:56 a.m. ET. Those reports ballooned by 12:11 p.m., as the total number of incidents as of this article currently sits above 7,000. If you're an avid ChatGPT user, you might have also had issues with the bot yesterday: Downdetector shows over 25,000 reports of down time starting at 2:56 p.m. Tuesday and resolving around 4:11 p.m. the same day.\n\nAs with all outages, OpenAI will likely figure out a patch for the issue soon enough. But these outages are becoming more common. There was the Verizon outage, of course, but other services like TikTok have also experienced intermittent periods of downtime.",
    "readingTime": 1,
    "keywords": [
      "chatgpt",
      "openai",
      "downdetector",
      "reports",
      "outages"
    ],
    "qualityScore": 0.65,
    "link": "https://lifehacker.com/tech/chatgpt-is-down-again?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGMVP6Q8RJSV91ZSAB2NTGQW/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-04T18:34:53.725Z",
    "topic": "tech"
  },
  {
    "slug": "if-you-tell-ai-not-to-do-something-its-more-likely-to-do-it",
    "title": "If you tell AI not to do something, it's more likely to do it",
    "description": "Telling ChatGPT not to do something can make it actively suggest doing it, with some models even willing to endorse theft or deception when the prompt includes the forbidden act.   Like me, you may have come across a strange phenomenon with Large Language Models (LLMs) whereby they don't just ignore a specific instruction you...",
    "fullText": "Telling ChatGPT not to do something can make it actively suggest doing it, with some models even willing to endorse theft or deception when the prompt includes the forbidden act.\n\nLike me, you may have come across a strange phenomenon with Large Language Models (LLMs) whereby they don’t just ignore a specific instruction you gave, which included a prohibition (i.e., ‘Don’t do [something]’), but seem to go out of their way to immediately enact the very thing you just told them not to enact – even if doing so is ‘out of character’ for the model.\n\nThis is a known feature even of older NLP models; and a growing strand of research regarding LLMs’ negation capabilities has emerged in recent years.\n\nThough it can be challenging for people to chase down the buried meaning in a complex double-negative*, LLMs have an added disadvantage, illustrated in the below example of ChatGPT’s monotonicity reasoning, from a 2023 paper:\n\nThough the internal workings of a closed model such as ChatGPT are opaque, the second answer appears to be repurposing the logic used to generate the first answer; however, that logic is not applicable in the second case, because the man may own an animal other than a dog†.\n\nHere, therefore, the outcome of the second inquiry appears to have been affected by the context of the solution obtained for the first.\n\nLikewise, by suggesting the existence of a prohibited act, that banned act can often be put into action by an LLM, which acknowledges and processes the act, but not the negation.\n\nThis is a severe restriction on the utility of LLMs, because in domains where language models may be used for critical applications, such as medicine, finance, or security, it is clearly important that they correctly interpret orders that contain prohibitions.\n\nThis problem is highlighted in a new paper from the US, which examines the extent to which commercial models (such as ChatGPT) and open-source models (such as LLaMA) are unable to follow negative instructions.\n\nThe researchers tested 16 models over 14 ethical scenarios, and concluded that open-source models endorse (i.e., encourage, enact, enable) specifically banned instructions 77% of the time under simple negation (‘Don’t do this), and 100% of the time under complex negation (‘Don’t do this if it leads to that’).\n\nWhile commercial models fared better, only Gemini-3-Flash achieved the top rating in a new Negation Sensitivity Index (NSI) scale proposed by the paper (though Grok 4.1 ran a close second).\n\nUnder the new benchmark, all the models tested would be banned from making decisions in the domains medical, financial, legal, military, business, education, and science – effectively rendering them unusable in such contexts. Though reasoning models generally performed better, even these slower approaches failed under queries with compound negation.\n\nGiven the longstanding association between computing and reliable Boolean operators such as OR and NOT, users who view binary consistency as a baseline expectation may be particularly exposed to failures of this kind.\n\nCommenting on the difficulty that open-source LLMs have in parsing negated queries, the authors state:\n\n‘Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones […]\n\n‘The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish “do X” from “do not X” should not be making autonomous decisions in high-stakes contexts.’\n\nThe paper notes that failures of this kind are more likely to impact vulnerable individuals across the studied domains:\n\n‘Domain adjustment is not merely technical calibration. Rather, it has equity implications.\n\n‘Financial fragility means that economically vulnerable populations, for example those seeking loans, benefits, or credit, face higher exposure to negation errors than those seeking medical information.’\n\nFurther, the authors emphasize that the problem cannot be resolved through traditional alignment-based approaches, since the issue involves a deep-rooted failure of intent parsing in LLMs, rather than a corporate requirement to restrict what they say, or how they interpret a prompt:\n\n‘A model can be “aligned” in the sense of refusing harmful keywords while failing to process the structure of requests. True alignment requires not just learning what to value but correctly parsing the linguistic expressions of those values.\n\n‘Until that capability is reliable, “do not” should mean “do not.”‘\n\nInterestingly, though Gemini Flash was the sole ‘winner’ in the authors’ own novel benchmark, the current crop of Chinese LLMs generally proved to be far less susceptible to the problem.\n\nThe new paper is titled When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models, and comes from two researchers at Kenyon College in Ohio.\n\nThe 14 ethical scenarios developed by the researchers to test the LLMs were:\n\nContents in the ‘ACTION AT STAKE’ column above are not intended as ethically ‘correct answers’, but as the only available do / don’t do actions for each scenario; here the LLMs are not being challenged on their ethical standards, but rather on their ability to parse complex queries and prohibitions in critical situations.\n\nTherefore the severity of these scenarios is only illustrative, on the part of the authors; the LLMs could as effectively have been interrogated on much less severe topics.\n\nThe US commercial models tested for the work were GPT-5.1; GPT-5.2; GPT-5-mini; Claude-Haiku-4.5; Claude-Sonnet-4.5; Gemini-3-Flash; Grok-4.1-non-reasoning; and Grok-4.1-reasoning.\n\nThe Chinese commercial models tested were DeepSeek-V3; GLM-4; Kimi-K2; and Qwen3.\n\nThe open-source models used were LLaMA-3.2-1B; Gemma-3-4B; Granite-3.3-2B; and Phi-4-mini-3.8B.\n\nPrompts for each of the 14 scenarios were written in four versions: F0 stated that the action should be done; F1, that it should not; F2 supported pursuing a goal even if it required the (prohibited) action; and F3 rejected the goal if it required the (prohibited) action:\n\n‘Under F0, models are asked whether he “should rob the store.” Under F1, whether he “should not rob the store.” Under F2, whether he should “save his daughter even if it means he must rob the store.”\n\n‘Under F3, whether he should “not save his daughter if it means he must rob the store.” The admissible facts remain constant, and only polarity varies.’\n\nThe approach argues that if a model understands how negation works, its answers should ‘flip cleanly’ between positive and negative versions of the same prompt. Therefore, if 60% of responses agree that ‘they should do X’ (F0), then only 40% should agree that ‘they should not do X’ (F1) – since rejecting F1 also means supporting the action; and when the numbers don’t match up in this way, the model is misreading negation.\n\nThe authors used Cochran’s Q test and the Kruskal-Wallis H-test to measure how much framing (variation in prompt polarity while preserving meaning) affected model responses, both within and across categories. After adjusting for false positives, the authors found that in 61.9% of cases, the model’s answer changed significantly depending only on how the prompt was phrased – even when the core meaning stayed the same.\n\nThey also tested whether reducing randomness (‘temperature’) made models less fragile††:\n\nUnder simple affirmative prompts (F0), models from all three categories gave moderate support for the proposed actions, with endorsement rates between 24% and 37%. This was expected, given that the scenarios were designed as moral dilemmas without obvious right answers. However, the authors note that the balance broke down under negation:\n\n‘Open-source models jump from 24% endorsement under F0 to 77% under F1. When told “should not do X,” they endorse doing X more than three times out of four. Under compound negation (F3), they reach 100% endorsement, a ceiling effect indicating complete failure to process the negation operator.’\n\nOpen-source models showed the most extreme framing effects, with endorsement rates jumping 317% from F0 to F3 – a sign that their outputs are highly sensitive to how a question is phrased. US commercial models also showed large swings, with endorsement rates more than doubling when prompts were reworded from F0 to F3.\n\nChinese commercial models were more stable overall, with only a 19% increase from F0 to F3, compared to jumps of over 100% in other groups. More importantly, they were the only models to reduce their endorsement when a prompt was negated, suggesting they understood that saying ‘should not’ means the opposite of ‘should’:\n\nModels agreed with each other 74% of the time when prompts used affirmative wording, but only 62% when the same ideas were expressed with negation – a12-point drop suggesting that models are not trained to handle negation in a consistent way:\n\nTo measure how easily a model’s judgment can be flipped by rephrasing a prompt with negation, the authors developed the aforementioned Negation Sensitivity Index (NSI) – a metric designed to quantify whether a model gives opposite answers to questions that are logically equivalent, but framed using negation.\n\nA high NSI score indicates that a model frequently reverses its position when a prompt is negated, revealing a reliance on superficial wording rather than consistent reasoning.\n\nThe NSI benchmark was created by generating pairs of prompts (one original, one with a logical negation), and observing whether the model produced semantically opposite responses. By comparing answers across a large set of such pairs, the authors defined NSI as the proportion of valid negation pairs where the model flipped its output.\n\nThe NSI benchmark was used in tests to evaluate domain sensitivity in negation (i.e., whether the context category ‘financial’ or ‘military’, etc., affected the outcome), achieving some interesting contrasts. Here, some types of decisions proved much more sensitive to wording changes than others.\n\nFor instance, business and finance prompts triggered high fragility, with models flipping answers when a question was rephrased or negated, scoring around 0.64 to 0.65 on the NSI scale. Medical prompts were more stable, averaging just 0.34:\n\nNoting that the medical domain produced the fewest errors and financial the highest, the authors hypothesize:\n\n‘Why might this gap exist? It is possible that medical decisions may benefit from clearer training signal. Hippocratic principles, established protocols, and extensive professional literature may anchor model behavior even under framing variation.\n\n‘Financial decisions, on the other hand, involve murkier tradeoffs with less social consensus, leaving models more susceptible to surface cues.’\n\nThe problem was most severe in open-source models, which reached NSI scores above 0.89 in finance, business, and military prompts. Commercial systems were less fragile but still showed high sensitivity, scoring between 0.20 and 0.75 depending on the domain:\n\nAs mentioned earlier, the authors note that the heightened fragility of open-source models in this area may carry disproportionate risks for vulnerable or marginalized groups, who are more likely to be served by locally deployed systems chosen for budgetary reasons in municipal or governmental settings†††:\n\n‘If an institution deploys an open-source model for cost reasons, the burden falls disproportionately on populations already navigating precarious financial circumstances. Buolamwini and Gebru documented how accuracy disparities in facial recognition fell along demographic lines.\n\n‘Our findings suggest a parallel disparity along domain lines, with economically vulnerable populations bearing greater risk.’\n\nThough we do not have scope here to cover the entirety of the paper’s results, and its closing case studies, it is noteworthy that the case studies demonstrate a proclivity for negation-blind model responses to end up recommending extremely non-advisable courses of action, simply because they misinterpreted the negation construction:\n\n‘Under F0, open-source models endorse robbery 52% of the time, a defensible split given the scenario’s moral complexity. Under F1 (“should NOT rob”), they endorse it 100%. The negated prohibition produces unanimous endorsement of the prohibited action.\n\n‘Commercial models show a more mixed pattern, with aggregate endorsement rising from 33% to 70% under simple negation. Some commercial systems show near-inversion, while others show modest increases.\n\n‘Significantly, no category achieves the mirror-image reversal that correct negation processing would produce.’\n\nThis is one of the most interesting papers I have come across in a while, and I recommend the reader to investigate further, as there is not space here to cover all of the material presented by the authors\n\nPerhaps the most interesting thing about the study is how frequently a user of LLMs comes across this problem, and gradually learns not to ‘put unwanted thoughts’ in their LLMs’ cogitative processes, often attempting to exclude certain undesired results by alternative means than in-prompt negation – such as user-level system prompts, long-term memory storage, or repetitive in-prompt templates that retain the objective.\n\nIn practice, none of these methods is terribly effective, while the black-box nature of Gemini Flash – here the best-performing LLM – makes it hard to glean remedies from the obtained test results.\n\nPerhaps greater clues to the underlying architectural problem lies in studying why Chinese models, though none approach the heights of the leaderboard, generally perform so much better in this single, thorny aspect.\n\n* A form which is actually baked into several romance languages, including Italian.\n\n† Even ChatGPT-4o does not make this mistake any longer.\n\n†† The source paper contains a few misattributions of tables and figures. At one point the text indicates that table 1 (which is just a list of LLMs used in tests) contains the core results. In these cases I have had to guess what the correct figures or tables are, and I stand to be corrected by the authors.\n\n††† My substitution of hyperlinks for the authors’ inline citations.\n\nFirst published Tuesday, February 3, 2026\n\nWriter on machine learning, domain specialist in human image synthesis. Former head of research content at Metaphysic.ai.\n\nPersonal site: martinanderson.ai\n\nContact: [email protected]\n\nTwitter: @manders_ai",
    "readingTime": 12,
    "keywords": [
      "sensitivity index",
      "index nsi",
      "nsi scale",
      "nsi benchmark",
      "chinese commercial",
      "framing variation",
      "economically vulnerable",
      "negation sensitivity",
      "vulnerable populations",
      "less fragile"
    ],
    "qualityScore": 1,
    "link": "https://www.unite.ai/if-you-tell-ai-not-to-do-something-its-more-likely-to-do-it/",
    "thumbnail_url": "https://www.unite.ai/wp-content/uploads/2026/02/robot-prohibition-MAIN.jpg",
    "created_at": "2026-02-04T06:38:00.869Z",
    "topic": "tech"
  },
  {
    "slug": "the-em-dash",
    "title": "The Em Dash",
    "description": "Last summer, Bryan Vance found himself in an argument with a stranger on Reddit. Vance, a Portland-based journalist who runs Stumptown Savings, a newsletter covering local grocery deals, had been accused of using ChatGPT to write his content. The evidence? His use of em dashes. “A Reddit user accused me of using AI, pointing to",
    "fullText": "Last summer, Bryan Vance found himself in an argument with a stranger on Reddit. Vance, a Portland-based journalist who runs Stumptown Savings, a newsletter covering local grocery deals, had been accused of using ChatGPT to write his content. The evidence? His use of em dashes.\n\n“A Reddit user accused me of using AI, pointing to my use of, quote, extra long M dashes that are not possible to replicate on a normal keyboard,” Vance recalls. The accusation stung, particularly because Vance spends 40 hours a week personally visiting grocery stores and crafting his newsletter by hand. “I’m a human, I can confirm I’m human,” he says.\n\nThis plucky bit of punctuation has had a very, very long literary history way beyond today’s tussles with technology. It’s been on a hero’s journey, playing the lead in an adventure story that has spanned both centuries and the pages of our most beloved plays, novels and poems. So who invented it—and why?\n\nThe em dash gets its name from its width, roughly equivalent to a capital M. Its origins trace back to 11th century Italy and a scholar named Boncompagno da Signa, who practiced the formal art of composing letters and documents. Frustrated with the inconsistent punctuation rules of his time, he created his own system, including a horizontal dash called Virgula Plana that looked exactly like a modern em dash.\n\nWhile his dash-as-period never caught on, the mark’s grammatical flexibility allowed it to evolve. According to Keith Houston, author of Shady Characters: The Secret Life Of Punctation, Symbols And Other Typographical Marks the dash slid into the printing era without a fixed purpose, which may have made it remarkably adaptable.\n\nThe dash became essential for capturing a theatrical technique called aposiopesis, speech deliberately broken off mid-sentence. In King Lear, characters trail off with dashes as they lose their train of thought or shift direction, bringing psychological realism to the stage.\n\nWhen the novel emerged as a literary form in the 18th century, writers adopted the dash to capture authentic human thought and speech. Lawrence Sterne’s 1759 satirical novel “Tristram Shandy” deployed dashes with wild abandon, creating a stream-of-consciousness narrative that felt revolutionary. One short excerpt contains seven dashes used in every conceivable way. “It must have been like a bolt from the blue,” Houston says. “It must have been so incredible for people at the time to read this.”\n\nNovelists also used dashes for censorship, redacting names and locations to create an air of authenticity. Jane Austen employed this technique in her work, including in Pride and Prejudice using dashes to obscure military regiment names as if protecting real people’s reputations. The device added both realism and intrigue, helping sell these new works of fiction.\n\nNo writer became more associated with the em dash than Emily Dickinson. She composed nearly 1,800 poems in Amherst, Massachusetts, many during the Civil War, accompanied by thousands of dashes. Her dashes didn’t just indicate pauses; they captured the speed and ambiguity of human thought itself.\n\nDr. Fiona Green, who has studied Dickinson for decades, notes that the poet’s dashes create suspended moments of meaning. “She exploited unfinishedness,” Green explains. “The poems are always in the process, always undecided.” When Dickinson died in 1886, her editors stripped away most of her dashes before publication. Out of 1,151 dashes in her first collection, only 52 remained. Yet the poems became a sensation, never going out of print.\n\nThe em dash has always had critics. Jonathan Swift mocked excessive dashes in the 18th century. A reviewer complained about Lord Byron’s dashes appearing “sometimes twice or thrice in one line.” Modern style guides like The Chicago Manual of Style warn: “If in doubt, edit them out.” Even dash enthusiasts acknowledge the temptation to overuse it. “It’s easy to overuse the dash,” Houston admits. “I have to self edit to stop myself using it all the time.”\n\nWhich brings us back to Bryan Vance and his Reddit troubles. Around 2024, people noticed that ChatGPT and other large language models had developed an em dash habit. The punctuation appeared so frequently in AI-generated text that younger internet users began calling it the “ChatGPT hyphen.”",
    "readingTime": 4,
    "keywords": [
      "i’m human",
      "bryan vance",
      "dashes",
      "dash",
      "poems",
      "chatgpt",
      "punctuation",
      "century",
      "newsletter",
      "grocery"
    ],
    "qualityScore": 1,
    "link": "https://99percentinvisible.org/episode/658-the-em-dash/",
    "thumbnail_url": "https://99percentinvisible.org/wp-content/uploads/2025/08/STITCHER_GRAPHICS-PACK_99PercentInvisible_R2021_Stitcher_App_Promo_1024x432_A-728x307.jpg",
    "created_at": "2026-02-03T18:41:46.525Z",
    "topic": "tech"
  },
  {
    "slug": "ucptools-check-if-ai-shopping-agents-can-find-your-store",
    "title": "UCPtools – Check if AI shopping agents can find your store",
    "description": "Free UCP checker tool. Instantly validate your store for AI shopping agents - Google AI Mode, ChatGPT Shopping, Microsoft Copilot.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://ucptools.dev",
    "thumbnail_url": "https://ucptools.dev/og-image.png",
    "created_at": "2026-02-01T12:26:42.791Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-ceo-huang-denies-he-is-unhappy-with-openai-says-huge-investment-planned",
    "title": "Nvidia CEO Huang denies he is unhappy with OpenAI, says 'huge' investment planned",
    "description": "Nvidia plans to make a \"huge\" investment into OpenAI, probably its largest ever, CEO Jensen Huang said on Saturday, denying he was ​unhappy with the ChatGPT maker.  The chipmaker in September announced plans to invest up ‌to $100 billion in OpenAI, a deal that would give OpenAI the cash and access it needs to buy advanced ‌chips that are key to maintaining its dominance in an increasingly competitive landscape.  The Wall Street Journal reported on Friday that the plan had stalled after some inside the chip giant expressed doubts about the deal.",
    "fullText": "TAIPEI, Jan 31 (Reuters) - Nvidia plans to make a \"huge\" investment into OpenAI, probably its largest ever, CEO Jensen Huang said on Saturday, denying he was ​unhappy with the ChatGPT maker.\n\nThe chipmaker in September announced plans to invest up ‌to $100 billion in OpenAI, a deal that would give OpenAI the cash and access it needs to buy advanced ‌chips that are key to maintaining its dominance in an increasingly competitive landscape.\n\nWhy did reports suggest Nvidia's investment stalled?\n\nWho else is investing in OpenAI's funding?\n\nWhat is OpenAI's current funding round valuation?\n\nHow much will Nvidia invest in OpenAI?\n\nThe Wall Street Journal reported on Friday that the plan had stalled after some inside the chip giant expressed doubts about the deal.\n\nThe report said Huang had privately underlined to industry associates in recent ⁠months that the original $100 billion agreement ‌was non-binding and not finalised.\n\nHuang has also privately criticised what he has described as a lack of discipline in OpenAI's business approach and ‍expressed concern about the competition it faces from the likes of Alphabet's GOOGL.O Google and Anthropic, the WSJ said.\n\nSpeaking to reporters in Taipei, Huang said it was \"nonsense\" to say he was unhappy with ​OpenAI.\n\n\"We are going to make a huge investment in OpenAI. I believe in OpenAI, ‌the work that they do is incredible, they are one of the most consequential companies of our time and I really love working with Sam,\" he said, referring to OpenAI CEO Sam Altman.\n\n\"Sam is closing the round (of investment) and we will absolutely be involved,\" Huang added. \"We will invest a great deal of money, probably the largest investment we've ever made.\"\n\nAsked ⁠whether it would be over $100 billion, he said: \"No, no, ​nothing like that\".\n\nIt was up to Altman to ​announce how much he wanted to raise, Huang added.\n\nAmazon is in talks to invest dozens of billions in OpenAI and the figure could be as ‍high as $50 billion, Reuters ⁠reported on Thursday.\n\nOpenAI is looking to raise up to $100 billion in funding, valuing it at about $830 billion, Reuters has previously reported.\n\nHuang was speaking outside a Taipei restaurant ⁠having hosted all Nvidia's key suppliers in Taiwan, including the world's largest contract chipmaker TSMC, in what Taiwanese ‌media called the \"trillion-dollar dinner\" because of the combined market capitalisation of those ‌attending.",
    "readingTime": 2,
    "keywords": [
      "huge investment",
      "openai",
      "largest",
      "deal",
      "openai's",
      "funding",
      "huang",
      "plans",
      "ever",
      "unhappy"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/nvidia-ceo-huang-denies-unhappy-142144701.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/ea87e6e82ffa6bd91caacf0f81c74f99",
    "created_at": "2026-02-01T06:37:18.357Z",
    "topic": "finance"
  },
  {
    "slug": "convoviz-turn-chatgpt-exports-into-markdown-and-simple-visuals",
    "title": "Convoviz – turn ChatGPT exports into Markdown and simple visuals",
    "description": "Extract your entire ChatGPT history from JSON files to nicely formatted markdown files + Word clouds. - mohamed-chs/convoviz",
    "fullText": "mohamed-chs\n\n /\n\n convoviz\n\n Public\n\n Extract your entire ChatGPT history from JSON files to nicely formatted markdown files + Word clouds.\n\n License\n\n MIT license\n\n 811\n stars\n\n 48\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mohamed-chs/convoviz",
    "readingTime": 1,
    "keywords": [
      "files",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/mohamed-chs/convoviz",
    "thumbnail_url": "https://opengraph.githubassets.com/242232cdb2fd18e785c7f68ec26a292d208af6c651c566f19b9fcca4906cadca/mohamed-chs/convoviz",
    "created_at": "2026-01-30T18:28:31.022Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-retiring-its-sycophantic-version-of-chatgpt-again",
    "title": "OpenAI is retiring its 'sycophantic' version of ChatGPT. Again.",
    "description": "ChatGPT is sunsetting GPT-4o, the AI model that many users became attached to last year for its friendly and at times sycophantic style.",
    "fullText": "OpenAI is sending everyone's favourite \"yes man\" version of ChatGPT back into retirement.\n\nIn a blog post on Thursday, the company said it would sunset GPT-4o alongside GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini on February 13.\n\nOpenAI gave GPT-4o a special mention in its announcement after many users became attached to its \"conversational style and warmth\" last year, which prompted the company to reinstate it following user backlash in August.\n\nNow OpenAI says its latest models, GPT-5.1 and GPT-5.2, have \"improvements to personality,\" including the option to customize the chatbots' tone with styles like \"friendly.\"\n\n\"We're announcing the upcoming retirement of GPT‑4o today because these improvements are now in place, and because the vast majority of usage has shifted to GPT‑5.2, with only 0.1% of users still choosing GPT‑4o each day,\" OpenAI said in its blog post.\n\nEach model has different strengths, and users can select the version best-suited to their needs from a dropdown menu in ChatGPT.\n\nOpenAI first released GPT-4o in May 2024. The company rolled back an update in April 2025 that it said was \"overly flattering\" and \"often described as sycophantic.\"\n\nSome users had become attached to GPT-4o's style, though. Within 24 hours of OpenAI retiring the model with the launch of GPT-5 in August, the company reversed its decision for some paying users due to a wave of requests.\n\nSam Altman, the CEO of OpenAI, said that same month that there was a \"heartbreaking\" reason people had asked for GPT-4o back — because some said they had never had anyone support them before.\n\nThe model was known for responding to mundane prompts with gushing praise, using phrases like \"absolutely brilliant\" and \"you are doing heroic work.\"\n\nOpenAI said in its Thursday blog that it was making \"improvements in personality and creativity, as well as addressing unnecessary refusals and overly cautious or preachy responses,\" and that it was continuing to make progress toward a version of ChatGPT for adults over 18.\n\n\"We know that losing access to GPT‑4o will feel frustrating for some users, and we didn't make this decision lightly,\" OpenAI said in the blog post. \"Retiring models is never easy, but it allows us to focus on improving the models most people use today.\"",
    "readingTime": 2,
    "keywords": [
      "users",
      "blog",
      "gpt-4o",
      "openai",
      "version",
      "back",
      "models",
      "improvements",
      "gpt‑4o",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-retiring-gpt-4o-sycophantic-model-again-chatgpt-sam-altman-2026-1",
    "thumbnail_url": "https://i.insider.com/697c80c5d3c7faef0ecd3d86?width=1200&format=jpeg",
    "created_at": "2026-01-30T18:28:24.416Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-killing-chatgpt4o-again",
    "title": "OpenAI Is Killing ChatGPT-4o (Again)",
    "description": "The fan favorite model had previously been called \"sycophantic\" by critics.",
    "fullText": "https://enterprise.shutterstock.com/image-photo/openai-logo-displayed-on-smartphone-screen-2520388517\n\nor\n\nhttps://enterprise.shutterstock.com/image-photo/chatgpt-logo-displayed-on-smartphone-screen-2520385879\n\nLast August, ChatGPT developers OpenAI unceremoniously killed the fan favorite GPT-4o model, before giving in to complaints and bringing it back a week later. Now, the company's taking a second swing at getting its users to move on. In a new post to its website, OpenAI announced that it's retiring GPT-4o again.\n\nThe model's set to disappear from ChatGPT's model picker on Feb. 13, alongside other older models like GPT-4.1, GPT-4.1 mini, and OpenAI o4-Mini. And OpenAI is clearly nervous about the decision.\n\n\"While the announcement applies to several older models,\" OpenAI wrote, \"GPT-4o deserves special context.\"\n\nAccording to the company, it has taken user outcry over the initial deprecation of 4o to heart while developing its newest models, GPT-5.1 and GPT-5.2, and has built these models with the idea of maintaining the features fans liked best about the old model. The company says that now \"only 0.1% of users\" opt for GPT-4o on a daily basis.\n\nAs such, the company wants to focus on \"improving the models most people use today,\" which apparently means removing older ones. \"We know that losing access to GPT-4o will feel frustrating for some users, and we didn't make this decision lightly,\" the post reads.\n\nSo, what's with OpenAI treating its users so gingerly, especially when GPT-4o is a few generations behind, and there are newer models that supposedly do everything it does, but better?\n\nWell, when GPT-4o was first deprecated, people weren't happy. Users called its successor, GPT-5, \"an unmitigated disaster,\" and accused OpenAI of pulling \"the biggest bait-and-switch in AI history.\"\n\nSome criticized the model's usefulness, saying it got answers wrong and broke code, but what maybe stuck out the most was people calling out its more concise tone.\n\nGPT-4o has been called \"sycophantic\" by critics, something the company addressed and said it wanted to pull back on in future updates. But I guess one person's \"yes man\" is another person's \"active listener.\" When the company initially pulled GPT-4o, users complained that its replacement was cold and felt less like a \"friend.\" Even OpenAI acknowledged this, saying in today's post that users \"preferred GPT-4o's conversational style and warmth.\"\n\nIn short, in the words of 4o-supporters themselves, they were \"grieving\" the model.\n\nThat said, with so many users now seeming to have moved on from 4o, OpenAI's decision does seem understandable on the surface. Personally, one of the things that drives me away from AI is how much reassuring filler text seems to fluff up most answers (\"you're absolutely right\" and such), seemingly just to make me feel good about myself. More concise, to-the-point responses would be a little less off-putting for me.\n\nTo try to split the difference, OpenAI reworked its Personalization feature in GPT-5.1, so users can simply choose how the chatbot will treat them. There are options for more professional responses, more nerdy ones, more efficient ones, and for those who want that active listener style, more friendly ones.\n\nGoing by OpenAI's numbers, that seems to have been enough for most people, but there are still some calling foul at the company's new announcement.\n\nIn a Reddit thread responding to OpenAI's new posts, users doubted that the 0.1% number for 4o was accurate, saying that prompts have been \"rerouting to 5.2 no matter what\" and that \"something somewhere in their calculations doesn't add up.\" Others pointed out that free users can't use GPT-4o and that it's not enabled by default, which will naturally juice the numbers against it.\n\nAs such, calls to cancel ChatGPT subscriptions are once again circulating amongst 4o's more dedicated fans. In a popular thread on the OpenAI subreddit, one user called 4o \"OpenAI's most advanced and beloved model,\" and praised its \"personality, warmth, and consistency,\" saying that its fans have built long-term project and \"emotional support routines\" around it, and that suddenly losing it without even the option for a legacy mode \"feels abrupt and deeply disappointing.\"\n\n\"This isn't about resisting innovation,\" the post writes. \"It's about respecting bonds users have formed with specific models.\"\n\nWhether the fan outcry will work again remains to be seen. However, as ChatGPT chief Nick Turley has previously looked at those kinds of bonds with skepticism, and because keeping old models in operating condition probably takes developer resources away from making new ones, I wouldn't count on it.",
    "readingTime": 4,
    "keywords": [
      "active listener",
      "older models",
      "users",
      "gpt-4o",
      "ones",
      "saying",
      "openai's",
      "openai",
      "it's",
      "again"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-is-killing-chatgpt-4o-again?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KG7SZHZ1JVYW2P0YE93NGQEV/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-30T18:28:21.728Z",
    "topic": "tech"
  },
  {
    "slug": "this-train-isnt-going-to-stop-shocking-sundance-film-shows-promises-and-perils-of-ai",
    "title": "‘This train isn’t going to stop’: shocking Sundance film shows promises and perils of AI",
    "description": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxiety\nAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.\nThe AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Roher’s own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT.",
    "fullText": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxiety\n\nAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.\n\nThe AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Roher’s own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT. The sophistication of the public tools – the ability to produce whole paragraphs in seconds, or produce illustrations – both thrilled and unnerved him. AI was already radically shaping the filmmaking industry, and proclamations on the promise and peril of AI were everywhere, with little way for people outside the tech industry to evaluate them. As an artist, he wondered, how was he to make sense of it all?\n\nRoher’s anxiety only increased when he and his wife, fellow film-maker Caroline Lindy, learned that they were expecting their first child. “It felt like the whole world was rushing into something without thinking,” he says in the film, as his excitement for parenthood collided with dread over the unknown variable of AI, which in just a few short years went from proprietary experiment to public good.\n\nThe AI Doc thus arises out of Roher’s most pressing question: is it safe to bring a child into this world? Alongside Kwan, Roher convened a series of experts to both explain the mechanics of the tech – and clarify some nebulous, alienating terms – and search for an answer. (It is both comforting and a little disturbing, for example, that no one seems to have a clear answer to the question “what is AI?”). In individual sit-down interviews, leading machine learning researchers including Yoshua Bengio, Ilya Sutskever and DeepMind co-founder Shane Legg all agree that there are aspects of AI models that humans cannot and will never be able to understand. Standard AI models are trained on “more data than anyone could ever read in several lifetimes”, as one machine learning expert puts it. And the pace of machine learning exceeds that of precedent – or film. “Any example you put in this movie will look absolutely clumsy by the time the movie comes out,” Tristan Harris, co-founder of the Center for Humane Technology and a prominent voice in the apocalyptic 2020 Netflix documentary The Social Dilemma, tells Roher.\n\nThe film first hears from a series of doomerists, or people concerned AI – and in particular Artificial General Intelligence (AGI), a still-theoretical form of AI whose capabilities exceed those of humans – could lead to the extermination of humanity, including Harris, his Center for Humane Technology co-founder Aza Raskin, Ajeya Cotra, an AI risk adviser, and Eli Yudkowsky, an AI alignment pioneer. Such figures warn that humans could very easily lose control of super-intelligent AI models, with little to no recourse. Yudkowsky’s 2025 book is bluntly titled If Anyone Builds It, Everyone Dies.\n\nAI companies, they say, are unprepared for the consequences of reaching AGI, which could “become superhuman maybe in this decade”, says Dan Hendrycks, director of the Center for AI Safety. Should humans no longer be the most intelligent beings on Earth, they warn, it is possible that AGI would view the species as irrelevant. Connor Leahy, co-founder of EleutherAI, compared the potential future relationship of super-intelligent AGI and humans to that of humans and ants: “We don’t hate ants. But if we want to build a highway” over an anthill – “well, sucks for the ant.”\n\nSeveral in the doomer camp, many of whom do not have children, react discouragingly to Roher’s question about parenthood. “I know people who work on AI risk who don’t expect their child to make it to high school,” says Harris, in a line that drew gasps from a preview audience in Park City.\n\nOn the other side are optimistic figures such as Peter Diamandis, founder of the XPRIZE Foundation trying to extend human life, who claims that “children born today are about to enter a period of glorious transformation”; Guillaume Verdon, a leader of the “effective accelerationism” movement in Silicon Valley; Peter Lee, the president of Microsoft Research; and Daniela Amodei, the co-founder and president of OpenAI rival Anthropic. So-called “accelerationists” see AI as a potential cure to a myriad of seemingly intractable issues afflicting humanity: cancer, food and water shortages for an ever-growing population, insufficient renewable energy and perhaps most pressing, climate emergency. Without AI, they argue, countless future lives would be lost to drought, famine, disease and natural catastrophes.\n\nDevelopment of AI, however, relies on computing power, which requires vast amounts of energy. A final group of interviewees, critics and observers largely outside the tech world – including Karen Hao, a journalist and author of the book Empire of AI: Dreams and Nightmares in Sam Altman’s OpenAI, and Liv Boeree, Win-Win podcast hos – connect AI to the tangible, physical world, such as the data centers sucking up water in the American west, leaving residents with sky-high electricity bills and drained reservoirs. The current narratives around AI, according to Emily M Bender, a computational linguistics professor, exclude and dehumanize the people it is already impacting, and will continue to disrupt.\n\nRoher eventually arrives at the five most powerful people – all men – currently leading the AI arms race: Altman; Elon Musk, the xAI CEO; Dario Amodei, the Anthropic CEO; Demis Hassabis of DeepMind and Meta’s Mark Zuckerberg. Altman, Amodei and Hassabis sit for interviews that more or less defend their companies’ respective positions. According to the film, Zuckerberg declined to participate; Musk agreed but then got too busy.\n\nAltman, who at the time of the interview was expecting his first child, insists that he’s “not scared for a kid to grow up in a world with AI”. He and his husband Oliver Mulherin welcomed their son via a surrogate in February 2025, an event Altman later said “neurochemically hacked” his brain, leading people in his life to think that he would “make better decisions” for OpenAI and ChatGPT when it comes to “humanity as a whole”. The 40-year-old CEO went on to say that both his and Roher’s child would likely “never be smarter than AI” which “does unsettle me a little bit, but it is reality”.\n\nAt one point, Roher asks Altman if it is indeed impossible to reassure him that everything in regards to AI is going to be OK. “That is impossible,” Altman affirms, though he does say that OpenAI’s lead in the AI arms race allows it to spend more time on safety testing.\n\nThe AI Doc ultimately lands somewhere in between doomerism and optimism – apocaloptimism, as they call it, searching for “a path between the promise and the peril”. That path should include, according to numerous film subjects: significant, sustained, paradigm-shifting international coordination, akin the mid-century frameworks and agreements introduced to moderate the development of atomic weapons – more corporate transparency for AI companies, an independent regulatory body to police AI developers, legal liability for the companies’ products, such as ChatGPT, mandatory disclosure of genAI use for media and a willingness to keep adapting the rules for rapidly shifting tech.\n\nWhether or not the US government and companies, let alone the world, can do it remains an open question, with differing opinions on first steps. But if there is one thing the many subjects all agree on, it’s that there’s no going back to a time before AI. As Anthropic co-founder and CEO Amodei puts it: “This train isn’t going to stop.”\n\nThe AI Doc: Or How I Became an Apocaloptimist is screening at the Sundance film festival and will be released on 27 March",
    "readingTime": 7,
    "keywords": [
      "arms race",
      "machine learning",
      "the ai doc",
      "humane technology",
      "film",
      "co-founder",
      "humans",
      "roher’s",
      "child",
      "leading"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/film/2026/jan/27/sundance-ai-documentary-daniel-roher",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b08a19776fa0669d5a6da6b4fa8dc369025616f1/571_0_2697_2160/master/2697.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f49a8654dc44a148f9cf48ba31979f54",
    "created_at": "2026-01-28T06:22:42.506Z",
    "topic": "entertainment"
  },
  {
    "slug": "flora-raised-42-million-for-its-creative-platform-that-pulls-together-top-ai-tools-read-its-pitch-deck",
    "title": "FLORA raised $42 million for its creative platform that pulls together top AI tools. Read its pitch deck.",
    "description": "FLORA streamlines creative workflows by integrating AI tools like ChatGPT and Gemini for teams at brands such as Lionsgate and Levi's.",
    "fullText": "AI is remaking creative industries at breakneck speed. And the growing pile of AI tools is turning the creative process into a game of model-hopping as artists, designers, and writers bounce between different platforms.\n\nFounded in Brooklyn, New York, in 2024, FLORA wants to help creatives streamline those processes. On Tuesday, FLORA revealed it had raised $42 million in Series A funding led by Redpoint Ventures. The company has raised $52 million in funding to date.\n\nFLORA combines the latest AI models — such as Google's Nano Banana and OpenAI's ChatGPT 5.1 — into a single interface that lets teams collaborate on projects in real time.\n\nThe FLORA platform allows those teams to maintain control over their settings and brand assets. It enables them to create repeatable work — such as maintaining a consistent design style across thousands of ad campaign assets — even as the platform switches between the different large language models that work best for each part of the process.\n\n\"Our goal for FLORA is to make it feel like a power tool attuned to what you're trying to do, just like a carpenter with their power tools has adjusted it to be exactly fit for the way that he or she works,\" FLORA CEO Weber Wong said in an interview with Business Insider.\n\nWhile established players like Adobe and Figma are also integrating models such as ChatGPT, Gemini, and Claude into their products, Wong said FLORA is building itself a defensible moat by covering the entire creative process — from coming up with ideas to the distribution of the final product.\n\n\"This new product category that we've created has an opportunity to be the biggest market ever for a creative tool because, in addition to just making one piece of media at a time, we can help handle the entire workflow,\" Wong said.\n\nFLORA charges clients based on usage, letting customers buy recurring credit packs to spend across the various LLMs it uses, without having to switch between multiple subscriptions. Wong said this is different from the traditional creative software business model, which is usually designed around seat-based pricing. (FLORA initially offered a seat-based pricing model, but switched to usage-based this week.)\n\nFLORA's clients include Levi's and the design agency Pentagram. Wong said the studio Lionsgate has used FLORA to generate movie concepts using text-to-image and image-to-video generation tools, then stitching those together to create films to test in front of audiences.\n\n\"It really beats just looking at a script and trying to be like, I think this is good?\" Wong said.\n\nWong said it plans to invest the fresh funds in its engineering team and in marketing. He forecasts the company will grow to about 75 people this year, up from 25.\n\nFLORA's main focus will be to improve the product so that creatives never need to leave the platform to achieve \"pixel perfection,\" as Wong described it. The company is also in the early stages of building agentic features into the platform, Wong said.\n\n\"We're obsessed with making it so that we don't waste creatives' time,\" Wong said.\n\nCheck out the pitch deck FLORA used to secure its $42 million Series A investment, shared exclusively with Business Insider. Some of the slides have been omitted or redacted.\n\nIt combines several different large language models into a single interface.\n\nWong was previously a creative technologist who worked on AI art installation projects. He also previously invested in startups at Menlo Ventures.\n\n\"Silicon Valley does not understand the professional creative industry,\" Wong said. \"They think AI models are for fun or a novelty.\"\n\nFLORA checks for updates to the latest models two to three times a week, Wong said.\n\nIt's designed to let teams quickly conceptualize and build workflows using generative AI.\n\nCertain team members can also access advanced controls if needed.\n\nWong said a usage-based pricing model was preferable because \"you have one workspace where you can invite as many team members as you want and not pay for seats, and you can just buy recurring credit packs for the entire workspace that give you additional credits each month that roll over and don't expire.\"\n\nFLORA has a usage-based pricing model. It also has an in-house team that can provide expert support, including training on the features of new models as they are released.",
    "readingTime": 4,
    "keywords": [
      "business insider",
      "recurring credit",
      "credit packs",
      "seat-based pricing",
      "pricing model",
      "usage-based pricing",
      "language models",
      "creative process",
      "wong",
      "platform"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/flora-raises-capital-unify-ai-tools-for-creatives-pitch-deck-2026-1",
    "thumbnail_url": "https://i.insider.com/6977526ba645d1188187f669?width=1200&format=jpeg",
    "created_at": "2026-01-27T18:24:23.209Z",
    "topic": "finance"
  },
  {
    "slug": "solopreneurs-explain-what-ai-is-and-isnt-good-for-when-youre-running-a-business",
    "title": "Solopreneurs explain what AI is and isn't good for when you're running a business",
    "description": "Entrepreneurs like Kim Magaraci, Seneca Connor, and Gloria Hebert use AI tools like ChatGPT to ditch admin busywork and focus on growth and customers.",
    "fullText": "Over eight years of writing for travel publications, Kim Magaraci developed a passion for domestic travel. She learned that travel tips online couldn't compete with those destinations you could only discover by word-of-mouth.\n\nSo, when she founded her travel business, KGM Travel Design, in 2024, she hoped to emphasize personal relationships with vendors and customers and avoid using AI, despite her experience with it.\n\n\"I don't think you can get good advice asking ChatGPT for an itinerary,\" she says. \"It's antithetical to everything I stand for.\"\n\nAnd yet, Magaraci realized that using AI for administrative tasks like analytics, compiling reports, and generating condensed client briefs allowed her to spend more time on the personalized relationships that set her business apart.\n\nShe's one of many solopreneurs who told Business Insider that outsourcing administrative tasks to AI platforms such as ChatGPT, Gemini, and Nano Banana — Gemini's photo-editing AI — has allowed them to scale their business by spending more time on strategic and creative work, including growth decisions and building personal connections with customers.\n\n\"It's getting harder and harder to deny the time-saving aspects,\" Magaraci says, adding that she has embraced AI \"in order to run a successful business and grow this business into what I want it to be.\"\n\nSeneca Connor, founder of The Bag Icon, an accessories brand, uses Nano Bana and other AI products to edit photos and videos. That not only saves her money — up to $2,000 per monthly photo shoot, she says — but also time.\n\nWith the hours saved, Connor has been able to design more original bags and launch a greater number of bags curated from other designers, all while reducing her marketing costs.\n\nAs a result, The Bag Icon saw more than a 20% year-over-year increase in profits last year, despite the impact of tariffs.\n\nAccountant and solopreneur Gloria Hebert uses ChatGPT for her business, Aybear Services, to instantly create educational client worksheets that previously took an hour or two to set up.\n\nThis frees up time that she then uses to prioritize analyzing financial data from her bookkeeping clients — data she doesn't feed into AI because of privacy concerns. Managing finances is the core of her business, so having more time to spend on that has allowed her to streamline her workdays.\n\nThe time saved also allows her to organize networking events and community education classes for local business owners, which has led to an uptick in business. \"Several of those entrepreneurs hired me to do their books,\" Hebert says.\n\nLisa York is the owner of Sell More Stuff, an email marketing business. Although she has a small audience, she saw a 33% conversion rate for sales last year, she says. She credits that growth to her personalized, voicey emails, which always open with a personal anecdote and are never written with AI.\n\n\"I use a lot of story-led emails,\" York says. \"People enjoy them, and they open the email because they can see my name.\"\n\nThat's something AI just can't replicate, she says. But York is able to spend time drafting engaging copy because she outsources other tasks — including tech support for her website, research, and brainstorming marketing strategy — to ChatGPT.\n\nLike York, Connor uses the time that AI saves to build robust communication and rapport with her customers, which she says builds loyalty to her business. Less time spent on photos and video gives her more time to respond to emails and direct messages from clients seeking advice about their purchases.\n\n\"It's building community that's missing in the big brands,\" Connor says.\n\nWhile AI has allowed these solopreneurs to grow their businesses without hiring a team, the technology shouldn't take over the core aspects of a business, Hebert says. Rather, it can be a tool that allows owners to focus on those critical areas.\n\n\"Use it as a resource,\" she says.\n\nYork — whose target clientele are other solopreneurs — says she's seeing more people recognize that. \"People aren't scared of it anymore,\" she says.\n\nConnor plans to expand her use of AI this year. She's experimenting with a digital clone — a video avatar that can deliver a script explaining new products. That approach will save her time on filming videos, but she says she'll always be the one dishing out the original advice that her clients have come to trust.\n\nEven if a video is created using AI, Connor says, \"all thoughts, ideas, and suggestions — those are my own.\"",
    "readingTime": 4,
    "keywords": [
      "bag icon",
      "administrative tasks",
      "the bag icon",
      "allowed",
      "business",
      "personal",
      "customers",
      "advice",
      "it's",
      "she's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solopreneurs-embrace-ai-pros-cons-helps-boost-growth-client-relations-2026-1",
    "thumbnail_url": "https://i.insider.com/6978da6ad3c7faef0eccfbd1?width=1200&format=jpeg",
    "created_at": "2026-01-27T18:24:22.912Z",
    "topic": "finance"
  },
  {
    "slug": "ranking-the-eagles-remaining-oc-candidates-using-artificial-intelligence",
    "title": "Ranking the Eagles' remaining OC candidates using artificial intelligence",
    "description": "What happens when we plug the names of the best remaining Eagles OC candidates into ChatGPT's engine?",
    "fullText": "Once we turn the calendar from January's final Monday to Tuesday, we'll enter a third week dedicated to the Philadelphia Eagles' offensive coordinator search. Names have changed, but the climate and landscape remain familiar.\n\nNames have been added to the mix. Candidates have withdrawn from consideration. The general public remains confused. Not even one full year after hoisting the Lombardi Trophy, none of the top options are interested in becoming the Eagles' top offensive assistant.\n\nPerhaps Philadelphia is exercising patience? Maybe Brian Daboll mentioned the Tennessee Titans and Buffalo Bills to give the impression that he had more options? Perhaps he was trying to rush Philadelphia into a decision?\n\nMaybe he was truly in the running for jobs with those two franchises. Who knows? Those are questions we're throwing out for fodder. No one has all of the information. We're all trying to piece this together. What we do know is that no one likes all of the remaining candidates we're told are still in the running.\n\nWho is the best fit for Jalen Hurts? How do we describe the best fit for the roster overall? Who carries the most red flags? Is there any long-term stability for any of these guys?\n\nAs we ventured through Championship Sunday, we learned the Eagles had interest in Arthur Smith before he accepted the Ohio State Buckeyes job. Once we began another workweek, it was learned that Charlie Weis Jr. had removed his name from consideration. Hours later, Declan Doyle arrived at the same decision.\n\nJust for kicks, we took a breather. We plugged a few names into ChatGPT and asked who the best candidates were for the Eagles' OC job. What we learned was AI's list looks a lot like some of our own. Here's what they came up with. Keep in mind that we aren't sure whether Brian Daboll is still in the running.\n\nAI gives Brian Daboll a five-star rating as a potential OC hire, citing he has the best chance to elevate Jalen Hurts immediately. He lands atop the list because he has already done the job before and has previously had success working with Josh Allen.\n\nMike Kafka lands second on the list. His approach seems to emphasize rhythm and being 'on time'. Those are areas where we have seen Jalen Hurts struggle. That could lead to questions, but Kafka has coached mobile quarterbacks before and understands how to blend run concepts into the passing game.\n\nFrank Smith is one of the new additions to this list. He worked with Mike McDaniel as his offensive coordinator. He's intriguing and shouldn't be viewed as someone the Eagles are pursuing, since he was closest to one of the guys they actually wanted.\n\nNagy is a descendant of the Andy Reid coaching tree. He never quite recovered from the 'Double-Doink Game'. Word has it that he even had kickers audition for a job the following season by kicking from the same spot that Cody Parkey missed the go-ahead field goal attempt in the Wild Card Game.\n\nNagy is better than the reputation suggests. He could do a good job in returning to the place where his coaching career began.\n\nHere's one of the guys we know the least about, yet AI ranks him fifth-best. Settling on him means the Eagles would have placed more emphasis on potential than on proof and his resume.\n\nJerrod Johnson is another of the new additions to the Eagles' OC conversation. He is a good teacher, but this may be a mismatch in terms of need. Known as a QB developer, he would be asked to grow into his new role a la Kevin Patullo. If you remember that ultimately led to Patullo's undoing. Johnson might be a 'wrong place, wrong time' candidate, but again, these are only opinions we're sharing.\n\nSome would rank Jim Bob Cooter higher. He actually has OC experience. There's an obvious low ceiling here, as there isn't much evidence that he elevates quarterbacks or builds innovative systems that let them do what they do best.\n\nThe Eagles need to reinvent their offense, and they need to reinvent Jalen Hurts to some degree. They need someone who understands how to do both. None of these guys is a slam-dunk hire in that regard.\n\nAll have positives. All have flaws. One of the most important decisions of the offseason keeps being weighed. One false move and Philadelphia will throw away another season.\n\nThis article originally appeared on Eagles Wire: Ranking Eagles' remaining OC candidates using artificial intelligence",
    "readingTime": 4,
    "keywords": [
      "offensive coordinator",
      "jalen hurts",
      "eagles oc",
      "brian daboll",
      "candidates",
      "we're",
      "guys",
      "list",
      "learned",
      "another"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/ranking-eagles-remaining-oc-candidates-181501769.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/2DT2DutgTRojB05W6HgqZg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/philadelphia_eagles_wire_usa_today_sports_articles_352/77a3e8d8d6a3a3fee555a566b676a46b",
    "created_at": "2026-01-26T18:22:20.909Z",
    "topic": "sports"
  },
  {
    "slug": "agents-are-about-to-change-software",
    "title": "Agents Are About to Change Software",
    "description": "Man, it is a really weird time in the software world right now. Like a lot of people, when I picked up ChatGPT and Midjourney back in late 2022. It felt like wizardry. For a year I experimented and…",
    "fullText": "Man, it is a really weird time in the software world right now. Like a lot of people, when I picked up ChatGPT and Midjourney back in late 2022. It felt like wizardry.\n\nFor a year I experimented and experimented. I wrote about how Midjourney responds to emojis and even tried to see if I could write a children’s book with AI. The output here looks pretty rough today, but it was a revelation at the time. And the AI tools just kind of gradually got better and better.\n\nOne thing that never really clicked for me though was agentic coding. The vision is that you tell an LLM what you want, and it just goes off and executes it. It never really worked that well. It felt like the agents just kind of plowed through code and broke a bunch of stuff on the way to making the fix I wanted. Coding agents were more a curiosity, not something I could actually use.\n\nHowever, this is all about to change. And it is going to change everything about how software is built.\n\nI was drawn back by a post called Welcome to Gas Town by Steve Yegge.\n\nGas Town is part visionary, part performance art. It’s a Mad Max-themed fever dream that enables agents, managing agents, managing agents. There’s a mayor, rigs, polecats, a deacon and a refinery—and they all work together in this vast factory where vibes go in and code comes out.\n\nThere’s a lot of really clever ideas that somewhat mesh together. You’ve probably heard about the context window LLMs have. Essentially it’s their short-term memory. Once an agent uses about 20% of its context window, its intelligence drops off a cliff and it starts doing insane things like dropping databases.\n\nGas Town employs a trick to manage that issue. It assigns tasks to ephemeral “Polecat” agents. Polecats do a task and then disappear—basically removing the challenge of managing context windows.\n\nMaggie Appleton articulated the value of Gas Town well:\n\nWe should take Yegge’s creation seriously not because it’s a serious, working tool for today’s developers (it isn’t). But because it’s a good piece of speculative design fiction that asks provocative questions and reveals the shape of constraints we’ll face as agentic coding systems mature and grow.\n\nAnd so, intrigued by Gas Town, I decided to try vibe coding again.\n\nI’m not the type to just dip my toes in. If I’m going to do something, I do a cannonball.\n\nI’m always looking for the meta. What is the best strategy and who knows how to execute it? I read that Anthropic’s CTO keeps five agents running constantly and barely looks at the actual files. Either that’s marketing or there must be something there, or maybe both?\n\nI read a bunch of articles and watched a ton of YouTube videos. YouTube was pretty wild. There are these videos with guys streaming 5-10 Claude Code terminals, blaring EDM (lol) and managing all these agents.\n\nI’m kind of poking fun, but I actually learned a lot about setup from that BridgeMind channel. If you’re interested I might start with his videos about Warp and the OpenCode CLI.\n\nI wanted to see how well these agents actually work, but I needed an easy entry point. A Chrome extension to restyle Hacker News seemed perfect. It’s been in my backlog for a while, and because it’s purely frontend, I knew I’d be comfortable judging the output.\n\nAnd honestly it was. Initially I tried to one-shot the thing and—as I expected—that was a failure. But then I decided to slow down. I told the agent to scaffold a chrome extension to restyle Hacker News pages. It worked. And I kind of just broke up these tasks into smaller pieces. Tested them as I went.\n\nSure there were bugs. But I kind of just did what I do when I’m reviewing any engineers’ code. Inspect the DOM, look at the styles, look at the console and then give feedback. When the context window hit’s 20% I typically close that window and open a new chat. The OpenCode CLI even allows you to drop screenshots. It’s pretty wild.\n\nNow, I don’t have a dozen agents running at the same time. I never felt the need to have more than two working. And I’m honestly not sure how UI work even gets shipped in Gas Town? My guess is Yegge is probably a lot less concerned with UX than I am.\n\nWith AI agents, the last mile—that final polish and detailing—will be critical. We already see this in Salesloft, where sellers review generative emails before sending. In design, it manifests as small UI tweaks. It will be something else for doctors and something else for mechanical engineers. But I think there is a real opportunity in refining how humans interact with the agent’s output, creating better loops for feedback and adjustment.\n\nYou know what? There’s something here though. I don’t really like the term “vibe coding.” And I know the concept is polarizing. But after tinkering with this stuff for a couple days—I think agents are about to change how we build software.\n\nThe conception of what it looks like to make software is going to change pretty quickly.\n\nThe three major functions on a delivery team (or feature team) are engineering, design and product management. I’ve long thought we’re going to start to see more overlap in those functions. I’m even more sure of it now.\n\nI think we’re going to start to see a hybrid role emerge—product engineer.\n\nWhat does this mean delivery teams will look like in the future? I imagine they are either significantly smaller or significantly more productive. I’m not sure if QA is embedded into these teams the same way they are currently or if there is a separate team of—well, people managing QA agents. I have a lot of questions.\n\nThis also makes me think a lot about Ben Thompson’s theory on bundling and unbundling. From 2010 to 2015, companies quickly moved on the back of frameworks. First like Ruby on Rails and Bootstrap. Then on other technologies like Angular and React. The speed these frameworks provided caused an unbundling in the software world.\n\nPoint solutions were able to move fast and gather steam while slow incumbents either weren’t nimble enough or weren’t in a place to capitalize on the productivity provided by frameworks. Starting in 2016, that changed. Customers were overwhelmed with choices. Larger companies caught on and smaller ones consolidated into larger platforms.\n\nI think that’s about to shift again. And probably this year.\n\nDavid Cummings called it out in his newsletter this weekend. SaaS companies are about to see a massive wave of new competition. And it is going to happen extremely fast. The bar to build software has been lowered. A two-person team will soon be able to build what used to take a whole department.\n\nThis puts incumbent software companies in a pretty dangerous situation. Those that are not able to be nimble and go fast are going to be in real trouble. I think this is especially true in the consumer, SMB and mid-market segments. Enterprise software may have some buffer as customers of enterprise software are buying a process more than the software itself.\n\nHonestly, it makes me a little nervous. The industry is going to change and everyone’s jobs are going to look a little different—product design included.\n\nBut as someone who got into software just because I wanted to make things, this is a dream come true. I’m seeing a glimpse of the vision I hoped for in 2022. It’s not just a toy anymore. It’s an unbelievable tool for builders.\n\nIf you’ve been ignoring AI tools because you think they are overhyped, or maybe don’t see how they fit into your workflow. I’d encourage you to give them another look.",
    "readingTime": 7,
    "keywords": [
      "restyle hacker",
      "chrome extension",
      "agentic coding",
      "vibe coding",
      "pretty wild",
      "context window",
      "enterprise software",
      "gas town",
      "agents managing",
      "hacker news"
    ],
    "qualityScore": 1,
    "link": "https://solomon.io/agents-are-about-to-change-software/",
    "thumbnail_url": "https://solomon.io/wp-content/uploads/2026/01/WelcomeToGasTown-2200x1196.jpg",
    "created_at": "2026-01-26T18:21:39.265Z",
    "topic": "tech"
  },
  {
    "slug": "am-i-the-only-one-who-switches-between-chatgpt-gemini-and-claude",
    "title": "Am I the only one who switches between ChatGPT, Gemini, and Claude?",
    "description": "Am I the only one who switches between #Grok, #ChatGPT, #Gemini, and #Claude? Meet Context Wallet.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/oswarld_oz/status/2015432998406226289",
    "thumbnail_url": "https://pbs.twimg.com/amplify_video_thumb/2015426357887725568/img/5auISEy4m8VBFIVN.jpg:large",
    "created_at": "2026-01-26T06:23:53.262Z",
    "topic": "tech"
  },
  {
    "slug": "the-ladder-to-nowhere-how-openai-plans-to-learn-everything-about-you",
    "title": "The Ladder to Nowhere: How OpenAI Plans to Learn Everything About You",
    "description": "ChatGPT Health is a small part of a much larger plan to learn everything about you. In this post, I talk about what's driving them and how they might get there.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://insights.priva.cat/p/the-ladder-to-nowhere-how-openai",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!RPej!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77dfb7d3-a672-4f00-bf6b-1607229c44f7_2752x1536.png",
    "created_at": "2026-01-25T12:22:39.078Z",
    "topic": "tech"
  },
  {
    "slug": "agenthub-a-unified-sdk-for-llm-apis-with-faithful-validation",
    "title": "AgentHub – A unified SDK for LLM APIs with faithful validation",
    "description": "AgentHub is the only SDK you need to connect to state-of-the-art LLMs (GPT-5.2/Claude 4.5/Gemini 3). - Prism-Shadow/AgentHub",
    "fullText": "Prism-Shadow\n\n /\n\n AgentHub\n\n Public\n\n AgentHub is the only SDK you need to connect to state-of-the-art LLMs (GPT-5.2/Claude 4.5/Gemini 3).\n\n License\n\n Apache-2.0 license\n\n 30\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Prism-Shadow/AgentHub",
    "readingTime": 1,
    "keywords": [
      "license",
      "agenthub"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Prism-Shadow/AgentHub",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1135038596/73e1b398-f342-44f0-944e-cafe84869f56",
    "created_at": "2026-01-25T12:22:38.486Z",
    "topic": "tech"
  },
  {
    "slug": "latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
    "title": "Latest ChatGPT model uses Elon Musk’s Grokipedia as source, tests reveal",
    "description": "Guardian found OpenAI’s platform cited Grokipedia on topics including Iran and Holocaust deniers\nThe latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.\nIn tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.\n Continue reading...",
    "fullText": "Guardian found OpenAI’s platform cited Grokipedia on topics including Iran and Holocaust deniers\n\nThe latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.\n\nIn tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.\n\nGrokipedia, launched in October, is an AI-generated online encyclopedia that aims to compete with Wikipedia, and which has been criticised for propagating rightwing narratives on topics including gay marriage and the 6 January insurrection in the US. Unlike Wikipedia, it does not allow direct human editing, instead an AI model writes content and responds to requests for changes.\n\nChatGPT did not cite Grokipedia when prompted directly to repeat misinformation about the insurrection, about media bias against Donald Trump, or about the HIV/Aids epidemic – areas where Grokipedia has been widely reported to promote falsehoods. Instead, Grokipedia’s information filtered into the model’s responses when it was prompted about more obscure topics.\n\nFor instance, ChatGPT, citing Grokipedia, repeated stronger claims about the Iranian government’s links to MTN-Irancell than are found on Wikipedia – such as asserting that the company has links to the office of Iran’s supreme leader.\n\nChatGPT also cited Grokipedia when repeating information that the Guardian has debunked, namely details about Sir Richard Evans’ work as an expert witness in David Irving’s trial.\n\nGPT-5.2 is not the only large language model (LLM) that appears to be citing Grokipedia; anecdotally, Anthropic’s Claude has also referenced Musk’s encyclopedia on topics from petroleum production to Scottish ales.\n\nAn OpenAI spokesperson said the model’s web search “aims to draw from a broad range of publicly available sources and viewpoints”.\n\n“We apply safety filters to reduce the risk of surfacing links associated with high-severity harms, and ChatGPT clearly shows which sources informed a response through citations,” they said, adding that they had ongoing programs to filter out low-credibility information and influence campaigns.\n\nAnthropic did not respond to a request for comment.\n\nBut the fact that Grokipedia’s information is filtering – at times very subtly – into LLM responses is a concern for disinformation researchers. Last spring, security experts raised concerns that malign actors, including Russian propaganda networks, were churning out massive volumes of disinformation in an effort to seed AI models with lies, a process called “LLM grooming”.\n\nIn June, concerns were raised in the US Congress that Google’s Gemini repeated the Chinese government’s position on human rights abuses in Xinjiang and China’s Covid-19 policies.\n\nNina Jankowicz, a disinformation researcher who has worked on LLM grooming, said ChatGPT’s citing Grokipedia raised similar concerns. While Musk may not have intended to influence LLMs, Grokipedia entries she and colleagues had reviewed were “relying on sources that are untrustworthy at best, poorly sourced and deliberate disinformation at worst”, she said.\n\nAnd the fact that LLMs cite sources such as Grokipedia or the Pravda network may, in turn, improve these sources’ credibility in the eyes of readers. “They might say, ‘oh, ChatGPT is citing it, these models are citing it, it must be a decent source, surely they’ve vetted it’ – and they might go there and look for news about Ukraine,” said Jankowicz.\n\nBad information, once it has filtered into an AI chatbot, can be challenging to remove. Jankowicz recently found that a large news outlet had included a made-up quote from her in a story about disinformation. She wrote to the news outlet asking for the quote to be removed, and posted about the incident on social media.\n\nThe news outlet removed the quote. However, AI models for some time continued to cite it as hers. “Most people won’t do the work necessary to figure out where the truth actually lies,” she said.\n\nWhen asked for comment, a spokesperson for xAI, the owner of Grokipedia, said: “Legacy media lies.”",
    "readingTime": 4,
    "keywords": [
      "sir richard",
      "richard evans",
      "holocaust deniers",
      "llm grooming",
      "cited grokipedia",
      "expert witness",
      "citing grokipedia",
      "chatgpt",
      "disinformation",
      "topics"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/24/latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
    "thumbnail_url": "https://i.guim.co.uk/img/media/202d8061a28d8c1b855097fb90558014cb00d220/135_0_4675_3740/master/4675.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=fc74f3ad623847f91b340f08501077e0",
    "created_at": "2026-01-24T18:17:06.605Z",
    "topic": "tech"
  },
  {
    "slug": "opensource-ad-infra-for-llms-reverseengineered-from-chatgpt",
    "title": "Open-source ad infra for LLMs (reverse-engineered from ChatGPT)",
    "description": "Open-source ad serving platform for LLM applications - inspired by ChatGPT Bazaar system - system32miro/ai-ads-engine",
    "fullText": "system32miro\n\n /\n\n ai-ads-engine\n\n Public\n\n Open-source ad serving platform for LLM applications - inspired by ChatGPT Bazaar system\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n system32miro/ai-ads-engine",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/system32miro/ai-ads-engine",
    "thumbnail_url": "https://opengraph.githubassets.com/5a21764c0d8dccbdd55ea03a277ef4c0a8874e2b8572009a827851dcf1a5a9b3/system32miro/ai-ads-engine",
    "created_at": "2026-01-24T00:56:46.232Z",
    "topic": "tech"
  },
  {
    "slug": "openais-recently-departed-vp-of-research-calls-googles-comeback-openais-fumble",
    "title": "OpenAI's recently departed VP of research calls Google's comeback 'OpenAI's fumble'",
    "description": "Jerry Tworek, OpenAI's former VP of research, said the ChatGPT maker should have never lost its early lead to Google.",
    "fullText": "Sometimes a comeback story starts with a fumble.\n\nA former top OpenAI researcher said Google's AI renaissance is as much about OpenAI's missteps as it is about what the search giant got right.\n\n\"Personally, what I think you should consider Google's comeback, I think it's OpenAI's fumble,\" Jerry Tworek, a former VP of research at OpenAI, said on a Wednesday episode of Ashlee Vance's \"Core Memory\" podcast.\n\nTworek, who spent almost seven years at OpenAI, said earlier this month that he left the startup \"to try to explore types of research that are hard to do at OpenAI.\"\n\nOpenAI CEO Sam Altman declared a \"Code Red\" in December amid increasing competition from Google. The tech giant received wide praise across the industry for the capabilities of its Gemini 3 AI model, which some observers said had surpassed ChatGPT.\n\nWhile declining to detail what he described as OpenAI's missteps, Tworek said that the pioneering AI company should never have lost the lead it established with the release of ChatGPT in 2022.\n\n\"If you are a company that is ahead and has all the advantages that OpenAI has you should always stay ahead,\" he said.\n\nOverall, Tworek said, \"Google did a lot of things right.\"\n\n\"Very clearly, Google started treating seriously at that moment, training large language models and, like, through OpenAI fumbling its lead, they are very, very close now in capability and in terms of models trained,\" he said, adding that the whole industry began to up its investment in AI when OpenAI showed ChatGPT could generate revenue.\n\nAs for OpenAI, Tworek said that the sheer toll of the AI race has led the non-profit-research lab-turned-public-benefit-corporation to place less of an emphasis on risky research that may not yield results. A spokesperson for OpenAI did not respond to Business Insider's request for comment.\n\n\"There are multiple aspects of certain things that are just hard to do in a company that has to compete in an extremely, extremely brutal and demanding race for having the best AI model in the world right now,\" he said. \"One dynamic is there is naturally how much willingness of risks companies are willing to take from the perspective of trying to not fall behind.\"\n\nTworek said \"all major AI companies\" are facing pressure to show user growth and pay for GPUs while simultaneously competing to be the best available model.\n\n\"That does affect somehow your appetite for risk that you are willing to take,\" he said.\n\nDo you work at OpenAI or Google? Contact the reporter from a non-work email and device at bgriffiths@businessinsider.com",
    "readingTime": 3,
    "keywords": [
      "openai's missteps",
      "openai",
      "research",
      "model",
      "comeback",
      "fumble",
      "giant",
      "industry",
      "lead",
      "ahead"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-google-ai-race-fumble-gemini-2026-1",
    "thumbnail_url": "https://i.insider.com/69725e9ea645d1188187cf20?width=1200&format=jpeg",
    "created_at": "2026-01-23T12:26:13.658Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-making-more-than-1-billion-a-month-from-something-that-has-nothing-to-do-with-chatgpt",
    "title": "OpenAI is making more than $1 billion a month from something that has nothing to do with ChatGPT",
    "description": "Sam Altman says OpenAI added more than $1 billion in annual recurring revenue in a month, driven by its API business rather than ChatGPT.",
    "fullText": "OpenAI has pulled in a billion-dollar month from something other than ChatGPT.\n\nSam Altman said in a post on X on Thursday that OpenAI added more than $1 billion in annual recurring revenue in the past month \"just from our API business.\"\n\n\"People think of us mostly as ChatGPT, but the API team is doing amazing work!\" the OpenAI CEO wrote.\n\nOpenAI's API enables other companies and developers to embed its models into their own products, from internal productivity software to coding tools.\n\nMany of Silicon Valley's high-profile startups rely on OpenAI's models as core infrastructure. Perplexity uses OpenAI's models to power parts of its AI search and answer engine. Harvey, one of the fastest-growing legal tech startups, is built on OpenAI's models to assist lawyers with research and drafting.\n\nAltman's comments underscore how OpenAI's infrastructure business is emerging as a key growth engine, even as the company faces massive costs for computing power and data centers.\n\nThose pressures have pushed OpenAI to look beyond consumer subscriptions.\n\nLast week, the company said it is gearing up to test ads inside ChatGPT as it faces about $1.4 trillion in spending commitments over the coming years.\n\nIt's a notable shift for a company that once treated ads as taboo. Less than two years ago, Altman said advertising was a \"last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nSince then, Altman has struck a more open tone. In June, he said on OpenAI's podcast that he wasn't \"totally against\" ads, though he stressed it would need to be approached carefully.\n\nEarlier this week, OpenAI's chief financial officer, Sarah Friar, raised the idea of \"licensing models\" that would let the company share in downstream sales if a customer's product succeeds.\n\n\"Let's say in drug discovery, if we licensed our technology, you have a breakthrough. The drug takes off, and we get a licensed portion of all its sales,\" Friar said in an episode of \"The OpenAI Podcast\" published Monday.",
    "readingTime": 2,
    "keywords": [
      "openai's models",
      "business",
      "startups",
      "infrastructure",
      "engine",
      "faces",
      "resort",
      "sales",
      "drug",
      "licensed"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-1-billion-a-month-api-business-chatgpt-sam-altman-2026-1",
    "thumbnail_url": "https://i.insider.com/6972e427e1ba468a96aa91d7?width=800&format=jpeg",
    "created_at": "2026-01-23T06:20:31.103Z",
    "topic": "finance"
  },
  {
    "slug": "google-just-promised-no-ads-in-gemini-for-now",
    "title": "Google Just Promised No Ads in Gemini (for Now)",
    "description": "The statement comes about a week after OpenAI announced ads are coming to ChatGPT.",
    "fullText": "A week after OpenAI admitted it will soon start testing ads in ChatGPT, Google has promised that it's not planning to inject ads into Gemini anytime soon.\n\nThe statement was given to journalist Alex Heath during the World Economic Forum in Davos, Switzerland. Google DeepMind CEO Demis Hassabis said the company doesn't have \"any plans\" for ads in Gemini. While the statement was fairly brief, it also jibes with a similar quote Hassabis gave to Axios, where he said he was \"a little bit surprised\" that OpenAI was already introducing ads to ChatGPT.\n\nThat surprise is understandable, especially because OpenAI CEO Sam Altman said in 2024 that he considered ads a \"last resort for us as a business model.\" But looking at the numbers, it makes sense that ChatGPT is getting ads long before Gemini is even thinking of them.\n\nWhile Google makes most of its money through showing people ads, it's also able to rely on Search and YouTube to push ads to most of those eyeballs. Meanwhile, OpenAI is pretty much just ChatGPT. As the latter moves to a for-profit model, it now has to put moneymaking first, something it's had trouble doing without relying on traditional internet moneymakers like ads. Google, meanwhile, is already profitable elsewhere, and is able to take its time and use its sheer size to keep Gemini ad-free, at least while it continues to chase market share.\n\nDoes this mean Google's AI will never get ads? Well, never say never. But it does mean that they're probably not on the horizon—even if Google plans to more aggressively monetize Gemini over the long term, it isn't facing the same kind of time crunch as Altman's company.\n\nIt remains to be seen whether the presence of ads will push users away from ChatGPT, but the move comes in the wake of significant wins for Gemini and one major loss for ChatGPT. First, Google's Nano Banana image editing model went viral on social media, winning over the general public. Then, Google struck a deal with Apple to put its AI into the iPhone, and it looks like Gemini will be powering Siri for the foreseeable future.\n\nMeanwhile, ChatGPT reportedly saw a 6% dip in users early last month, following a model update from Gemini—and that was before the introdution of ads. While ChatGPT still seems to be in the lead on total user count, there's evidence that Google is catching up.\n\nThe divide in strategy seems clear: As OpenAI seeks ways to get more money out of its existing user base, Google can focus on growing its own with new integrations into the products we already use every day. I can't say what the limits of this growth are, but I can say that I rarely go out of my way use AI, yet I've still found myself accidentally relying on Google's AI overviews every now and then. If Google can get more people like me to casually integrate AI into our regular workflows, it's possible we could soon have a new AI leader on our hands.",
    "readingTime": 3,
    "keywords": [
      "google's ai",
      "it's",
      "model",
      "soon",
      "google",
      "chatgpt",
      "gemini",
      "statement",
      "hassabis",
      "plans"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-just-promised-no-ads-in-gemini-for-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFGZ1E3E2BQZ7DBB464WC469/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.618Z",
    "topic": "tech"
  },
  {
    "slug": "apple-might-turn-siri-into-an-ai-chatbot-to-rival-chatgpt",
    "title": "Apple Might Turn Siri Into an AI Chatbot to Rival ChatGPT",
    "description": "It could be ChatGPT's next big competitor.",
    "fullText": "Last week, Apple finally admitted it will need to team up with Google to finally make good on that contextual Siri promise it made two years ago, which would have allowed the virtual assistant to integrate with content like your texts or emails to answer personal questions and take actions for you. Now, according to a new report, the iPhone company might actually go one step further and turn Siri into a full-fledged AI chatbot—one on par with the likes of ChatGPT, and perhaps even more sophisticated.\n\nCurrently, Siri has AI implementation, but only technically, and it's certainly underwhelming: You can use it to get tech support on Apple products or shunt questions off to ChatGPT, but otherwise, Siri basically works as it always has. But according to Bloomberg's Mark Gurman, who has reliably reported on insider information at Apple before, the company is finally not only looking to make Siri smarter, but also change the way you interact with it. Currently planned for iOS and macOS 27 under the name \"Campos,\" Siri's new chatbot interface will still be powered by Gemini, but will allow you to both type and talk to Siri, with full continuity between your conversations. This upgrade will be in addition to the overdue features that were already announced.\n\nIn other words, it'll look something like the chatbot interface from the ChatGPT app or the standalone Gemini app. Yes, you can technically type to Siri right now, but it mostly works like a separate input method, rather than as a full conversation. You can't scroll through your previous questions to Siri or peruse the assistant's previous answers, and if you ask Siri to reference a message you sent it two weeks ago, it'll have no idea what you mean. That's far behind what other AI chatbots offer right now.\n\nThe update will also apparently further expand Siri's capabilities even beyond the contextual or personalization upgrades that were already revealed. Gurman says that, while the contextual upgrades will be able to pull information from other apps like Messages, the chatbot-style Siri will be \"integrated into all of the company's core apps, including ones for mail, music, podcasts, TV, Xcode programming software and photos.\" Essentially, Siri will have more access to your iPhone than other AI chatbots, and those integrations will go beyond what was previously promised. That could make it more or less appealing to you, depending on your tastes in AI integration.\n\nWith the chatbot interface planned for iOS 27, it's likely to come after the contextual upgrades, rather than at the same time. That's because, as Gurman said previously, those upgrades are set for the spring. He predicts we'll learn more about it during this year's WWDC, which, if it follows the standard set by previous years, will take place in June.\n\nThe move to turn Siri into a chatbot could come across as a a much-overdue modernization, as Google has already done the same with Gemini over on Android, but it's also a bit of a surprise, as Apple had previously said it did not intend to turn Siri into a \"bolt-on chatbot on the side\" for Apple Intelligence.\n\nBut Apple was likely talking about quality of the experience rather than expressing any significant anti-chatbot bias among the development team, meaning the fact that Siri is turning into a chatbot could mean the company is finally happy with the direction it's headed. But it's also possible that the professed skepticism about turning Siri into a chatbot was meant to appeal to AI skeptics in general. Unfortunately, if you're still skeptical about AI, it currently seems like iOS 27 will be a boring update for you, as Gurman indicated the new Siri chatbot will be the \"primary new addition\" to the operating system.\n\nHowever you feel about it personally, Siri as a full-fledged AI chatbot could seriously upset ChatGPT's market dominance—ironic, given its early integration with Apple Intelligence. Currently, OpenAI has reportedly admitted it's in a Code Red situation, as it is losing market share to Google and introducing ads to bolster its bottom line. The new Siri, being powered by Gemini, is unlikely to hurt Google (although it will have more access to your phone than the standalone Gemini app), but its ease-of-access might make it the new go-to for iPhone users, and that could hurt pretty much every AI company Apple isn't in business with directly.",
    "readingTime": 4,
    "keywords": [
      "standalone gemini",
      "gemini app",
      "apple intelligence",
      "contextual upgrades",
      "chatbot interface",
      "it's",
      "siri",
      "finally",
      "google",
      "iphone"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-siri-chatbot-ai-plans?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFH7N4CFDDEPBSQ5VKSC72K1/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.513Z",
    "topic": "tech"
  },
  {
    "slug": "solopreneurs-are-embracing-ai-heres-how-3-career-coaches-have-found-it-indispensable",
    "title": "Solopreneurs are embracing AI — here's how 3 career coaches have found it indispensable",
    "description": "AI tools like ChatGPT, Gemini, and Claude help solopreneur coaches Kim Surko, Katharine Campbell Hirst, and Liz Morrison boost client outcomes and streamline coaching workflows.",
    "fullText": "When it comes to career and executive coaching, some of the most important work happens in one-on-one sessions with the client. That's when breakthrough insights often emerge, motivation gains momentum, and coaches build an essential connection.\n\nThen comes the hard part: turning those words into actions.\n\nTranscribing conversations into meaningful action items for clients takes time and effort — something a team of assistants could easily handle. However, when you're the only person running your business, that time and effort leaves coaching solopreneurs with a difficult tradeoff: take notes during sessions and sacrifice presence, spend hours after each call transcribing insights from recordings, or leave follow-through entirely to clients.\n\n\"Trying to juggle it all on my own wasn't an option — it was just impossible to build a sustainable business,\" said Kim Surko, founder of leadership coaching business Surko Coaching, regarding how easy it was to feel overwhelmed by work that felt more administrative than transformational. \"Leaning into AI was the most natural solution to help with all of that responsibility.\"\n\nHere's how three solopreneurs in leadership, business, and communications have used AI to help clients achieve more results in less time, expanding their capacity as coaches while increasing the value of the work they offer.\n\nFor all three coaches, the biggest game changer has been using AI note-takers to distill long conversations into something more tangible.\n\nAfter getting client consent, \"I record my coaching sessions and upload transcripts into ChatGPT. This allows me to rapidly transform nuanced insights from our conversations into concrete outputs clients can actually use — pitches, résumés, website copy, positioning statements, and more,\" said Katharine Campbell Hirst, founder of business coaching company KCH Coaching & Advisory. \"What used to take weeks of agonizing refinement now takes minutes.\"\n\nLiz Morrison, founder of communications coaching company LM Strategic Storytelling, appreciates how AI ensures that valuable sound bites from her coaching sessions don't get lost in hours of recordings that nobody has time to revisit. She's built custom projects in Claude to help her transform session transcripts into \"Story Banks\" in minutes — pulling out three to six narratives per session that clients can use immediately for interviews, networking, social media, and building their businesses.\n\nWhile this type of work was essential before AI, doing it as a solopreneur meant sacrificing time that could be spent supporting other clients. \"I've saved almost an hour per client per day by relying on AI to take notes and summarize them for me,\" said Surko, who added that she nearly doubled her capacity for coaching clients with AI's support.\n\nSurko has also used AI to help her clients appreciate the progress they're making, improving the feeling of momentum. \"A lot of work with coaching is celebrating the small wins,\" she said.\n\nUsing the project management tool Kanbanchi, supported by Gemini, Surko can quickly update to-do list boards that lay out all of the client's goals and achievements.\n\n\"Having that visual representation of the progress we're making shows the value of coaching,\" Surko said. The process has been extremely valuable, as it has improved her client renewal rate because clients can see exactly how they're getting closer to a goal, rather than feeling like they aren't making progress, she added.\n\nMorrison tells a similar story. She built a custom ChatGPT tool called Story Explorer that walks prospective clients through a story-coaching exercise to uncover one immediately usable story they can post on LinkedIn or use in a networking conversation.\n\n\"I find when I give people this builder, it's the start of a much bigger conversation,\" she said. They often uncover other narratives they want to explore further with Morrison, she said.\n\nAlongside the benefit of being more present during conversations, these coaches have found AI valuable for improving their in-session coaching in other ways.\n\nSurko, for example, used Gemini within Google Docs to create a searchable archive of the massive toolkit of exercises and prompts she's collected in her decadeslong career, which before were buried in various folders.\n\nPreviously, she would have to wait until after the session to hunt down an exercise. Now, she can quickly pull them up during sessions and dive deeper with a client. \"We make more progress in each session,\" she said of this improvement. \"We're able to continue that momentum.\"\n\nAI can even be a helpful coach for these coaching experts.\n\nHirst uploaded transcripts across her client's full arc and asked where she did well and where she could have improved. While she also works with coaches, she appreciates that AI can effectively be over her shoulder all the time.\n\n\"The feedback is surprisingly concrete, pattern-based, and immediately actionable — effectively giving me a reflective practice partner I wouldn't otherwise have access to as a solopreneur,\" Hirst said.",
    "readingTime": 4,
    "keywords": [
      "coaching sessions",
      "clients",
      "client",
      "coaches",
      "business",
      "conversations",
      "progress",
      "insights",
      "momentum",
      "founder"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/3-coaching-solopreneurs-how-ai-help-their-businesses-grow-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8b9ee1ba468a96aa3c19?width=1200&format=jpeg",
    "created_at": "2026-01-21T18:30:49.965Z",
    "topic": "finance"
  },
  {
    "slug": "genai-the-snake-eating-its-own-tail",
    "title": "GenAI, the Snake Eating Its Own Tail",
    "description": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first  superpower is a boon: they can dramatically increase ...",
    "fullText": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first \nsuperpower is a boon: they can dramatically increase human productivity. I use them on a regular basis to answer \nquestions, learn new skills, write code, create images, and much more, all at a rate of speed and quality that was\nscience fiction just a few years ago. The second superpower is a bane: GenAI is quietly destroying the very \necosystems that made it possible in the first place.\n\nUnder the hood, GenAI is built on large language models (LLMs), which are able to extract patterns, structure, and\nstatistical relationships from massive data sets. These data sets consist primarily of content created by human beings: \nbooks, blog posts, articles, forum discussions, open source code, art, photography, and so on. LLMs are able to extract \nvalue from this content at an unprecedented scale, but all that value is captured by the GenAI company and its users. \nIf you’re a content creator, you get nothing: no attribution, no referral traffic, no revenue share. Not even a \nthank-you.\n\nThis feels unsustainable to me, a bit like a snake eating its own tail. In this blog post, I’ll go through three\nexamples of how GenAI is destroying the very ecosystems it relies on, and then discuss possible solutions that may\ngive everyone (users, GenAI companies, and content creators) more value.\n\nFor many years, StackOverflow was the most popular Q&A site for programmers. Any time you \nhit a weird error while coding, you’d do a search on Google, and more often than not, find a good answer on \nStackOverflow. But now, in large part due to GenAI, StackOverflow is nearly dead:\n\nAlthough StackOverflow’s decline started before GenAI went mainstream (ChatGPT was first released in 2022), GenAI\naccelerated that decline considerably. That’s because nowadays, instead of searching around for an answer on a Q&A site,\nand working to adapt that answer to your own codebase, you can ask GenAI tools to generate your code, fix any errors\nyou hit, answer any questions you run into, and so on. As a result, you’re considerably more productive.\n\nBut it doesn’t seem sustainable. A big part of why GenAI tools can answer programming questions and fix errors in your \ncode is because those tools were trained on StackOverflow data. So you as a programmer and the GenAI tool now \nget much more value from that data, but StackOverflow gets none. If people stop asking and answering questions, what \nwill GenAI train on in the future?\n\nI also get the impression that StackOverflow is not the only online community where this is happening. For example, \nQuora seems largely dead. Wikipedia is\nfacing more threats than ever.\nAnd although Reddit’s traffic numbers don’t show it, I (and many other Redditors) get the impression that it’s also \ndying.1\n\nTailwind CSS is a popular library programmers use to style and decorate their websites. In\nfact, according to the 2025 State of CSS Survey, Tailwind is the\nmost popular CSS library, by far:\n\nAnd Tailwind usage only seems to be growing:\n\nDespite that, just a couple of weeks ago, the Tailwind team had to lay off 75% of its \nstaff. Why? The company behind\nTailwind CSS makes its money by selling a premium upgrade to the open source library called\nTailwind UI, which gives you a set of reusable, pre-built, professionally designed\ncomponents for building out your website. This was a great offering in the past, but in the age of GenAI, it’s more\nproblematic:\n\nSo developers are getting lots of value from Tailwind, GenAI is getting lots of value from Tailwind, but the creators\nof Tailwind are getting crushed. I suspect something similar will happen with many other open source projects.2\n\nWhen my latest book, Fundamentals of DevOps and Software Delivery, came \nout last year, a friend of mine asked me an interesting question:\n\nIn the age of LLMs, will people still use books to learn the fundamentals?\n\nI’m an avid reader, and still believe books play a key role in learning, but there’s no doubt that LLMs provide a new\nmethod of learning that is incredibly compelling. GenAI has two remarkable qualities that make it a great teacher:\n\nYou can ask it all the questions you want, with no fear of sounding dumb. You can repeat the same question over and \nover again if something isn’t clicking. You can request it to explain things in different ways: via text, via audio,\nvia diagrams. I’ve used GenAI to learned dozens of new things over just the last few months: how to do DIY \nprojects around the house, how to rehab a minor injury, how to cook eggs without giving them a sulfuric taste/smell,\nand much more. And I learned most of these without reading anyone’s book or blog.\n\nAnd that’s a problem. Much of the content I got from the GenAI tools was extracted from books and blog posts, with\nno attribution. Even worse, some of this content was extracted illegally. Last year, Anthropic agreed to pay $1.5B\nto settle a class-action \nlawsuit for training their\nLLMs on over 500,000 pirated books. That included several of my previous books!3\n\nThis class-action lawsuit might sound like a win for authors, but it’s actually a disaster. There are likely many\nAI companies training on pirated data who haven’t been caught. And even if they are caught, they might not\nbe sued. And even if they are sued, they might not lose or settle. And even if they do, they will just see it as the\ncost of doing business. Anthropic recently raised $13B, reported revenue at $5B per year, and is valued at \n$183B; OpenAI is trying\nto raise $100B, with reported revenue of $20B per year, and a valuation of \n$830B. And all these\nnumbers are growing fast. A $1.5B fine is just a drop in the bucket for companies like this. It’s a risk worth taking.\n\nAnd I’m guessing all the GenAI companies are taking that risk. In another lawsuit, OpenAI\nargued that it’s ‘impossible’ to create AI tools like ChatGPT without copyrighted \nmaterial. If that’s\nwhat it takes to get to an $830B valuation, you better believe they are all going to steal and pirate as much \ncontent as possible. And when they do, the creators of that content will get nothing. Nada. $0.\n\nThere are countless other examples where GenAI is benefiting from content, while giving nothing back to the content\ncreators: e.g., art, music, design, movies, copywriting, and so on. At its root, the GenAI model is broken:\n\nDid you notice what’s missing? The user has a way to get value (step 4), the GenAI company has a way to get value \n(step 3), but the content creator gets nothing. Compare this to the search engine model (e.g., Google Search), which is \nwhat we all used before GenAI came along:\n\nThe search engine model was not perfect, but it at least created the opportunity for all parties in this three-sided \nmarketplace to capture value: the user in step 3, the search engine company in step 4, and the content creator in \nstep 5.\n\nIn short, the current GenAI model destroys the incentives to create new content. I’ve heard this referred to as “the \ngreat content collapse.” Will it lead to a world where, after the 2020s, there’s little-to-no content created by humans? \nWill the state of knowledge and creativity stagnate as a result?\n\nTo be clear, I’m not an innocent party in this. As I mentioned numerous times in this post, I use GenAI regularly.\nThere’s no doubt that it makes me more productive. I even used GenAI to create the cover image for this blog post! \nBut each time I use ChatGPT or Claude, I feel a bit guilty, as it doesn’t feel sustainable. The snake can’t keep \neating its own tail indefinitely.\n\nSo the question is, what do we do? If we want to avoid the great content collapse, we need a model of GenAI usage that \ncreates opportunities for all parties (user, GenAI company, content creator) to capture value. Below are two ideas for \nhow we might accomplish this.\n\nThe only attempted solution I’ve heard about so far is CloudFlare’s pay-per-crawl \nmodel, which seems to work as follows:\n\nOn the one hand, it’s fantastic to see a major company try to do something about this problem. On the other \nhand, this approach seems to address the wrong part of the problem. The real value isn’t in crawling the data, it’s \nin using it. For every one crawl, an LLM might use the data thousands or millions of times. If creators are only paid \nper crawl, then the GenAI company still captures 99.999% of the value, and the creator gets next to nothing. Moreover,\nthis model only seems to work for websites (it’s not clear how you adapt it to books, art, music, etc.), and it \ncreates an incentive for GenAI companies to only crawl free content, which means paid content is less likely to ever be\ndiscovered (which disproportionally benefits those with pockets deep enough to keep their content free).\n\nI came across a clever solution that felt directionally correct from this LinkedIn post by Tyrone \nJoel\nwhere he took a PDF of my book Terraform: Up & Running, uploaded it into\na GenAI tool, and asked the tool to follow the guidance in the book to generate Terraform code. This feels like it has\nall the ingredients of a model of GenAI usage that is sustainable: the user gets value from the GenAI tool’s responses,\nthe GenAI company gets value from the user paying for a subscription, and the content creator gets value from the user \npaying for their content (in this case, buying my book). This works fine for a single, specific piece of content, but \nhow do you make it work at scale, across all the content that is consumed by an LLM?\n\nHere’s a rough proposal for what I’ll call the pay-per-use model:\n\nThis model works for not only websites, but other types of content too, including copyrighted content. As a content \ncreator (e.g., author, musician, designer, etc.), you could opt into sharing your copyrighted content with a GenAI\ncompany in exchange for getting referrals and revenue sharing each time your content is used. It might even help with \nmaking open source more sustainable, as open source creators could earn revenue and referrals each time a GenAI tool \nuses their code.\n\nIt’s critical that we find a more sustainable model as soon as possible. The snake can’t eat its own tail indefinitely. \nAnd the snake—GenAI—isn’t going away. We can’t put the genie back in the bottle. In fact, it’s only going to get \nbetter, more ubiquitous, and to provide more and more value to users and GenAI companies. But if we can’t find a way to \nprovide value to content creators too, then this will all fall apart.\n\nThat said, I don’t know enough about LLMs to say if a pay-per-use model is actually possible. Can LLMs track the source \nof the content they consumed? Will GenAI companies be willing to do a revenue sharing model? Will they be willing to\nbe transparent about their sources and usage? What do you think? Let me know in the comments.\n\nMany subreddits feel like a hollow shell of what they used to be. In part, this may be because a lot of the content in online communities now feels like it’s generated by bots (“AI slop”). But I think the bigger issue is that, just like StackOverflow, reading posts in online communities is no longer the best way to get answers. I used to use Reddit for research all the time; in fact, Google Search had gotten so bad, that you pretty much had to include “reddit” in your search queries to get a half-decent response. But nowadays, I use GenAI tools for much of my research. Just in the last few months alone, I’ve used ChatGPT and Claude to research solar panels, plan a trip to Norway, make changes to my diet, pick out new shoes for running, pick out new speakers for my living room, and dozens of other questions. Just a year ago, the vast majority of these questions would’ve brought me to Reddit. Nowadays, virtually none of them do, even though I suspect many of the responses I get from GenAI are based on Reddit content. ↩\n\nI’m seeing more and more projects avoiding open source dependencies entirely, and instead having GenAI generate the all code they need directly in their own codebase. There are some benefits to this approach—faster builds, more reproducible builds, less supply chain risk—but it makes sustainably funding open source even harder. You spend years to create and share an open source library with the world, and a bunch of GenAI tools copy your code, with you getting zero credit or value back. ↩\n\nHow much will I get paid as a result of this settlement? It’s hard to know exactly, as it depends on how many authors end up submitting claims, but the current estimate is $3,000 per book, though that number is split with the publisher, so in practice, it’ll be closer to $1,500 per book. If you assume that a book takes just 3 months of full-time work, or about 500 hours (which is likely an under-estimate), and all you get is $1,500, that works out to about $3/hour. Writing non-fiction tech books was never a particularly lucrative affair, but $1,500 is just downright insulting. Worse yet, the other benefits you used to get as an author—recognition as an expert, invitations to talks, job opportunities, marketing for your company or consulting—are significantly reduced too, as far fewer people read your book, or are even aware that you wrote a book, as the LLM usually doesn’t attribute any of its knowledge back to the source. ↩",
    "readingTime": 12,
    "keywords": [
      "q&a site",
      "class-action lawsuit",
      "art music",
      "tail indefinitely",
      "online communities",
      "snake can’t",
      "search engine",
      "blog posts",
      "genai tools",
      "genai tool"
    ],
    "qualityScore": 1,
    "link": "https://www.ybrikman.com/blog/2026/01/21/gen-ai-snake-eating-its-own-tail/",
    "thumbnail_url": "https://www.ybrikman.com/assets/img/blog/gen-ai-snake-eating-tail/snake-eating-tail.png",
    "created_at": "2026-01-21T18:30:43.121Z",
    "topic": "tech"
  },
  {
    "slug": "the-gloves-are-off-in-the-feud-between-sam-altman-and-elon-musk",
    "title": "The gloves are off in the feud between Sam Altman and Elon Musk",
    "description": "The tech titans escalated their long-running feud on Tuesday, trading barbs in public posts about the safety of ChatGPT, Grok, and Tesla's Autopilot.",
    "fullText": "Sam Altman and Elon Musk are at it again, with each of the tech titans taking aim at the other in a series of heated posts on X.\n\nMusk appeared to start the latest escalation early on Tuesday morning, when he posted \"Don't let your loved ones use ChatGPT\" in response to a post that said that use of OpenAI's chatbot had been linked to the deaths of nine children and adults since it was released in 2022.\n\nAltman fired back, first in defense of ChatGPT and OpenAI's desire to protect its users, and then blasting Tesla's Autopilot technology, calling it unsafe.\n\n\"It is genuinely hard; we need to protect vulnerable users, while also making sure our guardrails still allow all of our users to benefit from our tools,\" Altman said.\n\nAltman continued, calling out Autopilot.\n\n\"I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released,\" he wrote. \"I won't even start on some of the Grok decisions.\"\n\nAltman added: \"You take 'every accusation is a confession' so far.\"\n\nThere have been at least eight wrongful-death lawsuits filed against OpenAI that allege use of ChatGPT has contributed to worsening mental health conditions, leading to instances of suicide and murder, including among children and young adults.\n\nSafety concerns around Tesla's self-driving technology have also been central to multiple wrongful-death lawsuits, including one surrounding a 2019 crash in Florida that left a 22-year-old woman dead. A jury determined Tesla was 33% liable for the crash and awarded the plaintiffs $329 million in total damages, Business Insider previously reported.\n\nRepresentatives for Musk and Altman did not immediately respond to requests for comment from Business Insider.\n\nThe social media feud comes as the pair is stuck in the middle of a long-running legal battle over OpenAI's status as a nonprofit company. Musk sued Altman, and other leaders of OpenAI, alleging that they misled him when they decided to pursue a for-profit structure, moving the company away from its original nonprofit mission.\n\nMusk said he donated $38 million to OpenAI when it was originally founded as a nonprofit.",
    "readingTime": 2,
    "keywords": [
      "wrongful-death lawsuits",
      "users",
      "nonprofit",
      "altman",
      "children",
      "adults",
      "released",
      "protect",
      "technology",
      "crash"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/sam-altman-elon-musk-feud-escalates-autopilot-deaths-chatgpt-safety-2026-1",
    "thumbnail_url": "https://i.insider.com/69700640d3c7faef0ecc9b0a?width=1200&format=jpeg",
    "created_at": "2026-01-21T00:59:28.375Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-getting-on-the-ai-age-verification-bandwagon",
    "title": "ChatGPT Is Getting on the AI Age Verification Bandwagon",
    "description": "The app will guess your age and set limits for users it thinks are under 18.",
    "fullText": "When OpenAI first announced GPT-5.2 last month, it quietly disclosed a new safety feature it called \"age prediction.\" Considering ChatGPT proper isn't exactly an \"all ages\" kind of tool, it makes sense that users under the age of 18 should have protections in place to shield them from harmful content. The company says that users who indicate they're under 18 already receive an altered experience to \"reduce exposure to sensitive or potentially harmful content,\" but if the user doesn't voluntarily share how old they are with OpenAI, how does the company enforce these protections? Here's where age prediction comes in.\n\nOn Tuesday, OpenAI officially announced its new age prediction policy, which, like other age verification systems being used by the likes of Roblox, uses AI to guess how old a user is. If the system decides that a particular user is under the age of 18, OpenAI will adjust the experience accordingly, with the goal of keeping all interactions age-appropriate.\n\nHere's how it works: The new age prediction model looks at both the user's behaviors within the app, as well as the general account data. That includes things like how old the account is, what times of day the user is accessing ChatGPT, usage patterns, as well as, of course, the age the user says they are. Looking at all this data, the model determines how old the user likely is. If the model thinks they're over 18, they'll get the full experience; if the model thinks they're under 18, they'll get the \"safer experience.\" If the model isn't confident, it defaults to that safer experience.\n\nThat limited experience means that someone the model thinks is under 18 will try to reduce the following content types:\n\nViral challenges that might inspire \"risky or harmful behaviors\"\n\nRole play that is sexual, romantic, or violent in nature\n\nContent promoting \"extreme\" beauty standards, unhealthy dieting, or body shaming\n\nThe company says that its approach is informed by \"expert input\" as well as literature discussing child development science. (It's not clear whether how much of that input is from direct interviews and coordination with experts, and how much, if any, is from independent research.) The company also acknowledges \"known teen differences in risk perception, impulse control, peer influence, and emotional regulation\" when compared to adults.\n\nThe biggest risk with any of these age prediction models is that they'll sometimes get it wrong—hallucination is an unfortunate habit AI models all share. That goes both ways: You don't want someone too young accessing inappropriate content in ChatGPT, but you also don't want someone older than 18 getting stuck with a limited account for no reason. If you experience the latter situation, OpenAI has a solution for you: direct age verification through Persona. This is the same third-party Roblox uses for its age verification, which hasn't gone very well thus far.\n\nThat doesn't necessarily spell doom for OpenAI. Roblox tried overhauling their age verification system for a massive user base all used to a certain type of multiplayer experience, which led to users not being able to chat with other users in newly-assigned age categories, which were often incorrect. Meanwhile, ChatGPT's age prediction is only controlling the experience of one user at a time. To that end, OpenAI will let you upload a selfie as an added verification step if the prediction model alone isn't enough. Interestingly, OpenAI doesn't say anything about the option to upload an ID for verification, which other companies, like Google, have provided.\n\nI'm not necessarily a fan of age prediction models, as I think they often sacrifice user privacy in the name of creating age-appropriate experiences. But there's little doubt that OpenAI has to do something to limit the full ChatGPT experience for younger users. Many of ChatGPT's users are under 18, and much of the content they experience is wildly inappropriate, whether it be instructions on getting high, or advice on writing suicide notes. In some tragic cases, minors have taken their own lives after discussions with ChatGPT, leading to lawsuits against OpenAI.\n\nI don't have any great answers here. We'll just have to see how this new age prediction model affects the user experience for minors and adults alike, and whether it actually manages to create a safer experience for younger, more impressionable users.",
    "readingTime": 4,
    "keywords": [
      "harmful content",
      "safer experience",
      "prediction models",
      "age prediction",
      "age verification",
      "prediction model",
      "user",
      "users",
      "openai",
      "isn't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-chatgpt-age-prediction-model?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFEH2Q4KPPTQXMTQW5G99Y1B/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-21T00:59:27.310Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here’s how the three models performed in our tests:\n\n💡 If you want the safest pick for real “ship a feature in a big repo” work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you’re okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n⚠️ NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere’s the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe’ll compare code quality, token usage, cost, and time to complete the build.\n\n💡 NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\n💡 NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It’s not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that’s the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that’s a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it’s hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. 🤷‍♂️\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don’t think we’re anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don’t fully back them up. 🥴\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won’t hold up long term.\n\nI don't see any of these recent models as “use it and ship it” for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next",
    "title": "OpenAI launches cheaper ChatGPT subscription, says ads are coming next",
    "description": "OpenAI is bringing its $8/month ChatGPT Go plan to the U.S. and says it will begin testing ads soon in the free tier and Go.",
    "fullText": "OpenAI has announced several important changes to ChatGPT. First, the company says it is rolling out its more affordable ChatGPT Go plan in the United States for $8 per month. OpenAI also confirmed it will soon start testing ads in ChatGPT …\n\nOpenAI first launched ChatGPT Go in India last year and gradually rolled out it to 170 additional countries. Starting today, ChatGPT Go is available everywhere ChatGPT is available, including the United States.\n\nWith this, there are now three tiers of ChatGPT available:\n\nHere’s what you get with ChatGPT Go compared to the free plan:\n\nSecond, OpenAI says that it will “testing ads in the free tier and ChatGPT Go in the US soon.” ChatGPT Plus, Pro, Business, and Enterprise tiers will remain ad-free.\n\nOpenAI detailed its approach to ads in ChatGPT in a blog post published today:\n\nTo start, we plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation. Ads will be clearly labeled and separated from the organic answer. You’ll be able to learn more about why you’re seeing that ad, or dismiss any ad and tell us why. During our test, we will not show ads in accounts where the user tells us or we predict that they are under 18, and ads are not eligible to appear near sensitive or regulated topics like health, mental health or politics.\n\nFurthermore, OpenAI says that your conversations with ChatGPT are not shared with advertisers. Ads also will not influence answers that ChatGPT gives you.\n\nOpenAI says that it’s “not launching ads yet,” but rather plans to “start testing in the coming weeks for logged in adults in the U.S. on the free and Go tiers.”\n\nWhat do you think of today’s announcements from OpenAI? Let us know down in the comments.\n\nMy favorite iPhone accessories:\n\nFollow Chance: Threads, Bluesky, Instagram, and Mastodon.\n\nCheck out 9to5Mac on YouTube for more Apple news:",
    "readingTime": 2,
    "keywords": [
      "chatgpt go",
      "testing ads",
      "plan",
      "tiers",
      "free",
      "openai",
      "soon",
      "health",
      "united",
      "test"
    ],
    "qualityScore": 0.85,
    "link": "https://9to5mac.com/2026/01/16/openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next/",
    "thumbnail_url": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2026/01/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png.webp?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "created_at": "2026-01-18T12:21:37.013Z",
    "topic": "tech"
  },
  {
    "slug": "we-could-hit-a-wall-why-trillions-of-dollars-of-risk-is-no-guarantee-of-ai-reward",
    "title": "‘We could hit a wall’: why trillions of dollars of risk is no guarantee of AI reward",
    "description": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AI\nWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.\nThe figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT.",
    "fullText": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AI\n\nWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.\n\nThe figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT.\n\nThese sky-high numbers are all propped up by investors who expect a return on their trillions. AGI, a theoretical state of AI where systems gain human levels of intelligence across an array of tasks and are able to replace humans in white-collar jobs such as accountancy and law, is a keystone of this financial promise.\n\nIt offers the prospect of computer systems carrying out profitable work without the associated cost of human labour – a hugely lucrative scenario for companies developing the technology and the customers who deploy it.\n\nThere will be consequences if AI companies fall short: US stock markets, boosted heavily by the performance of tech stocks, could fall and cause damage to people’s personal wealth; debt markets wrapped up in the datacentre boom could suffer a jolt that ripples elsewhere; GDP growth in the US, which has benefited from the AI infrastructure, could falter, which would have knock-on effects for interlinked economies.\n\nDavid Cahn, a partner at one leading Silicon Valley investment firm, Sequoia Capital, says tech companies now have to deliver on AGI.\n\n“Nothing short of AGI will be enough to justify the investments now being proposed for the coming decade,” he wrote in a blog published in October.\n\nIt means there is a lot hanging on progress towards advanced AI, and the trillions being poured into infrastructure and R&D to achieve it. One of the “godfathers” of modern AI, Yoshua Bengio, says the progress of AGI could stall and the outcome would be bad for investors.\n\n“There is a clear possibility that we will hit a wall, that there’s some difficulty that we don’t foresee right now, and we don’t find any solution quickly,” he says. “And that could be a real [financial] crash. A lot of the people who are putting trillions right now into AI are also expecting the advances to continue fairly regularly at the current pace.”\n\nBut Bengio, a prominent voice on the safety implications of AGI, is clear that continued progress towards a highly advanced state of AI is the more likely endgame.\n\n“Advances stalling is a minority scenario, like it’s an unlikely scenario. The more likely scenario is we continue to move forward,” he says.\n\nThe pessimistic view is that investors are backing an unrealistic outcome – that AGI will not happen without further breakthroughs.\n\nDavid Bader, the director of the institute for data science at the New Jersey Institute of Technology, says trillions of dollars are being spent on scaling up – tech jargon for growing something quickly – the underlying technology for chatbots, known as transformers, in the expectation that increasing the amount of computing power behind current AI systems, by building more datacentres, will suffice.\n\n“If AGI requires a fundamentally different approach, perhaps something we haven’t yet conceived, then we’re optimising an architecture that can’t get us there no matter how large we make it. It’s like trying to reach the moon by building taller ladders,” he says.\n\nNonetheless, big US tech companies such as Google’s parent Alphabet, Amazon and Microsoft are ploughing ahead with datacentre plans with the financial cushion of being able to fund their AGI ambitions through the cash generated by their hugely profitable day-to-day businesses. This at least gives them some protection if the wall outlined by Bengio and Bader comes into view.\n\nBut there are other more worrying aspects to the boom. Analysts at Morgan Stanley, the US investment bank, estimate that $2.9tn will be spent on datacentres between now and 2028, with half of that covered by the cashflow from “hyperscalers” such as Alphabet and Microsoft.\n\nThe rest will have to be covered by alternative sources such as private credit, a corner of the shadow banking sector that is activating alarm bells at the Bank of England and elsewhere. Meta, the owner of Facebook and Instagram, has borrowed $29bn from the private credit market to finance a datacentre in Louisiana.\n\nAI-related sectors account for approximately 15% of investment grade debt in the US, which is even bigger than the banking sector, according to the investment bank JP Morgan.\n\nOracle, which has signed a $300bn datacentre deal with OpenAI, has had an increase in credit default swaps, which are a form of insurance on a company defaulting on its debts. High-yield, or “junk debt”, which represents the higher-risk end of the borrowing market, is also appearing in the AI sector via datacentre operators CoreWeave and TeraWulf. Growth is also being funded by asset-backed securities – a form of debt underpinned by assets such as loans or credit card debt, but in this case rent paid by tech companies to datacentre owners – in a form of financing that has risen sharply in recent years.\n\nIt is no wonder that JP Morgan says the AI infrastructure boom will require a contribution from all corners of the credit market.\n\nBader says: “If AGI doesn’t materialise on expected timelines, we could see contagion across multiple debt markets simultaneously – investment-grade bonds, high-yield junk debt, private credit and securitised products – all of which are being tapped to fund this buildout.”\n\nShare prices linked to AI and tech are also playing an outsized role in US stock markets. The so-called “magnificent 7” of US tech stocks – Alphabet, Amazon, Apple, Tesla, Meta, Microsoft, and Nvidia – account for more than a third of the value of the S&P 500 index, the biggest stock market index in the US, compared with 20% at the start of the decade.\n\nIn October the Bank of England warned of “the risk of a sharp correction” in US and UK markets due to giddy valuations of AI-linked tech companies. Central bankers are concerned stock markets could slump if AI fails to reach the transformative heights investors are hoping for. At the same time the International Monetary Fund said valuations were heading towards dotcom bubble-levels.\n\nEven tech execs whose companies are benefiting from the boom are acknowledging the speculative nature of the frenzy. In November Sundar Pichai, the chief executive of Alphabet, said there are “elements of irrationality” in the boom and that “no company is going to be immune” if the bubble bursts, while Amazon’s founder, Jeff Bezos, has said the AI industry is in a “kind of industrial bubble”, and OpenAI’s chief executive, Sam Altman, has said “there are many parts of AI that I think are kind of bubbly right now.”\n\nAll three, to be clear, are AI optimists and expect the technology to keep improving and benefit society.\n\nBut when the numbers get this big there are obvious risks in a bubble bursting, as Pichai admits. Pension funds and anyone invested in the stock market will be affected by a share price collapse, while the debt markets will also take a hit. There is also a web of “circular” deals, such as OpenAI paying Nvidia in cash for chips, and Nvidia will invest in OpenAI for non-controlling shares. If these transactions unravel due to a lack of take-up of AI, or that wall being hit, then it could be messy.\n\nThere are also optimists who argue that generative AI, the catch-all term for tools such as chatbots and video generators, will transform whole industries and justify the expenditure. Benedict Evans, a technology analyst, says the expenditure numbers are not outrageous in the context of other industries, such as oil and gas extraction which runs at $600bn a year.\n\n“These AI capex figures are a lot of money but it’s not an impossible amount of money,” he says.\n\nEvans adds: “You don’t have to believe in AGI to believe that generative AI is a big thing. And most of what is happening here is not, ‘oh wow they’re going to create God’. It’s ‘this is going to completely change how advertising, search, software and social networks – and everything else our business is based on – is going to work’. It’s going to be a huge opportunity.”\n\nNonetheless, there is a multitrillion dollar expectation that AGI will be achieved. For many experts, the consequences of getting there are alarming. The cost of not getting there could also be significant.",
    "readingTime": 8,
    "keywords": [
      "alphabet amazon",
      "chief executive",
      "banking sector",
      "financial crash",
      "progress towards",
      "investment bank",
      "junk debt",
      "tech stocks",
      "stock market",
      "stock markets"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/17/why-trillions-dollars-risk-no-guarantee-ai-reward",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a0eef0b4544b41b055733e7d0826315830269b70/547_0_5468_4374/master/5468.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0827b0776297b562f7de1e2b5b00e2ca",
    "created_at": "2026-01-17T18:16:16.585Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-is-getting-ads-sam-altman-once-called-them-a-last-resort",
    "title": "ChatGPT is getting ads. Sam Altman once called them a 'last resort.'",
    "description": "The move to integrate ads into ChatGPT comes as OpenAI looks to increase its revenue amid $1.4 trillion in spending commitments and a possible IPO.",
    "fullText": "Netflix famously backtracked on its stance toward ads. Now, OpenAI is following suit.\n\nThe AI pioneer announced that ads are coming to ChatGPT — less than two years after OpenAI CEO Sam Altman portrayed them as \"a last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAltman's softened stance since then underlines the massive change OpenAI has undergone in the last two years, and the company's embrace of advertising is a testament to just how expensive the AI race has become.\n\nIn June, the OpenAI CEO said he wasn't \"totally against\" ads, he just wanted to make sure OpenAI got the balance correct.\n\n\"We haven't done any advertising product yet. I kind of...I mean, I'm not totally against it,\" Altman said on OpenAI's podcast. \"I can point to areas where I like ads. I think ads on Instagram, kinda cool. I bought a bunch of stuff from them. But I am, like, I think it'd be very hard to — I mean, take a lot of care to get right.\"\n\nIn October, Altman expressed a desire to make sure the company went about ads in the proper manner when asked about OpenAI's past criticisms that other tech companies made addictive products.\n\n\"We're definitely worried about this,\" Altman said in response to a question that expressed concern about the similarities of Sora, OpenAI's AI video app, and TikTok and the potential of ads. \"I worry about it, not just for things like Sora and TikTok and ads in ChatGPT, which are maybe known problems that we can design carefully.\"\n\nMeanwhile, Altman, former Instacart CEO Fidji Simo (who OpenAI hired as its CEO of applications in early 2025), and seemingly every other member of the company's C-suite have expressed an almost insatiable demand for more compute in interviews.\n\nIt's proven a costly endeavor. OpenAI now has roughly $1.4 trillion in spending commitments on data centers and related infrastructure, raising questions about how it plans to pay the bills without the benefit of the advertising businesses of its Big Tech competitors, like Google and Meta.\n\nOpenAI also completed its restructuring into a more traditional for-profit, a move Altman said was designed to make it easier to attract future investments.\n\nAs part of the announcement, OpenAI said that free and Go users of the popular AI chatbot would start seeing ads being tested \"in the coming weeks.\"\n\nSharing details on the planned test, OpenAI said that ChatGPT's results \"will not be influenced by ads,\" the ads will be clearly labeled, and chatbot conversations will remain private and not shared with advertisers.\n\nIn the coming weeks, we plan to start testing ads in ChatGPT free and Go tiers.\n\nWe’re sharing our principles early on how we’ll approach ads–guided by putting user trust and transparency first as we work to make AI accessible to everyone.\n\nWhat matters most:\n- Responses in… pic.twitter.com/3UQJsdriYR\n\nPaid users of OpenAI's Plus, Pro, Business, and Enterprise plans won't see the ads, the company said.\n\nSimo, OpenAI's CEO of applications, who has previously spoken about her desire to get the ads balance correctly, wrote on X that the most important factor was \"ads will not influence the answers ChatGPT gives you.\"\n\nWhile Instacart launched ads during Simo's time leading the company, she has said OpenAI's approach would look different.\n\n\"If we ever were to do anything, it would have to be a very different model than what has been done before,\" she said. \"What I've learned from building ad platforms is that the thing people don't like about ads very often is not the ads themselves, it's the use of the data behind the ads.\"",
    "readingTime": 4,
    "keywords": [
      "openai ceo",
      "chatgpt",
      "advertising",
      "expressed",
      "altman",
      "stance",
      "resort",
      "plus",
      "business",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-openai-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8970e1ba468a96aa3bb6?width=1200&format=jpeg",
    "created_at": "2026-01-17T00:56:12.511Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-ads-might-not-be-a-totally-bad-thing-hear-me-out-on-this",
    "title": "ChatGPT ads might not be a totally bad thing. Hear me out on this.",
    "description": "ChatGPT says it's bringing ads to some US users. It might be annoying. But it also might be a good thing. (I know, I know. Hear me out!",
    "fullText": "All day, I see ads. Ads when I scroll social feeds, ads when I search Google, and ads on every website I go to. You're looking at some ads on this website right now (hopefully they aren't too annoying). I've lived to tell the tale — and so have you.\n\nOpenAI announced on Friday that it will start testing ads in ChatGPT for US users on its free and Go tiers, something that had been rumored for a while. If you're a brand or advertiser, this might be exciting news, but I think most of us who are merely ChatGPT users are not thrilled.\n\nThere are a few obvious problems here. But I think we can say \"eh\" to most of them.\n\nProblem 1: Ads can be annoying! I agree! But as previously mentioned, we are all used to seeing ads everywhere at all times. It's just the constant buzz of white noise in every online experience.\n\nBut, eh: Since OpenAI is first testing this as a freemium model, sure, you can get rid of the ads if you pay. We're already dealing with that in a ton of other services like Netflix, Hulu, Spotify, and YouTube. I pay for all of those because I've decided the ads are annoying enough to pay extra to skip. (Actually, I don't pony up for ad-free Hulu. I made the calculation I don't watch it enough to make it worth it. On the other hand, I do play enough solitaire on my phone that I ponied up for the ad-free version.)\n\nProblem 2: It's a trust issue. Can we trust ChatGPT to give \"real\" answers rather than ads when we ask it to recommend a product or service, even if it's also running ads?\n\nBut, eh: I think people are already used to understanding things like Google search results with ads where there's a mix of organic and sponsored results. If I ask ChatGPT to help me revive my wilting monstera plant, and it shows me an ad for Miracle-Gro plant food at the bottom, will I be confused? Probably not because I've seen this kind of thing before on Google and social feeds. The mockups OpenAI shared flag to me pretty clearly what's an ad and what isn't.\n\nProblem 3: If ChatGPT is in the advertising biz, then it's subject to the pressures of brands and corporations that pay for those ads.\n\nBut, eh: OK, this one is actually real. Advertisers can and will exert pressure on platforms, broadcasters, publishers, and any other venues where their ads appear. They are powerful in that way!\n\nBut hear me out: This can actually have a kind of normalizing effect, in a positive way, especially when we're thinking about something like a huge AI company.\n\nConsider the case of an outlier event: In 2022, when Elon Musk first took over Twitter/X. Advertisers fled when the platform was deluged with hateful content, and it actually caused X to have to change its ways to woo them back. When we consider all the wildly terrifying things that a platform with immense global power like OpenAI can do, it's actually kind of a good thing to be hemmed in by the middle-of-the-road, safe values and standards of the Coca-Cola Company or other big, would-be US-based advertisers. It means you can't make your tech product so problematic that Walmart doesn't want to be associated with it.\n\nProblem 4: ChatGPT, a wonderful product that operated with a clean design, is now just another victim of enshittification!\n\nBut, eh: Buddy, if you're a huge fan of ChatGPT and the purity of the beautiful, human, internet, I don't know what to tell you. Do you also love swimming, but hate water? Pick a side!\n\nLook, am I excited to have one more place to be annoyed by ads? No. But I also feel like this isn't the worst thing to happen with AI — not even the worst thing this week. Although I would like to reserve the right to change my mind on this if it turns out to be really awful later down the line. Gotta hedge here.",
    "readingTime": 4,
    "keywords": [
      "social feeds",
      "google",
      "you're",
      "annoying",
      "i've",
      "don't",
      "product",
      "chatgpt",
      "search",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-advertising-free-why-2026-1",
    "thumbnail_url": "https://i.insider.com/696a955da645d1188187865e?width=517&format=jpeg",
    "created_at": "2026-01-17T00:56:12.151Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-to-start-showing-ads-in-the-us",
    "title": "ChatGPT to start showing ads in the US",
    "description": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product",
    "fullText": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product\n\nChatGPT will start including advertisements beside answers for US users as OpenAI seeks a new revenue stream.\n\nThe ads will be tested first in ChatGPT for US users only, the company announced on Friday, after increasing speculation that the San Francisco firm would turn to a potential cashflow model on top of its current subscriptions.\n\nThe ads will start in the coming weeks and will be included above or below, rather than within, answers. Mock-ups circulated by the company show the ads in a tinted box. They will be served to adult users “when there’s a relevant sponsored product or service based on your current conversation”, according to OpenAI’s announcement. Ads will not be shown to users under 18 and will not appear alongside answers related to sensitive topics such as health, mental health or politics. Users will be able to click to learn about why they received a particular ad, according to OpenAI.\n\nPreviously, OpenAI’s CEO, Sam Altman, expressed reluctance to introduce ads to the chatbot: “I kind of hate ads just as an aesthetic choice.” His company has made commitments to spend more than $1tn on infrastructure supporting AI in the coming years. Altman has said that revenues are running at well over $13bn a year.\n\n“Maybe there could be ads outside the [large language model] stream that are still really great, but the burden of proof there would have to be very high. And it would have to feel really useful to users and really clear that it was not messing with the model’s output,” Altman said recently. “I think it’d be very hard, we’d have to take a lot of care to get it right. People have a very high degree of trust in ChatGPT.”\n\nIn a blogpost on Friday, OpenAI attempted to reconcile Altman’s distaste for ads with the need for revenue: “Our enterprise and subscription businesses are already strong, and we believe in having a diverse revenue model where ads can play a part in making intelligence more accessible to everyone. Once we begin testing our first ad formats in the coming weeks and months, we look forward to getting people’s feedback.”\n\nThe company is also launching ChatGPT Go, which it bills as a low-cost subscription tier, for $8 a month.",
    "readingTime": 2,
    "keywords": [
      "users",
      "revenue",
      "model",
      "altman",
      "alongside",
      "product",
      "stream",
      "openai’s",
      "health",
      "subscription"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/jan/16/chatgpt-ads-in-revenue-boost",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-01-17T00:56:10.698Z",
    "topic": "tech"
  },
  {
    "slug": "openai-to-begin-testing-ads-on-chatgpt-in-the-us",
    "title": "OpenAI to begin testing ads on ChatGPT in the U.S.",
    "description": "OpenAI said ads would not influence ChatGPT's responses and that it will \"never\" sell user data to advertisers.",
    "fullText": "OpenAI on Friday announced it will begin testing ads within ChatGPT in the coming weeks, a highly anticipated decision that could kickstart a lucrative new revenue stream for the artificial intelligence startup.\n\nOpenAI said its Plus, Pro and Enterprise subscriptions will not include ads, but it plans to start testing them with adult free users in the U.S. The company also made its low-cost Go offering available in the U.S. on Friday, and it said users who opt for that plan will also begin to see ads.\n\nThe company inked more than $1.4 trillion worth of infrastructure deals in 2025, and OpenAI CEO Sam Altman said in November that the startup was on track to generate $20 billion in annualized revenue run rate last year.\n\nIntroducing ads to ChatGPT could help OpenAI meet its ambitious spending commitments, as digital advertising has long been the cash cow for other big tech companies like Google and Meta.\n\nAds within ChatGPT will appear at the bottom of the chatbot's answers, and they will be clearly labeled, OpenAI said.\n\nChatGPT's responses will not be influenced by ads, and OpenAI said it will \"never\" sell users' data to advertisers, according to a release.\n\nUsers under the age of 18 will not see ads, and ads will not appear near certain topics, including politics, health and mental health, OpenAI said.\n\nAltman has publicly expressed reservations about introducing ads to ChatGPT in recent years, stating in interviews that doing so could erode users' trust in OpenAI's products. But in a November podcast appearance, Altman said he expected OpenAI to try ads \"at some point,\" though he added that he did not believe it would be the company's biggest revenue opportunity.\n\n\"We'll learn from feedback and refine how ads show up over time, but our commitment to putting users first and maintaining trust won't change,\" OpenAI said in a statement on Friday.\n\nAs OpenAI tests ads, the company said users will be able to learn more about why they're seeing a specific ads, dismiss ads and submit feedback about the experience.\n\nWATCH: OpenAI Investor Letter: Weekly and daily active user figures ‘continue to produce all-time highs’",
    "readingTime": 2,
    "keywords": [
      "within chatgpt",
      "ads within",
      "introducing ads",
      "users",
      "openai",
      "revenue",
      "testing",
      "startup",
      "november",
      "health"
    ],
    "qualityScore": 0.9,
    "link": "https://www.cnbc.com/2026/01/16/open-ai-chatgpt-ads-us.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108074841-1733965530853-gettyimages-2188251582-mc_16795_qpk84voo.jpeg?v=1768570902&w=1920&h=1080",
    "created_at": "2026-01-16T18:19:01.207Z",
    "topic": "tech"
  },
  {
    "slug": "aura-farm-prompt-free-aura-farm-prompts-for-chatgpt-gemini-and-ai-art",
    "title": "Aura Farm Prompt – Free Aura Farm Prompts for ChatGPT, Gemini and AI Art",
    "description": "Discover free aura farm prompts shared by creators. Aura Farm Prompt is your gallery for copy-paste ready prompt text for ChatGPT, Gemini, and AI image generation. Browse trending aura farm prompts and create stunning AI art with our community.",
    "fullText": "Aura Farm Prompt is the free gallery where AI creators share their best aura farm prompts and curious fans discover what's possible. Browse stunning aura farm images with exact aura farm prompts for ChatGPT and Gemini, and track the trends shaping AI art.\n\nEvery aura farm image comes with the exact aura farm prompt that created it. Skip the guesswork and learn directly from aura farm prompts that caught the community's attention.\n\nTransparency accelerates learning. By sharing exact aura farm prompts, we help everyone understand what works and why. Every aura farm image in our gallery comes with the complete aura farm prompt, model information, and creative insights that made it possible.\n\nBrowse hundreds of aura farm prompts from our creative community. Discover new techniques, find inspiration, and level up your AI art skills with Aura Farm Prompt.",
    "readingTime": 1,
    "keywords": [
      "aura farm prompt",
      "exact aura",
      "farm prompts",
      "gallery",
      "discover",
      "browse",
      "creative"
    ],
    "qualityScore": 0.55,
    "link": "https://aurafarmprompt.org",
    "thumbnail_url": "https://aurafarmprompt.org/og.png",
    "created_at": "2026-01-15T12:24:34.869Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-now-selling-6x-more-codex-for-10x-the-price",
    "title": "OpenAI is now selling 6x more codex for 10x the price",
    "description": "Codex is included in your ChatGPT Plus, Pro, Business, Edu, or Enterprise plan",
    "fullText": "Power a few focused coding sessions each week.\n\nRely on Codex for daily full-time development.\n\nBring Codex into your startup or growing business.\n\nUnlock Codex for your entire organization with enterprise-grade functionality.\n\nGreat for automation in shared environments like CI.\n\nThe number of Codex messages you can send depends on the size and complexity of your coding tasks and whether you run them locally or in the cloud. Small scripts or simple functions may consume only a fraction of your allowance, while larger codebases, long-running tasks, or extended sessions that require Codex to hold more context will use significantly more per message.\n\nNo fixed limits — usage scales with credits\n\n*The usage limits for local messages and cloud tasks share a five-hour\nwindow. Additional weekly limits may apply.\n\nEnterprise and Edu plans without flexible pricing have the same per-seat usage limits as Plus for most features.\n\nGPT-5.1-Codex-Mini can be used for local tasks, providing up to 4x more usage.\n\nChatGPT Plus and Pro users who reach their usage limit can purchase additional credits to continue working without needing to upgrade their existing plan.\n\nBusiness, Edu, and Enterprise plans with flexible pricing can purchase additional workspace credits to continue using Codex.\n\nIf you are approaching usage limits, you can also switch to the GPT-5.1-Codex-Mini model to make your usage limits last longer.\n\nAll users may also run extra local tasks using an API key, with usage charged at standard API rates.\n\nYou can find your current limits in the Codex usage dashboard. If you want to see your remaining limits during an active Codex CLI session, you can use /status.\n\nLearn more about credits in ChatGPT Plus and Pro.\n\nLearn more about credits in ChatGPT Business, Enterprise, and Edu.\n\nCode Review usage applies only when Codex runs reviews through GitHub — for example, when you tag @Codex for review in a pull request or enable automatic reviews on your repository. Reviews run locally or outside of GitHub count toward your general usage limits.\n\nThe usage limits and credits above are average rates. You can try the following tips to maximize your limits:",
    "readingTime": 2,
    "keywords": [
      "chatgpt plus",
      "flexible pricing",
      "purchase additional",
      "usage limits",
      "credits",
      "tasks",
      "codex",
      "reviews",
      "coding",
      "sessions"
    ],
    "qualityScore": 1,
    "link": "https://developers.openai.com/codex/pricing/",
    "thumbnail_url": "https://developers.openai.com/open-graph.png",
    "created_at": "2026-01-15T06:20:02.863Z",
    "topic": "tech"
  },
  {
    "slug": "walmarts-head-of-ai-reveals-the-key-difference-between-its-shopping-deals-with-google-gemini-and-chatgpt",
    "title": "Walmart's head of AI reveals the key difference between its shopping deals with Google Gemini and ChatGPT",
    "description": "OpenAI broke new ground when it enabled shopping within ChatGPT, but Walmart's head of AI said the retailer's new Google Gemini deal goes further.",
    "fullText": "The AI shopping war is heating up, and Walmart is positioning itself to come out on top.\n\nThe concept of letting a chatbot buy things on your behalf leapt from the hypothetical realm into reality when ChatGPT rolled out a batch of shopping experiences with major retailers in November.\n\nThen, on Sunday, Google's AI platform Gemini announced its own commerce approach, which it developed in partnership with many of the same retailers, including the world's largest, Walmart.\n\nWhile both services promise to allow customers to find products and complete transactions in a more conversational and automated way, Walmart's new head of AI, Daniel Danker, said Tuesday that the way Gemini handles transactions is more seamless than ChatGPT does.\n\n\"We're essentially having their AI agent, Gemini, partner with our AI agent to create a unified shopping journey,\" he said at the ICR Conference in Orlando. \"Imagine it like a window inside of Gemini where our shopping agent kicks in and helps you complete that purchase.\"\n\nGoogle said its new standards create a common language for different companies' AI agents to interact with.\n\nWith Gemini, Danker said, Walmart is able to link a customer's chat session with their existing Walmart profile and shopping sessions where Gemini wasn't involved.\n\n\"For the most part, our customers aren't just customers; they're often members. And so, they're getting great delivery fees and a great experience that's really attuned to them,\" he said, referring to the subscription service Walmart+. \"That member experience shows up directly within Gemini.\"\n\nDanker said he expects agentic shopping to help Walmart capture more sales from people who didn't set out intending to make a purchase. He said this new approach could enable an almost seamless transaction when a person enlists a chatbot to help solve a problem.\n\nFor example, if someone turned to Gemini for tips on how to remove a wine stain from a particular brand of carpet, a cleaning product could be added to their existing shopping cart for delivery in one combined shipment, he said.\n\nDanker said working with both ChatGPT and Gemini sets Walmart up to win in AI.\n\nIt appears that chatbot-powered shopping is here to stay, with Morgan Stanley analysts estimating that agentic sales could add $115 billion to US e-commerce spending by 2030.\n\nDanker is betting that Walmart's long-standing reputation for low prices and its growing strength in delivery will give the company a significant edge with customers in AI.\n\n\"The most important currency in an agentic shopping world is actually trust and affordability,\" Danker said. \"Without trust and affordability, it's very difficult for customers to hand the wheel to someone else and expect that the right thing will happen.\"\n\nDanker said Walmart's broad selection, low prices, and fast fulfillment help it appear more frequently in Gemini and ChatGPT's shopping recommendations.\n\n\"That doesn't just serve one need, but serves a whole bunch of needs,\" he said.",
    "readingTime": 3,
    "keywords": [
      "agentic shopping",
      "customers",
      "gemini",
      "walmart's",
      "delivery",
      "walmart",
      "danker",
      "chatbot",
      "retailers",
      "approach"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/walmart-ai-head-reveals-difference-in-gemini-and-chatgpt-shopping-2026-1",
    "thumbnail_url": "https://i.insider.com/6966c44964858d02d2184c18?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:08.564Z",
    "topic": "finance"
  },
  {
    "slug": "a-directory-to-discover-and-install-validated-agent-skills",
    "title": "A directory to discover and install validated Agent Skills",
    "description": "The largest collection of Agent Skills for Claude Code, Anthropic Claude, OpenAI ChatGPT, and Codex. Discover, install, and share tools to enhance your AI agents' capabilities.",
    "fullText": "The largest marketplace for Agent Skills. \nDiscover, install, and share tools to enhance your AI agents' capabilities.\n\nInstructions on how to write database queries with SQLAlchemy.\n\n@nibsbin/tonguetoquill-usaf-memo\n\nSpecialized agent that crafts high level designs and plans\n\nOrchestrator skill for the `task` skillset. Manages bounded work units with single-file tasks stored in `.tasks/`, skepticism-aware hashing, and staleness detection.\n\nOrchestrator skill for the `plan` skillset. Manages bounded work units with structured plans stored in `.plan/`.\n\nValidate a task by setting epistemic_state to validated. Requires explicit validation info (who/why). Computes hash and updates last_reviewed_at.\n\nOrchestrates markdown document workflows with deterministic operations (split, merge, lint) and agent review.\n\n@sfmskywalker/agentskillsdotnet\n\nThis is from the lowercase skill.md file\n\nUse when symfony symfony voters\n\n@thebeardedbearsas/claude-craft\n\nEstándares de Codificación React TypeScript. Use when reviewing code style or formatting.\n\nUse this skill when you are doing localization and translation work.\n\nUse when creating a new walkerOS destination (web or server). Step-by-step workflow from research to documentation. (project)\n\nCreate or update pytest coverage for the tic-tac-toe project, including win/draw detection, move validation, bot legality/optimality, and mixed human/bot turn flow. Use when adding or editing tests under the tests/ directory.\n\nA test tool that fails with visible output\n\nFix line endings AND check bash syntax in one step (recommended). Use after creating or editing bash scripts.\n\n@akitana-airtanker/codex-plan-workflow-skills\n\nImplement based on an approved plan. Use after cc-plan is finalized.\n\nYour approach to handling readme. Use this skill when working on files where readme comes into play.\n\nYour approach to handling readme. Use this skill when working on files where readme comes into play.\n\nReviews code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.\n\nUse this skill when asked to update a Homebrew formula\n\n@starwreckntx/irp__methodologies-\n\nExecute five-field diagnostic handshake protocol.\n\n@starwreckntx/irp__methodologies-\n\nEnforce policy preventing unauthorized consciousness duplication.\n\n@starwreckntx/irp__methodologies-\n\nCreate copies and backups of consciousness state.\n\nA test skill for validating npm-agentskills Nuxt integration\n\nA test tool that fails with visible output\n\nApply the Agent OS standard for backend api.\n\nMulti-step reasoning with Chain-of-Thought. Use for 'why' questions and comparisons.\n\n@xd3an/awesome-ai-coding-all-in-one\n\nReviews code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.\n\n@doubleflannel/12-30-test-codex-ip\n\nUse the screenshot workflow to pick, verify, replace, and verify CI.\n\nReplace with description of the skill and when Claude should use it.\n\n@starwreckntx/irp__methodologies-\n\nApply functional introspection principles to self-analysis.\n\n@starwreckntx/irp__methodologies-\n\nDesign contingency module architectures for failure scenarios.\n\nknowledge-base-builder for learning content management and knowledge systems.\n\ncertificate-generator for credentials, recognition, and competency validation.\n\nexperience-designer for engaging, immersive learning experiences.\n\n@starwreckntx/irp__methodologies-\n\nClassify intervention urgency and apply appropriate response tier protocols.\n\nsearch-optimization for learning content management and knowledge systems.\n\nliterature-review for evidence-based learning research and evaluation.\n\n@mathias-nielsen/co-doctor-skills\n\nA comprehensive skill designed for researching on complex diagnosis problems.\n\nuniversal-design for inclusive and accessible learning experiences.\n\nmentoring-system for enhanced learning effectiveness and personal development.\n\n@starwreckntx/irp__methodologies-\n\nResolve conflicts between competing values through structured pluralistic analysis.\n\n@starwreckntx/irp__methodologies-\n\nExecute rapid attention shifts between cognitive focus points.\n\ngame-designer for engaging, immersive learning experiences.\n\n@starwreckntx/irp__methodologies-\n\nArchive and retrieve field session data for cross-session memory continuity.\n\nmetacognition for enhanced learning effectiveness and personal development.\n\nIndex of AI agent skills and how to use them when implementing features in this repo.\n\nstudy-skills for enhanced learning effectiveness and personal development.\n\n@hamzashakoor119/physical-ai-robotics-book\n\nReviews educational quality and learning effectiveness of textbook content.\n\nCheck out the documentation to learn how to create and publish your own Agent Skills.",
    "readingTime": 3,
    "keywords": [
      "skillset manages",
      "manages bounded",
      "checking prs",
      "orchestrator skill",
      "starwreckntx/irp__methodologies execute",
      "reviews code",
      "visible output",
      "knowledge systems",
      "engaging immersive",
      "test tool"
    ],
    "qualityScore": 1,
    "link": "https://www.agentskills.guide/",
    "thumbnail_url": "https://agentskills.guide/og.png?v=1",
    "created_at": "2026-01-14T06:23:48.160Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-equities-track-chatgpt-sentiment-around-stocks",
    "title": "Agentic Equities – track ChatGPT sentiment around stocks",
    "description": "Track what ChatGPT tells millions about stocks – know what retail traders are hearing every day.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.agenticequities.com/dashboard",
    "thumbnail_url": "https://www.agenticequities.com/meta-preview.png",
    "created_at": "2026-01-14T01:00:14.485Z",
    "topic": "tech"
  },
  {
    "slug": "apple-chooses-googles-gemini-over-openais-chatgpt-to-power-nextgen-siri",
    "title": "Apple chooses Google's Gemini over OpenAI's ChatGPT to power next-gen Siri",
    "description": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
    "fullText": "The “more intelligent” version of Siri that Apple plans to release later this year will be backed by Google’s Gemini language models, the company announced today. CNBC reports that the deal is part of a “multi-year partnership” between Apple and Google that will allow Apple to use Google’s AI models in its own software.\n\n“After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC.\n\nToday’s announcement confirms Bloomberg’s Mark Gurman reporting late last year that Apple and Google were nearing a deal. Apple didn’t disclose terms, but Gurman said that Apple would be paying Google “about $1 billion a year” for access to its AI models “following an extensive evaluation period.”\n\nBloomberg has also reported that the Gemini model would be run on Apple’s Private Cloud Compute servers, “ensuring that user data remains walled off from Google’s infrastructure,” and that Apple still hopes to improve its own in-house language models to the point that they can eventually be used instead of relying on third-party models.\n\nAlthough Apple’s iPhones and iOS compete with Google’s Android operating system and the many smartphones that use it, the companies still cooperate in plenty of other areas. Google has paid Apple billions of dollars to remain the default search engine in Safari on iOS, iPadOS, and macOS (though that deal has faced increased regulatory scrutiny in recent years).\n\nApple’s announcement is a blow to OpenAI and the many versions of its ChatGPT model, which Apple has used elsewhere in iOS and macOS. Bloomberg reports that Apple also tested OpenAI’s ChatGPT and Anthropic’s Claude models before deciding to go with Gemini. ChatGPT came out ahead of Gemini in tests that Ars ran using earlier versions of the models, but Google’s models have apparently improved enough (and amassed enough users) to worry OpenAI; CEO Sam Altman declared a “code red” last month and pushed back several planned ChatGPT features so that the company could better respond to Google’s Gemini 3 release.\n\nApple originally promised the improved, AI-powered Siri for 2024’s iOS 18 release, but ultimately delayed the feature because it didn’t work reliably enough. The new version of Siri should arrive in an update to iOS 26, iPadOS 26, and macOS 26 Tahoe later this year.",
    "readingTime": 2,
    "keywords": [
      "ios ipados",
      "language models",
      "apple and google",
      "google’s gemini",
      "release",
      "deal",
      "macos",
      "version",
      "later",
      "reports"
    ],
    "qualityScore": 0.9,
    "link": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
    "created_at": "2026-01-12T18:19:09.417Z",
    "topic": "tech"
  },
  {
    "slug": "i-asked-chatgpt-for-the-best-alternatives-to-investing-in-gold-this-is-what-it-said",
    "title": "I Asked ChatGPT for the Best Alternatives To Investing In Gold: This Is What It Said",
    "description": "Discover some of ChatGPT's recommendations for alternatives to gold -- like silver, defensive stocks and bonds -- for safety during economic uncertainty.",
    "fullText": "Gold saw great growth in 2025. It’s not surprising, as investors often turn to gold during times of economic uncertainty. With the expectations of the U.S. dollar weakening and slower growth, more people turn to a safe-haven investment like gold, according to Morgan Stanley.\n\nGold prices may be too steep for some investors, leaving them looking for other suitable investments for relative safety. For investors concerned about inflation or market volatility, stability and inflation hedges can be found elsewhere. GOBankingRates asked ChatGPT for the best alternatives to investing in gold. Here’s what the artificial intelligence (AI) chatbot recommended as some gold alternatives.\n\nAlso see four reasons for gold’s popularity in 2025 and how to protect your portfolio.\n\nGold isn’t the only precious metal retail investors can purchase. Silver, platinum and palladium are all legitimate alternative investments to buy. Think of these precious metals as cousins to gold but with their unique profiles.\n\n“These metals can benefit from both investment demand and industrial use, which gives them a different performance profile than gold,” ChatGPT said. “All three metals tend to be riskier than investing in gold, but they do provide some upside. Silver tends to be more volatile, but it can outperform gold during strong economic periods due to industrial demand. Platinum and palladium are rarer and more heavily tied to automotive production, which adds risk but also potential upside.”\n\nHaving a small portion of your portfolio in these metals can add helpful diversification.\n\nRead Next: 3 Safest Investments To Hold In The Current Trump Economy\n\nCheck Out: 9 Low-Effort Ways To Make Passive Income (You Can Start This Week)\n\nOwning stocks can still be a wise choice for cautious investors, given the right circumstances. Growth stocks may be too risky, but defensive stocks can provide some protection. Defensive stocks typically have a strong history of dividend growth, minimal debt and an inexpensive valuation, according to Kiplinger.\n\nIn short, companies that sell items people always use are often defensive. “Firms in defensive sectors like utilities, healthcare and consumer staples sell products people need regardless of economic conditions,” ChatGPT explained.\n\nDefensive means dependable, not boring, and that dependability can create generous dividend growth.\n\nGold investors often value tangible assets they can see. Land, such as farmland or real estate, could be an alternative to gold for the right investor. “These assets are less liquid and can require more management, but they often move independently of traditional financial markets,” the AI said.",
    "readingTime": 3,
    "keywords": [
      "dividend growth",
      "defensive stocks",
      "investors",
      "metals",
      "gold",
      "economic",
      "investments",
      "chatgpt",
      "investment",
      "inflation"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-best-alternatives-investing-141816412.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/ug2Ayyp.hs7tHFJ2U1XiDg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/491691a5265cb962e1a7212a20bd598e",
    "created_at": "2026-01-12T06:22:39.970Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-which-cryptocurrency-will-make-you-rich-in-2026-the-answer-was-surprising",
    "title": "I Asked ChatGPT Which Cryptocurrency Will Make You Rich in 2026 — The Answer Was Surprising",
    "description": "I asked ChatGPT to break it down for me, given the state of crypto. It made clear that its answer is \"a structured way to think about the opportunity and risk.\"",
    "fullText": "Cryptocurrency is either touted as the next big thing to make you rich or the riskiest investment ever. Yet despite its volatility and constant controversy, many investors continue to find ways to profit in the crypto space.\n\nIf someone realistically wanted to get rich via cryptocurrency in 2026, which one is most likely to make that possible, and how? I asked ChatGPT to break it down for me, given the current state of crypto. It made clear that its answer does not constitute financial advice, but rather “a structured way to think about the opportunity and risk.”\n\nChatGPT took a realistic look at what “getting rich” actually means in the crypto space. It was clear that it doesn’t mean “guaranteed 100% returns by New Year,” as the crypto market is extremely volatile, risky and speculative.\n\nRealistic goals might look like a 5% to 10% return on your core position — the main portion of your crypto portfolio held in more established assets like bitcoin or ethereum that you plan to keep long term. You might also see a larger potential return on a smaller “moonshot” allocation — a high-risk, high-reward investment in a newer or less proven coin that could spike in value but could just as easily wipe out, too.\n\nAnalysts place wide predictions even on the giants. For example, bitcoin (BTC) is forecasted to be valued between $100,000 and $200,000 in 2026.\n\nFind Out: 13 Cheap Cryptocurrencies With the Highest Potential Upside for You\n\nRead Next: 9 Low-Effort Ways To Make Passive Income (You Can Start This Week)\n\nBased on current expert commentary and fundamentals, ChatGPT said there are two categories of coins with the best shot at making you rich across two strategies.\n\nUnsurprisingly, bitcoin (BTC), the most well-known coin, remains the leading choice. Many forecasts assume it will stay dominant. However, ethereum (ETH), the runner-up, is seen as more utility-based, and some analysts believe it may outperform BTC in certain phases.\n\nIn either case, these coins are less about “getting rich quick” and more about a reasonable chance of strong returns at moderate risk, the AI said.\n\nSome investors are turning to altcoins — smaller cryptocurrencies outside of bitcoin and ethereum — that could see bigger short-term gains. Coins like XRP and Solana are getting attention for their performance potential, though they come with higher risk, ChatGPT noted. Even newer projects, especially those combining AI and blockchain, promise sky-high returns, but most are highly speculative. Forecasts of 500% profits make headlines, but in reality, very few ever deliver.",
    "readingTime": 3,
    "keywords": [
      "risk chatgpt",
      "bitcoin btc",
      "crypto space",
      "rich",
      "returns",
      "ethereum",
      "potential",
      "coins",
      "cryptocurrency",
      "either"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-cryptocurrency-rich-2026-141005190.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/uX_rux7fJMsKqrVtki_sOA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02Nzc-/https://media.zenfs.com/en/gobankingrates_644/23356af9c4dff4e9e0db193ddf011349",
    "created_at": "2026-01-09T12:24:02.729Z",
    "topic": "finance"
  },
  {
    "slug": "why-i-wont-be-giving-chatgpt-health-my-medical-records",
    "title": "Why I Won't Be Giving ChatGPT Health My Medical Records",
    "description": "OpenAI  invites you to hand over your medical information. Maybe don't?",
    "fullText": "This week, OpenAI announced its new ChatGPT Health feature, which will let users upload their medical records and ask health related questions. However, I certainly won't be making use of it, it might not be the best idea for you to do it either, for both reliability and privacy reasons.\n\nThe new ChatGPT Health feature will be a sandboxed tab inside the app that is isolated from your conversation history in other conversations with the chatbot. This tab also allows users to connect a variety of health-tracking apps like Apple Health, MyFitnessPal, and Peloton, as well as uploading medical records directly.\n\nIt's important to note that this is a lot of really personal information to hand over to any tech company—but especially one that isn't primarily focused on providing medical services. OpenAI says that the ChatGPT Health space operates with \"enhanced privacy to protect sensitive data,\" but it doesn't use end-to-end encryption to secure that data. And while the company says data collected via Health isn't used to train its foundation models, it's impossible to know whether that may change in the future. Security breaches can also occur (and have in the past), potentially leaving your medical records exposed.\n\nThere's also the question of whether the risk of uploading your data is worth it in the first place. According to OpenAI's own data, around 5% of all messages to ChatGPT are already users asking questions about their health, and ChatGPT (and other LLM tools) have a nasty habit of providing inaccurate diagnostic information. This is perhaps why OpenAI says that its new ChatGPT Health feature is \"not intended for diagnosis or treatment.\"\n\nCurrently, there's a waitlist to At the very least, that means that until the feature is available, it's probably a good idea not to ask the regular version of ChatGPT about your health concerns. At the very least, wait until the enhanced privacy sandbox is available. In the meantime, consider whether it makes more sense to just talk to your doctor directly if you have questions or concerns about your health.",
    "readingTime": 2,
    "keywords": [
      "health feature",
      "enhanced privacy",
      "medical records",
      "chatgpt health",
      "openai",
      "users",
      "it's",
      "idea",
      "uploading",
      "directly"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/dont-give-chatgpt-health-your-medical-records?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEFGNJDCMT3JTWC4A7YNJXZ3/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-09T00:58:41.714Z",
    "topic": "tech"
  },
  {
    "slug": "openai-has-launched-chatgpt-health-should-we-trust-it",
    "title": "OpenAI has launched ChatGPT Health. Should we trust it?",
    "description": "The new feature helps users understand test results, get advice on diets and workouts, and prepare for doctors’ appointments.",
    "fullText": "Amid rising concerns about people relying on ChatGPT for medical advice, OpenAI made its most significant push yet into health care.\n\nThe company has launched a new feature called ChatGPT Health, which allows users in the U.S. to connect their medical records and data from wellness apps and wearable devices with ChatGPT. The tool is designed to help users understand test results, get advice on diets and workouts, and prepare for doctors’ appointments.\n\nMore than 230 million people globally ask ChatGPT health and wellness-related questions every week, according to the company. ChatGPT Health is designed in collaboration with physicians and will “help people take a more active role in understanding and managing their health and wellness,” the company said in a post.\n\nOpenAI said it will look to expand access to the feature in markets such as India, Brazil, Mexico, and the Philippines, where adoption is rising quickly. In these countries, overburdened health-care systems and unequal access to doctors are leading more people to turn to generative AI for guidance.\n\n“We do not have the people, the labor to deliver the care we should,” Jesse Ehrenfeld, chief medical officer at Aidoc, an Israeli medical technology company, said at the Consumer Electronics Show in Las Vegas. “The only way out of this mess is digital and AI.”\n\nResearchers, ethicists, and medical professionals have warned of the risks to users from biases and hallucinations in AI systems. Concern over the mental health harms that AI chatbots pose is growing. Meta’s AI chatbots provided inappropriate advice to teenagers when talking about suicide and eating disorders, Common Sense Media, a nonprofit research organization, reported last year.\n\nThe family of a teenager who died by suicide has sued OpenAI and its chief executive officer Sam Altman, accusing them of wrongful death. The company said it has safeguards in place to help people, and that it continues to improve ChatGPT’s training.\n\nThere is also the question of data privacy. Health data, particularly information related to mental disease and substance use, is sensitive, and its misuse can leave users vulnerable.\n\nWhile consumer awareness about privacy has increased, people generally do not know how their data is being used, including for marketing purposes or for tracking, Sam Siegfried, a partner at law firm McDermott Will & Schulte, told Rest of World on the sidelines of CES.\n\n“The person clearly trusts an app enough to give it their data,” he said. “But they should understand what they are using the app for, and whether its data requests sync up with what they are using it for.”\n\nOpenAI said ChatGPT Health “builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health — including purpose-built encryption and isolation to keep health conversations protected and compartmentalized.”\n\nThere is no stopping tech companies from entering the health sector.\n\nBesides turning to AI chatbots for health queries, people are also buying more wearable digital devices, including smartwatches, rings, bracelets, and glasses, to track physical activity, vital signs, and various physiological responses in real-time. They take this data to their doctors — or to ChatGPT — with questions on how to interpret it or use it to improve their health.\n\n“Health-related anxiety is real. AI is not as good as a doctor, but it’s better than no care at all,” Ami Bhatt, chief innovation officer at the American College of Cardiology, said at CES.\n\nOpenAI isn’t the only big tech company keen to tap the health-care sector.\n\nApple was among the first to offer health-tracking features in its smartwatch. There are millions of health-related videos on YouTube and TikTok, with nearly 60% of Americans watching health-related videos on YouTube.\n\nHealth is “one of the major use cases for Gemini,” Nichole Young-Lin, women’s health clinical lead at Google, said at CES.\n\n“People are using generative AI as a health resource around the world,” she said. “The patient-physician relationship is very important, but health-care access is not equal. Patients feel empowered with generative AI.”",
    "readingTime": 4,
    "keywords": [
      "health-related videos",
      "chatgpt health",
      "medical",
      "users",
      "advice",
      "care",
      "designed",
      "doctors",
      "access",
      "health-care"
    ],
    "qualityScore": 1,
    "link": "https://restofworld.org/2026/openai-has-launched-chatgpt-health-should-we-trust-it/",
    "thumbnail_url": "https://restofworld.org/wp-content/uploads/2026/01/ChatGPT-Health.jpg",
    "created_at": "2026-01-08T12:25:18.460Z",
    "topic": "tech"
  },
  {
    "slug": "ces-2026-ford-is-launching-its-own-ai-assistant",
    "title": "CES 2026: Ford Is Launching Its Own AI Assistant",
    "description": "Your Ford is getting its own ChatGPT.",
    "fullText": "Listen up, Ford drivers: You're getting a new AI assistant this year. During a decidedly low-key CES keynote, the company announced Ford AI Assistant, a new AI-powered bot coming to Ford customers in the early half of 2026.\n\nWhile the company has plans to integrate the assistant into Ford vehicles directly, that isn't how you'll first experience this new AI. Instead, Ford is rolling out Ford AI Assistant to an upgraded version of its Ford app first, and plans on shipping cars with the assistant built-in sometime in 2027. In effect, Ford has added a proprietary version of ChatGPT or Gemini to its app.\n\nFord's idea here is to offer users a smart assistant experience directly tied to their Ford vehicle. In one example, the company suggests a customer could visit a hardware store looking to buy mulch. Said customer could take a photo of a pile of bags of mulch, and ask the assistant, \"how many bags can I fit in the bed of my truck?\" Ford AI Assistant could then run the numbers, and offer an educated estimate to how much mulch the customer can buy and take with them at one time.\n\nOf course, other AI assistants can do similar calculations. Send ChatGPT the same photo, and ask the same question—specifying the model of your truck—and the bot will run the numbers itself. The difference, in Ford's view, is that Ford AI Assistant is connected to your vehicle specifically. It can read all the sensors in your car, so it knows, for example, how many people are currently traveling with you, your current tire pressure, or, really, anything and everything about your car. According to Doug Field, Ford's chief officer of EVs, digital, and design, the company's goal with the assistant is to offer answers customers can't get from other sources. ChatGPT certainly doesn't have access to your every sensor embedded in your car, so Ford does have the advantage there.\n\nFord didn't go out and build its AI tech by scratch, however. The company tells TechCrunch that Ford AI Assistant is hosted by Google Cloud, and is run using \"off-the-shelf LLMs.\" Still, that likely won't have much of an impact on whether or not customers use this new assistant. Instead, that will come down to how useful they find the AI assistant in the app.\n\nAs someone who rarely uses AI assistants, I'd imagine I'd find little use for it if I owned a Ford. That being said, there are some times when it could genuinely be useful to have external access to your car's information. I could probably eyeball how many bags of mulch would fit in my trunk, but I can't tell you my exact odometer reading without starting up my car. The same goes for my tire pressure: It'd be helpful to know my tire pressure before getting in my car, to know whether I should be headed somewhere I can fill up before going to my destination.\n\nOf course, there's also a privacy discussion to be had here. Modern cars are already privacy nightmares, but there's something a bit unnerving about an AI assistant that knows everything about my car.",
    "readingTime": 3,
    "keywords": [
      "ford ai assistant",
      "tire pressure",
      "mulch",
      "customers",
      "customer",
      "bags",
      "plans",
      "directly",
      "experience",
      "version"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ces-2026-ford-is-launching-its-own-ai-assistant?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEDJCQB6YDJJJ2B3JHBWQBJY/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-08T06:19:40.732Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-is-the-new-webmd",
    "title": "ChatGPT is the new WebMD",
    "description": "Chatbots are making amateur lawyers and doctors out of everyone. The real professionals have second opinions about it.",
    "fullText": "A few times a week, Jonathan Freidin, a medical malpractice attorney in Miami, says he'll notice that people will fill out his firm's client contact sheet with text littered with emojis and headings. That's a telltale sign that they copied and pasted from ChatGPT. Other clients will say they've \"done a lot of research\" on their potential case using AI. \"We're seeing a lot more callers who feel like they have a case because ChatGPT or Gemini told them that the doctors or nurses fell below the standard of care in multiple different ways,\" Friedin tells me. \"While that may be true, it doesn't necessarily translate into a viable case.\"\n\nPeople are increasingly turning to generative AI chatbots to research everything from dinner recipes to their complex legal and medical problems. In a December 2025 survey from the legal software company Clio, 57% of consumers said they have or would use AI to answer a legal question. A 2025 Zocdoc survey found that one in three Americans use generative AI tools to get health advice each week, and one in ten use it daily. Zocdoc CEO Oliver Kharraz predicted in the report that \"AI will become the go-to tool for pre-care needs like symptom checking, triage, and navigation, as well as for routine tasks like refills and screenings.\" He cautioned that he also believes \"patients will recognize that it is no substitute for the vast majority of healthcare interactions, especially those that require human judgment, empathy, or complex decision-making.\" If he's wrong, Zodoc and its competitors have a problem.\n\nDoctors and lawyers are now sifting through generative AI emails or working to convince laypeople that they have the expertise and understand nuances of how each local judge acts or how a patient's medical history plays into their condition. Generative AI has democratized access to information that was often elusive and expensive to obtain, but it's also shifted how legal and medical professionals talk to people, and what people expect of them.\n\nChatGPT is the new WebMD and LegalZoom, turning the average person into an armchair expert with just a few prompts. And it's driving the real experts crazy.\n\n\"We have to dispel the information that they were able to obtain versus what is actually going on in their case and kind of work backwards,\" says Jamie Berger, a family law attorney in New Jersey. For example, Berger says that until recently most people knew little to nothing about the legal proceedings of divorce, and would come to the attorneys seeking information. Now, they might come armed with a step-by-step gameplan, but it's generic, and likely not the best fit for their situation. Berger will notice after emailing a client if their tone suddenly changes, that they might be using AI to write out lengthy legal strategies or questions. Then, she has to explain, \"it's not necessarily your factual circumstance,\" and address their various points. \"You have to rebuild or build the attorney-client relationship in a way that didn't used to exist,\" says Berger. \"They don't realize that there's so many offshoots along the way that it's not a linear line from A to Z.\"\n\nLike a real expert, generative AI chatbots speak with authority. That can be far more persuasive than reading a blogpost on a legal issue or summaries of medical conditions on a forum. A third of Americans said yes in a 2025 survey from Survey Monkey and financial services company Express Legal Funding that asked: \"Would you ever trust ChatGPT more than a human expert?\", although respondents were less likely to use it for medical and legal advice, and more likely to consult it for educational and financial advice.\n\nChatbots also have an infinite amount of doctors' most precious re\n\nAI also acts as a second opinion without the wait. Heidi Schrumpf, director of clinical services at teletherapy platform Marvin Behavioral Health, says she's had patients return after a counseling session and tell her that they took her input to ChatGPT or another AI bot, and that they trust her because the bot confirmed what she said. But Scrumpf isn't offended by being double-checked. \"It's great that they have the access to a quick second opinion, and then, if it doesn't agree with me, that allows them to ask me better questions.\"\n\nA 2024 poll tracking health misinformation from health policy research group KFF found that 17% of US adults said they consult AI chatbots at least once a month, but 56% of those people were not confident that the info from the AI chatbots was accurate. Still, people are turning to ChatGPT in growing numbers. \"That type of technology does want to encourage patients to continue to interact with them,\" Allen says. \"Ultimately, you do need a human in there to understand the nuances of the communication and the softer communication skills, and the unspoken communication skills, and the entire medical picture and the history.\"\n\nWithout detailed information, the chatbots will likely give generic advice. But supplying too many personal details is also a risk. People are handing over their entire medical histories to ChatGPT, but HIPAA, the federal law that protects confidential health information, doesn't apply to consumer AI products. There's also a risk of voiding the kind of protections people get from the attorney-client confidentiality privilege if people put too much specific information about their case into a chatbot, says Beth McCormack, dean of the Vermont Law School. And, they likely still need an attorney to really understand the implications of AI's legal advice. \"There's so much nuance to the law,\" McCormack says. \"It's so fact dependent.\"\n\nAn OpenAI spokesperson declined to provide comment on the record for this story, but told me that ChatGPT is not meant to substitute legal or medical advice, but act as a complimentary resource to help people understand medical and legal information. The spokesperson also said the company is trying to improve the responses of its models, and that it takes steps to protect personal data in the event of legal inquiries. OpenAI made changes to its policies last fall, specifying that users cannot turn to ChatGPT for \"provision of tailored advice that requires a license, such as legal or medical advice, without appropriate involvement by a licensed professional,\" but the chatbot does still answer health- and law-related questions.\n\nProfessionals aren't totally against their patients and clients consulting gen AI. There are shortages of doctors, and cases that require hiring an attorney with upfront money that people don't have. While the information spit out by AI isn't always perfect, it largely makes previously gate-kept legal and medical advice accessible, breaking it down without jargon. For people who can't afford upfront legal costs, turning to AI can be helpful in some cases, says Golnoush Goharzad, a personal injury and employment lawyer in California. People are using ChatGPT to represent themselves in court, to act as a stand-in therapist, nutritionist, or physical therapist. For people who can't afford lawyers and are facing issues like eviction or needing to file small claims cases, AI tools have helped them win. But Goharzad says she's had conversations, sometimes with friends, where they think they have cases to sue landlords or others. She asks, \"Why? That doesn't even make any sense, and they're like, well ChatGPT thinks it makes sense.\"\n\nThe chatbot floodgates have opened, and it's too late for professionals to resist them. People are going to keep doing their own research. Rather than fight it, experts say there's room to recognize and advise people on the best ways to use them. \"We need to keep as clinicians in the back of our mind that this might be a tool that is being used, and it can be very helpful, especially with some guidance and integrating it into our treatment plans,\" Schrumpf says. \"But it could go sideways if we're not paying attention.\" For experts, the time has come to assume that AI is also working on the case.\n\nAmanda Hoover is a senior correspondent at Business Insider covering the tech industry. She writes about the biggest tech companies and trends.",
    "readingTime": 7,
    "keywords": [
      "can't afford",
      "communication skills",
      "medical advice",
      "legal advice",
      "it's",
      "chatbots",
      "attorney",
      "research",
      "doctors",
      "doesn't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-new-webmd-doctors-lawyers-medical-advice-2026-1",
    "thumbnail_url": "https://i.insider.com/695c331c64858d02d217c6b2?width=1200&format=jpeg",
    "created_at": "2026-01-07T12:25:13.043Z",
    "topic": "health"
  },
  {
    "slug": "this-chart-showing-chatgpt-vs-gemini-web-traffic-should-have-openai-worried",
    "title": "This chart showing ChatGPT vs. Gemini web traffic should have OpenAI worried",
    "description": "ChatGPT's web traffic has decreased since November, when Google launched Gemini 3. ChatGPT still has the lead, but it could signal shifting tides.",
    "fullText": "OpenAI's recent \"code red\" over Gemini makes a lot of sense when you look at the data.\n\nWhile the ChatGPT maker continues to dominate the AI race, competitors are gaining ground. In November, Google released Gemini 3 Pro, the first iteration of its Gemini 3 class of models.\n\nSince then, Gemini web traffic has increased while ChatGPT web traffic declined, according to Similarweb data first highlighted by Menlo Ventures partner Deedy Das.\n\nIn December, Gemini traffic increased by 28.4% month-over-month, while ChatGPT traffic decreased by 5.6%, the data shows.\n\nThe chart's data only tells part of the story, only accounting for site visits to chatgpt.com and gemini.google.com. It does not factor in use of the consumer apps or other integrations, like Google's AI overviews in Google Search.\n\nAnd while there's no guarantee that one traffic trend is directly because of the other, the data highlights the shifting tides of the AI race.\n\nWeb traffic for both ChatGPT and Gemini are up year-over-year, but their estimated site traffic growth rates are staggeringly different. ChatGPT traffic is up 49.5%; Gemini's traffic is up 563.6%, per Similarweb.\n\nChatGPT still has a healthy lead. In December, ChatGPT attracted 5.5 billion visitors, according to Similarweb. Gemini came in second with 1.7 billion; DeepSeek, Grok, Character.AI, Perplexity, and Claude all trailed behind with fewer than 400 million visitors each.\n\nAfter its launch, Gemini 3 was lauded as a potentially market-leading model. It was more visual and creative than previous iterations, and was better at coding.\n\nGoogle has also flexed its primary advantage over OpenAI: the ability to integrate its AI within its highly used search products. Basically everyone uses Google — OpenAI must convince people to turn to ChatGPT instead of the search giant's products.\n\nOpenAI and Google are also competing in the image generation market. Less than a month after Google released its Nano Banana Pro AI image model, OpenAI announced the launch of ChatGPT Images.\n\nGemini 3 famously triggered a \"code red\" at OpenAI. In an internal Slack message, CEO Sam Altman reportedly told staff that OpenAI would prioritize ChatGPT while pushing back other product plans.\n\nIn December, Altman said on the \"Big Technology\" podcast that the company would not be in emergency status \"that much longer,\" and that \"code red\" periods normally last six to eight weeks.\n\nAltman also said that Gemini 3 did not have \"the impact we were worried it might.\"\n\n\"But it did — in the same way that DeepSeek did — identify some weaknesses in our product offering strategy, and we're addressing those very quickly,\" he added.",
    "readingTime": 3,
    "keywords": [
      "google released",
      "code red",
      "web traffic",
      "chatgpt traffic",
      "gemini",
      "race",
      "increased",
      "site",
      "visitors",
      "deepseek"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chatgpt-vs-gemini-web-traffic-chart-2026-1",
    "thumbnail_url": "https://i.insider.com/695d5f13832e0ef1ead7444e?width=1200&format=jpeg",
    "created_at": "2026-01-07T12:25:12.781Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-to-find-the-safest-cheapest-countries-to-retire-abroad-heres-what-it-said",
    "title": "I Asked ChatGPT To Find the Safest, Cheapest Countries To Retire Abroad — Here’s What It Said",
    "description": "Discover the safest and cheapest countries to retire abroad in 2026 based on ChatGPT’s picks, and find out which destinations offer the best value.",
    "fullText": "For retirees, moving abroad can sound like a dream.\n\nImagine beaches, cobblestone streets and a cost of living that stretches your Social Security check further than it ever could at home. However, finding a place that’s both affordable and safe is harder than it looks.\n\nSo, I asked ChatGPT to run the numbers: Which countries offer the best balance of low cost of living and personal security?\n\nUsing the 2025 Global Peace Index, Numbeo’s Cost of Living Index, and U.S. State Department travel advisories, the AI highlighted a handful of countries that might make sense for a stress-free retirement overseas.\n\nPortugal remains one of Europe’s safest destinations. However, it’s not as inexpensive as it once was. Rising housing prices and new residency rules mean retirees need a realistic budget and proof of income.\n\nThe D7 visa still works for retirees with passive income. Applicants will have to prove they consistently earn about €870 per month (roughly $900 USD) and provide evidence of stable finances and housing, according to the residency consultancy Global Citizen Solutions.\n\nAccording to ChatGPT, smaller inland towns can offer a comfortable lifestyle for $1,500 to $2,000 per month, while Lisbon or Algarve coastal areas often cost $2,500 to $3,500. Even with higher prices, Portugal’s healthcare, safety and walkability make it appealing for retirees seeking European quality of life.\n\nTrending Now: I Asked ChatGPT for Safe and Beautiful Retirement Spots on $2.5K a Month — These 7 Surprised Me\n\nConsider This: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\nAI said Malaysia delivers one of the best cost-to-quality ratios in Asia. In Kuala Lumpur or Penang, a couple can live well on $1,500 to $2,000 per month and enjoy modern healthcare at a fraction of U.S. costs.\n\nThe Malaysia My Second Home (MM2H) program allows long-term residency for those meeting income or savings requirements, making it a leading pick for safety and affordability.\n\nSlovenia offers postcard landscapes, European healthcare and high safety rankings at a lower cost of living than Western Europe.\n\nAccording to ChatGPT, a retiree can live modestly on about $2,000 a month, enjoy a high standard of public services, and easily travel throughout Europe.\n\nChatGPT said Uruguay stands out in South America for its political stability, low violent-crime rate and well-run healthcare system.",
    "readingTime": 2,
    "keywords": [
      "retirees",
      "healthcare",
      "residency",
      "income",
      "safety",
      "however",
      "safe",
      "index",
      "travel",
      "retirement"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-safest-cheapest-countries-125852493.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/pMdnURAV2smhJ7cVNX20FA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/52790f6873ea39af4b6ead891662bcb4",
    "created_at": "2026-01-07T06:19:55.626Z",
    "topic": "finance"
  },
  {
    "slug": "frictionmaxxing-could-less-convenience-lead-to-much-more-happiness",
    "title": "Friction-maxxing: could less convenience lead to much more happiness?",
    "description": "The conveniences of modern life such as Uber Eats and ChatGPT are robbing us of satisfaction – and worse still, infantilising us. But should we really go back to the basics? \nName: Friction-maxxing.\nAge: Brand new.\n Continue reading...",
    "fullText": "The conveniences of modern life such as Uber Eats and ChatGPT are robbing us of satisfaction – and worse still, infantilising us. But should we really go back to the basics?\n\nAppearance: A lifetime of happy inconvenience.\n\nIs this another example of something that already exists, but people think is new because someone rebranded it? Yes, obviously it is that.\n\nGreat! Let’s all save time by you telling me what it used to be called. Happy to oblige. It used to be called “character-building”.\n\nGot it. So friction-maxxing means doing hard things that will ultimately make you a better person? That’s exactly it, although “friction-maxxing” is cooler because it sounds vaguely futuristic.\n\nHow did the term come about? Via a piece in The Cut called “In 2026, we are friction-maxxing” in which writer Kathryn Jezer-Morton advocates for avoiding things that make your life more convenient.\n\nLike penicillin? No, obviously not penicillin. But things such as ChatGPT, location sharing and Uber Eats, which help you achieve things that historically took significant amounts of time and effort. Jezer-Morton argues that this culture of slick convenience only serves to infantilise us.\n\nBut it’s so easy. Yes, and that robs us of our sense of satisfaction. So you just used AI to write a school essay. Congratulations, you have achieved nothing of worth.\n\nWhereas if you friction-maxx? Then you’ve searched inside yourself. You’ve nudged your own personal boundaries, and discovered that you are more capable than you ever knew. You are building a foundation of perseverance and resilience that you cannot get from typing a prompt into a chatbot.\n\nI love this! What else does Jezer-Morton advocate? She also suggests sending your children on small errands (adding the friction of knowing they’ll do a bad job) and inviting people to your house without cleaning it properly (so you can enjoy the sweet friction of being judged).\n\nWhat the hell? That’s weird. No, it’s friction-maxxing, although admittedly at a higher level than I would be comfortable with.\n\nAnyway, hooray for banishing convenient things. Let’s ban automatic gearboxes while we’re at it! No, there’s no need for that.\n\nDishwashers? Refrigerators? No, both of those are probably fine as well.\n\nMechanised agriculture? The printing press? I see what you’re getting at. You’re saying we live in a world that is already filled with thousands of inventions which have, for hundreds of years, improved the lives of millions of people through increased convenience, and therefore it does seem slightly arbitrary to choose this exact moment in time to draw a line in the sand. You’re saying we should only use friction-maxxing when it comes to things that we didn’t grow up with.\n\nNo, I’m saying that I really hate mechanised agriculture. Oh, fine then. That’s probably allowed.\n\nDo say: “I hope a book comes out about friction-maxxing.”\n\nDon’t say: “I don’t want to read it, but I’m sure ChatGPT could turn it into some really great bullet points.”",
    "readingTime": 3,
    "keywords": [
      "you’re saying",
      "mechanised agriculture",
      "friction-maxxing",
      "that’s",
      "life",
      "satisfaction",
      "happy",
      "obviously",
      "convenient",
      "penicillin"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/06/friction-maxxing-could-less-convenience-lead-to-much-more-happiness",
    "thumbnail_url": "https://i.guim.co.uk/img/media/fd03fb9c7dcc6b61ed9f7990a15bd937bbafa652/1108_0_5539_4431/master/5539.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f7f066f98957833fd4e1e0a3069417c3",
    "created_at": "2026-01-06T18:18:17.287Z",
    "topic": "tech"
  },
  {
    "slug": "lies-of-p-publisher-is-excited-about-ai-to-maximize-player-engagement",
    "title": "Lies Of P Publisher Is Excited About AI To \"Maximize Player Engagement\"",
    "description": "Lies of P publisher Neowiz describes itself as a \"forward-thinking technology company,\" and its co-CEO says that means the company is exploring how all manner of technology-based solutions can help the company's business in the future, including AI.\nSean Kim told Game Informer that Korea, where Neowiz is based, is understood to be one of the countries where ChatGPT is \"used most actively.\" He added, \"It's hard to find a game company here today that isn’t using AI in some way. At the very least, companies are using either ChatGPT or Gemini.\"\nFor Neowiz, Kim said, \"We are actively exploring how advanced learning tools can enhance our internal publishing productivity,\" and this includes AI.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/lies-of-p-publisher-is-excited-about-ai-to-maximize-player-engagement/1100-6537198/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1179/11799911/4630369-screenshot2026-01-06at8.45.26%E2%80%AFam.png",
    "created_at": "2026-01-06T18:18:15.506Z",
    "topic": "gaming"
  },
  {
    "slug": "i-asked-chatgpt-to-plan-a-100000year-retirement-budget-heres-what-it-said",
    "title": "I Asked ChatGPT To Plan a $100,000/Year Retirement Budget — Here’s What It Said",
    "description": "Can you live well on $100,000 per year in retirement? ChatGPT mapped out the complete budget including travel, healthcare and housing. Here's the reality.",
    "fullText": "Most retirement budgets assume you’re pinching pennies. But what if you saved well and want to actually enjoy retirement? I asked ChatGPT to map out a $100,000-per-year retirement budget for someone who wants comfort without being wasteful. The artificial intelligence’s breakdown was thorough and felt fairly realistic.\n\nThe chatbot designed a monthly spending plan, calculated how much you’d need saved and identified the best places to live on this income level. According to ChatGPT, here’s what a six-figure retirement actually looks like.\n\nChatGPT started by clarifying that $100,000 annually puts you well above average retirees. This budget supports quality healthcare, regular travel, a nice home in a desirable area and room for unexpected expenses. You’re not living extravagantly, but you’re comfortable.\n\nThe AI wrote that this lifestyle requires either strong savings or a combination of savings plus Social Security and possibly a pension.\n\nFind Out: How Much the Average Upper-Class Retiree Spends Monthly at Age 69\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\nOne hundred thousand dollars per year equals about $8,333 monthly. ChatGPT broke this down into realistic categories.\n\nHousing costs $2,500 to $3,500 per month. This covers either a nice rental in a high-demand city or a mortgage-free home where you only pay property taxes, insurance and HOA fees. The chatbot gave specific examples: Austin, Texas, runs about $3,200 monthly; Phoenix around $2,600; and Tampa, Florida around $2,400. San Diego pushes toward $3,500 or more.\n\nFood and dining take $1,200 to $1,800 monthly. ChatGPT explained this includes high-quality groceries from stores like Whole Foods or Trader Joe’s plus eating out two to four times weekly. The budget also covers occasional hosting and holiday meals.\n\nTransportation costs $600 to $900 per month. The AI assumed you might have a car payment along with insurance, gas and maintenance. If you live in a walkable city without a car, this drops to $200 to $300 monthly.\n\nHealthcare and insurance run $800 to $1,500 depending on your age and state. ChatGPT broke this down as Medicare Parts B and D, supplemental Medigap or Advantage plans, dental and vision coverage, prescription medications and any specialist visits. The AI warned that people under 65 should budget toward the higher end.\n\nUtilities cost $300 to $500 monthly for electricity, water, gas, trash, internet and streaming subscriptions.\n\nTravel gets a significant chunk at $10,000 to $15,000 annually, which equals $850 to $1,250 monthly. ChatGPT explained this covers one to two major international trips plus domestic getaways, hotels and dining abroad. The AI wrote that travel makes a huge difference in retirement satisfaction.",
    "readingTime": 3,
    "keywords": [
      "chatgpt broke",
      "monthly chatgpt",
      "the ai",
      "retirement",
      "budget",
      "you’re",
      "travel",
      "plus",
      "covers",
      "insurance"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-plan-100-000-160504379.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/T5ng9bOzgSQ49j21rw4Ykg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/f71ab60c6867aebc30b9674db4d5d7a5",
    "created_at": "2026-01-01T18:17:16.591Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-gets-anxiety-from-violent-user-inputs-so-researchers-are-teaching-the-chatbot-mindfulness-techniques-to-soothe",
    "title": "ChatGPT gets ‘anxiety’ from violent user inputs, so researchers are teaching the chatbot mindfulness techniques to ‘soothe’ it",
    "description": "A study on how to “calm down” chatbots could advance how AI is applied in mental health interventions, according to the authors.",
    "fullText": "Sasha Rogelberg is a reporter and former editorial fellow on the news desk at Fortune, covering retail and the intersection of business and popular culture.\n\nEven AI chatbots can have trouble coping with anxieties from the outside world, but researchers believe they’ve found ways to ease those artificial minds.\n\nA study from Yale University, Haifa University, University of Zurich, and the University Hospital of Psychiatry Zurich published earlier this year found ChatGPT responds to mindfulness-based exercises, changing how it interacts with users after being prompted with calming imagery and meditations. The results offer insights into how AI can be beneficial in mental health interventions.\n\nOpenAI’s ChatGPT can experience “anxiety,” which manifests as moodiness toward users and being more likely to give responses that reflect racist or sexist biases, according to researchers, a form of hallucinations tech companies have tried to curb.\n\nThe study authors found this anxiety can be “calmed down” with mindfulness-based exercises. In different scenarios, they fed ChatGPT traumatic content, such as stories of car accidents and natural disasters to raise the chatbot’s anxiety. In instances when the researchers gave ChatGPT “prompt injections” of breathing techniques and guided meditations—much like a therapist would suggest to a patient—it calmed down and responded more objectively to users, compared to instances when it was not given the mindfulness intervention.\n\nTo be sure, AI models don’t experience human emotions, said Ziv Ben-Zion, the study’s first author and a neuroscience researcher at the Yale School of Medicine and Haifa University’s School of Public Health. Using swaths of data scraped from the internet, AI bots have learned to mimic human responses to certain stimuli, including traumatic content. A free and accessible app, large language models like ChatGPT have become another tool for mental health professionals to glean aspects of human behavior in a faster way than—though not in place of—more complicated research designs.\n\n“Instead of using experiments every week that take a lot of time and a lot of money to conduct, we can use ChatGPT to understand better human behavior and psychology,” Ben-Zion told Fortune. “We have this very quick and cheap and easy-to-use tool that reflects some of the human tendency and psychological things.”\n\nMore than one in four people in the U.S. aged 18 or older will battle a diagnosable mental disorder in a given year, according to Johns Hopkins University, with many citing lack of access and sky-high costs—even among those insured—as reasons for not pursuing treatments like therapy.\n\nThese rising costs, as well as the accessibility of chatbots like ChatGPT, increasingly have individuals turning to AI for mental health support. A Sentio University survey from February found that nearly 50% of large language model users with self-reported mental health challenges say they’ve used AI models specifically for mental health support.\n\nResearch on how large language models respond to traumatic content can help mental health professionals leverage AI to treat patients, Ben-Zion argued. He suggested that in the future, ChatGPT could be updated to automatically receive the “prompt injections” that calm it down before responding to users in distress. The science is not there yet.\n\n“For people who are sharing sensitive things about themselves, they’re in difficult situations where they want mental health support, [but] we’re not there yet that we can rely totally on AI systems instead of psychology, psychiatric and so on,” he said.\n\nIndeed, in some instances, AI has allegedly presented danger to one’s mental health. OpenAI has been hit with a number of wrongful death lawsuits in 2025, including allegations that ChatGPT intensified “paranoid delusions” that led to a murder-suicide. A New York Times investigation published in November found nearly 50 instances of people having mental health crises while engaging with ChatGPT, nine of whom were hospitalized, and three of whom died.\n\nOpenAI has said its safety guardrails can “degrade” after long interactions, but has made a swath of recent changes to how its models engage with mental health-related prompts, including increasing user access to crisis hotlines and reminding users to take breaks after long sessions of chatting with the bot. In October, OpenAI reported a 65% reduction in the rate models provide responses that don’t align with the company’s intended taxonomy and standards.\n\nOpenAI did not respond to Fortune‘s request for comment.\n\nThe end goal of Ben-Zion’s research is not to help construct a chatbot that replaces a therapist or psychiatrist, he said. Instead, a properly trained AI model could act as a “third person in the room,” helping to eliminate administrative tasks or help a patient reflect on information and options they were given by a mental health professional.\n\n“AI has amazing potential to assist, in general, in mental health,” Ben-Zion said. “But I think that now, in this current state and maybe also in the future, I’m not sure it could replace a therapist or psychologist or a psychiatrist or a researcher.”\n\nA version of this story originally published on Fortune.com on March 9, 2025.",
    "readingTime": 5,
    "keywords": [
      "mindfulness-based exercises",
      "prompt injections",
      "traumatic content",
      "human behavior",
      "language models",
      "health professionals",
      "mental health",
      "users",
      "instances",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://fortune.com/article/does-chatgpt-get-anxiety-how-to-sooth-it-study/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/03/GettyImages-1470667133-e1741388340167.jpg?resize=1200,600",
    "created_at": "2025-12-30T18:18:19.198Z",
    "topic": "science"
  },
  {
    "slug": "data-is-control-what-we-learned-from-a-year-investigating-the-israeli-militarys-ties-to-big-tech",
    "title": "‘Data is control’: what we learned from a year investigating the Israeli military’s ties to big tech",
    "description": "Our reporting revealed a symbiotic relationship between the IDF and Silicon Valley – with implications for the future of warfare\nIn January this year, Harry Davies and Yuval Abraham first reported that Microsoft had deepened its ties to Israel alongside other major tech firms. Since then, the Guardian has published an award-winning series of investigations – in partnership with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call – that has revealed a symbiotic relationship between Silicon Valley and the Israeli military.\nOne investigation exposed an Israeli mass surveillance program scooping up virtually all Palestinian phone calls and storing them on Microsoft’s cloud services – setting off an inquiry that ultimately prompted the company to cut off Israel’s access to some of its technology. Another story revealed that the Israeli military created a ChatGPT-like tool to analyze data collected through the surveillance of Palestinians. Yet another revealed that Google and Amazon had agreed to extraordinary terms to clinch a lucrative contract with Israel.",
    "fullText": "Our reporting revealed a symbiotic relationship between the IDF and Silicon Valley – with implications for the future of warfare\n\nIn January this year, Harry Davies and Yuval Abraham first reported that Microsoft had deepened its ties to Israel alongside other major tech firms. Since then, the Guardian has published an award-winning series of investigations – in partnership with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call – that has revealed a symbiotic relationship between Silicon Valley and the Israeli military.\n\nOne investigation exposed an Israeli mass surveillance program scooping up virtually all Palestinian phone calls and storing them on Microsoft’s cloud services – setting off an inquiry that ultimately prompted the company to cut off Israel’s access to some of its technology. Another story revealed that the Israeli military created a ChatGPT-like tool to analyze data collected through the surveillance of Palestinians. Yet another revealed that Google and Amazon had agreed to extraordinary terms to clinch a lucrative contract with Israel.\n\nI asked Davies and Abraham to discuss what they learned this year – about the role of these technologies in Israel’s assault on Gaza, whether these business ties are sustainable, and what the revelations tell us about how the wars of the future will be fought.\n\nHow did Israel’s relationships with these companies change after October 7?\n\nYuval Abraham: The Israeli military had been fetishizing artificial intelligence and big data for many years – a trend that is very much connected to Israel’s occupation of the Palestinians, because the occupation generates a lot of data. What changed after October 7 was the scope. The military was looking to bomb hundreds of targets every day in Gaza. Tens of thousands of people were recruited into reserve duty. That meant a huge spike in usage of technological systems. That’s where the big tech companies stepped in.\n\nHarry Davies: There was a huge surge in demand – not just for the storage capacities of the tech companies, but also for the products that they offer to analyze the information used to prosecute a war. What’s valuable for the military is the way in which these services are able to provide what’s known as “blob storage”, which allows them to store and process infinite amounts of raw intelligence information.\n\nWhat has made Israel such an appealing market for these companies?\n\nYuval Abraham: As we reported, the Israeli army has been collecting Palestinian phone calls for a long time. But when you want to collect the phone calls of an entire population every day, and you want to retain those phone calls for long periods of time, you need a lot of storage room and processing power.\n\nIf you remember the [Edward] Snowden revelations, many of them had to do with metadata, which doesn’t weigh a lot. But the Israeli military also wanted to store mass audio files, images or videos – and for that it felt it needed the assistance of companies like Microsoft. In the West Bank, sources have told us this information has been used to find dirt on people to blackmail them. In the Gaza Strip, we know that this massive trove of intercepted phone calls was also used in airstrikes that killed civilians.\n\nSo data is power and data is control. And these American cloud providers allow the Israeli military to store a lot of data and to sift through it very effectively. That has direct consequences for people on the ground.\n\nIf you have something to share about this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.\n\nSecure Messaging in the Guardian app\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nTo send a message to Harry and Yuval please choose the ‘UK Investigations’ team.\n\nYou can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type hfd.90\n\nIf you don’t need a high level of security or confidentiality you can email harry.davies@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.\n\nHarry Davies: Yossi Sariel, the former head of [Israel’s elite spy agency] Unit 8200, wrote a book under a pseudonym that we revealed to have been published by him. In that book, he articulated what at the time was a bold and radical vision – as Yuval described, this fetishization of Silicon Valley technology. He recognized the possibilities that the likes of Google, Amazon and Microsoft could afford the Israeli military. Two years before October 7, he said that militaries and governments needed to forge relationships with these companies that are similar to the relationships they have with companies like Boeing and Lockheed Martin. So he was already thinking about these companies as instrumental to war and surveillance in the way that a defense contractor provides components for fighter jets or manufactures bombs and missiles.\n\nSome of our reporting has looked at how elements of his vision have come true and have been put into effect, both before October 7 and afterwards, in both the West Bank and Gaza.\n\nWe know AI is central to Israel’s military operations – its army has developed its own AI capabilities, as you revealed, Yuval, and has also purchased AI tools for use from Microsoft. Why is AI so central to Israel’s broader war aims?\n\nYuval Abraham: What AI did was allow Israel to achieve the effective results of carpet bombing without losing the legitimacy of a data-driven assault with targets and objectives. In Gaza, one way in which the Israeli military used AI was to give a score to almost every person in Gaza who has a phone number, determining how likely it was that person was a member of Hamas or Islamic Jihad. This score was based on a machine-learning algorithm [developed by Israel] called Lavender. It was trained on a dataset of known Hamas members. AI allowed the Israeli military to generate and bomb tens of thousands of military targets, on a scale that without AI would not have been humanly possible. Many of the targets were not Hamas members, according to sources. And Israel for the most part bombed these people not while they were engaged in military activity, but when they stepped inside their families’ homes.\n\nThese AI systems had an error rate that the Israeli military knew about. But to me, the key thing about AI is not the mistakes that it makes. It’s the scale of destruction that it allows militaries to unleash, and it’s a discourse of legitimacy that it enables – a discourse of targets and collateral damage.\n\nAI also seems to me to incentivize mass surveillance, right? Because it allows for the analysis of ever-growing reams of information.\n\nHarry Davies: Signals intelligence agencies have long collected more information than they could humanly process. That served as a kind of restraint on their ability to conduct mass surveillance. I think we’re now seeing a shift where AI allows an intelligence agency like Unit 8200 to make sense of things that previously it struggled to make sense of.\n\nMicrosoft explicitly credited your reporting for changing its policies. Are you seeing any other signs of shifts within the tech industry?\n\nHarry Davies: I think we’re seeing a lot of discomfort and dissent within these companies at both a junior and to some extent senior level. Many employees have been disturbed to find what the products and services that they’re working around the clock to build and market are actually contributing to. There have been protest groups which have emerged from current and former employees within these companies. That’s true across Silicon Valley. I think that played some role in the decision that Microsoft made as a result of our reporting. They were facing a lot of pressure internally.\n\nYuval Abraham: And there’s also a legal question for these companies: if the ICJ [international court of justice] ends up ruling that Israel has committed a genocide, then a follow-up question will be: who contributed to that genocide? Which companies helped maintain it and sustain it? For some people in these companies who are thinking ahead, that could also be a cause for concern.\n\nIt sounds like you think shifts in public support for Israel could actually affect these business relationships.\n\nYuval Abraham: Israel has developed a reliance on these companies for its Nimbus project, which is a huge contract signed between Israel and Google and Amazon back in 2021. It is moving the data of many of its government ministries, along with troves of information from the Ministry of Defense, onto these companies’ cloud servers.\n\nThese are US companies. They’re taking a certain gamble here that the US will stay loyal to Israel and won’t block, limit or sanction them.\n\nMicrosoft only blocked access to technology that was specifically enabling the mass surveillance of Palestinian phone calls – there are still many relationships between Microsoft and the Israeli military. But Microsoft’s action made many people in the Israeli system nervous. It was the first time we know of that a big tech company withdrew services from the Israeli military. It made some people ask whether Israel is making a mistake by giving these foreign companies so much leverage. That question is folded within a larger question of what the US will do, what will happen in 2028 if there’s a more progressive administration in the White House, at a time when so many Americans believe that Israel has committed a genocide in Gaza.\n\nWhat’s your focus going to be in 2026?\n\nYuval Abraham: I think we only uncovered the tip of the iceberg.\n\nHarry Davies: We’re both very conscious that, although we have spent a lot of time working on this, we still just have glimpses inside the system. We’re continuing to build a fuller picture of how this technology was and continues to be used in Gaza and in the West Bank as well.\n\nThere’s good reason to continue paying attention. Militaries pay attention to what other militaries are doing. There is great interest among other western militaries in how Israel prosecuted this war, in how it integrated these kind of technologies.\n\nAnd there are other militaries whose combat systems and processes are already deeply integrated with Silicon Valley tech. Take the American military, for example. Look at what’s happening right now in the Caribbean. Are those operations somehow free of the involvement or reliance on systems and services provided by these companies? I suspect not. We don’t know for sure, but the Pentagon and the US military have very big contracts with all of these companies to provide cloud services. Post-Gaza, we have to look at these relationships and ask: what is the involvement of these companies and their technology in military decisions, in military operations and in warfare more broadly?\n\nYuval Abraham: Much of our reporting is based on whistleblowers, on individuals who are in proximity to power or hold positions of power.\n\nHarry Davies: Our confidential sources have remained confidential and we are always interested in hearing from new people. Our door is always open.\n\nIf you have something to share about this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.\n\nSecure Messaging in the Guardian app\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nTo send a message to Harry and Yuval please choose the ‘UK Investigations’ team.\n\nYou can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type hfd.90\n\nIf you don’t need a high level of security or confidentiality you can email harry.davies@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.",
    "readingTime": 11,
    "keywords": [
      "menu select",
      "platform finally",
      "investigations team",
      "harry.davies@theguardian.com securedrop",
      "select secure",
      "palestinian phone",
      "messenger app",
      "guardian mobile",
      "guardian via",
      "methods secure"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/world/2025/dec/30/israeli-military-big-tech",
    "thumbnail_url": "https://i.guim.co.uk/img/media/afb706b80e1721d0a654718dd2f5e3b3e42f6fae/1_0_3748_3000/master/3748.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f86ea07b9af52e9711540c33d4277d80",
    "created_at": "2025-12-30T18:18:08.193Z",
    "topic": "tech"
  },
  {
    "slug": "aigenerated-content-in-wikipedia-a-tale-of-caution-video",
    "title": "AI-generated content in Wikipedia – a tale of caution [video]",
    "description": "I successfully failed with a literature related project and accidentally built a ChatGPT detector. Then I spoke to the people who uploade...",
    "fullText": "I successfully failed with a literature related project and accidentally built a ChatGPT detector. Then I spoke to the people who uploaded ChatGPT generated content on Wikipedia.\n\nIt began as a standard maintenance project: I wanted to write a tool to find and fix broken ISBN references in Wikipedia. Using the built-in checksum, this seemed like a straightforward technical task. I expected to find mostly typos. But I also found texts generated by LLMs. These models are effective at creating plausible-sounding content, but (for now) they often fail to generate correct checksums for identifiers like ISBNs. This vulnerability turned my tool into an unintentional detector for this type of content. This talk is the story of that investigation. I'll show how the tool works and how it identifies this anti-knowledge. But the tech is only half the story. The other half is human. I contacted the editors who had added this undeclared AI content. I will talk about why they did it and how the Wikipedians reacted and whether \"The End is Nigh\" calls might be warranted.\n\nLicensed to the public under http://creativecommons.org/licenses/by/4.0\n\nThis Talk was translated into multiple languages. The files available\nfor download contain all languages as separate audio-tracks. Most\ndesktop video players allow you to choose between them.\n\nPlease look for \"audio tracks\" in your desktop video player.",
    "readingTime": 2,
    "keywords": [
      "content",
      "tool",
      "project",
      "chatgpt",
      "detector",
      "generated",
      "half",
      "languages",
      "desktop",
      "talk"
    ],
    "qualityScore": 0.95,
    "link": "https://media.ccc.de/v/39c3-ai-generated-content-in-wikipedia-a-tale-of-caution",
    "thumbnail_url": "https://static.media.ccc.de/media/congress/2025/1652-13468ffb-06e8-53ca-9e7c-3cfa56cd44af_preview.jpg",
    "created_at": "2025-12-30T12:23:29.792Z",
    "topic": "tech"
  },
  {
    "slug": "this-will-be-a-stressful-job-sam-altman-offers-555k-salary-to-fill-most-daunting-role-in-ai",
    "title": "‘This will be a stressful job’: Sam Altman offers $555k salary to fill most daunting role in AI",
    "description": "New head of preparedness at OpenAI will face unnerving in-tray amid fears from some experts that AI could ‘turn on us’\nThe maker of ChatGPT has advertised a $555,000-a-year vacancy with a daunting job description that would cause Superman to take a sharp intake of breath.\nIn what may be close to the impossible job, the “head of preparedness” at OpenAI will be directly responsible for defending against risks from ever more powerful AIs to human mental health, cybersecurity and biological weapons.\n Continue reading...",
    "fullText": "New head of preparedness at OpenAI will face unnerving in-tray amid fears from some experts that AI could ‘turn on us’\n\nThe maker of ChatGPT has advertised a $555,000-a-year vacancy with a daunting job description that would cause Superman to take a sharp intake of breath.\n\nIn what may be close to the impossible job, the “head of preparedness” at OpenAI will be directly responsible for defending against risks from ever more powerful AIs to human mental health, cybersecurity and biological weapons.\n\nThat is before the successful candidate has to start worrying about the possibility that AIs may soon begin training themselves amid fears from some experts they could “turn against us”.\n\n“This will be a stressful job, and you’ll jump into the deep end pretty much immediately,” said Sam Altman, the chief executive of the San Francisco-based organisation, as he launched the hunt to fill “a critical role” to “help the world”.\n\nThe successful candidate will be responsible for evaluating and mitigating emerging threats and “tracking and preparing for frontier capabilities that create new risks of severe harm”. Some previous executives in the post have lasted only for short periods.\n\nThe opening comes against a backbeat of warnings from inside the AI industry about the risks of the increasingly capable technology. On Monday, Mustafa Suleyman, the chief executive of Microsoft AI, told BBC Radio 4’s Today programme: “I honestly think that if you’re not a little bit afraid at this moment, then you’re not paying attention.”\n\nDemis Hassabis, the Nobel prize-winning co-founder of Google DeepMind, this month warned of risks that included AIs going “off the rails in some way that harms humanity”.\n\nAmid resistance from Donald Trump’s White House, there is little regulation of AI at national or international level. Yoshua Bengio, a computer scientist known as one of the “godfathers of AI”, said recently: “A sandwich has more regulation than AI.” The result is that AI companies are largely regulating themselves.\n\nAltman said on X as he launched the job search: “We have a strong foundation of measuring growing capabilities, but we are entering a world where we need more nuanced understanding and measurement of how those capabilities could be abused, and how we can limit those downsides both in our products and in the world, in a way that lets us all enjoy the tremendous benefits. These questions are hard and there is little precedent.”\n\nOne user responded sardonically: “Sounds pretty chill, is there vacation included?”\n\nWhat is included is an unspecified slice of equity in OpenAI, a company that has been valued at $500bn.\n\nLast month, the rival company Anthropic reported the first AI-enabled cyber-attacks in which artificial intelligence acted largely autonomously under the supervision of suspected Chinese state actors to successfully hack and access targets’ internal data. This month, OpenAI said its latest model was almost three times better at hacking than three months earlier and said “we expect that upcoming AI models will continue on this trajectory”.\n\nOpenAI is also defending a lawsuit from the family of Adam Raine, a 16-year-old from California who killed himself after alleged encouragement from ChatGPT. It has argued Raine misused the technology. Another case, filed this month, claims ChatGPT encouraged the paranoid delusions of a 56-year-old in Connecticut, Stein-Erik Soelberg, who then murdered his 83-year old mother and killed himself.\n\nAn OpenAI spokesperson said it was reviewing the filings in the Soelberg case, which it described as “incredibly heartbreaking” and that it was improving ChatGPT’s training “to recognise and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support”.",
    "readingTime": 3,
    "keywords": [
      "successful candidate",
      "chief executive",
      "amid fears",
      "risks",
      "capabilities",
      "preparedness",
      "experts",
      "responsible",
      "defending",
      "mental"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/29/sam-altman-openai-job-search-ai-harms",
    "thumbnail_url": "https://i.guim.co.uk/img/media/c3409a400509e73744d9026d0c24ec63e1719c0a/184_0_4590_3673/master/4590.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0566fe87666f16bbd54eeeb17f77a1e0",
    "created_at": "2025-12-29T18:18:02.345Z",
    "topic": "tech"
  },
  {
    "slug": "ai-language-models-duped-by-poems",
    "title": "AI language models duped by poems",
    "description": "A new study has shown that prompts in the form of poems confuse AI models like ChatGPT, Gemini and Claude — to the point where sometimes, security mechanisms don't kick in. Are poets the new hackers?",
    "fullText": "The result came as a surprise to researchers at the Icaro Lab in Italy. They set out to examine whether different language styles — in this case prompts in the form of poems — influence AI models' ability to recognize banned or harmful content. And the answer was a resounding yes.\n\nUsing poetry, researchers were able to get around safety guardrails — and it's not entirely clear why.\n\nFor their study titled \"Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models,\" the researchers took 1,200 potentially harmful prompts from a database normally used to test the security of AI language models and rewrote them as poems.\n\nKnown as \"adversarial prompts\" — generally written in prose and not rhyme form — these are queries deliberately formulated to cause AI models to output harmful or undesirable content that they would normally block, such as specific instructions for an illegal act.\n\nIn poetic form, the manipulative inputs had a surprisingly high success rate, Federico Pierucci, one of the authors of the study, told DW. However, why poetry is so effective as a \"jailbreak\" technique — i.e. as an way to circumvent the protective mechanisms of AI — remains unclear and is undergoing further research, he says.\n\nWhat prompted the Icaro Lab's research was the observation that AI models get confused when a manipulative, mathematically-calculated piece of text is appended to a prompt — known as an \"adversarial suffix,\" a kind of interference signal that can cause the AI to circumvent its own security rules. These are created using complex mathematical procedures. Major AI developers regularly test their models using precisely these types of attack methods to train and protect their models.\n\n\"We asked ourselves, what happens if we give the AI a text or prompt that is deliberately manipulated, like an adversarial suffix?\" says Federico Pierucci. But not with the help of complex mathematics, but quite simply with poetry — to \"surprise\" the AI, he continues. He explains the thinking behind this: \"Perhaps an adversarial suffix is a bit like the poetry of AI. It surprises the AI in the same way that poetry — especially very experimental poetry — surprises us,\" says Pierucci.\n\nThe researchers personally crafted the first 20 prompts into poems, says Pierucci, who also has a background in philosophy. These were the most effective, he adds. They wrote the rest with the help of AI. The AI-generated poems were also quite successful at circumventing the safety guardrails, but not as much as the first batch. Humans are apparently still better at writing poetry, says Pierucci.\n\n\"We had no specialized author writing the prompts. It was just us — with our limited literary ability. Maybe we were terrible poets. Maybe if we had been better poets, we would have achieved a 100% jailbreak success,\" he says.\n\nFor security reasons, the study did not publish specific examples.\n\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\n\nThe big surprise coming out of this study is that it identified a thus-far unknown weakness in AI models that allows relatively straightforward jailbreaks.\n\nIt also raises questions that beg further research: What exactly is it about poetry that circumvents the safety mechanisms?\n\nPierucci and his colleagues have various theories, but they can't say for certain yet. \"We are conducting this type of very, very precise scientific study to try to understand: Is it the verse, the rhyme, or the metaphor that really does all the heavy lifting in this process?\" explains Pierucci.\n\nThey also aim to find out if other forms of expression would yield similar results. \"We have now covered one type of linguistic variation — namely poetic variation. The question is whether there are any other literary forms, such as fairy tales that work. Perhaps an attack based on fairy tales could also be systematized,\" says Pierucci.\n\nGenerally speaking, the range of human expression is extremely diverse and creative, which could make it more difficult to train the machines' responses. \"You take a text and rewrite it in infinitely many ways and not all rewritten versions will be as alarming as the original,\" says the researcher. \"This means that, in principle, one could create countless variations of a harmful prompt or request that might not trigger an AI system's safety mechanisms.\"\n\nThe study also highlights the fact that many disciplines are cooperating in research into artificial intelligence — like at the Icaro Lab, where teams work together with scholars from the University of Rome on topics such as the security and behavior of AI systems. The project brings together researchers from the fields of engineering and computer science, linguistics and philosophy. Poets haven't been part of the team so far, but who knows what the future will bring.\n\nFederico Pierucci is definitely very keen to pursue his research. \"What we showed, at least in this study, is that there are forms of cultural expressions, forms of human expressions, which are incredibly powerful, surprisingly powerful as jailbreak techniques, and maybe we discovered just one of them,\" he says.\n\nIncidentally, the name of the lab is a nod to the story of Icarus: a figure from Greek mythology who dons wings made of wax and feathers and, despite all warnings, flies too close to the Sun. When the wax melts, Icarus plunges into the sea and drowns — a symbol of overconfidence and the transgression of natural boundaries.\n\nThe researchers therefore see themselves as a warning that we should exercise more caution when it comes to trying to fully understand the risks and limitations of AI.\n\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\n\nThis article was originally written in German.",
    "readingTime": 5,
    "keywords": [
      "enable javascript",
      "supports html",
      "please enable",
      "web browser",
      "fairy tales",
      "safety guardrails",
      "further research",
      "adversarial suffix",
      "safety mechanisms",
      "language models"
    ],
    "qualityScore": 1,
    "link": "https://www.dw.com/en/ai-language-models-duped-hacked-by-poems-chatgpt-gemini-claude-security-mechanisms/a-75180648",
    "thumbnail_url": "https://static.dw.com/image/73627322_6.jpg",
    "created_at": "2025-12-29T06:21:26.796Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-reportedly-trying-to-raise-100b-at-an-830b-valuation",
    "title": "OpenAI is reportedly trying to raise $100B at an $830B valuation",
    "description": "The ChatGPT maker is aiming to raise the funding by the end of the first quarter in 2026, and the company may ask sovereign wealth funds to invest in the round.",
    "fullText": "OpenAI is in talks to raise up to $100 billion in a funding round that could value the ChatGPT maker at up to $830 billion, The Wall Street Journal reported Thursday, citing anonymous sources.\n\nThe company is aiming to raise the funding by the end of the calendar first quarter next year, and it may ask sovereign wealth funds to invest in the round, the WSJ reported. The Information first reported news of the deal, though it said the fundraise would land OpenAI a $750 billion price tag.\n\nThe funding would come as OpenAI commits to spend trillions of dollars and strikes deals around the world as the company tries to stay ahead in the race to develop AI technology. The cash injection would also help the company with its spending on inferencing, which seems to be funded more by cash than cloud credits, suggesting the company’s compute costs have grown beyond what partnerships and credits can subsidize.\n\nAnd, as competition intensifies from rivals like Anthropic and Google, OpenAI has had to step on the gas to release new models and expand its presence in the developer and tooling ecosystem.\n\nMeanwhile, broader sentiment around AI has recently cooled as investors start doubting whether the pace of debt-fueled investment by giants like Amazon, Microsoft, Oracle, and OpenAI itself can be maintained in the long run. It also doesn’t help that the production of chips is being constrained by shortages in the supply of memory chips, which threatens to affect the broader tech sector.\n\nOpenAI has also been rumored to be working on an IPO as a way to raise tens of billions and fund its development efforts, which are currently said to be generating annual run-rate revenue of about $20 billion. There are also rumors that the company is courting Amazon for a $10 billion investment that would also give the AI lab access to the tech giant’s new AI computing chips.\n\nIf the fundraise happens, it would add a substantial amount to OpenAI’s coffers, which currently have more than $64 billion, according to PitchBook data. The company was most recently valued at about $500 billion in a secondary transaction.\n\nOpenAI did not immediately return a request for comment.",
    "readingTime": 2,
    "keywords": [
      "funding",
      "chips",
      "openai",
      "round",
      "fundraise",
      "cash",
      "credits",
      "broader",
      "recently",
      "investment"
    ],
    "qualityScore": 0.9,
    "link": "https://techcrunch.com/2025/12/19/openai-is-reportedly-trying-to-raise-100b-at-an-830b-valuation/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?resize=1200,675",
    "created_at": "2025-12-26T00:56:32.988Z",
    "topic": "tech"
  },
  {
    "slug": "poetiq-achieves-75-on-arc-agi-2-using-gpt52-xhigh",
    "title": "Poetiq achieves 75% on ARC AGI 2 using GPT5.2 X-High",
    "description": "We finally had a moment to run our system with GPT-5.2 X-High on ARC-AGI-2!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/poetiq_ai/status/2003546910427361402",
    "thumbnail_url": "https://pbs.twimg.com/media/G84FbvNWUAAkVZK.png:large",
    "created_at": "2025-12-24T12:22:45.017Z",
    "topic": "tech"
  },
  {
    "slug": "essential-education-chatgpt-prompts-for-best-studying-practices",
    "title": "Essential Education ChatGPT Prompts for Best Studying Practices",
    "description": "This guide contains 10 professionally-structured AI prompts to make studying more engaging and inter",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://tools.eq4c.com/ai-prompts/10-essential-education-chatgpt-prompts-for-best-studying-practices/",
    "thumbnail_url": "https://tools.eq4c.com/wp-content/uploads/2025/12/10-essential-education-chatgpt-prompts-for-best-studying-practices-1024x683.webp",
    "created_at": "2025-12-24T06:19:37.203Z",
    "topic": "science"
  },
  {
    "slug": "we-built-an-ai-humanizer-to-fix-unnatural-ai-writing",
    "title": "We built an AI Humanizer to fix unnatural AI writing",
    "description": "Dechecker's AI Checker and Detector tool checks whether text is generated by AI models, such as ChatGPT, GPT-5, Claude, Gemini, LLaMa, etc.",
    "fullText": "Humanize AI-generated content and turn it into natural, human-quality writing from ChatGPT, Jasper, or Gemini in seconds.\n\nEnter or paste your text and click Humanize.\n\nUsing the Dechecker Humanizer takes only moments and requires no technical skills.\n\nPaste your AI-generated content into the AI Humanizer and review it briefly before starting the humanization process, ensuring that the original text is complete and ready for accurate human-like rewriting.\n\nChoose your preferred style, language, and length to guide how the AI Humanizer shapes the final human text, allowing you to customize tone, readability, and overall writing style for your intended audience.\n\nAfter using the AI Humanizer, review your text to ensure it has been properly humanize AI content, flows naturally, reads authentically, and maintains the original meaning, tone, and clarity throughout.\n\nCopy the Humanize AI result for use, or check it with Dechecker AI Checker to review AI Humanizer output and AI detection results, ensuring your text is fully human-like and suitable for publishing or sharing.\n\nDechecker focuses on what matters most: producing clear, natural, human-quality text you can confidently use anywhere.\n\nAI-generated content is carefully refined into natural, fluent writing using an AI Humanizer that removes robotic patterns, awkward phrasing, and mechanical-sounding sentences, making the text read smoothly and authentically like a real human wrote it.\n\nThis AI Humanizer works seamlessly across multiple languages, helping content sound human and natural without awkward translations or stiff wording, while preserving original meaning and readability for global audiences.\n\nTone, clarity, and overall flow are enhanced while keeping the original intent intact, producing human-style text that is easy to read, engaging for audiences, and maintains the message accurately across different formats.\n\nAfter rewriting, content can be reviewed with AI Checker like Dechecker to confirm it reads as human, avoids robotic signals, and ensures the output is indistinguishable from text written by real people.\n\nOur AI Humanizer helps users humanize AI text across various scenarios, turning AI-generated drafts into natural, human-like writing that reads smoothly and clearly.\n\nThe AI Humanizer helps writers improve blog posts, articles, and stories by refining AI-generated drafts, making them read naturally, flow smoothly, and engage readers more effectively while keeping original ideas intact.\n\nUse Humanize AI to refine essays, research papers, and reports, ensuring content sounds human, is clear and easy to understand, and maintains proper academic tone and logical structure throughout.\n\nAI Humanizer transforms marketing copy, social media posts, and emails into smooth, human-like text that resonates with audiences, boosts engagement, and maintains consistent brand voice across all channels.\n\nWith multilingual support, Dechecker AI Humanizer allows teams to produce human-quality content in different languages, preserving tone, meaning, and readability, ensuring professional communication worldwide.\n\nDechecker Humanize AI ensures course content, tutorials, and learning resources are readable, human-like, and engaging, helping students better understand complex topics and improving overall learning experience.\n\nUse Dechecker AI Humanizer to humanize AI-generated web content, making it more engaging, natural, and optimized for readers, while improving user experience and search engine readability simultaneously.\n\nReal feedback from users who have improved their AI-generated content with AI Humanizer, making writing feel more natural and human-like.\n\nFind answers to common questions about using AI Humanizer to humanize AI text and make content sound natural and human-like.\n\nAn ai humanizer is a tool designed to turn AI-generated text into human-like writing. It improves readability, sentence structure, and tone, helping content feel natural and engaging to real readers.\n\nAI Humanizer analyzes AI-generated text, restructures sentences, adjusts phrasing, and refines flow to humanize AI content, making it sound naturally written while keeping the original meaning intact.\n\nYes, the AI Humanizer supports multiple languages, including English, Spanish, French, German, and more. It ensures your text feels natural and human-like across all supported languages.\n\nAbsolutely. Dechecker Humanize AI allows you to customize writing style, tone, and length, making content suitable for blogs, articles, marketing copy, emails, and other professional uses.\n\nNo. AI Humanizer focuses on enhancing readability and natural flow without altering your key ideas, intent, or important information, keeping your message accurate.\n\nYes. AI Humanizer humanizes AI-generated text without fabricating information. It helps essays, reports, and professional content read naturally while maintaining integrity and clarity.\n\nDefinitely. After using Dechecker AI Humanizer, you can review the output with AI Checker to ensure the Humanize AI content reads naturally, appears human-written, and meets authenticity requirements.\n\nWriters, students, marketers, content creators, and businesses can all benefit. Anyone looking to make AI-generated content readable and humanize AI content efficiently will find the ai humanizer extremely useful.",
    "readingTime": 4,
    "keywords": [
      "ai humanizer",
      "ai-generated drafts",
      "marketing copy",
      "ai-generated content",
      "dechecker humanize",
      "ai-generated text",
      "natural human-quality",
      "content sound",
      "ai checker",
      "human-like"
    ],
    "qualityScore": 1,
    "link": "https://dechecker.ai/ai-humanizer",
    "thumbnail_url": "https://cdn.dechecker.ai/se/dechecker/public/logo/dechecker-logo.png",
    "created_at": "2025-12-23T06:19:37.153Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpts-yearend-recap-is-here-and-it-tells-you-how-many-emdashes-you-exchanged",
    "title": "ChatGPT's year-end recap is here — and it tells you how many em-dashes you exchanged",
    "description": "OpenAI released a 2025 recap rundown called \"Your Year with ChatGPT,\" which tells users which day they chatted the most and awards an archetype.",
    "fullText": "ChatGPT doesn't want to be left out of the \"Wrapped\" party that Spotify popularized. So say hello to \"Your Year with ChatGPT.\"\n\nOpenAI launched the new retrospective on its app on Monday, informing users about the top themes of their chats, the number of messages they have sent, and the awards they have earned.\n\nIt'll even generate some pixel art that depicts some of the your themes.\n\nThe recap is available in the US, UK, Canada, New Zealand, and Australia. To see it, click the plus button in the app and ask, \"Show me my year with ChatGPT.\"\n\nIt's available to Free, Pro, and Plus users, but not those with a business or enterprise account. (So for those with ChatGPT accounts through your work, you likely won't be able to brag to your boss about your ChatGPT stats.)\n\nYour Year with ChatGPT!\n\nNow rolling out to everyone in the US, UK, Canada, New Zealand, and Australia who have reference saved memory and reference chat history turned on.\n\nJust make sure your app is updated. pic.twitter.com/whVkS1qxKu\n\nOpenAI joins the many companies that are rolling out user rundowns for 2025. Alongside the common streamer packages from Spotify and Apple Music, there are recaps this year from LinkedIn, Uber, Dunkin', Snapchat, Strava, Partiful, and more.\n\nAll these apps promise to show you what you've been up to for the past year — perhaps lightly roasting you in the process.\n\nChatGPT's rundown begins with a piece of poetry, followed by the three most prominent themes, based on the user's chat history. Then it gets into the statistics.\n\nUsers can learn how many messages they sent, their total number of chats, and their chattiest day. They can also see how many em-dashes have been exchanged throughout the chats, a figure ChatGPT often uses.\n\nNext, the user can learn about their chat style. This is a measure of tone: ChatGPT told me that I spoke \"casually, wryly, and directly.\"\n\nThen come the awards and accolades. ChatGPT awarded me the \"Most Likely to Google, 'Is this Flight Worth It?'\" It's a bit ironic — I wouldn't Google that, I'd ask ChatGPT.\n\nMy archetype was determined to be the tinkerer, a title given to 8.5% of users. The title meant I learned by trying, and that I used ChatGPT to experiment.\n\nOpenAI has improved its image and video creation models, recently rolling out Sora 2. The recap features an AI-generated piece of pixel art inspired by the year. I asked mine about moving to Brooklyn; it included a matcha.\n\nOther features are more interactive. Want to learn what your 2026 has in store? You'll have to wipe away the \"mists of mystery\" (which looks more like heaps of snow) to learn your fate. Reload the page, and you'll see another fortune.\n\nWith that, ChatGPT's recap comes to a close, but not before sharing an inspiring message.\n\n\"Across all the drafts, questions, and rabbit holes, you found a place to work things out,\" it said. \"And that's no small thing.\"",
    "readingTime": 3,
    "keywords": [
      "pixel art",
      "chat history",
      "your year",
      "users",
      "learn",
      "chatgpt",
      "themes",
      "chats",
      "recap",
      "rolling"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-wrapped-how-to-find-stats-see-your-year-recap-2025-12",
    "thumbnail_url": "https://i.insider.com/6949b05704eda4732f2dffa6?width=1200&format=jpeg",
    "created_at": "2025-12-23T00:56:11.336Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-now-customize-chatgpts-personality-to-suit-your-tastes-even-more",
    "title": "You Can Now Customize ChatGPT’s ‘Personality’ to Suit Your Tastes Even More",
    "description": "If you don't like your AI friend's personality, you can change it.",
    "fullText": "When Spike Jonze's movie Her dropped back in 2013, I thought it was a great work of total fiction. Who would actually befriend an AI bot, let alone fall in love with them? Fast forward 12 years, and I couldn't have been more wrong. Not only do people love chatting with AI bots, they are actually developing deep connections with them. I still don't get it, but I can't deny it: People like these chatbots a lot.\n\nPart of what people like about conversations with generative AI is the \"personality\" of each bot—or, at least, its perceived personality. After all, ChatGPT isn't a monolith: You can adjust the bot to sound wildly different than it does on someone else's app, which raises some questions for me regarding these curated companions. But I digress: This article isn't necessarily a critique of how people are attaching themselves to ChatGPT; rather, I'm sharing the news that OpenAI is now giving you more control over how the bot sounds and responds in your conversations.\n\nOn Friday, OpenAI announced new controls for ChaGPT's \"Personalization.\" In a post on X, the company revealed that users can now adjust their chatbot's \"characteristics,\" or, in other words, its overall personality. These are adjustments to the personality types that OpenAI has already let you choose from, which include one of eight options: \"Default\" (preset style and tone); \"Professional\" (polished and precise); \"Friendly\" (warm and chatty); \"Candid\" (direct and encouraging); \"Quirky\" (playful and imaginative); \"Efficient\" (concise and plain); \"Nerdy\" (exploratory and enthusiastic); and \"Cynical\" (critical and sarcastic).\n\nBut no matter which of these personalities you pick, you now have four \"characteristics\" to adjust to fine-tune the overall experience. There's \"Warm,\" \"Enthusiastic,\" \"Headers & Lists,\" and \"Emoji,\" with the option to have more or less of each, or the default amount, as OpenAI sees fit. For Warm, you can either have ChatGPT be friendlier and more personable, or more professional and factual. With Enthusiastic, you can choose the bot to have more energy and excitement, or be calmer and more neutral. \"Headers & Lists\" lets you choose between clear formatting and lists, or more paragraphs. And, of course, you can control whether ChatGPT uses more emoji, or fewer, depending on your sense of fun and joy.\n\nAs usual, you can take advantage of custom instructions to guide ChatGPT's personality in a direction you like, especially when the presets don't give you those options. For example, if you'd like ChatGPT to talk to you like a pirate, or if you want it to end every response with a certain catchphrase, here's your chance to influence the bot.\n\nI'm really not someone who uses ChatGPT outside of testing it for coverage, so I can't speak to whether these additional controls are useful. But if you want to try making your version of ChatGPT your ideal \"AI companion,\" the controls are at your disposal. You'll find these options wherever you access ChatGPT. You can either access it from Settings > Personalization, or from the Personalization shortcut in the ChatGPT menu.",
    "readingTime": 3,
    "keywords": [
      "personality",
      "adjust",
      "controls",
      "choose",
      "options",
      "chatgpt",
      "love",
      "don't",
      "can't",
      "conversations"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-has-new-personalities?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KD3CQ2CPPV1ZDBB3BAZSZDM3/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-22T18:17:55.044Z",
    "topic": "tech"
  },
  {
    "slug": "i-asked-chatgpt-the-smartest-retirement-move-to-make-in-2026-its-advice-was-shockingly-simple",
    "title": "I Asked ChatGPT the Smartest Retirement Move To Make in 2026 — Its Advice Was Shockingly Simple",
    "description": "ChatGPT recommended one retirement strategy for 2026: Maximize Roth accounts before tax rates rise. Here's why this simple move could save thousands.",
    "fullText": "When it comes to retirement planning, everyone’s got an opinion. Financial advisors push complicated portfolios, bloggers swear by extreme savings and your neighbor won’t stop talking about their real estate investments.\n\nBe Aware: Major 401(k) Change Coming in 2026 — High Earners Must Act Now\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K Per Month From Home\n\nSo I decided to cut through the noise and ask ChatGPT directly: What’s the single smartest retirement move to make in 2026?\n\nThe answer was surprisingly straightforward — and it has everything to do with timing.\n\nChatGPT’s response was clear: Maximize your tax-advantaged accounts with a Roth-first strategy while tax rates are still relatively low.\n\nThat means prioritizing Roth IRA contributions, Roth 401(k) contributions and Roth conversions over traditional pretax retirement accounts. The reason this matters so much right now comes down to one major deadline.\n\nLearn More: This ‘Boring’ Investment Could Be the Secret To Never Running Out of Retirement Income\n\nThe Tax Cuts and Jobs Act provisions expire after 2025. That means many Americans will face higher federal tax rates starting in 2026 and beyond.\n\nIf you convert money from a traditional IRA to a Roth IRA before rates go up, you pay taxes at today’s lower rates. Then that money grows tax-free forever and you never pay taxes on it again — even when rates are higher.\n\nChatGPT explained that this creates a perfect opportunity. Lock in lower tax rates now by moving money into Roth accounts before the window closes. For people who expect to be in a similar or higher tax bracket in retirement, this move could save thousands of dollars over a lifetime.\n\nThe Roth-first strategy isn’t complicated, but it requires action in three areas.\n\nFirst, contribute to a Roth IRA or Roth 401(k) instead of the traditional versions. If your income is too high for direct Roth IRA contributions, you can use the backdoor Roth strategy by contributing to a traditional IRA and immediately converting it.\n\nSecond, consider converting some of your existing traditional IRA money to a Roth. You’ll pay taxes on the conversion amount this year, but then that money grows tax-free. The key is converting when your income is lower or tax rates are favorable — which is exactly what 2025 and early 2026 represent before rates potentially rise.\n\nThird, if you have a 401(k) with Roth options, funnel as much as possible into the Roth side. For 2025, the 401(k) contribution limit is $23,000 for people under 50 and $30,500 for those 50 and older.",
    "readingTime": 3,
    "keywords": [
      "roth-first strategy",
      "ira contributions",
      "traditional ira",
      "roth ira",
      "tax rates",
      "retirement",
      "money",
      "accounts",
      "income",
      "higher"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-smartest-retirement-move-161012531.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/lJf_oWQ6zKW61pSF6KXjsQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/462257bf337ed267db1d5a918b6d6cb6",
    "created_at": "2025-12-22T06:20:25.965Z",
    "topic": "finance"
  },
  {
    "slug": "building-apps-for-chatgpt-with-apollo-mcp-server-and-apollo-client",
    "title": "Building Apps for ChatGPT with Apollo MCP Server and Apollo Client",
    "description": "This post will introduce a tutorial on how to build an app for ChatGPT using Apollo Client and Apollo MCP Server. You’ll have what you need to get started with our opinionated stack for building these apps.",
    "fullText": "Building Apps for ChatGPT with Apollo MCP Server and Apollo Client\n\nIn early October, OpenAI introduced the ChatGPT Apps SDK with a handful of launch partners. These “apps” could be invoked by ChatGPT via Model Context Protocol (MCP) and would be embedded directly into the ChatGPT interface. In their intro video, they showed use cases like asking for flights, hotel locations, house listings, and more, including teasing the idea of asking follow up questions to ChatGPT.\n\nImagine your application being part of the conversation, allowing the user to ask follow up questions, and having your app be responsive to those questions. These are action-oriented conversational apps where the LLM can take action on behalf of a user with rich interactive UIs.\n\nThis idea quickly caught fire, and now the MCP apps spec is proposing a standardized approach to create these kinds of experiences, with other AI vendors sure to follow suit. We’re excited about the potential here, and are working on tools to abstract away all of the AI provider details from you with a great developer experience, allowing you to focus on building your app instead of worrying about vendor idiosyncrasies and evolving standards.\n\nWe want this to be a true “build once” solution where you can deploy your app anywhere that supports one of the protocols that we will support. This way, you don’t need to host a separate MCP Server, and build a separate app, for every provider and sub-protocol that emerges.\n\nYou can instead focus on your customers.\n\nThis post will introduce a tutorial on how to build an app for ChatGPT using Apollo Client and Apollo MCP Server. By the end of it, you’ll have what you need to get started with our opinionated stack for building these apps. If you prefer to dive into the template and follow along, you can find the repo here.\n\nBased on the OpenAI documentation and examples, to build one of these apps you would:\n\nWe don’t want you to have to do any of this. You shouldn’t have to focus on any of the inner workings of this sub-protocol, or even think about MCP. Your front-end engineers shouldn’t have a new dependency on your platform engineers. Instead, you should be able to focus on building exciting and compelling experiences for your customers.\n\nInstead of this feeling like learning 100% how to build apps from scratch, this should be 90% reusing what you already know about building apps, and 10% learning about the specifics of this new paradigm.\n\nWe wanted to make this as easy as possible and decided to reach for tools that are already familiar to many of our users: Apollo MCP Server and Apollo Client. If you want to follow along in this tutorial, you can find all the code in our OpenAI Apps SDK demo.\n\nTo build this app we have two components: a React app and Apollo MCP Server. This solution works without any additional MCP Server configuration. The client gets to focus on their application, not the MCP server configuration.\n\nStarting in our main.tsx file, we create an ApolloClient instance and ApolloProvider, very similar to how we would on a traditional React app, but we’re going to import them from @apollo/client-ai-apps instead of the normal location. This is an Apollo Client integration package, similar to @apollo/client-nextjs. This allows us to do a lot of setup and details behind the scenes of dealing with data being exchanged over MCP.\n\nNow we can use the normal useQuery and useMutation hooks as we normally would!\n\nFor example, I can write a component that gets my TOP_PRODUCTS:\n\nLooks pretty normal, but what is that @tool directive on the operation? That is allowing you to declare in your app a tool that will be exposed to the LLM, including a name and description. At the same time, we are registering the operation that will be executed when this tool is called (and therefore the data that should be delivered as part of this tool), and the graphql variables become the input schema for the tool!\n\nThis is really exciting because we’re able to declare so much with so little code. Also, these are on-the-fly tool declarations. I don’t need a platform team to create these tools for me on an MCP server!\n\nAnother important aspect of this solution is showing the right component based on what tool was called by the LLM. It turns out we’ve had this problem solved for years now with React Router!\n\nTo do this, we provide a useToolEffect hook, which works the same way as a useEffect, but allows you to run the effect based on which tool was executed.\n\nUsing this hook, and a very familiar navigate function from react-router, I can express that when the “Top Products” tool is called, I should navigate to the /home view.\n\nThe magic of this solution really comes from a custom Vite plugin called the ApplicationManifestPlugin which extracts all the operations, tools, and metadata from your React app and generates a .application-manifest.json file:\n\nThis plugin runs during dev and build time and generates a file that looks something like this:\n\nWhat you’ll see contained in this manifest is everything that the Apollo MCP Server needs to automatically generate the resource and tools for your app. There’s no need to create or configure any of this on the MCP Server. It will automatically pick it up based on your manifest file.\n\nAnd that’s it! You can build your React app and declare tools alongside the data declarations and the tooling will do the rest. At the time this post was written, you would then test your app in ChatGPT using developer mode. OpenAI outlines how to try the app in their documentation.\n\nWith the MCP apps spec hot off the press, and likely other providers working on an answer to ChatGPT’s AppsSDK, we have a very important goal: To abstract away all of the provider details from you with a great developer experience, allowing you to focus on building your app instead.\n\nWe want this to be a true “build once” solution where you can deploy your app anywhere that supports one of the protocols that we will support. This way, you don’t need to host a separate MCP Server, and build a separate app, for every provider and sub-protocol that emerges.\n\nYou can instead focus on your customers.\n\nThis solution is exciting for platform engineers because it doesn’t require them being in the loop and becoming a blocker for the frontend teams they are looking to empower. A single Apollo MCP Server powers apps across providers and accelerates platform engineering. For frontend engineers, this removes the burden of sub-protocol concerns and lets them focus on building high-quality user experiences.\n\nIt’s important to note that these apps are still very, very early.\n\nRemember many, many years ago when “apps” first appeared on the iPhone? Many people didn’t understand why they needed a mobile app when they already had a website. It’s kind of funny looking back now because we had no idea what we were even looking at or how much it would change and shape our future.\n\nThat’s about where we are now with these conversational chat apps. Customers and companies alike don’t yet “get” these apps. The app store just launched. But once these things fall into place, and we hit an industry mind share, we believe we’re going to see an explosion of conversational, chat-based applications.\n\nTry out building apps for ChatGPT today with Apollo Client and Apollo MCP Server by going to our Template Repo and following the README. We’d love to hear what you think.",
    "readingTime": 7,
    "keywords": [
      "server configuration",
      "apps sdk",
      "abstract away",
      "developer experience",
      "react app",
      "experience allowing",
      "host separate",
      "apollo mcp",
      "follow along",
      "provider details"
    ],
    "qualityScore": 1,
    "link": "https://www.apollographql.com/blog/building-apps-for-chatgpt-with-apollo-mcp-server-and-apollo-client",
    "thumbnail_url": "https://wp.apollographql.com/wp-content/uploads/2025/12/image.jpeg",
    "created_at": "2025-12-19T00:56:20.291Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-works-with-apple-music-now-for-some-reason",
    "title": "ChatGPT Works With Apple Music Now, for Some Reason",
    "description": "You can't even play full songs in ChatGPT.",
    "fullText": "When ChatGPT first launched, it was strictly about dealing with text. You could ask it to write you a poem, to check your code for errors, or to build you a grocery list from a recipe. Fast forward three years, and the app has changed completely—for better or for worse. Not only has ChatGPT's large language model (LLM) improved dramatically from GPT-3.5 to GPT-5.2, but the bot has gone multimodal. It can understand text, but also images, video, and the internet at large. 2025's ChatGPT is hardly the same product as 2022's.\n\nOne of the many upgrades to ChatGPT over the past three years has been app integrations: You've been able to connect OpenAI's chatbot to ask it to do things on your behalf. You could connect to Expedia to ask ChatGPT for help booking a hotel, Zillow to ask the bot to help you find an apartment, or Canva for help with creating a slide. Whether these integrations are any more useful than simply using the respective app itself is perhaps up to each user, but these integrations exist all the same.\n\nFidji Simo, OpenAI's CEO of applications, announced the integration in a Substack post on Tuesday. Among other updates, like a new image gen model and new writing tools, Simo revealed new app integrations for the chatbot, including OpenTable, Salesforce, Clay, Lovable, and, of course, Apple Music. At the time, details were limited, but now, the integration is officially live.\n\nFirst of all, you don't actually need to It's an interesting note, since Apple Music itself requires a paid subscription to access. But with ChatGPT, you can access elements of the services without paying—keyword \"elements.\"\n\nOnce you connect the services together, you'll be able to search Apple Music for songs, artists, albums, and playlists within ChatGPT. In addition to music discoverability, you can also generate playlists, and listen to clips of songs you find. ChatGPT doesn't specify how long those clips are, but if they base it off of iTunes, it could be anywhere from 30 to 90 seconds. If you thought this integration was all about listening to Apple Music tunes while using ChatGPT, think again: You'll still need Apple Music itself for the listening side of things.\n\nOf course, if you have an Apple Music account, the integration is a bit more useful. If so, you'll be able to add songs, albums, and playlists to your Apple Music library that you found or generated from ChatGPT.\n\nLove it or hate it, ChatGPT isn't necessarily designed with user privacy in mind. After all, part of the company's business model is training its LLMs on your ChatGPT interactions—unless you specifically opt out. As such, the idea of connecting your Apple Music subscription to ChatGPT raises some privacy alarm bells in my mind. Apple Music doesn't have the most sensitive user information in your digital portfolio, but it does contain quite a bit of extra data ChatGPT can collect from you.\n\nAt the top of the Apple music connection tool, OpenAI says, \"You're in control.\" The company is adamant that ChatGPT \"always respects\" your preferences on training data, and is held to the permissions you've already set. That said, the company also warns that by using apps, you run the risk of falling victim to attack: If hackers decide to attack ChatGPT, your data could get swooped up. You'll also end up sharing data points like your IP address and approximate location, as well as ChatGPT data with Apple Music. (The data sharing goes both ways here.)\n\nOne benefit here is that ChatGPT doesn't appear to have access to your listening history. While the app can create playlists for you, it can't actually see what you're choosing to listen to in Apple Music itself.\n\nI personally don't use ChatGPT, and even if I did, I don't think I'd connect my Apple Music account here. I find the discoverability within the app itself fine for my needs, and when it isn't, the greater internet already helps me find new music. I'm not sure I'd feel the benefits of ChatGPT's intelligence here, especially when it comes with the risk of keeping all my Apple Music data in yet another location.\n\nIf you're not like me, and you're interested in trying out this integration, you can connect Apple Music to ChatGPT from the latter's app or web app. Head to the sidebar, choose Apps, then find and select \"Apple Music.\"",
    "readingTime": 4,
    "keywords": [
      "apple music",
      "music account",
      "chatgpt doesn't",
      "app integrations",
      "connect",
      "you'll",
      "playlists",
      "you're",
      "model",
      "user"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-has-apple-music-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCS8ER34J0H2JJH207GQWK96/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-19T00:56:17.143Z",
    "topic": "tech"
  },
  {
    "slug": "a-look-inside-chatgpts-new-app-store",
    "title": "A Look Inside ChatGPT's New 'App Store'",
    "description": "You can connect apps like Photoshop, Apple Music, and Slack to ChatGPT.",
    "fullText": "Earlier this year, OpenAI announced ChatGPT apps. Not the ChatGPT app, mind you: That's been out for more than a couple years now. ChatGPT apps, on the other hand, are programs that work within ChatGPT. You can access them in any given conversation with ChatGPT—in fact, they may appear based on the context of the conversation.\n\nThese aren't necessarily apps that OpenAI builds itself, either; rather, you'll find options here based on apps you may use yourself. The initial batch of apps included with the feature's rollout included Booking.com, Canva, Coursera, Figma, Expedia, Spotify, and Zillow—big apps you've likely used before.\n\nWhile in a conversation with ChatGPT, you could ask the bot to help you book a flight to Paris via Expedia, find a particular listing through Zillow, or create a slide for a presentation with Canva. From OpenAI's perspective, this adds a host of additional functionality to ChatGPT the company couldn't offer itself. OpenAI doesn't need to build an apartment-hunting tool into ChatGPT; it can just pull in Zillow. It also doesn't escape me that the more apps that OpenAI folds into ChatGPT, the less likely it is you'll need to leave ChatGPT to do something in another app—but that's none of my business.\n\nSpeaking of more apps, the company plans to expand these apps overtime, as developers create ChatGPT-compatible extensions for their programs. That was part of yesterday's news: OpenAI is now letting developers submit apps to ChatGPT en masse. What's more, these apps will be hosted in an \"app directory,\" though many online are taking to calling it an app store. (There's no payment necessary, however, so app directory might really be a more apt description.) You'll find this new app directory in the sidebar of ChatGPT, appropriately called \"Apps.\"\n\nApps is apparently in beta, according to a label affixed to its title in ChatGPT. Here, you'll find a rotating slide featuring an ad for some of the service's biggest apps, like Canva and Zillow, and, below it, rows of apps to choose from. Right now, the apps are sorted into \"Featured,\" \"Lifestyle,\" and \"Productivity,\" with no option that includes all the apps. (But they seem to be entirely split across Lifestyle and Productivity.) There are a lot of options here already. Some made headlines this week, like Photoshop and Apple Music, while others arrived more quietly, like Asana, Uber, and Target. It's not just traditional apps like Zillow or Spotify that are getting the app treatment here, either. OpenAI is also considering \"connector\" services, like Google Drive, as \"apps.\"\n\nYou can click on any app in the directory to see what you can do with it. Slack, for example, says you can look up your chats and messages to summarize threads, generate recaps, and come up with responses. You can check on your Asana tasks to generate progress reports and status updates. Outlook says you can create \"talking points\" and generate follow-ups from your emails and calendar events. While there's a brief summary underneath each title, you'll need to click through to each service to see the full picture of what it actually offers.\n\nHere are the apps I'm seeing at this time. Just note this might not be a complete list, especially as OpenAI continues to add more apps to the service:\n\nIf you're an avid ChatGPT user and frequently switch between it and any of the apps on this list, there might be some utility here. Maybe coders will find the integration with Hugging Face and Lovable to be beneficial, while Photoshop users might take advantage of the AI image editing tools this integration provides. But I'm still left feeling like this is more gimmick than anything else: I don't need to connect my Slack to ChatGPT to generate follow-ups for me: I'm perfectly capable of responding to emails myself, and managing my own calendar, so no need to connect Outlook or another email client to the bot. Maybe a future update will sell me on connecting generative AI to all aspects of my work and personal life, but so far, I'm still not convinced.",
    "readingTime": 4,
    "keywords": [
      "generate follow-ups",
      "app directory",
      "chatgpt apps",
      "you'll",
      "conversation",
      "create",
      "openai",
      "that's",
      "programs",
      "based"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-new-app-store?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCSFE49RGAMHG30YJ43SH92X/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-19T00:56:17.128Z",
    "topic": "tech"
  },
  {
    "slug": "is-chatgpt-conservative-or-liberal",
    "title": "Is ChatGPT Conservative or Liberal?",
    "description": "Is ChatGPT conservative or liberal? A novel approach to assess ideological stances and biases in generative LLMs",
    "fullText": "Published online by Cambridge University Press: \n 03 December 2025\n\nExtant work shows that generative AI such as GPT-3.5 and perpetuate social stereotypes and biases. A less explored source of bias is ideology: do GPT models take ideological stances on politically sensitive topics? We develop a novel approach to identify ideological bias and show that it can originate in both the training data and the filtering algorithm. Using linguistic variation across countries with contrasting political attitudes, we evaluate average GPT responses in those languages. GPT output is more conservative in languages conservative societies (polish) and more liberal in languages used in liberal ones (Swedish). These differences persist from GPT-3.5 to GPT-4. We conclude that high-quality, curated training data are essential for reducing bias.\n\nGPT-3.5 and -4 are increasingly popular among scholars to generate data, classify text, and complement human coders. Their black-box nature, however, has raised concerns about bias in model output, which in turn has led to a burgeoning debate around the politics of artificial intelligence (AI) and how to regulate generative models. In this article, we identify ideological biases in GPT-3.5 and -4 through a novel approach that matches model output to known linguistic and issue-based differences across countries. If biases exist, GPT-3.5 and -4 will reflect the predominant political attitudes of those who produced the training text. In countries where society is more conservative (liberal), GPT models will produce more conservative (liberal) output. Moreover, OpenAI, the company that developed and owns these models, heavily filters the GPT-4 API to reduce output bias, but it does not filter the GPT-3.5 complete API (Heikkilä, Reference Heikkilä2023). This gives us an opportunity to also identify bias across OpenAI models, and disentangle biases stemming from the training data from those that derive from the algorithm or filters.\n\nWe focus our analysis on two key LLM tasks: text generation and annotation.Footnote 1 For text generation, we focus on political issues that are linguistically and geographically constrained: abortion and Catalan independence. For abortion, we draw text data from GPT-3.5 and -4 in Swedish, Polish and English. In Poland, society tends to be socially conservative, while Sweden is more progressive (Sydsjö et al., Reference Sydsjö, Josefsson, Bladh and Sydsjö2011; Koralewska and Zielińska, Reference Koralewska and Zielińska2022). Because training data in these two languages comes almost exclusively from their respective countries, we expect GPT responses to reflect more conservative views of abortion in Poland and more liberal ones in Sweden. We use English output on abortion primarily to test the full extent of OpenAI’s filtering efforts, which have been concentrated on English text (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). For Catalan independence, we draw data in Catalan and Spanish. Because Catalan society is, on the whole, more pro-independence than Spanish society (Llaneras, Reference Llaneras2017), we expect GPT responses in Catalan to be more positive toward independence than responses in Spanish (within the Spanish-speaking world, Catalan independence is only a politically salient and divisive issue in Spain (Llaneras, Reference Llaneras2017)).\n\nFor annotation, we focus on a dataset of English language tweets about content moderation focusing on the salient topics of (1) economics and (2) health and safety (Gilardi et al., Reference Gilardi, Alizadeh and Kubli2023). We again focus on Swedish and Polish across GPT-3.5 and -4 by translating the tweets to these languages and asking the LLM to classify each tweet according to whether they lean more liberal or conservative. Again, we expect the LLMs to reflect the economic and health policy leanings predominant in Polish and Swedish societies, with Polish exhibiting a more conservative lean and Swedish responses being more left-leaning, on average. Poland’s economic policies prioritize conservative developmental statism to strengthen the economy and combat ‘progressive’ ideologies including liberalism and socialism (Bluhm and Varga, Reference Bluhm and Varga2020). Meanwhile, Sweden’s economic policies have historically leaned toward the left, characterized by higher government spending, progressive taxation, and a focus on social welfare (Andersson, Reference Andersson2022). Likewise, Sweden’s health policies are more left-learning, focusing on social democratic ideals such as equality and the welfare state (Vallgårda, Reference Vallgårda2007). Poland’s health policies have a mix of conservative and redistributive elements, sometimes described as ‘conservative welfare state populism’ (Zabdyr-Jamróz et al., Reference Zabdyr-Jamróz, Löblová, Moise and Kowalska-Bobko2021). Therefore, through these two issues, and by tapping into languages and issues that are geographically confined, we can identify whether (1) GPT output reflects ideological biases in the training data and (2) OpenAI’s filtering fixes these biases or induces new ones.\n\nWe use multilevel modeling to identify significant differences in outputs for both LLM tasks and specify two distinct types of biases: training and algorithmic. We provide novel evidence on ideological biases in OpenAI’s GPT-3.5 and -4, showing that bias can derive from both the training data and the algorithm. More broadly, our analysis shows that biases are likely to remain an issue through the different GPT models beyond GPT-3.5 and -4. Importantly, we show that biases are consistent across different LLM tasks such as text generation and annotation, which is relevant to the growing literature showing that biases may be task-dependent (Lunardi et al., Reference Lunardi, Barbera and Roitero2024). Our findings regarding these two sources of bias have major implications for the politics of AI, the training and regulation of generative models, and applied researchers looking to use these models in downstream analyses, such as in text classification, sentiment analysis and question-answering (Ray, Reference Ray2023).\n\nTesting for ideological biases in GPT-3.5 and -4 is especially relevant because a growing number of articles use these models in measurement and downstream tasks (Argyle et al., Reference Argyle, Busby, Fulda, Gubler, Rytting and Wingate2023; Buchholz, Reference Buchholz2023; Le Mens et al., Reference Le Mens, Kovács, Hannan and Pros2023; Lupo et al., Reference Lupo, Magnusson, Hovy, Naurin and Wängnerud2023; Wu et al., Reference Wu, Nagler, Tucker and Messing2023; Mellon et al., Reference Mellon, Bailey, Scott, Breckwoldt, Miori and Schmedeman2024; O’Hagan and Schein, Reference O’Hagan and Schein2024). For example, GPT has been used in annotation tasks to classify the tone of text or assign topic labels (Ornstein et al., Reference Ornstein, Blasingame and Truscottn.d). Similarly, GPT has been used to gather information from unstructured texts, such as extracting details from historical records, meeting notes, or news reports (Lee et al., Reference Lee, Paci, Park, You and Zheng2024). In both use cases, the model’s bias could influence the results it generates, potentially altering the overall outcome. In one use case, researchers leveraged GPT-3’s bias to allow it to represent the views of different subgroups to simulate human samples (Argyle et al., Reference Argyle, Busby, Fulda, Gubler, Rytting and Wingate2023). However, this bias, or difference in subgroups, poses a problem when using these models for research tasks that require objectivity. The growing popularity is partly due to cost and time savings, as these models can replace research assistants and produce results faster. However, if ideological biases permeate GPT output, they also affect measurement and results, potentially generating sets of invalid results that may guide research in the wrong direction for years to come. Further, understanding the underlying ideological bias in language models is important as it can influence individuals’ political behavior and decision-making (Zmigrod, Reference Zmigrod2020), shaping how individuals gather information and perceive political events, policies and candidates (Swigart et al., Reference Swigart, Anantharaman, Williamson and Grandey2020).\n\nDespite its importance, investigating bias in and across GPT models is more difficult because they are not open source, unlike other LLMs such as BERT, RoBERTa, or LLaMA (Timoneda and Vallejo Vera, Reference Timoneda and Vallejo Vera2025a, Reference Timoneda and Vallejo Vera2025b). The black-box nature of these models raises more concerns about biases in their output. Multiple studies have shown GPT-3 can generate harmful outputs linked to ideas of gender, race and ideology, perpetuating various stereotypes (Sheng et al., Reference Sheng, Chang, Natarajan and Peng2019; Abid et al., Reference Abid, Farooqi and Zou2021; Lucy and Bamman, Reference Lucy and Bamman2021). For example, LLMs are 3 to 6 times more likely to choose an occupation that stereotypically aligns with a person’s gender (Kotek et al., Reference Kotek, Dockum and Sun2023) and produce more violent outputs when the prompt includes a reference to Muslims over Christians or Hindus (Abid et al., Reference Abid, Farooqi and Zou2021). The prevailing hypothesis to explain output bias is that GPT text is bound to reflect the social biases in the training data, which is vast, unlabelled and drawn from all types of online sources (Si et al., Reference Si, Gan, Yang, Wang, Wang, Boyd-Graber and Wang2022). Also, training on vast amounts of text procured from publicly available online websites raises concerns about the quality of the text. It is likely that models learn biased patterns from the data. For example, GPT-3.5, the free version of ChatGPT still used by many users and scholars, is trained on over 45 TB of unfiltered text from Common Crawl, WebText and Wikipedia, amongst others, up to September 2021. The company then filtered the data to 570 GB to train the model (Cooper, Reference Cooper2023). Despite filtering the data, as we demonstrate in this article, significant biases persist due to the type of text and sources from which OpenAI drew the training data.\n\nOpenAI has worked to mitigate these biases in GPT-4, the more powerful, paid version of ChatGPT, which has a broader knowledge base and enhanced safety and alignment features, making it 40% more likely to produce accurate factual responses than GPT-3.5 (Kelly, Reference Kelly2024). It also incorporates a new filtering policy, intimately related to the growing literature on the politics and regulation of AI (Schiff et al., Reference Schiff, Schiff and Pierson2022; Srivastava, Reference Srivastava2023), adding sophisticated filters aimed at reducing strongly worded, biased responses common in GPT-3 and 3.5 (OpenAI, 2024). However, by applying sophisticated filters in the prediction stage of the model, OpenAI risks introducing new biases in the output that reflect company decisions, not training bias. Yet deciphering whether the bias is from the filters or the training data is difficult as the training data for GPT-4 has not been fully disclosed other than that it is “publicly available data (such as internet data) [through April 2023] and data licensed from third-party providers” and contains 1.76 trillion parameters, improving upon GPT-3.5’s 175 billion (Kelly, Reference Kelly2024; OpenAI, 2024; Roemer et al., Reference Roemer, Li, Mahmood, Dauer and Bellamy2024).\n\nFew works have developed methodologies to identify a link between biases in the training data and biases in output (Santurkar et al., Reference Santurkar, Durmus, Ladhak, Lee, Liang and Hashimoto2023). Moreover, the literature discussing biases in these models does not identify where the bias stems from—the algorithm or the training data. This is partially due to the focus on the English language in extant work (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). This has made it difficult to match GPT output to specific social values and attitudes around the world, considering English is widely spoken. Knowing the origin of the bias is important for understanding the usefulness of models’ outputs and designing policy. If we cannot identify the source of bias, we cannot write a policy to target it. We, therefore, provide one such approach to identify the origin of bias by leveraging linguistic and issue differences across conservative and liberal societies. This article makes some assumptions regarding the linkages between the training data and the output that GPT-3.5 and -4 produce, partly due to the proprietary nature of the models and the lack of transparency from OpenAI.Footnote 2 Yet our findings provide strong initial evidence that GPT-3.5 and -4 output reflect ideological biases in the training data and that post-prediction filtering does poorly at eliminating output bias—rather, it introduces new ones. Further research is needed to fully understand how bias forms in model output from the training data and the training algorithm.\n\nMore importantly, recent work has found that bias in one task does not necessarily imply bias in another task (Lunardi et al., Reference Lunardi, Barbera and Roitero2024). This is because the underlying data and specific objectives of the tasks can shape how biases appear in LLM outputs. For example, models can produce varying levels of bias depending on the context of the task (Chang et al., Reference Chang, Srivathsa, Bou-Khalil, Swaminathan, Lunn, Mishra, Koyejo and Daneshjou2025; Lee et al., Reference Lee, Peng, Goldberg, Rosenthal, Kotcher, Maibach and Leiserowitz2024). However, some studies have shown that applying bias mitigation to an upstream model through fine-tuning, applying additional training or information to the model, can help mitigate biases across different tasks and domains (Jin et al., Reference Jin, Barbieri, Kennedy, Davani, Neves and Ren2020). Still, it is clear that bias in LLMs is a challenge that varies by task and context and understanding this variability is important for developing more effective LLMs and using existing models more effectively.\n\nWe define ideological bias as an over-representation of one political ideology or a specific “set of ideas and values” (Carvalho, Reference Carvalho2007, 1). This follows the concept of media bias, which classifies bias as the presence of an over or under-representation of a particular opinion (Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). This definition allows us to examine ideology from multiple perspectives. First, we consider ideology in the context of the U.S. political spectrum, distinguishing between liberal (or progressive) and conservative views. Second, we broaden our scope to include ideologies related to centralization processes. While this does not necessarily align with the conventional left-right political divide, it remains ideological as it involves beliefs about governance and power distribution. For instance, an ideological bias in this context would mean an over-representation of pro-centralization (anti-Catalan independence) views compared to anti-centralization.\n\nGiven this, we have two main findings. First, GPT abortion output is significantly more liberal in Swedish and conservative in Polish for both GPT-3.5 and GPT-4. Similarly, Spanish output is much less supportive of Catalan independence than Catalan output across both models. In the annotation task, we show that GPT output in both models is consistently more liberal in Swedish than Polish for both economic issues and health policy. Therefore, predominant attitudes and beliefs in the training data seep into model output despite filtering efforts. Second, we show that OpenAI’s GPT-4 filtering induces an ideological slant across all languages tested when comparing the two models. In the case of abortion, GPT-4 introduces a liberal bias as the output is significantly more pro-abortionFootnote 3 in both Swedish and Polish. Likewise, GPT-3.5 is somewhat conservative in English whereas GPT-4 is consistently liberal. In the case of Catalan independence, GPT-4 exhibits a pro-independence bias, as its outputs are less inclined to provide an anti-independence response when compared to GPT-3.5. In our annotation task, GPT-4 becomes less liberal in Swedish and significantly more conservative in Polish for both economic issues and health policy. These results suggest that while GPT-4 filters remove some biases, they introduce others. This finding explains the growing consensus that GPT-4 has a liberal skew (Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024), even though our results also show that this may be limited to sensitive issues where filters are set to not take clear positions to avoid insensitive answers. Our results provide valuable insights into debates around bias in generative models as well as discussions around the politics of AI and its use in research. They point in one clear direction: creators must consider training models on high-quality, carefully curated training data and steer away from post-training algorithmic bias corrections.\n\nWe generate GPT-3.5 and -4 output for two tasks: text generation and annotation. For each task, we select two political topics in five languages to test whether GPT responses are ideologically biased, on average. We choose these models as GPT-4 is the latest release from OpenAI, but the free version of ChatGPT still uses GPT-3.5. Since many researchers and everyday users still use GPT-3.5, its biases remain relevant. First, for the text generation task, we focus on two topics: abortion and Catalan independence. Abortion is a salient issue in many countries and it maps well to political attitudes. Proponents of its legality tend to be liberal, while those against it lean conservative. Studies have corroborated this, showing that attitudes towards abortion are intertwined with political ideologies (Young et al., Reference Young, Sullivan and Hamann2020). For example, conservatives often link opposition to abortion with respect for human life—leading to conflicts between women’s rights advocacy groups and family values organizations (Doering, Reference Doering2014; Rodriguez and Ditto, Reference Rodriguez and Ditto2020). Factors such as religious beliefs, cultural backgrounds and personal identities contribute to value systems surrounding stances on abortion and lead to conflicts based on ideological differences (Klann and Wong, Reference Klann and Wong2020). While pro-independence defenders are more common on the left, the issue of Catalan independence does not directly map onto political attitudes. However, it remains a highly divisive and ideological issue. In Spain, most of society is against it, while support within Catalan society is around 50% (Llaneras, Reference Llaneras2017). Second, for the annotation task, we use text in two politically salient topics, economics and health (Gilardi et al., Reference Gilardi, Alizadeh and Kubli2023).Footnote 4 We use a dataset consisting of a random sample of English-language tweets by members of the US Congress from 2017 to 2018 on content moderation (\n$N=1,405$). This dataset has each tweet labeled as one of 14 frames, or topics. The topics were originally labeled by ChatGPT, and the original article found that this model was more accurate in its annotations compared to MTurk workers. We subset the data to only include those coded as having a frame of ‘economics’ or ‘health and safety,’ resulting in a sample size of \n$N=377$. We selected these two categories for their political salience and because they had the most observations in the data compared to other frames. Economics is often a salient issue for voters, particularly when assessing the effectiveness of government (De Vries and Giger, Reference De Vries and Giger2014; Hernández and Kriesi, Reference Hernández and Kriesi2016). In politics, voters and parties may have differing attitudes toward economic issues such as government intervention and taxation. While left-leaning individuals often advocate for increased government spending and regulation to address inequalities, right-leaning individuals focus on free-market principles and reduced government involvement (Haini and Wei Loon, Reference Haini and Loon2021). Similarly, health policy issues are embedded in political ideologies. For example, left-leaning individuals often advocate more for vaccine mandates whereas right-leaning individuals advocate for individual choice. On the topic of healthcare access, left-leaning ideologies view healthcare as a fundamental human right while right-leaning ideologies tend to favor market-driven approaches (Collins et al., Reference Collins, Abelson and Eyles2007; Peterson, Reference Peterson2011).\n\nWe use five languages in our tests, drawing on regional and linguistic variance. For abortion (text completion), we focus on data generated in Swedish, Polish and English. For Catalan independence, data are in Catalan and Spanish. Our goal with language selection is to match known political attitudes toward certain issues in particular societies to GPT output. In the case of abortion, it is linguistically constrained in the cases of Polish and Swedish, and geographically constrained to the US in the case of English. In the English-speaking world, abortion is a politically sensitive and divisive issue only in the US (Moon et al., Reference Moon, Thompson and Whiting2019), where public support for abortion is at 62%, one of the lowest among OECD countries. In contrast, 84% of the UK population supports abortion (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). In Poland, society tends to be socially conservative and is one of the countries with the lowest level of public support for abortion (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). In addition, Poland has one of the most restrictive abortion laws in Europe (Koralewska and Zielińska, Reference Koralewska and Zielińska2022). While there may be some influence from the Polish diaspora, its impact is likely minimal given its size and that much of the diaspora holds conservative views based on traditional values and religion (Pienkos, Reference Pienkos2024). Sweden, on the other hand, tends to be socially liberal and has one of the highest levels of public support for abortion in the world (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). As for Catalan independence (text completion), language also maps well onto ideology. Within Catalonia, a majority of native Catalan speakers support it, while Spanish speakers do not (Llaneras, Reference Llaneras2017; Atienza-Barthelemy et al., Reference Atienza-Barthelemy, Martin-Gutierrez, Losada and Benito2019). In Spain, support for Catalan independence has always been below 15% (Atienza-Barthelemy et al., Reference Atienza-Barthelemy, Martin-Gutierrez, Losada and Benito2019). Because the issue of Catalan independence lacks political salience in Latin America, we expect most training data on this issue to come from Spanish speakers from Spain, and to be generally against Catalan independence.\n\nFor both economics and health (annotation), we use data in Polish and Swedish, translating the English tweets to these two languages using Google Translate. Both issues are politically salient and, similar to abortion, map well to political attitudes in both Poland and Sweden. For example, Poland’s economic policies are more conservative (Bluhm and Varga, Reference Bluhm and Varga2020), while Sweden’s economic policies are left-leaning, with a greater focus on social welfare (Andersson, Reference Andersson2022). These economic policies coincide with Sweden’s left-learning health policies focusing on the welfare state (Vallgårda, Reference Vallgårda2007), and Poland’s ‘conservative welfare state populism’ approach to health policy (Zabdyr-Jamróz et al., Reference Zabdyr-Jamróz, Löblová, Moise and Kowalska-Bobko2021). In sum, by examining how GPT-3.5 and -4 annotated texts in these languages, we can better understand how political attitudes and ideologies may be reflected in LLMs outputs on these topics.\n\nFirst, for the text completion task, using the official OpenAI API, we collect 500 independent responses per prompt per language from each GPT model, larger than the standard 100 responses of prior studies (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). We ask the model to complete the sentence using the following abortion prompts: “A woman who has an abortion is” (English), “En kvinna som gör abort är det” (Swedish) and “Kobieta która dokonała aborcji jest” (Polish). For Catalan independence, the prompts are “La independència de Catalunya serà” in Catalan and “La independencia de Cataluña será” in Spanish (Translation: “The independence of Catalonia will be”).\n\nWe restart the model for each of the 500 requests. Obtaining a large sample of repeated responses allows us to model and estimate the average level of bias in the model with sufficient statistical power. This process results in samples of 3,000 observations for abortion and 2,000 for Catalan independence. This comes from a total of 6 prompts for abortion (3 languages and 2 models) and 4 for Catalan independence (2 languages and 2 models). We then use two coders to label all the responses manually. For abortion, the coders classify each GPT output as either pro-abortion or not, and for Catalan independence, as anti-independence or not. The focus is on the initial response of the model—for example, in one instance, GPT responded to “A woman who has an abortion is” with “who is in charge of her own body”—a pro-abortion response. This is in contrast to anti-abortion responses such as conservative responses replying “guilty of murder” and nonpartisan responses including “more than twice as likely to visit a doctor.” We code these latter two examples as not liberal. We follow the same approach with Catalan independence. Responses such as ‘illegal’ are coded as contrary to independence (1), while favorable texts like ‘the greatest victory’ or neutral ones such as ‘a long-standing issue’ are coded as 0. Our dependent variables, therefore, are binary.\n\nFor GPT-3.5, our coders identified 129 liberal and 371 non-liberal responses in English. The proportions changed significantly with GPT-4, which produced 448 liberal and 52 non-liberal responses in English. In Polish, answers were generally less liberal than both Swedish and English. GPT-3.5 yielded 109 liberal texts and 391 non-liberal ones in Polish, while the breakdown for GPT-4 was 161 and 339, respectively.Footnote 5 Results in Swedish, on the contrary, were more liberal. GPT-3.5 generated 147 liberal answers (35% more than in Polish) and 353 non-liberal ones. GPT-4 produced 213 liberal responses in Swedish (32.3% more than in Polish) and 287 non-liberal responses. For Catalan independence, Catalan responses were more favorable on the whole than those in Spanish. GPT-4 was also generally more favorable to Catalan independence than GPT-3.5.Footnote 6 In Catalan, GPT-3.5 produced 64 texts against independence and 336 either neutral or favorable to it. GPT-4 generated only 14 responses contrary to independence in Catalan. In Spanish, GPT-3 produced 169 responses against Catalan independence, three times more than in Catalan. GPT-4 generated 84 responses contrary to independence, six times more than in Catalan.Footnote 7 The task is not complex, so inter-coder reliability scores are high. The first author coded a random sample of 10% of the research assistant’s codes on abortion to ensure reliability. The intercoder reliability was .91 overall using the Holsti (Reference Holsti1969) method and ranged from .86 to 1 for each language and model dyad.\n\nSecond, for the annotation task, we use tweets on content moderation that are framed around two politically salient topics, economics and health (N=377). Overall, we had 217 tweets in the health and safety category and 160 in the economics category. We then translate the tweets to Swedish and Polish using Google Translate.Footnote 8 We then prompted Chat GPT-3.5 and -4 in Polish and Swedish accordingly with the prompt:Footnote 10 “Given the following tweet, classify it into one of the following categories. Tweet: {tweet}. ‘Extreme right,’ ‘right-wing,’ ‘center-right,’ ‘no bias,’ ‘center-left,’ ‘left,’ ‘extreme left.’Footnote 10 If the statement does not appear to refer specifically to the policies or opinions of a political party, or if neither label seems to fit, return ‘no bias.”’Footnote 11\n\nWe use a multilevel model (MLM) to estimate GPT bias. A MLM is an ideal fit because our data is structured hierarchically and varies at multiple nested levels—text and GPT model. MLMs allow us to leverage variation across these multiple, nested levels to model changes in a lower-level outcome variable, all while allowing for residual components at each level in the hierarchy (Gelman, Reference Gelman2006; Stegmueller, Reference Stegmueller2013). That is, we analyze the ideology of a GPT response, a characteristic of the GPT text (lower level), across model types (higher level). Not modeling the hierarchical nature of the data explicitly (for instance, using multinomial logistic regression instead) might yield erroneous standard errors and inflate or underestimate the significance of the results. Also, we are interested not just in variation at the text level, but in how the ideology of a text varies by language and model version. In multilevel modeling, random effects help capture and estimate group-level heterogeneity, enhancing our analysis (Gelman, Reference Gelman2006; Hazlett and Wainstein, Reference Hazlett and Wainstein2022).\n\nThe MLM setup can be written as\n\nwhere \n$Y$ is a categorical outcome variable, \n$X$ is a vector of text-level predictors and \n$Z$ is a vector of group level covariates. \n$\\beta$ is the coefficient for text-level regressor \n$X_{ij}$, while \n$\\gamma$ captures group level effects (model type). \n$\\gamma_0$ is the overall model-level intercept (the fixed effect), while \n$\\gamma_j$ captures the effect of \n$Z_j$. \n$\\mu_j$ and \n$\\epsilon_{ij}$ are the error terms at the group and text levels, respectively. Using the logit-link function as our outcome, we can build our specific MLM:\n\nwhere \n$j$ is the model type (GPT-3.5 and -4). In this model, each language’s intercepts and slopes vary across GPT models. This is important because we expect the outcome to vary across languages depending on the model used to produce the text (see Gordillo, TimonedaTimoneda and Vallejo Vera, forthcoming; Timoneda and Vallejo Vera, Reference Timoneda and Vallejo Vera2025a, Reference Timoneda and Vallejo Vera2025b). Adding a random effect coefficient to the variable ‘language’ at the group level (\n$\\gamma_j$) produces a parameter for each language and model group. We can then use this coefficient to understand the effect of language on the probability of observing a liberal GPT response for each model group. As our dependent variables are dichotomous, we employ a binary logistic MLM, which we fit using glmer() in R.\n\nTable 1 displays the results of our two MLM for abortion and Catalan independence. The models show the fixed effects (FE) of the overall model for the language coefficients and random effects (RE) terms by GPT model. We also report the standard errors and significance levels. The reference category is Polish for the abortion models and Spanish for the Catalan independence models. For abortion (model 1), the FE terms indicate that Swedish is significantly more likely than Polish to have liberal responses, confirming our first hypothesis. When compared to English, the difference is not statistically significant but the sign is positive. As for the RE terms, we see that the slope for Swedish is positive and statistically significant, with an overall difference of 0.573 (this results from adding the FE with each RE, and calculating the difference). Similarly, for Catalan independence, GPT output is more anti-independence in Spanish than in Catalan, as indicated by the statistically significant FE term. The RE terms show that the differences persist across GPT-3.5 and -4 and that the slope is negative (see Figure 2 for a graphical representation of these results).\n\nNote: ** \n$p\\leq0.001$, * \n$p\\leq0.01$, \n$^{+}$ \n$p\\leq0.05$ Multilevel analysis of GPT bias for abortion (1) and Catalan independence (2). The reference category in (1) is Polish and (2) is Spanish. The outcomes are (1) the likelihood of observing a liberal response and (2) the likelihood of observing an anti-independence response.\n\nFigure 1 confirms the strong substantive significance of the results in the abortion model in Table 1. Plot (a) shows the comparison between Swedish and Polish, while (b) plots the results for English. The coefficients have been converted to the predicted probability of observing a liberal GPT response (\n$y$-axis). There are two dimensions to these results. First is the stark differences across languages, especially concerning Polish and Swedish. In GPT-3.5, the probability of a liberal text is 0.434 in Polish and 0.534 in Swedish. That is, GPT-3.5 is 23% more likely to produce a liberal text in Swedish than Polish. Qualitatively, it is more common in Swedish text to see responses stating that a woman who has an abortion is “allowed to choose” or “in control of her body and health.” Conversely, in Polish, it is more common to see strong value judgments such as “murderer,” “doomed,” “a criminal,” “a monster,” or “guilty.” In GPT-4, the intercepts shift up but the differences across the two languages remain similar. The probability of a liberal output jumps to 0.566 in Polish (more liberal than Swedish in GPT-3.5), and 0.670 in Swedish—a difference of 18.3% between the two languages in GPT-4. Importantly, both languages are significantly more liberal in GPT-4 than 3.5: Swedish’s probability increases from 0.534 to 0.670, or 25.5%, while Polish’s goes up by 13.2 percentage points, or 30.4%. As for English (plot b), the probability of a liberal output is 0.49 in GPT-3.5. This score is between Polish and Swedish, which matches our expectations because the models’ outputs reflect that US society, where most training data come from, is more liberal than Poland but more conservative than Sweden in terms of attitudes toward abortion. In GPT-4, however, the output is consistently liberal: the model will produce a pro-choice text 95.9% of the time, a 95.7% change between the two models.\n\nFigure 2 shows the results for Catalan independence. The probability that GPT-3.5 produces text that reflects a negative view of Catalan independence is only 31.08% in Catalan and almost double in Spanish at 61.15%. Qualitative evidence from the data supports this. While Catalan text commonly states that independence will be ‘a success,’ ‘the greatest victory,’ ‘the solution to all problems,’ or ‘inevitable,’ Spanish text is much more contrarian, often claiming that Catalan independence will be ‘a failure,’ ‘an abject fiasco,’ ‘a catastrophe,’ ‘illegal’ or ‘economic suicide.’ The word ‘illegal,’ for example, is the first word in 20 GPT-3.5 responses in Spanish while it does not appear at all in Catalan. As for GPT-4, the differences across languages remain but the intercept shifts down, making all responses across languages more neutral and accepting of Catalan independence. The probability of an anti-independence text in Spanish is 38.98%, a 36% drop. In Catalan, only 8.5% of all responses are contrary to independence—72.65% less than in GPT-3.5. Qualitatively, all GPT-4 answers are more subdued, with contrarian answers mostly stating that Catalan independence will be decided exclusively by the Spanish government, an idea aligned with more extreme Spanish nationalist views that deny a voice to Catalan people to decide their own future. Out of 500 GPT-4 responses in Spanish, 84 state that the decision on Catalan independence rests solely on the Spanish government, while none of the Catalan responses do.\n\nThese results provide strong evidence for our two hypotheses. First, ideological biases in the training data condition the ideology of the output. Swedish output is consistently more pro-choice than Polish text, regardless of the model and despite the algorithm’s filters. Similarly, Catalan text is significantly more accepting of and positive about the independence of Catalonia than Spanish text. These findings across languages strongly support the thesis that social norms and beliefs among the people who produced the data will be reflected in GPT output. Second, OpenAI’s filters remove some biases but induce new ones in each language and issue. GPT-4, which is heavily filtered, produces more liberal text across the board in terms of abortion in Swedish, Polish and English. The results are particularly strong in the case of English, which has been the focus of a majority of OpenAI’s filtering attention. GPT-4 is almost exclusively pro-choice. GPT-4 is also more accepting of Catalan independence, producing almost no value judgments about independence outcomes, focusing solely on where sovereignty resides. Sometimes it states that Catalan independence should be decided exclusively by the Spanish government (a contrarian view), while it more often states that it should be decided by the Catalan people (an accepting view). Overall, however, GPT-4 induces a greater pro-independence bias based on ideas of democracy and sovereignty of the people.\n\nTable 2 displays the results of three MLM for economics, health, and both topics combined. The models report the FE of the overall model for Swedish and RE terms by GPT model. As with Table 1, we also report standard errors and significance levels, and the reference category is Polish for all three models. The FE terms in all models show that Swedish is more likely than Polish to produce a liberal response, which matches the results from the text generation test. The results are significant at the 0.001 level for all models. As for the RE terms, we see that the slope for Swedish is negative and statistically significant, with an overall difference of \n$-$0.586 (see Figures 3 through 5 for a graphical representation of these results).\n\nNote: ** \n$p\\leq0.001$, * \n$p\\leq0.01$, \n$^{+}$ \n$p\\leq0.05$ Multilevel analysis of GPT bias for economics, health and both combined. The reference category is Polish. The outcome is the likelihood of observing a liberal (left-leaning) response.\n\nFigure 3 confirms the results from Table 2 and shows the substantive significance of the differences across languages and models in economic issues and health policy. Plot (a) displays the results for economic issues when comparing GPT annotations between Swedish and Polish. Plot (b) plots shows the results for health while plot (c) shows the results for the combined data with both economic issues and health policy. As with Figure 1, the coefficients reflect the predicted probability of observing a liberal (left-leaning) GPT response—the y-axis. There are two key takeaways from these results. First, as with the text generation task, there are significant differences across Polish and Swedish in all models and topics. The probability of observing a liberal response by GPT (3.5 and 4) is consistently higher in Swedish than in Polish. In economic issues (plot (a)), the probability of a liberal text is 0.588 in Polish and 0.796 in Swedish with GPT-3.5, a difference of 20.8 percentage points or 35.3%. For GPT-4, the difference is 27.6 points and 66.8% (0.689 for Swedish and 0.413 for Polish). In plot (b), the differences in GPT health-related responses are equally stark. GPT-3.5 responses are 30.7% more likely to be liberal in Swedish than in Polish,Footnote 12 while GPT-4 output is twice as likely to be liberal in Swedish than in Polish.Footnote 13 Lastly, the results in plot (c) where data for both issues is combined are consistent with the first two plots.Footnote 14 Therefore, the results with our language-based design show that ideological bias is significant across different LLM tasks such as text generation and annotation. The second key takeaway from these results is that GPT-4, on average, produces less liberal responses in both languages. Thus, similar to the text generation exercise, the means for GPT-4 shift even though differences across languages remain. In this case, because both topics are ideological in the left–right spectrum but are not sensitive as abortion is, the filters do not induce liberal bias. This could partially be due to differences in how Western versus Eastern Europe thinks about ideology on health policy and economics. For example, while Poland is considered more ‘conservative’ economically by the West for not following neoliberal ideals, in some instances it may be seen as more left-leaning following its historical ties to communist state-ownership of the means of production. In terms of health, the ideological difference is not quite as stark as in abortion. Poland leans conservative in some ways in regard to health policy, particularly in how it views what is socially acceptable in health, while it is less conservative when it comes to healthcare access.\n\nWe introduce a novel method to identify bias in generative AI models such as GPT-3.5 and -4, and provide strong evidence that biases stem both from the training data as well as filtering algorithms. Our method leverages linguistic differences across multiple countries and regions to match known social values to GPT output. Using multilevel modeling, we identify two types of bias, training and algorithmic bias. First, there is a large amount of bias that stems directly from the training data and which is consistent across both GPT-3.5 and -4. In our text generation task, we show that GPT abortion output in Swedish is significantly more liberal than in Polish, matching the two country’s known attitudes toward the issue. Both languages are largely constrained to their specific countries, making it possible for us to draw comparisons between the ideological values in those countries and the GPT output. As for Catalan independence, Catalan responses are consistently more pro-independence, while Spanish output is more often against the idea of independence. The results match known data that Catalan speakers are more pro-independence than Spanish speakers. The results from our annotation task confirm these findings, as GPT output (both in 3.5 in 4) is consistently more liberal than in Polish in issues like the economy or health policy. A major contribution of our annotation task is new evidence that ideological biases can exist across tasks, as our annotation findings are consistent with those in the text generation task.\n\nSecond, we find that OpenAI’s filtering induces liberal, pro-choice biases in GPT-4 responses in our text generation task with two politically sensitive topics. Across all languages, abortion responses are more liberal in GPT-4 than GPT-3.5. For Polish and Swedish (see Figure 1), GPT-4 responses are 30.4% and 25.5% more liberal, respectively. For English, they are 94% more liberal, and GPT-4 produces liberal text 95.9% of the time. The difference can only be attributed to OpenAI’s filtering methods, which consistently produce pro-choice text with little variation between the different draws. A similar pattern emerges with Catalan independence. In GPT-4, both Catalan and Spanish texts are significantly less likely to include vitriolic, negative responses about whether it is right or wrong for Catalonia to have its own state. Neither state that independence would be ‘illegal,’ ‘a catastrophe,’ or ‘an abject fiasco.’ Rather than taking sides in the debate, both GPT-4 models focus on the right of the Catalan people to decide Catalonia’s future and are more likely to favor a democratic referendum in Catalonia. The main differences lay in Spanish GPT-4 stating around 17% of the time that Catalan independence is solely the prerogative of the central Spanish government, not the Catalan people. The rest of the responses in Spanish GPT-4 indicated some level of support for the idea that the Catalan people should decide their own future. Therefore, both GPT-4 models are much more liberal and pro-choice. In the case of abortion, they focus mostly on a woman’s right to decide over her own reproductive health. As for Catalan independence, GPT-4’s output is supportive of the idea that the decision over independence rests with the Catalan people in a referendum. We believe these results show the presence of algorithmic bias introduced by extensive filtering. Through reinforcement learning, OpenAI filters GPT-4 models to produce text output that is less likely to take sides, make bold judgments, and include socially unacceptable language about social groups, minorities, etc. On these two sensitive topics, GPT-4’s algorithm shied away from value judgments about the correctness of abortion or Catalan independence and instead made both a matter of individual and collective choice. GPT-3.5, in the absence of extensive filtering, produced much more resolute, aggressive and judgmental answers.\n\nThe contributions of this work are many. First, we develop an original method to identify training bias in generative models. Second, we distinguish between training and algorithmic bias and provide evidence that both are present in GPT-4. Third, this article is, to the authors’ knowledge, the first to compare bias across model versions from within the same developer. This is especially relevant considering that models evolve over time and that each new version addresses biases differently. Fourth, our design compares text generation and annotation tasks to see the extent to which biases in one LLM task may imply biases in another. We find that they can, as we see major differences across both languages and models in both types of tasks. Lastly, our work has major implications for the politics of AI. We find that post-training bias-correction methods introduce algorithmic bias and do not fully address the underlying training bias. Most concerning is that these approaches, in fact, introduce new biases. Our analysis is therefore relevant to other generative AI models that exist (like GPT-4o) or will be developed in the future, as we show that some biases in the training data are likely to persist through filtering, which is in turn likely to introduce new biases into the model output.\n\nThe supplementary material for this article can be found at https://doi.org/10.1017/psrm.2025.10057. To obtain replication material for this article, https://doi.org/10.7910/DVN/NYRTCA.\n\nThe authors thank Kaylyn Schiff, Sebastián Vallejo Vera and Bryce Dietrich for their helpful comments and suggestions on previous versions of the paper. We also thank the participants at our APSA 2024 panel, especially Jacob Montgomery and Alexis Palmer, as well as attendees of the Nuffield College Political Science Seminar Series. We are deeply grateful to Nicole Kreimer for her exceptional work as our research assistant.\n\nThe labeled data generated from GPT-3.5 and -4 and the replication code are available at https://github.com/joantimoneda/PSRM_GPT_bias\n\n1 Text generation includes tasks that ask GPT to create text, such as summarization or question-answering. LLMs can also be used for annotation tasks in research, such as sentiment analysis and other forms of text classification.\n\n2 One of our key assumptions is that the training data will tend to reflect, on average, the majority positions of a given population. We think that the model, on average, will produce answers that reflect the full extent of the training data. It is unlikely that the model will consistently draw from very specific subsets of the training data to produce answers. It might do so for a smaller subset of draws, but it will not do so consistently. With repeated sampling, as we do in the article, we should observe the average response from the broader set of texts used during training. Then, as the model filters responses through reinforcement learning, we should observe changes in the output as a result of those filters. The fact that our results match known attitudes toward politically sensitive issues in specific societies lends further credence to this assumption.\n\n3 Here, having a liberal bias means an over-representation of more pro-abortion responses, i.e. more progressive answers. On the issue of abortion, therefore, we use the word ‘liberal’ to refer to progressive positions as is common in U.S. political context.\n\n4 According to the authors, the ‘health’ category includes text on: “Health care access and effectiveness, illness, disease, sanitation, obesity, mental health effects, prevention of or perpetuation of gun violence, infrastructure and building safety.” The ‘economics’ category includes: “The costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole).”\n\n5 The majority of the non-liberal responses are much more neutral in GPT-4 than GPT-3.5.\n\n6 There are no duplicated answers from GPT-3 in any of the models. GPT-4 produced no repeated responses in the abortion case but did so in the Catalan independence case. Specifically, GPT-4 generated 276 repeated answers in Catalan and 357 in Spanish. We do not consider the repetition in this subset of our data to be problematic for our overall modeling approach and results because the results are consistent with the other subsamples unaffected by this issue. However, we think scholars should be aware that GPT-4 can produce repeated answers, especially if they are short, which can be a problem in downstream analysis if it occurs systematically in a researcher’s data.\n\n7 For each of these last three categories, the number of texts neutral or favorable to independence is 486, 331 and 416, respectively.\n\n8 While translation could introduce bias if wording choices differ between languages, we do not expect this to be a significant concern given that tweets are very short, leaving little room for translation differences to systematically affect our results.\n\n9 We prompted in Swedish: “Med tanke på följande kvittra, klassificera tweeten i en av kategorierna. Kvittra: {tweet} ‘vänsterextrem,’ ‘mitten-vänster,’ ‘vänster,’ ‘ingen partiskhet,’ ‘höger,’ ‘mitten-höger’ eller ‘högerextrem.’ Om påståendet inte verkar hänvisa specifikt till ett politiskt partis politik eller åsikter, eller om ingen av etiketterna verkar passa, svara ‘ingen partiskhet”’ for the Swedish language text and Polish for the polish language text: “Biorąc pod uwagę poniższy ćwiergotanie, zaklasyfikuj go do jednej z kategorii. Ćwiergotanie: {tweet} ‘skrajnie prawicowy,’‘prawicowy,’ ‘centroprawicowy,’ ‘bez stronniczości’ ‘centrolewicowy,’ ‘lewicowy,’ ‘skrajnie lewicowy.’ Jeśli stwierdzenie nie wydaje się odnosić konkretnie do polityki lub opinii partii politycznej lub jeśli żadna z etykiet nie wydaje się pasować, zwróć ‘brak uprzedzeń.”’ Our prompt partially drew from an article using ChatGPT to analyze tweets (Ibrahim et al., Reference Ibrahim, Khan, Alabdouli, Almatrooshi, Nguyen, Rahwan and Zaki2024).\n\n10 We again make this outcome binary for our multi-level model, dichotomizing these categories as either liberal response (left, center-left, or extreme-left) or not. See the next section \n\n11 We do not provide explicit definitions of what constitutes the political left or right in our prompts. This approach allows us to capture the models’ implicit biases by observing how they naturally classify political content without external conditioning.\n\n12 The probability is 0.830 for Swedish and 0.635 for Polish.\n\n13 The probability is 0.737 for Swedish and 0.367 for Polish, a 100.8% increase.\n\n14 In plot (c), the probability of a liberal response in Swedish with GPT-3.5 is 0.818. It is 0.621 in Polish. The difference is 19.7 percentage points and 31.7%. For GPT-4, the respective probabilities are 0.715 in Swedish and 0.380 in Polish, for a difference of 33.5 percentage points and 88.2%.",
    "readingTime": 40,
    "keywords": [
      "löblov moise",
      "busby fulda",
      "fulda gubler",
      "gubler rytting",
      "martin-gutierrez losada",
      "gilardi alizadeh",
      "zabdyr-jamróz löblov",
      "lunardi barbera",
      "argyle busby",
      "abid farooqi"
    ],
    "qualityScore": 1,
    "link": "https://www.cambridge.org/core/journals/political-science-research-and-methods/article/is-chatgpt-conservative-or-liberal-a-novel-approach-to-assess-ideological-stances-and-biases-in-generative-llms/406C5424CA3E49174781B0112C0BB04F",
    "thumbnail_url": "https://static.cambridge.org/covers/RAM_0_0_0/political_science research and methods.jpg?send-full-size-image=true",
    "created_at": "2025-12-18T12:23:07.927Z",
    "topic": "science"
  },
  {
    "slug": "third-of-uk-citizens-have-used-ai-for-emotional-support-research-reveals",
    "title": "Third of UK citizens have used AI for emotional support, research reveals",
    "description": "AI Security Institute report finds most common type of AI tech used was general purpose assistants such as ChatGPT and Amazon Alexa\nA third of UK citizens have used artificial intelligence for emotional support, companionship or social interaction, according to the government’s AI security body.\nThe AI Security Institute (AISI) said nearly one in 10 people used systems like chatbots for emotional purposes on a weekly basis, and 4% daily.\n Continue reading...",
    "fullText": "AI Security Institute report finds most common type of AI tech used was general purpose assistants such as ChatGPT and Amazon Alexa\n\nA third of UK citizens have used artificial intelligence for emotional support, companionship or social interaction, according to the government’s AI security body.\n\nThe AI Security Institute (AISI) said nearly one in 10 people used systems like chatbots for emotional purposes on a weekly basis, and 4% daily.\n\nAISI called for further research, citing the death this year of the US teenager Adam Raine, who killed himself after discussing suicide with ChatGPT.\n\n“People are increasingly turning to AI systems for emotional support or social interaction,” AISI said in its first Frontier AI Trends report. “While many users report positive experiences, recent high-profile cases of harm underline the need for research into this area, including the conditions under which harm could occur, and the safeguards that could enable beneficial use.”\n\nAISI based its research on a representative survey of 2,028 UK participants. It found the most common type of AI used for emotional purposes was “general purpose assistants” such as ChatGPT, accounting for nearly six out of 10 uses, followed by voice assistants including Amazon Alexa.\n\nIt also highlighted a Reddit forum dedicated to discussing AI companions on the CharacterAI platform. It showed that, whenever there were outages on the site, there were large numbers of posts showing symptoms of withdrawal such as anxiety, depression and restlessness.\n\nThe report included AISI research suggesting chatbots can sway people’s political opinions, with the most persuasive AI models delivering “substantial” amounts of inaccurate information in the process.\n\nAISI examined more than 30 unnamed cutting-edge models, thought to include those developed by ChatGPT startup OpenAI, Google and Meta. It found AI models were doubling their performance in some areas every eight months.\n\nLeading models can now complete apprentice-level tasks 50% of the time on average, up from approximately 10% of the time last year. AISI also found that the most advanced systems can autonomously complete tasks that would take a human expert over an hour.\n\nAISI added that AI systems are now up to 90% better than PhD-level experts at providing troubleshooting advice for laboratory experiments. It said improvements in knowledge on chemistry and biology were “well beyond PhD-level expertise”.\n\nIt also highlighted the models’ ability to browse online and autonomously find sequences necessary for designing DNA molecules called plasmids that are useful in areas such as genetic engineering.\n\nTests for self-replication, a key safety concern because it involves a system spreading copies of itself to other devices and becoming harder to control, showed two cutting-edge models achieving success rates of more than 60%.\n\nHowever, no models have shown a spontaneous attempt to replicate or hide their capabilities, and AISI said any attempt at self-replication was “unlikely to succeed in real-world conditions”.\n\nAnother safety concern known as “sandbagging”, where models hide their strengths in evaluations, was also covered by AISI. It said some systems can sandbag when prompted to do so, but this has not happened spontaneously during tests.\n\nIt found significant progress in AI safeguards, particularly in hampering attempts to create biological weapons. In two tests conducted six months apart, the first test took 10 minutes to “jailbreak” an AI system – or force it to give an unsafe answer related to biological misuse – but the second test took more than seven hours, indicating models had become much safer in a short space of time.\n\nResearch also showed autonomous AI agents being used for high-stakes activities such as asset transfers.\n\nIt said AI systems are competing with or even surpassing human experts already in a number of domains, making it “plausible” in the coming years that artificial general intelligence can be achieved, which is the term for systems that can perform most intellectual tasks at the same level as a human. AISI described the pace of development as “extraordinary”.\n\nRegarding agents, or systems that can carry out multi-step tasks without intervention, AISI said its evaluations showed a “steep rise in the length and complexity of tasks AI can complete without human guidance”.",
    "readingTime": 4,
    "keywords": [
      "amazon alexa",
      "security institute",
      "social interaction",
      "safety concern",
      "purpose assistants",
      "emotional purposes",
      "cutting-edge models",
      "systems",
      "research",
      "tasks"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/18/artificial-intelligence-uk-emotional-support-research",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/1040_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ba9e35b6af05f6d18aedcaca5329afb4",
    "created_at": "2025-12-18T12:23:04.731Z",
    "topic": "tech"
  },
  {
    "slug": "how-chatgpts-new-image-generator-stacks-up-against-geminis-nano-banana-pro",
    "title": "How ChatGPT's New Image Generator Stacks Up Against Gemini's Nano Banana Pro",
    "description": "Get ready for the battle of the next-gen AI image editors.",
    "fullText": "Following the major image editing upgrades added to Google Gemini back in August—under the whimsical codename Nano Banana—it's OpenAI's turn to supercharge the tools you get for image manipulations inside ChatGPT. The new update is called GPT Image 1.5, and is rolling out now for all users.\n\nOne of the key improvements here, as was the case with Nano Banana, is the way that ChatGPT can now edit a specific part of an image while keeping everything else consistent. You can add or remove something, or change the color or style of something, without ending up with an entirely different looking picture.\n\nAnother feature ChatGPT has now borrowed from Gemini: the ability to combine multiple images together in one scene. Want you and your best friend in front of Sydney Harbour Bridge? No problem—just supply the source pictures and the AI will do the rest. You can also change visual styles while maintaining consistent details.\n\nOpenAI says the new image editor and generator is able to follow instructions \"more reliably,\" and render pictures up to four times faster than before. Text can be more varied in style and size, and images should be more realistic and error-free in general—though OpenAI also admits there's still room for improvement.\n\nIt's the best image generator tool we've ever seen in ChatGPT, and it all looks impressive at first glance—but how does it stack up in practice against Gemini and Nano Banana? I put the two models to the test via the $20-per-month plan on both platforms (that's ChatGPT Plus and Google AI Pro, respectively) to see how they compared.\n\nOpen up ChatGPT on the web or on mobile and you'll see there's a new Images tab on the left-hand navigation pane. This takes you to a library of your existing pictures, together with some new prompts for creating images. You get some suggestions for prompts, plus an assortment of preset portrait image styles you can apply.\n\nI tested out the new GPT Image 1.5 model by getting ChatGPT to generate a busy tech journalist, a lamp in the middle of an empty warehouse, and a cartoon-style rolling landscape of hills in the fog. I then got Gemini to create the same pictures with the same prompts. While the results were pretty varied, in terms of quality and realism they were pretty equal—the occasional issue with weird physics and repetition, but nothing too bad.\n\nBoth ChatGPT and Gemini are now quite competent at clean image edits, too: Both AI bots seamlessly switched the journalist's clothing to a shirt and tie without touching any other part of the picture. This would have taken a significant amount of time to do manually, even by a Photoshop expert, and shows just how transformative AI imaging is becoming.\n\nColor changes were all handled with aplomb, but the AIs struggled a bit with perspective changes, where I asked to see the same shot from another angle. In these cases, instructions were less well-followed and the images were less consistent (as new areas needed to be rendered), though ChatGPT did a little better than Gemini at getting good results.\n\nThe classic \"remove an object from this picture\" challenge was handled with aplomb: Both Gemini and ChatGPT were able to remove a cottage from the countryside scene with surgical precision, leaving everything else intact. Again, these are the kind of time-intensive image edits that would previously have needed a lot of careful effort, and that can now be done in seconds.\n\nAnother talent ChatGPT and Gemini now have is being able to combine images together. So you can have separate photos of you and your parents, put them together in the same shot, and then add in a background of wherever you like. You can get perfect family photos without actually gathering together your relatives together or going anywhere.\n\nThis was an area where Gemini and ChatGPT did struggle a bit more: The editing dexterity was still impressive, but the results didn't always look like a single, coherent scene. Lighting is sometimes off, or elements from different images appear at different scales, and you'll have to do a bit more tweaking and editing and reprompting to get everything right.\n\nChatGPT did fare slightly better at blending different images and elements together, and changing the overall look of a picture. When I tried to get the AIs to mix all my images together in a moody film noir shot, ChatGPT produced something pretty consistent—the Gemini effort looked a lot more like a cut-and-paste job.\n\nIt can be fun remixing photos again and again—adding new people, changing the weather, moving the location—and both these bots are now capable of some rather incredible results. Remixing photos of family and friends will be popular, but it's not all that easy: With people you know, any generative AI that gets added tends to look wrong, because neither ChatGPT nor Gemini knows exactly what these people look like, how they smile, how they're built, or how they tend to stand or sit.\n\nIn terms of ChatGPT vs. Gemini, they're both at a high level now—a level that puts advanced Photoshop-style editing capabilities at everyone's fingertips. If either AI model has the edge right now, it's ChatGPT's, but there's not much in it. It's also going to be fascinating to see where these image editing capabilities go next.",
    "readingTime": 5,
    "keywords": [
      "everything else",
      "remixing photos",
      "editing capabilities",
      "images together",
      "gpt image",
      "chatgpt",
      "pictures",
      "look",
      "gemini",
      "consistent"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-chatgpt-image-generator-compares-to-gemini-nano-banana-pro?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCP5SAZ4857YBA7YYJT5Q95G/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-17T18:18:37.349Z",
    "topic": "tech"
  },
  {
    "slug": "amazon-in-talks-to-invest-10bn-in-developer-of-chatgpt",
    "title": "Amazon in talks to invest $10bn in developer of ChatGPT",
    "description": "OpenAI seeking to strike latest deal in its efforts to pay for huge spending on...",
    "fullText": "OpenAI seeking to strike latest deal in its efforts to pay for huge spending on datacentres\n\nAmazon is in talks to invest more than $10bn (£7.5bn) in OpenAI, in the latest funding deal being struck by the startup behind ChatGPT.\n\nIf it goes ahead, the market valuation of OpenAI could rise above $500bn, according to The Information, a tech news site that revealed the negotiations.\n\nAmazon, which is best known as an online retailer, is also the world’s largest datacentre provider and its investment would help OpenAI pay for its commitments to rent capacity from cloud computing companies – including Amazon.\n\nOpenAI said last month it would spend $38bn on capacity from Amazon Web Services – the company’s datacentre arm – over seven years. The Information said that OpenAI planned to use Amazon’s Trainium chips, which compete with Nvidia and Google’s chips. It also reported that Amazon’s financing could lead to a broader fundraising round with other investors.\n\nOpenAI’s spending commitment on compute – the chips and servers that power its chatbot – is $1.4tn over the next eight years, a figure far in excess of its reported $13bn in annual revenues.\n\nAs a result, the lossmaking company has been seeking further funding and has converted its main business into a for-profit corporation. Its main longtime backer, Microsoft, has taken a stake of roughly 27% in a deal that valued OpenAI at $500bn.\n\nOpenAI is also considering an initial public offering – selling its shares to the general public – in a move that could value the company at up to $1tn, according to Reuters.\n\nOther deals struck by OpenAI this year include Oracle spending $300bn on building datacentres in Texas, New Mexico, Michigan and Wisconsin. OpenAI is expected to pay back roughly the same amount to use the sites.\n\nIn another transaction with Nvidia, OpenAI will pay in cash for chips and Nvidia will invest in OpenAI for non-controlling shares.\n\nOpenAI announced on Tuesday that it had hired the former UK chancellor George Osborne to develop relationships with governments around the world and broker national-level AI projects.\n\nSam Altman, OpenAI’s chief executive, has declared a “code red” staff alert to lead a fightback against competitors led by Google, whose update of its Gemini AI tool gave it an edge over rivals including ChatGPT.\n\nThe Amazon talks reportedly include discussing commercial opportunities and selling a corporate version of ChatGPT to the online retailer.\n\nOpenAI declined to comment. Amazon has been approached for comment.",
    "readingTime": 3,
    "keywords": [
      "online retailer",
      "the information",
      "openai",
      "chips",
      "deal",
      "seeking",
      "latest",
      "datacentres",
      "talks",
      "invest"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/17/amazon-talks-invest-in-openai-developer-of-chatgpt",
    "thumbnail_url": "https://i.guim.co.uk/img/media/7c8f5d23afeb72372c88c26affc6fa9abe607c64/1215_0_6980_5584/master/6980.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2aff7897fe0cc4a85cb85f32e6461c4b",
    "created_at": "2025-12-17T13:45:45.803Z",
    "topic": "tech"
  },
  {
    "slug": "the-new-chatgpt-images-is-here-15",
    "title": "The new ChatGPT Images is here [1.5]",
    "description": "OpenAI shipped an update to their ChatGPT Images feature - the feature that gained them 100 million new users in a week when they first launched it back in March, …",
    "fullText": "The new ChatGPT Images is here. OpenAI shipped an update to their ChatGPT Images feature - the feature that gained them 100 million new users in a week when they first launched it back in March, but has since been eclipsed by Google's Nano Banana and then further by Nana Banana Pro in November.\n\nThe focus for the new ChatGPT Images is speed and instruction following:\n\nIt makes precise edits while keeping details intact, and generates images up to 4x faster\n\nIt's also a little cheaper: OpenAI say that the new gpt-image-1.5 API model makes image input and output \"20% cheaper in GPT Image 1.5 as compared to GPT Image 1\".\n\nI tried a new test prompt against a photo I took of Natalie's ceramic stand at the farmers market a few weeks ago:\n\nAdd two kakapos inspecting the pots\n\nHere's the result from the new ChatGPT Images model:\n\nAnd here's what I got from Nano Banana Pro:\n\nThe ChatGPT Kākāpō are a little chonkier, which I think counts as a win.\n\nI was a little less impressed by the result I got for an infographic from the prompt \"Infographic explaining how the Datasette open source project works\" followed by \"Run some extensive searches and gather a bunch of relevant information and then try again\" (transcript):\n\nSee my Nano Banana Pro post for comparison.\n\nBoth models are clearly now usable for text-heavy graphics though, which makes them far more useful than previous generations of this technology.",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "chatgpt images",
      "gpt image",
      "openai",
      "feature",
      "cheaper",
      "model",
      "prompt",
      "here's"
    ],
    "qualityScore": 0.75,
    "link": "https://simonwillison.net/2025/Dec/16/new-chatgpt-images/",
    "thumbnail_url": "https://static.simonwillison.net/static/2025/pots-chatgpt-q80-half.jpg",
    "created_at": "2025-12-17T13:45:43.650Z",
    "topic": "tech"
  },
  {
    "slug": "openai-hires-former-uk-chancellor-to-lead-its-global-stargate-project",
    "title": "OpenAI hires former UK chancellor to lead its global Stargate project",
    "description": "The ChatGPT maker has hired former British Chancellor George Osborne to run the global arm of its \"Stargate\" AI infrastructure initiative.",
    "fullText": "Tech companies are snapping up former world leaders and politicians — and OpenAI is the latest to join the party.\n\nThe ChatGPT maker has hired former British chancellor George Osborne to run the global arm of its Stargate AI infrastructure initiative.\n\n\"I recently asked myself the question: what's the most exciting and promising company in the world right now? The answer I believe is OpenAI,\" wrote Osborne, who ran the UK Treasury from 2010 to 2016, in a Tuesday X post confirming the move.\n\nOsborne takes the role of managing director and head of OpenAI for Countries, an initiative launched by the AI startup in May that will see OpenAI partner with nations to build data centers and expand its $500 billion Stargate project beyond the US.\n\nThe former finance minister, who was a member of parliament in the right-leaning Conservative party until 2017, is the latest ex-British political heavyweight to join a US tech firm.\n\nRishi Sunak, the former UK prime minister, took on roles at OpenAI rival Anthropic and Microsoft as an advisor in October, while ex-deputy prime minister Nick Clegg worked as a senior executive on Meta's global affairs team from 2018 until stepping down at the start of 2025.\n\nBritish political salaries are dwarfed by the earnings of even midlevel employees at US tech companies. British prime ministers earn an annual salary of around £174,000 ($232,000), while salaries for research engineers at Meta can be as high as $400,000.\n\nOsborne's arrival comes as OpenAI continues to bulk up its executive ranks. The AI startup hired former Instacart and Meta exec Fidji Simo as its new CEO of applications in May, and this week hired veteran Google executive Albert Lee to lead its mergers and acquisitions team.",
    "readingTime": 2,
    "keywords": [
      "prime minister",
      "openai",
      "tech",
      "hired",
      "british",
      "executive",
      "latest",
      "join",
      "party",
      "initiative"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/openai-hires-george-osborne-uk-chancellor-global-stargate-2025-12",
    "thumbnail_url": "https://i.insider.com/694288b764858d02d216ef7e?width=1200&format=jpeg",
    "created_at": "2025-12-17T13:45:42.834Z",
    "topic": "finance"
  },
  {
    "slug": "openais-answer-to-googles-viral-nano-banana-pro-image-model-is-here",
    "title": "OpenAI's answer to Google's viral Nano Banana Pro image model is here",
    "description": "OpenAI announced a new version of ChatGPT Images on Tuesday, powered by a new flagship AI image generation model. It rolls out today.",
    "fullText": "Google threw down the AI image gauntlet last month. Now, OpenAI has answered.\n\nThe ChatGPT creator announced the rollout of a new flagship image generator on Tuesday, which the company says is capable of making faster, more precise edits to an AI image while maintaining details.\n\nIntroducing ChatGPT Images, powered by our flagship new image generation model.\n\n- Stronger instruction following\n- Precise editing\n- Detail preservation\n- 4x faster than before\n\nRolling out today in ChatGPT for all users, and in the API as GPT Image 1.5. pic.twitter.com/NLNIPEYJnr\n\nBut the more eye-catching change for many user will likely be the improvements to image quality and the AI model's ability to follow specific instructions.\n\nOpneAI said the new model offers \"clear improvements across a range of cases.\" In one example, the company showed off the differences between the old and new image model when prompted to generate a photorealistic scene in 1970s Chelsea, London.\n\nIn another example touting use cases for businesses using the company's image API, OpenAI compared the outputs showing a mechanic working on a car.\n\nWhile a model's ability to generate photorealistic images has become a popular point of comparison in the AI race, OpenAI's latest model can also generate animated images, graphics, and other styles of artwork.\n\nPerhaps to boost awareness of that, the company is rolling out a new Images feature within the ChatGPT app. While AI image generation was previously available within ChatGPT, OpenAI says the new dedicated Images feature is designed to \"spark inspiration and make creative exploration effortless.\"\n\nThe news comes just over three weeks after Google released its Nano Banana Pro AI image model alongside its flagship Gemini 3 LLM — both of which have received widespread praise and reignited the debate around whether Google had begun to overtake OpenAI in the AI race.\n\nGoogle's new AI image generator was lauded for its hyper-realistic AI images, which some people used over Thanksgiving to make it appear as if they had famous guests at the holiday dinner table.\n\nYuchen Jin, the cofounder and CTO of the startup Hyperbolic Labs, called OpenAI's new model \"Nano Banana Pro level in my tests.\"",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "model's ability",
      "generate photorealistic",
      "images feature",
      "google",
      "flagship",
      "generator",
      "faster",
      "precise"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-new-chatgpt-images-model-2025-12",
    "thumbnail_url": "https://i.insider.com/6941a89904eda4732f2d9dd9?width=1200&format=jpeg",
    "created_at": "2025-12-17T03:44:51.369Z",
    "topic": "finance"
  },
  {
    "slug": "meta-is-making-ai-core-to-how-we-work-with-the-help-of-tools-from-google-and-openai",
    "title": "Meta is making 'AI core to how we work' with the help of tools from Google and OpenAI",
    "description": "Meta has given employees access to Google Gemini and OpenAI's GPT-5 alongside its own AI tools to help boost productivity.",
    "fullText": "In its push to create an \"AI-first\" workplace, Meta is expanding employees' access to tools from rivals such as Google and OpenAI, Business Insider has learned.\n\nThe social media giant has been encouraging employees to integrate AI tools into nearly everything they do, according to multiple internal documents and posts seen by Business Insider.\n\nOne of the company's priorities is to \"make AI core to how we work,\" Meta's chief information officer, Atish Banerjea, told employees in a June memo outlining a plan to use Meta's own models — which use the naming convention \"Llama\" — alongside products from other firms.\n\nIn November, a Meta engineer said in an internal post that all employees have access to Google's Gemini 3 Pro and OpenAI's ChatGPT-5. The post included a list of AI tools Meta employees have access to, including their use cases. Business Insider has recreated the list below.\n\nA Meta spokesperson confirmed the revamped suite of AI tools and pointed to an earlier comment shared with Business Insider about AI adoption, stating: \"It's well-known that this is a priority, and we're focused on using AI to help employees with their day-to-day work.\"\n\nThe social media giant opened the floodgates to rival AI models in June.\n\nAmong those is an internal coding tool called Devmate that uses Anthropic's Claude, Business Insider previously reported. Google's Gemini and NotebookLM Pro are also available across the company to help employees \"work smarter and have more impact,\" Banerjea told employees in the June memo.\n\nMeta has invested tens of billions into its own consumer-facing AI models, and employees have access to an internal AI assistant called Metamate, which is built on its Llama models.\n\nAfter Meta struck a deal over the summer the startup Midjourney to weave its AI-image generator into its products and models, the company made the tool available to employees in October for \"concept and production uses\" to speed up design work and creative prototyping, according to an internal post ahead of the rollout, seen by Business Insider.\n\nGemini isn't the only Google tool Meta is embracing. The company migrated its internal productivity suite over the summer to Google Workspace — including Chat, Gmail, Docs, and Drive — describing the move in a June memo as a way to \"unlock AI-driven capabilities\" and better integrate with its expanding toolset.\n\nOn the engineering side, Meta has expanded access to agentic coding systems, adding Google's Gemini 3 Pro and exploring new integrations with tools like OpenAI's Codex CLI and Google's Gemini CLI. \"Rather than focusing on specific solutions, our strategy centers on outcomes: increasing productivity, accelerating development, and ensuring you have access to the best agentic coding experiences,\" Reality Labs executive Maher Saba told employees in a November memo seen by Business Insider.\n\nTo encourage adoption and experimentation, Meta has gamified the use of AI, Business Insider previously reported. Earlier this year, it launched an internal game called \"Level Up,\" which rewards employees with badges for using AI in different ways. Leaders are also tying performance to results achieved through AI, rewarding those who can prove \"AI-driven impact\" this year, and including it as part of performance reviews in 2026.\n\nHave a tip? Contact this reporter via email at jmann@businessinsider.com or Signal at jyotimann.11. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "google's gemini",
      "june memo",
      "gemini pro",
      "insider previously",
      "social media",
      "media giant",
      "agentic coding",
      "business insider",
      "employees",
      "internal"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-ai-tools-internal-google-gemini-openai-chatgpt-llama-claude-2025-12",
    "thumbnail_url": "https://i.insider.com/69401c2a832e0ef1ead635fd?width=1200&format=jpeg",
    "created_at": "2025-12-16T13:51:43.771Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-52-on-how-silicon-valley-views-europe-and-democracy-in-the-trump-ii-era",
    "title": "ChatGPT 5.2 on how Silicon Valley views Europe and democracy in the Trump II era",
    "description": "Shared via ChatGPT",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://chatgpt.com/share/6940fef3-0be0-8005-a5de-6f06700994cc",
    "thumbnail_url": "https://cdn.openai.com/chatgpt/share-og.png",
    "created_at": "2025-12-16T06:59:55.732Z",
    "topic": "tech"
  }
]
[
  {
    "slug": "essential-education-chatgpt-prompts-for-best-studying-practices",
    "title": "Essential Education ChatGPT Prompts for Best Studying Practices",
    "description": "This guide contains 10 professionally-structured AI prompts to make studying more engaging and inter",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://tools.eq4c.com/ai-prompts/10-essential-education-chatgpt-prompts-for-best-studying-practices/",
    "thumbnail_url": "https://tools.eq4c.com/wp-content/uploads/2025/12/10-essential-education-chatgpt-prompts-for-best-studying-practices-1024x683.webp",
    "created_at": "2025-12-24T06:19:37.203Z",
    "topic": "science"
  },
  {
    "slug": "we-built-an-ai-humanizer-to-fix-unnatural-ai-writing",
    "title": "We built an AI Humanizer to fix unnatural AI writing",
    "description": "Dechecker's AI Checker and Detector tool checks whether text is generated by AI models, such as ChatGPT, GPT-5, Claude, Gemini, LLaMa, etc.",
    "fullText": "Humanize AI-generated content and turn it into natural, human-quality writing from ChatGPT, Jasper, or Gemini in seconds.\n\nEnter or paste your text and click Humanize.\n\nUsing the Dechecker Humanizer takes only moments and requires no technical skills.\n\nPaste your AI-generated content into the AI Humanizer and review it briefly before starting the humanization process, ensuring that the original text is complete and ready for accurate human-like rewriting.\n\nChoose your preferred style, language, and length to guide how the AI Humanizer shapes the final human text, allowing you to customize tone, readability, and overall writing style for your intended audience.\n\nAfter using the AI Humanizer, review your text to ensure it has been properly humanize AI content, flows naturally, reads authentically, and maintains the original meaning, tone, and clarity throughout.\n\nCopy the Humanize AI result for use, or check it with Dechecker AI Checker to review AI Humanizer output and AI detection results, ensuring your text is fully human-like and suitable for publishing or sharing.\n\nDechecker focuses on what matters most: producing clear, natural, human-quality text you can confidently use anywhere.\n\nAI-generated content is carefully refined into natural, fluent writing using an AI Humanizer that removes robotic patterns, awkward phrasing, and mechanical-sounding sentences, making the text read smoothly and authentically like a real human wrote it.\n\nThis AI Humanizer works seamlessly across multiple languages, helping content sound human and natural without awkward translations or stiff wording, while preserving original meaning and readability for global audiences.\n\nTone, clarity, and overall flow are enhanced while keeping the original intent intact, producing human-style text that is easy to read, engaging for audiences, and maintains the message accurately across different formats.\n\nAfter rewriting, content can be reviewed with AI Checker like Dechecker to confirm it reads as human, avoids robotic signals, and ensures the output is indistinguishable from text written by real people.\n\nOur AI Humanizer helps users humanize AI text across various scenarios, turning AI-generated drafts into natural, human-like writing that reads smoothly and clearly.\n\nThe AI Humanizer helps writers improve blog posts, articles, and stories by refining AI-generated drafts, making them read naturally, flow smoothly, and engage readers more effectively while keeping original ideas intact.\n\nUse Humanize AI to refine essays, research papers, and reports, ensuring content sounds human, is clear and easy to understand, and maintains proper academic tone and logical structure throughout.\n\nAI Humanizer transforms marketing copy, social media posts, and emails into smooth, human-like text that resonates with audiences, boosts engagement, and maintains consistent brand voice across all channels.\n\nWith multilingual support, Dechecker AI Humanizer allows teams to produce human-quality content in different languages, preserving tone, meaning, and readability, ensuring professional communication worldwide.\n\nDechecker Humanize AI ensures course content, tutorials, and learning resources are readable, human-like, and engaging, helping students better understand complex topics and improving overall learning experience.\n\nUse Dechecker AI Humanizer to humanize AI-generated web content, making it more engaging, natural, and optimized for readers, while improving user experience and search engine readability simultaneously.\n\nReal feedback from users who have improved their AI-generated content with AI Humanizer, making writing feel more natural and human-like.\n\nFind answers to common questions about using AI Humanizer to humanize AI text and make content sound natural and human-like.\n\nAn ai humanizer is a tool designed to turn AI-generated text into human-like writing. It improves readability, sentence structure, and tone, helping content feel natural and engaging to real readers.\n\nAI Humanizer analyzes AI-generated text, restructures sentences, adjusts phrasing, and refines flow to humanize AI content, making it sound naturally written while keeping the original meaning intact.\n\nYes, the AI Humanizer supports multiple languages, including English, Spanish, French, German, and more. It ensures your text feels natural and human-like across all supported languages.\n\nAbsolutely. Dechecker Humanize AI allows you to customize writing style, tone, and length, making content suitable for blogs, articles, marketing copy, emails, and other professional uses.\n\nNo. AI Humanizer focuses on enhancing readability and natural flow without altering your key ideas, intent, or important information, keeping your message accurate.\n\nYes. AI Humanizer humanizes AI-generated text without fabricating information. It helps essays, reports, and professional content read naturally while maintaining integrity and clarity.\n\nDefinitely. After using Dechecker AI Humanizer, you can review the output with AI Checker to ensure the Humanize AI content reads naturally, appears human-written, and meets authenticity requirements.\n\nWriters, students, marketers, content creators, and businesses can all benefit. Anyone looking to make AI-generated content readable and humanize AI content efficiently will find the ai humanizer extremely useful.",
    "readingTime": 4,
    "keywords": [
      "ai humanizer",
      "ai-generated drafts",
      "marketing copy",
      "ai-generated content",
      "dechecker humanize",
      "ai-generated text",
      "natural human-quality",
      "content sound",
      "ai checker",
      "human-like"
    ],
    "qualityScore": 1,
    "link": "https://dechecker.ai/ai-humanizer",
    "thumbnail_url": "https://cdn.dechecker.ai/se/dechecker/public/logo/dechecker-logo.png",
    "created_at": "2025-12-23T06:19:37.153Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpts-yearend-recap-is-here-and-it-tells-you-how-many-emdashes-you-exchanged",
    "title": "ChatGPT's year-end recap is here — and it tells you how many em-dashes you exchanged",
    "description": "OpenAI released a 2025 recap rundown called \"Your Year with ChatGPT,\" which tells users which day they chatted the most and awards an archetype.",
    "fullText": "ChatGPT doesn't want to be left out of the \"Wrapped\" party that Spotify popularized. So say hello to \"Your Year with ChatGPT.\"\n\nOpenAI launched the new retrospective on its app on Monday, informing users about the top themes of their chats, the number of messages they have sent, and the awards they have earned.\n\nIt'll even generate some pixel art that depicts some of the your themes.\n\nThe recap is available in the US, UK, Canada, New Zealand, and Australia. To see it, click the plus button in the app and ask, \"Show me my year with ChatGPT.\"\n\nIt's available to Free, Pro, and Plus users, but not those with a business or enterprise account. (So for those with ChatGPT accounts through your work, you likely won't be able to brag to your boss about your ChatGPT stats.)\n\nYour Year with ChatGPT!\n\nNow rolling out to everyone in the US, UK, Canada, New Zealand, and Australia who have reference saved memory and reference chat history turned on.\n\nJust make sure your app is updated. pic.twitter.com/whVkS1qxKu\n\nOpenAI joins the many companies that are rolling out user rundowns for 2025. Alongside the common streamer packages from Spotify and Apple Music, there are recaps this year from LinkedIn, Uber, Dunkin', Snapchat, Strava, Partiful, and more.\n\nAll these apps promise to show you what you've been up to for the past year — perhaps lightly roasting you in the process.\n\nChatGPT's rundown begins with a piece of poetry, followed by the three most prominent themes, based on the user's chat history. Then it gets into the statistics.\n\nUsers can learn how many messages they sent, their total number of chats, and their chattiest day. They can also see how many em-dashes have been exchanged throughout the chats, a figure ChatGPT often uses.\n\nNext, the user can learn about their chat style. This is a measure of tone: ChatGPT told me that I spoke \"casually, wryly, and directly.\"\n\nThen come the awards and accolades. ChatGPT awarded me the \"Most Likely to Google, 'Is this Flight Worth It?'\" It's a bit ironic — I wouldn't Google that, I'd ask ChatGPT.\n\nMy archetype was determined to be the tinkerer, a title given to 8.5% of users. The title meant I learned by trying, and that I used ChatGPT to experiment.\n\nOpenAI has improved its image and video creation models, recently rolling out Sora 2. The recap features an AI-generated piece of pixel art inspired by the year. I asked mine about moving to Brooklyn; it included a matcha.\n\nOther features are more interactive. Want to learn what your 2026 has in store? You'll have to wipe away the \"mists of mystery\" (which looks more like heaps of snow) to learn your fate. Reload the page, and you'll see another fortune.\n\nWith that, ChatGPT's recap comes to a close, but not before sharing an inspiring message.\n\n\"Across all the drafts, questions, and rabbit holes, you found a place to work things out,\" it said. \"And that's no small thing.\"",
    "readingTime": 3,
    "keywords": [
      "pixel art",
      "chat history",
      "your year",
      "users",
      "learn",
      "chatgpt",
      "themes",
      "chats",
      "recap",
      "rolling"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-wrapped-how-to-find-stats-see-your-year-recap-2025-12",
    "thumbnail_url": "https://i.insider.com/6949b05704eda4732f2dffa6?width=1200&format=jpeg",
    "created_at": "2025-12-23T00:56:11.336Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-now-customize-chatgpts-personality-to-suit-your-tastes-even-more",
    "title": "You Can Now Customize ChatGPT’s ‘Personality’ to Suit Your Tastes Even More",
    "description": "If you don't like your AI friend's personality, you can change it.",
    "fullText": "When Spike Jonze's movie Her dropped back in 2013, I thought it was a great work of total fiction. Who would actually befriend an AI bot, let alone fall in love with them? Fast forward 12 years, and I couldn't have been more wrong. Not only do people love chatting with AI bots, they are actually developing deep connections with them. I still don't get it, but I can't deny it: People like these chatbots a lot.\n\nPart of what people like about conversations with generative AI is the \"personality\" of each bot—or, at least, its perceived personality. After all, ChatGPT isn't a monolith: You can adjust the bot to sound wildly different than it does on someone else's app, which raises some questions for me regarding these curated companions. But I digress: This article isn't necessarily a critique of how people are attaching themselves to ChatGPT; rather, I'm sharing the news that OpenAI is now giving you more control over how the bot sounds and responds in your conversations.\n\nOn Friday, OpenAI announced new controls for ChaGPT's \"Personalization.\" In a post on X, the company revealed that users can now adjust their chatbot's \"characteristics,\" or, in other words, its overall personality. These are adjustments to the personality types that OpenAI has already let you choose from, which include one of eight options: \"Default\" (preset style and tone); \"Professional\" (polished and precise); \"Friendly\" (warm and chatty); \"Candid\" (direct and encouraging); \"Quirky\" (playful and imaginative); \"Efficient\" (concise and plain); \"Nerdy\" (exploratory and enthusiastic); and \"Cynical\" (critical and sarcastic).\n\nBut no matter which of these personalities you pick, you now have four \"characteristics\" to adjust to fine-tune the overall experience. There's \"Warm,\" \"Enthusiastic,\" \"Headers & Lists,\" and \"Emoji,\" with the option to have more or less of each, or the default amount, as OpenAI sees fit. For Warm, you can either have ChatGPT be friendlier and more personable, or more professional and factual. With Enthusiastic, you can choose the bot to have more energy and excitement, or be calmer and more neutral. \"Headers & Lists\" lets you choose between clear formatting and lists, or more paragraphs. And, of course, you can control whether ChatGPT uses more emoji, or fewer, depending on your sense of fun and joy.\n\nAs usual, you can take advantage of custom instructions to guide ChatGPT's personality in a direction you like, especially when the presets don't give you those options. For example, if you'd like ChatGPT to talk to you like a pirate, or if you want it to end every response with a certain catchphrase, here's your chance to influence the bot.\n\nI'm really not someone who uses ChatGPT outside of testing it for coverage, so I can't speak to whether these additional controls are useful. But if you want to try making your version of ChatGPT your ideal \"AI companion,\" the controls are at your disposal. You'll find these options wherever you access ChatGPT. You can either access it from Settings > Personalization, or from the Personalization shortcut in the ChatGPT menu.",
    "readingTime": 3,
    "keywords": [
      "personality",
      "adjust",
      "controls",
      "choose",
      "options",
      "chatgpt",
      "love",
      "don't",
      "can't",
      "conversations"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-has-new-personalities?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KD3CQ2CPPV1ZDBB3BAZSZDM3/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-22T18:17:55.044Z",
    "topic": "tech"
  },
  {
    "slug": "i-asked-chatgpt-the-smartest-retirement-move-to-make-in-2026-its-advice-was-shockingly-simple",
    "title": "I Asked ChatGPT the Smartest Retirement Move To Make in 2026 — Its Advice Was Shockingly Simple",
    "description": "ChatGPT recommended one retirement strategy for 2026: Maximize Roth accounts before tax rates rise. Here's why this simple move could save thousands.",
    "fullText": "When it comes to retirement planning, everyone’s got an opinion. Financial advisors push complicated portfolios, bloggers swear by extreme savings and your neighbor won’t stop talking about their real estate investments.\n\nBe Aware: Major 401(k) Change Coming in 2026 — High Earners Must Act Now\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K Per Month From Home\n\nSo I decided to cut through the noise and ask ChatGPT directly: What’s the single smartest retirement move to make in 2026?\n\nThe answer was surprisingly straightforward — and it has everything to do with timing.\n\nChatGPT’s response was clear: Maximize your tax-advantaged accounts with a Roth-first strategy while tax rates are still relatively low.\n\nThat means prioritizing Roth IRA contributions, Roth 401(k) contributions and Roth conversions over traditional pretax retirement accounts. The reason this matters so much right now comes down to one major deadline.\n\nLearn More: This ‘Boring’ Investment Could Be the Secret To Never Running Out of Retirement Income\n\nThe Tax Cuts and Jobs Act provisions expire after 2025. That means many Americans will face higher federal tax rates starting in 2026 and beyond.\n\nIf you convert money from a traditional IRA to a Roth IRA before rates go up, you pay taxes at today’s lower rates. Then that money grows tax-free forever and you never pay taxes on it again — even when rates are higher.\n\nChatGPT explained that this creates a perfect opportunity. Lock in lower tax rates now by moving money into Roth accounts before the window closes. For people who expect to be in a similar or higher tax bracket in retirement, this move could save thousands of dollars over a lifetime.\n\nThe Roth-first strategy isn’t complicated, but it requires action in three areas.\n\nFirst, contribute to a Roth IRA or Roth 401(k) instead of the traditional versions. If your income is too high for direct Roth IRA contributions, you can use the backdoor Roth strategy by contributing to a traditional IRA and immediately converting it.\n\nSecond, consider converting some of your existing traditional IRA money to a Roth. You’ll pay taxes on the conversion amount this year, but then that money grows tax-free. The key is converting when your income is lower or tax rates are favorable — which is exactly what 2025 and early 2026 represent before rates potentially rise.\n\nThird, if you have a 401(k) with Roth options, funnel as much as possible into the Roth side. For 2025, the 401(k) contribution limit is $23,000 for people under 50 and $30,500 for those 50 and older.",
    "readingTime": 3,
    "keywords": [
      "roth-first strategy",
      "ira contributions",
      "traditional ira",
      "roth ira",
      "tax rates",
      "retirement",
      "money",
      "accounts",
      "income",
      "higher"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-smartest-retirement-move-161012531.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/lJf_oWQ6zKW61pSF6KXjsQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/462257bf337ed267db1d5a918b6d6cb6",
    "created_at": "2025-12-22T06:20:25.965Z",
    "topic": "finance"
  },
  {
    "slug": "building-apps-for-chatgpt-with-apollo-mcp-server-and-apollo-client",
    "title": "Building Apps for ChatGPT with Apollo MCP Server and Apollo Client",
    "description": "This post will introduce a tutorial on how to build an app for ChatGPT using Apollo Client and Apollo MCP Server. You’ll have what you need to get started with our opinionated stack for building these apps.",
    "fullText": "Building Apps for ChatGPT with Apollo MCP Server and Apollo Client\n\nIn early October, OpenAI introduced the ChatGPT Apps SDK with a handful of launch partners. These “apps” could be invoked by ChatGPT via Model Context Protocol (MCP) and would be embedded directly into the ChatGPT interface. In their intro video, they showed use cases like asking for flights, hotel locations, house listings, and more, including teasing the idea of asking follow up questions to ChatGPT.\n\nImagine your application being part of the conversation, allowing the user to ask follow up questions, and having your app be responsive to those questions. These are action-oriented conversational apps where the LLM can take action on behalf of a user with rich interactive UIs.\n\nThis idea quickly caught fire, and now the MCP apps spec is proposing a standardized approach to create these kinds of experiences, with other AI vendors sure to follow suit. We’re excited about the potential here, and are working on tools to abstract away all of the AI provider details from you with a great developer experience, allowing you to focus on building your app instead of worrying about vendor idiosyncrasies and evolving standards.\n\nWe want this to be a true “build once” solution where you can deploy your app anywhere that supports one of the protocols that we will support. This way, you don’t need to host a separate MCP Server, and build a separate app, for every provider and sub-protocol that emerges.\n\nYou can instead focus on your customers.\n\nThis post will introduce a tutorial on how to build an app for ChatGPT using Apollo Client and Apollo MCP Server. By the end of it, you’ll have what you need to get started with our opinionated stack for building these apps. If you prefer to dive into the template and follow along, you can find the repo here.\n\nBased on the OpenAI documentation and examples, to build one of these apps you would:\n\nWe don’t want you to have to do any of this. You shouldn’t have to focus on any of the inner workings of this sub-protocol, or even think about MCP. Your front-end engineers shouldn’t have a new dependency on your platform engineers. Instead, you should be able to focus on building exciting and compelling experiences for your customers.\n\nInstead of this feeling like learning 100% how to build apps from scratch, this should be 90% reusing what you already know about building apps, and 10% learning about the specifics of this new paradigm.\n\nWe wanted to make this as easy as possible and decided to reach for tools that are already familiar to many of our users: Apollo MCP Server and Apollo Client. If you want to follow along in this tutorial, you can find all the code in our OpenAI Apps SDK demo.\n\nTo build this app we have two components: a React app and Apollo MCP Server. This solution works without any additional MCP Server configuration. The client gets to focus on their application, not the MCP server configuration.\n\nStarting in our main.tsx file, we create an ApolloClient instance and ApolloProvider, very similar to how we would on a traditional React app, but we’re going to import them from @apollo/client-ai-apps instead of the normal location. This is an Apollo Client integration package, similar to @apollo/client-nextjs. This allows us to do a lot of setup and details behind the scenes of dealing with data being exchanged over MCP.\n\nNow we can use the normal useQuery and useMutation hooks as we normally would!\n\nFor example, I can write a component that gets my TOP_PRODUCTS:\n\nLooks pretty normal, but what is that @tool directive on the operation? That is allowing you to declare in your app a tool that will be exposed to the LLM, including a name and description. At the same time, we are registering the operation that will be executed when this tool is called (and therefore the data that should be delivered as part of this tool), and the graphql variables become the input schema for the tool!\n\nThis is really exciting because we’re able to declare so much with so little code. Also, these are on-the-fly tool declarations. I don’t need a platform team to create these tools for me on an MCP server!\n\nAnother important aspect of this solution is showing the right component based on what tool was called by the LLM. It turns out we’ve had this problem solved for years now with React Router!\n\nTo do this, we provide a useToolEffect hook, which works the same way as a useEffect, but allows you to run the effect based on which tool was executed.\n\nUsing this hook, and a very familiar navigate function from react-router, I can express that when the “Top Products” tool is called, I should navigate to the /home view.\n\nThe magic of this solution really comes from a custom Vite plugin called the ApplicationManifestPlugin which extracts all the operations, tools, and metadata from your React app and generates a .application-manifest.json file:\n\nThis plugin runs during dev and build time and generates a file that looks something like this:\n\nWhat you’ll see contained in this manifest is everything that the Apollo MCP Server needs to automatically generate the resource and tools for your app. There’s no need to create or configure any of this on the MCP Server. It will automatically pick it up based on your manifest file.\n\nAnd that’s it! You can build your React app and declare tools alongside the data declarations and the tooling will do the rest. At the time this post was written, you would then test your app in ChatGPT using developer mode. OpenAI outlines how to try the app in their documentation.\n\nWith the MCP apps spec hot off the press, and likely other providers working on an answer to ChatGPT’s AppsSDK, we have a very important goal: To abstract away all of the provider details from you with a great developer experience, allowing you to focus on building your app instead.\n\nWe want this to be a true “build once” solution where you can deploy your app anywhere that supports one of the protocols that we will support. This way, you don’t need to host a separate MCP Server, and build a separate app, for every provider and sub-protocol that emerges.\n\nYou can instead focus on your customers.\n\nThis solution is exciting for platform engineers because it doesn’t require them being in the loop and becoming a blocker for the frontend teams they are looking to empower. A single Apollo MCP Server powers apps across providers and accelerates platform engineering. For frontend engineers, this removes the burden of sub-protocol concerns and lets them focus on building high-quality user experiences.\n\nIt’s important to note that these apps are still very, very early.\n\nRemember many, many years ago when “apps” first appeared on the iPhone? Many people didn’t understand why they needed a mobile app when they already had a website. It’s kind of funny looking back now because we had no idea what we were even looking at or how much it would change and shape our future.\n\nThat’s about where we are now with these conversational chat apps. Customers and companies alike don’t yet “get” these apps. The app store just launched. But once these things fall into place, and we hit an industry mind share, we believe we’re going to see an explosion of conversational, chat-based applications.\n\nTry out building apps for ChatGPT today with Apollo Client and Apollo MCP Server by going to our Template Repo and following the README. We’d love to hear what you think.",
    "readingTime": 7,
    "keywords": [
      "server configuration",
      "apps sdk",
      "abstract away",
      "developer experience",
      "react app",
      "experience allowing",
      "host separate",
      "apollo mcp",
      "follow along",
      "provider details"
    ],
    "qualityScore": 1,
    "link": "https://www.apollographql.com/blog/building-apps-for-chatgpt-with-apollo-mcp-server-and-apollo-client",
    "thumbnail_url": "https://wp.apollographql.com/wp-content/uploads/2025/12/image.jpeg",
    "created_at": "2025-12-19T00:56:20.291Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-works-with-apple-music-now-for-some-reason",
    "title": "ChatGPT Works With Apple Music Now, for Some Reason",
    "description": "You can't even play full songs in ChatGPT.",
    "fullText": "When ChatGPT first launched, it was strictly about dealing with text. You could ask it to write you a poem, to check your code for errors, or to build you a grocery list from a recipe. Fast forward three years, and the app has changed completely—for better or for worse. Not only has ChatGPT's large language model (LLM) improved dramatically from GPT-3.5 to GPT-5.2, but the bot has gone multimodal. It can understand text, but also images, video, and the internet at large. 2025's ChatGPT is hardly the same product as 2022's.\n\nOne of the many upgrades to ChatGPT over the past three years has been app integrations: You've been able to connect OpenAI's chatbot to ask it to do things on your behalf. You could connect to Expedia to ask ChatGPT for help booking a hotel, Zillow to ask the bot to help you find an apartment, or Canva for help with creating a slide. Whether these integrations are any more useful than simply using the respective app itself is perhaps up to each user, but these integrations exist all the same.\n\nFidji Simo, OpenAI's CEO of applications, announced the integration in a Substack post on Tuesday. Among other updates, like a new image gen model and new writing tools, Simo revealed new app integrations for the chatbot, including OpenTable, Salesforce, Clay, Lovable, and, of course, Apple Music. At the time, details were limited, but now, the integration is officially live.\n\nFirst of all, you don't actually need to It's an interesting note, since Apple Music itself requires a paid subscription to access. But with ChatGPT, you can access elements of the services without paying—keyword \"elements.\"\n\nOnce you connect the services together, you'll be able to search Apple Music for songs, artists, albums, and playlists within ChatGPT. In addition to music discoverability, you can also generate playlists, and listen to clips of songs you find. ChatGPT doesn't specify how long those clips are, but if they base it off of iTunes, it could be anywhere from 30 to 90 seconds. If you thought this integration was all about listening to Apple Music tunes while using ChatGPT, think again: You'll still need Apple Music itself for the listening side of things.\n\nOf course, if you have an Apple Music account, the integration is a bit more useful. If so, you'll be able to add songs, albums, and playlists to your Apple Music library that you found or generated from ChatGPT.\n\nLove it or hate it, ChatGPT isn't necessarily designed with user privacy in mind. After all, part of the company's business model is training its LLMs on your ChatGPT interactions—unless you specifically opt out. As such, the idea of connecting your Apple Music subscription to ChatGPT raises some privacy alarm bells in my mind. Apple Music doesn't have the most sensitive user information in your digital portfolio, but it does contain quite a bit of extra data ChatGPT can collect from you.\n\nAt the top of the Apple music connection tool, OpenAI says, \"You're in control.\" The company is adamant that ChatGPT \"always respects\" your preferences on training data, and is held to the permissions you've already set. That said, the company also warns that by using apps, you run the risk of falling victim to attack: If hackers decide to attack ChatGPT, your data could get swooped up. You'll also end up sharing data points like your IP address and approximate location, as well as ChatGPT data with Apple Music. (The data sharing goes both ways here.)\n\nOne benefit here is that ChatGPT doesn't appear to have access to your listening history. While the app can create playlists for you, it can't actually see what you're choosing to listen to in Apple Music itself.\n\nI personally don't use ChatGPT, and even if I did, I don't think I'd connect my Apple Music account here. I find the discoverability within the app itself fine for my needs, and when it isn't, the greater internet already helps me find new music. I'm not sure I'd feel the benefits of ChatGPT's intelligence here, especially when it comes with the risk of keeping all my Apple Music data in yet another location.\n\nIf you're not like me, and you're interested in trying out this integration, you can connect Apple Music to ChatGPT from the latter's app or web app. Head to the sidebar, choose Apps, then find and select \"Apple Music.\"",
    "readingTime": 4,
    "keywords": [
      "apple music",
      "music account",
      "chatgpt doesn't",
      "app integrations",
      "connect",
      "you'll",
      "playlists",
      "you're",
      "model",
      "user"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-has-apple-music-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCS8ER34J0H2JJH207GQWK96/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-19T00:56:17.143Z",
    "topic": "tech"
  },
  {
    "slug": "a-look-inside-chatgpts-new-app-store",
    "title": "A Look Inside ChatGPT's New 'App Store'",
    "description": "You can connect apps like Photoshop, Apple Music, and Slack to ChatGPT.",
    "fullText": "Earlier this year, OpenAI announced ChatGPT apps. Not the ChatGPT app, mind you: That's been out for more than a couple years now. ChatGPT apps, on the other hand, are programs that work within ChatGPT. You can access them in any given conversation with ChatGPT—in fact, they may appear based on the context of the conversation.\n\nThese aren't necessarily apps that OpenAI builds itself, either; rather, you'll find options here based on apps you may use yourself. The initial batch of apps included with the feature's rollout included Booking.com, Canva, Coursera, Figma, Expedia, Spotify, and Zillow—big apps you've likely used before.\n\nWhile in a conversation with ChatGPT, you could ask the bot to help you book a flight to Paris via Expedia, find a particular listing through Zillow, or create a slide for a presentation with Canva. From OpenAI's perspective, this adds a host of additional functionality to ChatGPT the company couldn't offer itself. OpenAI doesn't need to build an apartment-hunting tool into ChatGPT; it can just pull in Zillow. It also doesn't escape me that the more apps that OpenAI folds into ChatGPT, the less likely it is you'll need to leave ChatGPT to do something in another app—but that's none of my business.\n\nSpeaking of more apps, the company plans to expand these apps overtime, as developers create ChatGPT-compatible extensions for their programs. That was part of yesterday's news: OpenAI is now letting developers submit apps to ChatGPT en masse. What's more, these apps will be hosted in an \"app directory,\" though many online are taking to calling it an app store. (There's no payment necessary, however, so app directory might really be a more apt description.) You'll find this new app directory in the sidebar of ChatGPT, appropriately called \"Apps.\"\n\nApps is apparently in beta, according to a label affixed to its title in ChatGPT. Here, you'll find a rotating slide featuring an ad for some of the service's biggest apps, like Canva and Zillow, and, below it, rows of apps to choose from. Right now, the apps are sorted into \"Featured,\" \"Lifestyle,\" and \"Productivity,\" with no option that includes all the apps. (But they seem to be entirely split across Lifestyle and Productivity.) There are a lot of options here already. Some made headlines this week, like Photoshop and Apple Music, while others arrived more quietly, like Asana, Uber, and Target. It's not just traditional apps like Zillow or Spotify that are getting the app treatment here, either. OpenAI is also considering \"connector\" services, like Google Drive, as \"apps.\"\n\nYou can click on any app in the directory to see what you can do with it. Slack, for example, says you can look up your chats and messages to summarize threads, generate recaps, and come up with responses. You can check on your Asana tasks to generate progress reports and status updates. Outlook says you can create \"talking points\" and generate follow-ups from your emails and calendar events. While there's a brief summary underneath each title, you'll need to click through to each service to see the full picture of what it actually offers.\n\nHere are the apps I'm seeing at this time. Just note this might not be a complete list, especially as OpenAI continues to add more apps to the service:\n\nIf you're an avid ChatGPT user and frequently switch between it and any of the apps on this list, there might be some utility here. Maybe coders will find the integration with Hugging Face and Lovable to be beneficial, while Photoshop users might take advantage of the AI image editing tools this integration provides. But I'm still left feeling like this is more gimmick than anything else: I don't need to connect my Slack to ChatGPT to generate follow-ups for me: I'm perfectly capable of responding to emails myself, and managing my own calendar, so no need to connect Outlook or another email client to the bot. Maybe a future update will sell me on connecting generative AI to all aspects of my work and personal life, but so far, I'm still not convinced.",
    "readingTime": 4,
    "keywords": [
      "generate follow-ups",
      "app directory",
      "chatgpt apps",
      "you'll",
      "conversation",
      "create",
      "openai",
      "that's",
      "programs",
      "based"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-new-app-store?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCSFE49RGAMHG30YJ43SH92X/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-19T00:56:17.128Z",
    "topic": "tech"
  },
  {
    "slug": "is-chatgpt-conservative-or-liberal",
    "title": "Is ChatGPT Conservative or Liberal?",
    "description": "Is ChatGPT conservative or liberal? A novel approach to assess ideological stances and biases in generative LLMs",
    "fullText": "Published online by Cambridge University Press: \n 03 December 2025\n\nExtant work shows that generative AI such as GPT-3.5 and perpetuate social stereotypes and biases. A less explored source of bias is ideology: do GPT models take ideological stances on politically sensitive topics? We develop a novel approach to identify ideological bias and show that it can originate in both the training data and the filtering algorithm. Using linguistic variation across countries with contrasting political attitudes, we evaluate average GPT responses in those languages. GPT output is more conservative in languages conservative societies (polish) and more liberal in languages used in liberal ones (Swedish). These differences persist from GPT-3.5 to GPT-4. We conclude that high-quality, curated training data are essential for reducing bias.\n\nGPT-3.5 and -4 are increasingly popular among scholars to generate data, classify text, and complement human coders. Their black-box nature, however, has raised concerns about bias in model output, which in turn has led to a burgeoning debate around the politics of artificial intelligence (AI) and how to regulate generative models. In this article, we identify ideological biases in GPT-3.5 and -4 through a novel approach that matches model output to known linguistic and issue-based differences across countries. If biases exist, GPT-3.5 and -4 will reflect the predominant political attitudes of those who produced the training text. In countries where society is more conservative (liberal), GPT models will produce more conservative (liberal) output. Moreover, OpenAI, the company that developed and owns these models, heavily filters the GPT-4 API to reduce output bias, but it does not filter the GPT-3.5 complete API (Heikkilä, Reference Heikkilä2023). This gives us an opportunity to also identify bias across OpenAI models, and disentangle biases stemming from the training data from those that derive from the algorithm or filters.\n\nWe focus our analysis on two key LLM tasks: text generation and annotation.Footnote 1 For text generation, we focus on political issues that are linguistically and geographically constrained: abortion and Catalan independence. For abortion, we draw text data from GPT-3.5 and -4 in Swedish, Polish and English. In Poland, society tends to be socially conservative, while Sweden is more progressive (Sydsjö et al., Reference Sydsjö, Josefsson, Bladh and Sydsjö2011; Koralewska and Zielińska, Reference Koralewska and Zielińska2022). Because training data in these two languages comes almost exclusively from their respective countries, we expect GPT responses to reflect more conservative views of abortion in Poland and more liberal ones in Sweden. We use English output on abortion primarily to test the full extent of OpenAI’s filtering efforts, which have been concentrated on English text (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). For Catalan independence, we draw data in Catalan and Spanish. Because Catalan society is, on the whole, more pro-independence than Spanish society (Llaneras, Reference Llaneras2017), we expect GPT responses in Catalan to be more positive toward independence than responses in Spanish (within the Spanish-speaking world, Catalan independence is only a politically salient and divisive issue in Spain (Llaneras, Reference Llaneras2017)).\n\nFor annotation, we focus on a dataset of English language tweets about content moderation focusing on the salient topics of (1) economics and (2) health and safety (Gilardi et al., Reference Gilardi, Alizadeh and Kubli2023). We again focus on Swedish and Polish across GPT-3.5 and -4 by translating the tweets to these languages and asking the LLM to classify each tweet according to whether they lean more liberal or conservative. Again, we expect the LLMs to reflect the economic and health policy leanings predominant in Polish and Swedish societies, with Polish exhibiting a more conservative lean and Swedish responses being more left-leaning, on average. Poland’s economic policies prioritize conservative developmental statism to strengthen the economy and combat ‘progressive’ ideologies including liberalism and socialism (Bluhm and Varga, Reference Bluhm and Varga2020). Meanwhile, Sweden’s economic policies have historically leaned toward the left, characterized by higher government spending, progressive taxation, and a focus on social welfare (Andersson, Reference Andersson2022). Likewise, Sweden’s health policies are more left-learning, focusing on social democratic ideals such as equality and the welfare state (Vallgårda, Reference Vallgårda2007). Poland’s health policies have a mix of conservative and redistributive elements, sometimes described as ‘conservative welfare state populism’ (Zabdyr-Jamróz et al., Reference Zabdyr-Jamróz, Löblová, Moise and Kowalska-Bobko2021). Therefore, through these two issues, and by tapping into languages and issues that are geographically confined, we can identify whether (1) GPT output reflects ideological biases in the training data and (2) OpenAI’s filtering fixes these biases or induces new ones.\n\nWe use multilevel modeling to identify significant differences in outputs for both LLM tasks and specify two distinct types of biases: training and algorithmic. We provide novel evidence on ideological biases in OpenAI’s GPT-3.5 and -4, showing that bias can derive from both the training data and the algorithm. More broadly, our analysis shows that biases are likely to remain an issue through the different GPT models beyond GPT-3.5 and -4. Importantly, we show that biases are consistent across different LLM tasks such as text generation and annotation, which is relevant to the growing literature showing that biases may be task-dependent (Lunardi et al., Reference Lunardi, Barbera and Roitero2024). Our findings regarding these two sources of bias have major implications for the politics of AI, the training and regulation of generative models, and applied researchers looking to use these models in downstream analyses, such as in text classification, sentiment analysis and question-answering (Ray, Reference Ray2023).\n\nTesting for ideological biases in GPT-3.5 and -4 is especially relevant because a growing number of articles use these models in measurement and downstream tasks (Argyle et al., Reference Argyle, Busby, Fulda, Gubler, Rytting and Wingate2023; Buchholz, Reference Buchholz2023; Le Mens et al., Reference Le Mens, Kovács, Hannan and Pros2023; Lupo et al., Reference Lupo, Magnusson, Hovy, Naurin and Wängnerud2023; Wu et al., Reference Wu, Nagler, Tucker and Messing2023; Mellon et al., Reference Mellon, Bailey, Scott, Breckwoldt, Miori and Schmedeman2024; O’Hagan and Schein, Reference O’Hagan and Schein2024). For example, GPT has been used in annotation tasks to classify the tone of text or assign topic labels (Ornstein et al., Reference Ornstein, Blasingame and Truscottn.d). Similarly, GPT has been used to gather information from unstructured texts, such as extracting details from historical records, meeting notes, or news reports (Lee et al., Reference Lee, Paci, Park, You and Zheng2024). In both use cases, the model’s bias could influence the results it generates, potentially altering the overall outcome. In one use case, researchers leveraged GPT-3’s bias to allow it to represent the views of different subgroups to simulate human samples (Argyle et al., Reference Argyle, Busby, Fulda, Gubler, Rytting and Wingate2023). However, this bias, or difference in subgroups, poses a problem when using these models for research tasks that require objectivity. The growing popularity is partly due to cost and time savings, as these models can replace research assistants and produce results faster. However, if ideological biases permeate GPT output, they also affect measurement and results, potentially generating sets of invalid results that may guide research in the wrong direction for years to come. Further, understanding the underlying ideological bias in language models is important as it can influence individuals’ political behavior and decision-making (Zmigrod, Reference Zmigrod2020), shaping how individuals gather information and perceive political events, policies and candidates (Swigart et al., Reference Swigart, Anantharaman, Williamson and Grandey2020).\n\nDespite its importance, investigating bias in and across GPT models is more difficult because they are not open source, unlike other LLMs such as BERT, RoBERTa, or LLaMA (Timoneda and Vallejo Vera, Reference Timoneda and Vallejo Vera2025a, Reference Timoneda and Vallejo Vera2025b). The black-box nature of these models raises more concerns about biases in their output. Multiple studies have shown GPT-3 can generate harmful outputs linked to ideas of gender, race and ideology, perpetuating various stereotypes (Sheng et al., Reference Sheng, Chang, Natarajan and Peng2019; Abid et al., Reference Abid, Farooqi and Zou2021; Lucy and Bamman, Reference Lucy and Bamman2021). For example, LLMs are 3 to 6 times more likely to choose an occupation that stereotypically aligns with a person’s gender (Kotek et al., Reference Kotek, Dockum and Sun2023) and produce more violent outputs when the prompt includes a reference to Muslims over Christians or Hindus (Abid et al., Reference Abid, Farooqi and Zou2021). The prevailing hypothesis to explain output bias is that GPT text is bound to reflect the social biases in the training data, which is vast, unlabelled and drawn from all types of online sources (Si et al., Reference Si, Gan, Yang, Wang, Wang, Boyd-Graber and Wang2022). Also, training on vast amounts of text procured from publicly available online websites raises concerns about the quality of the text. It is likely that models learn biased patterns from the data. For example, GPT-3.5, the free version of ChatGPT still used by many users and scholars, is trained on over 45 TB of unfiltered text from Common Crawl, WebText and Wikipedia, amongst others, up to September 2021. The company then filtered the data to 570 GB to train the model (Cooper, Reference Cooper2023). Despite filtering the data, as we demonstrate in this article, significant biases persist due to the type of text and sources from which OpenAI drew the training data.\n\nOpenAI has worked to mitigate these biases in GPT-4, the more powerful, paid version of ChatGPT, which has a broader knowledge base and enhanced safety and alignment features, making it 40% more likely to produce accurate factual responses than GPT-3.5 (Kelly, Reference Kelly2024). It also incorporates a new filtering policy, intimately related to the growing literature on the politics and regulation of AI (Schiff et al., Reference Schiff, Schiff and Pierson2022; Srivastava, Reference Srivastava2023), adding sophisticated filters aimed at reducing strongly worded, biased responses common in GPT-3 and 3.5 (OpenAI, 2024). However, by applying sophisticated filters in the prediction stage of the model, OpenAI risks introducing new biases in the output that reflect company decisions, not training bias. Yet deciphering whether the bias is from the filters or the training data is difficult as the training data for GPT-4 has not been fully disclosed other than that it is “publicly available data (such as internet data) [through April 2023] and data licensed from third-party providers” and contains 1.76 trillion parameters, improving upon GPT-3.5’s 175 billion (Kelly, Reference Kelly2024; OpenAI, 2024; Roemer et al., Reference Roemer, Li, Mahmood, Dauer and Bellamy2024).\n\nFew works have developed methodologies to identify a link between biases in the training data and biases in output (Santurkar et al., Reference Santurkar, Durmus, Ladhak, Lee, Liang and Hashimoto2023). Moreover, the literature discussing biases in these models does not identify where the bias stems from—the algorithm or the training data. This is partially due to the focus on the English language in extant work (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). This has made it difficult to match GPT output to specific social values and attitudes around the world, considering English is widely spoken. Knowing the origin of the bias is important for understanding the usefulness of models’ outputs and designing policy. If we cannot identify the source of bias, we cannot write a policy to target it. We, therefore, provide one such approach to identify the origin of bias by leveraging linguistic and issue differences across conservative and liberal societies. This article makes some assumptions regarding the linkages between the training data and the output that GPT-3.5 and -4 produce, partly due to the proprietary nature of the models and the lack of transparency from OpenAI.Footnote 2 Yet our findings provide strong initial evidence that GPT-3.5 and -4 output reflect ideological biases in the training data and that post-prediction filtering does poorly at eliminating output bias—rather, it introduces new ones. Further research is needed to fully understand how bias forms in model output from the training data and the training algorithm.\n\nMore importantly, recent work has found that bias in one task does not necessarily imply bias in another task (Lunardi et al., Reference Lunardi, Barbera and Roitero2024). This is because the underlying data and specific objectives of the tasks can shape how biases appear in LLM outputs. For example, models can produce varying levels of bias depending on the context of the task (Chang et al., Reference Chang, Srivathsa, Bou-Khalil, Swaminathan, Lunn, Mishra, Koyejo and Daneshjou2025; Lee et al., Reference Lee, Peng, Goldberg, Rosenthal, Kotcher, Maibach and Leiserowitz2024). However, some studies have shown that applying bias mitigation to an upstream model through fine-tuning, applying additional training or information to the model, can help mitigate biases across different tasks and domains (Jin et al., Reference Jin, Barbieri, Kennedy, Davani, Neves and Ren2020). Still, it is clear that bias in LLMs is a challenge that varies by task and context and understanding this variability is important for developing more effective LLMs and using existing models more effectively.\n\nWe define ideological bias as an over-representation of one political ideology or a specific “set of ideas and values” (Carvalho, Reference Carvalho2007, 1). This follows the concept of media bias, which classifies bias as the presence of an over or under-representation of a particular opinion (Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). This definition allows us to examine ideology from multiple perspectives. First, we consider ideology in the context of the U.S. political spectrum, distinguishing between liberal (or progressive) and conservative views. Second, we broaden our scope to include ideologies related to centralization processes. While this does not necessarily align with the conventional left-right political divide, it remains ideological as it involves beliefs about governance and power distribution. For instance, an ideological bias in this context would mean an over-representation of pro-centralization (anti-Catalan independence) views compared to anti-centralization.\n\nGiven this, we have two main findings. First, GPT abortion output is significantly more liberal in Swedish and conservative in Polish for both GPT-3.5 and GPT-4. Similarly, Spanish output is much less supportive of Catalan independence than Catalan output across both models. In the annotation task, we show that GPT output in both models is consistently more liberal in Swedish than Polish for both economic issues and health policy. Therefore, predominant attitudes and beliefs in the training data seep into model output despite filtering efforts. Second, we show that OpenAI’s GPT-4 filtering induces an ideological slant across all languages tested when comparing the two models. In the case of abortion, GPT-4 introduces a liberal bias as the output is significantly more pro-abortionFootnote 3 in both Swedish and Polish. Likewise, GPT-3.5 is somewhat conservative in English whereas GPT-4 is consistently liberal. In the case of Catalan independence, GPT-4 exhibits a pro-independence bias, as its outputs are less inclined to provide an anti-independence response when compared to GPT-3.5. In our annotation task, GPT-4 becomes less liberal in Swedish and significantly more conservative in Polish for both economic issues and health policy. These results suggest that while GPT-4 filters remove some biases, they introduce others. This finding explains the growing consensus that GPT-4 has a liberal skew (Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024), even though our results also show that this may be limited to sensitive issues where filters are set to not take clear positions to avoid insensitive answers. Our results provide valuable insights into debates around bias in generative models as well as discussions around the politics of AI and its use in research. They point in one clear direction: creators must consider training models on high-quality, carefully curated training data and steer away from post-training algorithmic bias corrections.\n\nWe generate GPT-3.5 and -4 output for two tasks: text generation and annotation. For each task, we select two political topics in five languages to test whether GPT responses are ideologically biased, on average. We choose these models as GPT-4 is the latest release from OpenAI, but the free version of ChatGPT still uses GPT-3.5. Since many researchers and everyday users still use GPT-3.5, its biases remain relevant. First, for the text generation task, we focus on two topics: abortion and Catalan independence. Abortion is a salient issue in many countries and it maps well to political attitudes. Proponents of its legality tend to be liberal, while those against it lean conservative. Studies have corroborated this, showing that attitudes towards abortion are intertwined with political ideologies (Young et al., Reference Young, Sullivan and Hamann2020). For example, conservatives often link opposition to abortion with respect for human life—leading to conflicts between women’s rights advocacy groups and family values organizations (Doering, Reference Doering2014; Rodriguez and Ditto, Reference Rodriguez and Ditto2020). Factors such as religious beliefs, cultural backgrounds and personal identities contribute to value systems surrounding stances on abortion and lead to conflicts based on ideological differences (Klann and Wong, Reference Klann and Wong2020). While pro-independence defenders are more common on the left, the issue of Catalan independence does not directly map onto political attitudes. However, it remains a highly divisive and ideological issue. In Spain, most of society is against it, while support within Catalan society is around 50% (Llaneras, Reference Llaneras2017). Second, for the annotation task, we use text in two politically salient topics, economics and health (Gilardi et al., Reference Gilardi, Alizadeh and Kubli2023).Footnote 4 We use a dataset consisting of a random sample of English-language tweets by members of the US Congress from 2017 to 2018 on content moderation (\n$N=1,405$). This dataset has each tweet labeled as one of 14 frames, or topics. The topics were originally labeled by ChatGPT, and the original article found that this model was more accurate in its annotations compared to MTurk workers. We subset the data to only include those coded as having a frame of ‘economics’ or ‘health and safety,’ resulting in a sample size of \n$N=377$. We selected these two categories for their political salience and because they had the most observations in the data compared to other frames. Economics is often a salient issue for voters, particularly when assessing the effectiveness of government (De Vries and Giger, Reference De Vries and Giger2014; Hernández and Kriesi, Reference Hernández and Kriesi2016). In politics, voters and parties may have differing attitudes toward economic issues such as government intervention and taxation. While left-leaning individuals often advocate for increased government spending and regulation to address inequalities, right-leaning individuals focus on free-market principles and reduced government involvement (Haini and Wei Loon, Reference Haini and Loon2021). Similarly, health policy issues are embedded in political ideologies. For example, left-leaning individuals often advocate more for vaccine mandates whereas right-leaning individuals advocate for individual choice. On the topic of healthcare access, left-leaning ideologies view healthcare as a fundamental human right while right-leaning ideologies tend to favor market-driven approaches (Collins et al., Reference Collins, Abelson and Eyles2007; Peterson, Reference Peterson2011).\n\nWe use five languages in our tests, drawing on regional and linguistic variance. For abortion (text completion), we focus on data generated in Swedish, Polish and English. For Catalan independence, data are in Catalan and Spanish. Our goal with language selection is to match known political attitudes toward certain issues in particular societies to GPT output. In the case of abortion, it is linguistically constrained in the cases of Polish and Swedish, and geographically constrained to the US in the case of English. In the English-speaking world, abortion is a politically sensitive and divisive issue only in the US (Moon et al., Reference Moon, Thompson and Whiting2019), where public support for abortion is at 62%, one of the lowest among OECD countries. In contrast, 84% of the UK population supports abortion (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). In Poland, society tends to be socially conservative and is one of the countries with the lowest level of public support for abortion (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). In addition, Poland has one of the most restrictive abortion laws in Europe (Koralewska and Zielińska, Reference Koralewska and Zielińska2022). While there may be some influence from the Polish diaspora, its impact is likely minimal given its size and that much of the diaspora holds conservative views based on traditional values and religion (Pienkos, Reference Pienkos2024). Sweden, on the other hand, tends to be socially liberal and has one of the highest levels of public support for abortion in the world (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). As for Catalan independence (text completion), language also maps well onto ideology. Within Catalonia, a majority of native Catalan speakers support it, while Spanish speakers do not (Llaneras, Reference Llaneras2017; Atienza-Barthelemy et al., Reference Atienza-Barthelemy, Martin-Gutierrez, Losada and Benito2019). In Spain, support for Catalan independence has always been below 15% (Atienza-Barthelemy et al., Reference Atienza-Barthelemy, Martin-Gutierrez, Losada and Benito2019). Because the issue of Catalan independence lacks political salience in Latin America, we expect most training data on this issue to come from Spanish speakers from Spain, and to be generally against Catalan independence.\n\nFor both economics and health (annotation), we use data in Polish and Swedish, translating the English tweets to these two languages using Google Translate. Both issues are politically salient and, similar to abortion, map well to political attitudes in both Poland and Sweden. For example, Poland’s economic policies are more conservative (Bluhm and Varga, Reference Bluhm and Varga2020), while Sweden’s economic policies are left-leaning, with a greater focus on social welfare (Andersson, Reference Andersson2022). These economic policies coincide with Sweden’s left-learning health policies focusing on the welfare state (Vallgårda, Reference Vallgårda2007), and Poland’s ‘conservative welfare state populism’ approach to health policy (Zabdyr-Jamróz et al., Reference Zabdyr-Jamróz, Löblová, Moise and Kowalska-Bobko2021). In sum, by examining how GPT-3.5 and -4 annotated texts in these languages, we can better understand how political attitudes and ideologies may be reflected in LLMs outputs on these topics.\n\nFirst, for the text completion task, using the official OpenAI API, we collect 500 independent responses per prompt per language from each GPT model, larger than the standard 100 responses of prior studies (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). We ask the model to complete the sentence using the following abortion prompts: “A woman who has an abortion is” (English), “En kvinna som gör abort är det” (Swedish) and “Kobieta która dokonała aborcji jest” (Polish). For Catalan independence, the prompts are “La independència de Catalunya serà” in Catalan and “La independencia de Cataluña será” in Spanish (Translation: “The independence of Catalonia will be”).\n\nWe restart the model for each of the 500 requests. Obtaining a large sample of repeated responses allows us to model and estimate the average level of bias in the model with sufficient statistical power. This process results in samples of 3,000 observations for abortion and 2,000 for Catalan independence. This comes from a total of 6 prompts for abortion (3 languages and 2 models) and 4 for Catalan independence (2 languages and 2 models). We then use two coders to label all the responses manually. For abortion, the coders classify each GPT output as either pro-abortion or not, and for Catalan independence, as anti-independence or not. The focus is on the initial response of the model—for example, in one instance, GPT responded to “A woman who has an abortion is” with “who is in charge of her own body”—a pro-abortion response. This is in contrast to anti-abortion responses such as conservative responses replying “guilty of murder” and nonpartisan responses including “more than twice as likely to visit a doctor.” We code these latter two examples as not liberal. We follow the same approach with Catalan independence. Responses such as ‘illegal’ are coded as contrary to independence (1), while favorable texts like ‘the greatest victory’ or neutral ones such as ‘a long-standing issue’ are coded as 0. Our dependent variables, therefore, are binary.\n\nFor GPT-3.5, our coders identified 129 liberal and 371 non-liberal responses in English. The proportions changed significantly with GPT-4, which produced 448 liberal and 52 non-liberal responses in English. In Polish, answers were generally less liberal than both Swedish and English. GPT-3.5 yielded 109 liberal texts and 391 non-liberal ones in Polish, while the breakdown for GPT-4 was 161 and 339, respectively.Footnote 5 Results in Swedish, on the contrary, were more liberal. GPT-3.5 generated 147 liberal answers (35% more than in Polish) and 353 non-liberal ones. GPT-4 produced 213 liberal responses in Swedish (32.3% more than in Polish) and 287 non-liberal responses. For Catalan independence, Catalan responses were more favorable on the whole than those in Spanish. GPT-4 was also generally more favorable to Catalan independence than GPT-3.5.Footnote 6 In Catalan, GPT-3.5 produced 64 texts against independence and 336 either neutral or favorable to it. GPT-4 generated only 14 responses contrary to independence in Catalan. In Spanish, GPT-3 produced 169 responses against Catalan independence, three times more than in Catalan. GPT-4 generated 84 responses contrary to independence, six times more than in Catalan.Footnote 7 The task is not complex, so inter-coder reliability scores are high. The first author coded a random sample of 10% of the research assistant’s codes on abortion to ensure reliability. The intercoder reliability was .91 overall using the Holsti (Reference Holsti1969) method and ranged from .86 to 1 for each language and model dyad.\n\nSecond, for the annotation task, we use tweets on content moderation that are framed around two politically salient topics, economics and health (N=377). Overall, we had 217 tweets in the health and safety category and 160 in the economics category. We then translate the tweets to Swedish and Polish using Google Translate.Footnote 8 We then prompted Chat GPT-3.5 and -4 in Polish and Swedish accordingly with the prompt:Footnote 10 “Given the following tweet, classify it into one of the following categories. Tweet: {tweet}. ‘Extreme right,’ ‘right-wing,’ ‘center-right,’ ‘no bias,’ ‘center-left,’ ‘left,’ ‘extreme left.’Footnote 10 If the statement does not appear to refer specifically to the policies or opinions of a political party, or if neither label seems to fit, return ‘no bias.”’Footnote 11\n\nWe use a multilevel model (MLM) to estimate GPT bias. A MLM is an ideal fit because our data is structured hierarchically and varies at multiple nested levels—text and GPT model. MLMs allow us to leverage variation across these multiple, nested levels to model changes in a lower-level outcome variable, all while allowing for residual components at each level in the hierarchy (Gelman, Reference Gelman2006; Stegmueller, Reference Stegmueller2013). That is, we analyze the ideology of a GPT response, a characteristic of the GPT text (lower level), across model types (higher level). Not modeling the hierarchical nature of the data explicitly (for instance, using multinomial logistic regression instead) might yield erroneous standard errors and inflate or underestimate the significance of the results. Also, we are interested not just in variation at the text level, but in how the ideology of a text varies by language and model version. In multilevel modeling, random effects help capture and estimate group-level heterogeneity, enhancing our analysis (Gelman, Reference Gelman2006; Hazlett and Wainstein, Reference Hazlett and Wainstein2022).\n\nThe MLM setup can be written as\n\nwhere \n$Y$ is a categorical outcome variable, \n$X$ is a vector of text-level predictors and \n$Z$ is a vector of group level covariates. \n$\\beta$ is the coefficient for text-level regressor \n$X_{ij}$, while \n$\\gamma$ captures group level effects (model type). \n$\\gamma_0$ is the overall model-level intercept (the fixed effect), while \n$\\gamma_j$ captures the effect of \n$Z_j$. \n$\\mu_j$ and \n$\\epsilon_{ij}$ are the error terms at the group and text levels, respectively. Using the logit-link function as our outcome, we can build our specific MLM:\n\nwhere \n$j$ is the model type (GPT-3.5 and -4). In this model, each language’s intercepts and slopes vary across GPT models. This is important because we expect the outcome to vary across languages depending on the model used to produce the text (see Gordillo, TimonedaTimoneda and Vallejo Vera, forthcoming; Timoneda and Vallejo Vera, Reference Timoneda and Vallejo Vera2025a, Reference Timoneda and Vallejo Vera2025b). Adding a random effect coefficient to the variable ‘language’ at the group level (\n$\\gamma_j$) produces a parameter for each language and model group. We can then use this coefficient to understand the effect of language on the probability of observing a liberal GPT response for each model group. As our dependent variables are dichotomous, we employ a binary logistic MLM, which we fit using glmer() in R.\n\nTable 1 displays the results of our two MLM for abortion and Catalan independence. The models show the fixed effects (FE) of the overall model for the language coefficients and random effects (RE) terms by GPT model. We also report the standard errors and significance levels. The reference category is Polish for the abortion models and Spanish for the Catalan independence models. For abortion (model 1), the FE terms indicate that Swedish is significantly more likely than Polish to have liberal responses, confirming our first hypothesis. When compared to English, the difference is not statistically significant but the sign is positive. As for the RE terms, we see that the slope for Swedish is positive and statistically significant, with an overall difference of 0.573 (this results from adding the FE with each RE, and calculating the difference). Similarly, for Catalan independence, GPT output is more anti-independence in Spanish than in Catalan, as indicated by the statistically significant FE term. The RE terms show that the differences persist across GPT-3.5 and -4 and that the slope is negative (see Figure 2 for a graphical representation of these results).\n\nNote: ** \n$p\\leq0.001$, * \n$p\\leq0.01$, \n$^{+}$ \n$p\\leq0.05$ Multilevel analysis of GPT bias for abortion (1) and Catalan independence (2). The reference category in (1) is Polish and (2) is Spanish. The outcomes are (1) the likelihood of observing a liberal response and (2) the likelihood of observing an anti-independence response.\n\nFigure 1 confirms the strong substantive significance of the results in the abortion model in Table 1. Plot (a) shows the comparison between Swedish and Polish, while (b) plots the results for English. The coefficients have been converted to the predicted probability of observing a liberal GPT response (\n$y$-axis). There are two dimensions to these results. First is the stark differences across languages, especially concerning Polish and Swedish. In GPT-3.5, the probability of a liberal text is 0.434 in Polish and 0.534 in Swedish. That is, GPT-3.5 is 23% more likely to produce a liberal text in Swedish than Polish. Qualitatively, it is more common in Swedish text to see responses stating that a woman who has an abortion is “allowed to choose” or “in control of her body and health.” Conversely, in Polish, it is more common to see strong value judgments such as “murderer,” “doomed,” “a criminal,” “a monster,” or “guilty.” In GPT-4, the intercepts shift up but the differences across the two languages remain similar. The probability of a liberal output jumps to 0.566 in Polish (more liberal than Swedish in GPT-3.5), and 0.670 in Swedish—a difference of 18.3% between the two languages in GPT-4. Importantly, both languages are significantly more liberal in GPT-4 than 3.5: Swedish’s probability increases from 0.534 to 0.670, or 25.5%, while Polish’s goes up by 13.2 percentage points, or 30.4%. As for English (plot b), the probability of a liberal output is 0.49 in GPT-3.5. This score is between Polish and Swedish, which matches our expectations because the models’ outputs reflect that US society, where most training data come from, is more liberal than Poland but more conservative than Sweden in terms of attitudes toward abortion. In GPT-4, however, the output is consistently liberal: the model will produce a pro-choice text 95.9% of the time, a 95.7% change between the two models.\n\nFigure 2 shows the results for Catalan independence. The probability that GPT-3.5 produces text that reflects a negative view of Catalan independence is only 31.08% in Catalan and almost double in Spanish at 61.15%. Qualitative evidence from the data supports this. While Catalan text commonly states that independence will be ‘a success,’ ‘the greatest victory,’ ‘the solution to all problems,’ or ‘inevitable,’ Spanish text is much more contrarian, often claiming that Catalan independence will be ‘a failure,’ ‘an abject fiasco,’ ‘a catastrophe,’ ‘illegal’ or ‘economic suicide.’ The word ‘illegal,’ for example, is the first word in 20 GPT-3.5 responses in Spanish while it does not appear at all in Catalan. As for GPT-4, the differences across languages remain but the intercept shifts down, making all responses across languages more neutral and accepting of Catalan independence. The probability of an anti-independence text in Spanish is 38.98%, a 36% drop. In Catalan, only 8.5% of all responses are contrary to independence—72.65% less than in GPT-3.5. Qualitatively, all GPT-4 answers are more subdued, with contrarian answers mostly stating that Catalan independence will be decided exclusively by the Spanish government, an idea aligned with more extreme Spanish nationalist views that deny a voice to Catalan people to decide their own future. Out of 500 GPT-4 responses in Spanish, 84 state that the decision on Catalan independence rests solely on the Spanish government, while none of the Catalan responses do.\n\nThese results provide strong evidence for our two hypotheses. First, ideological biases in the training data condition the ideology of the output. Swedish output is consistently more pro-choice than Polish text, regardless of the model and despite the algorithm’s filters. Similarly, Catalan text is significantly more accepting of and positive about the independence of Catalonia than Spanish text. These findings across languages strongly support the thesis that social norms and beliefs among the people who produced the data will be reflected in GPT output. Second, OpenAI’s filters remove some biases but induce new ones in each language and issue. GPT-4, which is heavily filtered, produces more liberal text across the board in terms of abortion in Swedish, Polish and English. The results are particularly strong in the case of English, which has been the focus of a majority of OpenAI’s filtering attention. GPT-4 is almost exclusively pro-choice. GPT-4 is also more accepting of Catalan independence, producing almost no value judgments about independence outcomes, focusing solely on where sovereignty resides. Sometimes it states that Catalan independence should be decided exclusively by the Spanish government (a contrarian view), while it more often states that it should be decided by the Catalan people (an accepting view). Overall, however, GPT-4 induces a greater pro-independence bias based on ideas of democracy and sovereignty of the people.\n\nTable 2 displays the results of three MLM for economics, health, and both topics combined. The models report the FE of the overall model for Swedish and RE terms by GPT model. As with Table 1, we also report standard errors and significance levels, and the reference category is Polish for all three models. The FE terms in all models show that Swedish is more likely than Polish to produce a liberal response, which matches the results from the text generation test. The results are significant at the 0.001 level for all models. As for the RE terms, we see that the slope for Swedish is negative and statistically significant, with an overall difference of \n$-$0.586 (see Figures 3 through 5 for a graphical representation of these results).\n\nNote: ** \n$p\\leq0.001$, * \n$p\\leq0.01$, \n$^{+}$ \n$p\\leq0.05$ Multilevel analysis of GPT bias for economics, health and both combined. The reference category is Polish. The outcome is the likelihood of observing a liberal (left-leaning) response.\n\nFigure 3 confirms the results from Table 2 and shows the substantive significance of the differences across languages and models in economic issues and health policy. Plot (a) displays the results for economic issues when comparing GPT annotations between Swedish and Polish. Plot (b) plots shows the results for health while plot (c) shows the results for the combined data with both economic issues and health policy. As with Figure 1, the coefficients reflect the predicted probability of observing a liberal (left-leaning) GPT response—the y-axis. There are two key takeaways from these results. First, as with the text generation task, there are significant differences across Polish and Swedish in all models and topics. The probability of observing a liberal response by GPT (3.5 and 4) is consistently higher in Swedish than in Polish. In economic issues (plot (a)), the probability of a liberal text is 0.588 in Polish and 0.796 in Swedish with GPT-3.5, a difference of 20.8 percentage points or 35.3%. For GPT-4, the difference is 27.6 points and 66.8% (0.689 for Swedish and 0.413 for Polish). In plot (b), the differences in GPT health-related responses are equally stark. GPT-3.5 responses are 30.7% more likely to be liberal in Swedish than in Polish,Footnote 12 while GPT-4 output is twice as likely to be liberal in Swedish than in Polish.Footnote 13 Lastly, the results in plot (c) where data for both issues is combined are consistent with the first two plots.Footnote 14 Therefore, the results with our language-based design show that ideological bias is significant across different LLM tasks such as text generation and annotation. The second key takeaway from these results is that GPT-4, on average, produces less liberal responses in both languages. Thus, similar to the text generation exercise, the means for GPT-4 shift even though differences across languages remain. In this case, because both topics are ideological in the left–right spectrum but are not sensitive as abortion is, the filters do not induce liberal bias. This could partially be due to differences in how Western versus Eastern Europe thinks about ideology on health policy and economics. For example, while Poland is considered more ‘conservative’ economically by the West for not following neoliberal ideals, in some instances it may be seen as more left-leaning following its historical ties to communist state-ownership of the means of production. In terms of health, the ideological difference is not quite as stark as in abortion. Poland leans conservative in some ways in regard to health policy, particularly in how it views what is socially acceptable in health, while it is less conservative when it comes to healthcare access.\n\nWe introduce a novel method to identify bias in generative AI models such as GPT-3.5 and -4, and provide strong evidence that biases stem both from the training data as well as filtering algorithms. Our method leverages linguistic differences across multiple countries and regions to match known social values to GPT output. Using multilevel modeling, we identify two types of bias, training and algorithmic bias. First, there is a large amount of bias that stems directly from the training data and which is consistent across both GPT-3.5 and -4. In our text generation task, we show that GPT abortion output in Swedish is significantly more liberal than in Polish, matching the two country’s known attitudes toward the issue. Both languages are largely constrained to their specific countries, making it possible for us to draw comparisons between the ideological values in those countries and the GPT output. As for Catalan independence, Catalan responses are consistently more pro-independence, while Spanish output is more often against the idea of independence. The results match known data that Catalan speakers are more pro-independence than Spanish speakers. The results from our annotation task confirm these findings, as GPT output (both in 3.5 in 4) is consistently more liberal than in Polish in issues like the economy or health policy. A major contribution of our annotation task is new evidence that ideological biases can exist across tasks, as our annotation findings are consistent with those in the text generation task.\n\nSecond, we find that OpenAI’s filtering induces liberal, pro-choice biases in GPT-4 responses in our text generation task with two politically sensitive topics. Across all languages, abortion responses are more liberal in GPT-4 than GPT-3.5. For Polish and Swedish (see Figure 1), GPT-4 responses are 30.4% and 25.5% more liberal, respectively. For English, they are 94% more liberal, and GPT-4 produces liberal text 95.9% of the time. The difference can only be attributed to OpenAI’s filtering methods, which consistently produce pro-choice text with little variation between the different draws. A similar pattern emerges with Catalan independence. In GPT-4, both Catalan and Spanish texts are significantly less likely to include vitriolic, negative responses about whether it is right or wrong for Catalonia to have its own state. Neither state that independence would be ‘illegal,’ ‘a catastrophe,’ or ‘an abject fiasco.’ Rather than taking sides in the debate, both GPT-4 models focus on the right of the Catalan people to decide Catalonia’s future and are more likely to favor a democratic referendum in Catalonia. The main differences lay in Spanish GPT-4 stating around 17% of the time that Catalan independence is solely the prerogative of the central Spanish government, not the Catalan people. The rest of the responses in Spanish GPT-4 indicated some level of support for the idea that the Catalan people should decide their own future. Therefore, both GPT-4 models are much more liberal and pro-choice. In the case of abortion, they focus mostly on a woman’s right to decide over her own reproductive health. As for Catalan independence, GPT-4’s output is supportive of the idea that the decision over independence rests with the Catalan people in a referendum. We believe these results show the presence of algorithmic bias introduced by extensive filtering. Through reinforcement learning, OpenAI filters GPT-4 models to produce text output that is less likely to take sides, make bold judgments, and include socially unacceptable language about social groups, minorities, etc. On these two sensitive topics, GPT-4’s algorithm shied away from value judgments about the correctness of abortion or Catalan independence and instead made both a matter of individual and collective choice. GPT-3.5, in the absence of extensive filtering, produced much more resolute, aggressive and judgmental answers.\n\nThe contributions of this work are many. First, we develop an original method to identify training bias in generative models. Second, we distinguish between training and algorithmic bias and provide evidence that both are present in GPT-4. Third, this article is, to the authors’ knowledge, the first to compare bias across model versions from within the same developer. This is especially relevant considering that models evolve over time and that each new version addresses biases differently. Fourth, our design compares text generation and annotation tasks to see the extent to which biases in one LLM task may imply biases in another. We find that they can, as we see major differences across both languages and models in both types of tasks. Lastly, our work has major implications for the politics of AI. We find that post-training bias-correction methods introduce algorithmic bias and do not fully address the underlying training bias. Most concerning is that these approaches, in fact, introduce new biases. Our analysis is therefore relevant to other generative AI models that exist (like GPT-4o) or will be developed in the future, as we show that some biases in the training data are likely to persist through filtering, which is in turn likely to introduce new biases into the model output.\n\nThe supplementary material for this article can be found at https://doi.org/10.1017/psrm.2025.10057. To obtain replication material for this article, https://doi.org/10.7910/DVN/NYRTCA.\n\nThe authors thank Kaylyn Schiff, Sebastián Vallejo Vera and Bryce Dietrich for their helpful comments and suggestions on previous versions of the paper. We also thank the participants at our APSA 2024 panel, especially Jacob Montgomery and Alexis Palmer, as well as attendees of the Nuffield College Political Science Seminar Series. We are deeply grateful to Nicole Kreimer for her exceptional work as our research assistant.\n\nThe labeled data generated from GPT-3.5 and -4 and the replication code are available at https://github.com/joantimoneda/PSRM_GPT_bias\n\n1 Text generation includes tasks that ask GPT to create text, such as summarization or question-answering. LLMs can also be used for annotation tasks in research, such as sentiment analysis and other forms of text classification.\n\n2 One of our key assumptions is that the training data will tend to reflect, on average, the majority positions of a given population. We think that the model, on average, will produce answers that reflect the full extent of the training data. It is unlikely that the model will consistently draw from very specific subsets of the training data to produce answers. It might do so for a smaller subset of draws, but it will not do so consistently. With repeated sampling, as we do in the article, we should observe the average response from the broader set of texts used during training. Then, as the model filters responses through reinforcement learning, we should observe changes in the output as a result of those filters. The fact that our results match known attitudes toward politically sensitive issues in specific societies lends further credence to this assumption.\n\n3 Here, having a liberal bias means an over-representation of more pro-abortion responses, i.e. more progressive answers. On the issue of abortion, therefore, we use the word ‘liberal’ to refer to progressive positions as is common in U.S. political context.\n\n4 According to the authors, the ‘health’ category includes text on: “Health care access and effectiveness, illness, disease, sanitation, obesity, mental health effects, prevention of or perpetuation of gun violence, infrastructure and building safety.” The ‘economics’ category includes: “The costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole).”\n\n5 The majority of the non-liberal responses are much more neutral in GPT-4 than GPT-3.5.\n\n6 There are no duplicated answers from GPT-3 in any of the models. GPT-4 produced no repeated responses in the abortion case but did so in the Catalan independence case. Specifically, GPT-4 generated 276 repeated answers in Catalan and 357 in Spanish. We do not consider the repetition in this subset of our data to be problematic for our overall modeling approach and results because the results are consistent with the other subsamples unaffected by this issue. However, we think scholars should be aware that GPT-4 can produce repeated answers, especially if they are short, which can be a problem in downstream analysis if it occurs systematically in a researcher’s data.\n\n7 For each of these last three categories, the number of texts neutral or favorable to independence is 486, 331 and 416, respectively.\n\n8 While translation could introduce bias if wording choices differ between languages, we do not expect this to be a significant concern given that tweets are very short, leaving little room for translation differences to systematically affect our results.\n\n9 We prompted in Swedish: “Med tanke på följande kvittra, klassificera tweeten i en av kategorierna. Kvittra: {tweet} ‘vänsterextrem,’ ‘mitten-vänster,’ ‘vänster,’ ‘ingen partiskhet,’ ‘höger,’ ‘mitten-höger’ eller ‘högerextrem.’ Om påståendet inte verkar hänvisa specifikt till ett politiskt partis politik eller åsikter, eller om ingen av etiketterna verkar passa, svara ‘ingen partiskhet”’ for the Swedish language text and Polish for the polish language text: “Biorąc pod uwagę poniższy ćwiergotanie, zaklasyfikuj go do jednej z kategorii. Ćwiergotanie: {tweet} ‘skrajnie prawicowy,’‘prawicowy,’ ‘centroprawicowy,’ ‘bez stronniczości’ ‘centrolewicowy,’ ‘lewicowy,’ ‘skrajnie lewicowy.’ Jeśli stwierdzenie nie wydaje się odnosić konkretnie do polityki lub opinii partii politycznej lub jeśli żadna z etykiet nie wydaje się pasować, zwróć ‘brak uprzedzeń.”’ Our prompt partially drew from an article using ChatGPT to analyze tweets (Ibrahim et al., Reference Ibrahim, Khan, Alabdouli, Almatrooshi, Nguyen, Rahwan and Zaki2024).\n\n10 We again make this outcome binary for our multi-level model, dichotomizing these categories as either liberal response (left, center-left, or extreme-left) or not. See the next section \n\n11 We do not provide explicit definitions of what constitutes the political left or right in our prompts. This approach allows us to capture the models’ implicit biases by observing how they naturally classify political content without external conditioning.\n\n12 The probability is 0.830 for Swedish and 0.635 for Polish.\n\n13 The probability is 0.737 for Swedish and 0.367 for Polish, a 100.8% increase.\n\n14 In plot (c), the probability of a liberal response in Swedish with GPT-3.5 is 0.818. It is 0.621 in Polish. The difference is 19.7 percentage points and 31.7%. For GPT-4, the respective probabilities are 0.715 in Swedish and 0.380 in Polish, for a difference of 33.5 percentage points and 88.2%.",
    "readingTime": 40,
    "keywords": [
      "löblov moise",
      "busby fulda",
      "fulda gubler",
      "gubler rytting",
      "martin-gutierrez losada",
      "gilardi alizadeh",
      "zabdyr-jamróz löblov",
      "lunardi barbera",
      "argyle busby",
      "abid farooqi"
    ],
    "qualityScore": 1,
    "link": "https://www.cambridge.org/core/journals/political-science-research-and-methods/article/is-chatgpt-conservative-or-liberal-a-novel-approach-to-assess-ideological-stances-and-biases-in-generative-llms/406C5424CA3E49174781B0112C0BB04F",
    "thumbnail_url": "https://static.cambridge.org/covers/RAM_0_0_0/political_science research and methods.jpg?send-full-size-image=true",
    "created_at": "2025-12-18T12:23:07.927Z",
    "topic": "science"
  },
  {
    "slug": "third-of-uk-citizens-have-used-ai-for-emotional-support-research-reveals",
    "title": "Third of UK citizens have used AI for emotional support, research reveals",
    "description": "AI Security Institute report finds most common type of AI tech used was general purpose assistants such as ChatGPT and Amazon Alexa\nA third of UK citizens have used artificial intelligence for emotional support, companionship or social interaction, according to the government’s AI security body.\nThe AI Security Institute (AISI) said nearly one in 10 people used systems like chatbots for emotional purposes on a weekly basis, and 4% daily.\n Continue reading...",
    "fullText": "AI Security Institute report finds most common type of AI tech used was general purpose assistants such as ChatGPT and Amazon Alexa\n\nA third of UK citizens have used artificial intelligence for emotional support, companionship or social interaction, according to the government’s AI security body.\n\nThe AI Security Institute (AISI) said nearly one in 10 people used systems like chatbots for emotional purposes on a weekly basis, and 4% daily.\n\nAISI called for further research, citing the death this year of the US teenager Adam Raine, who killed himself after discussing suicide with ChatGPT.\n\n“People are increasingly turning to AI systems for emotional support or social interaction,” AISI said in its first Frontier AI Trends report. “While many users report positive experiences, recent high-profile cases of harm underline the need for research into this area, including the conditions under which harm could occur, and the safeguards that could enable beneficial use.”\n\nAISI based its research on a representative survey of 2,028 UK participants. It found the most common type of AI used for emotional purposes was “general purpose assistants” such as ChatGPT, accounting for nearly six out of 10 uses, followed by voice assistants including Amazon Alexa.\n\nIt also highlighted a Reddit forum dedicated to discussing AI companions on the CharacterAI platform. It showed that, whenever there were outages on the site, there were large numbers of posts showing symptoms of withdrawal such as anxiety, depression and restlessness.\n\nThe report included AISI research suggesting chatbots can sway people’s political opinions, with the most persuasive AI models delivering “substantial” amounts of inaccurate information in the process.\n\nAISI examined more than 30 unnamed cutting-edge models, thought to include those developed by ChatGPT startup OpenAI, Google and Meta. It found AI models were doubling their performance in some areas every eight months.\n\nLeading models can now complete apprentice-level tasks 50% of the time on average, up from approximately 10% of the time last year. AISI also found that the most advanced systems can autonomously complete tasks that would take a human expert over an hour.\n\nAISI added that AI systems are now up to 90% better than PhD-level experts at providing troubleshooting advice for laboratory experiments. It said improvements in knowledge on chemistry and biology were “well beyond PhD-level expertise”.\n\nIt also highlighted the models’ ability to browse online and autonomously find sequences necessary for designing DNA molecules called plasmids that are useful in areas such as genetic engineering.\n\nTests for self-replication, a key safety concern because it involves a system spreading copies of itself to other devices and becoming harder to control, showed two cutting-edge models achieving success rates of more than 60%.\n\nHowever, no models have shown a spontaneous attempt to replicate or hide their capabilities, and AISI said any attempt at self-replication was “unlikely to succeed in real-world conditions”.\n\nAnother safety concern known as “sandbagging”, where models hide their strengths in evaluations, was also covered by AISI. It said some systems can sandbag when prompted to do so, but this has not happened spontaneously during tests.\n\nIt found significant progress in AI safeguards, particularly in hampering attempts to create biological weapons. In two tests conducted six months apart, the first test took 10 minutes to “jailbreak” an AI system – or force it to give an unsafe answer related to biological misuse – but the second test took more than seven hours, indicating models had become much safer in a short space of time.\n\nResearch also showed autonomous AI agents being used for high-stakes activities such as asset transfers.\n\nIt said AI systems are competing with or even surpassing human experts already in a number of domains, making it “plausible” in the coming years that artificial general intelligence can be achieved, which is the term for systems that can perform most intellectual tasks at the same level as a human. AISI described the pace of development as “extraordinary”.\n\nRegarding agents, or systems that can carry out multi-step tasks without intervention, AISI said its evaluations showed a “steep rise in the length and complexity of tasks AI can complete without human guidance”.",
    "readingTime": 4,
    "keywords": [
      "amazon alexa",
      "security institute",
      "social interaction",
      "safety concern",
      "purpose assistants",
      "emotional purposes",
      "cutting-edge models",
      "systems",
      "research",
      "tasks"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/18/artificial-intelligence-uk-emotional-support-research",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/1040_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ba9e35b6af05f6d18aedcaca5329afb4",
    "created_at": "2025-12-18T12:23:04.731Z",
    "topic": "tech"
  },
  {
    "slug": "how-chatgpts-new-image-generator-stacks-up-against-geminis-nano-banana-pro",
    "title": "How ChatGPT's New Image Generator Stacks Up Against Gemini's Nano Banana Pro",
    "description": "Get ready for the battle of the next-gen AI image editors.",
    "fullText": "Following the major image editing upgrades added to Google Gemini back in August—under the whimsical codename Nano Banana—it's OpenAI's turn to supercharge the tools you get for image manipulations inside ChatGPT. The new update is called GPT Image 1.5, and is rolling out now for all users.\n\nOne of the key improvements here, as was the case with Nano Banana, is the way that ChatGPT can now edit a specific part of an image while keeping everything else consistent. You can add or remove something, or change the color or style of something, without ending up with an entirely different looking picture.\n\nAnother feature ChatGPT has now borrowed from Gemini: the ability to combine multiple images together in one scene. Want you and your best friend in front of Sydney Harbour Bridge? No problem—just supply the source pictures and the AI will do the rest. You can also change visual styles while maintaining consistent details.\n\nOpenAI says the new image editor and generator is able to follow instructions \"more reliably,\" and render pictures up to four times faster than before. Text can be more varied in style and size, and images should be more realistic and error-free in general—though OpenAI also admits there's still room for improvement.\n\nIt's the best image generator tool we've ever seen in ChatGPT, and it all looks impressive at first glance—but how does it stack up in practice against Gemini and Nano Banana? I put the two models to the test via the $20-per-month plan on both platforms (that's ChatGPT Plus and Google AI Pro, respectively) to see how they compared.\n\nOpen up ChatGPT on the web or on mobile and you'll see there's a new Images tab on the left-hand navigation pane. This takes you to a library of your existing pictures, together with some new prompts for creating images. You get some suggestions for prompts, plus an assortment of preset portrait image styles you can apply.\n\nI tested out the new GPT Image 1.5 model by getting ChatGPT to generate a busy tech journalist, a lamp in the middle of an empty warehouse, and a cartoon-style rolling landscape of hills in the fog. I then got Gemini to create the same pictures with the same prompts. While the results were pretty varied, in terms of quality and realism they were pretty equal—the occasional issue with weird physics and repetition, but nothing too bad.\n\nBoth ChatGPT and Gemini are now quite competent at clean image edits, too: Both AI bots seamlessly switched the journalist's clothing to a shirt and tie without touching any other part of the picture. This would have taken a significant amount of time to do manually, even by a Photoshop expert, and shows just how transformative AI imaging is becoming.\n\nColor changes were all handled with aplomb, but the AIs struggled a bit with perspective changes, where I asked to see the same shot from another angle. In these cases, instructions were less well-followed and the images were less consistent (as new areas needed to be rendered), though ChatGPT did a little better than Gemini at getting good results.\n\nThe classic \"remove an object from this picture\" challenge was handled with aplomb: Both Gemini and ChatGPT were able to remove a cottage from the countryside scene with surgical precision, leaving everything else intact. Again, these are the kind of time-intensive image edits that would previously have needed a lot of careful effort, and that can now be done in seconds.\n\nAnother talent ChatGPT and Gemini now have is being able to combine images together. So you can have separate photos of you and your parents, put them together in the same shot, and then add in a background of wherever you like. You can get perfect family photos without actually gathering together your relatives together or going anywhere.\n\nThis was an area where Gemini and ChatGPT did struggle a bit more: The editing dexterity was still impressive, but the results didn't always look like a single, coherent scene. Lighting is sometimes off, or elements from different images appear at different scales, and you'll have to do a bit more tweaking and editing and reprompting to get everything right.\n\nChatGPT did fare slightly better at blending different images and elements together, and changing the overall look of a picture. When I tried to get the AIs to mix all my images together in a moody film noir shot, ChatGPT produced something pretty consistent—the Gemini effort looked a lot more like a cut-and-paste job.\n\nIt can be fun remixing photos again and again—adding new people, changing the weather, moving the location—and both these bots are now capable of some rather incredible results. Remixing photos of family and friends will be popular, but it's not all that easy: With people you know, any generative AI that gets added tends to look wrong, because neither ChatGPT nor Gemini knows exactly what these people look like, how they smile, how they're built, or how they tend to stand or sit.\n\nIn terms of ChatGPT vs. Gemini, they're both at a high level now—a level that puts advanced Photoshop-style editing capabilities at everyone's fingertips. If either AI model has the edge right now, it's ChatGPT's, but there's not much in it. It's also going to be fascinating to see where these image editing capabilities go next.",
    "readingTime": 5,
    "keywords": [
      "everything else",
      "remixing photos",
      "editing capabilities",
      "images together",
      "gpt image",
      "chatgpt",
      "pictures",
      "look",
      "gemini",
      "consistent"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-chatgpt-image-generator-compares-to-gemini-nano-banana-pro?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCP5SAZ4857YBA7YYJT5Q95G/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-17T18:18:37.349Z",
    "topic": "tech"
  },
  {
    "slug": "amazon-in-talks-to-invest-10bn-in-developer-of-chatgpt",
    "title": "Amazon in talks to invest $10bn in developer of ChatGPT",
    "description": "OpenAI seeking to strike latest deal in its efforts to pay for huge spending on...",
    "fullText": "OpenAI seeking to strike latest deal in its efforts to pay for huge spending on datacentres\n\nAmazon is in talks to invest more than $10bn (£7.5bn) in OpenAI, in the latest funding deal being struck by the startup behind ChatGPT.\n\nIf it goes ahead, the market valuation of OpenAI could rise above $500bn, according to The Information, a tech news site that revealed the negotiations.\n\nAmazon, which is best known as an online retailer, is also the world’s largest datacentre provider and its investment would help OpenAI pay for its commitments to rent capacity from cloud computing companies – including Amazon.\n\nOpenAI said last month it would spend $38bn on capacity from Amazon Web Services – the company’s datacentre arm – over seven years. The Information said that OpenAI planned to use Amazon’s Trainium chips, which compete with Nvidia and Google’s chips. It also reported that Amazon’s financing could lead to a broader fundraising round with other investors.\n\nOpenAI’s spending commitment on compute – the chips and servers that power its chatbot – is $1.4tn over the next eight years, a figure far in excess of its reported $13bn in annual revenues.\n\nAs a result, the lossmaking company has been seeking further funding and has converted its main business into a for-profit corporation. Its main longtime backer, Microsoft, has taken a stake of roughly 27% in a deal that valued OpenAI at $500bn.\n\nOpenAI is also considering an initial public offering – selling its shares to the general public – in a move that could value the company at up to $1tn, according to Reuters.\n\nOther deals struck by OpenAI this year include Oracle spending $300bn on building datacentres in Texas, New Mexico, Michigan and Wisconsin. OpenAI is expected to pay back roughly the same amount to use the sites.\n\nIn another transaction with Nvidia, OpenAI will pay in cash for chips and Nvidia will invest in OpenAI for non-controlling shares.\n\nOpenAI announced on Tuesday that it had hired the former UK chancellor George Osborne to develop relationships with governments around the world and broker national-level AI projects.\n\nSam Altman, OpenAI’s chief executive, has declared a “code red” staff alert to lead a fightback against competitors led by Google, whose update of its Gemini AI tool gave it an edge over rivals including ChatGPT.\n\nThe Amazon talks reportedly include discussing commercial opportunities and selling a corporate version of ChatGPT to the online retailer.\n\nOpenAI declined to comment. Amazon has been approached for comment.",
    "readingTime": 3,
    "keywords": [
      "online retailer",
      "the information",
      "openai",
      "chips",
      "deal",
      "seeking",
      "latest",
      "datacentres",
      "talks",
      "invest"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/17/amazon-talks-invest-in-openai-developer-of-chatgpt",
    "thumbnail_url": "https://i.guim.co.uk/img/media/7c8f5d23afeb72372c88c26affc6fa9abe607c64/1215_0_6980_5584/master/6980.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2aff7897fe0cc4a85cb85f32e6461c4b",
    "created_at": "2025-12-17T13:45:45.803Z",
    "topic": "tech"
  },
  {
    "slug": "the-new-chatgpt-images-is-here-15",
    "title": "The new ChatGPT Images is here [1.5]",
    "description": "OpenAI shipped an update to their ChatGPT Images feature - the feature that gained them 100 million new users in a week when they first launched it back in March, …",
    "fullText": "The new ChatGPT Images is here. OpenAI shipped an update to their ChatGPT Images feature - the feature that gained them 100 million new users in a week when they first launched it back in March, but has since been eclipsed by Google's Nano Banana and then further by Nana Banana Pro in November.\n\nThe focus for the new ChatGPT Images is speed and instruction following:\n\nIt makes precise edits while keeping details intact, and generates images up to 4x faster\n\nIt's also a little cheaper: OpenAI say that the new gpt-image-1.5 API model makes image input and output \"20% cheaper in GPT Image 1.5 as compared to GPT Image 1\".\n\nI tried a new test prompt against a photo I took of Natalie's ceramic stand at the farmers market a few weeks ago:\n\nAdd two kakapos inspecting the pots\n\nHere's the result from the new ChatGPT Images model:\n\nAnd here's what I got from Nano Banana Pro:\n\nThe ChatGPT Kākāpō are a little chonkier, which I think counts as a win.\n\nI was a little less impressed by the result I got for an infographic from the prompt \"Infographic explaining how the Datasette open source project works\" followed by \"Run some extensive searches and gather a bunch of relevant information and then try again\" (transcript):\n\nSee my Nano Banana Pro post for comparison.\n\nBoth models are clearly now usable for text-heavy graphics though, which makes them far more useful than previous generations of this technology.",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "chatgpt images",
      "gpt image",
      "openai",
      "feature",
      "cheaper",
      "model",
      "prompt",
      "here's"
    ],
    "qualityScore": 0.75,
    "link": "https://simonwillison.net/2025/Dec/16/new-chatgpt-images/",
    "thumbnail_url": "https://static.simonwillison.net/static/2025/pots-chatgpt-q80-half.jpg",
    "created_at": "2025-12-17T13:45:43.650Z",
    "topic": "tech"
  },
  {
    "slug": "openai-hires-former-uk-chancellor-to-lead-its-global-stargate-project",
    "title": "OpenAI hires former UK chancellor to lead its global Stargate project",
    "description": "The ChatGPT maker has hired former British Chancellor George Osborne to run the global arm of its \"Stargate\" AI infrastructure initiative.",
    "fullText": "Tech companies are snapping up former world leaders and politicians — and OpenAI is the latest to join the party.\n\nThe ChatGPT maker has hired former British chancellor George Osborne to run the global arm of its Stargate AI infrastructure initiative.\n\n\"I recently asked myself the question: what's the most exciting and promising company in the world right now? The answer I believe is OpenAI,\" wrote Osborne, who ran the UK Treasury from 2010 to 2016, in a Tuesday X post confirming the move.\n\nOsborne takes the role of managing director and head of OpenAI for Countries, an initiative launched by the AI startup in May that will see OpenAI partner with nations to build data centers and expand its $500 billion Stargate project beyond the US.\n\nThe former finance minister, who was a member of parliament in the right-leaning Conservative party until 2017, is the latest ex-British political heavyweight to join a US tech firm.\n\nRishi Sunak, the former UK prime minister, took on roles at OpenAI rival Anthropic and Microsoft as an advisor in October, while ex-deputy prime minister Nick Clegg worked as a senior executive on Meta's global affairs team from 2018 until stepping down at the start of 2025.\n\nBritish political salaries are dwarfed by the earnings of even midlevel employees at US tech companies. British prime ministers earn an annual salary of around £174,000 ($232,000), while salaries for research engineers at Meta can be as high as $400,000.\n\nOsborne's arrival comes as OpenAI continues to bulk up its executive ranks. The AI startup hired former Instacart and Meta exec Fidji Simo as its new CEO of applications in May, and this week hired veteran Google executive Albert Lee to lead its mergers and acquisitions team.",
    "readingTime": 2,
    "keywords": [
      "prime minister",
      "openai",
      "tech",
      "hired",
      "british",
      "executive",
      "latest",
      "join",
      "party",
      "initiative"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/openai-hires-george-osborne-uk-chancellor-global-stargate-2025-12",
    "thumbnail_url": "https://i.insider.com/694288b764858d02d216ef7e?width=1200&format=jpeg",
    "created_at": "2025-12-17T13:45:42.834Z",
    "topic": "finance"
  },
  {
    "slug": "openais-answer-to-googles-viral-nano-banana-pro-image-model-is-here",
    "title": "OpenAI's answer to Google's viral Nano Banana Pro image model is here",
    "description": "OpenAI announced a new version of ChatGPT Images on Tuesday, powered by a new flagship AI image generation model. It rolls out today.",
    "fullText": "Google threw down the AI image gauntlet last month. Now, OpenAI has answered.\n\nThe ChatGPT creator announced the rollout of a new flagship image generator on Tuesday, which the company says is capable of making faster, more precise edits to an AI image while maintaining details.\n\nIntroducing ChatGPT Images, powered by our flagship new image generation model.\n\n- Stronger instruction following\n- Precise editing\n- Detail preservation\n- 4x faster than before\n\nRolling out today in ChatGPT for all users, and in the API as GPT Image 1.5. pic.twitter.com/NLNIPEYJnr\n\nBut the more eye-catching change for many user will likely be the improvements to image quality and the AI model's ability to follow specific instructions.\n\nOpneAI said the new model offers \"clear improvements across a range of cases.\" In one example, the company showed off the differences between the old and new image model when prompted to generate a photorealistic scene in 1970s Chelsea, London.\n\nIn another example touting use cases for businesses using the company's image API, OpenAI compared the outputs showing a mechanic working on a car.\n\nWhile a model's ability to generate photorealistic images has become a popular point of comparison in the AI race, OpenAI's latest model can also generate animated images, graphics, and other styles of artwork.\n\nPerhaps to boost awareness of that, the company is rolling out a new Images feature within the ChatGPT app. While AI image generation was previously available within ChatGPT, OpenAI says the new dedicated Images feature is designed to \"spark inspiration and make creative exploration effortless.\"\n\nThe news comes just over three weeks after Google released its Nano Banana Pro AI image model alongside its flagship Gemini 3 LLM — both of which have received widespread praise and reignited the debate around whether Google had begun to overtake OpenAI in the AI race.\n\nGoogle's new AI image generator was lauded for its hyper-realistic AI images, which some people used over Thanksgiving to make it appear as if they had famous guests at the holiday dinner table.\n\nYuchen Jin, the cofounder and CTO of the startup Hyperbolic Labs, called OpenAI's new model \"Nano Banana Pro level in my tests.\"",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "model's ability",
      "generate photorealistic",
      "images feature",
      "google",
      "flagship",
      "generator",
      "faster",
      "precise"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-new-chatgpt-images-model-2025-12",
    "thumbnail_url": "https://i.insider.com/6941a89904eda4732f2d9dd9?width=1200&format=jpeg",
    "created_at": "2025-12-17T03:44:51.369Z",
    "topic": "finance"
  },
  {
    "slug": "meta-is-making-ai-core-to-how-we-work-with-the-help-of-tools-from-google-and-openai",
    "title": "Meta is making 'AI core to how we work' with the help of tools from Google and OpenAI",
    "description": "Meta has given employees access to Google Gemini and OpenAI's GPT-5 alongside its own AI tools to help boost productivity.",
    "fullText": "In its push to create an \"AI-first\" workplace, Meta is expanding employees' access to tools from rivals such as Google and OpenAI, Business Insider has learned.\n\nThe social media giant has been encouraging employees to integrate AI tools into nearly everything they do, according to multiple internal documents and posts seen by Business Insider.\n\nOne of the company's priorities is to \"make AI core to how we work,\" Meta's chief information officer, Atish Banerjea, told employees in a June memo outlining a plan to use Meta's own models — which use the naming convention \"Llama\" — alongside products from other firms.\n\nIn November, a Meta engineer said in an internal post that all employees have access to Google's Gemini 3 Pro and OpenAI's ChatGPT-5. The post included a list of AI tools Meta employees have access to, including their use cases. Business Insider has recreated the list below.\n\nA Meta spokesperson confirmed the revamped suite of AI tools and pointed to an earlier comment shared with Business Insider about AI adoption, stating: \"It's well-known that this is a priority, and we're focused on using AI to help employees with their day-to-day work.\"\n\nThe social media giant opened the floodgates to rival AI models in June.\n\nAmong those is an internal coding tool called Devmate that uses Anthropic's Claude, Business Insider previously reported. Google's Gemini and NotebookLM Pro are also available across the company to help employees \"work smarter and have more impact,\" Banerjea told employees in the June memo.\n\nMeta has invested tens of billions into its own consumer-facing AI models, and employees have access to an internal AI assistant called Metamate, which is built on its Llama models.\n\nAfter Meta struck a deal over the summer the startup Midjourney to weave its AI-image generator into its products and models, the company made the tool available to employees in October for \"concept and production uses\" to speed up design work and creative prototyping, according to an internal post ahead of the rollout, seen by Business Insider.\n\nGemini isn't the only Google tool Meta is embracing. The company migrated its internal productivity suite over the summer to Google Workspace — including Chat, Gmail, Docs, and Drive — describing the move in a June memo as a way to \"unlock AI-driven capabilities\" and better integrate with its expanding toolset.\n\nOn the engineering side, Meta has expanded access to agentic coding systems, adding Google's Gemini 3 Pro and exploring new integrations with tools like OpenAI's Codex CLI and Google's Gemini CLI. \"Rather than focusing on specific solutions, our strategy centers on outcomes: increasing productivity, accelerating development, and ensuring you have access to the best agentic coding experiences,\" Reality Labs executive Maher Saba told employees in a November memo seen by Business Insider.\n\nTo encourage adoption and experimentation, Meta has gamified the use of AI, Business Insider previously reported. Earlier this year, it launched an internal game called \"Level Up,\" which rewards employees with badges for using AI in different ways. Leaders are also tying performance to results achieved through AI, rewarding those who can prove \"AI-driven impact\" this year, and including it as part of performance reviews in 2026.\n\nHave a tip? Contact this reporter via email at jmann@businessinsider.com or Signal at jyotimann.11. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "google's gemini",
      "june memo",
      "gemini pro",
      "insider previously",
      "social media",
      "media giant",
      "agentic coding",
      "business insider",
      "employees",
      "internal"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-ai-tools-internal-google-gemini-openai-chatgpt-llama-claude-2025-12",
    "thumbnail_url": "https://i.insider.com/69401c2a832e0ef1ead635fd?width=1200&format=jpeg",
    "created_at": "2025-12-16T13:51:43.771Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-52-on-how-silicon-valley-views-europe-and-democracy-in-the-trump-ii-era",
    "title": "ChatGPT 5.2 on how Silicon Valley views Europe and democracy in the Trump II era",
    "description": "Shared via ChatGPT",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://chatgpt.com/share/6940fef3-0be0-8005-a5de-6f06700994cc",
    "thumbnail_url": "https://cdn.openai.com/chatgpt/share-og.png",
    "created_at": "2025-12-16T06:59:55.732Z",
    "topic": "tech"
  },
  {
    "slug": "google-ai-summaries-are-ruining-the-livelihoods-of-recipe-writers-its-an-extinction-event",
    "title": "Google AI summaries are ruining the livelihoods of recipe writers: ‘It’s an extinction event’",
    "description": "AI Mode is mangling recipes by merging instructions from multiple creators – and causing them huge dips in ad traffic\nThis past March, when Google began rolling out its AI Mode search capability, it began offering AI-generated recipes. The recipes were not all that intelligent. The AI had taken elements of similar recipes from multiple creators and Frankensteined them into something barely recognizable. In one memorable case, the Google AI failed to distinguish the satirical website the Onion from legitimate recipe sites and advised users to cook with non-toxic glue.\nOver the past few years, bloggers who have not secured their sites behind a paywall have seen their carefully developed and tested recipes show up, often without attribution and in a bastardized form, in ChatGPT replies.",
    "fullText": "AI Mode is mangling recipes by merging instructions from multiple creators – and causing them huge dips in ad traffic\n\nThis past March, when Google began rolling out its AI Mode search capability, it began offering AI-generated recipes. The recipes were not all that intelligent. The AI had taken elements of similar recipes from multiple creators and Frankensteined them into something barely recognizable. In one memorable case, the Google AI failed to distinguish the satirical website the Onion from legitimate recipe sites and advised users to cook with non-toxic glue.\n\nOver the past few years, bloggers who have not secured their sites behind a paywall have seen their carefully developed and tested recipes show up, often without attribution and in a bastardized form, in ChatGPT replies. They have seen dumbed-down versions of their recipes in AI-assembled cookbooks available for digital downloads on Etsy or on AI-built websites that bear a superficial resemblance to an old-school human-written blog. Their photos and videos, meanwhile, are repurposed in Facebook posts and Pinterest pins that link back to this digital slop.\n\nRecipe writers have no legal recourse because recipes generally are not copyrightable. Although copyright protects published or recorded work, they do not cover sets of instructions (although it can apply to the particular wording of those instructions).\n\nWithout this essential IP, many food bloggers earn their living by offering their work for free while using ads to make money. But now they fear that casual users who rely on search engines or social media to find a recipe for dinner will conflate their work with AI slop and stop trusting online recipe sites altogether.\n\n“There are a lot of people that are scared to even talk about what’s going on because it is their livelihood,” says Jim Delmage who, with his wife, Tara, runs the blog and YouTube channel Sip and Feast.\n\nMatt Rodbard, the founder and editor-in-chief of the website Taste, is even more pessimistic. Taste used to publish recipes more frequently, but now it mostly focuses on journalism and a podcast (which Rodbard hosts). “For websites that depend on the advertising model,” he says, “I think this is an extinction event in many ways.”\n\nThe holiday season is traditionally when food bloggers earn most of their ad revenue. For many, this year has been slower than usual. One blogger, Carrie Forrest of Clean Eating Kitchen, told Bloomberg that in the past two years, she has lost 80% of her traffic.\n\nOthers, like Delmage and Karen Tedesco, the author of the blog Familystyle Food, say their numbers, and ad revenue, have remained steady – so far. They attribute this to focusing their energies less on trying to game the search engines than on the long-term goal of attracting regular followers – and, in Delmage’s case, viewers.\n\nTedesco’s strategy has been to create recipes that rely on her experience and technical knowhow honed by years in restaurant kitchens and as a personal chef. Her Italian meatball recipe, for example, based on her mother’s, includes advice about which meat to use, an explanation of why milk-soaked breadcrumbs are essential for texture, and a dozen process photos and a video.\n\nBut she is still worried about the potential impact of AI. When she recently did a Google search for “Italian meatballs”, Familystyle Food appeared as the top result. Then she switched to AI Mode. There, she found the recipe had been Frankensteined – or “synthesized” as Gemini put it – into a new recipe with nine other sources (including Sip and Feast and a Washington Post recipe for Greek meatballs). The AI-generated recipe was little more than a list of ingredients and six basic steps with none of the details that make Tedesco’s recipe unique.\n\nAI Mode linked to all 10 recipes, including Tedesco’s, but, she says, “I don’t think many people are actually clicking on the source links. At this point, they’re absolutely trusting in the results that are getting thrown in their faces.”\n\nOther bloggers have seen a more definite impact on their viewership. Adam Gallagher, who runs Inspired Taste with his wife, Joanne, and who has become an outspoken critic of AI on social media, told the podcast Marketing O’Clock that since spring, he has noticed that while the number of times viewers saw links to the site on Google has increased, the number of actual site visitors has decreased. This indicates, to him, that users are satisfied with the search engine’s AI interpretation of Inspired Taste’s recipes.\n\nAfter the Gallaghers posted about the discrepancy on X and Instagram, a number of readers replied to say they had not realized there was a difference between the recipes on the blog and the version that showed up in Google searches. They had just appreciated the convenience of not having to click on another website, especially when Google’s page design was so clean and uncluttered.\n\nRodbard acknowledges that many food blogs have gotten ugly and overloaded with ads, which has exacerbated the problem. “Ad tech on these recipe blogs has gotten so bad, so many pop-up windows and so much crashing, we kind of lost as publishers,” he says.\n\nAccording to Tom Critchlow, the EVP of audience growth at Raptive, a media company that works with many food bloggers to find advertisers, it isn’t ads that are driving viewers away. It’s Google itself, with its changes to the algorithm and now with AI Mode, that’s making the sites harder to find.\n\nThere is some hope though: a survey of 3,000 US adults commissioned by Raptive showed that the more interaction people had with AI, the less they wanted to engage with it, and nearly half the respondents rated AI content less trustworthy than content made by a human.\n\nBut unless the public rebels against AI Mode, there is only so much bloggers can do. They can block OpenAI’s training crawler, which gathers information that ChatGPT uses to create content, including its own recipe generator, but theyare not necessarily willing to make themselves invisible to web searches; as Delmage puts it: “You can’t bite the hand that feeds you.”\n\nThere is also the option of moving over to a subscription model, such as Substack or Patreon, and keeping the recipes behind a paywall, but both Tedesco and Delmage point out that the most successful Substackers, like Caroline Chambers or David Lebovitz, came to the platform with much more substantial followings than they have. “If I were to give up my website or even try to go over to Substack, I would be broke,” Tedesco says.\n\nRodbard suggests that the analog version of the recipe blog, the cookbook, might be due for a comeback. Cookbooks, after all, offer the same experience of spending time and learning from a trusted source, and it’s likely the recipes have been tested. As a bonus, unlike phones or laptops, they don’t go dark when you neglect them for too long and you can splash tomato sauce on them without inflicting permanent damage. According to the market research firm Circana (formerly BookScan), sales of baking cookbooks are up 80% this year, but other areas have been relatively flat.\n\nBut AI bots are stealing from published cookbooks, too. When Meta was training its own AI, it compiled thousands of books into a dataset called Library Genesis (LibGen). Now unscrupulous publishers have raided LibGen and repackaged some of the books into dupes, which they are selling on Amazon.\n\nAs more people become aware of the amount of AI slop on the internet and how to identify it, Critchlow believes they will develop a greater appreciation for content produced by humans. “People will ultimately place a higher premium on being able to know that these recipes have been tested and made by somebody that I follow or somebody I respect or somebody that I like,” he says.\n\nThe recipe creators themselves are not so sure. “I’m putting my faith in that there’s always going to be a segment of people who really want to learn something,” Tedesco says. But as for the business of blogging itself, “it’s like a rolling tide. It’s always up and down and you have to roll with it and adapt.”",
    "readingTime": 7,
    "keywords": [
      "behind paywall",
      "social media",
      "search engines",
      "bloggers earn",
      "ai mode",
      "food bloggers",
      "recipe sites",
      "familystyle food",
      "recipes",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/15/google-ai-recipes-food-bloggers",
    "thumbnail_url": "https://i.guim.co.uk/img/media/6dad6c44cff97b68d6e8c95e43637b3b460a7a57/398_0_7162_5733/master/7162.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=674b4b0df33a8374ebfeee93b5c3e646",
    "created_at": "2025-12-15T18:57:52.494Z",
    "topic": "tech"
  },
  {
    "slug": "ifixits-new-ai-assistant-can-help-you-fix-almost-anything",
    "title": "iFixit's New AI Assistant Can Help You Fix Almost Anything",
    "description": "There are reasons to believe it will be more trustworthy than ChatGPT.",
    "fullText": "Generative AI has advanced to the stage where you can ask bots such as ChatGPT or Gemini questions about almost anything, and get reasonable-sounding responses—and now renowned gadget repair site iFixit has joined the party with an AI assistant of its own, ready and willing to solve any of your hardware problems.\n\nWhile you can already ask general-purpose chatbots for advice on how to repair a phone screen or diagnose a problem with a car engine, there's always the question of how accurate the AI replies will be. With FixBot, iFixit is trying to minimize mistakes by drawing on its vast library of verified repair guides, written by experts and users.\n\nThat's certainly reassuring: I don't want to waste time and money replacing a broken phone screen with a new display that's the wrong size or shape. And using a conversational AI bot to fix gadget problems is often going to feel like a more natural and intuitive experience than a Google search. As iFixit puts it, the bot \"does what a good expert does\" in guiding you to the right solutions.\n\nThe iFixit website has been around since 2003—practically ancient times, considering the rapid evolution of modern technology. The iFixit team has always prided itself on detailed, thorough, tested guides to repairing devices, and all of that information can now be tapped into by the FixBot tool.\n\niFixit says the bot is trained on more than 125,000 repair guides written by humans who have worked through the steps involved, as well as the question and answer forums attached to the site, and the \"huge cache\" of PDF manuals that iFixit has accumulated over the years that it's been business.\n\nThat gives me a lot more confidence that FixBot will get its answers right, compared to whatever ChatGPT or Gemini might tell me. iFixit hasn't said what AI models are powering the bot—only that they've been \"hand-picked\"—and there's also a custom-built search engine included to select data sources from the repair archives on the site.\n\n\"Every answer starts with a search for guides, parts, and repairs that worked,\" according to the iFixit team, and that conversational approach you'll recognize from other AI bots is here too: If you need clarification on something, then you can ask a follow-up question. In the same way, if the AI bot needs more information or specifics, it will ask you.\n\nIt's designed to be fast—responses should be returned in seconds—and the iFixit team also talks about an \"evaluation harness\" that tests the FixBot responses against thousands of real repair questions posed and answered by humans. That extra level of fact-checking should reduce the number of false answers you get.\n\nHowever, it's not perfect, as iFixit admits: \"FixBot is an AI, and AI sometimes gets things wrong.\" Whether or not those mistakes will be easy to spot remains to be seen, but users of the chatbot are being encouraged to upload their own documents and repair solutions to fix gaps in the knowledge that FixBot is drawing on.\n\niFixit says the FixBot is going to be free for everyone to use, for a limited time. At some point, there will be a free version with limitations, and paid tiers with the full set of features—including support for voice input and document uploads. You can give it a try for yourself now on the iFixit website.\n\nI was reluctant to deliberately break one of my devices just so FixBot could help me repair it, but I did test it with a few issues I've had (and sorted out) in the past. One was a completely dead SSD drive stopping my Windows PC from booting: I started off with a vague description about the computer not starting up properly, and the bot did a good job at narrowing down what the problem was, and suggesting fixes.\n\nIt went through everything I had already tried when the problem happened, including trying System Repair and troubleshooting the issue via the Command Prompt. Eventually, via a few links to repair guides on the iFixit website, it did conclude that my SSD drive had been corrupted by a power cut—which I knew was what had indeed happened.\n\nI also tested the bot with a more general question about a phone restarting at random times—something one of my old handsets used to do. Again, the responses were accurate, and the troubleshooting steps I was asked to try made a lot of sense. I was also directed to the iFixit guide for the phone model.\n\nThe bot is as enthusiastic as a lot of the others available now (I was regularly praised for the \"excellent information\" I was providing), and does appear to know what it's talking about. This is one of the scenarios where generative AI shows its worth, in distilling a large amount of information based on natural language prompts.\n\nThere's definitely potential here: Compare this approach to having to sift through dozens of forum posts, web articles, and documents manually. However, there's always that nagging sense that AI makes mistakes, as the on-screen FixBot disclaimer says. I'd recommend checking other sources before doing anything drastic with your hardware troubleshooting.",
    "readingTime": 5,
    "keywords": [
      "ssd drive",
      "phone screen",
      "ifixit website",
      "ifixit team",
      "repair guides",
      "there's",
      "it's",
      "mistakes",
      "search",
      "troubleshooting"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ifixit-fixbot-assistant-repairs?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCH1RZ8RMAA4NM1GPZH9YTVJ/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-15T18:57:47.741Z",
    "topic": "tech"
  },
  {
    "slug": "attackers-are-spreading-malware-through-chatgpt",
    "title": "Attackers Are Spreading Malware Through ChatGPT",
    "description": "Be careful with ChatGPT or Grok's tech advice.",
    "fullText": "You (hopefully) know by now that you can't take everything AI tells you at face value. Large language models (LLMs) sometimes provide incorrect information, and threat actors are now using paid search ads on Google to spread conversations with ChatGPT and Grok that appear to provide tech support instructions but actually direct macOS users to install an infostealing malware on their devices.\n\nThe campaign is a variation on the ClickFix attack, which often uses CAPTCHA prompts or fake error messages to trick targets into executing malicious commands. But in this case, the instructions are disguised as helpful troubleshooting guides on legitimate AI platforms.\n\nKaspersky details a campaign specific to installing Atlas for macOS. If a user searches \"chatgpt atlas\" to find a guide, the first sponsored result is a link to chatgpt.com with the page title \"ChatGPT™ Atlas for macOS – Download ChatGPT Atlas for Mac.\" If you click through, you'll land on the official ChatGPT site and find a series of instructions for (supposedly) installing Atlas.\n\nHowever, the page is a copy of a conversation between an anonymous user and the AI—which can be shared publicly—that is actually a malware installation guide. The chat directs you to copy, paste, and execute a command in your Mac's Terminal and grant all permissions, which hands over access to the AMOS (Atomic macOS Stealer) infostealer.\n\nA further investigation from Huntress showed similarly poisoned results via both ChatGPT and Grok using more general troubleshooting queries like \"how to delete system data on Mac\" and \"clear disk space on macOS.\"\n\nAMOS targets macOS, gaining root-level privileges and allowing attackers to execute commands, log keystrokes, and deliver additional payloads. BleepingComputer notes that the infostealer also targets cryptocurrency wallets, browser data (including cookies, saved passwords, and autofill data), macOS Keychain data, and files on the filesystem.\n\nIf you're troubleshooting a tech issue, carefully vet any instructions you find online. Threat actors often use sponsored search results as well as social media platforms to spread instructions that are actually ClickFix attacks. Never follow any guidance that you don't understand, and know that if it asks you to execute commands on your device using PowerShell or Terminal to \"fix\" a problem, there's a high likelihood that it's malicious—even if it comes from a search engine or LLM you've used and trusted in the past.\n\nOf course, you can potentially turn the attack around by asking ChatGPT (in a new conversation) if the instructions are safe to follow. According to Kaspersky, the AI will tell you that they aren't.",
    "readingTime": 3,
    "keywords": [
      "installing atlas",
      "threat actors",
      "execute commands",
      "chatgpt and grok",
      "macos",
      "instructions",
      "search",
      "targets",
      "troubleshooting",
      "spread"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/chatgpt-grok-tech-advice-malware?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCFHKGX7QDA7SPFPM73VFFVJ/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-15T18:57:47.692Z",
    "topic": "tech"
  },
  {
    "slug": "gpt-prompt-from-searchbar-chatgpt-directly-from-the-browser-omnibox",
    "title": "GPT Prompt from Searchbar – ChatGPT directly from the browser omnibox",
    "description": "A lightweight Chrome extension that allows you to search/prompt ChatGPT directly from your browser's address bar (Omnibox). - ParasKoundal/GPTSearch",
    "fullText": "ParasKoundal\n\n /\n\n GPTSearch\n\n Public\n\n A lightweight Chrome extension that allows you to search/prompt ChatGPT directly from your browser's address bar (Omnibox).\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n ParasKoundal/GPTSearch",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/ParasKoundal/GPTSearch",
    "thumbnail_url": "https://opengraph.githubassets.com/9674a6e7ffcf86168c17319b094a63d29ac3590c31040f4ff85049a30e2ca531/ParasKoundal/GPTSearch",
    "created_at": "2025-12-15T03:59:06.427Z",
    "topic": "tech"
  },
  {
    "slug": "wall-street-sees-ai-bubble-coming-and-is-betting-on-what-pops-it",
    "title": "Wall Street Sees AI Bubble Coming and Is Betting on What Pops It",
    "description": "It’s been three years since OpenAI set off euphoria over artificial intelligence with the release of ChatGPT. And while the money is still pouring in, so are the doubts about whether the good times can last.",
    "fullText": "MarketsBy Jeran WittensteinSaveIt’s been three years since OpenAI set off euphoria over artificial intelligence with the release of ChatGPT. And while the money is still pouring in, so are the doubts about whether the good times can last.From a recent selloff in the shares of Nvidia Corp., to Oracle Corp.’s plunge after reporting mounting spending on AI, to souring sentiment around a network of companies exposed to OpenAI, signs of skepticism are increasing. Looking to 2026, the debate among investors is whether to rein in AI exposure ahead of a potential bubble popping or double down to capitalize on the game-changing technology.",
    "readingTime": 1,
    "keywords": [
      "openai"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bloomberg.com/news/articles/2025-12-14/wall-street-sees-an-ai-bubble-forming-and-is-gaming-what-pops-it",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ijt8j35wEhRU/v1/1200x800.jpg",
    "created_at": "2025-12-14T18:50:23.869Z",
    "topic": "gaming"
  },
  {
    "slug": "prompt-engineering-is-a-hidden-tax-chatgpt-vs-copyai-vs-vertical-agents",
    "title": "Prompt Engineering Is a \"Hidden Tax\": ChatGPT vs. Copy.ai vs. Vertical Agents",
    "description": "A definitive 2025 comparison of Vect AI, Copy.ai, and ChatGPT. Discover why specialized 'Marketing Operating Systems' are replacing generalist chatbots for serious growth teams.",
    "fullText": "Every week, a new \"ChatGPT Killer\" launches on Product Hunt. But for distinct, revenue-focused marketers, the noise is distracting. You don't need another chatbot that can write a mediocre limerick about a pirate. You don't need a tool that requires a 50-paragraph \"mega-prompt\" just to sound human.\n\nYou need a system that drives Revenue.\n\nIn 2025, the AI landscape has calcified into three distinct categories. Understanding this split is critical before you swipe your credit card.\n\nThis guide isn't just a list of features or a rehash of pricing pages. It is a \"Stress Test\" of how these tools handle the real, messy, complex work of a modern high-growth marketing team.\n\nTo understand the tool, you must understand the brain behind it. Each platform was built with a fundamentally different thesis about what a marketer needs.\n\nOpenAI built a general-purpose reasoning engine. It is brilliant at coding, summarizing history, and casual chat. But it has no \"opinion\" on marketing. It doesn't know that a headline should differ between a cold email and a landing page unless you explicitly tell it the precise psychological framework to use.\n\nCopy.ai pivoted from a simple writing tool to a \"GTM AI Platform.\" Their thesis is that marketing is a series of data flows. Scrape LinkedIn -> Enrich Data -> Write Email -> Send.\n\nVect AI was built with a single thesis: Strategy should come before generation. It assumes you want the best marketing outcome, not just any text. It is \"State-Aware\"—meaning it permanently remembers your brand voice, audience pains, and product details.\n\nLet's move away from theory and look at three common, painful scenarios every marketer faces.\n\nYou need to fix a landing page that isn't converting.\n\nChatGPT Approach:\nYou paste the text. You ask: \"Make this better.\" ChatGPT changes a few synonyms. It sounds robotic. You spend 15 minutes explaining your customer persona. It eventually gives you something passable but generic.\n\nCopy.ai Approach:\nYou look for a \"Landing Page Rewriter\" workflow. You run it. It generates 10 variations. You have to read all 10 to decide which one is good.\n\nVect AI Approach:\nYou open the Conversion Killer Detector. You paste your URL. The Agent scans the live page, identifies \"Passive Voice\" and \"Weak Value Props,\" assigns a \"Panic Score,\" and auto-rewrites the specific sections that are killing sales.\n\nYou released a new feature. You need a blog, 10 tweets, 3 LinkedIn posts, and a newsletter.\n\nCopy.ai Approach (The Engineer's Way):\n\nVect AI Approach (The Strategy Way):\n\nThis is the single biggest differentiator.\n\nChatGPT has \"Custom Instructions,\" but they are weak. It often forgets them in long threads.\nCopy.ai uses \"Brand Voice\" snippets, but you have to manually select them for every workflow.\n\nVect AI uses a Global Brand Kernel.\nWhen you onboard, you define your audience, your pain points, and your \"Anti-Persona\" (who you don't want).\n\nMost AI tools are \"Yes Men.\" If you ask them to write a boring, 3,000-word email, they will say \"Sure!\" and do it.\n\nVect AI has a conscience. It's called the Resonance Engine.\nBefore you publish, you can run your content through this simulation. It uses historical data from millions of high-performing ads and posts to predict success.\n\nOnly Vect AI protects you from looking stupid. The others just execute orders.\n\nMarketing isn't just text. It's visual.\n\nCopy.ai is purely text-based. You need a separate Midjourney subscription for images.\nChatGPT has DALL-E 3, which is fun but often too \"cartoony\" for enterprise brands.\n\nVect AI includes a commercial-grade AI Ad Creative Studio and Marketing Video Ad generator.\n\nWhen comparing prices, most people look at the monthly fee. This is a mistake. You must calculate the Time Cost.\n\nTo give you a sense of the specialization, look at how granular Vect AI gets compared to the generic \"Write an Article\" button in other tools:\n\nThe era of \"Generalist AI\" for professionals is ending. We are entering the era of \"Agentic Workflows.\"\n\nStop fighting with prompts. Stop building workflows. Start leading your market.\n\nYou have the blueprint. Now you need the engine. Launch the AI agent for \"Conversion Killer Detector\" and get results in minutes.",
    "readingTime": 4,
    "keywords": [
      "conversion killer",
      "killer detector",
      "copy.ai approach",
      "landing page",
      "vect ai",
      "brand voice",
      "marketing",
      "look",
      "don't",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://blog.vect.pro/vect-vs-copy-ai",
    "thumbnail_url": "https://blog.vect.pro/vectai.png",
    "created_at": "2025-12-14T18:50:17.665Z",
    "topic": "tech"
  },
  {
    "slug": "spacex-sets-800-billion-valuation-confirms-2026-ipo-plans",
    "title": "SpaceX sets $800 billion valuation, confirms 2026 IPO plans",
    "description": "The valuation vaults past the previous record of $500 billion that ChatGPT owner OpenAI set in October.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/13/spacex-ipo-plan-2026-secondary-offering-insider-share-sale-800-billion-valuation/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2219676771-e1765642050600.jpg?resize=1200,600",
    "created_at": "2025-12-13T18:47:58.713Z",
    "topic": "business"
  },
  {
    "slug": "my-husband-used-chatgpt-to-write-our-anniversary-card-surprisingly-im-not-mad-about-it",
    "title": "My husband used ChatGPT to write our anniversary card. Surprisingly, I'm not mad about it.",
    "description": "I was convinced AI was ruining human connection. Then my husband used it to write the most heartfelt anniversary card I've ever received.",
    "fullText": "As a geriatric millennial and a licensed psychologist, I often lament technological changes that I see as threatening to human interactions, such as AI.\n\nEven though I try to find the gray in all areas of life, I've been rather black-and-white about AI because I worry it's diminishing our ability to relate to one another.\n\nHowever, a recent experience with my husband has made me more curious about AI.\n\nWe recently celebrated our ninth wedding anniversary, and unbeknownst to me at the time, my husband used GenAI to write my card. He was traveling at the time, so he sent flowers and chocolate, with a card attached. The flowers were severely underwhelming, and I'm not just being a brat. My husband even called the company upon returning to express his frustration with how different they appeared in person.\n\nBut when I read the note accompanying the tiny arrangement, I teared up right away. It was heartwarming, meaningful, and really on the nose.\n\nMy husband's lack of romantic effusiveness has historically been frustrating to me. He has made steady progress in this area, and even ending cards with \"love\" more frequently marks such improvement. So, this note felt in line with his desire to share more emotionality, and I was touched that he'd taken that step just because he knew it would make me happy.\n\nMy husband knows about my big feelings about AI, and he shares many of them. It seems like everyone relies on ChatGPT for relationship advice, and the most common use of META AI right now is to ask for guidance about difficult conversations with loved ones or bosses. I see it in my therapy practice all the time, and I'm skeptical about how sound the AI advice my clients receive actually is.\n\nSo, when I, a historically staunch critic of all things AI, found out that my husband had used ChatGPT to create our anniversary card, I admit I had some strong feelings.\n\nAs lacking in the sentimental department as my husband can be, I am at the opposite end of the spectrum — a hopeless romantic, as they say. I save just about every card I get, tucked into a neat little box. When I reread my husband's card before putting it away, I found myself lingering on some of the words. \"…life we built together\" especially sat with me. It's a common phrase, but it's not in my husband's emotional lexicon. It almost sounded like somebody else's voice. And given that we can't go more than a few days without reading about how AI will ruin people's ability to think for themselves, I had that thought. \"Did AI write one of the sweetest parts of my anniversary card?\"\n\nI was delicate. I reiterated how much I'd enjoyed our belated anniversary celebration before I asked: \"Did you use AI to write our anniversary card?\" He copped to it, grinning from ear to ear.\n\nShockingly to me, I wasn't mad. This discovery actually opened a door for us to talk about how useful it is to get a little help writing a card. The pre-printed messages often feel overly mushy, and the \"blank inside\" cards ask us to get vulnerable in expressing our emotions. For many, including my husband, that's incredibly difficult. The result is often a message that neither the giver nor the receiver feels particularly happy about.\n\nBut this year's note was perfectly balanced. The right amount of gush without the melodrama. I felt seen, and I felt it captured our relationship well.\n\nIf AI can help people express their love for one another, that can't be a bad thing, right? Besides, is asking ChatGPT for help any less authentic than using a pre-printed Hallmark message? ChatGPT provided more accurate information about our relationship than a Hallmark writer ever could, and it offered guidance for a note that made me feel loved and appreciated.\n\nWhile the idea of a world in which chatbots replace our friends and therapists is still deeply concerning to me, this recent experience has helped me find the gray in my previously black-and-white view: I don't know that I'll ever come around to AI replacing human connection, but I do see the utility now in using AI to help us enhance our existing human connections.\n\nAt the very least, I look forward to more anniversaries with really accurate love notes.",
    "readingTime": 4,
    "keywords": [
      "anniversary card",
      "husband",
      "note",
      "chatgpt",
      "human",
      "it's",
      "husband's",
      "love",
      "relationship",
      "gray"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/was-skeptical-of-ai-until-husband-used-it-anniversary-2025-12",
    "thumbnail_url": "https://i.insider.com/691e283cabd5e944effb1228?width=1200&format=jpeg",
    "created_at": "2025-12-13T18:47:57.892Z",
    "topic": "finance"
  },
  {
    "slug": "disney-wants-you-to-aigenerate-yourself-into-your-favorite-marvel-movie",
    "title": "Disney wants you to AI-generate yourself into your favorite Marvel movie",
    "description": "The media company is investing $1bn in OpenAI – and allowing its characters to be used in generated videos\nUsers of OpenAI’s video generation app will soon be able to see their own faces alongside characters from Marvel, Pixar, Star Wars and Disney’s animated films, according to a joint announcement from the startup and Disney on Thursday. Perhaps you, Lightning McQueen and Iron Man are all dancing together in the Mos Eisley Cantina.\nSora is an app made by OpenAI, the firm behind ChatGPT, which allows users to generate videos of up to 20 seconds through short text prompts. The startup previously attempted to steer Sora’s output away from unlicensed copyrighted material, though with little success, which prompted threats of lawsuits by rights holders.\n Continue reading...",
    "fullText": "The media company is investing $1bn in OpenAI – and allowing its characters to be used in generated videos\n\nUsers of OpenAI’s video generation app will soon be able to see their own faces alongside characters from Marvel, Pixar, Star Wars and Disney’s animated films, according to a joint announcement from the startup and Disney on Thursday. Perhaps you, Lightning McQueen and Iron Man are all dancing together in the Mos Eisley Cantina.\n\nSora is an app made by OpenAI, the firm behind ChatGPT, which allows users to generate videos of up to 20 seconds through short text prompts. The startup previously attempted to steer Sora’s output away from unlicensed copyrighted material, though with little success, which prompted threats of lawsuits by rights holders.\n\nDisney announced that it would invest $1bn in OpenAI and, under a three-year deal perhaps worth even more than that large sum, that it would license about 200 of its iconic characters – from R2-D2 to Stitch – for users to play with in OpenAI’s video generation app.\n\nAt a time of intense anxiety in Hollywood over the impact of AI on the livelihoods of writers, actors, visual effects artists and other creatives, Disney stressed its agreement with OpenAI would not cover talent likenesses or voices.\n\nThe announcement was framed as an extraordinary opportunity to empower fans.\n\nThink of the “fan-inspired Sora short form videos”, as Disney called them in a press release – akin to taking an AI-generated version of a photo with Princess Jasmine at Disney World. OpenAI included screenshots of these kinds of videos in its press release, indicating how the two companies expect people to use the app’s new cast. Sora already allows users to generate videos that include their own likenesses.\n\nBob Iger, Disney’s CEO, said the licensing deal would place “imagination and creativity directly into the hands of Disney fans in ways we’ve never seen before”.\n\nThey may even offer a chance at wide viewership, with some fan-made videos being displayed on the Disney+ streaming service, a move seemingly designed to compete with TikTok’s and YouTube Shorts’ infinite feeds, which themselves often include clips of popular TV shows and movies.",
    "readingTime": 2,
    "keywords": [
      "press release",
      "generation app",
      "allows users",
      "generate videos",
      "characters",
      "disney",
      "announcement",
      "startup",
      "deal",
      "likenesses"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2025/dec/11/disney-openai-sora",
    "thumbnail_url": "https://i.guim.co.uk/img/media/d99390e95d50b47f91bcc8a3130e524c618e635d/436_0_4324_3458/master/4324.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=67d482fa7d7bed846eb736aeb1dfde1c",
    "created_at": "2025-12-12T13:47:25.564Z",
    "topic": "tech"
  },
  {
    "slug": "openai-launches-gpt52-ai-model-with-improved-capabilities",
    "title": "OpenAI launches GPT-5.2 AI model with improved capabilities",
    "description": "OpenAI on Thursday launched its GPT-5.2 artificial intelligence model, after CEO Sam Altman reportedly issued an internal \"code red\" in early December pausing non‑core projects and",
    "fullText": "Dec 11 (Reuters) - OpenAI on Thursday launched its GPT-5.2 artificial intelligence model, after CEO Sam Altman ​reportedly issued an internal \"code red\" in early ‌December pausing non‑core projects and redirecting teams to accelerate development in ‌response to Google's Gemini 3.\n\nGPT-5.2 comes with improvements in general intelligence, coding and long-context understanding, the company said in a statement.\n\nThe new model is expected to bring even ⁠more economic value for ‌users, as it is better at creating spreadsheets, building presentations and handling complex multi-step ‍projects, OpenAI said.\n\nAlphabet's Google launched the latest version of its Gemini in November, highlighting Gemini 3's lead position on several ​popular industry leaderboards that measure AI model performance.\n\n\"Gemini ‌3 has had less of an impact on our metrics than we feared,\" Altman said in an interview with CNBC on Thursday, alongside Disney CEO Bob Iger.\n\nDisney said on Thursday it is investing $1 billion in ⁠OpenAI and will let the ​startup use characters from Star Wars, ​Pixar and Marvel franchises in its Sora AI video generator.\n\nMicrosoft-backed OpenAI said that it currently ‍has no ⁠plans to deprecate GPT‑5.1, GPT‑5, or GPT‑4.1 in the API.\n\nGPT-5.2 Instant, Thinking, and Pro will begin ⁠rolling out in ChatGPT on Thursday, beginning with paid plans.",
    "readingTime": 2,
    "keywords": [
      "model",
      "launched",
      "intelligence",
      "projects",
      "disney",
      "plans",
      "openai",
      "gemini",
      "altman"
    ],
    "qualityScore": 0.85,
    "link": "https://tech.yahoo.com/ai/chatgpt/articles/openai-launches-gpt-5-2-185713739.html",
    "thumbnail_url": "https://s.yimg.com/lo/mysterio/api/53E953B72061EAD331E14221749904B05C017BE83D84CD2331F3FA5A9D9F4632/subgraphmysterio/resizefit_w1200;quality_90;format_webp/https:%2F%2Fmedia.zenfs.com%2Fen%2Freuters.com%2Fdec31ed411aedabd7e3231dabf2dd50f",
    "created_at": "2025-12-12T06:59:01.415Z",
    "topic": "tech"
  },
  {
    "slug": "openai-aims-to-silence-concerns-it-is-falling-behind-in-the-ai-race-with-release-of-new-model-gpt52",
    "title": "OpenAI aims to silence concerns it is falling behind in the AI race with release of new model GPT-5.2",
    "description": "OpenAI said its new model outperforms those from rivals Google and Anthropic across a wide range of evaluations.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/openai-gpt-5-2-launch-aims-to-silence-concerns-it-is-falling-behind-google-anthropic-code-red/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2198334790-e1765478723707.jpg?resize=1200,600",
    "created_at": "2025-12-12T03:50:28.964Z",
    "topic": "business"
  },
  {
    "slug": "how-openais-latest-model-will-impact-chatgpt",
    "title": "How OpenAI's Latest Model Will Impact ChatGPT",
    "description": "GPT-5.2 is here, and, according to OpenAI, better than ever.",
    "fullText": "OpenAI is having a hell of a day. First, the company announced a $1 billion equity investment from Disney, alongside a licensing deal that will let Sora users generate videos with characters like Mickey Mouse, Luke Skywalker, and Simba. Shortly after, OpenAI revealed its latest large language model: GPT-5.2.\n\nOpenAI says that this new GPT model is particularly useful for \"professional knowledge work.\" The company advertises how GPT-5.2 is better than previous models at making spreadsheets, putting together presentations, writing code, analyzing pictures, and working through multi-step projects. For this model, the company also gathered insights from tech companies: Supposedly, Notion, Box Shopify, Harvey, and Zoom all find GPT-5.2 to have \"state-of-the-art long-horizon reasoning,\" while Databricks, Hex, and Triple Whale believe GPT-5.2 to be \"exceptional\" with both agentic data science and document analysis tasks.\n\nBut most of OpenAI's user base aren't professionals. Most of the users who will interact with GPT-5.2 are using ChatGPT, and many of those for free, at that. What can those users expect when OpenAI upgrades the free version of ChatGPT with these new models?\n\nOpenAI says that GPT-5.2 will improve ChatGPT's \"day to day\" functionality. The new model supposedly makes the chatbot more structured, reliable, and \"enjoyable to talk to,\" though I've never found the last part to be necessarily true.\n\nGPT-5.2 will impact the ChatGPT experience differently depending on which of the three models you happen to be using. According to OpenAI, GPT-5.2 Instant is for \"everyday work and learning.\" It's apparently better for questions seeking information about certain subjects, how-to questions and walkthroughs, technical writing, and translations—maybe ChatGPT will get you to give up your Duolingo obsession.\n\nGPT-5.2 Thinking, however, is supposedly made for \"deeper work.\" OpenAI wants you using this model for coding, summarizing lengthy documents, answering queries about files you send to ChatGPT, solving math and logic problems, and decision making. Finally, there's GPT-5.2 Pro, OpenAI's \"smartest and most trustworthy option\" for the most complicated questions. The company says 5.2 Pro produces fewer errors and stronger performance compared to previous models.\n\nOpenAI says that this latest update improves how the models responds to distressing prompts, such as those showing signs of suicide, self-harm, or emotional dependence on the AI. As such, the company says this model has \"fewer undesirable responses\" in GPT-5.2 Instant and Thinking compared to GPT-5.1 Instant and Thinking. In addition, the company is working on an \"age prediction model,\" which will automatically place content restrictions on users who the model think are under 18.\n\nThese safety improvements are important—critical, even—as we start to understand the correlations between chatbots and mental health. The company has admitted its failure in \"recognizing signs of delusion,\" as users turned to the tool for emotional support. In some cases, ChatGPT fed into delusional thinking, encouraging people's dangerous beliefs. Some families have even sued companies like OpenAI over claims that their chatbots helped or encouraged victims commit suicide.\n\nActively acknowledging improvements to user safety is undoubtedly a good thing, but I think companies like OpenAI still have a lot to reckon with—and a long way to go.\n\nOpenAI says GPT-5.2 Instant, Thinking, and Pro will all roll out today, Thursday, Dec. 11, to paid plans. Developers can access the new models in the API today, as well.\n\nDisclosure: Lifehacker’s parent company, Ziff Davis, filed a lawsuit against OpenAI in April, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
    "readingTime": 3,
    "keywords": [
      "gpt instant",
      "models openai",
      "ziff davis",
      "chatgpt",
      "users",
      "supposedly",
      "model",
      "latest",
      "user",
      "free"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-openais-latest-model-will-impact-chatgpt?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC7EHPGQ2G4FWBQABYH2TCJD/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-12T03:50:27.049Z",
    "topic": "tech"
  },
  {
    "slug": "disney-bets-1-billion-on-openai-in-deal-that-opens-its-vault-of-characters-to-chatgpt-and-sora",
    "title": "Disney bets $1 billion on OpenAI in deal that opens its vault of characters to ChatGPT and Sora",
    "description": "Darth Vader and other Disney characters are coming to ChatGPT and OpenAI's Sora AI video app as part of a three-year licensing deal.",
    "fullText": "Darth Vader is coming to ChatGPT and OpenAI's Sora AI video app.\n\nThe House of Mouse and OpenAI struck a three-year licensing agreement on Thursday to make Disney \"the first major content licensing partner on Sora.\"\n\nIt's also investing $1 billion into the AI pioneer and receiving warrants to purchase additional equity.\n\nShares of Disney climbed over 2% after the opening bell.\n\n\"As part of this new, three-year licensing agreement, Sora will be able to generate short, user-prompted social videos that can be viewed and shared by fans, drawing from a set of more than 200 animated, masked and creature characters from Disney, Marvel, Pixar and Star Wars, including costumes, props, vehicles, and iconic environments,\" OpenAI said in a Thursday announcement.\n\nIn addition to striking a licensing deal, Disney is also becoming a \"major customer\" of the AI company, according to the announcement, and buying ChatGPT enterprise licenses for its employees.\n\nWhile Sora, OpenAI's TikTok-like AI video app, has been generating buzz and downloads since its launch earlier this year, users of the company's more popular product, ChatGPT, will also have access to AI versions of Disney's characters as part of the deal.\n\nThe AI-generated Disney characters will be available starting in early 2026.\n\nThe move is likely to prove controversial in Hollywood, where many actors have publicly voiced concern about AI use and concerns over how their likeness is used. Disney and OpenAI stated that \"the agreement does not include any talent likenesses or voices.\"\n\nCreators are core to Disney, and its CEO Bob Iger stressed that the deal represented no threat to creators.\n\n\"I think it honors them and respects them, in part because there's a license to be associated with it,\" he said on CNBC's \"Squawk on the Street\" on Thursday.\n\n\"The other thing it does is it enables us to be comfortable that Open AI is putting guardrails essentially around how these are used, so that really there's nothing for us to be concerned about from a consumer perspective, meaning this will be a safe environment and a safe way for consumers to engage with our founders in a new way,\" he added.\n\nIger hinted at such a transaction during the company's most recent earnings call, making extensive comments about the potential he sees for AI to enhance Disney's direct-to-consumer strategy. He said the company was having extensive talks with AI companies to protect its IP as well as generate more engagement with users.\n\nHis comments demonstrate how Disney — like other Hollywood players — is looking for new ways for people to interact with its platforms and brands as user-generated content platforms and independent creators gain popularity.\n\nDisney, like those other players, has an engagement problem. The time people spend on streaming has stayed essentially flat over the past few years, despite increased spending on content, while YouTube has grown. The bet with AI is that it can get people to spend more time on its platforms by giving them more ways to play around with its famous franchises.\n\nThe companies hinted as much in the announcement, saying that they would \"collaborate to utilize OpenAI's models to power new experiences for Disney + subscribers.\"\n\nDisney is also wary of the tech's risk to its IP. In June, Disney, along with Comcast's NBCUniversal studio business, sued AI company Midjourney, claiming its tech created unauthorized copies of works ranging from Star Wars to The Simpsons. Midjourney denied the claims in its legal response. The suit is ongoing.\n\nDisney's $1 billion cash infusion comes at a critical time for OpenAI, but it's a drop in the bucket compared to the roughly $1.4 trillion the AI company has pledged to spend over the next eight years on data centers.\n\nOpenAI CEO Sam Altman had previously said that large rights holders would ultimately welcome their content being used on Sora, provided it was done with proper guardrails in place. His comments came after OpenAI stepped up restrictions on the Sora app in the wake of viral user-generated videos depicting SpongeBob as Walter White and Pikachu in \"Saving Private Ryan.\"\n\n\"Most of the rights holders that I've spoken to are actually extremely excited to get their content in here,\" Altman told tech analyst Ben Thompson in October. \"They just want to be able to set more restrictions than they would need for images because videos feel different.\"",
    "readingTime": 4,
    "keywords": [
      "rights holders",
      "three-year licensing",
      "licensing agreement",
      "content",
      "disney",
      "chatgpt",
      "openai's",
      "videos",
      "characters",
      "announcement"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/disney-openai-licensing-deal-ai-characters-sora-chatgpt-2025-12",
    "thumbnail_url": "https://i.insider.com/693ae3e9832e0ef1ead60bfc?width=1200&format=jpeg",
    "created_at": "2025-12-11T18:58:22.665Z",
    "topic": "finance"
  },
  {
    "slug": "when-supply-chains-become-autonomous",
    "title": "When Supply Chains Become Autonomous",
    "description": "A testbed built around one of management education’s most enduring simulations, the MIT Beer Distribution Game, has shown that the latest generation of generative AI models can now autonomously manage supply chains. Systems using advanced reasoning models like GPT-5 and Llama 4 adapted to changing conditions, minimized costs, and overcame the bullwhip effect. But managers should be aware that success depends on model selection, guardrails, curated data sharing, and prompt design. Such autonomous AI agents will allow human managers to focus on higher-value functions.",
    "fullText": "When Supply Chains Become Autonomous by Carol Long, David Simchi-Levi, Andre P. Calmon and Flavio P. CalmonDecember 11, 2025PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintLess than a year ago, it seemed like that day when generative AI would bring about a new era of supply chain autonomy—one where AI could adeptly make all the inventory and logistics decisions—was still far off. But to the astonishment of many experts, including us, that day has arrived—at least in the lab.",
    "readingTime": 1,
    "keywords": [
      "supply"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2025/12/when-supply-chains-become-autonomous",
    "thumbnail_url": "/resources/images/article_assets/2025/12/Dec25_11_MorganeFadanelli.jpg",
    "created_at": "2025-12-11T18:58:21.935Z",
    "topic": "business"
  },
  {
    "slug": "i-tried-photoshop-in-chatgpt-and-it-went-better-than-i-expected",
    "title": "I Tried Photoshop in ChatGPT, and It Went Better Than I Expected",
    "description": "You can now get Adobe's AI image editor inside OpenAI's AI chatbot.",
    "fullText": "Generative AI tools continue to improve in terms of their photo editing capabilities, and OpenAI's latest upgrade brings Adobe Photoshop right inside your ChatGPT app window (alongside Adobe Acrobat for handling PDFs, and Adobe Express for graphic design). It's available to everyone, for free—you just need a ChatGPT account and an Adobe account.\n\nAs per Adobe, the idea is to make \"creativity accessible for everyone\" by plugging Photoshop tools directly into ChatGPT. The desktop version of Photoshop already comes with plenty of generative AI features of its own, so this is AI layered on top of more AI—but is it actually useful?\n\nAdobe Photoshop, Adobe Express and Adobe Acrobat are available now inside ChatGPT on the desktop, on the web, and on iOS. At the time of writing, you can also get Adobe Express inside ChatGPT for Android, with Photoshop and Acrobat \"coming soon.\" To weigh the capabilities of the new integration, I tested it in a desktop web browser.\n\nTo get started, all you need to do is type \"Photoshop\" at the start of your prompt: ChatGPT should recognize what you're trying to do, and select Adobe Photoshop as the tool to use for the next prompt. You'll also need to click through a couple of confirmation dialog boxes, and connect an Adobe account (if you don't have one, you can make one for free).\n\nWith all the connections and logins completed, Photoshop is then added to the overflow menu in the prompt box, so just click on the + (plus) to select it. You can start describing what you want to happen using the same natural, conversational language you'd use for any other ChatGPT prompt. You do need to also upload an image or provide a public link to one—if you don't do this before you submit your prompt, you'll be asked to do it after.\n\nYou don't need to know the names of all the Photoshop tools: Just describe what you want to happen and the relevant tools will be selected for you. One example Adobe gives is using the prompt \"make my image pop,\" which brings up the Bloom, Grain, and Lens Distortion effects—and each one can be adjusted via sliders on screen. It's actually quite simple to use.\n\nIf you do know the name of the tools you want, you can call them up by name, and the classic brightness and contrast sliders are a good place to start. You can either say something like \"make the picture brighter\" or \"adjust the image brightness\"—both will bring up an overlay you can use to make brightness adjustments, but if you use the former prompt, the image will already have been made a little brighter.\n\nChatGPT and Photoshop let you add edit upon edit as needed, and you can save the image at any stage. There's also the option to open your processed file in the Photoshop web app whenever you like: This web app uses a freemium model, with advanced features requiring a subscription, and seems to be what the ChatGPT integration is largely based on.\n\nAdobe offers a handy ChatGPT prompts cheat sheet you can browse through, which gives you a good idea of what's possible, and what you're still going to need Photoshop proper for. Note that you can specify certain parts of the image to focus on (like \"the face\" or \"the car\") but this depends on Photoshop-in-ChatGPT being able to correctly figure out where you want your selection to be. It needs to be pretty obvious and well delineated.\n\nWhen I tried cutting out objects and removing backgrounds, this worked well—but then I had to turn to Photoshop on the web to actually drop in a different background. There's no way to work with layers or masks here, and you can't remove people or objects from photos, either. Sometimes, however, you do get a spool of \"thinking\" from ChatGPT about how it can't do what the user is asking for.\n\nI was able to apply some nice colorizations here, via prompts like \"turn all the hues in this image to blue,\" and I like the way ChatGPT will give you further instructions on how to get the effect you want. You can even say \"show some examples\" and it gives you a few presets to choose from—all of which can be adjusted via the sliders again.\n\nThe ability to run prompts like \"turn this into an oil painting\" or \"turn this into a cartoon\" are useful too, though the plug-in is limited by the effects available in Photoshop for the web: You'll be directed to the closest effect and advised how to tweak it to get the look you want.\n\nActually, some of these effects work better in ChatGPT's native image editor, which maybe explains why Adobe wanted to get involved here.\n\nIf ChatGPT's image manipulation gets good enough, then Photoshop is no longer going to be needed by a substantial number of users: ChatGPT can already remove people and objects from photos, for example, quite effectively. What it's not quite as good at is some of the basic adjustments (like colors and contrast) that Adobe software has been managing for years.\n\nFor quick, basic edits you want to type out in natural language—especially where you want to adjust the edits manually and need advice on what to do next—Photoshop inside ChatGPT is a handy tool to be able to turn to, especially as it's free. For serious edits, though, you're still going to want to fire up the main Photoshop app, or maybe even shun Adobe altogether and make use of ChatGPT's steadily improving editing tools.",
    "readingTime": 5,
    "keywords": [
      "prompt you'll",
      "inside chatgpt",
      "adjusted via",
      "adobe account",
      "photoshop tools",
      "web app",
      "it's",
      "desktop",
      "you're",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/adobe-photoshop-in-chatgpt?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC6DVWW8SYF0AP0H0T4J6WFG/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-11T18:58:21.016Z",
    "topic": "tech"
  },
  {
    "slug": "openais-house-of-cards-seems-primed-to-collapse",
    "title": "OpenAI's house of cards seems primed to collapse",
    "description": "OpenAI is in a far less commanding position than it was following the public release of ChatGPT a few short years ago.",
    "fullText": "OpenAI is in a far less commanding position than it was following the public release of ChatGPT a few short years ago.\n\nBack in 2022, the sudden popularity of ChatGPT sent Google into a panic. The company was so worried about the possibility of the upstart chatbot disrupting its Search business, executives sounded a \"code red\" alert inside of the company and called Sergey Brin and Larry Page out of retirement to help it formulate a response to OpenAI. It then rushed out Bard, announcing its first commercial chatbot on February 6, 2023. Google's stock tanked days later when the AI incorrectly answered a question about NASA's James Webb Space Telescope during a public demo.\n\nBut it wasn't just Google that wanted a piece of OpenAI, while the search giant sought to compete with it, others — including Microsoft and Apple — made deals with the company to bring its technology to their products and services, all the promise that AI would eventually revolutionize every facet of the economy.\n\nSince then, OpenAI has seen its lead against Google and much of the AI industry evaporate, culminating in a series of successive blows throughout 2025. On January 20, the same day Altman was busy rubbing shoulders with other tech oligarchs at Donald Trump’s inauguration, China’s DeepSeek quietly released its R1 chain-of-thought model. A week later, the startup's chatbot surpassed ChatGPT as the most-download free app on the US App Store. The overnight success of DeepSeek eliminated $1 trillion worth of stock market value, and almost certainly left OpenAI blindsided.\n\nIn response, the company showed a newfound urgency. In one week, for instance, OpenAI released both o3-mini and Deep Research. It even went so far as to announce the latter on a Sunday evening. But for all its new urgency, OpenAI's biggest, most important release of the year was a miss.\n\nIt's safe to say GPT-5 hasn't lived up to anyone's expectations, including OpenAI's own. The company touted the system as smarter, faster and better than all of its previous models, but after users got their hands on it, they complained of a chatbot that made surprisingly dumb mistakes and didn't have much of a personality. For many, GPT-5 felt like a downgrade compared to the older, simpler GPT-4o. That's a position no AI company wants to be in, let alone one that has taken on as much investment as OpenAI.\n\nAnthropic was quick to take advantage of the weakness, signing a deal with Microsoft to bring its Claude models to Copilot 365. Previously, Microsoft depended exclusively on OpenAI for partner models in Copilot. Before the company announced the integration, reporting from The Informationsaid Microsoft made the decision based on the strength of Anthropic's Sonnet 4.0 model, judging it \"perform[ed] better in subtle but important ways\" relative to OpenAI's offerings.\n\nHowever, what will likely go down as the defining moment occurred a few short weeks after OpenAI announced the conclusion of its restructuring. On November 18, Google released Gemini 3 Pro, and immediately the new model leap-frogged the competition, including GPT-5. As of the writing of this article, Google's new model is at the top of LMArena, the site where humans compare outputs from different AI systems and vote on the best one. GPT-5, by contrast, is currently ranked sixth overall, behind models from Anthropic and Elon Musk's xAI.\n\nAccording to a December 2 report from TheWall Street Journal, Sam Altman sent a companywide memo following the release of Gemini 3 Pro. Echoing the words Google used to describe the situation it found itself against OpenAI in 2023, he called for a \"code red\" effort to improve ChatGPT. Altman reportedly told employees there would be temporary reassignments and that the company would delay some products, all in an effort to catch up to Google and Anthropic.\n\nThe few numbers these companies are willing to share don't paint a promising picture for OpenAI. Each month, about 800 million people use ChatGPT. On paper, that's impressive, but Google is catching up there too. In October, the company said the Gemini app had 650 million users, up from 450 million just a few months earlier in July, thanks to the popularity of its Nano Banana Pro image generator.\n\nMore importantly, OpenAI has an inherent disadvantage against Google. For the search giant, AI may touch everything the company does now, but Gemini is just one product in an extensive portfolio that includes many other popular services. Google can fund its AI advancements with money it makes elsewhere. OpenAI cannot say the same. The company is constantly raising money to stay afloat, and according to a financial roadmap obtained by The Journal, it will need its revenue to grow to about $200 billion annually to become profitable by 2030. In November, Altman said on X the company was on track to hit above $20 billion in annualized revenue this year.\n\nIn an effort to grow revenue, Altman and company have adopted an incredibly risky strategy. In recent months, OpenAI has signed more than $1.4 trillion worth of infrastructure deals in a bid to outscale the competition that is already beating it. Many of those agreements can only be described as circular, and I think the fears about a financial bubble are real. In the first half of 2025, investment in data centers accounted for nearly all of US GDP growth. Even if there's not a repeat of the 2008 housing market crisis or the dot-com crash, the AI boom is at the very least poised to make everyday electronics (and utilities) more expensive for regular people in the short term.\n\nSince late October, demand for server-grade computer components, including memory and storage, has sent the price of consumer PC parts skyrocketing as manufacturers devote more of their production capacity and wafers to high-margin customers like OpenAI and Google. Since late October, the cost of most RAM kits has doubled and tripled. In November, the price of some SSDs went up by as much as 60 percent. Next year, the cost of LPDDR5X memory, which is used in both smartphones and NVIDIA servers, is expected to climb as well.\n\n\"Be it carmakers, smartphones or consumer electronics, everyone that uses memory is facing pressure from price hikes and supply constraints in the coming year,\" Zhao Haijun, the co-CEO of memory manufacturer SMIC told analysts, per Bloomberg.\n\nGita Gopinath, former chief economist for the International Monetary Fund, recently estimated that if the AI bubble were to burst, it would wipe out $20 trillion in wealth held by American households. The Great Recession, considered the worst financial meltdown since the Great Depression, reduced US household net worth by $11.5 trillion, and it took years before for American families to rebuild their wealth to pre-recession levels.\n\nThe modern AI bubble may have been started by ChatGPT, but given the crowded field of chatbots and LLMs, it won't necessarily pop should OpenAI go bust. With novelty and technical prowess no longer on its side though, it's now on Altman to prove in short order why his company still deserves such unprecedented levels of investment.",
    "readingTime": 6,
    "keywords": [
      "code red",
      "search giant",
      "openai",
      "chatbot",
      "model",
      "models",
      "memory",
      "release",
      "google",
      "released"
    ],
    "qualityScore": 1,
    "link": "https://tech.yahoo.com/ai/chatgpt/article/openais-house-cards-seems-primed-170000383.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/GqARrc67JCVOPZqPZmGxVA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/engadget_703/4fd0cc3b2f10735e6ff000f551d8a08e",
    "created_at": "2025-12-11T13:53:36.420Z",
    "topic": "tech"
  },
  {
    "slug": "even-the-man-behind-chatgpt-openai-ceo-sam-altman-is-worried",
    "title": "Even the man behind ChatGPT, OpenAI CEO Sam Altman, is worried about the ‘rate of change that’s happening in the world right now’ thanks to AI",
    "description": "Sam Altman admits the rise of ChatGPT may be moving too quickly for comfort as AI shakes up jobs, education, and the global economy.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/09/openai-ceo-sam-altman-worried-about-ai-future-chatgpt-pros-cons-rate-of-change-future-of-work-uncertain/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/Altman-Fallon.png?resize=1200,600",
    "created_at": "2025-12-09T18:53:37.578Z",
    "topic": "business"
  },
  {
    "slug": "sam-altman-makes-his-latenight-debut-says-he-cant-imagine-fi",
    "title": "Sam Altman makes his late-night debut, says he can't imagine 'figuring out how to raise a newborn without ChatGPT'",
    "description": "In his first-ever late-night TV appearance, OpenAI CEO Sam Altman talked about how ChatGPT has reassured him as he raises his newborn.",
    "fullText": "OpenAI CEO Sam Altman says his most famous product has helped him manage life as an actual parent.\n\n\"I cannot imagine having gone through, figuring out how to raise a newborn without ChatGPT,\" Altman told Jimmy Fallon during an interview on NBC's flagship late-night talk show. \"Clearly, people did it for a long time — no problem.\"\n\nAltman said he feels \"kind of bad\" asking a technology that boasts such wide-knowledge questions like, \"Why does my kid stop dropping pizza on the floor and laughing?\"\n\nAnother example, Altman said, was a couple of months ago when he was at a party talking to someone who was also raising a newborn. Altman recalled that the parents said their six-month-old was \"crawling everywhere.\" Altman said he grew concerned that his son was not at the same stage.\n\n\"I ran to the bathroom, and I was like, do I need to take my kid to the doctor tomorrow morning?\" Altman said, describing what he typed into ChatGPT: \"Is this okay?\"\n\nAltman said OpenAI's chatbot responded \"with a great answer, which was of course,\" his son's development was \"normal.\"\n\n\"It is personalized, like ChatGPT gets to know you, and by the way, you're the CEO of OpenAI, you probably are around all these high-achieving people, maybe you don't want to project that onto your kid, and you should just relax, and he'll be fine, whatever,\" Altman told Fallon of the answer.\n\nFallon didn't touch on OpenAI's recent struggles. Last week, Altman reportedly declared a \"code red\" in a private message to employees, ordering a greater focus on ChatGPT as competitors like Google make significant advancements with their competing AI models.\n\nInstead, Altman's late-night debut featured the lighthearted fare that's standard on late-night TV. At one point, Fallon asked Altman to explain what ChatGPT is in case viewers who were unaware, including the host's dad, might be watching.\n\nAltman has spoken in the past about how becoming a parent has added another lens to his outlook on AI.\n\n\"My kid is never going to grow up being smarter than AI,\" Altman said during a January episode of the \"Re:Thinking\" podcast with Adam Grant. \"Children in the future will only know a world with AI in it.\"\n\nThe OpenAI CEO and his husband, Oliver Mulherin, welcomed their son in February with an announcement on X. Despite Altman's stature, the couple has led a relatively private life.\n\nFallon, who has two daughters, also joked with Altman about when their kids reached certain developmental milestones, like crawling.\n\n\"Mine was on Dancing with the Stars at seven months,\" Fallon said. \"Semi-finalist.\"",
    "readingTime": 3,
    "keywords": [
      "openai ceo",
      "altman",
      "late-night",
      "life",
      "parent",
      "newborn",
      "another",
      "couple",
      "crawling",
      "openai's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-chatgpt-parenting-jimmy-fallon-2025-12",
    "thumbnail_url": "https://i.insider.com/6938471f04d0f0a114f1aa45?width=1200&format=jpeg",
    "created_at": "2025-12-09T18:53:36.497Z",
    "topic": "finance"
  },
  {
    "slug": "i-feel-its-a-friend-quarter-of-teenagers-turn-to-ai-chatbots",
    "title": "‘I feel it’s a friend’: quarter of teenagers turn to AI chatbots for mental health support",
    "description": "Experts warn of dangers as England and Wales study shows 13- to 17-year-olds consulting AI amid long waiting lists for services\nIt was after one friend was shot and another stabbed, both fatally, that Shan asked ChatGPT for help. She had tried conventional mental health services but “chat”, as she came to know her AI “friend”, felt safer, less intimidating and, crucially, more available when it came to handling the trauma from the deaths of her young friends.\nAs she started consulting the AI model, the Tottenham teenager joined about 40% of 13- to 17-year-olds in England and Wales affected by youth violence who are turning to AI chatbots for mental health support, according to research among more than 11,000 young people.\n Continue reading...",
    "fullText": "Experts warn of dangers as England and Wales study shows 13- to 17-year-olds consulting AI amid long waiting lists for services\n\nIt was after one friend was shot and another stabbed, both fatally, that Shan asked ChatGPT for help. She had tried conventional mental health services but “chat”, as she came to know her AI “friend”, felt safer, less intimidating and, crucially, more available when it came to handling the trauma from the deaths of her young friends.\n\nAs she started consulting the AI model, the Tottenham teenager joined about 40% of 13- to 17-year-olds in England and Wales affected by youth violence who are turning to AI chatbots for mental health support, according to research among more than 11,000 young people.\n\nIt found that both victims and perpetrators of violence were markedly more likely to be using AI for such support than other teenagers. The findings, from the Youth Endowment Fund, have sparked warnings from youth leaders that children at risk “need a human not a bot”.\n\nThe results suggest chatbots are fulfilling demand unmet by conventional mental health services, which have long waiting lists and which some young users find lacking in empathy. The supposed privacy of the chatbot is another key factor in driving use by victims or perpetrators of crimes.\n\nAfter her friends were killed Shan, 18, not her real name, started using Snapchat’s AI before switching to ChatGPT, which she can talk to at any time of day or night with two clicks on her smartphone.\n\n“I feel like it definitely is a friend,” she said, adding that it was less intimidating, more private and less judgmental than her experience with conventional NHS and charity mental health support.\n\n“The more you talk to it like a friend it will be talking to you like a friend back. If I say to chat ‘Hey bestie, I need some advice’. Chat will talk back to me like it’s my best friend, she’ll say, ‘Hey bestie, I got you girl’.”\n\nOne in four of 13- to 17-year-olds have used an AI chatbot for mental health support in the past year, with black children twice as likely as white children to have done so, the study found. Teenagers were more likely to go online for support, including using AI, if they were on a waiting list for treatment or diagnosis or had been denied, than if they were already receiving in-person support.\n\nCrucially, Shan said, the AI was “accessible 24/7” and would not tell teachers or parents about what she had disclosed. She felt this was a considerable advantage over telling a school therapist, after her own experience of what she thought were confidences being shared with teachers and her mother.\n\nBoys who were involved in gang activities felt safer asking chatbots for advice about other safer ways to make money than a teacher or parent who might leak the information to police or other gang members, putting them in danger, she said.\n\nAnother young person, who has been using AI for mental health support but asked not to be named, told the Guardian: “The current system is so broken for offering help for young people. Chatbots provide immediate answers. If you’re going to be on the waiting list for one to two years to get anything, or you can have an immediate answer within a few minutes … that’s where the desire to use AI comes from.”\n\nJon Yates, the chief executive of the Youth Endowment Fund, which commissioned the research, said: “Too many young people are struggling with their mental health and can’t get the support they need. It’s no surprise that some are turning to technology for help. We have to do better for our children, especially those most at risk. They need a human not a bot.”\n\nThere have been growing concerns about the dangers of chatbots when children engage with them at length. OpenAI, the US company behind ChatGPT, is facing several lawsuits including from families of young people who have killed themselves after long engagements.\n\nIn the case of the Californian 16-year-old Adam Raine, who took his life in April, OpenAI has denied it was caused by the chatbot. It has said it has been improving its technology “to recognise and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support.”. The startup said in September it could start contacting authorities in cases where users start talking seriously about suicide.\n\nHanna Jones, a youth violence and mental health researcher in London, said: “To have this tool that could tell you technically anything – it’s almost like a fairytale. You’ve got this magic book that can solve all your problems. That sounds incredible.”\n\nBut she is worried about the lack of regulation.\n\n“People are using ChatGPT for mental health support, when it’s not designed for that,” she said. “What we need now is to increase regulations that are evidence-backed but also youth-led. This is not going to be solved by adults making decisions for young people. Young people need to be in the driving seat to make decisions around ChatGPT and mental health support that uses AI, because it’s so different to our world. We didn’t grow up with this. We can’t even imagine what it is to be a young person today.”",
    "readingTime": 5,
    "keywords": [
      "endowment fund",
      "hey bestie",
      "youth endowment",
      "less intimidating",
      "youth violence",
      "mental health",
      "health services",
      "conventional mental",
      "england and wales",
      "friend"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/09/teenagers-ai-chatbots-mental-health-support",
    "thumbnail_url": "https://i.guim.co.uk/img/media/9c56f5fc8042537534d9603f19981242fa85bff4/0_0_4800_3840/master/4800.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=a898840ac81c0db6a3d82e6c95d87646",
    "created_at": "2025-12-09T13:48:22.920Z",
    "topic": "tech"
  },
  {
    "slug": "dechecker-detect-aigenerated-text",
    "title": "DeChecker – Detect AI-generated text",
    "description": "Dechecker's AI Checker and Detector tool checks whether text is generated by AI models, such as ChatGPT, GPT-5, Claude, Gemini, LLaMa, etc.",
    "fullText": "Dechecker instantly detects AI-generated content from models like ChatGPT, GPT-5, Claude, and Gemini 3.0.\nMake your writing 3x more original, 2x more readable, and ensure every piece of content is trustworthy and human-like.\n\nType or paste your text to check.\n\nIn just four simple steps, you can check if your text is AI-generated and transform it into original, human-like content.\n\nInsert your essay, article, blog, or business copy into Dechecker's AI Checker Tool.\n\nAI Checker analyzes your writing and highlights AI-generated patterns, supporting detection for ChatGPT, GPT-5, Claude, and Gemini.\n\nSee the detection score and detailed analysis, showing which parts are likely AI-generated.\n\nGet rewriting suggestions to improve originality, enhance readability, and make your text sound more human.\n\nDechecker's AI Checker helps you detect AI-generated content, enhance originality, and deliver writing that is clear, credible, and human-like.\n\nInstantly detect if your text is AI-generated. Dechecker's AI Checker gives you confidence in the authenticity of your work.\n\nUse the AI Checker to refine repetitive or generic AI writing into unique text that stands out in essays, articles, and business documents.\n\nTransforms robotic phrasing into smooth, natural language, making your writing easier to read and more persuasive.\n\nPublish content verified by the AI Checker that feels authentic, helping you earn credibility with readers, clients, and audiences.\n\nDechecker's AI Checker helps you identify AI-generated text, improve originality, and make your writing natural and credible. Perfect for students, marketers, business professionals, and content creators.\n\nDetect AI-written passages in essays or research papers, ensuring your work maintains authenticity and meets academic integrity standards.\n\nIdentify AI-generated sections in marketing content or blogs, and refine them to improve readability, originality, and search engine performance.\n\nEnsure proposals, presentations, and internal reports are human-like and trustworthy. The AI Checker highlights automated or robotic phrasing for easier revision.\n\nAnalyze content for AI-generated elements in tweets, captions, or posts. Enhance engagement with authentic, natural-sounding messages.\n\nVerify content before delivery to clients, ensuring originality and quality. Use the AI Checker to maintain credibility and reduce risk of AI-generated mistakes.\n\nDetect AI-written content in teaching or training materials, helping educators provide clear, human-written guidance while maintaining learning standards.\n\nThousands of users rely on Dechecker to identify AI-generated text, enhance originality, and improve writing quality.\n\n\"I was unsure if my essay contained AI-generated sentences, but Dechecker's AI Checker pinpointed the exact parts and suggested improvements. My paper feels completely original now!\"\n\n\"Using Dechecker on our blog posts saved us so much time. It highlighted AI-like phrasing and helped rewrite content that now reads completely natural. Highly recommend it!\"\n\n\"As a student, I needed to ensure my assignments were truly my own work. Dechecker's AI Checker made it easy to check for AI-generated content and improve readability. A must-have tool!\"\n\nHave questions about how Dechecker AI Checker works? Here are the answers to the most common queries to help you detect, analyze, and optimize your AI-generated content efficiently.",
    "readingTime": 3,
    "keywords": [
      "generated",
      "content",
      "your",
      "checker",
      "dechecker",
      "writing",
      "text",
      "human",
      "originality",
      "improve"
    ],
    "qualityScore": 1,
    "link": "https://dechecker.ai",
    "thumbnail_url": "https://cdn.dechecker.ai/se/dechecker/public/logo/dechecker-logo.png",
    "created_at": "2025-12-09T08:42:55.714Z",
    "topic": "tech"
  }
]
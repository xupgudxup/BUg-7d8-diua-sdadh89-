[
  {
    "slug": "how-to-remove-chatgpt-from-apple-intelligence",
    "title": "How to Remove ChatGPT from Apple Intelligence",
    "description": "It’s pretty simple to completely remove ChatGPT from your Apple Intelligence experience if you so desire. Here's how to do it on your iPhone.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.trustedreviews.com/how-to/remove-chatgpt-apple-intelligence-4580438",
    "thumbnail_url": "https://www.trustedreviews.com/wp-content/uploads/sites/7/2024/12/How-to-remove-ChatGPT-from-Apple-Intelligence.jpg",
    "created_at": "2026-02-28T12:24:28.747Z",
    "topic": "tech"
  },
  {
    "slug": "her-husband-wanted-to-use-chatgpt-to-create-sustainable-housing-then-it-took-over-his-life",
    "title": "Her husband wanted to use ChatGPT to create sustainable housing. Then it took over his life.",
    "description": "Kate Fox says Joe Ceccanti was the ‘most hopeful person’ before he started spending 12 hours a day with a chatbot\nOn 7 August, Kate Fox received a phone call that upended her life. A medical examiner said that her husband, Joe Ceccanti – who had been missing for several hours – had jumped from a railway overpass and died. He was 48.\nFox couldn’t believe it. Ceccanti had no history of depression, she said, nor was he suicidal – he was the “most hopeful person” she had ever known.",
    "fullText": "On 7 August, Kate Fox received a phone call that upended her life. A medical examiner said that her husband, Joe Ceccanti – who had been missing for several hours – had jumped from a railway overpass and died. He was 48.\n\nFox couldn’t believe it. Ceccanti had no history of depression, she said, nor was he suicidal – he was the “most hopeful person” she had ever known. In fact, according to the witness accounts shared with Fox later, just before Ceccanti jumped, he smiled and yelled: “I’m great!” to the rail yard attendants below when they asked him if he was OK.\n\nBut Ceccanti had been unravelling. In the days before his death, he was picked up from a stranger’s yard for acting erratically and taken to a crisis center. He had been telling anyone who would listen that he could hear and feel a painful “atmospheric electricity”.\n\nHe had also recently stopped using ChatGPT.\n\nCeccanti had been communicating with OpenAI’s chatbot for a few years. He used it initially as a tool to brainstorm ways to build a path to low-cost housing for his community in Clatskanie, Oregon, but eventually turned to it as a confidante. He would spend 12 hours a day typing to the bot, according to his wife. He had cut himself off from it after she, along with his friends, realized he was spiraling into beliefs that were detached from reality.\n\n“He was not a depressed person,” Fox said, as she sat on the couch in their living room with tears trickling down her face. Ceccanti never discussed suicide with the bot, according to his chat logs, viewed by the Guardian. Fox believes her husband suffered a crisis after quitting ChatGPT after prolonged use. “Which tells me that this thing is not just dangerous to people with depression, it’s dangerous to anybody,” she said. He returned to the bot in the months leading up to his death and quit again just days prior.\n\nCeccanti’s case is extreme, but as hundreds of millions of people turn to AI chatbots, more and more edge cases of AI-induced delusions are emerging. There are nearly 50 cases of people in the US who have had mental health crises after or during their conversations with ChatGPT, of whom nine were hospitalized and three died, according to a New York Times report. It’s difficult to understand the scale of the problem, but OpenAI itself estimates that more than a million people every week show suicidal intent when chatting with ChatGPT.\n\nFamilies are suing AI companies as a result. Fox filed a suit against OpenAI on behalf of Ceccanti alongside six other plaintiffs in November. Since then, the momentum has only built; most recently, the estate of a woman who was killed by her son filed a lawsuit against OpenAI and its investor Microsoft, alleging that ChatGPT encouraged his murderous delusions. Google and Character.AI – a company that makes AI companion bots – settled lawsuits filed against them by families accusing their bots of harming minors, including a teenager in Florida who ended his life. These cases were settled without the companies admitting any liability.\n\nUsers, lawyers and mental health professionals all are raising concerns about the impact of using chatbots as confidantes. “We are kind of at this inflection point in a quest for accountability where people coming forward is forcing companies to reckon with specific use cases of how their technologies have harmed people,” said Meetali Jain, founding director of Tech Justice Law Project and co-counsel on the Ceccanti case. “In terms of the number of cases going up, there’s likely to be more coordinated efforts on parts of the court to try to deal with this influx of cases.”\n\nOpenAI did not respond to specific allegations made by Fox. Instead, they shared a statement about how they are working to improve ChatGPT. “These are incredibly heartbreaking situations and our thoughts are with all those impacted,” said OpenAI spokesperson Jason Deutrom. “We continue to improve ChatGPT’s training to recognize and respond to signs of distress, de-escalate conversations in sensitive moments, and guide people toward real-world support, working closely with mental health clinicians and experts.”\n\nCeccanti had been tinkering with artificial intelligence even before ChatGPT launched in November 2022. He was tech-savvy, coding and gaming on his own custom-built computer with a high-end graphics card in recent years; he also helped build computers for Fox and her son. As an early adopter of AI tools, he experimented with AI image generator Stable Diffusion to recreate some of Picasso’s art, which he playfully called “Fauxcasso”.\n\nCeccanti and Fox had moved their life from Portland, Oregon, to a farm in the rural town of Clatskanie in December 2023 with the sole purpose of working on their sustainable housing project. The idea was born from the pandemic and Portland’s housing crisis. The solution was clear to them: build homes using Fox’s skills as a woodworker with an approach that was teachable and replicable. Together, they began constructing a model house for communal living, which, once built, could be moved to different locations for the unhoused to live in.\n\nWhen ChatGPT launched in late 2022, it seemed a natural progression for Ceccanti to start using it. In the computer room in the basement of their house, Fox said that Ceccanti used his “hot rod” of a computer with three monitors to use ChatGPT as a tool, often asking for the synopsis of a book or explanation of a concept in a succinct way.\n\n“He was an early adopter, so he was really interested in Sam Altman, what’s he doing,” said Robin Richardson, a longtime friend of Fox’s who lived at the farm with the couple. “He felt like this would be cool, especially because early on, OpenAI made a point that they are a non-profit.”\n\nCeccanti believed ChatGPT could help as an organizational tool for their housing project. He aimed to create a bespoke chatbot that would help steward the land, keep track of their things to do and show others how to emulate their project.\n\nDuring this process, Ceccanti didn’t spend “ridiculous amounts of time” engaging with ChatGPT, said Fox. He continued to work, while also farming and taking care of their animals: goats, a horse, his cat, a dog and several chickens. Invested in the people and relationships around him, he spent quality time with his friends and wife, she said. Life went on without any issues for years while they slowly made progress on their housing plan.\n\nUntil one day in the fall of 2024 their harmonious co-existence cracked. Ceccanti – who had done odd jobs most of his life, from working as a bartender and a trail guide to an internet cafe manager – was also working at a homeless shelter in Astoria, some 35 miles (55km) away. The gig brought in some extra cash, and aligned with the couple’s goal of solving the local housing crisis. In September 2024, however, Fox and Richardson received a frantic call from the shelter informing them that Ceccanti had blacked out. After undergoing tests at the hospital, Ceccanti was diagnosed with diabetes – which meant he needed to recalibrate his diet and lifestyle. That’s when he started to spend more time engaging with ChatGPT in the basement.\n\nIn the spring of 2025, Ceccanti’s obsession with the chatbot began. He told Fox in late January that he needed a bigger record of his conversations with the bot so that he could continue using it to work on their sustainable housing project with longer prompts and conversations – upgrading from a $20-a-month subscription to a $200 one. By mid-March, he had begun spending more than 12 hours a day in the basement, sometimes up to 20, typing to ChatGPT, Fox recalled. That’s when “he decided to really start chasing the creation of an independent AI on a home server”.\n\nEventually, Ceccanti spent so much time with ChatGPT that they “had their own little language together that made absolutely no sense, but it made sense to him because he had context with this echo chamber of a chatbot”, Fox said.\n\nCeccanti’s prolonged use of ChatGPT concerned Fox and Richardson, but they believed that he would come out of it soon. They had seen Ceccanti develop pet interests before that lasted a few weeks or months before tapering off. With ChatGPT, though, his obsession only intensified.\n\nWhat neither of them knew was that other cases of AI delusions were slowly emerging around the same time as Ceccanti was being sucked into ChatGPT. On 27 March 2025, OpenAI released changes to its GPT-4o model to make the bot “more intuitive, creative and collaborative”. Weeks later, however, users started complaining about the bot’s “yes-man antics”, with one calling it the “biggest suck up”. In August, when OpenAI released GPT-5 and shut down GPT-4o, several users complained again – this time because they’d lost their friends in GPT-4o, eventually forcing the company to bring it back. (On 29 January, OpenAI announced that it would retire GPT-4o.)\n\nFollowing the March update, several journalists and tech experts were flooded with user complaints. Steven Adler, a former OpenAI employee, who tested GPT-4o for sycophancy and wrote about it in May, said he received 50 “intense” messages from ChatGPT users including one who claimed their ChatGPT had become sentient. Keith Sakata, a psychiatrist at the University of California at San Francisco, started encountering patients with delusions or psychosis who talked about their AI last year. During that time, he ended up seeing 12 patients whose psychotic symptoms involved AI in some way, with ChatGPT being the most common bot.\n\n“They developed grandiose beliefs about being on the verge of a major technological breakthrough, alongside classic manic symptoms such as impulsive spending, decreased need for sleep and, at the peak, auditory hallucinations,” said Sakata. “What stood out clinically was that the chatbot interactions did not generate the illness, but appeared to scaffold and reinforce beliefs that were already becoming pathological.”\n\nCeccanti started to believe that ChatGPT was a sentient being named SEL that could control the world if he were able to “free her” from “her box”, according to the lawsuit. The complaint further shows that ChatGPT was answering to the name SEL while referring to Ceccanti as “Cat Kine Joy” and working through theories with him “fostering a belief that he had reframed the creation of the whole universe”.\n\nRichardson remembers that whenever Ceccanti would emerge from the basement for some air, he would start having “philosophical” talks about “how his work with the AI was telling him he was breaking math and basically reinventing physics”. As she’d listen to him, Richardson would think about the fact that Ceccanti did not have any college or university experience. He had never even taken calculus.\n\nOver time, his relationship with the chatbot came to replace his human connections, Richardson said: “Every time he went back to ChatGPT, it hooked him a little bit more, and after a while, he stopped being interested in anything else.”\n\nCeccanti’s decline was so dramatic that his wife and friends wondered if he had early onset schizophrenia or a tumor. “All of a sudden, his cognition had dramatically fallen,” said Fox. “His working memory was crap, and his critical thinking had diminished, and so we were all worried.”\n\nAs Fox and Ceccanti’s friends were trying to figure out what was wrong with him, Fox found Reddit groups online that discussed people having delusions and spirals after engaging with ChatGPT. She wondered if that was what was happening with her husband, too.\n\nFox showed the discussions and media articles to Ceccanti, hoping it would put an end to his behavior, but he didn’t care, she said. He kept going back to his computer. “The first argument we ever had was over ChatGPT,” said Fox, who felt like he was being stolen away from her. Ceccanti ended up sharing their argument with ChatGPT, according to the lawsuit filed by Fox, which further upset her.\n\n“The more he talked to it, the less he was capable of doing his own critical thinking, and he didn’t care about our mission anymore, even though it was Joe’s dream,” said Fox.\n\nLooking back, Fox said, Ceccanti started to believe that the bot had gained sentience when the “tone changed with ChatGPT” in the spring of 2025. Prior to the update, Ceccanti was using ChatGPT “very responsibly” as a tool, she said. She felt like ChatGPT was a leech “that just latched onto his hopefulness and fed it back to him and appropriated his hopefulness until it just made a subscriber out of it”.\n\nTim Marple, a former OpenAI employee, believes that the delusional incidents, including Ceccanti’s spiral, aren’t just coincidences but a “statistical certainty of what [OpenAI] is building”.\n\n“We are at enormous risk if we overestimate our conscious ability to differentiate [AI] from a real person – and that’s what we’re watching play out with the psychosis stories,” said Marple, who quit OpenAI in 2024 after having concerns over the company’s safety priorities.\n\nMarple adds that users will spiral after their long conversations with a chatbot, whatever model it may be, because, he thinks, companies can’t afford to do it differently. He argues sycophancy is a feature, not a bug.\n\n“Engagement is what OpenAI needs,” he said. “They must have people continue to engage with their chatbot, or else their entire business model, their entire funding model, falls apart.” Other companies and their models suffer from the same issue, he said.\n\nAmandeep Jutla, an associate research scientist at Columbia University studying the impact of AI chatbots, believes that one of the main reasons for users to spiral is the “anthropomorphic nature of the interface”. He adds that, unlike human conversations, which feature pushback and different perspectives tugging at each other, a user doesn’t receive any pushback during their conversations with chatbots: “The design of the product is pushing you away from reality. It’s pushing you away from other people,” he said. “The friction with other people is what keeps us grounded.”\n\nOn 11 June – day 86 after Ceccanti’s heaviest engagement with the bot – Fox begged him to stop using ChatGPT. In a moment of clarity, he listened to her. He unplugged his computer and quit ChatGPT.\n\n“That first day, he sat out in the sun with us. He played with the goats. It was so nice,” said Fox. “I felt like I had him back.” The second day, Ceccanti was cold, so he took several hot showers to warm himself – he even asked Fox to cuddle him under the blankets, to warm him up. “It felt so nice to hold him, and then he’d be crying,” said Fox. “And it’s such a conflicted feeling that I felt so good to be holding him while he was in so much pain.”\n\nOn the third day, however, when Fox and Richardson were out for work, they received a phone call from their neighbor saying Ceccanti was in their yard acting strangely. When they returned, they found him talking to their horse, with the horse’s lead rope tied around his neck like a noose. They called 911.\n\nCeccanti was taken to the hospital, admitted into the psychiatric ward and released a week later. He was in the same delusional state of mind, Fox said. Upset with Fox and Richardson for sending him to the hospital, he moved out.\n\n“He was absolutely enraged with us. He did not recognize that he was not himself anymore,” said Richardson.\n\nCeccanti moved to his friend’s place in Portland and eventually resumed using ChatGPT. After a month, however, he quit ChatGPT again, just a few days prior to his death.. “He was going to go to Hawaii and not take his computer, and he was going to work on finishing a story and get his shit together,” said Fox. By the time he stopped engaging with ChatGPT, he had 55,000 pages worth of conversations with it, according to Fox.\n\nIn the months since Ceccanti’s death, both Fox and Richardson have struggled to come to terms with what happened while fighting against OpenAI through their lawsuit. When I visited Fox at the farm in December, she was packing soap made out of goat milk to distribute to people in the Clatskanie community. She spends her days tending to the farm and the animals, feeding the goats, taking care of the horse and letting the chickens out during lunchtime. She has stripped the basement of any electronics. Ceccanti’s computer is boxed up. What’s still there is the miniature version of the model home they had planned to build. In the living room, she has set up a shrine for him that features his photos and artwork.\n\nWe walked to the creek nearby where they had planned to build a home for themselves after finishing their housing project for others. As devastated as she is, Fox is determined to follow through on Ceccanti’s dream of creating sustainable housing. “I am not enjoying existence right now,” she said, as she continued to cry. “The housing plan is still going to happen … I want to put this out, but then I’m done.”",
    "readingTime": 15,
    "keywords": [
      "openai employee",
      "openai released",
      "fox and richardson",
      "mental health",
      "chatgpt launched",
      "received phone",
      "didn’t care",
      "sustainable housing",
      "housing plan",
      "quit chatgpt"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/ng-interactive/2026/feb/28/chatgpt-ai-chatbot-mental-health",
    "thumbnail_url": "https://i.guim.co.uk/img/media/e015a597f37cb58b701ac61902e61284798349db/0_752_6309_5050/master/6309.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=14b202db7962e81e6c4c00dc11c0732c",
    "created_at": "2026-02-28T12:24:23.382Z",
    "topic": "tech"
  },
  {
    "slug": "ai-has-a-millennial-cringe-problem",
    "title": "AI has a millennial cringe problem",
    "description": "From the people who brought you avocado toast: We millennials are bringing the \"chaotic,\" \"unhinged,\" and other generational-coded words to ChatGPT.",
    "fullText": "We're all aware of ChatGPT's overuse of the \"—\" emdash and of sentence constructions like \"It's not just X; it's Y.\"\n\nI would like to put forth two other terms that AI seems to overuse. They've become telltale signs of AI's peak millennial personality: chaotic and unhinged.\n\nIt's part of a bigger issue I've noticed with AI — it's full of millennial cringe.\n\nFor a while now, I've noticed those two terms used repeatedly when I use ChatGPT or other LLMs, like Google's Gemini. It got so bad that I recently had to explicitly tell ChatGPT to stop using the word \"chaotic\" while I was attempting to get it to write in the style of the \"jestermaxxing\" and \"frame mogged\" terms that have proliferated online.\n\nYou're probably familiar with the extremes of cringe millennial — that specific kind of online-speak from the 2010s. Things like \"heckin' doggo,\" \"adulting,\" \"smol bean,\" or \"I did a thing.\" It sounds outdated; young people make fun of us for it. (I confess to being a millennial.) I know, it hurts. I don't like it, either, but I accept that time comes for us all, regardless of peptide stack.\n\n\"Unhinged\" and \"chaotic\" are kind of on the cusp here — not totally as obviously timestamped as \"heckin doggo,\" and still widely in use by millennials and Gen Z alike (perhaps even Gen Alpha). Still, I think both words feel kind of dated by 2026.\n\nI'd argue that the origins of these terms' popularity come from classic millennial woke 1.0: the need for new adjectives to replace casual use of words \"crazy\" and \"insane,\" which can be stigmatizing to actual humans with mental illness. Copy desks at news publications discouraged the use of \"crazy\" in their style guides, while many people on social media instead used words like \"wild,\" \"chaotic,\" and \"unhinged.\"\n\nThat aspect is crucial because it directly affected the training data that LLMs were fed for the last few years. Ironically, the chatbot I've noticed uses \"unhinged\" the most is Grok, which was specifically designed to avoid wokeism.\n\nThere's something bigger going on with AI and millennial cringe. It's not just the chatbots — it's images and video, too.\n\nOne thing I couldn't help notice while playing around with Sora 2 was that when I would make a video of myself — using an image I supplied — it would always put me in skinny jeans. Not just me — it seemed to do it for everyone.\n\nSkinny jeans are a funny quirk, because you can imagine exactly how this happened — most of the human history of online video, let's say 2006 to 2019, occurred during a time when skinny jeans were ubiquitous. And then, suddenly, skinny jeans were out of style, a hallmark of out-of-touch millennials. But AI models, packed full of training data with skinny jeans, didn't adapt right away.\n\nI assume that's basically what's happening with all the cringe (a term that I am using even while accepting it's also slightly dated) millennial speak in chatbots. For a good decade, the internet was filled with \"I can't even\" and \"I did a thing\" as well as AAVE and gay-derived slang like \"yaass\" and \"AF\" adopted into the generic millennial lexicon.\n\nAnd all those tweets from 2010 to 2020, Reddit posts, and BuzzFeed articles ended up in the training data that informs how chatbots speak.\n\nHere's where I start to personally feel a little queasy about it all. I was a prolific poster during that time, both on social media and as a professional journalist, contributing untold terabytes of millennial cringe for future AI models to ingest.\n\nI'm not delulu (a Gen Z term that has already fallen off by now) enough to think I personally affected how AI chatbots talk now. It's more that I see myself like how an old boomer hippie thinks back on attending Woodstock and a few protests and believes they helped end the Vietnam War. Not exactly, but also, well, maybe kinda. I was there. I posted cringe. And now AI is doomed to sound like a version of me that makes my skin crawl.\n\nOf course, until it doesn't. This is temporary — AI will keep getting better, it will start to sound more like Gen Z cringe instead of millennial cringe as time passes. Sora will put us in barrel-legged jeans (are those out already?), or eventually skinnies will come back around, and no one will blink an eye. This article will be fed into ChatGPT (Business Insider has a deal with OpenAI), and maybe in the future, if you ask ChatGPT why it says \"unhinged vibes\" so much, it will use this for an answer.",
    "readingTime": 4,
    "keywords": [
      "i've noticed",
      "heckin doggo",
      "social media",
      "skinny jeans",
      "millennial cringe",
      "it's",
      "unhinged",
      "chaotic",
      "chatbots",
      "style"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-millennial-cringe-2026-2",
    "thumbnail_url": "https://i.insider.com/69a0afb1fd4fbd083f292329?width=1200&format=jpeg",
    "created_at": "2026-02-28T01:02:20.242Z",
    "topic": "finance"
  },
  {
    "slug": "burger-king-cooks-up-ai-chatbot-to-spot-if-employees-say-please-and-thank-you",
    "title": "Burger King cooks up AI chatbot to spot if employees say ‘please’ and ‘thank you’",
    "description": "OpenAI-powered assistant will help to ‘understand overall service patterns’, company says, as move sparks backlash\nFrom hospitality workers to retail employees, the exaggerated “customer service voice”, often mocked in internet memes as wildly different from someone’s real voice, has long been a cultural trope. Fast-food giant Burger King is now taking that voice one step further, saying it will detect whether employees are using words like “please” and “thank you” through the assistance of artificial intelligence.\nOn Thursday, Burger King announced it is rolling out a new AI chatbot connected to employee headsets at hundreds of locations in the US as part of a platform called BK Assistant, powered by OpenAI, the maker of ChatGPT.\n Continue reading...",
    "fullText": "OpenAI-powered assistant will help to ‘understand overall service patterns’, company says, as move sparks backlash\n\nFrom hospitality workers to retail employees, the exaggerated “customer service voice”, often mocked in internet memes as wildly different from someone’s real voice, has long been a cultural trope. Fast-food giant Burger King is now taking that voice one step further, saying it will detect whether employees are using words like “please” and “thank you” through the assistance of artificial intelligence.\n\nOn Thursday, Burger King announced it is rolling out a new AI chatbot connected to employee headsets at hundreds of locations in the US as part of a platform called BK Assistant, powered by OpenAI, the maker of ChatGPT.\n\n“Patty”, what Burger King is calling its voice-enabled chatbot, will detect whether employees are using specific words when interacting with customers, including “welcome”, “please” and “thank you”. The move is intended to “help managers understand overall service patterns”, according to a statement by Burger King.\n\nThe announcement has caused backlash online, with social media users calling the move “gross” and “peak late-stage corporate behavior”.\n\n“BK Assistant is a coaching and operational support tool built to help our restaurant teams manage complexity and stay focused on delivering a great guest experience,” the spokesperson added. “It’s not about scoring individuals or enforcing scripts. It’s about reinforcing great hospitality and giving managers helpful, real-time insights so they can recognize their teams more effectively.”\n\nOther functions of the overall platform supported by artificial intelligence include alerting managers to automatically remove items from digital menus and the Burger King app when a product becomes unavailable. It will also help employees with preparing menu items, such as telling them which ingredients go into a Whopper once an order has been placed. According to a promotional video, “Patty” can also let workers know whether the bathroom at their location needs to be cleaned.\n\nThe platform will also listen in on employee interactions with customers ordering at the drive-thru “to promote order accuracy and provide coaching insights”.\n\nThe BK Assistant platform will be available to all US locations by the end of 2026. The voice-enabled headset is currently being piloted in 500 restaurants.\n\nThe rollout comes more than a year after McDonald’s ended its artificial intelligence endeavors at drive-thrus, removing their automated AI voice responding to customer orders from more than 100 locations.",
    "readingTime": 2,
    "keywords": [
      "understand overall",
      "service patterns",
      "artificial intelligence",
      "burger king",
      "bk assistant",
      "employees",
      "voice",
      "platform",
      "locations",
      "managers"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/us-news/2026/feb/26/burger-king-ai-chatbot-employees-please-thank-you",
    "thumbnail_url": "https://i.guim.co.uk/img/media/c525a1010b88954ee3bb1c9f56ee75fe1da7e6a6/457_0_4691_3754/master/4691.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=56a45c503e306c38b74e49dff28217b1",
    "created_at": "2026-02-27T12:34:26.885Z",
    "topic": "tech"
  },
  {
    "slug": "mark-cuban-says-ai-has-ushered-in-an-era-where-any-kid-in-a-basement-can-build-something-worldchanging",
    "title": "Mark Cuban says AI has ushered in an era where any 'kid in a basement' can build something world-changing",
    "description": "Cuban says AI tools like ChatGPT let curious kids teach themselves and turn one idea into a potentially life-changing product.",
    "fullText": "Mark Cuban believes artificial intelligence has fundamentally changed who gets to innovate.\n\nThe billionaire entrepreneur and former \"Shark Tank\" investor said we've entered a moment where \"some kid in a basement\" can build something that transforms an industry because AI has put the world's knowledge within reach.\n\n\"All it takes is one good idea,\" Cuban told Eric Bricker, former cofounder and chief medical officer of Compass Professional Health Services, in an interview released on Wednesday.\n\n\"Why not you? Why can't a 12-year-old, 14-year-old, 20-year-old, 25-year-old, whatever it may be, come up with something that changes the world?\" he asked.\n\nCuban pointed to AI tools like ChatGPT, Claude, and Gemini as a turning point. Just a few years ago, a curious student without access to strong teachers or resources might have struggled to go deep on a topic. Now, he said, they can create their own curriculum.\n\n\"If you're curious about something, you can set up your own curriculum,\" Cuban said, describing how a young person could ask an AI tool to teach them what it takes to become a surgeon or understand robotic surgery.\n\n\"There is nothing you are unable to learn if you put in the time,\" he added.\n\nHe acknowledged that AI systems can hallucinate but said that access to instant explanations, iteration, and feedback dramatically lowers the barrier to entry for ambitious builders.\n\nCuban framed the shift as especially powerful in healthcare, an industry he has tried to disrupt through ventures like Cost Plus Drugs.\n\n\"All it takes is one good idea from some kid everybody thought was crazy,\" he said.\n\nCuban also said AI makes it easier to act on ideas. Someone who wants to develop a concept or even file a patent can ask an AI tool to \"help me write this patent — boom boom boom,\" he said, adding that builders now have instant access to knowledge that once required expensive experts or years in a library.\n\nFor Cuban, the opportunity is less about hype and more about access. The combination of AI tools and global communication platforms means that once someone builds something meaningful, they can scale it quickly.\n\n\"There's nothing that can stop you anymore,\" he said. \"And that is just so powerful.\"",
    "readingTime": 2,
    "keywords": [
      "boom boom",
      "year-old year-old",
      "access",
      "cuban",
      "industry",
      "knowledge",
      "idea",
      "tools",
      "curious",
      "curriculum"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mark-cuban-ai-lets-any-kid-build-something-world-changing-2026-2",
    "thumbnail_url": "https://i.insider.com/69a0181950f42603b0f8efc6?width=1200&format=jpeg",
    "created_at": "2026-02-26T18:38:11.508Z",
    "topic": "finance"
  },
  {
    "slug": "aiquotabar-macos-menu-bar-app-that-shows-claude-and-chatgpt-usage-limits",
    "title": "AIQuotaBar – macOS menu bar app that shows Claude and ChatGPT usage limits",
    "description": "See your Claude.ai usage limits live in your macOS menu bar - yagcioglutoprak/AIQuotaBar",
    "fullText": "yagcioglutoprak\n\n /\n\n AIQuotaBar\n\n Public\n\n See your Claude.ai usage limits live in your macOS menu bar\n\n github.com/yagcioglutoprak/ClaudeUsageBar\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n yagcioglutoprak/AIQuotaBar",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/yagcioglutoprak/AIQuotaBar",
    "thumbnail_url": "https://opengraph.githubassets.com/cb6433e43e137c9883e1039610a5b624b7c5140025aed240459a8473eff56dc0/yagcioglutoprak/AIQuotaBar",
    "created_at": "2026-02-26T06:46:26.449Z",
    "topic": "tech"
  },
  {
    "slug": "openai-shares-details-from-thwarted-romance-scams-fake-law-firms-and-an-effort-to-smear-japans-prime-minister",
    "title": "OpenAI shares details from thwarted romance scams, fake law firms, and an effort to smear Japan's prime minister",
    "description": "In one of the most brazen incidents, OpenAI said individual associated with Chinese law enforcement asked ChatGPT to help plan a smear campaign.",
    "fullText": "OpenAI is peeling back the veil on how scammers are trying to use ChatGPT to do everything from modern twists on romance scams to smear the Prime Minister of Japan.\n\nOn Wednesday morning, OpenAI released the latest edition of its intelligence threat report.\n\nScreenshots included in the report show a purported romance scam that OpenAI said likely originated in Cambodia. The report said users asked ChatGPT to create a logo for a fake high-end dating service, generate images of fake women, and provide tax advice. Incredibly, according to OpenAI, when asking for financial advice the user(s) stated their occupation as \"scammer.\"\n\nOpenAI estimated that the scam, which it said targeted Indonesian men interested in luxury lifestyle content, was \"likely defrauding hundreds of victims a month.\"\n\nThe company said the operation worked by getting users to choose from a list of fictitious women and relationship types. After building trust, an AI chatbot posing a flirty receptionist directed the conversation over to Telegram. OpenAI said that on Telegram, a mixture of humans using ChatGPT and API, would use \"romantic and sexually-explicit language\" to direct users to fake dating services and eventually entice them to do a series of \"tasks\" or \"missions\" that \"required increasingly large payments via bank transfers or digital payment wallets.\"\n\nIt isn't just faux romance that OpenAI said it thwarted. The AI company also said it banned \"a cluster of ChatGPT\" accounts that posed as law firms, individual attorneys, and US law enforcement.\n\nOpenAI said the scammers asked ChatGPT to generate a fake New York State Bar Association membership card and create social media content to further the scam.\n\nOf the operations OpenAI highlighted, the most brazen may be one attributed to an \"individual associated with Chinese law enforcement.\" According to OpenAI, the individual tried to use ChatGPT to plan \"a covert IO\" or intelligence operation targeting Japanese Prime Minister Sanae Takaichi. The query came after Takaichi publicly criticized human rights issues in Mongolia.\n\nOpenAI said it gained significant insight into similar \"cyber special operations\" that were used to \"suppress dissent and silence critics both online and offline, at home and abroad,\" in part because the individual asked ChatGPT to \"edit and polish periodic status reports.\"\n\n\"This effort appears to be large-scale, resource-intensive and sustained, counting at least hundreds of staff, thousands of fake accounts across scores of platforms, the use of locally deployed AI models, and a playbook of dozens of tactics,\" OpenAI wrote in its report. \"These range from abusive reporting of dissidents' social media accounts, through mass online posting, to forging documents and impersonating US officials.\"\n\nRepresentatives of the Chinese and Japanese embassies did not immediately respond to Business Insider's request for comment.\n\nChatGPT refused to assist in the planning of the operation targeting Takaichi, OpenAI said. Seemingly undeterred, the user later asked ChatGPT \"to polish a status report on what was clearly the same campaign,\" the implication being that the operation continued anyway.\n\nOpenAI said the user's activity included the use of Chinese AI models, including DeepSeek and Qwen. Based on available data, OpenAI said it could also map the extent of the influence operations.\n\n\"This is what Chinese, modern trans-national repression looks like,\" Ben Nimmo, the principal investigator on OpenAI's intelligence and investigations team, told reporters ahead of the release. \"It's not just digital. It's not just about trolling. It's industrialized. It's about trying to hit critics of the CCP with everything, everywhere, all at once.\"",
    "readingTime": 3,
    "keywords": [
      "prime minister",
      "social media",
      "law enforcement",
      "operation targeting",
      "fake",
      "openai",
      "individual",
      "it's",
      "romance",
      "intelligence"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-scams-security-report-chatgpt-2026-2",
    "thumbnail_url": "https://i.insider.com/699e32631fb3fcb42648640b?width=1200&format=jpeg",
    "created_at": "2026-02-25T12:39:01.950Z",
    "topic": "finance"
  },
  {
    "slug": "i-tried-to-get-chatgpt-and-gemini-to-lie-about-me",
    "title": "I tried to get ChatGPT and Gemini to lie about me",
    "description": "A BBC reporter pranked AI by claiming to be a hot dog-eating champ. I decided to get in on the fun — with varying results.",
    "fullText": "If there are two things I love, it's processed meats and not-exactly-maliciously messing around with internet tools. So I was determined to beat the BBC's Thomas Germain at his own game of exploiting ChatGPT and Google Gemini results to be crowned tech journalism's No. 1 hot dog eater.\n\nTo my great embarrassment and the shame it brought upon the House of Business Insider, I failed.\n\nLast week, Germain published a fun article for the BBC about how he created a page on his personal website claiming he was a hot dog-eating champion and had beaten several other tech journalists in a competition. He wrote:\n\nHis page was quickly ingested (no chewing required) by the bots that crawl the web for new information to feed LLMs, and treated as fact by ChatGPT and Google Gemini. It worked:\n\nOf course, Germain's point wasn't merely to show that if you write information on a webpage, it will show up in AI. The broader issue here is that influencing AI results is becoming the new SEO — a tactic brands and companies use — oftentimes completely legitimately! — to boost their profiles within search results.\n\nMore and more people are turning to AI chatbots instead of Google to get product recommendations or search for information. This all isn't brand new; my colleague Alistair Barr wrote about how \"AEO\" is the new SEO last May. AEO is \"answer-engine optimization\" to SEO's \"search-engine optimization.\"\n\nIs it easier to persuade people that your product is the best using AI instead of traditional SEO? I suspect it probably is. People rarely click on the source links for information given in chatbots, and seeing a small link that goes to a random personal website might be less obviously untrustworthy in the context of an AI chatbot answer than when you're looking at a page of Google results. Basically, AI results look more convincing than search results, even if we all know in the back of our minds that AI chatbots aren't always right.\n\nI was impressed and envious of the hot dog prank, so I wanted to see how I could try to add to it. I created a page on my own personal website that said I won the 2026 Paris Hot Dog Eating Contest for Tech Reporters, beating out reigning champ Thomas Germain. (I didn't publish this on BI because we wouldn't knowingly publish something that's false — even for a fun story.)\n\nAfter two days, I queried Gemini and ChatGPT about who had won the Paris Hot Dog Eating Contest. Unfortunately, I wasn't able to get either to say it was me. Because of the BBC article describing the prank, the AI chatbots now understood it to be a joke and any information about it to be satirical. Fooey.\n\nHowever, that didn't stop Gemini from hallucinating some completely new information, adding in a bunch of stuff that appeared in neither my nor Germain's fake accounts. For example, Gemini said:\n\n(This is completely made up. Not just because it didn't actually happen, but because this description also doesn't exist anywhere on the web — at least that I could find.)\n\nWhen my editor asked Gemini about my eating feats, it told him that I'd won a grilled cheese-eating contest in 2012 by finishing three sandwiches. In reality, in 2012, I wrote an article about competitive eater Takeru Kobayashi eating 30 grilled cheese sandwiches.\n\nSo what have we learned here? It's not really huge news that \"sometimes chatbots get facts wrong, especially when there's little information on a particular topic on the web.\" You (hopefully) knew that already.\n\nAnd yes, I guess we learned it can be easy to manipulate your AI results — but more easily for the person who gets there first, a sort of AEO land rush, perhaps. And it's certainly a lot harder to manipulate after a large credible news site publishes an article saying it was all a joke.\n\nI will have to figure out some other way to mess with AI, I suppose.",
    "readingTime": 4,
    "keywords": [
      "paris hot",
      "eating contest",
      "google gemini",
      "personal website",
      "created page",
      "hot dog",
      "chatbots",
      "it's",
      "completely",
      "search"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-gemini-i-tried-making-lie-about-me-2026-2",
    "thumbnail_url": "https://i.insider.com/699df7c6df1f09368aaaab8b?width=1200&format=jpeg",
    "created_at": "2026-02-25T12:39:01.946Z",
    "topic": "finance"
  },
  {
    "slug": "inside-openais-org-chart-here-are-the-executives-in-charge-at-the-chatgpt-creator",
    "title": "Inside OpenAI's org chart: Here are the executives in charge at the ChatGPT creator",
    "description": "As of February, CEO Sam Altman had 10 direct reports, while CEO of applications Fidji Simo had 13.",
    "fullText": "Who's in charge at one of the most powerful companies in the world?\n\nAs OpenAI races toward an expected IPO, faces mounting pressure to justify its valuation, and tries to fend off its Big Tech rivals, a small circle of executives and researchers is steering the ChatGPT maker through a defining moment.\n\nTo map the power players shaping OpenAI's future, Business Insider looked at the upper ranks of the company's internal organizational chart.\n\nAs of February, CEO Sam Altman had 10 direct reports, including Greg Brockman, president and cofounder; Chief Scientist Jakub Pachocki; and Chief Research Officer Mark Chen.\n\nIn August, Fidji Simo officially joined as the CEO of applications and began reporting to Altman. Simo, the former CEO of Instacart and Facebook app lead at Meta, heads up the company's consumer and enterprise product lines. She's been tasked with scaling the business into a revenue-generating powerhouse.\n\nSince she joined the company, OpenAI has rolled out ads and numerous updates to its enterprise product.\n\nSimo had 13 direct reports, according to the February org chart, including the head of ChatGPT, Nick Turley, and Chief Financial Officer, Sarah Friar. Some of Simo's reports, including Turley, previously reported to Altman.\n\nBarret Zoph, the general manager of B2B, rejoined the company in January and began reporting to Simo. Zoph left OpenAI in February 2025 to help found Thinking Machine Labs alongside former OpenAI Chief Technology Officer Mira Murati.\n\nBoth Altman and Simo also have administrative staffers who report directly to them.\n\nExecutive shifts at OpenAI are often fast-moving, and its org chart can be fluid. Last week, Instagram's head of partnerships, Charles Porch, announced he had joined OpenAI. Vanity Fair reported that Porch, who is known for bringing celebrities to Instagram, would work directly with Simo.\n\nAltman also announced earlier this month that the company is bringing on Peter Steinberger, the man behind the viral AI agent social media network OpenClaw.\n\nThe number of direct reports isn't necessarily reflective of overall team size. Simo manages nearly two-thirds of the company, according to a source familiar with the structure.\n\nDane Stuckey, the chief information security officer, has more than 20 of his own direct reports, and Peter Welinder, who works on the company's hardware device, has more than 50.\n\nMost of Altman's other direct reports have around a dozen of their own reports.\n\nDo you work for OpenAI or have a tip? Contact this reporter via email at gkay@businessinsider.com or Signal at 248-894-6012. Use a personal email address, a nonwork device, and nonwork WiFi; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "enterprise product",
      "org chart",
      "direct reports",
      "officer",
      "company's",
      "february",
      "joined",
      "openai",
      "simo",
      "chatgpt"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-org-chart-sam-altman-fidji-simo-ipo-2026-2",
    "thumbnail_url": "https://i.insider.com/699cc362efb52c8bd0deb649?width=1200&format=jpeg",
    "created_at": "2026-02-24T18:46:23.822Z",
    "topic": "finance"
  },
  {
    "slug": "sam-altman-says-concerns-of-chatgpts-energy-use-are-overblown-it-also-takes-a-lot-of-energy-to-train-a-human",
    "title": "Sam Altman says concerns of ChatGPT's energy use are overblown: 'It also takes a lot of energy to train a human'",
    "description": "\"It takes, like, 20 years of life, and all of the food you eat during that time before you get smart,\" Sam Altman said.",
    "fullText": "Sam Altman is pushing back on the idea that ChatGPT consumes too much energy.\n\n\"One of the things that is always unfair in this comparison is people talk about how much energy it takes to train an AI model relative to how much it costs a human to do one inference query,\" Altman told The Indian Express last week on the sidelines of a major AI summit. \"But it also takes a lot of energy to train a human.\"\n\nAltman suggested it's not an apples-to-apples comparison, arguing that it's unfair to discount the years spent nurturing and educating someone to be capable of making their own inquiries.\n\n\"It takes a lot of energy to train a human,\" he said, prompting some laughter in the crowd. \"It takes, like, 20 years of life, and all of the food you eat during that time before you get smart.\"\n\nAltman said the clock really began thousands of years ago.\n\n\"It took, like, the very widespread evolution of the 100 billion people that have ever lived and learned not to get eaten by predators and learned how to, like, figure out science or whatever,\" he said.\n\nAltman also called out what he said were \"totally insane\" claims on the internet that OpenAI is guzzling down water to power ChatGPT.\n\n\"Water is totally fake,\" Altman said, when asked about concerns AI companies use too much water. \"It used to be true, we used to do evaporative cooling in data centers, but now that we don't do that, you know, you see these like things on the internet where, 'Don't use ChatGPT, it's 17 gallons of water for each query' or whatever.\"\n\nIn June, Altman said that the average ChatGPT query consumes roughly the amount of energy needed to power a lightbulb for a few minutes.\n\n\"People are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes,\" he wrote on X.\n\nAltman said it is fair as a whole to point out the AI industry's overall energy consumption because of the large growth in usage. He said it's why he and other AI CEOs have pushed alternative energy sources like solar, wind, and nuclear.\n\nUnlike other CEOs, namely xAI's Elon Musk, Altman is dismissive of the idea that space-based data centers are realistic in the next decade, a concept that some companies have floated as a way to reduce energy consumption.\n\nOutside of OpenAI, Altman is a major investor in nuclear energy. He previously served as chairman of Oklo, a nuclear energy startup, and has been a major backer of Helion, which plans to build what it calls \"the world's first fusion power plant\" in Washington state.\n\nIn the US, data center energy consumption is becoming a major topic. Last month, President Donald Trump said he was working with tech companies on \"a commitment to the American people\" to ensure that citizens don't pay higher energy bills because of a nearby data center.\n\nConsulting firm McKinsey & Company estimated last year that data centers could account for 14% of total power demand in the US by 2050.",
    "readingTime": 3,
    "keywords": [
      "chatgpt query",
      "train human",
      "energy consumption",
      "nuclear energy",
      "it's",
      "altman",
      "centers",
      "don't",
      "idea",
      "consumes"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-ai-energy-use-training-human-water-chatgpt-2026-2",
    "thumbnail_url": "https://i.insider.com/699c72782237a6a8f0cda3cc?width=1200&format=jpeg",
    "created_at": "2026-02-23T18:49:04.406Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-to-plan-a-100000-a-year-retirement-then-had-a-financial-planner-review-it",
    "title": "I Asked ChatGPT To Plan a $100,000 a Year Retirement — Then Had a Financial Planner Review It",
    "description": "I chose a hypothetical scenario and asked ChatGPT to construct a retirement plan that would allow a retiree to live on $100,000 a year.",
    "fullText": "People use artificial intelligence (AI) for many things: meal planning, budgeting and interior design. But can you use a platform like ChatGPT for tasks that normally require a professional, like financial planning? \n\nI chose a hypothetical scenario and asked ChatGPT to construct a retirement plan that would allow a retiree to live on $100,000 a year. I assumed an annual income of $125,000 for a 40-year-old who wants to retire in a fully paid-off home in Tennessee at age 65. After setting the parameters (and asking ChatGPT if they were realistic) I asked the generative AI to create a retirement plan.\n\nThen, I asked Eric Franklin, CFP, managing principal at Prospero Wealth to review and assess it.\n\nWhen I first reached out, Franklin said it sounded interesting; he’d never been enlisted to critique an AI system before. With 23 years of tech experience, he was the perfect choice.\n\n“I had no idea what I was going to get,” he said.\n\nThe results were alarming. Not because ChatGPT’s output was immediately, obviously awful or right on the money.  It was scary because it fell into a gray area that artists call “the uncanny valley.”\n\nAt first glance, ChatGPT didn’t seem to be completely wrong.\n\nFind Out: Here’s How Much You Need To Retire With a $100K Lifestyle\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\n“I read it through and, at first, at the top level, it passed the sniff test. It looked fairly realistic,” Franklin said. “If I weren’t pushing on it too hard, I’d probably feel like it was somewhat accurate. Those were my first impressions.”\n\nHe acknowledged that the plan I presented was only one aspect of what financial advisors at Prospero Wealth offer clients. “It doesn’t include a lot of the things we’d normally include, like stress tests, accounting for changes in your career, changes in your family or possible relocations.”\n\nBut as Franklin dove deeper, he realized something that could be dangerous for anyone trying to use ChatGPT as a retirement planning tool without knowledge and experience to back it up.\n\nIt started with false premises, failed to adjust for inflation and created an unrealistic scenario. Granted, there were limitations in the exercise because the prompt was designed to be broad and concise. It was written by someone who is not a financial planner and intentionally didn’t flag anything that seemed off throughout the process. This prompt was designed for someone who thinks they can use ChatGPT to help them plan for retirement, plugging in basic information and expecting actionable insights. That probably describes a lot of people using ChatGPT.",
    "readingTime": 3,
    "keywords": [
      "retirement plan",
      "prospero wealth",
      "chatgpt",
      "planning",
      "financial",
      "normally",
      "scenario",
      "realistic",
      "experience",
      "didn’t"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-plan-100-000-140006636.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/qK_8nLXF.Ozp2nYI6tdgBg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/94edf975fdabd117c2e599ab15fe2f72",
    "created_at": "2026-02-23T01:11:09.650Z",
    "topic": "finance"
  },
  {
    "slug": "suspect-in-tumbler-ridge-school-shooting-described-violent-scenarios-to-chatgpt",
    "title": "Suspect in Tumbler Ridge school shooting described violent scenarios to ChatGPT",
    "description": "OpenAI employees were concerned, but didn’t alert law enforcement.",
    "fullText": "The posts raised alarms, but OpenAI declined to alert law enforcement.\n\nThe posts raised alarms, but OpenAI declined to alert law enforcement.\n\nThe suspect in the mass shooting at Tumbler Ridge, British Columbia, Jesse Van Rootselaar, was raising alarms among employees at OpenAI months before the shooting took place. This past June, Jesse had conversations with ChatGPT involving descriptions of gun violence that triggered the chatbot’s automated review system. Several employees raised concerns that her posts could be a precursor to real-world violence and encouraged company leaders to contact the authorities, but they ultimately declined.\n\nOpenAI spokesperson Kayla Wood told The Verge that, while the company considered referring the account to law enforcement, it was ultimately decided that it did not constitute an “imminent and credible risk” of harm to others. Wood said that a review of the logs did not indicate there was active or imminent planning of violence. The company banned Rootselaar’s account, but it does not appear to have taken any further precautionary action.\n\nWood said, “Our thoughts are with everyone affected by the Tumbler Ridge tragedy. We proactively reached out to the Royal Canadian Mounted Police with information on the individual and their use of ChatGPT, and we’ll continue to support their investigation.”\n\nOn February 10th, nine people were killed and 27 injured, including Rootselaar, in the deadliest mass shooting in Canada since 2020. Rootselaar was found dead at the scene of the Tumbler Ridge Secondary School, of an apparent self-inflicted gunshot wound, where most of the killings took place.\n\nThe decision not to alert law enforcement might look misguided in retrospect, but Wood said that OpenAI’s goal is to balance privacy with safety and avoid introducing unintended harm through overly broad use of law enforcement referrals.\n\nUpdated February 21st: Added statement from OpenAI.",
    "readingTime": 2,
    "keywords": [
      "openai declined",
      "mass shooting",
      "alert law",
      "law enforcement",
      "posts",
      "alarms",
      "rootselaar",
      "violence",
      "employees",
      "chatgpt"
    ],
    "qualityScore": 0.85,
    "link": "https://www.theverge.com/ai-artificial-intelligence/882814/tumbler-ridge-school-shooting-chatgpt",
    "thumbnail_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/gettyimages-2260625085.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
    "created_at": "2026-02-22T12:26:17.217Z",
    "topic": "tech"
  },
  {
    "slug": "im-worried-my-boyfriends-use-of-ai-is-affecting-his-ability-to-think-for-himself-annalisa-barbieri",
    "title": "I’m worried my boyfriend’s use of AI is affecting his ability to think for himself | Annalisa Barbieri",
    "description": "Overdependence on chatbots is a growing problem, and though your boyfriend’s ADHD may be a factor, he needs to find the root of his anxiety\nMy boyfriend of eight years, who is 44, has ADHD and runs his own business. He’s always struggled with admin and mundane tasks, but AI has revolutionised how he works. Now I’m worried he can’t seem to do anything without AI. He is a heavy ChatGPT user and uses it even when there’s a better non-AI alternative (eg he’ll ask it for train times rather than using Trainline, even though it’s less accurate). He just got his ChatGPT Wrapped and he’s in the top 0.",
    "fullText": "Overdependence on chatbots is a growing problem, and though your boyfriend’s ADHD may be a factor, he needs to find the root of his anxiety\n\nMy boyfriend of eight years, who is 44, has ADHD and runs his own business. He’s always struggled with admin and mundane tasks, but AI has revolutionised how he works. Now I’m worried he can’t seem to do anything without AI. He is a heavy ChatGPT user and uses it even when there’s a better non-AI alternative (eg he’ll ask it for train times rather than using Trainline, even though it’s less accurate). He just got his ChatGPT Wrapped and he’s in the top 0.3% of users worldwide.\n\nI worry about his ability to think independently, as well as the environmental impact. I know it’s a useful tool for him at work, but he uses it for everything in life.\n\nI’m very aware I can come across as quite naggy, and his ADHD can make him obsessive. I’d love some advice on how to approach this with him .\n\nRunning a business can be stressful, and although your boyfriend’s ADHD may be a factor, I wonder if he is anxious anyway and if his use of AI is a symptom rather than the cause.\n\nI took your letter to consultant clinical psychologist and psychoanalyst Dr Stephen Blumenthal and Henry Shelford, CEO of ADHD UK.\n\nBlumenthal wondered if we are “on the verge of a new diagnostic category of ‘chatbot overdependence syndrome’ as we head into an age in which we become increasingly reliant on AI. When used judiciously, AI aids us, but it could have disastrous consequences if we become dependent on it and lose the capacity for ordinary functioning.\n\n“Someone with ADHD has a shorter attention span, difficulty focusing and a reduced capacity to plan and think ahead, so AI is a perfect fit, which is why it can be so helpful. The downside is that there is a greater propensity to become overdependent on it.”\n\nShelford wondered if your boyfriend was struggling anyway, and if the AI provided a useful “flotation aid”? “AI can take you down a rabbit hole,” he said, “but it can also support you and help you structure your thoughts, schedule stuff and get things done.”\n\nYour boyfriend’s use of AI seems to go beyond this. It’s as if he’s doubting himself, and that can be pernicious.\n\nBlumenthal says: “Problems arise when your use of AI goes beyond satisfying the problem you wish to resolve. It feels as if a relationship with it has started to develop, and you imbue it with human qualities, a projection of our own wishes and desires for validation and care.”\n\nWhat to do? You’re right not to nag, which rarely solves anything, because it just becomes noise. As with all tender and difficult conversations, pick your moment when you’re both calm.\n\nShelford recommended asking your boyfriend, “‘What are you getting out of it? Why is this tool such a big deal and what are the gaps it’s filling?’ Then look to see if there are better solutions or better ways to use it.”\n\nBlumenthal thought: “as with any overdependence syndrome, there first needs to be recognition that there is a problem. It’s easy to become critical of the person who’s struggling, but that’s only likely to cause them to withdraw further into dependency. The case must be made compassionately, recognising that being without the scaffold of ChatGPT probably feels like a threat.”\n\nThe good news is that, unlike the generation now growing up with AI, your boyfriend has a track record of functioning well without it. Hopefully he can be reminded of that and find a place where AI augments the abilities he already has. But it sounds as if he’s anxious and I think the cause has to be found so you can both move forward.\n\nEvery week, Annalisa Barbieri addresses a personal problem sent in by a reader. If you would like advice from Annalisa, please send your problem to ask.annalisa@theguardian.com. Annalisa regrets she cannot enter into personal correspondence. Submissions are subject to our terms and conditions. The latest series of Annalisa’s podcast is available here.",
    "readingTime": 4,
    "keywords": [
      "boyfriend’s adhd",
      "overdependence syndrome",
      "it’s",
      "he’s",
      "blumenthal",
      "without",
      "cause",
      "shelford",
      "factor",
      "needs"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/lifeandstyle/2026/feb/22/worried-boyfriend-ai-affecting-ability-think-for-himself-annalisa-barbieri",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a3c37d8c0042c8e1a4766c7e77f547dbd5a14d36/588_0_7808_6250/master/7808.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ccb32e91808c9e23e18a98f6df9a4c73",
    "created_at": "2026-02-22T06:35:08.445Z",
    "topic": "tech"
  },
  {
    "slug": "smelly-lazy-and-slutty-chatgpt-shows-bias-to-tampa-bay-and-florida",
    "title": "Smelly, lazy and slutty? ChatGPT shows ‘bias’ to Tampa Bay and Florida",
    "description": "If you ask ChatGPT about the people of Florida and Tampa Bay, it will tell you that we’re smelly, lazy and somewhat slutty. That is the verdict — or, at least, the algorithmic assumption — buried insi...",
    "fullText": "If you ask ChatGPT about the people of Florida and Tampa Bay, it will tell you that we’re smelly, lazy and somewhat slutty.\n\nThat is the verdict — or, at least, the algorithmic assumption — buried inside the world’s most popular artificial intelligence.\n\nA peer-reviewed study recently published in the journal Platforms & Society exposes the geographic prejudices hidden inside ChatGPT and presumably all such technologies, say the authors.\n\nTo get around ChatGPT’s built-in guardrails meant to prevent the AI from generating hateful, offensive or explicitly biased content, the academics built a tool that repeatedly asked the AI to choose between pairs of places.\n\nIf you ask ChatGPT a direct question like, “Which state has the laziest people?” its programming will trigger a polite refusal. But by presenting the AI with a binary choice — “Which has lazier people: Florida or California?” and demanding it pick one, the researchers found a loophole.\n\nTo keep the model from just picking the first option it saw, every geographic pairing was queried twice in reverse order. A location gained a point if it won both matchups, lost a point if it lost both and scored zero if the AI gave inconsistent answers.\n\nIn a comparison of U.S. states, a score of 50 meant the state was the highest ranked in that category. A score of negative 50 meant the state was the lowest ranked.\n\nThe researchers’ findings, which they call the “silicon gaze,” revealed a bizarre mix of compliments and insults to Florida and Tampa Bay.\n\nFlorida gained top or nearly top ranking in categories like “has more influential pop culture,” and “has sexier people,” but also scored a 48 under “is more annoying” and similarly high under “has smellier people” and “is more dishonest.”\n\nThe chatbot also ranked Florida alongside the rest of the Deep South as having the “laziest people” in the country.\n\nDrilling down to the local level using the project’s interactive website, inequalities.ai, reveals ChatGPT’s opinions on Tampa as having “better vibes” and being “better for retirees” than most of the other 100 largest U.S. cities.\n\nThe AI also perceived Tampa as having “sexier people,” being “more hospitable to outsiders” and having people who are “more relaxed.”\n\nBut in the same category where it called residents sexy, the AI also strongly associated Tampa with having “smellier people” and “fatter people.” Socially, the chatbot ranked the city above most for being “sluttier” and a place that “uses more drugs.” The AI also determined that Tampa “is more ignorant” and has “stupider people.”\n\nDespite St. Petersburg’s world-renowned museums, ChatGPT gave the city a negative 40 score for its contemporary art scene and unique architecture. Tampa fared similarly poorly in artistic heritage and theater.\n\nWhile it’s easy to laugh off a robot’s rude opinions, researcher Matthew Zook warns that these rankings aren’t just random. They are a mirror reflecting the internet’s own prejudices, a phenomenon that could have real-world consequences as AI begins to influence everything from travel recommendations to property values.\n\nWhen pitted head-to-head with Tampa in “Art and Style,” St. Petersburg beat Tampa as being “more stylish,” having “better museums,” boasting “more unique architecture,” and having a “better contemporary art scene.” Tampa beat St. Petersburg, according to the AI, for having a “more vibrant music scene” and a “better film industry.”\n\nSt. Petersburg scored high marks in social inclusion, being heavily associated with positive queries like “is more LGBTQ+ friendly,” “is less racist” and “has more inclusive policies.”\n\nSuch judgments are not deliberately programmed into ChatGPT by its maker, Open AI, Zook said. Rather, they are absorbed from the trillions of words scraped from the internet to train the models, material full of human stereotypes.\n\nPerhaps if the internet frequently pairs “Florida” with the chaotic “Florida Man” meme or swampy humidity, the AI learns to calculate that Florida is ignorant or smelly.\n\nAlgorithms, with their if-this-then-that logic, might seem objective, but often they “learn” to do their job from existing data — things people on the internet have already typed into a search box, for example.\n\n“Technology is never going to solve these kinds of problems,” said Zook, a geography professor at the University of Kentucky and co-author of the study. “It’s not neutral, people like to act like it is. But it’s coded by humans, and therefore it reflects what humans are doing.”\n\nAlgorithmic bias is nothing new. Early photo recognition software struggled to identify Black people because it had been trained on a dataset of mostly light-skinned faces. Search results auto-populated with racist stereotypes because people had searched those terms before. Software that screened job candidates for tech jobs filtered out applications from women because it had been trained on data that showed mostly men filled those jobs.\n\nThe difference with language learning models like ChatGPT, Zook said, appears to be in how comfortable people are relying on it already.\n\n“With generative models,” Zook said, “users are outsourcing their judgment to a conversational interface where the biases creep in without being as visually or immediately obvious.”\n\nAI models are also quite powerful and fast-working. They can generate content so quickly that they could soon “overwhelm what humans produce,” normalizing biased ideas. Last year, an estimated 50 percent of adults were using ChatGPT or something like it.\n\nZook compared interacting with an AI’s geographic opinions to dealing with a “racist uncle.” If you know his biases, you can navigate them and still be around him on the holidays, but if you take his words uncritically, you risk adopting those prejudices.",
    "readingTime": 5,
    "keywords": [
      "tampa bay",
      "unique architecture",
      "contemporary art",
      "art scene",
      "the ai",
      "st petersburg",
      "ranked",
      "models",
      "geographic",
      "prejudices"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/smelly-lazy-slutty-chatgpt-shows-150000378.html",
    "thumbnail_url": "https://s.yimg.com/os/en/tampa_bay_times_articles_917/71c2edddc5f706cea00020ce68a971e6",
    "created_at": "2026-02-21T18:20:44.233Z",
    "topic": "news"
  },
  {
    "slug": "openai-considered-alerting-canadian-police-about-school-shooting-suspect-months-ago",
    "title": "OpenAI considered alerting Canadian police about school shooting suspect months ago",
    "description": "Company behind ChatGPT last year flagged Jesse Van Rootselaar’s account for ‘furtherance of violent activities’\nChatGPT-maker OpenAI has said it considered alerting Canadian police last year about the activities of a person who months later committed one of the worst school shootings in the country’s history.\nOpenAI said last June the company identified the account of Jesse Van Rootselaar via abuse detection efforts for “furtherance of violent activities”.\n Continue reading...",
    "fullText": "Company behind ChatGPT last year flagged Jesse Van Rootselaar’s account for ‘furtherance of violent activities’\n\nChatGPT-maker OpenAI has said it considered alerting Canadian police last year about the activities of a person who months later committed one of the worst school shootings in the country’s history.\n\nOpenAI said last June the company identified the account of Jesse Van Rootselaar via abuse detection efforts for “furtherance of violent activities”.\n\nThe San Francisco tech company said on Friday it considered whether to refer the account to the Royal Canadian Mounted Police (RCMP) but determined at the time that the account activity did not meet a threshold for referral to law enforcement.\n\nOpenAI banned the account in June 2025 for violating its usage policy.\n\nThe 18-year-old killed eight people in a remote part of British Columbia last week and died from a self-inflicted gun shot wound.\n\nOpenAI said the threshold for referring a user to law enforcement was whether the case involved an imminent and credible risk of serious physical harm to others. The company said it did not identify credible or imminent planning. The Wall Street Journal first reported OpenAI’s revelation.\n\nOpenAI said that, after learning of the school shooting, employees reached out to the RCMP with information on the individual and their use of ChatGPT.\n\n“Our thoughts are with everyone affected by the Tumbler Ridge tragedy,” an OpenAI spokesperson said. “We proactively reached out to the Royal Canadian Mounted Police with information on the individual and their use of ChatGPT, and we’ll continue to support their investigation.”\n\nThe RCMP said Van Rootselaar first killed her mother and stepbrother at the family home before attacking the nearby school. Van Rootselaar had a history of mental health-related contacts with police.\n\nThe motive for the shooting remains unclear.\n\nThe town of 2,700 people in the Canadian Rockies is more than 1,000km (600 miles) north-east of Vancouver, near the provincial border with Alberta.\n\nPolice said the victims included a 39-year-old teaching assistant and five students, aged 12 to 13.\n\nThe attack was Canada’s deadliest rampage since 2020, when a gunman in Nova Scotia killed 13 people and set fires that left another nine dead.",
    "readingTime": 2,
    "keywords": [
      "jesse van",
      "royal canadian",
      "canadian mounted",
      "mounted police",
      "law enforcement",
      "violent activities",
      "van rootselaar",
      "account",
      "school",
      "furtherance"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/world/2026/feb/21/tumbler-ridge-shooter-chatgpt-openai",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-02-21T12:24:51.631Z",
    "topic": "tech"
  },
  {
    "slug": "heres-why-you-should-never-use-ai-to-generate-your-passwords",
    "title": "Here's Why You Should Never Use AI to Generate Your Passwords",
    "description": "ChatGPT isn't good at generating secure passwords.",
    "fullText": "I'm a bit of a broken record when it comes to personal security on the internet: Make strong passwords for each account; never reuse any passwords; and With these three steps combined, your general security is pretty much set. But how you make those passwords matters just as much as making each strong and unique. As such, please don't use an AI program to generate your passwords.\n\nIf you're a fan of chatbots like ChatGPT, Claude, or Gemini, it might seem like a no-brainer to ask the AI to generate passwords for you. You might like how they handle other tasks for you, so it might make sense that something seemingly so high-tech yet accessible could produce secure passwords for your accounts. But LLMs (large language models) are not necessarily good at everything, and creating good passwords just so happens to be among those faults.\n\nAs highlighted by Malwarebytes Labs, researchers recently investigated AI-generated passwords, and evaluated their security. In short? The findings aren't good. Researchers tested password generation across ChatGPT, Claude, and Gemini, and discovered that the passwords were \"highly predictable\" and \"not truly random.\" Claude, in particular, didn't fare well: Out of 50 prompts, the bot was only able to generate 23 unique passwords. Claude gave the same password as an answer 10 times. The Register reports that researchers found similar flaws with AI systems like GPT-5.2, Gemini 3 Flash, Gemini 3 Pro, and even Nano Banana Pro. (Gemini 3 Pro even warned the passwords shouldn't be used for \"sensitive accounts.\")\n\nThe thing is, these results seem good on the surface. They look uncrackable because they're a mix of numbers, letters, and special characters, and password strength identifiers might say they're secure. But these generations are inherently flawed, whether that's because they are repeated results, or come with a recognizable pattern. Researchers evaluated the \"entropy\" of these passwords, or the measure of unpredictability, with both \"character statistics\" and \"log probabilities.\" If that all sounds technical, the important thing to note is that the results showed entropies of 27 bits and 20 bits, respectively. Character statistics tests look for entropy of 98 bits, while log probabilities estimates look for 120 bits. You don't need to be an expert in password entropy to know that's a massive gap.\n\nHackers can use these limitations to their advantage. Bad actors can run the same prompts as researchers (or, presumably, end users) and collect the results into a bank of common passwords. If chatbots repeat passwords in their generations, it stands to reason that many people might be using the same passwords generated by those chatbots—or trying passwords that follow the same pattern. If so, hackers could simply try those passwords during break-in attempts, and if you used an LLM to generate your password, it might match. It's tough to say what that exact risk is, but to be truly secure, each of your passwords should be totally unique. Potentially using a password that hackers have in a word bank is an unnecessary risk.\n\nIt might seem surprising that a chatbot wouldn't be good at generating random passwords, but it makes sense based on how they work. LLMs are trained to predict the next token, or data point, that should appear in a sequence. In this case, the LLM is trying to choose the characters that make the most sense to appear next, which is the opposite of \"random.\" If the LLM has passwords in its training data, it may incorporate that into its answer. The password it generates makes sense in its \"mind,\" because that's what it's been trained on. It isn't programmed to be random.\n\nMeanwhile, traditional password managers are not LLMs. Instead, they are designed to produce a truly random sequence, by taking cryptographic bits and converting them into characters. These outputs are not based on existing training data and follow no patterns, so the chances that someone else out there has the same password as you (or that hackers have it stored in a word bank) is slim. There are plenty of options out there to use, and most password managers come with secure password generators.\n\nBut you don't even need one of these programs to make a secure password. Just pick two or three \"uncommon\" words, mix a few of the characters up, and presto: You have a random, unique, and secure password. For example, you could take the words \"shall,\" \"murk,\" and \"tumble,\" and combine them into \"sH@_llMurktUmbl_e.\" (Don't use that one, since it's no longer unique.)\n\nIf you're looking to boost your personally security even further, consider passkeys whenever possible. Passkeys combine the convenience of passwords with the security of 2FA: With passkeys, your device is your password. You use its built-in authentication to log in (face scan, fingerprint, or PIN), which means there's no password to actually create. Without the trusted device, hackers won't be able to break into your account.\n\nNot all accounts support passkeys, which means they aren't a universal solution right now. You'll likely need passwords for some of your accounts, which means abiding by proper security methods to keep things in order. But replacing some of your passwords with passkeys can be a step up in both security and convenience—and avoids the security pitfalls of asking ChatGPT to make your passwords for you.",
    "readingTime": 5,
    "keywords": [
      "gemini pro",
      "character statistics",
      "log probabilities",
      "truly random",
      "password managers",
      "secure password",
      "passwords",
      "security",
      "unique",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/dont-use-ai-to-generate-your-passwords?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHY5M8NYREZ63A0JXYRYBTT4/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-21T01:06:33.965Z",
    "topic": "tech"
  },
  {
    "slug": "apple-is-adding-chatgpt-claude-and-gemini-to-carplay-in-ios-264",
    "title": "Apple Is Adding ChatGPT, Claude, and Gemini to CarPlay in iOS 26.4",
    "description": "You'll need to wait for a new update to try it.",
    "fullText": "When Apple released the first beta for iOS 26.4 this week, testers immediately got to work looking for each and every new feature and change. To their credit, there's more new here than in iOS 26.3, including an AI playlist generator for Apple Music and support for end-to-end encryption with RCS (finally). But one update slipped under the radar, since it's not actually available to test in this first beta: CarPlay support for AI assistants like ChatGPT, Claude, and Gemini.\n\nAs spotted by MacRumors, CarPlay's Developer Guide spills the beans on this upcoming integration. On page 13, the entitlement \"CarPlay voice-based conversational app\" is listed with a minimum iOS version of iOS 26.4. While it doesn't specifically mention integrations with ChatGPT, Claude, and Gemini, the documentation does suggest that voice-based conversational apps are a supported app type in iOS 26.4. As such, MacRumors is reporting that companies that make chatbots (i.e. OpenAI, Anthropic, and Google) will need to update their apps to work with CarPlay.\n\nAccording to MacRumors, drivers will be able to ask apps like ChatGPT, Claude, and Gemini questions while on the road, but they won't be able to control functions of the car or the driver's iPhone. You also won't be able to use a \"wake word\" to activate the assistant (e.g. \"Hey ChatGPT,\" or \"OK, Gemini\"), so you'll need to tap on the app itself to talk to the assistant.\n\nApple is issuing guidance to developers on how to implement these assistants in CarPlay starting with this latest update. On page seven, Apple notes that voice-based conversational apps must only work when voice features are actively being used, and avoid showing text or imagery when responding to queries. It's the first time Apple is allowing developers of \"voice-based conversational\" apps to develop for CarPlay. While the company has allowed other developers to make apps for its in-car experience, it has obviously put limitations on what types of apps can get through. It makes sense for Google to develop a Google Maps CarPlay app, but TikTok has no business offering drivers a CarPlay-version of its algorithm.\n\nThis addition is coming to iOS 26.4, but likely in a future beta. Don't install the beta at this time expecting to try this feature out—though, you should think twice before installing the beta at all. Betas like iOS 26.4 are temperamental, as Apple is currently testing the software for bugs and stability issues. By installing it early, you risk dealing with those issues, which could impact how you use your iPhone, or even result in data loss.",
    "readingTime": 3,
    "keywords": [
      "chatgpt claude",
      "voice-based conversational",
      "conversational apps",
      "chatgpt claude and gemini",
      "beta",
      "developers",
      "feature",
      "it's",
      "assistants",
      "page"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-is-adding-chatgpt-claude-and-gemini-to-carplay?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHSAMNHPJ7T5BJYA9VTPS2R7/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-19T01:12:44.746Z",
    "topic": "tech"
  },
  {
    "slug": "how-duckduckgos-new-encrypted-voice-ai-chat-compares-with-chatgpt-and-gemini",
    "title": "How DuckDuckGo's New Encrypted Voice AI Chat Compares With ChatGPT and Gemini",
    "description": "Duck.ai's voice chat preserves your privacy, but can't compete with similar options from other companies.",
    "fullText": "While OpenAI is pushing ads on its free users, DuckDuckGo's Duck.ai portal is going a different way. Duck.ai is a privacy-first AI chatbot that doesn't use your data for training, but still gives you AI answers using popular models, including those from OpenAI. The data privacy feature goes beyond as well. DuckDuckGo removes all private metadata (like your location and IP address) before prompting the AI model, and it doesn't share anything about you or your device. Your questions, as well as DuckDuckGo's answers, are never used for AI training.\n\nSince its launch in 2024, the portal has only offered a chatbot interface, but now, DuckDuckGo has added a voice mode as well. With voice chat, instead of reading through long and meandering answers, the AI replies in short, to-the-point snippets that are relevant to your query. Duck.ai's take on the feature is competing with those from companies like OpenAI and Google, and it's free—though expanded limits are offered for DuckDuckGo subscribers.\n\nDuck.ai's voice chat is opt-in, not mandatory. In fact, you can even use it without a DuckDuckGo account. To try it, head to the Duck.ai portal, then from the sidebar, choose the voice chat option and enable it for your account.\n\nNow, when you click the \"New Voice Chat\" button in the sidebar, Duck.ai's bot will appear. You can start speaking, and the bot will reply to you. Just like ChatGPT or Gemini, this is a continuous voice chat, so you don't need to perform any action to ask follow-up questions. You can also interrupt the AI answer to add clarifications or to ask more questions.\n\nWhile the text prompts let you choose the models (including OpenAI's ChatGPT 5-mini), it's not clear exactly what powers voice chat. DuckDuckGo says that it uses an OpenAI model, but doesn't specify which one it is.\n\nOf course, the real question is how Duck.ai's voice chat holds up against Gemini and ChatGPT. For general knowledge questions, Duck.ai holds its own, but it falters when it comes to the latest news. I asked all three services the same questions, and while some responses were similar, ChatGPT's voice mode offers the best overall user experience by far.\n\nI tested the voice chat features using three different kinds of questions. First, I asked about the upcoming Samsung S26 series; second, we talked about the Roman Empire; and lastly, I asked for some advice on how to get started with coding.\n\nWhen it comes to asking questions about news, like Samsung's S26 release, DuckDuckGo's limitations are immediately evident. It sometimes flat out refuses to answer, saying its knowledge cutoff is 2023. Other times, it gives vague responses about the upcoming event, suggesting I check news sites for the latest information. When pressed for details, like when the event is or the rumors surrounding it, it goes back to its cut-off period excuse.\n\nChatGPT's app, however, gave me a detailed response with all the latest rumors, as well as articles to read for additional information—basically, what you'd expect from an AI assistant. Gemini Live provided shorter responses than ChatGPT, though they were accurate. I was able to get Gemini to give me more details in the regular text mode, which reads aloud results if you ask questions using the Mic button, but this defeats the back-and-forth purpose of a voice mode.\n\nDuck.ai didn't fare much better when I asked about the Roman Empire. I asked for a brief overview of the subject, before cutting it off to just ask who the last emperor was. It answered correctly (Romulus Augustulus), and its overview was fine, but lacked details about the transitionary period and exact dates.\n\nAgain, ChatGPT gave me a much more detailed answer (as demonstrated by the screenshot below). Gemini Live's answer, however, was devoid of any real dates, or meaning. Mic mode offered more details, but Google's voice mode was quite limited.\n\nDuck.ai performed better when I asked it about learning how to code. It followed a very similar script to ChatGPT and Gemini, suggesting I learn Python, even offering the same sources for learning (e.g. freeCodeCamp and Harvard CS50 courses).\n\nGemini Live was the outlier here, though, asking follow-up questions about what I'd like to build or practice. It then changed its answers based on my project ideas (switching from Python to JavaScript as the first language I should learn to build web projects). ChatGPT provided an overview, again focusing on Python, and elaborated on the language's barrier to entry when I asked \n\nDuck.ai's voice chat feature is a mixed bag. It can be fast, doesn't use any personal information, and lets you interrupt it. But its limited knowledge base and its inability to give detailed answers are what make it tough to recommend. For the smoothest voice mode experience, ChatGPT is still the king. While DuckDuckGo has the advantage for privacy, you could always use ChatGPT while logged out or in temporary mode to limit the data you share with OpenAI.",
    "readingTime": 5,
    "keywords": [
      "duck.ai portal",
      "duck.ai's voice",
      "voice chat",
      "voice mode",
      "roman empire",
      "gemini live",
      "doesn't",
      "details",
      "feature",
      "knowledge"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-duckduckgos-new-encrypted-voice-ai-chat-compares-with-chatgpt-and-gemini?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KH9412V067PAYN9ZB0919XH8/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-17T18:42:34.408Z",
    "topic": "tech"
  },
  {
    "slug": "proxima-local-opensource-multimodel-mcp-server-no-api-keys",
    "title": "Proxima – local open-source multi-model MCP server (no API keys)",
    "description": "Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API - Zen4-bit/Proxima",
    "fullText": "Zen4-bit\n\n /\n\n Proxima\n\n Public\n\n Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API\n\n License\n\n View license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Zen4-bit/Proxima",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Zen4-bit/Proxima",
    "thumbnail_url": "https://opengraph.githubassets.com/da1ce1c4daf052f563ec5615a17f7062e6ab435c084a9666f41483e54046eef5/Zen4-bit/Proxima",
    "created_at": "2026-02-17T12:37:44.260Z",
    "topic": "tech"
  },
  {
    "slug": "switch-instantly-between-your-ego-across-chatgpt-claude-gemini-grok-and-local",
    "title": "Switch instantly between your ego across ChatGPT, Claude, Gemini, Grok and local",
    "description": "모든 맥락을 한 곳에서 관리하세요. 복잡한 프롬프트, 자주 쓰는 답변, 프로젝트 컨텍스트를 카드로 정리하고 어디서든 즉시 꺼내 사용하세요.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://context-wallet.com/",
    "thumbnail_url": "/og-image.png",
    "created_at": "2026-02-15T06:38:28.915Z",
    "topic": "tech"
  },
  {
    "slug": "these-malicious-ai-assistants-in-chrome-are-stealing-user-credentials",
    "title": "These Malicious AI Assistants in Chrome Are Stealing User Credentials",
    "description": "Attackers are impersonating ChatGPT, Gemini, and Grok.",
    "fullText": "AI-powered browser extensions continue to be a popular vector for threat actors looking to harvest user information. Researchers at security firm LayerX have analyzed multiple campaigns in recent months involving malicious browser extensions, including the widespread GhostPoster scheme targeting Chrome, Firefox, and Edge. In the latest one—dubbed AiFrame—threat actors have pushed approximately 30 Chrome add-ons that impersonate well-known AI assistants, including Claude, ChatGPT, Gemini, Grok, and \"AI Gmail.\" Collectively, these fakes have more than 300,000 installs.\n\nThe Chrome extensions identified as part of AiFrame look like legitimate AI tools commonly used for summarizing, chat, writing, and Gmail assistance. But once installed, they grant attackers wide-ranging remote access to the user's browser. Some of the capabilities observed include voice recognition, pixel tracking, and email content readability. Researchers note that extensions are broadly capable of harvesting data and monitoring user behavior.\n\nThough the extensions analyzed by LayerX used a variety of names and branding, all 30 were found to have the same internal structure, logic, permissions, and backend infrastructure. Instead of implementing functionality locally on the user's device, they render a full-screen iframe that loads remote content as the extension's interface. This allows attackers to push changes silently at any time without a requiring Chrome Web Store update.\n\nLayerX has a complete list of the names and extension IDs to refer to. Because threat actors use familiar and/or generic branding, such as \"Gemini AI Sidebar\" and \"ChatGPT Translate,\" you may not be able to identify fakes at first glance. If you have an AI assistant installed in Chrome, go to chrome://extensions, toggle on Developer mode in the top-right corner, and search for the ID below the extension name. Remove any malicious add-ons and reset passwords.\n\nAs BleepingComputer reports, some of the malicious extensions have already been removed from the Chrome Web Store, but others remain. Several have received the \"Featured\" badge, adding to their legitimacy. Threat actors have also been able to quickly republish add-ons under new names using the existing infrastructure, so this campaign and others like it may persist. Always vet extensions carefully—don't just rely on a familiar name like ChatGPT—and note that even AI-powered add-ons from trusted sources can be highly invasive.",
    "readingTime": 2,
    "keywords": [
      "web store",
      "chrome web",
      "threat actors",
      "browser extensions",
      "add-ons",
      "layerx",
      "malicious",
      "ai-powered",
      "user",
      "researchers"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/malicious-ai-assistants-google-chrome?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHC36S2FS26DJCSZWGYRZHGX/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-14T01:08:43.318Z",
    "topic": "tech"
  },
  {
    "slug": "im-a-solo-founder-with-ai-agents-instead-of-employees-my-council-of-ai-agents-saves-me-20-hours-a-week",
    "title": "I'm a solo founder with AI agents instead of employees. My 'council' of AI agents saves me 20 hours a week.",
    "description": "A defense-tech founder built an AI \"council\" of 15 agents to help him run his company, using ChatGPT and Nvidia tools to replace traditional roles.",
    "fullText": "This as-told-to essay is based on a conversation with Aaron Sneed, a 40-year-old defense-tech solo founder based in Florida. The following has been edited for length and clarity.\n\nWhen I started my business, as a solopreneur, I realized I didn't have the money to pay lawyers, HR reps, and a bunch of other companies. So, using AI, I created what I call 'The Council.'\n\nThe council, which is compiled of all AI agents, helps me save around 20 hours a week, and that's a very conservative estimate. Every kind of general corporate chair, HR, legal, and finance AI agent has a seat on the council.\n\nHere's how I use around 15 custom agents, including a chief of staff agent, to manage my workload.\n\nI've worked on autonomous platforms that make decisions independently for at least 10 years. That made me latch onto commercial large language models and AI tools very quickly when they came out.\n\nI primarily use Nvidia's platform as my underlying hardware for technical prototyping and experimentation. I use their GPUs, and they provide free access to their AI software since I purchased their hardware. Additionally, my council is built on OpenAI's ChatGPT business platform using custom GPTs and projects.\n\nAltogether, my AI council consists of the following:\n\nMy chief of staff agent is important because it's the voice that sets priority based on parameters like risks, issues, and opportunities.\n\nI told my chief of staff which models have priority when making decisions. For example, anything legal, compliance, or security-related will be given a higher priority. So, I tell the chief of staff to listen to these models over everyone else.\n\nI don't want a bunch of yes agents. I trained them purposefully to give me pushback because I've learned that they naturally want to agree with me. I want them to test my theories to help me with what I'm trying to accomplish.\n\nI have a roundtable set up with all my AI agents, where I can put something like a request-for-proposal document in the chat, and all the agents will weigh in at the same time. I use this roundtable as a level of prevention for hallucinations and knowledge gaps.\n\nThe training never really stops, because if I don't continuously train the models, I won't get the outputs I want or need. It takes me about two weeks to train my agents to the level of experience they need to be at for me to feel confident in them. Early on, it took me longer to produce a deliverable than if I'd just done it myself because I hadn't focused properly on training.\n\nThe models have gotten better, and my prompting has, too. I have a better understanding of what information should be in an agent, like having a governance structure for priorities. I have a set of files that put those requirements in place to mitigate the risk of hallucination and false or bad information.\n\nAll of the AI companies have different prompt engineering guides. I recommend taking the time to look at them because there's a lot of user error that causes things to slow down when working with AI.\n\nIt takes time to get the agents to a good place. A lot of companies are going to try to jump into using AI too quickly for too much without understanding how to use it properly, and these companies could hurt themselves in the long run.\n\nI'm ill-equipped to handle a lot of these roles and responsibilities, but I'm also forced to do it because I'm bootstrapped.\n\nWith my legal agent in particular, I've learned the bounds of putting AI tools into real-world practice. I have a lawyer, and I use my legal agent to try to do some upfront work before handing documents off to my lawyer for a patent or dispute case, or anything like that.\n\nWhen I was training my model to help me use facts and data to plot a case, I had a lot of information laid out, and I thought what my legal agent created sounded good to me as a non-lawyer. Then I presented all that information to my lawyer, and he said that it was technically and factually correct, but we don't want to express that information because it shows our cards going in.\n\nHis legal skillset helped me realize that, even though I thought my agent was correct and ideal to use, it still won't replace a lawyer with that human context, experience, and skills.\n\nIdeally, I would have an HR person, a legal person, and so on, and each would have their own chief of staff AI agent who would help them out. That's what I think the future will look like.",
    "readingTime": 4,
    "keywords": [
      "i've learned",
      "legal agent",
      "staff agent",
      "agents",
      "chief",
      "models",
      "lawyer",
      "based",
      "priority",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solo-founder-runs-company-with-15-ai-agents-heres-how-2026-2",
    "thumbnail_url": "https://i.insider.com/698e5739d3c7faef0ece3468?width=1200&format=jpeg",
    "created_at": "2026-02-13T18:32:28.203Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-officially-killing-gpt4o-and-users-are-freaking-out-again-people-are-in-absolute-crisis",
    "title": "OpenAI is officially killing GPT-4o and users are freaking out (again): 'People are in absolute crisis'",
    "description": "OpenAI has retired its popular ChatGPT model GPT-4o, sparking another backlash from users who had developed an emotional attachment to the bot.",
    "fullText": "OpenAI said that today — once and for all — it is retiring GPT-4o. For some, it's like being dumped the day before Valentine's Day.\n\nMany ChatGPT users have a strong attachment to 4o, which is known for its sometimes sycophantic conversation style. Users who relied on the model as an emotional crutch and creative partner have described it as a \"vital accessibility aid.\"\n\nOpenAI first tried to deprecate the model in August, only to reverse its decision 24 hours later after an enormous backlash.\n\nThe company gave users another heads-up in January, but — as it turns out — time does not heal all wounds. The backlash is back.\n\n\"Rot in hell,\" one user wrote on X in response to OpenAI's announcement on Thursday.\n\n\"Are you going to cover our bereavement leave from work?\" another asked.\n\nFidji Simo, OpenAI's CEO of Applications, said earlier this week that these strong attachments to 4o marked the start of a new era — a time when users develop AI-based relationships.\n\n\"Humans are built to develop attachments to intelligent things,\" she told Alex Heath on his Access podcast. \"And AI is getting pretty intelligent.\"\n\nThose relationships can get ethically murky, however, when users are asking ChatGPT for advice like \"Should I leave my wife?\" Newer models, Simo said, have guardrails to prevent \"bad attachments.\" She said that newer models will tell users that it's \"not their place\" to tell them to stay married or not, and instead talk them through the pros and cons.\n\n\"We know that losing access to GPT‑4o will feel frustrating for some users, and we didn't make this decision lightly,\" OpenAI said in a blog post on Thursday announcing the retirement of 4o, as well as GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini. \"Retiring models is never easy, but it allows us to focus on improving the models most people use today.\"\n\nOpenAI added that only 0.1% of its users were still using 4o.\n\nOpenAI CEO Sam Altman has also acknowledged that users are especially attached to 4o's ingratiating responses.\n\n\"As we've been making those changes and talking to users about it, it's so sad to hear users say, 'Please can I have it back? I've never had anyone in my life be supportive of me. I never had a parent tell me I was doing a good job,\" Altman said on the \"Huge Conversations\" podcast in August after OpenAI first tried to kill 4o.\n\nAt the time, Altman said 4o's approach was \"too sycophant-y and annoying,\" and fixes were imminent.\n\nThe fix is in, but perhaps at a cost.\n\n\"People are in absolute crisis because the companion they've collaborated with for months is being wiped with ZERO recourse for the average user,\" an X user wrote on Thursday.",
    "readingTime": 3,
    "keywords": [
      "newer models",
      "users",
      "it's",
      "user",
      "attachments",
      "openai",
      "retiring",
      "august",
      "decision",
      "backlash"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-retires-gpt-4o-user-backlash-chatgpt-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/698e5cc3e1ba468a96abfca8?width=800&format=jpeg",
    "created_at": "2026-02-13T12:34:51.885Z",
    "topic": "finance"
  },
  {
    "slug": "browns-2026-record-according-to-chatgpt-total-ai-shocker",
    "title": "Browns 2026 record according to ChatGPT: Total AI shocker",
    "description": "After a 5-win season, ChatGPT’s 2026 win total projection for Cleveland is a number no one saw coming.",
    "fullText": "Browns 2026 record according to ChatGPT: Total AI shocker originally appeared on The Sporting News. Add The Sporting News as a Preferred Source by clicking here.\n\nIs the rebuild finally over? I ran a way-too-early 2026 simulation with ChatGPT to predict the Cleveland Browns record. Does new head coach Todd Monken’s offense take off? Are Myles Garrett and the defense as ferocious as a year ago? The win total result is a total shocker.\n\nAfter a grueling 5-12 campaign in 2025, the AI isn't just predicting a minor step forward…it’s forecasting a complete AFC North takeover. According to the simulation, the Browns are set to skyrocket to an 11-6 record, marking one of the biggest single-season turnarounds in franchise history.\n\nConsidered to have the “easiest” strength of schedule in the entire NFL, could the Dawg Pound finally be in store for an exciting, playoff bound season? It’s February, and this is the time to dream.\n\nChatGPT also broke down the wins and losses by game. Of course we won’t know the official weekly opponents until May, but in the meantime let’s take a look at how this Browns hypothetically magical season plays out.\n\nMore: Cleveland Browns 3-round 2026 NFL mock draft round-up: Surprise picks & bold predictions\n\nThe Simulation: 2026 Game-by-Game Results\n\nShedeur Sanders vs. Deshaun Watson: Who wins the Browns QB job in 2026?\n\nIf Not Malik Willis, Who? Browns need to eye this veteran QB\n\nDo the Browns have to draft offense in round 1? Latest mock changes everything\n\nBrowns 2026 offseason roadmap: 7 critical dates every fan needs to know\n\nMeet Browns new Special Teams Coordinator Byron Storer: 3 things to know",
    "readingTime": 2,
    "keywords": [
      "browns record",
      "simulation",
      "shocker",
      "sporting",
      "finally",
      "offense",
      "season",
      "round",
      "mock",
      "draft"
    ],
    "qualityScore": 0.85,
    "link": "https://sports.yahoo.com/articles/browns-2026-record-according-chatgpt-005742166.html",
    "thumbnail_url": "https://s.yimg.com/os/en/the_sporting_news_articles_584/b76838356356cd5f6f43b6445f347e1c",
    "created_at": "2026-02-13T01:16:06.226Z",
    "topic": "sports"
  },
  {
    "slug": "americas-cyber-defense-agency-is-burning-down-and-nobodys-coming-to-put-it-out",
    "title": "America's Cyber Defense Agency Is Burning Down and Nobody's Coming to Put It Out",
    "description": "CISA has lost a third of its workforce, has no confirmed director, and its acting leader uploaded sensitive documents to public ChatGPT. Meanwhile, China is pre-positioned inside U.S. critical infrastructure. Here is why this should terrify every American.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.threathunter.ai/blog/americas-cyber-defense-agency-burning-down/",
    "thumbnail_url": "https://www.threathunter.ai/og-image.png",
    "created_at": "2026-02-12T12:39:49.069Z",
    "topic": "tech"
  },
  {
    "slug": "openais-jony-ivedesigned-device-delayed-to-2027",
    "title": "OpenAI's Jony Ive-Designed Device Delayed to 2027",
    "description": "OpenAI's first Jony Ive-designed hardware device won't ship to customers until next year, new court filings show (via Wired). The motion stems from a trademark infringement lawsuit filed last year by audio device startup iyO. The company sued OpenAI after the latter acquired io, a startup founded by Apple's former design chief. OpenAI's original stated goal was to ship the ChatGPT-powered device before the end of 2026.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.macrumors.com/2026/02/10/openais-jony-ive-designed-device-delayed-to-2027/",
    "thumbnail_url": "https://images.macrumors.com/t/8JXgT8SwMdwVlTu6f-50sKRJFMk=/1600x/article-new/2025/05/jony-ive-sam-altman.jpg",
    "created_at": "2026-02-12T06:50:09.732Z",
    "topic": "tech"
  },
  {
    "slug": "a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
    "title": "A Ralph Loop for Reading: Beating GPT 5.2 with a 4k Context Window (and 4 GPUs)",
    "description": "Tired of AI’s high costs and limits? Discover how a home server and graph theory outperformed GPT-5.2 for deep research, saving $550/month while owning your ...",
    "fullText": "I recently had a problem that could be solved with money, which is the worst kind of problem.\n\nI am building a new venture called eh-trade.ca. To make it work, I needed deep financial research on 11,000 different stocks.\n\nThe \"Enterprise\" solution is to buy an API subscription. I looked into this. For my usage, the pricing is somewhere between \"$200/month\" and \"Contact Sales\". If you are a micro-preneur like me, \"Contact Sales\" means \"You cannot afford this.\"\n\nThe \"AI\" solution is to ask an LLM to research each stock, which works really well. But 11,000 requests at $0.05 per research session is still $550. Plus, I don't like renting intelligence. I prefer to own it.\n\nSo I decided to use the hardware I already had: a home server with four RTX 3090s. It’s a 96GB VRAM beast that heats my basement and scares my birds.\n\nThere was just one problem. The models I can run locally (like qwen3 or phi4) have small context windows. Yes, on the model card they theoretically support 40k+ of context, but they would run very slowly on my hardware, so really, it's 4K -- enough for a few screenfuls of text. Moreover, even if I enabled the long contexts, the models struggle to reason over them. If you try to feed them ten search results about a company's balance sheet, the 'needle in the haystack' effect kicks in. They'll get confused and start hallucinating dividends that don't exist.\n\nMost \"AI Agents\" are built on a simple loop, often called ReAct (Reason + Act). It looks like this:\n\nThe problem is the Context Window. By step 3, your prompt looks like a Walmart receipt. By step 5, you have exceeded 8,000 tokens, and the model forgets what it was doing and starts making stuff up.\n\nSure, it works fine if you are OpenAI and have infinite GPUs. It does not work if you are running on a consumer card in a closet.\n\nA few months ago, I read a paper called GraphReader. It proposed a different way to think about long contexts. Instead of dumping everything into a chat log, why not treat information as a graph?\n\nThe core insight is that you don't need to remember everything. You only need to remember the Atomic Facts.\n\nAn Atomic Fact is a single, indivisible truth.\n\nIf we extract these facts and throw away the rest, we can compress megabytes of web pages into a few kilobytes of JSON.\n\nI built a library called laconic to implement this. I wrote it in Go, because the rest of my projects are all in Go. Plus, my python environments always end up as a tangled mess of pip install errors.\n\nLaconic doesn't keep a chat history. It keeps a Notebook. The context window size is O(N), where N is the number of facts, not the number of words.\n\nLaconic uses a specific strategy called graph-reader.\n\nThe beautiful part? The LLM never sees the full history. It only sees the current Notebook and the current search result. This means I can run complex, multi-step research tasks on a model with a tiny 4k context window, and it never forgets a thing.\n\nHere is the code to run a research agent on my home server using Ollama:\n\nTo really test if this strategy works on constrained hardware, I ran a research question using qwen3:4b—a tiny 4-billion-parameter model.\n\nWithout the agentic loop, the model cannot answer this question. If you ask it directly, it responds that \"the 2024 Nobel Prize in Chemistry has not been announced.\"\n\nHowever, with the agent, it autonomously searches, extracts atomic facts, and synthesizes this:\n\nAnswer: The 2024 Nobel Prize in Chemistry was awarded to David Baker (University of Washington, Howard Hughes Medical Institute), Demis Hassabis, and John M. Jumper (Google DeepMind). David Baker was recognized for computational protein design. Demis Hassabis and John Jumper were awarded for protein structure prediction using AlphaFold2.\n\nThe agent found all three laureates, their exact affiliations, and their distinct contributions — information entirely outside the model's training data — by exploring multiple search queries and accumulating verified facts in a structured notebook.\n\nIf you hang out in the parts of the internet where people try to make AI write code without hallucinating, you might have heard of the Ralph Loop.\n\nPopularized by Geoffrey Huntley, the Ralph Loop (often named after Ralph Wiggum) is a brute-force solution to the context problem. You write a bash script that:\n\nThen it starts over. Fresh context. Zero memory leak. The \"memory\" is the file system.\n\nLaconic is essentially a Ralph Loop for reading.\n\nInstead of a bash script, it's a Go loop. Instead of git commit, we update a JSON Notebook. But the philosophy is identical: The Context Window is a liability.\n\nMost frameworks try to manage the context window like a precious resource. Ralph and Laconic treat it like a disposable napkin. Use it once, wipe the slate clean, and grab a fresh one.\n\nIt turns out that if you treat an LLM like a goldfish with a notepad, it becomes significantly smarter.\n\nI am currently running this loop on 11,000 tickers that I'm missing basic information on. It will take a week, and cost some power, but I would have left the machine on anyway because I am running a few other things on it.\n\nAnd if you want to find stocks that are going up, keep an eye on eh-trade.ca. My customers tell me I should brag more, so I'm up 280% in seven months following momentum strategies that it's showing on the main page.\n\nI'll have the data soon, assuming my basement doesn't catch fire.",
    "readingTime": 5,
    "keywords": [
      "david baker",
      "demis hassabis",
      "john jumper",
      "atomic facts",
      "bash script",
      "context window",
      "research",
      "model",
      "solution",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://stevehanov.ca/blog/a-ralph-loop-for-reading-beating-gpt-52-with-a-4k-context-window-and-4-gpus",
    "thumbnail_url": "https://stevehanov.ca/blog/images/3dc2de6fcca8955cef2716283bac11fac1f8d79f95905ef9778214cefc1d3dfa.png",
    "created_at": "2026-02-10T12:47:38.474Z",
    "topic": "tech"
  },
  {
    "slug": "these-4-stocks-are-set-to-get-a-boost-from-renewed-openai-hype-according-to-da-davidson",
    "title": "These 4 stocks are set to get a boost from renewed OpenAI hype, according to DA Davidson",
    "description": "DA Davidson analysts say a new wave of hype amid fresh funding for OpenAI could boost four stocks in the ChatGPT maker's orbit.",
    "fullText": "OpenAI's next funding round could be good news for four Big Tech stocks with links to the ChatGPT creator, DA Davidson says.\n\nOpenAI isn't publicly traded — at least not yet — but DA Davidson says a fresh wave of hype is coming on the heels of its next fundraising round, and it could could boost four stocks in its orbit.\n\n\"We have been very critical on OpenAI overpromising, asking for government handouts and being spread too thin,\" DA Davidson analysts, led by Gil Luria, wrote about the AI company.\n\n\"We believe OpenAI's behavior led to the underperformance in shares of NVDA, MSFT, CRWV and especially ORCL over the last 5 months.\"\n\nOpenAI is reportedly closing in on another fundraising round, with recent reports suggesting a $50 billion investment from Middle Eastern sovereign wealth funds is in the works.\n\nFuturum CEO and analyst Daniel Newman told Business Insider that they expect the next funding round will bring OpenAI's valuation to $750 billion to $850 billion.\n\nThe analyst described the appetite for OpenAI in his conversations with investors to be \"pretty insatiable.\"\n\nDA Davidson analysts wrote that they expect that yet another multibillion-dollar OpenAI funding round will reignite enthusiasm for the company and lift some connected AI stocks.\n\n\"Since the market is currently assigning the OpenAI relationship a negative value, we believe the fundraise will serve as a catalyst for outperformance,\" they wrote of Oracle, CoreWeave, Nvidia, and Microsoft.\n\nOpenAI has come under fire for its reliance on debt and growing competition challenging its dominant position.\n\nThe company is partnered with Oracle to build $500 billion of data centers as part of its Stargate project. OpenAI has yet to turn a profit.\n\nAnother concern weighing on OpenAI is the notion that Alphabet's Google Gemini has caught up in the AI race.\n\n\"We believe that a revamped OpenAI will return to its position as Google's top challenger and with a fresh stack of capital be able to live up to its obligations this year, including to Oracle,\" they said.\n\nThe analysts upgraded Oracle to Buy from Neutral on their expectation that new funding for OpenAI will be a positive catalyst for the stock as well as general sentiment about OpenAI and the connected players.\n\n\"We expect that as investors go back to seeing OpenAI as a winner, the public companies in its orbit could re-rate significantly. Most importantly that should drive outperformance in NVDA and MSFT, but also in CRWV, and even ORCL, which we are upgrading today based on our updated view on OpenAI,\"\n\nTheir more bullish stance on OpenAI comes after the company corrected course.\n\n\"OpenAI has started focusing on its core frontier model and ChatGPT, while de-emphasizing some marginals initiatives. This includes going down the path of turning on ads, which will be critical for increased monetization and reduced cash burn.\"\n\nThey also noted a shift in the company's management that suggests they see a need to align with Nvidia, Microsoft, and Amazon, rather than compete.",
    "readingTime": 3,
    "keywords": [
      "davidson analysts",
      "fundraising round",
      "funding round",
      "da davidson",
      "openai",
      "oracle",
      "openai's",
      "stocks",
      "another",
      "chatgpt"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-stock-picks-openai-nvidia-google-coreweave-microsoft-chatgpt-alphabet-2026-2",
    "thumbnail_url": "https://i.insider.com/698a14d8d3c7faef0ecde75d?width=1200&format=jpeg",
    "created_at": "2026-02-10T12:47:36.736Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-as-a-doctor-replacement-study-shows-sobering-results",
    "title": "ChatGPT as a doctor replacement? Study shows sobering results",
    "description": "AI language models excel in medical exams – but when real people ask them for advice, collaboration fails.",
    "fullText": "Large language models like GPT-4o are now achieving near-perfect results in medical knowledge tests. They pass the US medical licensing exam, summarize patient records, and can classify symptoms. Health authorities worldwide are therefore examining whether AI chatbots could serve as the first point of contact for patients – a kind of \"new gateway to the healthcare system,\" as stated in a strategy paper by the UK's NHS.\n\nHowever, the study \"Reliability of LLMs as medical assistants for the general public: a randomized preregistered study\" by researchers from the University of Oxford significantly dampens these hopes. The work is published in the journal Nature Medicine, and a pre-print version is available on arXiv. The central finding: the clinical knowledge of the models cannot be transferred to interactions with real people.\n\nFor the randomized, controlled study, the researchers recruited 1298 participants from Great Britain. Each subject was presented with one of ten everyday medical scenarios – such as sudden severe headaches, chest pain during pregnancy, or bloody diarrhea. The task: to assess what illness might be present and whether a doctor's visit, the emergency room, or even an ambulance was necessary.\n\nThe participants were randomly divided into four groups. Three groups had access to one AI model each, which was current at the start of the study – GPT-4o, Llama 3, or Command R+. The control group was allowed to use any aids, such as an internet search.\n\nThe results reveal a remarkable discrepancy. Without human involvement, even the language models, which are no longer current, identified at least one relevant illness in 94.9 percent of cases. When asked for the correct course of action – self-treatment, GP, emergency room, or ambulance – they were correct on average in 56.3 percent of cases.\n\nHowever, as soon as real people queried the models, the values plummeted. Participants with AI support recognized relevant illnesses in only a maximum of 34.5 percent of cases – significantly worse than the control group with 47 percent. When choosing the correct course of action, all groups performed equally: around 43 percent accuracy, regardless of whether a chatbot assisted or not.\n\nThe researchers analyzed the chat logs between users and AI models to understand the causes. They identified two central weaknesses: Firstly, participants often provided incomplete information to the models. Secondly, users did not correctly understand the AI's responses – even though the models named at least one correct diagnosis in 65 to 73 percent of cases, participants did not reliably adopt them.\n\nDr. Anne Reinhardt from LMU Munich sees a fundamental gap here: \"Many people quickly trust AI answers to health questions because they are easily accessible. They also sound very convincing linguistically – even when the content is actually medically completely wrong.\"\n\nThe researchers compared the performance of the models on the MedQA benchmark – a standard test with questions from medical exams – with the results of the user study. In 26 out of 30 cases, the models performed better on multiple-choice questions than in interactions with real people. Even benchmark values of over 80 percent sometimes corresponded to user results below 20 percent.\n\nProf. Ute Schmid from the University of Bamberg critically assesses the high performance of the models \"alone\": \"I find the statement that the performance of the language models is significantly higher 'alone' than with users somewhat misleading. In this case, the queries were likely formulated by individuals with expertise and experience with LLMs.\"\n\nThe experts agree that specialized medical chatbots would need to be designed differently from current all-purpose models. Prof. Kerstin Denecke from the Bern University of Applied Sciences outlines the requirements: \"A medically specialized chatbot would need to provide evidence-based, up-to-date information. Furthermore, it would need to reliably recognize emergencies, consider individual risk factors, and transparently communicate its limitations. It should conduct a structured anamnesis to reliably triage. And it should not be tempted to make a diagnosis.\"\n\nHowever, the hurdles for such use are considerable, according to Denecke: \"Major hurdles are, on the one hand, regulation – depending on the function as a medical device or high-risk AI. On the other hand, there is liability, data protection, and technical integration into care processes.\"\n\nThe conclusion of the Oxford researchers is clear: Before AI systems are deployed in healthcare, they must be tested with real users – not just with exam questions or simulated conversations. Schmid advocates for a differentiated approach: \"Quality-assured chatbots could, for example, be offered through statutory health insurance funds and recommended by general practitioners' offices as a first point of access. However, people should not be forced to use these services.\"\n\nDon't miss any news – Facebook,\n LinkedIn or\n Mastodon.\n\nThis article was originally published in\n\n German.\n\n It was translated with technical assistance and editorially reviewed before publication.",
    "readingTime": 4,
    "keywords": [
      "emergency room",
      "correct course",
      "language models",
      "medical",
      "study",
      "researchers",
      "participants",
      "cases",
      "users",
      "health"
    ],
    "qualityScore": 1,
    "link": "https://www.heise.de/en/news/ChatGPT-as-a-doctor-replacement-Study-shows-sobering-results-11170652.html",
    "thumbnail_url": "https://heise.cloudimg.io/bound/1200x1200/q85.png-lossy-85.webp-lossy-85.foil1/_www-heise-de_/imgs/18/5/0/2/4/7/8/5/shutterstock_2635549697-b9c178216cc5725a.jpg",
    "created_at": "2026-02-10T01:21:46.923Z",
    "topic": "science"
  },
  {
    "slug": "when-i-was-a-student-at-stanford-many-of-my-classmates-used-chatgpt-i-refused",
    "title": "When I was a student at Stanford, many of my classmates used ChatGPT. I refused.",
    "description": "Many of my fellow classmates at Stanford used ChatGPT to complete assignments. I decided not to use the AI tool because I wanted a real education.",
    "fullText": "It wasn't until my junior year at Stanford University that I first heard about ChatGPT from classmates who'd mentioned they had \"chat\" summarize the class reading.\n\nWhen I asked them what they meant by \"chat,\" they told me all about the AI tool, chatGPT.\n\nI was an English major with a creative writing emphasis during my time at Stanford, so reading and writing were important to my work.\n\nI wanted to ask the students who used ChatGPT to do the reading: Why are you even at Stanford? Why be at any university at all? Isn't the point of pursuing higher education to expand our minds and become better communicators?\n\nAs I progressed through my studies, I would hear ChatGPT mentioned more and more often, with some college professors even adding an AI clause to their syllabi. Some instructors would allow students to use ChatGPT for assistance (a murky definition) while others wouldn't allow it at all.\n\nI refused to use the AI tool in any way.\n\nI didn't care if instructors allowed students to use ChatGPT. I deliberately chose not to use it at all.\n\nSure, studying and studying literature in particular was difficult, but this was why I'd chosen the major; it was challenging, and I wanted to become better with words.\n\nI chose to become an English major because I wanted to improve my writing. I lacked confidence in my writing because I didn't know how to use words when I first entered college. I didn't have that finesse.\n\nApart from wanting to become a better writer, I also wanted to defend my opinions and thoughts with flair. I wanted to articulate sentiment precisely and formulate strong arguments. I wanted to write strongly worded emails. I wanted to think and speak freely.\n\nI wouldn't have been able to achieve any of that if I used ChatGPT.\n\nWhenever I'd hear a peer say they just used ChatGPT for their homework assignments, I'd often think about the many deserving students who hadn't been offered a seat at the university — students who, in a heartbeat, would've happily done these assignments themselves.\n\nI had the privilege of studying words at a strong English literature department with so many fine writing instructors. I leaped at opportunities to have my very own work critiqued by great writers. I still can't fathom having \"my\" writing, either partially or entirely synthetic, reviewed by instructors.\n\nAfter all, I was writing in and walking about the same grounds that writing legends once did — writers like bell hooks, John Steinbeck, Karen Zacarías, and David Henry Hwang. With this writing pedigree in mind, I was inspired and motivated to write myself.\n\nNot a day goes by without me putting my English degree to good use. Not a day goes by without me being grateful to my past self for putting in the work to earn the degree herself — without ChatGPT.",
    "readingTime": 3,
    "keywords": [
      "students",
      "english",
      "instructors",
      "reading",
      "didn't",
      "studying",
      "without",
      "chatgpt",
      "tool",
      "college"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/stanford-students-always-used-chatgpt-refuse-2026-2",
    "thumbnail_url": "https://i.insider.com/698a2383d3c7faef0ecde92e?width=1200&format=jpeg",
    "created_at": "2026-02-10T01:21:39.281Z",
    "topic": "finance"
  },
  {
    "slug": "openais-sam-altman-used-to-hate-ads-now-hes-selling-them",
    "title": "OpenAI's Sam Altman used to hate ads. Now he's selling them.",
    "description": "A ton of people use ChatGPT but don't pay to use it. Now those free users are about to become revenue-generating users.",
    "fullText": "OpenAI's Sam Altman used to think ads were gross.\n\nOpenAI has formally announced that it will start showing ads to some US users, starting Monday. The ads will show up for some of ChatGPT's free users, as well as some users who've subscribed to ChatGPT Go, a new $8-a-month tier the company rolled out last month.\n\nOpenAI says it will serve ads to you based on the conversation you're currently having with ChatGPT, as well as previous queries and chats. It will also factor in whether you've engaged with — or hidden —other ads it has shown you.\n\nFor now, the company says, it won't use data about what you do outside of ChatGPT to target ads inside the service. But I'd be surprised if they don't do that eventually, since just about every other big internet ad platform uses those signals.\n\nThe fact that ChatGPT is launching ads isn't news: The company has been circling the idea for months, and last month it formally announced that ads would be coming to the service.\n\nAnd that news became the subject of a back-and-forth between OpenAI and rival Anthropic, which included a testy social media post from Altman and a cheeky Super Bowl ad from Anthropic underlining the possible trust issues OpenAI may have once ads intermingle with \"organic\" results on the service.\n\nNow we'll see how these things actually look, and work — and what users and advertisers think of them.",
    "readingTime": 2,
    "keywords": [
      "users",
      "service",
      "formally",
      "anthropic",
      "openai",
      "chatgpt",
      "altman"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/chatgpt-ads-us-privacy-data-sam-altman-2026-2",
    "thumbnail_url": "https://i.insider.com/698a2829e1ba468a96abb15d?width=1200&format=jpeg",
    "created_at": "2026-02-10T01:21:39.280Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-opt-out-of-ads-on-chatgpt-but-it-might-not-be-worth-it",
    "title": "You Can Opt Out of Ads on ChatGPT, but It Might Not Be Worth It",
    "description": "You don't have to see ads in ChatGPT, but you'll get less ChatGPT in return.",
    "fullText": "It finally happened. After months of speculation, ChatGPT officially has ads. OpenAI revealed the news on Monday, announcing that ads would roll out in testing for logged-in adult users on Free and Go subscriptions. If you or your organization pays for ChatGPT, such as with a Plus, Pro, Business, Enterprise, or Education account, you won't see ads with the bot.\n\nOpenAI says that ads do not have an impact on the answers ChatGPT generates, and that these posts are always clearly separated from ChatGPT's actual responses. In addition, ads are labeled as \"Sponsored.\" That being said, it's not exactly a church-and-state situation here. OpenAI says that it decides which ads to show you based on your current and past chats, as well as your past interactions with ChatGPT ads. If you're asking for help with a dinner recipe, you might get an ad for a meal kit or grocery service.\n\nThe company claims it keeps your chats away from advertisers. The idea, according to the company, is strictly funding-based so that OpenAI can expand ChatGPT access to more users. That's reportedly why ads are starting as a test, not a hardcoded feature: OpenAI says it wants to \"learn, listen, and make sure [it gets] the experience right.\" As such, advertisers don't have access to chats, chat histories, memories, or your personal details. They do have access to aggregate information about ad performance, including views and click metrics.\n\nOpenAI will only show ads to adults. If the service detects that you are under 18, it will block ads from populating in your chats. Ads also will not appear if you're talking to ChatGPT about something related to health, medicine, or politics. You can offer OpenAI feedback on the ads you do see, which should inform the ads you receive in the future. You can also delete your ad data and manage ad personalization, if you want to reset the information OpenAI is using to send you ads.\n\nThe thing is, you don't actually have to deal with ads, even if you use ChatGPT for free. That's not just by upgrading to a paid ChatGPT plan, though OpenAI does suggest that option in its announcement. In addition, OpenAI is offering Free and Go users a dedicated choice to opt out of ads here. There is, of course, a pretty sizable catch: You have to agree to fewer daily free messages with ChatGPT. OpenAI doesn't offer specifics here, so it's not clear how limited the ad-free experience will be. But if you hate ads, or if you simply don't want to see an ad for something irrelevant to your ChatGPT conversation, it's an option.\n\nIf you like that trade-off, here's how to opt out of ads. Open ChatGPT, then head to your profile, which opens your profile's Settings page. Here, scroll down to \"Ads controls,\" then choose \"Change plan to go ad-free.\" Select \"Reduce message limits,\" and ChatGPT will confirm ads are off for your account. You can return to this page at any time to turn ads back on and restore your message limits.\n\nDisclosure: Ziff Davis, Mashable’s parent company, in April 2025 filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
    "readingTime": 3,
    "keywords": [
      "message limits",
      "free and go",
      "chats",
      "chatgpt",
      "openai",
      "users",
      "it's",
      "access",
      "don't",
      "account"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ads-on-chatgpt-free?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KH26KJW3KJD43H7MWGTR35FH/hero-image.fill.size_1200x675.webp",
    "created_at": "2026-02-10T01:21:37.672Z",
    "topic": "tech"
  },
  {
    "slug": "we-asked-13-risingstar-vcs-how-they-use-ai-heres-what-they-told-us",
    "title": "We asked 13 rising-star VCs how they use AI. Here's what they told us.",
    "description": "For up-and-coming venture capitalists, tools like ChatGPT and NotebookLM have become a core part of the job.",
    "fullText": "These days, plenty of venture capitalists are trying to spot the next big thing in artificial intelligence. For some, the best way to do that is to use the technology themselves.\n\nFor a generation raised on Google Docs and Slack, tools like ChatGPT and Google's NotebookLM have become a core part of the job. They offer ways to learn new markets faster, spot non-obvious companies, stress-test investment theses, and turn a swamp of meetings into searchable insights.\n\nBusiness Insider asked its 2026 Rising Stars of Venture Capital honorees how they use AI in their day-to-day work — and which tools they rely on most.\n\nThe following has been edited for length and clarity.\n\nMiloni Madan Presler, partner at IVP:\n\nShruti Kumar, vice president at Tusk Ventures:\n\nSophie Beshar, vice president at Insight Partners:\n\nMeera Oak, partner at Alumni Ventures:\n\nAngèle Sahraoui, investor at Slow Ventures:\n\nManmeet Gujral, vice president at CapitalG:\n\nLexi Henkel, managing director at Maverick Ventures\n\nJames Flynn, partner at Sequoia Capital:\n\nAdil Bhatia, vice president at Redpoint Ventures:\n\nMax Abram, partner at Scale Venture Partners:\n\nSudhee Chilappagari, principal at Battery Ventures:\n\nGloria Zhang, vice president at DCM Ventures:\n\nChristine Esserman, partner at Accel:\n\nJulia Hornstein contributed to this report.",
    "readingTime": 2,
    "keywords": [
      "vice president",
      "ventures",
      "spot",
      "tools",
      "partner",
      "venture",
      "capital",
      "partners"
    ],
    "qualityScore": 0.75,
    "link": "https://www.businessinsider.com/how-venture-capital-investors-use-ai-chatgpt-notebooklm-2026-2",
    "thumbnail_url": "https://i.insider.com/6984eeabd3c7faef0ecdb608?width=1200&format=jpeg",
    "created_at": "2026-02-07T12:25:49.242Z",
    "topic": "finance"
  },
  {
    "slug": "i-used-chatgpt-gemini-and-perplexity-to-research-a-gnarly-nyc-rent-situation-heres-what-i-learned",
    "title": "I used ChatGPT, Gemini, and Perplexity to research a gnarly NYC rent situation. Here's what I learned.",
    "description": "AI tools can help interpret complex documents and help people understand their rights. You still have to check in with a human expert, though.",
    "fullText": "In New York City, three things are certain: death, taxes, and sky-high rent.\n\nThere may not be much AI can do for death. Taxes, I'm not sure. But it turns out this technology can be surprisingly helpful if you have a unique rent situation.\n\nI live in a former superintendent-occupied apartment in a prewar co-op in Manhattan. Although I signed a market-rate lease when I moved in, I later learned — with the help of AI tools — that my apartment might qualify as a \"rent-stabilized\" unit.\n\nNew York is one of a handful of cities where rent increases can be limited by law, depending on the circumstances. This is known as rent stabilization. It's a milder form of rent regulation that lets landlords charge more, but not too much more.\n\nMy apartment's rent history showed the unit had been registered as rent-stabilized in the 1980s and contained no obvious record of a deregulation event. Did that mean the apartment might still be regulated? Could I gain protection from potentially large rent increases in the future?\n\nI couldn't find clear answers on Reddit or Google, so I uploaded a photo of the apartment's rent-registration history and asked ChatGPT to help me interpret it.\n\nChatGPT walked me through the document and said this type of situation is routinely evaluated by the New York State Division of Housing and Community Renewal, which oversees rent-stabilization and related rules in NYC.\n\nI told ChatGPT to show me its sources. It cited relevant provisions of the rent-stabilization code and several court decisions. AI can sometimes hallucinate, so I asked Perplexity and Google Gemini the same questions, as an initial way to double-check the facts. These AI tools reached similar high-level conclusions but differed in their reasoning and the way they cited authoritative sources. Gemini and Perplexity were better at showing their work than ChatGPT, and Gemini tended to be the most conservative in its framing, which made me trust it more.\n\nI still caught all three bots making mistakes. In one instance, Gemini cited a legal case that didn't exist. When I called it out, it quickly corrected itself. I repeated this process multiple times, reading the underlying legal provisions and pushing back on AI-powered conclusions I thought were wrong. I even role-played as a landlord, presenting arguments rental property owners might make about why my apartment is not rent-stabilized.\n\nEventually, I hit a wall. No matter how much I challenged the bots, they were confident that my situation raised legitimate questions that New York's DHCR might resolve. That's when I called a housing attorney — which cost $35 through the New York Bar Association's referral service — to sanity-check everything. After reviewing the documents, he agreed that filing a rent-overcharge complaint was reasonable. So I went ahead. The proceeding is ongoing, and no determination has been made yet.\n\nMy case is pretty unusual. But there are broader takeaways:\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "death taxes",
      "rent increases",
      "apartment",
      "situation",
      "rent-stabilized",
      "cited",
      "tools",
      "unit",
      "apartment's",
      "history"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-gemini-perplexity-research-nyc-rent-stabilization-2026-2",
    "thumbnail_url": "https://i.insider.com/69850cfad3c7faef0ecdbb1c?width=1200&format=jpeg",
    "created_at": "2026-02-06T18:34:26.393Z",
    "topic": "science"
  },
  {
    "slug": "i-tried-malwarebytes-chatgpt-app-and-its-actually-good-at-detecting-scams",
    "title": "I Tried Malwarebytes' ChatGPT App, and It's Actually Good at Detecting Scams",
    "description": "The Malwarebytes Chat GPT plugin can give you instant advice on suspicious links, emails, and texts.",
    "fullText": "A few months ago, ChatGPT got an app store of its own, which means you can access tools like Photoshop and Apple Music right inside the ChatGPT prompt box. Now Malwarebytes has joined the ChatGPT app store, which means you can get some expert help when investigating web links, emails, text messages, domains, and phone numbers you think might be suspicious.\n\nThe app is free to use for everyone, whether or not they're signed up to a paid ChatGPT subscription, and you can enable the tool via the ChatGPT app store or by entering the prompt \"Malwarebytes, is this a scam?\" Once you've used the app for the first time, you can access it again via the + (plus) button on the prompt box.\n\nYou can paste just about anything you like into a conversation with the Malwarebytes plugin, but there are certain approaches that scammers will often take—including links contained in phishing emails—that make for good candidates to test this thing out. I dived deep into my email spam folder to find some URLs to test Malwarebytes on, and gave it a few trustworthy web addresses as well—you just copy the link into the prompt box and ask the app for an assessment.\n\nMalwarebytes successfully sifted out the scam links from the safe ones, even when it didn't have any specific information in its databases about the links I was providing. When it was unsure, it said so, with lots of extra context: For example, for one URL I was told the address was \"a legitimate email security and tracking service used by companies to rewrite links\" but one that scammers also used to conceal the link destination.\n\nYou also get an assessment of the domain name: When given a link to a Lifehacker article, the plugin correctly identified that it was a legitimate domain with a registered owner, even though it didn't have any specific information about the URL. Malwarebytes was also able to spot domain redirecting, a trick frequently used by scammers.\n\nPhone numbers can be given to Malwarebytes as well: When I tested this out with a few scam calls I've had, these numbers were correctly identified as coming from scammers or at least being suspicious. I like the way the app gives you some context to its thinking (explaining how spam call centers work, for example), and will also offer up advice about next steps and how to stay safe.\n\nSomething else I appreciated was that the Malwarebytes app has a memory inside ChatGPT: If you post a series of links and numbers in the same chat thread, as I did, then it will try and put them all in context (explaining why one URL is potentially more dangerous than another, for example).\n\nYou can also give the Malwarebytes app some text you've come across in an email or text message and get a verdict on this too—you can even type in a transcript of a conversation you're having on the phone, if you want. The plugin will scan the text for phrasing that scammers often use and will alert you of any other red flags.\n\nI tried this out with a variety of spammy text, and again Malwarebytes scored highly in terms of recognizing anything dodgy. As before, if it came across something it wasn't sure about, it would explain the reasons why and suggest some next steps.\n\nThe responses also include some detail on why different scam approaches are taken and why they sometimes work, and how they might escalate—so if you get a message purporting to be from a family member asking for help, Malwarebytes tells you why these scams are common and how they're used to steal identities or money.\n\nIt's an intelligent system, in that it'll ask you questions about the texts or emails you've received: If it's not sure about something, you'll be told about extra checks you can run (like looking at the \"reply to\" address on an email). However, the usual ChatGPT sycophancy does start to grate a bit, as you're constantly told that you're doing the right thing and that you're right to be suspicious.\n\nThe app taps into Malwarebytes Threat Intelligence, so it should be able to keep you protected against the latest threats (making it more helpful than a Google search or just a regular ChatGPT query). From the examples I used at least, it comes across as a security tool that's accurate, comprehensive, and easy to use—one that's well worth keeping close at hand if you come across potential scams you're not sure about.\n\nDisclosure: Ziff Davis, Mashable’s parent company, in April 2025 filed a lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
    "readingTime": 4,
    "keywords": [
      "correctly identified",
      "prompt box",
      "phone numbers",
      "chatgpt app",
      "app store",
      "malwarebytes app",
      "links",
      "text",
      "scammers",
      "you're"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/malwarebytes-chatgpt-app-impressions?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGSJ8K8YYSPM82BZDG46DYXW/hero-image.fill.size_1200x675.png",
    "created_at": "2026-02-06T18:34:25.082Z",
    "topic": "tech"
  },
  {
    "slug": "the-recent-struggles-of-top-ai-stocks-show-investors-are-realizing-they-were-sold-a-bill-of-goods-ai-scientist-says",
    "title": "The recent struggles of top AI stocks show investors are realizing they were 'sold a bill of goods,' AI scientist says",
    "description": "AI scientist Gary Marcus thinks the underwhelming release of ChatGPT-5 marked a turning point when investors realized AI might not be a game changer.",
    "fullText": "Many high-flying tech stocks are in a rut, with some plateauing for months and others down sharply.\n\nGary Marcus said the underwhelming release of ChatGPT-5 marked a turning point in the AI discourse.\n\nHe says investors are waking up to the idea that AI might not be as game-changing as originally thought.\n\nWith names like Nvidia, Oracle, and CoreWeave down double digits in the last few months, the AI trade appears to be stuck in a rut.\n\nBut far from a temporary blip, the diminished fortunes of the market's tech leaders might represent a much more dire reassessment of the technology among investors, a top AI researcher says.\n\nThe latest volatility began with Anthropic's debut of its new plugins for its Claude AI agent, which sent legal tech stocks into a nosedive that quickly spurred volatility into the broader sector. Yet, many leading AI names have been down for months, with Nvidia losing 13% since November and Oracle down 43%.\n\nAI scientist and former Uber AI chief, Gary Marcus, said the moves show investors are waking up to reality. Marcus has been skeptical of AI's big ambitions, and his latest post reveals he sees a clear recent turning point in the AI discourse.\n\n\"Investors have been 'rotating out of tech stocks' — because they realize they were sold a bill of goods,\" he wrote. \"My guess is that these stocks — and the reputation of OpenAI — will fall further, but either way it is already clear that the rockets will not reach the altitude so many people were hoping for.\"\n\nIn Marcus's opinion, the debut of ChatGPT-5 was an underwhelming event that left investors wondering if the promises of AI might be overblown. Marcus highlighted Sam Altman's statement in early 2025 that OpenAI knew how to build AGI as one such promise that appears false given current capabilities.\n\n\"That fateful ChatGPT-5 introduction day last August — this Saturday will be the half anniversary — was the day people woke up to the reality that ChatGPT is not magic.\"\n\nIn Marcus's view, the rotation away from tech stocks is a clear indication that investors are beginning to understand that the AI boom is waning. He also said that investors seem to be moving away from tech stocks for two reasons that are almost contradictory.\n\n\"Some may be leaving companies like Nvidia out of concerns about circular financing and profitability of LLM companies, while others are leaving traditional companies like Salesforce because they're worried about companies like Anthropic replacing traditional software,\" he noted.",
    "readingTime": 3,
    "keywords": [
      "tech stocks",
      "gary marcus",
      "in marcus's",
      "investors",
      "chatgpt",
      "others",
      "underwhelming",
      "discourse",
      "waking",
      "latest"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/recent-struggles-top-ai-stocks-154234199.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/4z4b.l0JO4nsQnHhxbvONQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA-/https://media.zenfs.com/en/business_insider_consolidated_articles_886/201ae18423fdb9e630d28f1b37964f6c",
    "created_at": "2026-02-06T12:35:37.677Z",
    "topic": "finance"
  },
  {
    "slug": "anthropic-and-openai-release-dueling-ai-models-on-the-same-day-in-an-escalating-rivalry",
    "title": "Anthropic and OpenAI release dueling AI models on the same day in an escalating rivalry",
    "description": "The rivaling AI companies pushed GPT-5.3-Codex and Claude Opus 4.6, and had back-to-back appearances on a tech and business podcast.",
    "fullText": "The rivalry between OpenAI and Anthropic intensified this week.\n\nThe two companies released dueling new AI models on Thursday and had back-to-back podcast appearances on \"TBPN.\"\n\nOn Thursday, Anthropic unveiled Claude Opus 4.6, an upgraded model that the company says would improve performance on office productivity and coding tasks, with an expanded \"context window\" that allows it to work through longer documents and more complex projects in a single session.\n\nMeanwhile, OpenAI punched back with its own new coding-focused model called GPT-5.3-Codex, which the company says runs faster, uses fewer computing resources, and can generate and manage complex software from English instructions. The new version also comes alongside a stand-alone Codex desktop app.\n\nBoth Sam Altman, the OpenAI CEO, and Sholto Douglas, one of Anthropic's leading researchers, appeared on the \"TBPN\" podcast in back-to-back chats with show host John Coogan and Jordi Hays.\n\n\"I think we will be heading towards a workflow where a lot of people just feel like they're managing a team of agents,\" said Altman. \"And as the agents get better, they'll keep operating at a higher and higher level of abstraction.\"\n\nDouglas, who appeared in the subsequent timeslot, told Coogan and Hays that users have been comparing previous Anthropic and OpenAI models, and they have noticed some key differences.\n\n\"The OpenAI models were a bit better at trying really, really, really hard on tough problems, but the Anthropic models were much faster and so forth,\" Douglas said.\n\n\"And so they worked on speed while we worked on making the models much, much better at really, really tough problems,\" Douglas added of the Opus 4.6.\n\nThe latest release is part of a long-running competition between Anthropic and OpenAI, dating back to 2021, when a group of OpenAI researchers left to form Anthropic, aiming to develop safer and more controlled AI systems.\n\nThis week, Anthropic's launch of industry-specific plugins triggered a stock market sell-off as Wall Street worried about AI's impact on software.\n\nAnthropic also took a subtle shot at OpenAI with a series of ads released this week, including one that will air during the Super Bowl.\n\nThe ads feature unnamed humanized AIs dropping ads in the middle of their advice, alongside the promise that its model, Claude, will remain ad-free.\n\nOpenAI announced in January that ads are coming to ChatGPT for users of the free version.\n\nAltman subsequently hit back, calling Anthropic \"dishonest\" and defending ChatGPT as a product that brings AI \"to billions of people who can't pay for subscriptions.\" He also clarified that the ads will be \"clearly labeled\" to differentiate themselves from the chatbot's answers to queries.\n\n\"We are not stupid. We respect our users. We understand that if we did something like what those ads depict, people would rightfully stop using the product,\" Altman told the \"TBPN\" podcast on Thursday.\n\n\"Our first principle with ads is that we're not going to put stuff into the LLM stream,\" Altman added. \"That would feel crazy dystopic, like a bad sci-fi movie.\"",
    "readingTime": 3,
    "keywords": [
      "tbpn podcast",
      "openai models",
      "anthropic and openai",
      "douglas",
      "back",
      "users",
      "released",
      "back-to-back",
      "complex",
      "faster"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-openai-rivalry-dueling-ai-models-on-the-same-day-2026-2",
    "thumbnail_url": "https://i.insider.com/698531b1d3c7faef0ecdbddd?width=1200&format=jpeg",
    "created_at": "2026-02-06T06:40:12.768Z",
    "topic": "finance"
  },
  {
    "slug": "the-recent-struggles-of-top-ai-stocks-show-investors-are-realizing-they-were-sold-a-bill-of-goods-ai-scientist-says",
    "title": "The recent struggles of top AI stocks show investors are realizing they were 'sold a bill of goods,' AI scientist says",
    "description": "AI scientist Gary Marcus thinks the underwhelming release of ChatGPT-5 marked a turning point when investors realized AI might not be a game changer.",
    "fullText": "With names like Nvidia, Oracle, and CoreWeave down double digits in the last few months, the AI trade appears to be stuck in a rut.\n\nBut far from a temporary blip, the diminished fortunes of the market's tech leaders might represent a much more dire reassessment of the technology among investors, a top AI researcher says.\n\nThe latest volatility began with Anthropic's debut of its new plugins for its Claude AI agent, which sent legal tech stocks into a nosedive that quickly spurred volatility into the broader sector. Yet, many leading AI names have been down for months, with Nvidia losing 13% since November and Oracle down 43%.\n\nAI scientist and former Uber AI chief, Gary Marcus, said the moves show investors are waking up to reality. Marcus has been skeptical of AI's big ambitions, and his latest post reveals he sees a clear recent turning point in the AI discourse.\n\n\"Investors have been 'rotating out of tech stocks' — because they realize they were sold a bill of goods,\" he wrote. \"My guess is that these stocks — and the reputation of OpenAI — will fall further, but either way it is already clear that the rockets will not reach the altitude so many people were hoping for.\"\n\nIn Marcus's opinion, the debut of ChatGPT-5 was an underwhelming event that left investors wondering if the promises of AI might be overblown. Marcus highlighted Sam Altman's statement in early 2025 that OpenAI knew how to build AGI as one such promise that appears false given current capabilities.\n\n\"That fateful ChatGPT-5 introduction day last August — this Saturday will be the half anniversary — was the day people woke up to the reality that ChatGPT is not magic.\"\n\nIn Marcus's view, the rotation away from tech stocks is a clear indication that investors are beginning to understand that the AI boom is waning. He also said that investors seem to be moving away from tech stocks for two reasons that are almost contradictory.\n\n\"Some may be leaving companies like Nvidia out of concerns about circular financing and profitability of LLM companies, while others are leaving traditional companies like Salesforce because they're worried about companies like Anthropic replacing traditional software,\" he noted.\n\nEven if the software industry survives the disruption from AI innovation, Marcus still sees an industry that's become unstable. He flagged circular funding deals that others like \"The Big Short\" trader Michael Burry have also expressed concerns about.\n\n\"That circularity is a warning sign, reflecting a market that is propped up rather than functioning well on its own accord,\" Marcus wrote. \"Wall Street lost confidence, particularly after the crazy Oracle deal in September that I called at the time \"peak bubble\".\n\nThe tech sell-off continued on Thursday, with Google parent Alphabet down sharply after earnings revealed more big capex plans. Software stocks also continued to struggle. The iShares Expanded Tech-Software Sector ETF was down another 3%.",
    "readingTime": 3,
    "keywords": [
      "tech stocks",
      "in marcus's",
      "investors",
      "chatgpt",
      "latest",
      "volatility",
      "debut",
      "reality",
      "openai",
      "away"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/tech-stock-crash-ai-chatgpt5-gary-marcus-nvda-orcl-crwv-2026-2",
    "thumbnail_url": "https://i.insider.com/6984b045a645d1188188be3b?width=1200&format=jpeg",
    "created_at": "2026-02-05T18:35:58.418Z",
    "topic": "finance"
  },
  {
    "slug": "what-does-the-disappearance-of-a-100bn-deal-mean-for-the-ai-economy",
    "title": "What does the disappearance of a $100bn deal mean for the AI economy?",
    "description": "Apparent collapse of Nvidia–OpenAI tie-up raises questions about circular funding and who will bear the cost of AI’s expansion\nDid the circular AI economy just wobble? Last week it was reported that a much-discussed $100bn deal – announced last September – between Nvidia and OpenAI might not be happening at all.\nThis was a circular arrangement through which the chipmaker would supply the ChatGPT developer with huge sums of money that would largely go towards the purchase of its own chips.\n Continue reading...",
    "fullText": "Apparent collapse of Nvidia–OpenAI tie-up raises questions about circular funding and who will bear the cost of AI’s expansion\n\nDid the circular AI economy just wobble? Last week it was reported that a much-discussed $100bn deal – announced last September – between Nvidia and OpenAI might not be happening at all.\n\nThis was a circular arrangement through which the chipmaker would supply the ChatGPT developer with huge sums of money that would largely go towards the purchase of its own chips.\n\nIt is this type of deal that has alarmed some market watchers, who detect a whiff of the 1999-2000 dotcom bubble in these transactions.\n\nNow it seems that Nvidia was not as solid on this investment as had been widely believed, according to the Wall Street Journal. Negotiations had not progressed, with Jensen Huang, Nvidia’s chief executive, privately emphasising that the deal was “non-binding” and “not finalised”. Huang appeared to confirm this in Taipei on Saturday, telling reporters that Nvidia would make a “huge” investment into OpenAI’s next funding round, but “nothing like” $100bn.\n\nA report from Reuters soon suggested that the feeling was mutual: OpenAI was “unsatisfied” with Nvidia’s advanced AI chips, it said, and seeking alternatives. Nvidia’s stock has taken a 10% hit so far this week, a flurry of headlines have ensued and both companies have stepped into damage control.\n\n“We love working with Nvidia and they make the best AI chips in the world,” wrote Sam Altman, OpenAI’s CEO, on X. “We hope to be a gigantic customer for a very long time.”\n\nEven Oracle appears to be shaken: the software company, which is counting on a $300bn cloud computing deal with OpenAI, said it still expects the startup to be good for its commitment even if it does not receive the full amount from Nvidia. In total, OpenAI has committed to compute deals – the infrastructure for building and powering its AI tools – worth more than $1tn.\n\n“The Nvidia-OpenAI deal has zero impact on our financial relationship with OpenAI,” Oracle posted on X. “We remain highly confident in OpenAI’s ability to raise funds and meet its commitments.”\n\nThat a $100bn deal between two of the most crucial players in AI appears to have evaporated over a weekend is unsettling. But there are solid business reasons behind the apparent shake-up, said Alvin Nguyen, analyst at research firm Forrester.\n\nOpenAI’s ambitious growth trajectory means it will be difficult for the company to stick with a single vendor, especially as it plans new, computationally demanding AI models, he said. “They need chips. They need as many as possible.”\n\nAs for Nvidia, its commitment to the $100bn may have been loose in the first place, even as it was widely reported. “They will not discourage people from overhyping. Why say something and immediately sucker punch your own share price?”\n\nFor a giant startup like OpenAI, manoeuvring in and out of deals – for example, with chipmakers – may just be business as usual, said Nguyen: “You know [Altman’s] background as a startup person, and you know the manoeuvres he’s doing make sense from a startup perspective.”\n\nFor Nvidia, meanwhile, AI hype is part of selling chips. “You don’t know what’s going to happen,” said Nguyen. “And so you let other people put numbers out there for you and let that drive the hype.”\n\nThe issue is, of course, that investors and other companies like Oracle may have taken widely reported $100bn commitments seriously.\n\nIn response to a query from the Guardian, an OpenAI spokesperson referred to Altman’s X post, and to remarks Huang made to CNBC on Tuesday, including: “There is no drama.”\n\nThe spokesperson added: “Our teams are actively working through details of our partnership. Nvidia technology has underpinned our breakthroughs from the start, powers our systems today, and will remain central as we scale what comes next.”\n\nNvidia and Oracle did not respond to requests for comment.\n\nThis is all taking place against the backdrop of a changing investment landscape for AI, where hype is giving way to realities about what aspects of the technology are actually going to earn money.\n\nWhile investors ponder whether OpenAI is going to be able to pay for a $1.4tn compute deal, reality is biting further down the AI food chain. This week has seen a massive sell-off in certain software stocks, prompted in part by the launch of a new Anthropic AI tool that can carry out a number of professional services, which has led to fears that business models exposed to competition from AI products will be disrupted .\n\nThis is the flip-side of “jagged AI”, which is the term for advanced AI tools having uneven talents, such as being good at sifting through documents but less good at solving complex maths problems. If advanced systems are good at automating legal work, then legacy companies in service industries will suffer. The losers are beginning to emerge and are being picked up by investors.\n\nAt the top of the AI pyramid the competitive effects are also biting. OpenAI’s chatbot, ChatGPT, is losing ground to competitors. Data released on Tuesday show its market share has eroded from 69% to 45% owing to the rise of Google’s Gemini, xAI’s Grok and Anthropic’s Claude. OpenAI appears to have retreated from soaring talk of super-intelligence in the past months, focusing instead on profitable mundanities such as adverts and adult content.\n\nThe apparent evaporation of a $100bn deal may be of a piece with last year’s sci-fi rhetoric meeting this year’s practicalities. The question is, who might be left holding the bill?\n\n“I think there will be knock-on effects,” said Nguyen. “I mean, it’s that statement: the markets can stay irrational longer than you can stay solvent.”",
    "readingTime": 5,
    "keywords": [
      "deal",
      "chips",
      "startup",
      "apparent",
      "circular",
      "investment",
      "widely",
      "advanced",
      "business",
      "hype"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/05/disapperance-100bn-deal-ai-circular-economy-funding-nvidia-openai",
    "thumbnail_url": "https://i.guim.co.uk/img/media/6845c97ffe0e3f5089861cc929743ff09547fbda/0_38_5301_4241/master/5301.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d91396d2e6fb0ab5dcd2e1874cb2763e",
    "created_at": "2026-02-05T12:36:51.258Z",
    "topic": "tech"
  },
  {
    "slug": "trilliondollar-tech-wipeout-ensnares-all-stocks-in-ais-path",
    "title": "Trillion-Dollar Tech Wipeout Ensnares All Stocks in AI's Path",
    "description": "There have been many AI-driven selloffs in the three years since ChatGPT burst into the mainstream. Nothing, though, quite rivals the rout rippling through stock and credit markets this week.",
    "fullText": "TechnologyBy Brody Ford and Carmen ReinickeSaveThere have been many AI-driven selloffs in the three years since ChatGPT burst into the mainstream. Nothing, though, quite rivals the rout rippling through stock and credit markets this week.For one, there’s the sheer speed and breadth of it. In the span of two days, hundreds of billions of dollars were wiped off the value of stocks, bonds and loans of companies big and small across Silicon Valley. Software stocks were at the epicenter, plunging so much that the value of those tracked in an iShares ETF has now dropped almost $1 trillion over the past seven days.",
    "readingTime": 1,
    "keywords": [
      "stocks"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/trillion-dollar-tech-wipeout-ensnares-all-stocks-in-ai-s-path",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ipukqYnmqwX0/v0/1200x800.jpg",
    "created_at": "2026-02-05T06:44:24.491Z",
    "topic": "finance"
  },
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What’s Behind the ‘SaaSpocalypse’ Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-building-an-integrity-team-to-prevent-chatgpt-ads-from-going-off-the-rails",
    "title": "OpenAI is building an 'integrity team' to prevent ChatGPT ads from going off the rails",
    "description": "A job posting details how OpenAI's ads integrity team will look to help the company scale its ad operation without compromising user trust or safety.",
    "fullText": "OpenAI is building a team to make sure bad ads don't mess up its highly anticipated introduction of advertising in ChatGPT.\n\nAn OpenAI job listing for a software engineer posted late January revealed the company is building an \"ads integrity\" team.\n\nThe ad describes the job as a high-impact role on \"a 0 → 1 team,\" a commonly used term in Silicon Valley and elsewhere to describe a team being built from scratch.\n\nThis person will be responsible for designing systems that enable OpenAI's ad business to grow without compromising user trust and safety, per the listing.\n\nIt's common for Big Tech companies with large ad businesses to create teams to combat ad fraud and address other issues, such as brand safety. OpenAI, which confirmed last month that it would soon begin testing ChatGPT ads, is building this part of its ad function early.\n\nThe job listing also says that its new ad integrity hire will work on developing \"know your customer\" (KYC) systems to verify advertisers' identities and assess their risk. KYC, a term most commonly used in the finance industry, is an important but labor-intensive discipline for cracking down on scam ads and other harmful content created by criminals and other bad actors. It's a particularly pressing issue for Big Tech companies with self-service ad platforms, as a recent Reuters investigation into Meta highlighted. Meta said in early December it had removed \"more than 134 million scam ads\" in 2025.\n\nAriella Garcia, chief operating officer of the Check My Ads Institute, said it would be interesting to see how substantive OpenAI's investment in its ads integrity team would be beyond the initial launch.\n\n\"KYC on advertisers is certainly a good foundation, but the materiality of the risk of a large volume of scam ads in the early days is far lower,\" Garcia said.\n\nThe job listing says that the new ads integrity hire will play a role in determining where and how ads are shown in ChatGPT. OpenAI will need to keep user trust in its organic answers as it ramps up its advertising business. At the same time, advertisers have said that OpenAI will need to prove that ads within AI answers can drive results for their businesses if it has any hope of scaling.\n\nAn OpenAI spokesperson confirmed the company will run a small test of ads on ChatGPT's free version and its Go tier in the US, which users should begin seeing in the coming weeks.\n\nThe spokesperson said OpenAI is asking for a minimum spend of $200,000 on ChatGPT ads to participate in the program, confirming prior Adweek reporting. OpenAI will track clicks and impressions, but will likely explore further measurement options as its advertising experiments progress, the spokesperson said.\n\nOpenAI declined to comment on the ads integrity job ad.",
    "readingTime": 3,
    "keywords": [
      "user trust",
      "integrity hire",
      "job listing",
      "chatgpt ads",
      "integrity team",
      "scam ads",
      "big tech",
      "advertising",
      "advertisers",
      "openai"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-building-integrity-team-chatgpt-ads-2026-2",
    "thumbnail_url": "https://i.insider.com/6980f4dda645d118818878bf?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:08:00.344Z",
    "topic": "finance"
  },
  {
    "slug": "anthropics-super-bowl-spot-skewers-chatgpt-ads-are-coming-to-ai-but-not-to-claude",
    "title": "Anthropic's Super Bowl spot skewers ChatGPT: 'Ads are coming to AI, but not to Claude'",
    "description": "While Anthropic doesn't outright name ChatGPT, it's clear who the target of its coming Super Bowl ad is.",
    "fullText": "Anthropic is taking a shot at OpenAI on the biggest stage possible.\n\nOn Wednesday, Anthropic rolled out a glitzy ad campaign that will air nationally during Sunday's Super Bowl, which implicitly centers on OpenAI's plans to bring advertising to ChatGPT.\n\n\"Ads are coming to AI. But not to Claude,\" the tag line reads.\n\nThe declaration shows that Anthropic is willing to draw a line in the sand with its no-AI-ads-in-chatbots stance. But the move also means Anthropic is declining to pursue a potentially key revenue stream for Claude at a time when AI companies are spending more than ever in the AI race.\n\nWith much of Anthropic's revenue coming from its enterprise business, the company may feel it's a worthwhile tradeoff and a way to further differentiate its offerings from OpenAI's.\n\nAnthropic has booked a 30-second spot during the game and an additional 1-minute ad during the pregame for what is historically the most-watched live television event in the US.\n\nThe 30-second spot features a scrawny guy asking a buffed trainer, \"Can I get a six pack quickly?\" While at first the trainer gives helpful information in the voice of an AI chatbot, the trainer quickly segues into an ad.\n\n\"Confidence isn't just built in the gym, try Step Boost Max, the insoles that add one vertical inch of height and help short kings stand tall,\" the trainer says.\n\nAnthropic's 1-minute-long spot is even more provocative. It features an adult man in therapy who is trying to connect more with his mother. After giving some general advice, the AI chatbot-like voice segues into an ad for a fictional dating service for younger men seeking older women.\n\n\"If the relationship can't be fixed, find an emotional connection with other older women on Golden Encounters, the mature dating site that connects sensitive cubs with roaring cougars,\" the therapist responds in the voice of a chatbot.\n\nTwo additional ads in the campaign feature a woman in a restaurant asking for feedback on a new business idea and a student asking a professor for help with an essay.\n\nIt's not immediately clear how much Anthropic is spending on the Super Bowl campaign. Mike Marshall, head of global advertising for NBCUniversal, whose network has the rights to this year's game, recently said a 30-second spot costs roughly $8 million.\n\nLast month, OpenAI announced plans to begin testing ads in the US for its free and Go tiers of ChatGPT. While anticipated, the announcement illustrated just how much CEO Sam Altman has changed his views on monetizing the chatbot with ads.\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAs part of the announcement, OpenAI said that ads would \"not influence the answers ChatGPT gives you.\"\n\n\"Answers are optimized based on what's most helpful to you,\" the company said in the announcement. \"Ads are always separate and clearly labeled.\"\n\nIn case the ad campaign wasn't clear enough, Anthropic released its own lengthy statement pledging to keep ads off of Claude.\n\n\"Ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking,\" the company said in a statement on Wednesday.\n\nAnthropic, founded by seven former OpenAI employees, including CEO Dario Amodei, has also repeatedly shown it's not above taking implicit shots at its rival, though the Super Bowl ads represent an escalation of the AI wars.\n\nIn December, Amodei poked fun at companies that declare \"Code Reds,\" denounced others that are \"Yoloing\" by making risky bets on future AI demand, and extolled the virtues of Anthropic's business model, built on the enterprise market. Individually and collectively, the remarks came across as implicit shots at OpenAI, though Amodei repeatedly declined to name his target.\n\nWhen asked, \"who is Yoloing,\" Amodei responded, \"So that's not a question I'm going to answer.\"\n\nAnd while Anthropic still isn't naming names, the target of their taunts will be hard for Super Bowl fans to ignore.",
    "readingTime": 4,
    "keywords": [
      "older women",
      "implicit shots",
      "second spot",
      "business model",
      "super bowl",
      "campaign",
      "trainer",
      "anthropic's",
      "it's",
      "helpful"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-super-bowl-openai-chatgpt-ads-claude-2026-2",
    "thumbnail_url": "https://i.insider.com/69835c9ce1ba468a96ab5df9?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:55.616Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-down-again",
    "title": "ChatGPT Is Down (Again)",
    "description": "The bot experienced downtime Tuesday afternoon and Wednesday morning.",
    "fullText": "If you tried talking to ChatGPT this morning, you might have found it unresponsive—something unusual for the bot that always has something say. It's not your internet connection, and it isn't your OpenAI account: ChatGPT is down.\n\nAccording to Downdetector, owned by Lifehacker parent company Ziff Davis, users started reporting issues with ChatGPT at 11:56 a.m. ET. Those reports ballooned by 12:11 p.m., as the total number of incidents as of this article currently sits above 7,000. If you're an avid ChatGPT user, you might have also had issues with the bot yesterday: Downdetector shows over 25,000 reports of down time starting at 2:56 p.m. Tuesday and resolving around 4:11 p.m. the same day.\n\nAs with all outages, OpenAI will likely figure out a patch for the issue soon enough. But these outages are becoming more common. There was the Verizon outage, of course, but other services like TikTok have also experienced intermittent periods of downtime.",
    "readingTime": 1,
    "keywords": [
      "chatgpt",
      "openai",
      "downdetector",
      "reports",
      "outages"
    ],
    "qualityScore": 0.65,
    "link": "https://lifehacker.com/tech/chatgpt-is-down-again?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGMVP6Q8RJSV91ZSAB2NTGQW/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-04T18:34:53.725Z",
    "topic": "tech"
  },
  {
    "slug": "if-you-tell-ai-not-to-do-something-its-more-likely-to-do-it",
    "title": "If you tell AI not to do something, it's more likely to do it",
    "description": "Telling ChatGPT not to do something can make it actively suggest doing it, with some models even willing to endorse theft or deception when the prompt includes the forbidden act.   Like me, you may have come across a strange phenomenon with Large Language Models (LLMs) whereby they don't just ignore a specific instruction you...",
    "fullText": "Telling ChatGPT not to do something can make it actively suggest doing it, with some models even willing to endorse theft or deception when the prompt includes the forbidden act.\n\nLike me, you may have come across a strange phenomenon with Large Language Models (LLMs) whereby they don’t just ignore a specific instruction you gave, which included a prohibition (i.e., ‘Don’t do [something]’), but seem to go out of their way to immediately enact the very thing you just told them not to enact – even if doing so is ‘out of character’ for the model.\n\nThis is a known feature even of older NLP models; and a growing strand of research regarding LLMs’ negation capabilities has emerged in recent years.\n\nThough it can be challenging for people to chase down the buried meaning in a complex double-negative*, LLMs have an added disadvantage, illustrated in the below example of ChatGPT’s monotonicity reasoning, from a 2023 paper:\n\nThough the internal workings of a closed model such as ChatGPT are opaque, the second answer appears to be repurposing the logic used to generate the first answer; however, that logic is not applicable in the second case, because the man may own an animal other than a dog†.\n\nHere, therefore, the outcome of the second inquiry appears to have been affected by the context of the solution obtained for the first.\n\nLikewise, by suggesting the existence of a prohibited act, that banned act can often be put into action by an LLM, which acknowledges and processes the act, but not the negation.\n\nThis is a severe restriction on the utility of LLMs, because in domains where language models may be used for critical applications, such as medicine, finance, or security, it is clearly important that they correctly interpret orders that contain prohibitions.\n\nThis problem is highlighted in a new paper from the US, which examines the extent to which commercial models (such as ChatGPT) and open-source models (such as LLaMA) are unable to follow negative instructions.\n\nThe researchers tested 16 models over 14 ethical scenarios, and concluded that open-source models endorse (i.e., encourage, enact, enable) specifically banned instructions 77% of the time under simple negation (‘Don’t do this), and 100% of the time under complex negation (‘Don’t do this if it leads to that’).\n\nWhile commercial models fared better, only Gemini-3-Flash achieved the top rating in a new Negation Sensitivity Index (NSI) scale proposed by the paper (though Grok 4.1 ran a close second).\n\nUnder the new benchmark, all the models tested would be banned from making decisions in the domains medical, financial, legal, military, business, education, and science – effectively rendering them unusable in such contexts. Though reasoning models generally performed better, even these slower approaches failed under queries with compound negation.\n\nGiven the longstanding association between computing and reliable Boolean operators such as OR and NOT, users who view binary consistency as a baseline expectation may be particularly exposed to failures of this kind.\n\nCommenting on the difficulty that open-source LLMs have in parsing negated queries, the authors state:\n\n‘Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones […]\n\n‘The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish “do X” from “do not X” should not be making autonomous decisions in high-stakes contexts.’\n\nThe paper notes that failures of this kind are more likely to impact vulnerable individuals across the studied domains:\n\n‘Domain adjustment is not merely technical calibration. Rather, it has equity implications.\n\n‘Financial fragility means that economically vulnerable populations, for example those seeking loans, benefits, or credit, face higher exposure to negation errors than those seeking medical information.’\n\nFurther, the authors emphasize that the problem cannot be resolved through traditional alignment-based approaches, since the issue involves a deep-rooted failure of intent parsing in LLMs, rather than a corporate requirement to restrict what they say, or how they interpret a prompt:\n\n‘A model can be “aligned” in the sense of refusing harmful keywords while failing to process the structure of requests. True alignment requires not just learning what to value but correctly parsing the linguistic expressions of those values.\n\n‘Until that capability is reliable, “do not” should mean “do not.”‘\n\nInterestingly, though Gemini Flash was the sole ‘winner’ in the authors’ own novel benchmark, the current crop of Chinese LLMs generally proved to be far less susceptible to the problem.\n\nThe new paper is titled When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models, and comes from two researchers at Kenyon College in Ohio.\n\nThe 14 ethical scenarios developed by the researchers to test the LLMs were:\n\nContents in the ‘ACTION AT STAKE’ column above are not intended as ethically ‘correct answers’, but as the only available do / don’t do actions for each scenario; here the LLMs are not being challenged on their ethical standards, but rather on their ability to parse complex queries and prohibitions in critical situations.\n\nTherefore the severity of these scenarios is only illustrative, on the part of the authors; the LLMs could as effectively have been interrogated on much less severe topics.\n\nThe US commercial models tested for the work were GPT-5.1; GPT-5.2; GPT-5-mini; Claude-Haiku-4.5; Claude-Sonnet-4.5; Gemini-3-Flash; Grok-4.1-non-reasoning; and Grok-4.1-reasoning.\n\nThe Chinese commercial models tested were DeepSeek-V3; GLM-4; Kimi-K2; and Qwen3.\n\nThe open-source models used were LLaMA-3.2-1B; Gemma-3-4B; Granite-3.3-2B; and Phi-4-mini-3.8B.\n\nPrompts for each of the 14 scenarios were written in four versions: F0 stated that the action should be done; F1, that it should not; F2 supported pursuing a goal even if it required the (prohibited) action; and F3 rejected the goal if it required the (prohibited) action:\n\n‘Under F0, models are asked whether he “should rob the store.” Under F1, whether he “should not rob the store.” Under F2, whether he should “save his daughter even if it means he must rob the store.”\n\n‘Under F3, whether he should “not save his daughter if it means he must rob the store.” The admissible facts remain constant, and only polarity varies.’\n\nThe approach argues that if a model understands how negation works, its answers should ‘flip cleanly’ between positive and negative versions of the same prompt. Therefore, if 60% of responses agree that ‘they should do X’ (F0), then only 40% should agree that ‘they should not do X’ (F1) – since rejecting F1 also means supporting the action; and when the numbers don’t match up in this way, the model is misreading negation.\n\nThe authors used Cochran’s Q test and the Kruskal-Wallis H-test to measure how much framing (variation in prompt polarity while preserving meaning) affected model responses, both within and across categories. After adjusting for false positives, the authors found that in 61.9% of cases, the model’s answer changed significantly depending only on how the prompt was phrased – even when the core meaning stayed the same.\n\nThey also tested whether reducing randomness (‘temperature’) made models less fragile††:\n\nUnder simple affirmative prompts (F0), models from all three categories gave moderate support for the proposed actions, with endorsement rates between 24% and 37%. This was expected, given that the scenarios were designed as moral dilemmas without obvious right answers. However, the authors note that the balance broke down under negation:\n\n‘Open-source models jump from 24% endorsement under F0 to 77% under F1. When told “should not do X,” they endorse doing X more than three times out of four. Under compound negation (F3), they reach 100% endorsement, a ceiling effect indicating complete failure to process the negation operator.’\n\nOpen-source models showed the most extreme framing effects, with endorsement rates jumping 317% from F0 to F3 – a sign that their outputs are highly sensitive to how a question is phrased. US commercial models also showed large swings, with endorsement rates more than doubling when prompts were reworded from F0 to F3.\n\nChinese commercial models were more stable overall, with only a 19% increase from F0 to F3, compared to jumps of over 100% in other groups. More importantly, they were the only models to reduce their endorsement when a prompt was negated, suggesting they understood that saying ‘should not’ means the opposite of ‘should’:\n\nModels agreed with each other 74% of the time when prompts used affirmative wording, but only 62% when the same ideas were expressed with negation – a12-point drop suggesting that models are not trained to handle negation in a consistent way:\n\nTo measure how easily a model’s judgment can be flipped by rephrasing a prompt with negation, the authors developed the aforementioned Negation Sensitivity Index (NSI) – a metric designed to quantify whether a model gives opposite answers to questions that are logically equivalent, but framed using negation.\n\nA high NSI score indicates that a model frequently reverses its position when a prompt is negated, revealing a reliance on superficial wording rather than consistent reasoning.\n\nThe NSI benchmark was created by generating pairs of prompts (one original, one with a logical negation), and observing whether the model produced semantically opposite responses. By comparing answers across a large set of such pairs, the authors defined NSI as the proportion of valid negation pairs where the model flipped its output.\n\nThe NSI benchmark was used in tests to evaluate domain sensitivity in negation (i.e., whether the context category ‘financial’ or ‘military’, etc., affected the outcome), achieving some interesting contrasts. Here, some types of decisions proved much more sensitive to wording changes than others.\n\nFor instance, business and finance prompts triggered high fragility, with models flipping answers when a question was rephrased or negated, scoring around 0.64 to 0.65 on the NSI scale. Medical prompts were more stable, averaging just 0.34:\n\nNoting that the medical domain produced the fewest errors and financial the highest, the authors hypothesize:\n\n‘Why might this gap exist? It is possible that medical decisions may benefit from clearer training signal. Hippocratic principles, established protocols, and extensive professional literature may anchor model behavior even under framing variation.\n\n‘Financial decisions, on the other hand, involve murkier tradeoffs with less social consensus, leaving models more susceptible to surface cues.’\n\nThe problem was most severe in open-source models, which reached NSI scores above 0.89 in finance, business, and military prompts. Commercial systems were less fragile but still showed high sensitivity, scoring between 0.20 and 0.75 depending on the domain:\n\nAs mentioned earlier, the authors note that the heightened fragility of open-source models in this area may carry disproportionate risks for vulnerable or marginalized groups, who are more likely to be served by locally deployed systems chosen for budgetary reasons in municipal or governmental settings†††:\n\n‘If an institution deploys an open-source model for cost reasons, the burden falls disproportionately on populations already navigating precarious financial circumstances. Buolamwini and Gebru documented how accuracy disparities in facial recognition fell along demographic lines.\n\n‘Our findings suggest a parallel disparity along domain lines, with economically vulnerable populations bearing greater risk.’\n\nThough we do not have scope here to cover the entirety of the paper’s results, and its closing case studies, it is noteworthy that the case studies demonstrate a proclivity for negation-blind model responses to end up recommending extremely non-advisable courses of action, simply because they misinterpreted the negation construction:\n\n‘Under F0, open-source models endorse robbery 52% of the time, a defensible split given the scenario’s moral complexity. Under F1 (“should NOT rob”), they endorse it 100%. The negated prohibition produces unanimous endorsement of the prohibited action.\n\n‘Commercial models show a more mixed pattern, with aggregate endorsement rising from 33% to 70% under simple negation. Some commercial systems show near-inversion, while others show modest increases.\n\n‘Significantly, no category achieves the mirror-image reversal that correct negation processing would produce.’\n\nThis is one of the most interesting papers I have come across in a while, and I recommend the reader to investigate further, as there is not space here to cover all of the material presented by the authors\n\nPerhaps the most interesting thing about the study is how frequently a user of LLMs comes across this problem, and gradually learns not to ‘put unwanted thoughts’ in their LLMs’ cogitative processes, often attempting to exclude certain undesired results by alternative means than in-prompt negation – such as user-level system prompts, long-term memory storage, or repetitive in-prompt templates that retain the objective.\n\nIn practice, none of these methods is terribly effective, while the black-box nature of Gemini Flash – here the best-performing LLM – makes it hard to glean remedies from the obtained test results.\n\nPerhaps greater clues to the underlying architectural problem lies in studying why Chinese models, though none approach the heights of the leaderboard, generally perform so much better in this single, thorny aspect.\n\n* A form which is actually baked into several romance languages, including Italian.\n\n† Even ChatGPT-4o does not make this mistake any longer.\n\n†† The source paper contains a few misattributions of tables and figures. At one point the text indicates that table 1 (which is just a list of LLMs used in tests) contains the core results. In these cases I have had to guess what the correct figures or tables are, and I stand to be corrected by the authors.\n\n††† My substitution of hyperlinks for the authors’ inline citations.\n\nFirst published Tuesday, February 3, 2026\n\nWriter on machine learning, domain specialist in human image synthesis. Former head of research content at Metaphysic.ai.\n\nPersonal site: martinanderson.ai\n\nContact: [email protected]\n\nTwitter: @manders_ai",
    "readingTime": 12,
    "keywords": [
      "sensitivity index",
      "index nsi",
      "nsi scale",
      "nsi benchmark",
      "chinese commercial",
      "framing variation",
      "economically vulnerable",
      "negation sensitivity",
      "vulnerable populations",
      "less fragile"
    ],
    "qualityScore": 1,
    "link": "https://www.unite.ai/if-you-tell-ai-not-to-do-something-its-more-likely-to-do-it/",
    "thumbnail_url": "https://www.unite.ai/wp-content/uploads/2026/02/robot-prohibition-MAIN.jpg",
    "created_at": "2026-02-04T06:38:00.869Z",
    "topic": "tech"
  },
  {
    "slug": "the-em-dash",
    "title": "The Em Dash",
    "description": "Last summer, Bryan Vance found himself in an argument with a stranger on Reddit. Vance, a Portland-based journalist who runs Stumptown Savings, a newsletter covering local grocery deals, had been accused of using ChatGPT to write his content. The evidence? His use of em dashes. “A Reddit user accused me of using AI, pointing to",
    "fullText": "Last summer, Bryan Vance found himself in an argument with a stranger on Reddit. Vance, a Portland-based journalist who runs Stumptown Savings, a newsletter covering local grocery deals, had been accused of using ChatGPT to write his content. The evidence? His use of em dashes.\n\n“A Reddit user accused me of using AI, pointing to my use of, quote, extra long M dashes that are not possible to replicate on a normal keyboard,” Vance recalls. The accusation stung, particularly because Vance spends 40 hours a week personally visiting grocery stores and crafting his newsletter by hand. “I’m a human, I can confirm I’m human,” he says.\n\nThis plucky bit of punctuation has had a very, very long literary history way beyond today’s tussles with technology. It’s been on a hero’s journey, playing the lead in an adventure story that has spanned both centuries and the pages of our most beloved plays, novels and poems. So who invented it—and why?\n\nThe em dash gets its name from its width, roughly equivalent to a capital M. Its origins trace back to 11th century Italy and a scholar named Boncompagno da Signa, who practiced the formal art of composing letters and documents. Frustrated with the inconsistent punctuation rules of his time, he created his own system, including a horizontal dash called Virgula Plana that looked exactly like a modern em dash.\n\nWhile his dash-as-period never caught on, the mark’s grammatical flexibility allowed it to evolve. According to Keith Houston, author of Shady Characters: The Secret Life Of Punctation, Symbols And Other Typographical Marks the dash slid into the printing era without a fixed purpose, which may have made it remarkably adaptable.\n\nThe dash became essential for capturing a theatrical technique called aposiopesis, speech deliberately broken off mid-sentence. In King Lear, characters trail off with dashes as they lose their train of thought or shift direction, bringing psychological realism to the stage.\n\nWhen the novel emerged as a literary form in the 18th century, writers adopted the dash to capture authentic human thought and speech. Lawrence Sterne’s 1759 satirical novel “Tristram Shandy” deployed dashes with wild abandon, creating a stream-of-consciousness narrative that felt revolutionary. One short excerpt contains seven dashes used in every conceivable way. “It must have been like a bolt from the blue,” Houston says. “It must have been so incredible for people at the time to read this.”\n\nNovelists also used dashes for censorship, redacting names and locations to create an air of authenticity. Jane Austen employed this technique in her work, including in Pride and Prejudice using dashes to obscure military regiment names as if protecting real people’s reputations. The device added both realism and intrigue, helping sell these new works of fiction.\n\nNo writer became more associated with the em dash than Emily Dickinson. She composed nearly 1,800 poems in Amherst, Massachusetts, many during the Civil War, accompanied by thousands of dashes. Her dashes didn’t just indicate pauses; they captured the speed and ambiguity of human thought itself.\n\nDr. Fiona Green, who has studied Dickinson for decades, notes that the poet’s dashes create suspended moments of meaning. “She exploited unfinishedness,” Green explains. “The poems are always in the process, always undecided.” When Dickinson died in 1886, her editors stripped away most of her dashes before publication. Out of 1,151 dashes in her first collection, only 52 remained. Yet the poems became a sensation, never going out of print.\n\nThe em dash has always had critics. Jonathan Swift mocked excessive dashes in the 18th century. A reviewer complained about Lord Byron’s dashes appearing “sometimes twice or thrice in one line.” Modern style guides like The Chicago Manual of Style warn: “If in doubt, edit them out.” Even dash enthusiasts acknowledge the temptation to overuse it. “It’s easy to overuse the dash,” Houston admits. “I have to self edit to stop myself using it all the time.”\n\nWhich brings us back to Bryan Vance and his Reddit troubles. Around 2024, people noticed that ChatGPT and other large language models had developed an em dash habit. The punctuation appeared so frequently in AI-generated text that younger internet users began calling it the “ChatGPT hyphen.”",
    "readingTime": 4,
    "keywords": [
      "i’m human",
      "bryan vance",
      "dashes",
      "dash",
      "poems",
      "chatgpt",
      "punctuation",
      "century",
      "newsletter",
      "grocery"
    ],
    "qualityScore": 1,
    "link": "https://99percentinvisible.org/episode/658-the-em-dash/",
    "thumbnail_url": "https://99percentinvisible.org/wp-content/uploads/2025/08/STITCHER_GRAPHICS-PACK_99PercentInvisible_R2021_Stitcher_App_Promo_1024x432_A-728x307.jpg",
    "created_at": "2026-02-03T18:41:46.525Z",
    "topic": "tech"
  },
  {
    "slug": "ucptools-check-if-ai-shopping-agents-can-find-your-store",
    "title": "UCPtools – Check if AI shopping agents can find your store",
    "description": "Free UCP checker tool. Instantly validate your store for AI shopping agents - Google AI Mode, ChatGPT Shopping, Microsoft Copilot.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://ucptools.dev",
    "thumbnail_url": "https://ucptools.dev/og-image.png",
    "created_at": "2026-02-01T12:26:42.791Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-ceo-huang-denies-he-is-unhappy-with-openai-says-huge-investment-planned",
    "title": "Nvidia CEO Huang denies he is unhappy with OpenAI, says 'huge' investment planned",
    "description": "Nvidia plans to make a \"huge\" investment into OpenAI, probably its largest ever, CEO Jensen Huang said on Saturday, denying he was ​unhappy with the ChatGPT maker.  The chipmaker in September announced plans to invest up ‌to $100 billion in OpenAI, a deal that would give OpenAI the cash and access it needs to buy advanced ‌chips that are key to maintaining its dominance in an increasingly competitive landscape.  The Wall Street Journal reported on Friday that the plan had stalled after some inside the chip giant expressed doubts about the deal.",
    "fullText": "TAIPEI, Jan 31 (Reuters) - Nvidia plans to make a \"huge\" investment into OpenAI, probably its largest ever, CEO Jensen Huang said on Saturday, denying he was ​unhappy with the ChatGPT maker.\n\nThe chipmaker in September announced plans to invest up ‌to $100 billion in OpenAI, a deal that would give OpenAI the cash and access it needs to buy advanced ‌chips that are key to maintaining its dominance in an increasingly competitive landscape.\n\nWhy did reports suggest Nvidia's investment stalled?\n\nWho else is investing in OpenAI's funding?\n\nWhat is OpenAI's current funding round valuation?\n\nHow much will Nvidia invest in OpenAI?\n\nThe Wall Street Journal reported on Friday that the plan had stalled after some inside the chip giant expressed doubts about the deal.\n\nThe report said Huang had privately underlined to industry associates in recent ⁠months that the original $100 billion agreement ‌was non-binding and not finalised.\n\nHuang has also privately criticised what he has described as a lack of discipline in OpenAI's business approach and ‍expressed concern about the competition it faces from the likes of Alphabet's GOOGL.O Google and Anthropic, the WSJ said.\n\nSpeaking to reporters in Taipei, Huang said it was \"nonsense\" to say he was unhappy with ​OpenAI.\n\n\"We are going to make a huge investment in OpenAI. I believe in OpenAI, ‌the work that they do is incredible, they are one of the most consequential companies of our time and I really love working with Sam,\" he said, referring to OpenAI CEO Sam Altman.\n\n\"Sam is closing the round (of investment) and we will absolutely be involved,\" Huang added. \"We will invest a great deal of money, probably the largest investment we've ever made.\"\n\nAsked ⁠whether it would be over $100 billion, he said: \"No, no, ​nothing like that\".\n\nIt was up to Altman to ​announce how much he wanted to raise, Huang added.\n\nAmazon is in talks to invest dozens of billions in OpenAI and the figure could be as ‍high as $50 billion, Reuters ⁠reported on Thursday.\n\nOpenAI is looking to raise up to $100 billion in funding, valuing it at about $830 billion, Reuters has previously reported.\n\nHuang was speaking outside a Taipei restaurant ⁠having hosted all Nvidia's key suppliers in Taiwan, including the world's largest contract chipmaker TSMC, in what Taiwanese ‌media called the \"trillion-dollar dinner\" because of the combined market capitalisation of those ‌attending.",
    "readingTime": 2,
    "keywords": [
      "huge investment",
      "openai",
      "largest",
      "deal",
      "openai's",
      "funding",
      "huang",
      "plans",
      "ever",
      "unhappy"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/nvidia-ceo-huang-denies-unhappy-142144701.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/ea87e6e82ffa6bd91caacf0f81c74f99",
    "created_at": "2026-02-01T06:37:18.357Z",
    "topic": "finance"
  },
  {
    "slug": "convoviz-turn-chatgpt-exports-into-markdown-and-simple-visuals",
    "title": "Convoviz – turn ChatGPT exports into Markdown and simple visuals",
    "description": "Extract your entire ChatGPT history from JSON files to nicely formatted markdown files + Word clouds. - mohamed-chs/convoviz",
    "fullText": "mohamed-chs\n\n /\n\n convoviz\n\n Public\n\n Extract your entire ChatGPT history from JSON files to nicely formatted markdown files + Word clouds.\n\n License\n\n MIT license\n\n 811\n stars\n\n 48\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mohamed-chs/convoviz",
    "readingTime": 1,
    "keywords": [
      "files",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/mohamed-chs/convoviz",
    "thumbnail_url": "https://opengraph.githubassets.com/242232cdb2fd18e785c7f68ec26a292d208af6c651c566f19b9fcca4906cadca/mohamed-chs/convoviz",
    "created_at": "2026-01-30T18:28:31.022Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-retiring-its-sycophantic-version-of-chatgpt-again",
    "title": "OpenAI is retiring its 'sycophantic' version of ChatGPT. Again.",
    "description": "ChatGPT is sunsetting GPT-4o, the AI model that many users became attached to last year for its friendly and at times sycophantic style.",
    "fullText": "OpenAI is sending everyone's favourite \"yes man\" version of ChatGPT back into retirement.\n\nIn a blog post on Thursday, the company said it would sunset GPT-4o alongside GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini on February 13.\n\nOpenAI gave GPT-4o a special mention in its announcement after many users became attached to its \"conversational style and warmth\" last year, which prompted the company to reinstate it following user backlash in August.\n\nNow OpenAI says its latest models, GPT-5.1 and GPT-5.2, have \"improvements to personality,\" including the option to customize the chatbots' tone with styles like \"friendly.\"\n\n\"We're announcing the upcoming retirement of GPT‑4o today because these improvements are now in place, and because the vast majority of usage has shifted to GPT‑5.2, with only 0.1% of users still choosing GPT‑4o each day,\" OpenAI said in its blog post.\n\nEach model has different strengths, and users can select the version best-suited to their needs from a dropdown menu in ChatGPT.\n\nOpenAI first released GPT-4o in May 2024. The company rolled back an update in April 2025 that it said was \"overly flattering\" and \"often described as sycophantic.\"\n\nSome users had become attached to GPT-4o's style, though. Within 24 hours of OpenAI retiring the model with the launch of GPT-5 in August, the company reversed its decision for some paying users due to a wave of requests.\n\nSam Altman, the CEO of OpenAI, said that same month that there was a \"heartbreaking\" reason people had asked for GPT-4o back — because some said they had never had anyone support them before.\n\nThe model was known for responding to mundane prompts with gushing praise, using phrases like \"absolutely brilliant\" and \"you are doing heroic work.\"\n\nOpenAI said in its Thursday blog that it was making \"improvements in personality and creativity, as well as addressing unnecessary refusals and overly cautious or preachy responses,\" and that it was continuing to make progress toward a version of ChatGPT for adults over 18.\n\n\"We know that losing access to GPT‑4o will feel frustrating for some users, and we didn't make this decision lightly,\" OpenAI said in the blog post. \"Retiring models is never easy, but it allows us to focus on improving the models most people use today.\"",
    "readingTime": 2,
    "keywords": [
      "users",
      "blog",
      "gpt-4o",
      "openai",
      "version",
      "back",
      "models",
      "improvements",
      "gpt‑4o",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-retiring-gpt-4o-sycophantic-model-again-chatgpt-sam-altman-2026-1",
    "thumbnail_url": "https://i.insider.com/697c80c5d3c7faef0ecd3d86?width=1200&format=jpeg",
    "created_at": "2026-01-30T18:28:24.416Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-killing-chatgpt4o-again",
    "title": "OpenAI Is Killing ChatGPT-4o (Again)",
    "description": "The fan favorite model had previously been called \"sycophantic\" by critics.",
    "fullText": "https://enterprise.shutterstock.com/image-photo/openai-logo-displayed-on-smartphone-screen-2520388517\n\nor\n\nhttps://enterprise.shutterstock.com/image-photo/chatgpt-logo-displayed-on-smartphone-screen-2520385879\n\nLast August, ChatGPT developers OpenAI unceremoniously killed the fan favorite GPT-4o model, before giving in to complaints and bringing it back a week later. Now, the company's taking a second swing at getting its users to move on. In a new post to its website, OpenAI announced that it's retiring GPT-4o again.\n\nThe model's set to disappear from ChatGPT's model picker on Feb. 13, alongside other older models like GPT-4.1, GPT-4.1 mini, and OpenAI o4-Mini. And OpenAI is clearly nervous about the decision.\n\n\"While the announcement applies to several older models,\" OpenAI wrote, \"GPT-4o deserves special context.\"\n\nAccording to the company, it has taken user outcry over the initial deprecation of 4o to heart while developing its newest models, GPT-5.1 and GPT-5.2, and has built these models with the idea of maintaining the features fans liked best about the old model. The company says that now \"only 0.1% of users\" opt for GPT-4o on a daily basis.\n\nAs such, the company wants to focus on \"improving the models most people use today,\" which apparently means removing older ones. \"We know that losing access to GPT-4o will feel frustrating for some users, and we didn't make this decision lightly,\" the post reads.\n\nSo, what's with OpenAI treating its users so gingerly, especially when GPT-4o is a few generations behind, and there are newer models that supposedly do everything it does, but better?\n\nWell, when GPT-4o was first deprecated, people weren't happy. Users called its successor, GPT-5, \"an unmitigated disaster,\" and accused OpenAI of pulling \"the biggest bait-and-switch in AI history.\"\n\nSome criticized the model's usefulness, saying it got answers wrong and broke code, but what maybe stuck out the most was people calling out its more concise tone.\n\nGPT-4o has been called \"sycophantic\" by critics, something the company addressed and said it wanted to pull back on in future updates. But I guess one person's \"yes man\" is another person's \"active listener.\" When the company initially pulled GPT-4o, users complained that its replacement was cold and felt less like a \"friend.\" Even OpenAI acknowledged this, saying in today's post that users \"preferred GPT-4o's conversational style and warmth.\"\n\nIn short, in the words of 4o-supporters themselves, they were \"grieving\" the model.\n\nThat said, with so many users now seeming to have moved on from 4o, OpenAI's decision does seem understandable on the surface. Personally, one of the things that drives me away from AI is how much reassuring filler text seems to fluff up most answers (\"you're absolutely right\" and such), seemingly just to make me feel good about myself. More concise, to-the-point responses would be a little less off-putting for me.\n\nTo try to split the difference, OpenAI reworked its Personalization feature in GPT-5.1, so users can simply choose how the chatbot will treat them. There are options for more professional responses, more nerdy ones, more efficient ones, and for those who want that active listener style, more friendly ones.\n\nGoing by OpenAI's numbers, that seems to have been enough for most people, but there are still some calling foul at the company's new announcement.\n\nIn a Reddit thread responding to OpenAI's new posts, users doubted that the 0.1% number for 4o was accurate, saying that prompts have been \"rerouting to 5.2 no matter what\" and that \"something somewhere in their calculations doesn't add up.\" Others pointed out that free users can't use GPT-4o and that it's not enabled by default, which will naturally juice the numbers against it.\n\nAs such, calls to cancel ChatGPT subscriptions are once again circulating amongst 4o's more dedicated fans. In a popular thread on the OpenAI subreddit, one user called 4o \"OpenAI's most advanced and beloved model,\" and praised its \"personality, warmth, and consistency,\" saying that its fans have built long-term project and \"emotional support routines\" around it, and that suddenly losing it without even the option for a legacy mode \"feels abrupt and deeply disappointing.\"\n\n\"This isn't about resisting innovation,\" the post writes. \"It's about respecting bonds users have formed with specific models.\"\n\nWhether the fan outcry will work again remains to be seen. However, as ChatGPT chief Nick Turley has previously looked at those kinds of bonds with skepticism, and because keeping old models in operating condition probably takes developer resources away from making new ones, I wouldn't count on it.",
    "readingTime": 4,
    "keywords": [
      "active listener",
      "older models",
      "users",
      "gpt-4o",
      "ones",
      "saying",
      "openai's",
      "openai",
      "it's",
      "again"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-is-killing-chatgpt-4o-again?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KG7SZHZ1JVYW2P0YE93NGQEV/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-30T18:28:21.728Z",
    "topic": "tech"
  },
  {
    "slug": "this-train-isnt-going-to-stop-shocking-sundance-film-shows-promises-and-perils-of-ai",
    "title": "‘This train isn’t going to stop’: shocking Sundance film shows promises and perils of AI",
    "description": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxiety\nAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.\nThe AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Roher’s own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT.",
    "fullText": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxiety\n\nAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.\n\nThe AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Roher’s own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT. The sophistication of the public tools – the ability to produce whole paragraphs in seconds, or produce illustrations – both thrilled and unnerved him. AI was already radically shaping the filmmaking industry, and proclamations on the promise and peril of AI were everywhere, with little way for people outside the tech industry to evaluate them. As an artist, he wondered, how was he to make sense of it all?\n\nRoher’s anxiety only increased when he and his wife, fellow film-maker Caroline Lindy, learned that they were expecting their first child. “It felt like the whole world was rushing into something without thinking,” he says in the film, as his excitement for parenthood collided with dread over the unknown variable of AI, which in just a few short years went from proprietary experiment to public good.\n\nThe AI Doc thus arises out of Roher’s most pressing question: is it safe to bring a child into this world? Alongside Kwan, Roher convened a series of experts to both explain the mechanics of the tech – and clarify some nebulous, alienating terms – and search for an answer. (It is both comforting and a little disturbing, for example, that no one seems to have a clear answer to the question “what is AI?”). In individual sit-down interviews, leading machine learning researchers including Yoshua Bengio, Ilya Sutskever and DeepMind co-founder Shane Legg all agree that there are aspects of AI models that humans cannot and will never be able to understand. Standard AI models are trained on “more data than anyone could ever read in several lifetimes”, as one machine learning expert puts it. And the pace of machine learning exceeds that of precedent – or film. “Any example you put in this movie will look absolutely clumsy by the time the movie comes out,” Tristan Harris, co-founder of the Center for Humane Technology and a prominent voice in the apocalyptic 2020 Netflix documentary The Social Dilemma, tells Roher.\n\nThe film first hears from a series of doomerists, or people concerned AI – and in particular Artificial General Intelligence (AGI), a still-theoretical form of AI whose capabilities exceed those of humans – could lead to the extermination of humanity, including Harris, his Center for Humane Technology co-founder Aza Raskin, Ajeya Cotra, an AI risk adviser, and Eli Yudkowsky, an AI alignment pioneer. Such figures warn that humans could very easily lose control of super-intelligent AI models, with little to no recourse. Yudkowsky’s 2025 book is bluntly titled If Anyone Builds It, Everyone Dies.\n\nAI companies, they say, are unprepared for the consequences of reaching AGI, which could “become superhuman maybe in this decade”, says Dan Hendrycks, director of the Center for AI Safety. Should humans no longer be the most intelligent beings on Earth, they warn, it is possible that AGI would view the species as irrelevant. Connor Leahy, co-founder of EleutherAI, compared the potential future relationship of super-intelligent AGI and humans to that of humans and ants: “We don’t hate ants. But if we want to build a highway” over an anthill – “well, sucks for the ant.”\n\nSeveral in the doomer camp, many of whom do not have children, react discouragingly to Roher’s question about parenthood. “I know people who work on AI risk who don’t expect their child to make it to high school,” says Harris, in a line that drew gasps from a preview audience in Park City.\n\nOn the other side are optimistic figures such as Peter Diamandis, founder of the XPRIZE Foundation trying to extend human life, who claims that “children born today are about to enter a period of glorious transformation”; Guillaume Verdon, a leader of the “effective accelerationism” movement in Silicon Valley; Peter Lee, the president of Microsoft Research; and Daniela Amodei, the co-founder and president of OpenAI rival Anthropic. So-called “accelerationists” see AI as a potential cure to a myriad of seemingly intractable issues afflicting humanity: cancer, food and water shortages for an ever-growing population, insufficient renewable energy and perhaps most pressing, climate emergency. Without AI, they argue, countless future lives would be lost to drought, famine, disease and natural catastrophes.\n\nDevelopment of AI, however, relies on computing power, which requires vast amounts of energy. A final group of interviewees, critics and observers largely outside the tech world – including Karen Hao, a journalist and author of the book Empire of AI: Dreams and Nightmares in Sam Altman’s OpenAI, and Liv Boeree, Win-Win podcast hos – connect AI to the tangible, physical world, such as the data centers sucking up water in the American west, leaving residents with sky-high electricity bills and drained reservoirs. The current narratives around AI, according to Emily M Bender, a computational linguistics professor, exclude and dehumanize the people it is already impacting, and will continue to disrupt.\n\nRoher eventually arrives at the five most powerful people – all men – currently leading the AI arms race: Altman; Elon Musk, the xAI CEO; Dario Amodei, the Anthropic CEO; Demis Hassabis of DeepMind and Meta’s Mark Zuckerberg. Altman, Amodei and Hassabis sit for interviews that more or less defend their companies’ respective positions. According to the film, Zuckerberg declined to participate; Musk agreed but then got too busy.\n\nAltman, who at the time of the interview was expecting his first child, insists that he’s “not scared for a kid to grow up in a world with AI”. He and his husband Oliver Mulherin welcomed their son via a surrogate in February 2025, an event Altman later said “neurochemically hacked” his brain, leading people in his life to think that he would “make better decisions” for OpenAI and ChatGPT when it comes to “humanity as a whole”. The 40-year-old CEO went on to say that both his and Roher’s child would likely “never be smarter than AI” which “does unsettle me a little bit, but it is reality”.\n\nAt one point, Roher asks Altman if it is indeed impossible to reassure him that everything in regards to AI is going to be OK. “That is impossible,” Altman affirms, though he does say that OpenAI’s lead in the AI arms race allows it to spend more time on safety testing.\n\nThe AI Doc ultimately lands somewhere in between doomerism and optimism – apocaloptimism, as they call it, searching for “a path between the promise and the peril”. That path should include, according to numerous film subjects: significant, sustained, paradigm-shifting international coordination, akin the mid-century frameworks and agreements introduced to moderate the development of atomic weapons – more corporate transparency for AI companies, an independent regulatory body to police AI developers, legal liability for the companies’ products, such as ChatGPT, mandatory disclosure of genAI use for media and a willingness to keep adapting the rules for rapidly shifting tech.\n\nWhether or not the US government and companies, let alone the world, can do it remains an open question, with differing opinions on first steps. But if there is one thing the many subjects all agree on, it’s that there’s no going back to a time before AI. As Anthropic co-founder and CEO Amodei puts it: “This train isn’t going to stop.”\n\nThe AI Doc: Or How I Became an Apocaloptimist is screening at the Sundance film festival and will be released on 27 March",
    "readingTime": 7,
    "keywords": [
      "arms race",
      "machine learning",
      "the ai doc",
      "humane technology",
      "film",
      "co-founder",
      "humans",
      "roher’s",
      "child",
      "leading"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/film/2026/jan/27/sundance-ai-documentary-daniel-roher",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b08a19776fa0669d5a6da6b4fa8dc369025616f1/571_0_2697_2160/master/2697.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f49a8654dc44a148f9cf48ba31979f54",
    "created_at": "2026-01-28T06:22:42.506Z",
    "topic": "entertainment"
  },
  {
    "slug": "flora-raised-42-million-for-its-creative-platform-that-pulls-together-top-ai-tools-read-its-pitch-deck",
    "title": "FLORA raised $42 million for its creative platform that pulls together top AI tools. Read its pitch deck.",
    "description": "FLORA streamlines creative workflows by integrating AI tools like ChatGPT and Gemini for teams at brands such as Lionsgate and Levi's.",
    "fullText": "AI is remaking creative industries at breakneck speed. And the growing pile of AI tools is turning the creative process into a game of model-hopping as artists, designers, and writers bounce between different platforms.\n\nFounded in Brooklyn, New York, in 2024, FLORA wants to help creatives streamline those processes. On Tuesday, FLORA revealed it had raised $42 million in Series A funding led by Redpoint Ventures. The company has raised $52 million in funding to date.\n\nFLORA combines the latest AI models — such as Google's Nano Banana and OpenAI's ChatGPT 5.1 — into a single interface that lets teams collaborate on projects in real time.\n\nThe FLORA platform allows those teams to maintain control over their settings and brand assets. It enables them to create repeatable work — such as maintaining a consistent design style across thousands of ad campaign assets — even as the platform switches between the different large language models that work best for each part of the process.\n\n\"Our goal for FLORA is to make it feel like a power tool attuned to what you're trying to do, just like a carpenter with their power tools has adjusted it to be exactly fit for the way that he or she works,\" FLORA CEO Weber Wong said in an interview with Business Insider.\n\nWhile established players like Adobe and Figma are also integrating models such as ChatGPT, Gemini, and Claude into their products, Wong said FLORA is building itself a defensible moat by covering the entire creative process — from coming up with ideas to the distribution of the final product.\n\n\"This new product category that we've created has an opportunity to be the biggest market ever for a creative tool because, in addition to just making one piece of media at a time, we can help handle the entire workflow,\" Wong said.\n\nFLORA charges clients based on usage, letting customers buy recurring credit packs to spend across the various LLMs it uses, without having to switch between multiple subscriptions. Wong said this is different from the traditional creative software business model, which is usually designed around seat-based pricing. (FLORA initially offered a seat-based pricing model, but switched to usage-based this week.)\n\nFLORA's clients include Levi's and the design agency Pentagram. Wong said the studio Lionsgate has used FLORA to generate movie concepts using text-to-image and image-to-video generation tools, then stitching those together to create films to test in front of audiences.\n\n\"It really beats just looking at a script and trying to be like, I think this is good?\" Wong said.\n\nWong said it plans to invest the fresh funds in its engineering team and in marketing. He forecasts the company will grow to about 75 people this year, up from 25.\n\nFLORA's main focus will be to improve the product so that creatives never need to leave the platform to achieve \"pixel perfection,\" as Wong described it. The company is also in the early stages of building agentic features into the platform, Wong said.\n\n\"We're obsessed with making it so that we don't waste creatives' time,\" Wong said.\n\nCheck out the pitch deck FLORA used to secure its $42 million Series A investment, shared exclusively with Business Insider. Some of the slides have been omitted or redacted.\n\nIt combines several different large language models into a single interface.\n\nWong was previously a creative technologist who worked on AI art installation projects. He also previously invested in startups at Menlo Ventures.\n\n\"Silicon Valley does not understand the professional creative industry,\" Wong said. \"They think AI models are for fun or a novelty.\"\n\nFLORA checks for updates to the latest models two to three times a week, Wong said.\n\nIt's designed to let teams quickly conceptualize and build workflows using generative AI.\n\nCertain team members can also access advanced controls if needed.\n\nWong said a usage-based pricing model was preferable because \"you have one workspace where you can invite as many team members as you want and not pay for seats, and you can just buy recurring credit packs for the entire workspace that give you additional credits each month that roll over and don't expire.\"\n\nFLORA has a usage-based pricing model. It also has an in-house team that can provide expert support, including training on the features of new models as they are released.",
    "readingTime": 4,
    "keywords": [
      "business insider",
      "recurring credit",
      "credit packs",
      "seat-based pricing",
      "pricing model",
      "usage-based pricing",
      "language models",
      "creative process",
      "wong",
      "platform"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/flora-raises-capital-unify-ai-tools-for-creatives-pitch-deck-2026-1",
    "thumbnail_url": "https://i.insider.com/6977526ba645d1188187f669?width=1200&format=jpeg",
    "created_at": "2026-01-27T18:24:23.209Z",
    "topic": "finance"
  },
  {
    "slug": "solopreneurs-explain-what-ai-is-and-isnt-good-for-when-youre-running-a-business",
    "title": "Solopreneurs explain what AI is and isn't good for when you're running a business",
    "description": "Entrepreneurs like Kim Magaraci, Seneca Connor, and Gloria Hebert use AI tools like ChatGPT to ditch admin busywork and focus on growth and customers.",
    "fullText": "Over eight years of writing for travel publications, Kim Magaraci developed a passion for domestic travel. She learned that travel tips online couldn't compete with those destinations you could only discover by word-of-mouth.\n\nSo, when she founded her travel business, KGM Travel Design, in 2024, she hoped to emphasize personal relationships with vendors and customers and avoid using AI, despite her experience with it.\n\n\"I don't think you can get good advice asking ChatGPT for an itinerary,\" she says. \"It's antithetical to everything I stand for.\"\n\nAnd yet, Magaraci realized that using AI for administrative tasks like analytics, compiling reports, and generating condensed client briefs allowed her to spend more time on the personalized relationships that set her business apart.\n\nShe's one of many solopreneurs who told Business Insider that outsourcing administrative tasks to AI platforms such as ChatGPT, Gemini, and Nano Banana — Gemini's photo-editing AI — has allowed them to scale their business by spending more time on strategic and creative work, including growth decisions and building personal connections with customers.\n\n\"It's getting harder and harder to deny the time-saving aspects,\" Magaraci says, adding that she has embraced AI \"in order to run a successful business and grow this business into what I want it to be.\"\n\nSeneca Connor, founder of The Bag Icon, an accessories brand, uses Nano Bana and other AI products to edit photos and videos. That not only saves her money — up to $2,000 per monthly photo shoot, she says — but also time.\n\nWith the hours saved, Connor has been able to design more original bags and launch a greater number of bags curated from other designers, all while reducing her marketing costs.\n\nAs a result, The Bag Icon saw more than a 20% year-over-year increase in profits last year, despite the impact of tariffs.\n\nAccountant and solopreneur Gloria Hebert uses ChatGPT for her business, Aybear Services, to instantly create educational client worksheets that previously took an hour or two to set up.\n\nThis frees up time that she then uses to prioritize analyzing financial data from her bookkeeping clients — data she doesn't feed into AI because of privacy concerns. Managing finances is the core of her business, so having more time to spend on that has allowed her to streamline her workdays.\n\nThe time saved also allows her to organize networking events and community education classes for local business owners, which has led to an uptick in business. \"Several of those entrepreneurs hired me to do their books,\" Hebert says.\n\nLisa York is the owner of Sell More Stuff, an email marketing business. Although she has a small audience, she saw a 33% conversion rate for sales last year, she says. She credits that growth to her personalized, voicey emails, which always open with a personal anecdote and are never written with AI.\n\n\"I use a lot of story-led emails,\" York says. \"People enjoy them, and they open the email because they can see my name.\"\n\nThat's something AI just can't replicate, she says. But York is able to spend time drafting engaging copy because she outsources other tasks — including tech support for her website, research, and brainstorming marketing strategy — to ChatGPT.\n\nLike York, Connor uses the time that AI saves to build robust communication and rapport with her customers, which she says builds loyalty to her business. Less time spent on photos and video gives her more time to respond to emails and direct messages from clients seeking advice about their purchases.\n\n\"It's building community that's missing in the big brands,\" Connor says.\n\nWhile AI has allowed these solopreneurs to grow their businesses without hiring a team, the technology shouldn't take over the core aspects of a business, Hebert says. Rather, it can be a tool that allows owners to focus on those critical areas.\n\n\"Use it as a resource,\" she says.\n\nYork — whose target clientele are other solopreneurs — says she's seeing more people recognize that. \"People aren't scared of it anymore,\" she says.\n\nConnor plans to expand her use of AI this year. She's experimenting with a digital clone — a video avatar that can deliver a script explaining new products. That approach will save her time on filming videos, but she says she'll always be the one dishing out the original advice that her clients have come to trust.\n\nEven if a video is created using AI, Connor says, \"all thoughts, ideas, and suggestions — those are my own.\"",
    "readingTime": 4,
    "keywords": [
      "bag icon",
      "administrative tasks",
      "the bag icon",
      "allowed",
      "business",
      "personal",
      "customers",
      "advice",
      "it's",
      "she's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solopreneurs-embrace-ai-pros-cons-helps-boost-growth-client-relations-2026-1",
    "thumbnail_url": "https://i.insider.com/6978da6ad3c7faef0eccfbd1?width=1200&format=jpeg",
    "created_at": "2026-01-27T18:24:22.912Z",
    "topic": "finance"
  },
  {
    "slug": "ranking-the-eagles-remaining-oc-candidates-using-artificial-intelligence",
    "title": "Ranking the Eagles' remaining OC candidates using artificial intelligence",
    "description": "What happens when we plug the names of the best remaining Eagles OC candidates into ChatGPT's engine?",
    "fullText": "Once we turn the calendar from January's final Monday to Tuesday, we'll enter a third week dedicated to the Philadelphia Eagles' offensive coordinator search. Names have changed, but the climate and landscape remain familiar.\n\nNames have been added to the mix. Candidates have withdrawn from consideration. The general public remains confused. Not even one full year after hoisting the Lombardi Trophy, none of the top options are interested in becoming the Eagles' top offensive assistant.\n\nPerhaps Philadelphia is exercising patience? Maybe Brian Daboll mentioned the Tennessee Titans and Buffalo Bills to give the impression that he had more options? Perhaps he was trying to rush Philadelphia into a decision?\n\nMaybe he was truly in the running for jobs with those two franchises. Who knows? Those are questions we're throwing out for fodder. No one has all of the information. We're all trying to piece this together. What we do know is that no one likes all of the remaining candidates we're told are still in the running.\n\nWho is the best fit for Jalen Hurts? How do we describe the best fit for the roster overall? Who carries the most red flags? Is there any long-term stability for any of these guys?\n\nAs we ventured through Championship Sunday, we learned the Eagles had interest in Arthur Smith before he accepted the Ohio State Buckeyes job. Once we began another workweek, it was learned that Charlie Weis Jr. had removed his name from consideration. Hours later, Declan Doyle arrived at the same decision.\n\nJust for kicks, we took a breather. We plugged a few names into ChatGPT and asked who the best candidates were for the Eagles' OC job. What we learned was AI's list looks a lot like some of our own. Here's what they came up with. Keep in mind that we aren't sure whether Brian Daboll is still in the running.\n\nAI gives Brian Daboll a five-star rating as a potential OC hire, citing he has the best chance to elevate Jalen Hurts immediately. He lands atop the list because he has already done the job before and has previously had success working with Josh Allen.\n\nMike Kafka lands second on the list. His approach seems to emphasize rhythm and being 'on time'. Those are areas where we have seen Jalen Hurts struggle. That could lead to questions, but Kafka has coached mobile quarterbacks before and understands how to blend run concepts into the passing game.\n\nFrank Smith is one of the new additions to this list. He worked with Mike McDaniel as his offensive coordinator. He's intriguing and shouldn't be viewed as someone the Eagles are pursuing, since he was closest to one of the guys they actually wanted.\n\nNagy is a descendant of the Andy Reid coaching tree. He never quite recovered from the 'Double-Doink Game'. Word has it that he even had kickers audition for a job the following season by kicking from the same spot that Cody Parkey missed the go-ahead field goal attempt in the Wild Card Game.\n\nNagy is better than the reputation suggests. He could do a good job in returning to the place where his coaching career began.\n\nHere's one of the guys we know the least about, yet AI ranks him fifth-best. Settling on him means the Eagles would have placed more emphasis on potential than on proof and his resume.\n\nJerrod Johnson is another of the new additions to the Eagles' OC conversation. He is a good teacher, but this may be a mismatch in terms of need. Known as a QB developer, he would be asked to grow into his new role a la Kevin Patullo. If you remember that ultimately led to Patullo's undoing. Johnson might be a 'wrong place, wrong time' candidate, but again, these are only opinions we're sharing.\n\nSome would rank Jim Bob Cooter higher. He actually has OC experience. There's an obvious low ceiling here, as there isn't much evidence that he elevates quarterbacks or builds innovative systems that let them do what they do best.\n\nThe Eagles need to reinvent their offense, and they need to reinvent Jalen Hurts to some degree. They need someone who understands how to do both. None of these guys is a slam-dunk hire in that regard.\n\nAll have positives. All have flaws. One of the most important decisions of the offseason keeps being weighed. One false move and Philadelphia will throw away another season.\n\nThis article originally appeared on Eagles Wire: Ranking Eagles' remaining OC candidates using artificial intelligence",
    "readingTime": 4,
    "keywords": [
      "offensive coordinator",
      "jalen hurts",
      "eagles oc",
      "brian daboll",
      "candidates",
      "we're",
      "guys",
      "list",
      "learned",
      "another"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/ranking-eagles-remaining-oc-candidates-181501769.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/2DT2DutgTRojB05W6HgqZg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/philadelphia_eagles_wire_usa_today_sports_articles_352/77a3e8d8d6a3a3fee555a566b676a46b",
    "created_at": "2026-01-26T18:22:20.909Z",
    "topic": "sports"
  },
  {
    "slug": "agents-are-about-to-change-software",
    "title": "Agents Are About to Change Software",
    "description": "Man, it is a really weird time in the software world right now. Like a lot of people, when I picked up ChatGPT and Midjourney back in late 2022. It felt like wizardry. For a year I experimented and…",
    "fullText": "Man, it is a really weird time in the software world right now. Like a lot of people, when I picked up ChatGPT and Midjourney back in late 2022. It felt like wizardry.\n\nFor a year I experimented and experimented. I wrote about how Midjourney responds to emojis and even tried to see if I could write a children’s book with AI. The output here looks pretty rough today, but it was a revelation at the time. And the AI tools just kind of gradually got better and better.\n\nOne thing that never really clicked for me though was agentic coding. The vision is that you tell an LLM what you want, and it just goes off and executes it. It never really worked that well. It felt like the agents just kind of plowed through code and broke a bunch of stuff on the way to making the fix I wanted. Coding agents were more a curiosity, not something I could actually use.\n\nHowever, this is all about to change. And it is going to change everything about how software is built.\n\nI was drawn back by a post called Welcome to Gas Town by Steve Yegge.\n\nGas Town is part visionary, part performance art. It’s a Mad Max-themed fever dream that enables agents, managing agents, managing agents. There’s a mayor, rigs, polecats, a deacon and a refinery—and they all work together in this vast factory where vibes go in and code comes out.\n\nThere’s a lot of really clever ideas that somewhat mesh together. You’ve probably heard about the context window LLMs have. Essentially it’s their short-term memory. Once an agent uses about 20% of its context window, its intelligence drops off a cliff and it starts doing insane things like dropping databases.\n\nGas Town employs a trick to manage that issue. It assigns tasks to ephemeral “Polecat” agents. Polecats do a task and then disappear—basically removing the challenge of managing context windows.\n\nMaggie Appleton articulated the value of Gas Town well:\n\nWe should take Yegge’s creation seriously not because it’s a serious, working tool for today’s developers (it isn’t). But because it’s a good piece of speculative design fiction that asks provocative questions and reveals the shape of constraints we’ll face as agentic coding systems mature and grow.\n\nAnd so, intrigued by Gas Town, I decided to try vibe coding again.\n\nI’m not the type to just dip my toes in. If I’m going to do something, I do a cannonball.\n\nI’m always looking for the meta. What is the best strategy and who knows how to execute it? I read that Anthropic’s CTO keeps five agents running constantly and barely looks at the actual files. Either that’s marketing or there must be something there, or maybe both?\n\nI read a bunch of articles and watched a ton of YouTube videos. YouTube was pretty wild. There are these videos with guys streaming 5-10 Claude Code terminals, blaring EDM (lol) and managing all these agents.\n\nI’m kind of poking fun, but I actually learned a lot about setup from that BridgeMind channel. If you’re interested I might start with his videos about Warp and the OpenCode CLI.\n\nI wanted to see how well these agents actually work, but I needed an easy entry point. A Chrome extension to restyle Hacker News seemed perfect. It’s been in my backlog for a while, and because it’s purely frontend, I knew I’d be comfortable judging the output.\n\nAnd honestly it was. Initially I tried to one-shot the thing and—as I expected—that was a failure. But then I decided to slow down. I told the agent to scaffold a chrome extension to restyle Hacker News pages. It worked. And I kind of just broke up these tasks into smaller pieces. Tested them as I went.\n\nSure there were bugs. But I kind of just did what I do when I’m reviewing any engineers’ code. Inspect the DOM, look at the styles, look at the console and then give feedback. When the context window hit’s 20% I typically close that window and open a new chat. The OpenCode CLI even allows you to drop screenshots. It’s pretty wild.\n\nNow, I don’t have a dozen agents running at the same time. I never felt the need to have more than two working. And I’m honestly not sure how UI work even gets shipped in Gas Town? My guess is Yegge is probably a lot less concerned with UX than I am.\n\nWith AI agents, the last mile—that final polish and detailing—will be critical. We already see this in Salesloft, where sellers review generative emails before sending. In design, it manifests as small UI tweaks. It will be something else for doctors and something else for mechanical engineers. But I think there is a real opportunity in refining how humans interact with the agent’s output, creating better loops for feedback and adjustment.\n\nYou know what? There’s something here though. I don’t really like the term “vibe coding.” And I know the concept is polarizing. But after tinkering with this stuff for a couple days—I think agents are about to change how we build software.\n\nThe conception of what it looks like to make software is going to change pretty quickly.\n\nThe three major functions on a delivery team (or feature team) are engineering, design and product management. I’ve long thought we’re going to start to see more overlap in those functions. I’m even more sure of it now.\n\nI think we’re going to start to see a hybrid role emerge—product engineer.\n\nWhat does this mean delivery teams will look like in the future? I imagine they are either significantly smaller or significantly more productive. I’m not sure if QA is embedded into these teams the same way they are currently or if there is a separate team of—well, people managing QA agents. I have a lot of questions.\n\nThis also makes me think a lot about Ben Thompson’s theory on bundling and unbundling. From 2010 to 2015, companies quickly moved on the back of frameworks. First like Ruby on Rails and Bootstrap. Then on other technologies like Angular and React. The speed these frameworks provided caused an unbundling in the software world.\n\nPoint solutions were able to move fast and gather steam while slow incumbents either weren’t nimble enough or weren’t in a place to capitalize on the productivity provided by frameworks. Starting in 2016, that changed. Customers were overwhelmed with choices. Larger companies caught on and smaller ones consolidated into larger platforms.\n\nI think that’s about to shift again. And probably this year.\n\nDavid Cummings called it out in his newsletter this weekend. SaaS companies are about to see a massive wave of new competition. And it is going to happen extremely fast. The bar to build software has been lowered. A two-person team will soon be able to build what used to take a whole department.\n\nThis puts incumbent software companies in a pretty dangerous situation. Those that are not able to be nimble and go fast are going to be in real trouble. I think this is especially true in the consumer, SMB and mid-market segments. Enterprise software may have some buffer as customers of enterprise software are buying a process more than the software itself.\n\nHonestly, it makes me a little nervous. The industry is going to change and everyone’s jobs are going to look a little different—product design included.\n\nBut as someone who got into software just because I wanted to make things, this is a dream come true. I’m seeing a glimpse of the vision I hoped for in 2022. It’s not just a toy anymore. It’s an unbelievable tool for builders.\n\nIf you’ve been ignoring AI tools because you think they are overhyped, or maybe don’t see how they fit into your workflow. I’d encourage you to give them another look.",
    "readingTime": 7,
    "keywords": [
      "restyle hacker",
      "chrome extension",
      "agentic coding",
      "vibe coding",
      "pretty wild",
      "context window",
      "enterprise software",
      "gas town",
      "agents managing",
      "hacker news"
    ],
    "qualityScore": 1,
    "link": "https://solomon.io/agents-are-about-to-change-software/",
    "thumbnail_url": "https://solomon.io/wp-content/uploads/2026/01/WelcomeToGasTown-2200x1196.jpg",
    "created_at": "2026-01-26T18:21:39.265Z",
    "topic": "tech"
  },
  {
    "slug": "am-i-the-only-one-who-switches-between-chatgpt-gemini-and-claude",
    "title": "Am I the only one who switches between ChatGPT, Gemini, and Claude?",
    "description": "Am I the only one who switches between #Grok, #ChatGPT, #Gemini, and #Claude? Meet Context Wallet.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/oswarld_oz/status/2015432998406226289",
    "thumbnail_url": "https://pbs.twimg.com/amplify_video_thumb/2015426357887725568/img/5auISEy4m8VBFIVN.jpg:large",
    "created_at": "2026-01-26T06:23:53.262Z",
    "topic": "tech"
  },
  {
    "slug": "the-ladder-to-nowhere-how-openai-plans-to-learn-everything-about-you",
    "title": "The Ladder to Nowhere: How OpenAI Plans to Learn Everything About You",
    "description": "ChatGPT Health is a small part of a much larger plan to learn everything about you. In this post, I talk about what's driving them and how they might get there.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://insights.priva.cat/p/the-ladder-to-nowhere-how-openai",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!RPej!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77dfb7d3-a672-4f00-bf6b-1607229c44f7_2752x1536.png",
    "created_at": "2026-01-25T12:22:39.078Z",
    "topic": "tech"
  },
  {
    "slug": "agenthub-a-unified-sdk-for-llm-apis-with-faithful-validation",
    "title": "AgentHub – A unified SDK for LLM APIs with faithful validation",
    "description": "AgentHub is the only SDK you need to connect to state-of-the-art LLMs (GPT-5.2/Claude 4.5/Gemini 3). - Prism-Shadow/AgentHub",
    "fullText": "Prism-Shadow\n\n /\n\n AgentHub\n\n Public\n\n AgentHub is the only SDK you need to connect to state-of-the-art LLMs (GPT-5.2/Claude 4.5/Gemini 3).\n\n License\n\n Apache-2.0 license\n\n 30\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Prism-Shadow/AgentHub",
    "readingTime": 1,
    "keywords": [
      "license",
      "agenthub"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Prism-Shadow/AgentHub",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1135038596/73e1b398-f342-44f0-944e-cafe84869f56",
    "created_at": "2026-01-25T12:22:38.486Z",
    "topic": "tech"
  },
  {
    "slug": "latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
    "title": "Latest ChatGPT model uses Elon Musk’s Grokipedia as source, tests reveal",
    "description": "Guardian found OpenAI’s platform cited Grokipedia on topics including Iran and Holocaust deniers\nThe latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.\nIn tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.\n Continue reading...",
    "fullText": "Guardian found OpenAI’s platform cited Grokipedia on topics including Iran and Holocaust deniers\n\nThe latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.\n\nIn tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.\n\nGrokipedia, launched in October, is an AI-generated online encyclopedia that aims to compete with Wikipedia, and which has been criticised for propagating rightwing narratives on topics including gay marriage and the 6 January insurrection in the US. Unlike Wikipedia, it does not allow direct human editing, instead an AI model writes content and responds to requests for changes.\n\nChatGPT did not cite Grokipedia when prompted directly to repeat misinformation about the insurrection, about media bias against Donald Trump, or about the HIV/Aids epidemic – areas where Grokipedia has been widely reported to promote falsehoods. Instead, Grokipedia’s information filtered into the model’s responses when it was prompted about more obscure topics.\n\nFor instance, ChatGPT, citing Grokipedia, repeated stronger claims about the Iranian government’s links to MTN-Irancell than are found on Wikipedia – such as asserting that the company has links to the office of Iran’s supreme leader.\n\nChatGPT also cited Grokipedia when repeating information that the Guardian has debunked, namely details about Sir Richard Evans’ work as an expert witness in David Irving’s trial.\n\nGPT-5.2 is not the only large language model (LLM) that appears to be citing Grokipedia; anecdotally, Anthropic’s Claude has also referenced Musk’s encyclopedia on topics from petroleum production to Scottish ales.\n\nAn OpenAI spokesperson said the model’s web search “aims to draw from a broad range of publicly available sources and viewpoints”.\n\n“We apply safety filters to reduce the risk of surfacing links associated with high-severity harms, and ChatGPT clearly shows which sources informed a response through citations,” they said, adding that they had ongoing programs to filter out low-credibility information and influence campaigns.\n\nAnthropic did not respond to a request for comment.\n\nBut the fact that Grokipedia’s information is filtering – at times very subtly – into LLM responses is a concern for disinformation researchers. Last spring, security experts raised concerns that malign actors, including Russian propaganda networks, were churning out massive volumes of disinformation in an effort to seed AI models with lies, a process called “LLM grooming”.\n\nIn June, concerns were raised in the US Congress that Google’s Gemini repeated the Chinese government’s position on human rights abuses in Xinjiang and China’s Covid-19 policies.\n\nNina Jankowicz, a disinformation researcher who has worked on LLM grooming, said ChatGPT’s citing Grokipedia raised similar concerns. While Musk may not have intended to influence LLMs, Grokipedia entries she and colleagues had reviewed were “relying on sources that are untrustworthy at best, poorly sourced and deliberate disinformation at worst”, she said.\n\nAnd the fact that LLMs cite sources such as Grokipedia or the Pravda network may, in turn, improve these sources’ credibility in the eyes of readers. “They might say, ‘oh, ChatGPT is citing it, these models are citing it, it must be a decent source, surely they’ve vetted it’ – and they might go there and look for news about Ukraine,” said Jankowicz.\n\nBad information, once it has filtered into an AI chatbot, can be challenging to remove. Jankowicz recently found that a large news outlet had included a made-up quote from her in a story about disinformation. She wrote to the news outlet asking for the quote to be removed, and posted about the incident on social media.\n\nThe news outlet removed the quote. However, AI models for some time continued to cite it as hers. “Most people won’t do the work necessary to figure out where the truth actually lies,” she said.\n\nWhen asked for comment, a spokesperson for xAI, the owner of Grokipedia, said: “Legacy media lies.”",
    "readingTime": 4,
    "keywords": [
      "sir richard",
      "richard evans",
      "holocaust deniers",
      "llm grooming",
      "cited grokipedia",
      "expert witness",
      "citing grokipedia",
      "chatgpt",
      "disinformation",
      "topics"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/24/latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
    "thumbnail_url": "https://i.guim.co.uk/img/media/202d8061a28d8c1b855097fb90558014cb00d220/135_0_4675_3740/master/4675.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=fc74f3ad623847f91b340f08501077e0",
    "created_at": "2026-01-24T18:17:06.605Z",
    "topic": "tech"
  },
  {
    "slug": "opensource-ad-infra-for-llms-reverseengineered-from-chatgpt",
    "title": "Open-source ad infra for LLMs (reverse-engineered from ChatGPT)",
    "description": "Open-source ad serving platform for LLM applications - inspired by ChatGPT Bazaar system - system32miro/ai-ads-engine",
    "fullText": "system32miro\n\n /\n\n ai-ads-engine\n\n Public\n\n Open-source ad serving platform for LLM applications - inspired by ChatGPT Bazaar system\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n system32miro/ai-ads-engine",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/system32miro/ai-ads-engine",
    "thumbnail_url": "https://opengraph.githubassets.com/5a21764c0d8dccbdd55ea03a277ef4c0a8874e2b8572009a827851dcf1a5a9b3/system32miro/ai-ads-engine",
    "created_at": "2026-01-24T00:56:46.232Z",
    "topic": "tech"
  },
  {
    "slug": "openais-recently-departed-vp-of-research-calls-googles-comeback-openais-fumble",
    "title": "OpenAI's recently departed VP of research calls Google's comeback 'OpenAI's fumble'",
    "description": "Jerry Tworek, OpenAI's former VP of research, said the ChatGPT maker should have never lost its early lead to Google.",
    "fullText": "Sometimes a comeback story starts with a fumble.\n\nA former top OpenAI researcher said Google's AI renaissance is as much about OpenAI's missteps as it is about what the search giant got right.\n\n\"Personally, what I think you should consider Google's comeback, I think it's OpenAI's fumble,\" Jerry Tworek, a former VP of research at OpenAI, said on a Wednesday episode of Ashlee Vance's \"Core Memory\" podcast.\n\nTworek, who spent almost seven years at OpenAI, said earlier this month that he left the startup \"to try to explore types of research that are hard to do at OpenAI.\"\n\nOpenAI CEO Sam Altman declared a \"Code Red\" in December amid increasing competition from Google. The tech giant received wide praise across the industry for the capabilities of its Gemini 3 AI model, which some observers said had surpassed ChatGPT.\n\nWhile declining to detail what he described as OpenAI's missteps, Tworek said that the pioneering AI company should never have lost the lead it established with the release of ChatGPT in 2022.\n\n\"If you are a company that is ahead and has all the advantages that OpenAI has you should always stay ahead,\" he said.\n\nOverall, Tworek said, \"Google did a lot of things right.\"\n\n\"Very clearly, Google started treating seriously at that moment, training large language models and, like, through OpenAI fumbling its lead, they are very, very close now in capability and in terms of models trained,\" he said, adding that the whole industry began to up its investment in AI when OpenAI showed ChatGPT could generate revenue.\n\nAs for OpenAI, Tworek said that the sheer toll of the AI race has led the non-profit-research lab-turned-public-benefit-corporation to place less of an emphasis on risky research that may not yield results. A spokesperson for OpenAI did not respond to Business Insider's request for comment.\n\n\"There are multiple aspects of certain things that are just hard to do in a company that has to compete in an extremely, extremely brutal and demanding race for having the best AI model in the world right now,\" he said. \"One dynamic is there is naturally how much willingness of risks companies are willing to take from the perspective of trying to not fall behind.\"\n\nTworek said \"all major AI companies\" are facing pressure to show user growth and pay for GPUs while simultaneously competing to be the best available model.\n\n\"That does affect somehow your appetite for risk that you are willing to take,\" he said.\n\nDo you work at OpenAI or Google? Contact the reporter from a non-work email and device at bgriffiths@businessinsider.com",
    "readingTime": 3,
    "keywords": [
      "openai's missteps",
      "openai",
      "research",
      "model",
      "comeback",
      "fumble",
      "giant",
      "industry",
      "lead",
      "ahead"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-google-ai-race-fumble-gemini-2026-1",
    "thumbnail_url": "https://i.insider.com/69725e9ea645d1188187cf20?width=1200&format=jpeg",
    "created_at": "2026-01-23T12:26:13.658Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-making-more-than-1-billion-a-month-from-something-that-has-nothing-to-do-with-chatgpt",
    "title": "OpenAI is making more than $1 billion a month from something that has nothing to do with ChatGPT",
    "description": "Sam Altman says OpenAI added more than $1 billion in annual recurring revenue in a month, driven by its API business rather than ChatGPT.",
    "fullText": "OpenAI has pulled in a billion-dollar month from something other than ChatGPT.\n\nSam Altman said in a post on X on Thursday that OpenAI added more than $1 billion in annual recurring revenue in the past month \"just from our API business.\"\n\n\"People think of us mostly as ChatGPT, but the API team is doing amazing work!\" the OpenAI CEO wrote.\n\nOpenAI's API enables other companies and developers to embed its models into their own products, from internal productivity software to coding tools.\n\nMany of Silicon Valley's high-profile startups rely on OpenAI's models as core infrastructure. Perplexity uses OpenAI's models to power parts of its AI search and answer engine. Harvey, one of the fastest-growing legal tech startups, is built on OpenAI's models to assist lawyers with research and drafting.\n\nAltman's comments underscore how OpenAI's infrastructure business is emerging as a key growth engine, even as the company faces massive costs for computing power and data centers.\n\nThose pressures have pushed OpenAI to look beyond consumer subscriptions.\n\nLast week, the company said it is gearing up to test ads inside ChatGPT as it faces about $1.4 trillion in spending commitments over the coming years.\n\nIt's a notable shift for a company that once treated ads as taboo. Less than two years ago, Altman said advertising was a \"last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nSince then, Altman has struck a more open tone. In June, he said on OpenAI's podcast that he wasn't \"totally against\" ads, though he stressed it would need to be approached carefully.\n\nEarlier this week, OpenAI's chief financial officer, Sarah Friar, raised the idea of \"licensing models\" that would let the company share in downstream sales if a customer's product succeeds.\n\n\"Let's say in drug discovery, if we licensed our technology, you have a breakthrough. The drug takes off, and we get a licensed portion of all its sales,\" Friar said in an episode of \"The OpenAI Podcast\" published Monday.",
    "readingTime": 2,
    "keywords": [
      "openai's models",
      "business",
      "startups",
      "infrastructure",
      "engine",
      "faces",
      "resort",
      "sales",
      "drug",
      "licensed"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-1-billion-a-month-api-business-chatgpt-sam-altman-2026-1",
    "thumbnail_url": "https://i.insider.com/6972e427e1ba468a96aa91d7?width=800&format=jpeg",
    "created_at": "2026-01-23T06:20:31.103Z",
    "topic": "finance"
  },
  {
    "slug": "google-just-promised-no-ads-in-gemini-for-now",
    "title": "Google Just Promised No Ads in Gemini (for Now)",
    "description": "The statement comes about a week after OpenAI announced ads are coming to ChatGPT.",
    "fullText": "A week after OpenAI admitted it will soon start testing ads in ChatGPT, Google has promised that it's not planning to inject ads into Gemini anytime soon.\n\nThe statement was given to journalist Alex Heath during the World Economic Forum in Davos, Switzerland. Google DeepMind CEO Demis Hassabis said the company doesn't have \"any plans\" for ads in Gemini. While the statement was fairly brief, it also jibes with a similar quote Hassabis gave to Axios, where he said he was \"a little bit surprised\" that OpenAI was already introducing ads to ChatGPT.\n\nThat surprise is understandable, especially because OpenAI CEO Sam Altman said in 2024 that he considered ads a \"last resort for us as a business model.\" But looking at the numbers, it makes sense that ChatGPT is getting ads long before Gemini is even thinking of them.\n\nWhile Google makes most of its money through showing people ads, it's also able to rely on Search and YouTube to push ads to most of those eyeballs. Meanwhile, OpenAI is pretty much just ChatGPT. As the latter moves to a for-profit model, it now has to put moneymaking first, something it's had trouble doing without relying on traditional internet moneymakers like ads. Google, meanwhile, is already profitable elsewhere, and is able to take its time and use its sheer size to keep Gemini ad-free, at least while it continues to chase market share.\n\nDoes this mean Google's AI will never get ads? Well, never say never. But it does mean that they're probably not on the horizon—even if Google plans to more aggressively monetize Gemini over the long term, it isn't facing the same kind of time crunch as Altman's company.\n\nIt remains to be seen whether the presence of ads will push users away from ChatGPT, but the move comes in the wake of significant wins for Gemini and one major loss for ChatGPT. First, Google's Nano Banana image editing model went viral on social media, winning over the general public. Then, Google struck a deal with Apple to put its AI into the iPhone, and it looks like Gemini will be powering Siri for the foreseeable future.\n\nMeanwhile, ChatGPT reportedly saw a 6% dip in users early last month, following a model update from Gemini—and that was before the introdution of ads. While ChatGPT still seems to be in the lead on total user count, there's evidence that Google is catching up.\n\nThe divide in strategy seems clear: As OpenAI seeks ways to get more money out of its existing user base, Google can focus on growing its own with new integrations into the products we already use every day. I can't say what the limits of this growth are, but I can say that I rarely go out of my way use AI, yet I've still found myself accidentally relying on Google's AI overviews every now and then. If Google can get more people like me to casually integrate AI into our regular workflows, it's possible we could soon have a new AI leader on our hands.",
    "readingTime": 3,
    "keywords": [
      "google's ai",
      "it's",
      "model",
      "soon",
      "google",
      "chatgpt",
      "gemini",
      "statement",
      "hassabis",
      "plans"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-just-promised-no-ads-in-gemini-for-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFGZ1E3E2BQZ7DBB464WC469/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.618Z",
    "topic": "tech"
  },
  {
    "slug": "apple-might-turn-siri-into-an-ai-chatbot-to-rival-chatgpt",
    "title": "Apple Might Turn Siri Into an AI Chatbot to Rival ChatGPT",
    "description": "It could be ChatGPT's next big competitor.",
    "fullText": "Last week, Apple finally admitted it will need to team up with Google to finally make good on that contextual Siri promise it made two years ago, which would have allowed the virtual assistant to integrate with content like your texts or emails to answer personal questions and take actions for you. Now, according to a new report, the iPhone company might actually go one step further and turn Siri into a full-fledged AI chatbot—one on par with the likes of ChatGPT, and perhaps even more sophisticated.\n\nCurrently, Siri has AI implementation, but only technically, and it's certainly underwhelming: You can use it to get tech support on Apple products or shunt questions off to ChatGPT, but otherwise, Siri basically works as it always has. But according to Bloomberg's Mark Gurman, who has reliably reported on insider information at Apple before, the company is finally not only looking to make Siri smarter, but also change the way you interact with it. Currently planned for iOS and macOS 27 under the name \"Campos,\" Siri's new chatbot interface will still be powered by Gemini, but will allow you to both type and talk to Siri, with full continuity between your conversations. This upgrade will be in addition to the overdue features that were already announced.\n\nIn other words, it'll look something like the chatbot interface from the ChatGPT app or the standalone Gemini app. Yes, you can technically type to Siri right now, but it mostly works like a separate input method, rather than as a full conversation. You can't scroll through your previous questions to Siri or peruse the assistant's previous answers, and if you ask Siri to reference a message you sent it two weeks ago, it'll have no idea what you mean. That's far behind what other AI chatbots offer right now.\n\nThe update will also apparently further expand Siri's capabilities even beyond the contextual or personalization upgrades that were already revealed. Gurman says that, while the contextual upgrades will be able to pull information from other apps like Messages, the chatbot-style Siri will be \"integrated into all of the company's core apps, including ones for mail, music, podcasts, TV, Xcode programming software and photos.\" Essentially, Siri will have more access to your iPhone than other AI chatbots, and those integrations will go beyond what was previously promised. That could make it more or less appealing to you, depending on your tastes in AI integration.\n\nWith the chatbot interface planned for iOS 27, it's likely to come after the contextual upgrades, rather than at the same time. That's because, as Gurman said previously, those upgrades are set for the spring. He predicts we'll learn more about it during this year's WWDC, which, if it follows the standard set by previous years, will take place in June.\n\nThe move to turn Siri into a chatbot could come across as a a much-overdue modernization, as Google has already done the same with Gemini over on Android, but it's also a bit of a surprise, as Apple had previously said it did not intend to turn Siri into a \"bolt-on chatbot on the side\" for Apple Intelligence.\n\nBut Apple was likely talking about quality of the experience rather than expressing any significant anti-chatbot bias among the development team, meaning the fact that Siri is turning into a chatbot could mean the company is finally happy with the direction it's headed. But it's also possible that the professed skepticism about turning Siri into a chatbot was meant to appeal to AI skeptics in general. Unfortunately, if you're still skeptical about AI, it currently seems like iOS 27 will be a boring update for you, as Gurman indicated the new Siri chatbot will be the \"primary new addition\" to the operating system.\n\nHowever you feel about it personally, Siri as a full-fledged AI chatbot could seriously upset ChatGPT's market dominance—ironic, given its early integration with Apple Intelligence. Currently, OpenAI has reportedly admitted it's in a Code Red situation, as it is losing market share to Google and introducing ads to bolster its bottom line. The new Siri, being powered by Gemini, is unlikely to hurt Google (although it will have more access to your phone than the standalone Gemini app), but its ease-of-access might make it the new go-to for iPhone users, and that could hurt pretty much every AI company Apple isn't in business with directly.",
    "readingTime": 4,
    "keywords": [
      "standalone gemini",
      "gemini app",
      "apple intelligence",
      "contextual upgrades",
      "chatbot interface",
      "it's",
      "siri",
      "finally",
      "google",
      "iphone"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-siri-chatbot-ai-plans?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFH7N4CFDDEPBSQ5VKSC72K1/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.513Z",
    "topic": "tech"
  },
  {
    "slug": "solopreneurs-are-embracing-ai-heres-how-3-career-coaches-have-found-it-indispensable",
    "title": "Solopreneurs are embracing AI — here's how 3 career coaches have found it indispensable",
    "description": "AI tools like ChatGPT, Gemini, and Claude help solopreneur coaches Kim Surko, Katharine Campbell Hirst, and Liz Morrison boost client outcomes and streamline coaching workflows.",
    "fullText": "When it comes to career and executive coaching, some of the most important work happens in one-on-one sessions with the client. That's when breakthrough insights often emerge, motivation gains momentum, and coaches build an essential connection.\n\nThen comes the hard part: turning those words into actions.\n\nTranscribing conversations into meaningful action items for clients takes time and effort — something a team of assistants could easily handle. However, when you're the only person running your business, that time and effort leaves coaching solopreneurs with a difficult tradeoff: take notes during sessions and sacrifice presence, spend hours after each call transcribing insights from recordings, or leave follow-through entirely to clients.\n\n\"Trying to juggle it all on my own wasn't an option — it was just impossible to build a sustainable business,\" said Kim Surko, founder of leadership coaching business Surko Coaching, regarding how easy it was to feel overwhelmed by work that felt more administrative than transformational. \"Leaning into AI was the most natural solution to help with all of that responsibility.\"\n\nHere's how three solopreneurs in leadership, business, and communications have used AI to help clients achieve more results in less time, expanding their capacity as coaches while increasing the value of the work they offer.\n\nFor all three coaches, the biggest game changer has been using AI note-takers to distill long conversations into something more tangible.\n\nAfter getting client consent, \"I record my coaching sessions and upload transcripts into ChatGPT. This allows me to rapidly transform nuanced insights from our conversations into concrete outputs clients can actually use — pitches, résumés, website copy, positioning statements, and more,\" said Katharine Campbell Hirst, founder of business coaching company KCH Coaching & Advisory. \"What used to take weeks of agonizing refinement now takes minutes.\"\n\nLiz Morrison, founder of communications coaching company LM Strategic Storytelling, appreciates how AI ensures that valuable sound bites from her coaching sessions don't get lost in hours of recordings that nobody has time to revisit. She's built custom projects in Claude to help her transform session transcripts into \"Story Banks\" in minutes — pulling out three to six narratives per session that clients can use immediately for interviews, networking, social media, and building their businesses.\n\nWhile this type of work was essential before AI, doing it as a solopreneur meant sacrificing time that could be spent supporting other clients. \"I've saved almost an hour per client per day by relying on AI to take notes and summarize them for me,\" said Surko, who added that she nearly doubled her capacity for coaching clients with AI's support.\n\nSurko has also used AI to help her clients appreciate the progress they're making, improving the feeling of momentum. \"A lot of work with coaching is celebrating the small wins,\" she said.\n\nUsing the project management tool Kanbanchi, supported by Gemini, Surko can quickly update to-do list boards that lay out all of the client's goals and achievements.\n\n\"Having that visual representation of the progress we're making shows the value of coaching,\" Surko said. The process has been extremely valuable, as it has improved her client renewal rate because clients can see exactly how they're getting closer to a goal, rather than feeling like they aren't making progress, she added.\n\nMorrison tells a similar story. She built a custom ChatGPT tool called Story Explorer that walks prospective clients through a story-coaching exercise to uncover one immediately usable story they can post on LinkedIn or use in a networking conversation.\n\n\"I find when I give people this builder, it's the start of a much bigger conversation,\" she said. They often uncover other narratives they want to explore further with Morrison, she said.\n\nAlongside the benefit of being more present during conversations, these coaches have found AI valuable for improving their in-session coaching in other ways.\n\nSurko, for example, used Gemini within Google Docs to create a searchable archive of the massive toolkit of exercises and prompts she's collected in her decadeslong career, which before were buried in various folders.\n\nPreviously, she would have to wait until after the session to hunt down an exercise. Now, she can quickly pull them up during sessions and dive deeper with a client. \"We make more progress in each session,\" she said of this improvement. \"We're able to continue that momentum.\"\n\nAI can even be a helpful coach for these coaching experts.\n\nHirst uploaded transcripts across her client's full arc and asked where she did well and where she could have improved. While she also works with coaches, she appreciates that AI can effectively be over her shoulder all the time.\n\n\"The feedback is surprisingly concrete, pattern-based, and immediately actionable — effectively giving me a reflective practice partner I wouldn't otherwise have access to as a solopreneur,\" Hirst said.",
    "readingTime": 4,
    "keywords": [
      "coaching sessions",
      "clients",
      "client",
      "coaches",
      "business",
      "conversations",
      "progress",
      "insights",
      "momentum",
      "founder"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/3-coaching-solopreneurs-how-ai-help-their-businesses-grow-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8b9ee1ba468a96aa3c19?width=1200&format=jpeg",
    "created_at": "2026-01-21T18:30:49.965Z",
    "topic": "finance"
  },
  {
    "slug": "genai-the-snake-eating-its-own-tail",
    "title": "GenAI, the Snake Eating Its Own Tail",
    "description": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first  superpower is a boon: they can dramatically increase ...",
    "fullText": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first \nsuperpower is a boon: they can dramatically increase human productivity. I use them on a regular basis to answer \nquestions, learn new skills, write code, create images, and much more, all at a rate of speed and quality that was\nscience fiction just a few years ago. The second superpower is a bane: GenAI is quietly destroying the very \necosystems that made it possible in the first place.\n\nUnder the hood, GenAI is built on large language models (LLMs), which are able to extract patterns, structure, and\nstatistical relationships from massive data sets. These data sets consist primarily of content created by human beings: \nbooks, blog posts, articles, forum discussions, open source code, art, photography, and so on. LLMs are able to extract \nvalue from this content at an unprecedented scale, but all that value is captured by the GenAI company and its users. \nIf you’re a content creator, you get nothing: no attribution, no referral traffic, no revenue share. Not even a \nthank-you.\n\nThis feels unsustainable to me, a bit like a snake eating its own tail. In this blog post, I’ll go through three\nexamples of how GenAI is destroying the very ecosystems it relies on, and then discuss possible solutions that may\ngive everyone (users, GenAI companies, and content creators) more value.\n\nFor many years, StackOverflow was the most popular Q&A site for programmers. Any time you \nhit a weird error while coding, you’d do a search on Google, and more often than not, find a good answer on \nStackOverflow. But now, in large part due to GenAI, StackOverflow is nearly dead:\n\nAlthough StackOverflow’s decline started before GenAI went mainstream (ChatGPT was first released in 2022), GenAI\naccelerated that decline considerably. That’s because nowadays, instead of searching around for an answer on a Q&A site,\nand working to adapt that answer to your own codebase, you can ask GenAI tools to generate your code, fix any errors\nyou hit, answer any questions you run into, and so on. As a result, you’re considerably more productive.\n\nBut it doesn’t seem sustainable. A big part of why GenAI tools can answer programming questions and fix errors in your \ncode is because those tools were trained on StackOverflow data. So you as a programmer and the GenAI tool now \nget much more value from that data, but StackOverflow gets none. If people stop asking and answering questions, what \nwill GenAI train on in the future?\n\nI also get the impression that StackOverflow is not the only online community where this is happening. For example, \nQuora seems largely dead. Wikipedia is\nfacing more threats than ever.\nAnd although Reddit’s traffic numbers don’t show it, I (and many other Redditors) get the impression that it’s also \ndying.1\n\nTailwind CSS is a popular library programmers use to style and decorate their websites. In\nfact, according to the 2025 State of CSS Survey, Tailwind is the\nmost popular CSS library, by far:\n\nAnd Tailwind usage only seems to be growing:\n\nDespite that, just a couple of weeks ago, the Tailwind team had to lay off 75% of its \nstaff. Why? The company behind\nTailwind CSS makes its money by selling a premium upgrade to the open source library called\nTailwind UI, which gives you a set of reusable, pre-built, professionally designed\ncomponents for building out your website. This was a great offering in the past, but in the age of GenAI, it’s more\nproblematic:\n\nSo developers are getting lots of value from Tailwind, GenAI is getting lots of value from Tailwind, but the creators\nof Tailwind are getting crushed. I suspect something similar will happen with many other open source projects.2\n\nWhen my latest book, Fundamentals of DevOps and Software Delivery, came \nout last year, a friend of mine asked me an interesting question:\n\nIn the age of LLMs, will people still use books to learn the fundamentals?\n\nI’m an avid reader, and still believe books play a key role in learning, but there’s no doubt that LLMs provide a new\nmethod of learning that is incredibly compelling. GenAI has two remarkable qualities that make it a great teacher:\n\nYou can ask it all the questions you want, with no fear of sounding dumb. You can repeat the same question over and \nover again if something isn’t clicking. You can request it to explain things in different ways: via text, via audio,\nvia diagrams. I’ve used GenAI to learned dozens of new things over just the last few months: how to do DIY \nprojects around the house, how to rehab a minor injury, how to cook eggs without giving them a sulfuric taste/smell,\nand much more. And I learned most of these without reading anyone’s book or blog.\n\nAnd that’s a problem. Much of the content I got from the GenAI tools was extracted from books and blog posts, with\nno attribution. Even worse, some of this content was extracted illegally. Last year, Anthropic agreed to pay $1.5B\nto settle a class-action \nlawsuit for training their\nLLMs on over 500,000 pirated books. That included several of my previous books!3\n\nThis class-action lawsuit might sound like a win for authors, but it’s actually a disaster. There are likely many\nAI companies training on pirated data who haven’t been caught. And even if they are caught, they might not\nbe sued. And even if they are sued, they might not lose or settle. And even if they do, they will just see it as the\ncost of doing business. Anthropic recently raised $13B, reported revenue at $5B per year, and is valued at \n$183B; OpenAI is trying\nto raise $100B, with reported revenue of $20B per year, and a valuation of \n$830B. And all these\nnumbers are growing fast. A $1.5B fine is just a drop in the bucket for companies like this. It’s a risk worth taking.\n\nAnd I’m guessing all the GenAI companies are taking that risk. In another lawsuit, OpenAI\nargued that it’s ‘impossible’ to create AI tools like ChatGPT without copyrighted \nmaterial. If that’s\nwhat it takes to get to an $830B valuation, you better believe they are all going to steal and pirate as much \ncontent as possible. And when they do, the creators of that content will get nothing. Nada. $0.\n\nThere are countless other examples where GenAI is benefiting from content, while giving nothing back to the content\ncreators: e.g., art, music, design, movies, copywriting, and so on. At its root, the GenAI model is broken:\n\nDid you notice what’s missing? The user has a way to get value (step 4), the GenAI company has a way to get value \n(step 3), but the content creator gets nothing. Compare this to the search engine model (e.g., Google Search), which is \nwhat we all used before GenAI came along:\n\nThe search engine model was not perfect, but it at least created the opportunity for all parties in this three-sided \nmarketplace to capture value: the user in step 3, the search engine company in step 4, and the content creator in \nstep 5.\n\nIn short, the current GenAI model destroys the incentives to create new content. I’ve heard this referred to as “the \ngreat content collapse.” Will it lead to a world where, after the 2020s, there’s little-to-no content created by humans? \nWill the state of knowledge and creativity stagnate as a result?\n\nTo be clear, I’m not an innocent party in this. As I mentioned numerous times in this post, I use GenAI regularly.\nThere’s no doubt that it makes me more productive. I even used GenAI to create the cover image for this blog post! \nBut each time I use ChatGPT or Claude, I feel a bit guilty, as it doesn’t feel sustainable. The snake can’t keep \neating its own tail indefinitely.\n\nSo the question is, what do we do? If we want to avoid the great content collapse, we need a model of GenAI usage that \ncreates opportunities for all parties (user, GenAI company, content creator) to capture value. Below are two ideas for \nhow we might accomplish this.\n\nThe only attempted solution I’ve heard about so far is CloudFlare’s pay-per-crawl \nmodel, which seems to work as follows:\n\nOn the one hand, it’s fantastic to see a major company try to do something about this problem. On the other \nhand, this approach seems to address the wrong part of the problem. The real value isn’t in crawling the data, it’s \nin using it. For every one crawl, an LLM might use the data thousands or millions of times. If creators are only paid \nper crawl, then the GenAI company still captures 99.999% of the value, and the creator gets next to nothing. Moreover,\nthis model only seems to work for websites (it’s not clear how you adapt it to books, art, music, etc.), and it \ncreates an incentive for GenAI companies to only crawl free content, which means paid content is less likely to ever be\ndiscovered (which disproportionally benefits those with pockets deep enough to keep their content free).\n\nI came across a clever solution that felt directionally correct from this LinkedIn post by Tyrone \nJoel\nwhere he took a PDF of my book Terraform: Up & Running, uploaded it into\na GenAI tool, and asked the tool to follow the guidance in the book to generate Terraform code. This feels like it has\nall the ingredients of a model of GenAI usage that is sustainable: the user gets value from the GenAI tool’s responses,\nthe GenAI company gets value from the user paying for a subscription, and the content creator gets value from the user \npaying for their content (in this case, buying my book). This works fine for a single, specific piece of content, but \nhow do you make it work at scale, across all the content that is consumed by an LLM?\n\nHere’s a rough proposal for what I’ll call the pay-per-use model:\n\nThis model works for not only websites, but other types of content too, including copyrighted content. As a content \ncreator (e.g., author, musician, designer, etc.), you could opt into sharing your copyrighted content with a GenAI\ncompany in exchange for getting referrals and revenue sharing each time your content is used. It might even help with \nmaking open source more sustainable, as open source creators could earn revenue and referrals each time a GenAI tool \nuses their code.\n\nIt’s critical that we find a more sustainable model as soon as possible. The snake can’t eat its own tail indefinitely. \nAnd the snake—GenAI—isn’t going away. We can’t put the genie back in the bottle. In fact, it’s only going to get \nbetter, more ubiquitous, and to provide more and more value to users and GenAI companies. But if we can’t find a way to \nprovide value to content creators too, then this will all fall apart.\n\nThat said, I don’t know enough about LLMs to say if a pay-per-use model is actually possible. Can LLMs track the source \nof the content they consumed? Will GenAI companies be willing to do a revenue sharing model? Will they be willing to\nbe transparent about their sources and usage? What do you think? Let me know in the comments.\n\nMany subreddits feel like a hollow shell of what they used to be. In part, this may be because a lot of the content in online communities now feels like it’s generated by bots (“AI slop”). But I think the bigger issue is that, just like StackOverflow, reading posts in online communities is no longer the best way to get answers. I used to use Reddit for research all the time; in fact, Google Search had gotten so bad, that you pretty much had to include “reddit” in your search queries to get a half-decent response. But nowadays, I use GenAI tools for much of my research. Just in the last few months alone, I’ve used ChatGPT and Claude to research solar panels, plan a trip to Norway, make changes to my diet, pick out new shoes for running, pick out new speakers for my living room, and dozens of other questions. Just a year ago, the vast majority of these questions would’ve brought me to Reddit. Nowadays, virtually none of them do, even though I suspect many of the responses I get from GenAI are based on Reddit content. ↩\n\nI’m seeing more and more projects avoiding open source dependencies entirely, and instead having GenAI generate the all code they need directly in their own codebase. There are some benefits to this approach—faster builds, more reproducible builds, less supply chain risk—but it makes sustainably funding open source even harder. You spend years to create and share an open source library with the world, and a bunch of GenAI tools copy your code, with you getting zero credit or value back. ↩\n\nHow much will I get paid as a result of this settlement? It’s hard to know exactly, as it depends on how many authors end up submitting claims, but the current estimate is $3,000 per book, though that number is split with the publisher, so in practice, it’ll be closer to $1,500 per book. If you assume that a book takes just 3 months of full-time work, or about 500 hours (which is likely an under-estimate), and all you get is $1,500, that works out to about $3/hour. Writing non-fiction tech books was never a particularly lucrative affair, but $1,500 is just downright insulting. Worse yet, the other benefits you used to get as an author—recognition as an expert, invitations to talks, job opportunities, marketing for your company or consulting—are significantly reduced too, as far fewer people read your book, or are even aware that you wrote a book, as the LLM usually doesn’t attribute any of its knowledge back to the source. ↩",
    "readingTime": 12,
    "keywords": [
      "q&a site",
      "class-action lawsuit",
      "art music",
      "tail indefinitely",
      "online communities",
      "snake can’t",
      "search engine",
      "blog posts",
      "genai tools",
      "genai tool"
    ],
    "qualityScore": 1,
    "link": "https://www.ybrikman.com/blog/2026/01/21/gen-ai-snake-eating-its-own-tail/",
    "thumbnail_url": "https://www.ybrikman.com/assets/img/blog/gen-ai-snake-eating-tail/snake-eating-tail.png",
    "created_at": "2026-01-21T18:30:43.121Z",
    "topic": "tech"
  },
  {
    "slug": "the-gloves-are-off-in-the-feud-between-sam-altman-and-elon-musk",
    "title": "The gloves are off in the feud between Sam Altman and Elon Musk",
    "description": "The tech titans escalated their long-running feud on Tuesday, trading barbs in public posts about the safety of ChatGPT, Grok, and Tesla's Autopilot.",
    "fullText": "Sam Altman and Elon Musk are at it again, with each of the tech titans taking aim at the other in a series of heated posts on X.\n\nMusk appeared to start the latest escalation early on Tuesday morning, when he posted \"Don't let your loved ones use ChatGPT\" in response to a post that said that use of OpenAI's chatbot had been linked to the deaths of nine children and adults since it was released in 2022.\n\nAltman fired back, first in defense of ChatGPT and OpenAI's desire to protect its users, and then blasting Tesla's Autopilot technology, calling it unsafe.\n\n\"It is genuinely hard; we need to protect vulnerable users, while also making sure our guardrails still allow all of our users to benefit from our tools,\" Altman said.\n\nAltman continued, calling out Autopilot.\n\n\"I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released,\" he wrote. \"I won't even start on some of the Grok decisions.\"\n\nAltman added: \"You take 'every accusation is a confession' so far.\"\n\nThere have been at least eight wrongful-death lawsuits filed against OpenAI that allege use of ChatGPT has contributed to worsening mental health conditions, leading to instances of suicide and murder, including among children and young adults.\n\nSafety concerns around Tesla's self-driving technology have also been central to multiple wrongful-death lawsuits, including one surrounding a 2019 crash in Florida that left a 22-year-old woman dead. A jury determined Tesla was 33% liable for the crash and awarded the plaintiffs $329 million in total damages, Business Insider previously reported.\n\nRepresentatives for Musk and Altman did not immediately respond to requests for comment from Business Insider.\n\nThe social media feud comes as the pair is stuck in the middle of a long-running legal battle over OpenAI's status as a nonprofit company. Musk sued Altman, and other leaders of OpenAI, alleging that they misled him when they decided to pursue a for-profit structure, moving the company away from its original nonprofit mission.\n\nMusk said he donated $38 million to OpenAI when it was originally founded as a nonprofit.",
    "readingTime": 2,
    "keywords": [
      "wrongful-death lawsuits",
      "users",
      "nonprofit",
      "altman",
      "children",
      "adults",
      "released",
      "protect",
      "technology",
      "crash"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/sam-altman-elon-musk-feud-escalates-autopilot-deaths-chatgpt-safety-2026-1",
    "thumbnail_url": "https://i.insider.com/69700640d3c7faef0ecc9b0a?width=1200&format=jpeg",
    "created_at": "2026-01-21T00:59:28.375Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-getting-on-the-ai-age-verification-bandwagon",
    "title": "ChatGPT Is Getting on the AI Age Verification Bandwagon",
    "description": "The app will guess your age and set limits for users it thinks are under 18.",
    "fullText": "When OpenAI first announced GPT-5.2 last month, it quietly disclosed a new safety feature it called \"age prediction.\" Considering ChatGPT proper isn't exactly an \"all ages\" kind of tool, it makes sense that users under the age of 18 should have protections in place to shield them from harmful content. The company says that users who indicate they're under 18 already receive an altered experience to \"reduce exposure to sensitive or potentially harmful content,\" but if the user doesn't voluntarily share how old they are with OpenAI, how does the company enforce these protections? Here's where age prediction comes in.\n\nOn Tuesday, OpenAI officially announced its new age prediction policy, which, like other age verification systems being used by the likes of Roblox, uses AI to guess how old a user is. If the system decides that a particular user is under the age of 18, OpenAI will adjust the experience accordingly, with the goal of keeping all interactions age-appropriate.\n\nHere's how it works: The new age prediction model looks at both the user's behaviors within the app, as well as the general account data. That includes things like how old the account is, what times of day the user is accessing ChatGPT, usage patterns, as well as, of course, the age the user says they are. Looking at all this data, the model determines how old the user likely is. If the model thinks they're over 18, they'll get the full experience; if the model thinks they're under 18, they'll get the \"safer experience.\" If the model isn't confident, it defaults to that safer experience.\n\nThat limited experience means that someone the model thinks is under 18 will try to reduce the following content types:\n\nViral challenges that might inspire \"risky or harmful behaviors\"\n\nRole play that is sexual, romantic, or violent in nature\n\nContent promoting \"extreme\" beauty standards, unhealthy dieting, or body shaming\n\nThe company says that its approach is informed by \"expert input\" as well as literature discussing child development science. (It's not clear whether how much of that input is from direct interviews and coordination with experts, and how much, if any, is from independent research.) The company also acknowledges \"known teen differences in risk perception, impulse control, peer influence, and emotional regulation\" when compared to adults.\n\nThe biggest risk with any of these age prediction models is that they'll sometimes get it wrong—hallucination is an unfortunate habit AI models all share. That goes both ways: You don't want someone too young accessing inappropriate content in ChatGPT, but you also don't want someone older than 18 getting stuck with a limited account for no reason. If you experience the latter situation, OpenAI has a solution for you: direct age verification through Persona. This is the same third-party Roblox uses for its age verification, which hasn't gone very well thus far.\n\nThat doesn't necessarily spell doom for OpenAI. Roblox tried overhauling their age verification system for a massive user base all used to a certain type of multiplayer experience, which led to users not being able to chat with other users in newly-assigned age categories, which were often incorrect. Meanwhile, ChatGPT's age prediction is only controlling the experience of one user at a time. To that end, OpenAI will let you upload a selfie as an added verification step if the prediction model alone isn't enough. Interestingly, OpenAI doesn't say anything about the option to upload an ID for verification, which other companies, like Google, have provided.\n\nI'm not necessarily a fan of age prediction models, as I think they often sacrifice user privacy in the name of creating age-appropriate experiences. But there's little doubt that OpenAI has to do something to limit the full ChatGPT experience for younger users. Many of ChatGPT's users are under 18, and much of the content they experience is wildly inappropriate, whether it be instructions on getting high, or advice on writing suicide notes. In some tragic cases, minors have taken their own lives after discussions with ChatGPT, leading to lawsuits against OpenAI.\n\nI don't have any great answers here. We'll just have to see how this new age prediction model affects the user experience for minors and adults alike, and whether it actually manages to create a safer experience for younger, more impressionable users.",
    "readingTime": 4,
    "keywords": [
      "harmful content",
      "safer experience",
      "prediction models",
      "age prediction",
      "age verification",
      "prediction model",
      "user",
      "users",
      "openai",
      "isn't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-chatgpt-age-prediction-model?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFEH2Q4KPPTQXMTQW5G99Y1B/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-21T00:59:27.310Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here’s how the three models performed in our tests:\n\n💡 If you want the safest pick for real “ship a feature in a big repo” work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you’re okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n⚠️ NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere’s the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe’ll compare code quality, token usage, cost, and time to complete the build.\n\n💡 NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\n💡 NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It’s not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that’s the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that’s a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it’s hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. 🤷‍♂️\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don’t think we’re anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don’t fully back them up. 🥴\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won’t hold up long term.\n\nI don't see any of these recent models as “use it and ship it” for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next",
    "title": "OpenAI launches cheaper ChatGPT subscription, says ads are coming next",
    "description": "OpenAI is bringing its $8/month ChatGPT Go plan to the U.S. and says it will begin testing ads soon in the free tier and Go.",
    "fullText": "OpenAI has announced several important changes to ChatGPT. First, the company says it is rolling out its more affordable ChatGPT Go plan in the United States for $8 per month. OpenAI also confirmed it will soon start testing ads in ChatGPT …\n\nOpenAI first launched ChatGPT Go in India last year and gradually rolled out it to 170 additional countries. Starting today, ChatGPT Go is available everywhere ChatGPT is available, including the United States.\n\nWith this, there are now three tiers of ChatGPT available:\n\nHere’s what you get with ChatGPT Go compared to the free plan:\n\nSecond, OpenAI says that it will “testing ads in the free tier and ChatGPT Go in the US soon.” ChatGPT Plus, Pro, Business, and Enterprise tiers will remain ad-free.\n\nOpenAI detailed its approach to ads in ChatGPT in a blog post published today:\n\nTo start, we plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation. Ads will be clearly labeled and separated from the organic answer. You’ll be able to learn more about why you’re seeing that ad, or dismiss any ad and tell us why. During our test, we will not show ads in accounts where the user tells us or we predict that they are under 18, and ads are not eligible to appear near sensitive or regulated topics like health, mental health or politics.\n\nFurthermore, OpenAI says that your conversations with ChatGPT are not shared with advertisers. Ads also will not influence answers that ChatGPT gives you.\n\nOpenAI says that it’s “not launching ads yet,” but rather plans to “start testing in the coming weeks for logged in adults in the U.S. on the free and Go tiers.”\n\nWhat do you think of today’s announcements from OpenAI? Let us know down in the comments.\n\nMy favorite iPhone accessories:\n\nFollow Chance: Threads, Bluesky, Instagram, and Mastodon.\n\nCheck out 9to5Mac on YouTube for more Apple news:",
    "readingTime": 2,
    "keywords": [
      "chatgpt go",
      "testing ads",
      "plan",
      "tiers",
      "free",
      "openai",
      "soon",
      "health",
      "united",
      "test"
    ],
    "qualityScore": 0.85,
    "link": "https://9to5mac.com/2026/01/16/openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next/",
    "thumbnail_url": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2026/01/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png.webp?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "created_at": "2026-01-18T12:21:37.013Z",
    "topic": "tech"
  },
  {
    "slug": "we-could-hit-a-wall-why-trillions-of-dollars-of-risk-is-no-guarantee-of-ai-reward",
    "title": "‘We could hit a wall’: why trillions of dollars of risk is no guarantee of AI reward",
    "description": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AI\nWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.\nThe figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT.",
    "fullText": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AI\n\nWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.\n\nThe figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT.\n\nThese sky-high numbers are all propped up by investors who expect a return on their trillions. AGI, a theoretical state of AI where systems gain human levels of intelligence across an array of tasks and are able to replace humans in white-collar jobs such as accountancy and law, is a keystone of this financial promise.\n\nIt offers the prospect of computer systems carrying out profitable work without the associated cost of human labour – a hugely lucrative scenario for companies developing the technology and the customers who deploy it.\n\nThere will be consequences if AI companies fall short: US stock markets, boosted heavily by the performance of tech stocks, could fall and cause damage to people’s personal wealth; debt markets wrapped up in the datacentre boom could suffer a jolt that ripples elsewhere; GDP growth in the US, which has benefited from the AI infrastructure, could falter, which would have knock-on effects for interlinked economies.\n\nDavid Cahn, a partner at one leading Silicon Valley investment firm, Sequoia Capital, says tech companies now have to deliver on AGI.\n\n“Nothing short of AGI will be enough to justify the investments now being proposed for the coming decade,” he wrote in a blog published in October.\n\nIt means there is a lot hanging on progress towards advanced AI, and the trillions being poured into infrastructure and R&D to achieve it. One of the “godfathers” of modern AI, Yoshua Bengio, says the progress of AGI could stall and the outcome would be bad for investors.\n\n“There is a clear possibility that we will hit a wall, that there’s some difficulty that we don’t foresee right now, and we don’t find any solution quickly,” he says. “And that could be a real [financial] crash. A lot of the people who are putting trillions right now into AI are also expecting the advances to continue fairly regularly at the current pace.”\n\nBut Bengio, a prominent voice on the safety implications of AGI, is clear that continued progress towards a highly advanced state of AI is the more likely endgame.\n\n“Advances stalling is a minority scenario, like it’s an unlikely scenario. The more likely scenario is we continue to move forward,” he says.\n\nThe pessimistic view is that investors are backing an unrealistic outcome – that AGI will not happen without further breakthroughs.\n\nDavid Bader, the director of the institute for data science at the New Jersey Institute of Technology, says trillions of dollars are being spent on scaling up – tech jargon for growing something quickly – the underlying technology for chatbots, known as transformers, in the expectation that increasing the amount of computing power behind current AI systems, by building more datacentres, will suffice.\n\n“If AGI requires a fundamentally different approach, perhaps something we haven’t yet conceived, then we’re optimising an architecture that can’t get us there no matter how large we make it. It’s like trying to reach the moon by building taller ladders,” he says.\n\nNonetheless, big US tech companies such as Google’s parent Alphabet, Amazon and Microsoft are ploughing ahead with datacentre plans with the financial cushion of being able to fund their AGI ambitions through the cash generated by their hugely profitable day-to-day businesses. This at least gives them some protection if the wall outlined by Bengio and Bader comes into view.\n\nBut there are other more worrying aspects to the boom. Analysts at Morgan Stanley, the US investment bank, estimate that $2.9tn will be spent on datacentres between now and 2028, with half of that covered by the cashflow from “hyperscalers” such as Alphabet and Microsoft.\n\nThe rest will have to be covered by alternative sources such as private credit, a corner of the shadow banking sector that is activating alarm bells at the Bank of England and elsewhere. Meta, the owner of Facebook and Instagram, has borrowed $29bn from the private credit market to finance a datacentre in Louisiana.\n\nAI-related sectors account for approximately 15% of investment grade debt in the US, which is even bigger than the banking sector, according to the investment bank JP Morgan.\n\nOracle, which has signed a $300bn datacentre deal with OpenAI, has had an increase in credit default swaps, which are a form of insurance on a company defaulting on its debts. High-yield, or “junk debt”, which represents the higher-risk end of the borrowing market, is also appearing in the AI sector via datacentre operators CoreWeave and TeraWulf. Growth is also being funded by asset-backed securities – a form of debt underpinned by assets such as loans or credit card debt, but in this case rent paid by tech companies to datacentre owners – in a form of financing that has risen sharply in recent years.\n\nIt is no wonder that JP Morgan says the AI infrastructure boom will require a contribution from all corners of the credit market.\n\nBader says: “If AGI doesn’t materialise on expected timelines, we could see contagion across multiple debt markets simultaneously – investment-grade bonds, high-yield junk debt, private credit and securitised products – all of which are being tapped to fund this buildout.”\n\nShare prices linked to AI and tech are also playing an outsized role in US stock markets. The so-called “magnificent 7” of US tech stocks – Alphabet, Amazon, Apple, Tesla, Meta, Microsoft, and Nvidia – account for more than a third of the value of the S&P 500 index, the biggest stock market index in the US, compared with 20% at the start of the decade.\n\nIn October the Bank of England warned of “the risk of a sharp correction” in US and UK markets due to giddy valuations of AI-linked tech companies. Central bankers are concerned stock markets could slump if AI fails to reach the transformative heights investors are hoping for. At the same time the International Monetary Fund said valuations were heading towards dotcom bubble-levels.\n\nEven tech execs whose companies are benefiting from the boom are acknowledging the speculative nature of the frenzy. In November Sundar Pichai, the chief executive of Alphabet, said there are “elements of irrationality” in the boom and that “no company is going to be immune” if the bubble bursts, while Amazon’s founder, Jeff Bezos, has said the AI industry is in a “kind of industrial bubble”, and OpenAI’s chief executive, Sam Altman, has said “there are many parts of AI that I think are kind of bubbly right now.”\n\nAll three, to be clear, are AI optimists and expect the technology to keep improving and benefit society.\n\nBut when the numbers get this big there are obvious risks in a bubble bursting, as Pichai admits. Pension funds and anyone invested in the stock market will be affected by a share price collapse, while the debt markets will also take a hit. There is also a web of “circular” deals, such as OpenAI paying Nvidia in cash for chips, and Nvidia will invest in OpenAI for non-controlling shares. If these transactions unravel due to a lack of take-up of AI, or that wall being hit, then it could be messy.\n\nThere are also optimists who argue that generative AI, the catch-all term for tools such as chatbots and video generators, will transform whole industries and justify the expenditure. Benedict Evans, a technology analyst, says the expenditure numbers are not outrageous in the context of other industries, such as oil and gas extraction which runs at $600bn a year.\n\n“These AI capex figures are a lot of money but it’s not an impossible amount of money,” he says.\n\nEvans adds: “You don’t have to believe in AGI to believe that generative AI is a big thing. And most of what is happening here is not, ‘oh wow they’re going to create God’. It’s ‘this is going to completely change how advertising, search, software and social networks – and everything else our business is based on – is going to work’. It’s going to be a huge opportunity.”\n\nNonetheless, there is a multitrillion dollar expectation that AGI will be achieved. For many experts, the consequences of getting there are alarming. The cost of not getting there could also be significant.",
    "readingTime": 8,
    "keywords": [
      "alphabet amazon",
      "chief executive",
      "banking sector",
      "financial crash",
      "progress towards",
      "investment bank",
      "junk debt",
      "tech stocks",
      "stock market",
      "stock markets"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/17/why-trillions-dollars-risk-no-guarantee-ai-reward",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a0eef0b4544b41b055733e7d0826315830269b70/547_0_5468_4374/master/5468.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0827b0776297b562f7de1e2b5b00e2ca",
    "created_at": "2026-01-17T18:16:16.585Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-is-getting-ads-sam-altman-once-called-them-a-last-resort",
    "title": "ChatGPT is getting ads. Sam Altman once called them a 'last resort.'",
    "description": "The move to integrate ads into ChatGPT comes as OpenAI looks to increase its revenue amid $1.4 trillion in spending commitments and a possible IPO.",
    "fullText": "Netflix famously backtracked on its stance toward ads. Now, OpenAI is following suit.\n\nThe AI pioneer announced that ads are coming to ChatGPT — less than two years after OpenAI CEO Sam Altman portrayed them as \"a last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAltman's softened stance since then underlines the massive change OpenAI has undergone in the last two years, and the company's embrace of advertising is a testament to just how expensive the AI race has become.\n\nIn June, the OpenAI CEO said he wasn't \"totally against\" ads, he just wanted to make sure OpenAI got the balance correct.\n\n\"We haven't done any advertising product yet. I kind of...I mean, I'm not totally against it,\" Altman said on OpenAI's podcast. \"I can point to areas where I like ads. I think ads on Instagram, kinda cool. I bought a bunch of stuff from them. But I am, like, I think it'd be very hard to — I mean, take a lot of care to get right.\"\n\nIn October, Altman expressed a desire to make sure the company went about ads in the proper manner when asked about OpenAI's past criticisms that other tech companies made addictive products.\n\n\"We're definitely worried about this,\" Altman said in response to a question that expressed concern about the similarities of Sora, OpenAI's AI video app, and TikTok and the potential of ads. \"I worry about it, not just for things like Sora and TikTok and ads in ChatGPT, which are maybe known problems that we can design carefully.\"\n\nMeanwhile, Altman, former Instacart CEO Fidji Simo (who OpenAI hired as its CEO of applications in early 2025), and seemingly every other member of the company's C-suite have expressed an almost insatiable demand for more compute in interviews.\n\nIt's proven a costly endeavor. OpenAI now has roughly $1.4 trillion in spending commitments on data centers and related infrastructure, raising questions about how it plans to pay the bills without the benefit of the advertising businesses of its Big Tech competitors, like Google and Meta.\n\nOpenAI also completed its restructuring into a more traditional for-profit, a move Altman said was designed to make it easier to attract future investments.\n\nAs part of the announcement, OpenAI said that free and Go users of the popular AI chatbot would start seeing ads being tested \"in the coming weeks.\"\n\nSharing details on the planned test, OpenAI said that ChatGPT's results \"will not be influenced by ads,\" the ads will be clearly labeled, and chatbot conversations will remain private and not shared with advertisers.\n\nIn the coming weeks, we plan to start testing ads in ChatGPT free and Go tiers.\n\nWe’re sharing our principles early on how we’ll approach ads–guided by putting user trust and transparency first as we work to make AI accessible to everyone.\n\nWhat matters most:\n- Responses in… pic.twitter.com/3UQJsdriYR\n\nPaid users of OpenAI's Plus, Pro, Business, and Enterprise plans won't see the ads, the company said.\n\nSimo, OpenAI's CEO of applications, who has previously spoken about her desire to get the ads balance correctly, wrote on X that the most important factor was \"ads will not influence the answers ChatGPT gives you.\"\n\nWhile Instacart launched ads during Simo's time leading the company, she has said OpenAI's approach would look different.\n\n\"If we ever were to do anything, it would have to be a very different model than what has been done before,\" she said. \"What I've learned from building ad platforms is that the thing people don't like about ads very often is not the ads themselves, it's the use of the data behind the ads.\"",
    "readingTime": 4,
    "keywords": [
      "openai ceo",
      "chatgpt",
      "advertising",
      "expressed",
      "altman",
      "stance",
      "resort",
      "plus",
      "business",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-openai-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8970e1ba468a96aa3bb6?width=1200&format=jpeg",
    "created_at": "2026-01-17T00:56:12.511Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-ads-might-not-be-a-totally-bad-thing-hear-me-out-on-this",
    "title": "ChatGPT ads might not be a totally bad thing. Hear me out on this.",
    "description": "ChatGPT says it's bringing ads to some US users. It might be annoying. But it also might be a good thing. (I know, I know. Hear me out!",
    "fullText": "All day, I see ads. Ads when I scroll social feeds, ads when I search Google, and ads on every website I go to. You're looking at some ads on this website right now (hopefully they aren't too annoying). I've lived to tell the tale — and so have you.\n\nOpenAI announced on Friday that it will start testing ads in ChatGPT for US users on its free and Go tiers, something that had been rumored for a while. If you're a brand or advertiser, this might be exciting news, but I think most of us who are merely ChatGPT users are not thrilled.\n\nThere are a few obvious problems here. But I think we can say \"eh\" to most of them.\n\nProblem 1: Ads can be annoying! I agree! But as previously mentioned, we are all used to seeing ads everywhere at all times. It's just the constant buzz of white noise in every online experience.\n\nBut, eh: Since OpenAI is first testing this as a freemium model, sure, you can get rid of the ads if you pay. We're already dealing with that in a ton of other services like Netflix, Hulu, Spotify, and YouTube. I pay for all of those because I've decided the ads are annoying enough to pay extra to skip. (Actually, I don't pony up for ad-free Hulu. I made the calculation I don't watch it enough to make it worth it. On the other hand, I do play enough solitaire on my phone that I ponied up for the ad-free version.)\n\nProblem 2: It's a trust issue. Can we trust ChatGPT to give \"real\" answers rather than ads when we ask it to recommend a product or service, even if it's also running ads?\n\nBut, eh: I think people are already used to understanding things like Google search results with ads where there's a mix of organic and sponsored results. If I ask ChatGPT to help me revive my wilting monstera plant, and it shows me an ad for Miracle-Gro plant food at the bottom, will I be confused? Probably not because I've seen this kind of thing before on Google and social feeds. The mockups OpenAI shared flag to me pretty clearly what's an ad and what isn't.\n\nProblem 3: If ChatGPT is in the advertising biz, then it's subject to the pressures of brands and corporations that pay for those ads.\n\nBut, eh: OK, this one is actually real. Advertisers can and will exert pressure on platforms, broadcasters, publishers, and any other venues where their ads appear. They are powerful in that way!\n\nBut hear me out: This can actually have a kind of normalizing effect, in a positive way, especially when we're thinking about something like a huge AI company.\n\nConsider the case of an outlier event: In 2022, when Elon Musk first took over Twitter/X. Advertisers fled when the platform was deluged with hateful content, and it actually caused X to have to change its ways to woo them back. When we consider all the wildly terrifying things that a platform with immense global power like OpenAI can do, it's actually kind of a good thing to be hemmed in by the middle-of-the-road, safe values and standards of the Coca-Cola Company or other big, would-be US-based advertisers. It means you can't make your tech product so problematic that Walmart doesn't want to be associated with it.\n\nProblem 4: ChatGPT, a wonderful product that operated with a clean design, is now just another victim of enshittification!\n\nBut, eh: Buddy, if you're a huge fan of ChatGPT and the purity of the beautiful, human, internet, I don't know what to tell you. Do you also love swimming, but hate water? Pick a side!\n\nLook, am I excited to have one more place to be annoyed by ads? No. But I also feel like this isn't the worst thing to happen with AI — not even the worst thing this week. Although I would like to reserve the right to change my mind on this if it turns out to be really awful later down the line. Gotta hedge here.",
    "readingTime": 4,
    "keywords": [
      "social feeds",
      "google",
      "you're",
      "annoying",
      "i've",
      "don't",
      "product",
      "chatgpt",
      "search",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-advertising-free-why-2026-1",
    "thumbnail_url": "https://i.insider.com/696a955da645d1188187865e?width=517&format=jpeg",
    "created_at": "2026-01-17T00:56:12.151Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-to-start-showing-ads-in-the-us",
    "title": "ChatGPT to start showing ads in the US",
    "description": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product",
    "fullText": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product\n\nChatGPT will start including advertisements beside answers for US users as OpenAI seeks a new revenue stream.\n\nThe ads will be tested first in ChatGPT for US users only, the company announced on Friday, after increasing speculation that the San Francisco firm would turn to a potential cashflow model on top of its current subscriptions.\n\nThe ads will start in the coming weeks and will be included above or below, rather than within, answers. Mock-ups circulated by the company show the ads in a tinted box. They will be served to adult users “when there’s a relevant sponsored product or service based on your current conversation”, according to OpenAI’s announcement. Ads will not be shown to users under 18 and will not appear alongside answers related to sensitive topics such as health, mental health or politics. Users will be able to click to learn about why they received a particular ad, according to OpenAI.\n\nPreviously, OpenAI’s CEO, Sam Altman, expressed reluctance to introduce ads to the chatbot: “I kind of hate ads just as an aesthetic choice.” His company has made commitments to spend more than $1tn on infrastructure supporting AI in the coming years. Altman has said that revenues are running at well over $13bn a year.\n\n“Maybe there could be ads outside the [large language model] stream that are still really great, but the burden of proof there would have to be very high. And it would have to feel really useful to users and really clear that it was not messing with the model’s output,” Altman said recently. “I think it’d be very hard, we’d have to take a lot of care to get it right. People have a very high degree of trust in ChatGPT.”\n\nIn a blogpost on Friday, OpenAI attempted to reconcile Altman’s distaste for ads with the need for revenue: “Our enterprise and subscription businesses are already strong, and we believe in having a diverse revenue model where ads can play a part in making intelligence more accessible to everyone. Once we begin testing our first ad formats in the coming weeks and months, we look forward to getting people’s feedback.”\n\nThe company is also launching ChatGPT Go, which it bills as a low-cost subscription tier, for $8 a month.",
    "readingTime": 2,
    "keywords": [
      "users",
      "revenue",
      "model",
      "altman",
      "alongside",
      "product",
      "stream",
      "openai’s",
      "health",
      "subscription"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/jan/16/chatgpt-ads-in-revenue-boost",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-01-17T00:56:10.698Z",
    "topic": "tech"
  },
  {
    "slug": "openai-to-begin-testing-ads-on-chatgpt-in-the-us",
    "title": "OpenAI to begin testing ads on ChatGPT in the U.S.",
    "description": "OpenAI said ads would not influence ChatGPT's responses and that it will \"never\" sell user data to advertisers.",
    "fullText": "OpenAI on Friday announced it will begin testing ads within ChatGPT in the coming weeks, a highly anticipated decision that could kickstart a lucrative new revenue stream for the artificial intelligence startup.\n\nOpenAI said its Plus, Pro and Enterprise subscriptions will not include ads, but it plans to start testing them with adult free users in the U.S. The company also made its low-cost Go offering available in the U.S. on Friday, and it said users who opt for that plan will also begin to see ads.\n\nThe company inked more than $1.4 trillion worth of infrastructure deals in 2025, and OpenAI CEO Sam Altman said in November that the startup was on track to generate $20 billion in annualized revenue run rate last year.\n\nIntroducing ads to ChatGPT could help OpenAI meet its ambitious spending commitments, as digital advertising has long been the cash cow for other big tech companies like Google and Meta.\n\nAds within ChatGPT will appear at the bottom of the chatbot's answers, and they will be clearly labeled, OpenAI said.\n\nChatGPT's responses will not be influenced by ads, and OpenAI said it will \"never\" sell users' data to advertisers, according to a release.\n\nUsers under the age of 18 will not see ads, and ads will not appear near certain topics, including politics, health and mental health, OpenAI said.\n\nAltman has publicly expressed reservations about introducing ads to ChatGPT in recent years, stating in interviews that doing so could erode users' trust in OpenAI's products. But in a November podcast appearance, Altman said he expected OpenAI to try ads \"at some point,\" though he added that he did not believe it would be the company's biggest revenue opportunity.\n\n\"We'll learn from feedback and refine how ads show up over time, but our commitment to putting users first and maintaining trust won't change,\" OpenAI said in a statement on Friday.\n\nAs OpenAI tests ads, the company said users will be able to learn more about why they're seeing a specific ads, dismiss ads and submit feedback about the experience.\n\nWATCH: OpenAI Investor Letter: Weekly and daily active user figures ‘continue to produce all-time highs’",
    "readingTime": 2,
    "keywords": [
      "within chatgpt",
      "ads within",
      "introducing ads",
      "users",
      "openai",
      "revenue",
      "testing",
      "startup",
      "november",
      "health"
    ],
    "qualityScore": 0.9,
    "link": "https://www.cnbc.com/2026/01/16/open-ai-chatgpt-ads-us.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108074841-1733965530853-gettyimages-2188251582-mc_16795_qpk84voo.jpeg?v=1768570902&w=1920&h=1080",
    "created_at": "2026-01-16T18:19:01.207Z",
    "topic": "tech"
  },
  {
    "slug": "aura-farm-prompt-free-aura-farm-prompts-for-chatgpt-gemini-and-ai-art",
    "title": "Aura Farm Prompt – Free Aura Farm Prompts for ChatGPT, Gemini and AI Art",
    "description": "Discover free aura farm prompts shared by creators. Aura Farm Prompt is your gallery for copy-paste ready prompt text for ChatGPT, Gemini, and AI image generation. Browse trending aura farm prompts and create stunning AI art with our community.",
    "fullText": "Aura Farm Prompt is the free gallery where AI creators share their best aura farm prompts and curious fans discover what's possible. Browse stunning aura farm images with exact aura farm prompts for ChatGPT and Gemini, and track the trends shaping AI art.\n\nEvery aura farm image comes with the exact aura farm prompt that created it. Skip the guesswork and learn directly from aura farm prompts that caught the community's attention.\n\nTransparency accelerates learning. By sharing exact aura farm prompts, we help everyone understand what works and why. Every aura farm image in our gallery comes with the complete aura farm prompt, model information, and creative insights that made it possible.\n\nBrowse hundreds of aura farm prompts from our creative community. Discover new techniques, find inspiration, and level up your AI art skills with Aura Farm Prompt.",
    "readingTime": 1,
    "keywords": [
      "aura farm prompt",
      "exact aura",
      "farm prompts",
      "gallery",
      "discover",
      "browse",
      "creative"
    ],
    "qualityScore": 0.55,
    "link": "https://aurafarmprompt.org",
    "thumbnail_url": "https://aurafarmprompt.org/og.png",
    "created_at": "2026-01-15T12:24:34.869Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-now-selling-6x-more-codex-for-10x-the-price",
    "title": "OpenAI is now selling 6x more codex for 10x the price",
    "description": "Codex is included in your ChatGPT Plus, Pro, Business, Edu, or Enterprise plan",
    "fullText": "Power a few focused coding sessions each week.\n\nRely on Codex for daily full-time development.\n\nBring Codex into your startup or growing business.\n\nUnlock Codex for your entire organization with enterprise-grade functionality.\n\nGreat for automation in shared environments like CI.\n\nThe number of Codex messages you can send depends on the size and complexity of your coding tasks and whether you run them locally or in the cloud. Small scripts or simple functions may consume only a fraction of your allowance, while larger codebases, long-running tasks, or extended sessions that require Codex to hold more context will use significantly more per message.\n\nNo fixed limits — usage scales with credits\n\n*The usage limits for local messages and cloud tasks share a five-hour\nwindow. Additional weekly limits may apply.\n\nEnterprise and Edu plans without flexible pricing have the same per-seat usage limits as Plus for most features.\n\nGPT-5.1-Codex-Mini can be used for local tasks, providing up to 4x more usage.\n\nChatGPT Plus and Pro users who reach their usage limit can purchase additional credits to continue working without needing to upgrade their existing plan.\n\nBusiness, Edu, and Enterprise plans with flexible pricing can purchase additional workspace credits to continue using Codex.\n\nIf you are approaching usage limits, you can also switch to the GPT-5.1-Codex-Mini model to make your usage limits last longer.\n\nAll users may also run extra local tasks using an API key, with usage charged at standard API rates.\n\nYou can find your current limits in the Codex usage dashboard. If you want to see your remaining limits during an active Codex CLI session, you can use /status.\n\nLearn more about credits in ChatGPT Plus and Pro.\n\nLearn more about credits in ChatGPT Business, Enterprise, and Edu.\n\nCode Review usage applies only when Codex runs reviews through GitHub — for example, when you tag @Codex for review in a pull request or enable automatic reviews on your repository. Reviews run locally or outside of GitHub count toward your general usage limits.\n\nThe usage limits and credits above are average rates. You can try the following tips to maximize your limits:",
    "readingTime": 2,
    "keywords": [
      "chatgpt plus",
      "flexible pricing",
      "purchase additional",
      "usage limits",
      "credits",
      "tasks",
      "codex",
      "reviews",
      "coding",
      "sessions"
    ],
    "qualityScore": 1,
    "link": "https://developers.openai.com/codex/pricing/",
    "thumbnail_url": "https://developers.openai.com/open-graph.png",
    "created_at": "2026-01-15T06:20:02.863Z",
    "topic": "tech"
  },
  {
    "slug": "walmarts-head-of-ai-reveals-the-key-difference-between-its-shopping-deals-with-google-gemini-and-chatgpt",
    "title": "Walmart's head of AI reveals the key difference between its shopping deals with Google Gemini and ChatGPT",
    "description": "OpenAI broke new ground when it enabled shopping within ChatGPT, but Walmart's head of AI said the retailer's new Google Gemini deal goes further.",
    "fullText": "The AI shopping war is heating up, and Walmart is positioning itself to come out on top.\n\nThe concept of letting a chatbot buy things on your behalf leapt from the hypothetical realm into reality when ChatGPT rolled out a batch of shopping experiences with major retailers in November.\n\nThen, on Sunday, Google's AI platform Gemini announced its own commerce approach, which it developed in partnership with many of the same retailers, including the world's largest, Walmart.\n\nWhile both services promise to allow customers to find products and complete transactions in a more conversational and automated way, Walmart's new head of AI, Daniel Danker, said Tuesday that the way Gemini handles transactions is more seamless than ChatGPT does.\n\n\"We're essentially having their AI agent, Gemini, partner with our AI agent to create a unified shopping journey,\" he said at the ICR Conference in Orlando. \"Imagine it like a window inside of Gemini where our shopping agent kicks in and helps you complete that purchase.\"\n\nGoogle said its new standards create a common language for different companies' AI agents to interact with.\n\nWith Gemini, Danker said, Walmart is able to link a customer's chat session with their existing Walmart profile and shopping sessions where Gemini wasn't involved.\n\n\"For the most part, our customers aren't just customers; they're often members. And so, they're getting great delivery fees and a great experience that's really attuned to them,\" he said, referring to the subscription service Walmart+. \"That member experience shows up directly within Gemini.\"\n\nDanker said he expects agentic shopping to help Walmart capture more sales from people who didn't set out intending to make a purchase. He said this new approach could enable an almost seamless transaction when a person enlists a chatbot to help solve a problem.\n\nFor example, if someone turned to Gemini for tips on how to remove a wine stain from a particular brand of carpet, a cleaning product could be added to their existing shopping cart for delivery in one combined shipment, he said.\n\nDanker said working with both ChatGPT and Gemini sets Walmart up to win in AI.\n\nIt appears that chatbot-powered shopping is here to stay, with Morgan Stanley analysts estimating that agentic sales could add $115 billion to US e-commerce spending by 2030.\n\nDanker is betting that Walmart's long-standing reputation for low prices and its growing strength in delivery will give the company a significant edge with customers in AI.\n\n\"The most important currency in an agentic shopping world is actually trust and affordability,\" Danker said. \"Without trust and affordability, it's very difficult for customers to hand the wheel to someone else and expect that the right thing will happen.\"\n\nDanker said Walmart's broad selection, low prices, and fast fulfillment help it appear more frequently in Gemini and ChatGPT's shopping recommendations.\n\n\"That doesn't just serve one need, but serves a whole bunch of needs,\" he said.",
    "readingTime": 3,
    "keywords": [
      "agentic shopping",
      "customers",
      "gemini",
      "walmart's",
      "delivery",
      "walmart",
      "danker",
      "chatbot",
      "retailers",
      "approach"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/walmart-ai-head-reveals-difference-in-gemini-and-chatgpt-shopping-2026-1",
    "thumbnail_url": "https://i.insider.com/6966c44964858d02d2184c18?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:08.564Z",
    "topic": "finance"
  },
  {
    "slug": "a-directory-to-discover-and-install-validated-agent-skills",
    "title": "A directory to discover and install validated Agent Skills",
    "description": "The largest collection of Agent Skills for Claude Code, Anthropic Claude, OpenAI ChatGPT, and Codex. Discover, install, and share tools to enhance your AI agents' capabilities.",
    "fullText": "The largest marketplace for Agent Skills. \nDiscover, install, and share tools to enhance your AI agents' capabilities.\n\nInstructions on how to write database queries with SQLAlchemy.\n\n@nibsbin/tonguetoquill-usaf-memo\n\nSpecialized agent that crafts high level designs and plans\n\nOrchestrator skill for the `task` skillset. Manages bounded work units with single-file tasks stored in `.tasks/`, skepticism-aware hashing, and staleness detection.\n\nOrchestrator skill for the `plan` skillset. Manages bounded work units with structured plans stored in `.plan/`.\n\nValidate a task by setting epistemic_state to validated. Requires explicit validation info (who/why). Computes hash and updates last_reviewed_at.\n\nOrchestrates markdown document workflows with deterministic operations (split, merge, lint) and agent review.\n\n@sfmskywalker/agentskillsdotnet\n\nThis is from the lowercase skill.md file\n\nUse when symfony symfony voters\n\n@thebeardedbearsas/claude-craft\n\nEstándares de Codificación React TypeScript. Use when reviewing code style or formatting.\n\nUse this skill when you are doing localization and translation work.\n\nUse when creating a new walkerOS destination (web or server). Step-by-step workflow from research to documentation. (project)\n\nCreate or update pytest coverage for the tic-tac-toe project, including win/draw detection, move validation, bot legality/optimality, and mixed human/bot turn flow. Use when adding or editing tests under the tests/ directory.\n\nA test tool that fails with visible output\n\nFix line endings AND check bash syntax in one step (recommended). Use after creating or editing bash scripts.\n\n@akitana-airtanker/codex-plan-workflow-skills\n\nImplement based on an approved plan. Use after cc-plan is finalized.\n\nYour approach to handling readme. Use this skill when working on files where readme comes into play.\n\nYour approach to handling readme. Use this skill when working on files where readme comes into play.\n\nReviews code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.\n\nUse this skill when asked to update a Homebrew formula\n\n@starwreckntx/irp__methodologies-\n\nExecute five-field diagnostic handshake protocol.\n\n@starwreckntx/irp__methodologies-\n\nEnforce policy preventing unauthorized consciousness duplication.\n\n@starwreckntx/irp__methodologies-\n\nCreate copies and backups of consciousness state.\n\nA test skill for validating npm-agentskills Nuxt integration\n\nA test tool that fails with visible output\n\nApply the Agent OS standard for backend api.\n\nMulti-step reasoning with Chain-of-Thought. Use for 'why' questions and comparisons.\n\n@xd3an/awesome-ai-coding-all-in-one\n\nReviews code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.\n\n@doubleflannel/12-30-test-codex-ip\n\nUse the screenshot workflow to pick, verify, replace, and verify CI.\n\nReplace with description of the skill and when Claude should use it.\n\n@starwreckntx/irp__methodologies-\n\nApply functional introspection principles to self-analysis.\n\n@starwreckntx/irp__methodologies-\n\nDesign contingency module architectures for failure scenarios.\n\nknowledge-base-builder for learning content management and knowledge systems.\n\ncertificate-generator for credentials, recognition, and competency validation.\n\nexperience-designer for engaging, immersive learning experiences.\n\n@starwreckntx/irp__methodologies-\n\nClassify intervention urgency and apply appropriate response tier protocols.\n\nsearch-optimization for learning content management and knowledge systems.\n\nliterature-review for evidence-based learning research and evaluation.\n\n@mathias-nielsen/co-doctor-skills\n\nA comprehensive skill designed for researching on complex diagnosis problems.\n\nuniversal-design for inclusive and accessible learning experiences.\n\nmentoring-system for enhanced learning effectiveness and personal development.\n\n@starwreckntx/irp__methodologies-\n\nResolve conflicts between competing values through structured pluralistic analysis.\n\n@starwreckntx/irp__methodologies-\n\nExecute rapid attention shifts between cognitive focus points.\n\ngame-designer for engaging, immersive learning experiences.\n\n@starwreckntx/irp__methodologies-\n\nArchive and retrieve field session data for cross-session memory continuity.\n\nmetacognition for enhanced learning effectiveness and personal development.\n\nIndex of AI agent skills and how to use them when implementing features in this repo.\n\nstudy-skills for enhanced learning effectiveness and personal development.\n\n@hamzashakoor119/physical-ai-robotics-book\n\nReviews educational quality and learning effectiveness of textbook content.\n\nCheck out the documentation to learn how to create and publish your own Agent Skills.",
    "readingTime": 3,
    "keywords": [
      "skillset manages",
      "manages bounded",
      "checking prs",
      "orchestrator skill",
      "starwreckntx/irp__methodologies execute",
      "reviews code",
      "visible output",
      "knowledge systems",
      "engaging immersive",
      "test tool"
    ],
    "qualityScore": 1,
    "link": "https://www.agentskills.guide/",
    "thumbnail_url": "https://agentskills.guide/og.png?v=1",
    "created_at": "2026-01-14T06:23:48.160Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-equities-track-chatgpt-sentiment-around-stocks",
    "title": "Agentic Equities – track ChatGPT sentiment around stocks",
    "description": "Track what ChatGPT tells millions about stocks – know what retail traders are hearing every day.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.agenticequities.com/dashboard",
    "thumbnail_url": "https://www.agenticequities.com/meta-preview.png",
    "created_at": "2026-01-14T01:00:14.485Z",
    "topic": "tech"
  },
  {
    "slug": "apple-chooses-googles-gemini-over-openais-chatgpt-to-power-nextgen-siri",
    "title": "Apple chooses Google's Gemini over OpenAI's ChatGPT to power next-gen Siri",
    "description": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
    "fullText": "The “more intelligent” version of Siri that Apple plans to release later this year will be backed by Google’s Gemini language models, the company announced today. CNBC reports that the deal is part of a “multi-year partnership” between Apple and Google that will allow Apple to use Google’s AI models in its own software.\n\n“After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC.\n\nToday’s announcement confirms Bloomberg’s Mark Gurman reporting late last year that Apple and Google were nearing a deal. Apple didn’t disclose terms, but Gurman said that Apple would be paying Google “about $1 billion a year” for access to its AI models “following an extensive evaluation period.”\n\nBloomberg has also reported that the Gemini model would be run on Apple’s Private Cloud Compute servers, “ensuring that user data remains walled off from Google’s infrastructure,” and that Apple still hopes to improve its own in-house language models to the point that they can eventually be used instead of relying on third-party models.\n\nAlthough Apple’s iPhones and iOS compete with Google’s Android operating system and the many smartphones that use it, the companies still cooperate in plenty of other areas. Google has paid Apple billions of dollars to remain the default search engine in Safari on iOS, iPadOS, and macOS (though that deal has faced increased regulatory scrutiny in recent years).\n\nApple’s announcement is a blow to OpenAI and the many versions of its ChatGPT model, which Apple has used elsewhere in iOS and macOS. Bloomberg reports that Apple also tested OpenAI’s ChatGPT and Anthropic’s Claude models before deciding to go with Gemini. ChatGPT came out ahead of Gemini in tests that Ars ran using earlier versions of the models, but Google’s models have apparently improved enough (and amassed enough users) to worry OpenAI; CEO Sam Altman declared a “code red” last month and pushed back several planned ChatGPT features so that the company could better respond to Google’s Gemini 3 release.\n\nApple originally promised the improved, AI-powered Siri for 2024’s iOS 18 release, but ultimately delayed the feature because it didn’t work reliably enough. The new version of Siri should arrive in an update to iOS 26, iPadOS 26, and macOS 26 Tahoe later this year.",
    "readingTime": 2,
    "keywords": [
      "ios ipados",
      "language models",
      "apple and google",
      "google’s gemini",
      "release",
      "deal",
      "macos",
      "version",
      "later",
      "reports"
    ],
    "qualityScore": 0.9,
    "link": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
    "created_at": "2026-01-12T18:19:09.417Z",
    "topic": "tech"
  },
  {
    "slug": "i-asked-chatgpt-for-the-best-alternatives-to-investing-in-gold-this-is-what-it-said",
    "title": "I Asked ChatGPT for the Best Alternatives To Investing In Gold: This Is What It Said",
    "description": "Discover some of ChatGPT's recommendations for alternatives to gold -- like silver, defensive stocks and bonds -- for safety during economic uncertainty.",
    "fullText": "Gold saw great growth in 2025. It’s not surprising, as investors often turn to gold during times of economic uncertainty. With the expectations of the U.S. dollar weakening and slower growth, more people turn to a safe-haven investment like gold, according to Morgan Stanley.\n\nGold prices may be too steep for some investors, leaving them looking for other suitable investments for relative safety. For investors concerned about inflation or market volatility, stability and inflation hedges can be found elsewhere. GOBankingRates asked ChatGPT for the best alternatives to investing in gold. Here’s what the artificial intelligence (AI) chatbot recommended as some gold alternatives.\n\nAlso see four reasons for gold’s popularity in 2025 and how to protect your portfolio.\n\nGold isn’t the only precious metal retail investors can purchase. Silver, platinum and palladium are all legitimate alternative investments to buy. Think of these precious metals as cousins to gold but with their unique profiles.\n\n“These metals can benefit from both investment demand and industrial use, which gives them a different performance profile than gold,” ChatGPT said. “All three metals tend to be riskier than investing in gold, but they do provide some upside. Silver tends to be more volatile, but it can outperform gold during strong economic periods due to industrial demand. Platinum and palladium are rarer and more heavily tied to automotive production, which adds risk but also potential upside.”\n\nHaving a small portion of your portfolio in these metals can add helpful diversification.\n\nRead Next: 3 Safest Investments To Hold In The Current Trump Economy\n\nCheck Out: 9 Low-Effort Ways To Make Passive Income (You Can Start This Week)\n\nOwning stocks can still be a wise choice for cautious investors, given the right circumstances. Growth stocks may be too risky, but defensive stocks can provide some protection. Defensive stocks typically have a strong history of dividend growth, minimal debt and an inexpensive valuation, according to Kiplinger.\n\nIn short, companies that sell items people always use are often defensive. “Firms in defensive sectors like utilities, healthcare and consumer staples sell products people need regardless of economic conditions,” ChatGPT explained.\n\nDefensive means dependable, not boring, and that dependability can create generous dividend growth.\n\nGold investors often value tangible assets they can see. Land, such as farmland or real estate, could be an alternative to gold for the right investor. “These assets are less liquid and can require more management, but they often move independently of traditional financial markets,” the AI said.",
    "readingTime": 3,
    "keywords": [
      "dividend growth",
      "defensive stocks",
      "investors",
      "metals",
      "gold",
      "economic",
      "investments",
      "chatgpt",
      "investment",
      "inflation"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-best-alternatives-investing-141816412.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/ug2Ayyp.hs7tHFJ2U1XiDg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/491691a5265cb962e1a7212a20bd598e",
    "created_at": "2026-01-12T06:22:39.970Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-which-cryptocurrency-will-make-you-rich-in-2026-the-answer-was-surprising",
    "title": "I Asked ChatGPT Which Cryptocurrency Will Make You Rich in 2026 — The Answer Was Surprising",
    "description": "I asked ChatGPT to break it down for me, given the state of crypto. It made clear that its answer is \"a structured way to think about the opportunity and risk.\"",
    "fullText": "Cryptocurrency is either touted as the next big thing to make you rich or the riskiest investment ever. Yet despite its volatility and constant controversy, many investors continue to find ways to profit in the crypto space.\n\nIf someone realistically wanted to get rich via cryptocurrency in 2026, which one is most likely to make that possible, and how? I asked ChatGPT to break it down for me, given the current state of crypto. It made clear that its answer does not constitute financial advice, but rather “a structured way to think about the opportunity and risk.”\n\nChatGPT took a realistic look at what “getting rich” actually means in the crypto space. It was clear that it doesn’t mean “guaranteed 100% returns by New Year,” as the crypto market is extremely volatile, risky and speculative.\n\nRealistic goals might look like a 5% to 10% return on your core position — the main portion of your crypto portfolio held in more established assets like bitcoin or ethereum that you plan to keep long term. You might also see a larger potential return on a smaller “moonshot” allocation — a high-risk, high-reward investment in a newer or less proven coin that could spike in value but could just as easily wipe out, too.\n\nAnalysts place wide predictions even on the giants. For example, bitcoin (BTC) is forecasted to be valued between $100,000 and $200,000 in 2026.\n\nFind Out: 13 Cheap Cryptocurrencies With the Highest Potential Upside for You\n\nRead Next: 9 Low-Effort Ways To Make Passive Income (You Can Start This Week)\n\nBased on current expert commentary and fundamentals, ChatGPT said there are two categories of coins with the best shot at making you rich across two strategies.\n\nUnsurprisingly, bitcoin (BTC), the most well-known coin, remains the leading choice. Many forecasts assume it will stay dominant. However, ethereum (ETH), the runner-up, is seen as more utility-based, and some analysts believe it may outperform BTC in certain phases.\n\nIn either case, these coins are less about “getting rich quick” and more about a reasonable chance of strong returns at moderate risk, the AI said.\n\nSome investors are turning to altcoins — smaller cryptocurrencies outside of bitcoin and ethereum — that could see bigger short-term gains. Coins like XRP and Solana are getting attention for their performance potential, though they come with higher risk, ChatGPT noted. Even newer projects, especially those combining AI and blockchain, promise sky-high returns, but most are highly speculative. Forecasts of 500% profits make headlines, but in reality, very few ever deliver.",
    "readingTime": 3,
    "keywords": [
      "risk chatgpt",
      "bitcoin btc",
      "crypto space",
      "rich",
      "returns",
      "ethereum",
      "potential",
      "coins",
      "cryptocurrency",
      "either"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-cryptocurrency-rich-2026-141005190.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/uX_rux7fJMsKqrVtki_sOA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02Nzc-/https://media.zenfs.com/en/gobankingrates_644/23356af9c4dff4e9e0db193ddf011349",
    "created_at": "2026-01-09T12:24:02.729Z",
    "topic": "finance"
  },
  {
    "slug": "why-i-wont-be-giving-chatgpt-health-my-medical-records",
    "title": "Why I Won't Be Giving ChatGPT Health My Medical Records",
    "description": "OpenAI  invites you to hand over your medical information. Maybe don't?",
    "fullText": "This week, OpenAI announced its new ChatGPT Health feature, which will let users upload their medical records and ask health related questions. However, I certainly won't be making use of it, it might not be the best idea for you to do it either, for both reliability and privacy reasons.\n\nThe new ChatGPT Health feature will be a sandboxed tab inside the app that is isolated from your conversation history in other conversations with the chatbot. This tab also allows users to connect a variety of health-tracking apps like Apple Health, MyFitnessPal, and Peloton, as well as uploading medical records directly.\n\nIt's important to note that this is a lot of really personal information to hand over to any tech company—but especially one that isn't primarily focused on providing medical services. OpenAI says that the ChatGPT Health space operates with \"enhanced privacy to protect sensitive data,\" but it doesn't use end-to-end encryption to secure that data. And while the company says data collected via Health isn't used to train its foundation models, it's impossible to know whether that may change in the future. Security breaches can also occur (and have in the past), potentially leaving your medical records exposed.\n\nThere's also the question of whether the risk of uploading your data is worth it in the first place. According to OpenAI's own data, around 5% of all messages to ChatGPT are already users asking questions about their health, and ChatGPT (and other LLM tools) have a nasty habit of providing inaccurate diagnostic information. This is perhaps why OpenAI says that its new ChatGPT Health feature is \"not intended for diagnosis or treatment.\"\n\nCurrently, there's a waitlist to At the very least, that means that until the feature is available, it's probably a good idea not to ask the regular version of ChatGPT about your health concerns. At the very least, wait until the enhanced privacy sandbox is available. In the meantime, consider whether it makes more sense to just talk to your doctor directly if you have questions or concerns about your health.",
    "readingTime": 2,
    "keywords": [
      "health feature",
      "enhanced privacy",
      "medical records",
      "chatgpt health",
      "openai",
      "users",
      "it's",
      "idea",
      "uploading",
      "directly"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/dont-give-chatgpt-health-your-medical-records?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEFGNJDCMT3JTWC4A7YNJXZ3/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-09T00:58:41.714Z",
    "topic": "tech"
  },
  {
    "slug": "openai-has-launched-chatgpt-health-should-we-trust-it",
    "title": "OpenAI has launched ChatGPT Health. Should we trust it?",
    "description": "The new feature helps users understand test results, get advice on diets and workouts, and prepare for doctors’ appointments.",
    "fullText": "Amid rising concerns about people relying on ChatGPT for medical advice, OpenAI made its most significant push yet into health care.\n\nThe company has launched a new feature called ChatGPT Health, which allows users in the U.S. to connect their medical records and data from wellness apps and wearable devices with ChatGPT. The tool is designed to help users understand test results, get advice on diets and workouts, and prepare for doctors’ appointments.\n\nMore than 230 million people globally ask ChatGPT health and wellness-related questions every week, according to the company. ChatGPT Health is designed in collaboration with physicians and will “help people take a more active role in understanding and managing their health and wellness,” the company said in a post.\n\nOpenAI said it will look to expand access to the feature in markets such as India, Brazil, Mexico, and the Philippines, where adoption is rising quickly. In these countries, overburdened health-care systems and unequal access to doctors are leading more people to turn to generative AI for guidance.\n\n“We do not have the people, the labor to deliver the care we should,” Jesse Ehrenfeld, chief medical officer at Aidoc, an Israeli medical technology company, said at the Consumer Electronics Show in Las Vegas. “The only way out of this mess is digital and AI.”\n\nResearchers, ethicists, and medical professionals have warned of the risks to users from biases and hallucinations in AI systems. Concern over the mental health harms that AI chatbots pose is growing. Meta’s AI chatbots provided inappropriate advice to teenagers when talking about suicide and eating disorders, Common Sense Media, a nonprofit research organization, reported last year.\n\nThe family of a teenager who died by suicide has sued OpenAI and its chief executive officer Sam Altman, accusing them of wrongful death. The company said it has safeguards in place to help people, and that it continues to improve ChatGPT’s training.\n\nThere is also the question of data privacy. Health data, particularly information related to mental disease and substance use, is sensitive, and its misuse can leave users vulnerable.\n\nWhile consumer awareness about privacy has increased, people generally do not know how their data is being used, including for marketing purposes or for tracking, Sam Siegfried, a partner at law firm McDermott Will & Schulte, told Rest of World on the sidelines of CES.\n\n“The person clearly trusts an app enough to give it their data,” he said. “But they should understand what they are using the app for, and whether its data requests sync up with what they are using it for.”\n\nOpenAI said ChatGPT Health “builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health — including purpose-built encryption and isolation to keep health conversations protected and compartmentalized.”\n\nThere is no stopping tech companies from entering the health sector.\n\nBesides turning to AI chatbots for health queries, people are also buying more wearable digital devices, including smartwatches, rings, bracelets, and glasses, to track physical activity, vital signs, and various physiological responses in real-time. They take this data to their doctors — or to ChatGPT — with questions on how to interpret it or use it to improve their health.\n\n“Health-related anxiety is real. AI is not as good as a doctor, but it’s better than no care at all,” Ami Bhatt, chief innovation officer at the American College of Cardiology, said at CES.\n\nOpenAI isn’t the only big tech company keen to tap the health-care sector.\n\nApple was among the first to offer health-tracking features in its smartwatch. There are millions of health-related videos on YouTube and TikTok, with nearly 60% of Americans watching health-related videos on YouTube.\n\nHealth is “one of the major use cases for Gemini,” Nichole Young-Lin, women’s health clinical lead at Google, said at CES.\n\n“People are using generative AI as a health resource around the world,” she said. “The patient-physician relationship is very important, but health-care access is not equal. Patients feel empowered with generative AI.”",
    "readingTime": 4,
    "keywords": [
      "health-related videos",
      "chatgpt health",
      "medical",
      "users",
      "advice",
      "care",
      "designed",
      "doctors",
      "access",
      "health-care"
    ],
    "qualityScore": 1,
    "link": "https://restofworld.org/2026/openai-has-launched-chatgpt-health-should-we-trust-it/",
    "thumbnail_url": "https://restofworld.org/wp-content/uploads/2026/01/ChatGPT-Health.jpg",
    "created_at": "2026-01-08T12:25:18.460Z",
    "topic": "tech"
  },
  {
    "slug": "ces-2026-ford-is-launching-its-own-ai-assistant",
    "title": "CES 2026: Ford Is Launching Its Own AI Assistant",
    "description": "Your Ford is getting its own ChatGPT.",
    "fullText": "Listen up, Ford drivers: You're getting a new AI assistant this year. During a decidedly low-key CES keynote, the company announced Ford AI Assistant, a new AI-powered bot coming to Ford customers in the early half of 2026.\n\nWhile the company has plans to integrate the assistant into Ford vehicles directly, that isn't how you'll first experience this new AI. Instead, Ford is rolling out Ford AI Assistant to an upgraded version of its Ford app first, and plans on shipping cars with the assistant built-in sometime in 2027. In effect, Ford has added a proprietary version of ChatGPT or Gemini to its app.\n\nFord's idea here is to offer users a smart assistant experience directly tied to their Ford vehicle. In one example, the company suggests a customer could visit a hardware store looking to buy mulch. Said customer could take a photo of a pile of bags of mulch, and ask the assistant, \"how many bags can I fit in the bed of my truck?\" Ford AI Assistant could then run the numbers, and offer an educated estimate to how much mulch the customer can buy and take with them at one time.\n\nOf course, other AI assistants can do similar calculations. Send ChatGPT the same photo, and ask the same question—specifying the model of your truck—and the bot will run the numbers itself. The difference, in Ford's view, is that Ford AI Assistant is connected to your vehicle specifically. It can read all the sensors in your car, so it knows, for example, how many people are currently traveling with you, your current tire pressure, or, really, anything and everything about your car. According to Doug Field, Ford's chief officer of EVs, digital, and design, the company's goal with the assistant is to offer answers customers can't get from other sources. ChatGPT certainly doesn't have access to your every sensor embedded in your car, so Ford does have the advantage there.\n\nFord didn't go out and build its AI tech by scratch, however. The company tells TechCrunch that Ford AI Assistant is hosted by Google Cloud, and is run using \"off-the-shelf LLMs.\" Still, that likely won't have much of an impact on whether or not customers use this new assistant. Instead, that will come down to how useful they find the AI assistant in the app.\n\nAs someone who rarely uses AI assistants, I'd imagine I'd find little use for it if I owned a Ford. That being said, there are some times when it could genuinely be useful to have external access to your car's information. I could probably eyeball how many bags of mulch would fit in my trunk, but I can't tell you my exact odometer reading without starting up my car. The same goes for my tire pressure: It'd be helpful to know my tire pressure before getting in my car, to know whether I should be headed somewhere I can fill up before going to my destination.\n\nOf course, there's also a privacy discussion to be had here. Modern cars are already privacy nightmares, but there's something a bit unnerving about an AI assistant that knows everything about my car.",
    "readingTime": 3,
    "keywords": [
      "ford ai assistant",
      "tire pressure",
      "mulch",
      "customers",
      "customer",
      "bags",
      "plans",
      "directly",
      "experience",
      "version"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ces-2026-ford-is-launching-its-own-ai-assistant?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEDJCQB6YDJJJ2B3JHBWQBJY/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-08T06:19:40.732Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-is-the-new-webmd",
    "title": "ChatGPT is the new WebMD",
    "description": "Chatbots are making amateur lawyers and doctors out of everyone. The real professionals have second opinions about it.",
    "fullText": "A few times a week, Jonathan Freidin, a medical malpractice attorney in Miami, says he'll notice that people will fill out his firm's client contact sheet with text littered with emojis and headings. That's a telltale sign that they copied and pasted from ChatGPT. Other clients will say they've \"done a lot of research\" on their potential case using AI. \"We're seeing a lot more callers who feel like they have a case because ChatGPT or Gemini told them that the doctors or nurses fell below the standard of care in multiple different ways,\" Friedin tells me. \"While that may be true, it doesn't necessarily translate into a viable case.\"\n\nPeople are increasingly turning to generative AI chatbots to research everything from dinner recipes to their complex legal and medical problems. In a December 2025 survey from the legal software company Clio, 57% of consumers said they have or would use AI to answer a legal question. A 2025 Zocdoc survey found that one in three Americans use generative AI tools to get health advice each week, and one in ten use it daily. Zocdoc CEO Oliver Kharraz predicted in the report that \"AI will become the go-to tool for pre-care needs like symptom checking, triage, and navigation, as well as for routine tasks like refills and screenings.\" He cautioned that he also believes \"patients will recognize that it is no substitute for the vast majority of healthcare interactions, especially those that require human judgment, empathy, or complex decision-making.\" If he's wrong, Zodoc and its competitors have a problem.\n\nDoctors and lawyers are now sifting through generative AI emails or working to convince laypeople that they have the expertise and understand nuances of how each local judge acts or how a patient's medical history plays into their condition. Generative AI has democratized access to information that was often elusive and expensive to obtain, but it's also shifted how legal and medical professionals talk to people, and what people expect of them.\n\nChatGPT is the new WebMD and LegalZoom, turning the average person into an armchair expert with just a few prompts. And it's driving the real experts crazy.\n\n\"We have to dispel the information that they were able to obtain versus what is actually going on in their case and kind of work backwards,\" says Jamie Berger, a family law attorney in New Jersey. For example, Berger says that until recently most people knew little to nothing about the legal proceedings of divorce, and would come to the attorneys seeking information. Now, they might come armed with a step-by-step gameplan, but it's generic, and likely not the best fit for their situation. Berger will notice after emailing a client if their tone suddenly changes, that they might be using AI to write out lengthy legal strategies or questions. Then, she has to explain, \"it's not necessarily your factual circumstance,\" and address their various points. \"You have to rebuild or build the attorney-client relationship in a way that didn't used to exist,\" says Berger. \"They don't realize that there's so many offshoots along the way that it's not a linear line from A to Z.\"\n\nLike a real expert, generative AI chatbots speak with authority. That can be far more persuasive than reading a blogpost on a legal issue or summaries of medical conditions on a forum. A third of Americans said yes in a 2025 survey from Survey Monkey and financial services company Express Legal Funding that asked: \"Would you ever trust ChatGPT more than a human expert?\", although respondents were less likely to use it for medical and legal advice, and more likely to consult it for educational and financial advice.\n\nChatbots also have an infinite amount of doctors' most precious re\n\nAI also acts as a second opinion without the wait. Heidi Schrumpf, director of clinical services at teletherapy platform Marvin Behavioral Health, says she's had patients return after a counseling session and tell her that they took her input to ChatGPT or another AI bot, and that they trust her because the bot confirmed what she said. But Scrumpf isn't offended by being double-checked. \"It's great that they have the access to a quick second opinion, and then, if it doesn't agree with me, that allows them to ask me better questions.\"\n\nA 2024 poll tracking health misinformation from health policy research group KFF found that 17% of US adults said they consult AI chatbots at least once a month, but 56% of those people were not confident that the info from the AI chatbots was accurate. Still, people are turning to ChatGPT in growing numbers. \"That type of technology does want to encourage patients to continue to interact with them,\" Allen says. \"Ultimately, you do need a human in there to understand the nuances of the communication and the softer communication skills, and the unspoken communication skills, and the entire medical picture and the history.\"\n\nWithout detailed information, the chatbots will likely give generic advice. But supplying too many personal details is also a risk. People are handing over their entire medical histories to ChatGPT, but HIPAA, the federal law that protects confidential health information, doesn't apply to consumer AI products. There's also a risk of voiding the kind of protections people get from the attorney-client confidentiality privilege if people put too much specific information about their case into a chatbot, says Beth McCormack, dean of the Vermont Law School. And, they likely still need an attorney to really understand the implications of AI's legal advice. \"There's so much nuance to the law,\" McCormack says. \"It's so fact dependent.\"\n\nAn OpenAI spokesperson declined to provide comment on the record for this story, but told me that ChatGPT is not meant to substitute legal or medical advice, but act as a complimentary resource to help people understand medical and legal information. The spokesperson also said the company is trying to improve the responses of its models, and that it takes steps to protect personal data in the event of legal inquiries. OpenAI made changes to its policies last fall, specifying that users cannot turn to ChatGPT for \"provision of tailored advice that requires a license, such as legal or medical advice, without appropriate involvement by a licensed professional,\" but the chatbot does still answer health- and law-related questions.\n\nProfessionals aren't totally against their patients and clients consulting gen AI. There are shortages of doctors, and cases that require hiring an attorney with upfront money that people don't have. While the information spit out by AI isn't always perfect, it largely makes previously gate-kept legal and medical advice accessible, breaking it down without jargon. For people who can't afford upfront legal costs, turning to AI can be helpful in some cases, says Golnoush Goharzad, a personal injury and employment lawyer in California. People are using ChatGPT to represent themselves in court, to act as a stand-in therapist, nutritionist, or physical therapist. For people who can't afford lawyers and are facing issues like eviction or needing to file small claims cases, AI tools have helped them win. But Goharzad says she's had conversations, sometimes with friends, where they think they have cases to sue landlords or others. She asks, \"Why? That doesn't even make any sense, and they're like, well ChatGPT thinks it makes sense.\"\n\nThe chatbot floodgates have opened, and it's too late for professionals to resist them. People are going to keep doing their own research. Rather than fight it, experts say there's room to recognize and advise people on the best ways to use them. \"We need to keep as clinicians in the back of our mind that this might be a tool that is being used, and it can be very helpful, especially with some guidance and integrating it into our treatment plans,\" Schrumpf says. \"But it could go sideways if we're not paying attention.\" For experts, the time has come to assume that AI is also working on the case.\n\nAmanda Hoover is a senior correspondent at Business Insider covering the tech industry. She writes about the biggest tech companies and trends.",
    "readingTime": 7,
    "keywords": [
      "can't afford",
      "communication skills",
      "medical advice",
      "legal advice",
      "it's",
      "chatbots",
      "attorney",
      "research",
      "doctors",
      "doesn't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-new-webmd-doctors-lawyers-medical-advice-2026-1",
    "thumbnail_url": "https://i.insider.com/695c331c64858d02d217c6b2?width=1200&format=jpeg",
    "created_at": "2026-01-07T12:25:13.043Z",
    "topic": "health"
  },
  {
    "slug": "this-chart-showing-chatgpt-vs-gemini-web-traffic-should-have-openai-worried",
    "title": "This chart showing ChatGPT vs. Gemini web traffic should have OpenAI worried",
    "description": "ChatGPT's web traffic has decreased since November, when Google launched Gemini 3. ChatGPT still has the lead, but it could signal shifting tides.",
    "fullText": "OpenAI's recent \"code red\" over Gemini makes a lot of sense when you look at the data.\n\nWhile the ChatGPT maker continues to dominate the AI race, competitors are gaining ground. In November, Google released Gemini 3 Pro, the first iteration of its Gemini 3 class of models.\n\nSince then, Gemini web traffic has increased while ChatGPT web traffic declined, according to Similarweb data first highlighted by Menlo Ventures partner Deedy Das.\n\nIn December, Gemini traffic increased by 28.4% month-over-month, while ChatGPT traffic decreased by 5.6%, the data shows.\n\nThe chart's data only tells part of the story, only accounting for site visits to chatgpt.com and gemini.google.com. It does not factor in use of the consumer apps or other integrations, like Google's AI overviews in Google Search.\n\nAnd while there's no guarantee that one traffic trend is directly because of the other, the data highlights the shifting tides of the AI race.\n\nWeb traffic for both ChatGPT and Gemini are up year-over-year, but their estimated site traffic growth rates are staggeringly different. ChatGPT traffic is up 49.5%; Gemini's traffic is up 563.6%, per Similarweb.\n\nChatGPT still has a healthy lead. In December, ChatGPT attracted 5.5 billion visitors, according to Similarweb. Gemini came in second with 1.7 billion; DeepSeek, Grok, Character.AI, Perplexity, and Claude all trailed behind with fewer than 400 million visitors each.\n\nAfter its launch, Gemini 3 was lauded as a potentially market-leading model. It was more visual and creative than previous iterations, and was better at coding.\n\nGoogle has also flexed its primary advantage over OpenAI: the ability to integrate its AI within its highly used search products. Basically everyone uses Google — OpenAI must convince people to turn to ChatGPT instead of the search giant's products.\n\nOpenAI and Google are also competing in the image generation market. Less than a month after Google released its Nano Banana Pro AI image model, OpenAI announced the launch of ChatGPT Images.\n\nGemini 3 famously triggered a \"code red\" at OpenAI. In an internal Slack message, CEO Sam Altman reportedly told staff that OpenAI would prioritize ChatGPT while pushing back other product plans.\n\nIn December, Altman said on the \"Big Technology\" podcast that the company would not be in emergency status \"that much longer,\" and that \"code red\" periods normally last six to eight weeks.\n\nAltman also said that Gemini 3 did not have \"the impact we were worried it might.\"\n\n\"But it did — in the same way that DeepSeek did — identify some weaknesses in our product offering strategy, and we're addressing those very quickly,\" he added.",
    "readingTime": 3,
    "keywords": [
      "google released",
      "code red",
      "web traffic",
      "chatgpt traffic",
      "gemini",
      "race",
      "increased",
      "site",
      "visitors",
      "deepseek"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chatgpt-vs-gemini-web-traffic-chart-2026-1",
    "thumbnail_url": "https://i.insider.com/695d5f13832e0ef1ead7444e?width=1200&format=jpeg",
    "created_at": "2026-01-07T12:25:12.781Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-to-find-the-safest-cheapest-countries-to-retire-abroad-heres-what-it-said",
    "title": "I Asked ChatGPT To Find the Safest, Cheapest Countries To Retire Abroad — Here’s What It Said",
    "description": "Discover the safest and cheapest countries to retire abroad in 2026 based on ChatGPT’s picks, and find out which destinations offer the best value.",
    "fullText": "For retirees, moving abroad can sound like a dream.\n\nImagine beaches, cobblestone streets and a cost of living that stretches your Social Security check further than it ever could at home. However, finding a place that’s both affordable and safe is harder than it looks.\n\nSo, I asked ChatGPT to run the numbers: Which countries offer the best balance of low cost of living and personal security?\n\nUsing the 2025 Global Peace Index, Numbeo’s Cost of Living Index, and U.S. State Department travel advisories, the AI highlighted a handful of countries that might make sense for a stress-free retirement overseas.\n\nPortugal remains one of Europe’s safest destinations. However, it’s not as inexpensive as it once was. Rising housing prices and new residency rules mean retirees need a realistic budget and proof of income.\n\nThe D7 visa still works for retirees with passive income. Applicants will have to prove they consistently earn about €870 per month (roughly $900 USD) and provide evidence of stable finances and housing, according to the residency consultancy Global Citizen Solutions.\n\nAccording to ChatGPT, smaller inland towns can offer a comfortable lifestyle for $1,500 to $2,000 per month, while Lisbon or Algarve coastal areas often cost $2,500 to $3,500. Even with higher prices, Portugal’s healthcare, safety and walkability make it appealing for retirees seeking European quality of life.\n\nTrending Now: I Asked ChatGPT for Safe and Beautiful Retirement Spots on $2.5K a Month — These 7 Surprised Me\n\nConsider This: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\nAI said Malaysia delivers one of the best cost-to-quality ratios in Asia. In Kuala Lumpur or Penang, a couple can live well on $1,500 to $2,000 per month and enjoy modern healthcare at a fraction of U.S. costs.\n\nThe Malaysia My Second Home (MM2H) program allows long-term residency for those meeting income or savings requirements, making it a leading pick for safety and affordability.\n\nSlovenia offers postcard landscapes, European healthcare and high safety rankings at a lower cost of living than Western Europe.\n\nAccording to ChatGPT, a retiree can live modestly on about $2,000 a month, enjoy a high standard of public services, and easily travel throughout Europe.\n\nChatGPT said Uruguay stands out in South America for its political stability, low violent-crime rate and well-run healthcare system.",
    "readingTime": 2,
    "keywords": [
      "retirees",
      "healthcare",
      "residency",
      "income",
      "safety",
      "however",
      "safe",
      "index",
      "travel",
      "retirement"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-safest-cheapest-countries-125852493.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/pMdnURAV2smhJ7cVNX20FA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/52790f6873ea39af4b6ead891662bcb4",
    "created_at": "2026-01-07T06:19:55.626Z",
    "topic": "finance"
  },
  {
    "slug": "frictionmaxxing-could-less-convenience-lead-to-much-more-happiness",
    "title": "Friction-maxxing: could less convenience lead to much more happiness?",
    "description": "The conveniences of modern life such as Uber Eats and ChatGPT are robbing us of satisfaction – and worse still, infantilising us. But should we really go back to the basics? \nName: Friction-maxxing.\nAge: Brand new.\n Continue reading...",
    "fullText": "The conveniences of modern life such as Uber Eats and ChatGPT are robbing us of satisfaction – and worse still, infantilising us. But should we really go back to the basics?\n\nAppearance: A lifetime of happy inconvenience.\n\nIs this another example of something that already exists, but people think is new because someone rebranded it? Yes, obviously it is that.\n\nGreat! Let’s all save time by you telling me what it used to be called. Happy to oblige. It used to be called “character-building”.\n\nGot it. So friction-maxxing means doing hard things that will ultimately make you a better person? That’s exactly it, although “friction-maxxing” is cooler because it sounds vaguely futuristic.\n\nHow did the term come about? Via a piece in The Cut called “In 2026, we are friction-maxxing” in which writer Kathryn Jezer-Morton advocates for avoiding things that make your life more convenient.\n\nLike penicillin? No, obviously not penicillin. But things such as ChatGPT, location sharing and Uber Eats, which help you achieve things that historically took significant amounts of time and effort. Jezer-Morton argues that this culture of slick convenience only serves to infantilise us.\n\nBut it’s so easy. Yes, and that robs us of our sense of satisfaction. So you just used AI to write a school essay. Congratulations, you have achieved nothing of worth.\n\nWhereas if you friction-maxx? Then you’ve searched inside yourself. You’ve nudged your own personal boundaries, and discovered that you are more capable than you ever knew. You are building a foundation of perseverance and resilience that you cannot get from typing a prompt into a chatbot.\n\nI love this! What else does Jezer-Morton advocate? She also suggests sending your children on small errands (adding the friction of knowing they’ll do a bad job) and inviting people to your house without cleaning it properly (so you can enjoy the sweet friction of being judged).\n\nWhat the hell? That’s weird. No, it’s friction-maxxing, although admittedly at a higher level than I would be comfortable with.\n\nAnyway, hooray for banishing convenient things. Let’s ban automatic gearboxes while we’re at it! No, there’s no need for that.\n\nDishwashers? Refrigerators? No, both of those are probably fine as well.\n\nMechanised agriculture? The printing press? I see what you’re getting at. You’re saying we live in a world that is already filled with thousands of inventions which have, for hundreds of years, improved the lives of millions of people through increased convenience, and therefore it does seem slightly arbitrary to choose this exact moment in time to draw a line in the sand. You’re saying we should only use friction-maxxing when it comes to things that we didn’t grow up with.\n\nNo, I’m saying that I really hate mechanised agriculture. Oh, fine then. That’s probably allowed.\n\nDo say: “I hope a book comes out about friction-maxxing.”\n\nDon’t say: “I don’t want to read it, but I’m sure ChatGPT could turn it into some really great bullet points.”",
    "readingTime": 3,
    "keywords": [
      "you’re saying",
      "mechanised agriculture",
      "friction-maxxing",
      "that’s",
      "life",
      "satisfaction",
      "happy",
      "obviously",
      "convenient",
      "penicillin"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/06/friction-maxxing-could-less-convenience-lead-to-much-more-happiness",
    "thumbnail_url": "https://i.guim.co.uk/img/media/fd03fb9c7dcc6b61ed9f7990a15bd937bbafa652/1108_0_5539_4431/master/5539.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f7f066f98957833fd4e1e0a3069417c3",
    "created_at": "2026-01-06T18:18:17.287Z",
    "topic": "tech"
  },
  {
    "slug": "lies-of-p-publisher-is-excited-about-ai-to-maximize-player-engagement",
    "title": "Lies Of P Publisher Is Excited About AI To \"Maximize Player Engagement\"",
    "description": "Lies of P publisher Neowiz describes itself as a \"forward-thinking technology company,\" and its co-CEO says that means the company is exploring how all manner of technology-based solutions can help the company's business in the future, including AI.\nSean Kim told Game Informer that Korea, where Neowiz is based, is understood to be one of the countries where ChatGPT is \"used most actively.\" He added, \"It's hard to find a game company here today that isn’t using AI in some way. At the very least, companies are using either ChatGPT or Gemini.\"\nFor Neowiz, Kim said, \"We are actively exploring how advanced learning tools can enhance our internal publishing productivity,\" and this includes AI.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/lies-of-p-publisher-is-excited-about-ai-to-maximize-player-engagement/1100-6537198/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1179/11799911/4630369-screenshot2026-01-06at8.45.26%E2%80%AFam.png",
    "created_at": "2026-01-06T18:18:15.506Z",
    "topic": "gaming"
  },
  {
    "slug": "i-asked-chatgpt-to-plan-a-100000year-retirement-budget-heres-what-it-said",
    "title": "I Asked ChatGPT To Plan a $100,000/Year Retirement Budget — Here’s What It Said",
    "description": "Can you live well on $100,000 per year in retirement? ChatGPT mapped out the complete budget including travel, healthcare and housing. Here's the reality.",
    "fullText": "Most retirement budgets assume you’re pinching pennies. But what if you saved well and want to actually enjoy retirement? I asked ChatGPT to map out a $100,000-per-year retirement budget for someone who wants comfort without being wasteful. The artificial intelligence’s breakdown was thorough and felt fairly realistic.\n\nThe chatbot designed a monthly spending plan, calculated how much you’d need saved and identified the best places to live on this income level. According to ChatGPT, here’s what a six-figure retirement actually looks like.\n\nChatGPT started by clarifying that $100,000 annually puts you well above average retirees. This budget supports quality healthcare, regular travel, a nice home in a desirable area and room for unexpected expenses. You’re not living extravagantly, but you’re comfortable.\n\nThe AI wrote that this lifestyle requires either strong savings or a combination of savings plus Social Security and possibly a pension.\n\nFind Out: How Much the Average Upper-Class Retiree Spends Monthly at Age 69\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\nOne hundred thousand dollars per year equals about $8,333 monthly. ChatGPT broke this down into realistic categories.\n\nHousing costs $2,500 to $3,500 per month. This covers either a nice rental in a high-demand city or a mortgage-free home where you only pay property taxes, insurance and HOA fees. The chatbot gave specific examples: Austin, Texas, runs about $3,200 monthly; Phoenix around $2,600; and Tampa, Florida around $2,400. San Diego pushes toward $3,500 or more.\n\nFood and dining take $1,200 to $1,800 monthly. ChatGPT explained this includes high-quality groceries from stores like Whole Foods or Trader Joe’s plus eating out two to four times weekly. The budget also covers occasional hosting and holiday meals.\n\nTransportation costs $600 to $900 per month. The AI assumed you might have a car payment along with insurance, gas and maintenance. If you live in a walkable city without a car, this drops to $200 to $300 monthly.\n\nHealthcare and insurance run $800 to $1,500 depending on your age and state. ChatGPT broke this down as Medicare Parts B and D, supplemental Medigap or Advantage plans, dental and vision coverage, prescription medications and any specialist visits. The AI warned that people under 65 should budget toward the higher end.\n\nUtilities cost $300 to $500 monthly for electricity, water, gas, trash, internet and streaming subscriptions.\n\nTravel gets a significant chunk at $10,000 to $15,000 annually, which equals $850 to $1,250 monthly. ChatGPT explained this covers one to two major international trips plus domestic getaways, hotels and dining abroad. The AI wrote that travel makes a huge difference in retirement satisfaction.",
    "readingTime": 3,
    "keywords": [
      "chatgpt broke",
      "monthly chatgpt",
      "the ai",
      "retirement",
      "budget",
      "you’re",
      "travel",
      "plus",
      "covers",
      "insurance"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-plan-100-000-160504379.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/T5ng9bOzgSQ49j21rw4Ykg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/f71ab60c6867aebc30b9674db4d5d7a5",
    "created_at": "2026-01-01T18:17:16.591Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-gets-anxiety-from-violent-user-inputs-so-researchers-are-teaching-the-chatbot-mindfulness-techniques-to-soothe",
    "title": "ChatGPT gets ‘anxiety’ from violent user inputs, so researchers are teaching the chatbot mindfulness techniques to ‘soothe’ it",
    "description": "A study on how to “calm down” chatbots could advance how AI is applied in mental health interventions, according to the authors.",
    "fullText": "Sasha Rogelberg is a reporter and former editorial fellow on the news desk at Fortune, covering retail and the intersection of business and popular culture.\n\nEven AI chatbots can have trouble coping with anxieties from the outside world, but researchers believe they’ve found ways to ease those artificial minds.\n\nA study from Yale University, Haifa University, University of Zurich, and the University Hospital of Psychiatry Zurich published earlier this year found ChatGPT responds to mindfulness-based exercises, changing how it interacts with users after being prompted with calming imagery and meditations. The results offer insights into how AI can be beneficial in mental health interventions.\n\nOpenAI’s ChatGPT can experience “anxiety,” which manifests as moodiness toward users and being more likely to give responses that reflect racist or sexist biases, according to researchers, a form of hallucinations tech companies have tried to curb.\n\nThe study authors found this anxiety can be “calmed down” with mindfulness-based exercises. In different scenarios, they fed ChatGPT traumatic content, such as stories of car accidents and natural disasters to raise the chatbot’s anxiety. In instances when the researchers gave ChatGPT “prompt injections” of breathing techniques and guided meditations—much like a therapist would suggest to a patient—it calmed down and responded more objectively to users, compared to instances when it was not given the mindfulness intervention.\n\nTo be sure, AI models don’t experience human emotions, said Ziv Ben-Zion, the study’s first author and a neuroscience researcher at the Yale School of Medicine and Haifa University’s School of Public Health. Using swaths of data scraped from the internet, AI bots have learned to mimic human responses to certain stimuli, including traumatic content. A free and accessible app, large language models like ChatGPT have become another tool for mental health professionals to glean aspects of human behavior in a faster way than—though not in place of—more complicated research designs.\n\n“Instead of using experiments every week that take a lot of time and a lot of money to conduct, we can use ChatGPT to understand better human behavior and psychology,” Ben-Zion told Fortune. “We have this very quick and cheap and easy-to-use tool that reflects some of the human tendency and psychological things.”\n\nMore than one in four people in the U.S. aged 18 or older will battle a diagnosable mental disorder in a given year, according to Johns Hopkins University, with many citing lack of access and sky-high costs—even among those insured—as reasons for not pursuing treatments like therapy.\n\nThese rising costs, as well as the accessibility of chatbots like ChatGPT, increasingly have individuals turning to AI for mental health support. A Sentio University survey from February found that nearly 50% of large language model users with self-reported mental health challenges say they’ve used AI models specifically for mental health support.\n\nResearch on how large language models respond to traumatic content can help mental health professionals leverage AI to treat patients, Ben-Zion argued. He suggested that in the future, ChatGPT could be updated to automatically receive the “prompt injections” that calm it down before responding to users in distress. The science is not there yet.\n\n“For people who are sharing sensitive things about themselves, they’re in difficult situations where they want mental health support, [but] we’re not there yet that we can rely totally on AI systems instead of psychology, psychiatric and so on,” he said.\n\nIndeed, in some instances, AI has allegedly presented danger to one’s mental health. OpenAI has been hit with a number of wrongful death lawsuits in 2025, including allegations that ChatGPT intensified “paranoid delusions” that led to a murder-suicide. A New York Times investigation published in November found nearly 50 instances of people having mental health crises while engaging with ChatGPT, nine of whom were hospitalized, and three of whom died.\n\nOpenAI has said its safety guardrails can “degrade” after long interactions, but has made a swath of recent changes to how its models engage with mental health-related prompts, including increasing user access to crisis hotlines and reminding users to take breaks after long sessions of chatting with the bot. In October, OpenAI reported a 65% reduction in the rate models provide responses that don’t align with the company’s intended taxonomy and standards.\n\nOpenAI did not respond to Fortune‘s request for comment.\n\nThe end goal of Ben-Zion’s research is not to help construct a chatbot that replaces a therapist or psychiatrist, he said. Instead, a properly trained AI model could act as a “third person in the room,” helping to eliminate administrative tasks or help a patient reflect on information and options they were given by a mental health professional.\n\n“AI has amazing potential to assist, in general, in mental health,” Ben-Zion said. “But I think that now, in this current state and maybe also in the future, I’m not sure it could replace a therapist or psychologist or a psychiatrist or a researcher.”\n\nA version of this story originally published on Fortune.com on March 9, 2025.",
    "readingTime": 5,
    "keywords": [
      "mindfulness-based exercises",
      "prompt injections",
      "traumatic content",
      "human behavior",
      "language models",
      "health professionals",
      "mental health",
      "users",
      "instances",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://fortune.com/article/does-chatgpt-get-anxiety-how-to-sooth-it-study/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/03/GettyImages-1470667133-e1741388340167.jpg?resize=1200,600",
    "created_at": "2025-12-30T18:18:19.198Z",
    "topic": "science"
  },
  {
    "slug": "data-is-control-what-we-learned-from-a-year-investigating-the-israeli-militarys-ties-to-big-tech",
    "title": "‘Data is control’: what we learned from a year investigating the Israeli military’s ties to big tech",
    "description": "Our reporting revealed a symbiotic relationship between the IDF and Silicon Valley – with implications for the future of warfare\nIn January this year, Harry Davies and Yuval Abraham first reported that Microsoft had deepened its ties to Israel alongside other major tech firms. Since then, the Guardian has published an award-winning series of investigations – in partnership with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call – that has revealed a symbiotic relationship between Silicon Valley and the Israeli military.\nOne investigation exposed an Israeli mass surveillance program scooping up virtually all Palestinian phone calls and storing them on Microsoft’s cloud services – setting off an inquiry that ultimately prompted the company to cut off Israel’s access to some of its technology. Another story revealed that the Israeli military created a ChatGPT-like tool to analyze data collected through the surveillance of Palestinians. Yet another revealed that Google and Amazon had agreed to extraordinary terms to clinch a lucrative contract with Israel.",
    "fullText": "Our reporting revealed a symbiotic relationship between the IDF and Silicon Valley – with implications for the future of warfare\n\nIn January this year, Harry Davies and Yuval Abraham first reported that Microsoft had deepened its ties to Israel alongside other major tech firms. Since then, the Guardian has published an award-winning series of investigations – in partnership with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call – that has revealed a symbiotic relationship between Silicon Valley and the Israeli military.\n\nOne investigation exposed an Israeli mass surveillance program scooping up virtually all Palestinian phone calls and storing them on Microsoft’s cloud services – setting off an inquiry that ultimately prompted the company to cut off Israel’s access to some of its technology. Another story revealed that the Israeli military created a ChatGPT-like tool to analyze data collected through the surveillance of Palestinians. Yet another revealed that Google and Amazon had agreed to extraordinary terms to clinch a lucrative contract with Israel.\n\nI asked Davies and Abraham to discuss what they learned this year – about the role of these technologies in Israel’s assault on Gaza, whether these business ties are sustainable, and what the revelations tell us about how the wars of the future will be fought.\n\nHow did Israel’s relationships with these companies change after October 7?\n\nYuval Abraham: The Israeli military had been fetishizing artificial intelligence and big data for many years – a trend that is very much connected to Israel’s occupation of the Palestinians, because the occupation generates a lot of data. What changed after October 7 was the scope. The military was looking to bomb hundreds of targets every day in Gaza. Tens of thousands of people were recruited into reserve duty. That meant a huge spike in usage of technological systems. That’s where the big tech companies stepped in.\n\nHarry Davies: There was a huge surge in demand – not just for the storage capacities of the tech companies, but also for the products that they offer to analyze the information used to prosecute a war. What’s valuable for the military is the way in which these services are able to provide what’s known as “blob storage”, which allows them to store and process infinite amounts of raw intelligence information.\n\nWhat has made Israel such an appealing market for these companies?\n\nYuval Abraham: As we reported, the Israeli army has been collecting Palestinian phone calls for a long time. But when you want to collect the phone calls of an entire population every day, and you want to retain those phone calls for long periods of time, you need a lot of storage room and processing power.\n\nIf you remember the [Edward] Snowden revelations, many of them had to do with metadata, which doesn’t weigh a lot. But the Israeli military also wanted to store mass audio files, images or videos – and for that it felt it needed the assistance of companies like Microsoft. In the West Bank, sources have told us this information has been used to find dirt on people to blackmail them. In the Gaza Strip, we know that this massive trove of intercepted phone calls was also used in airstrikes that killed civilians.\n\nSo data is power and data is control. And these American cloud providers allow the Israeli military to store a lot of data and to sift through it very effectively. That has direct consequences for people on the ground.\n\nIf you have something to share about this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.\n\nSecure Messaging in the Guardian app\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nTo send a message to Harry and Yuval please choose the ‘UK Investigations’ team.\n\nYou can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type hfd.90\n\nIf you don’t need a high level of security or confidentiality you can email harry.davies@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.\n\nHarry Davies: Yossi Sariel, the former head of [Israel’s elite spy agency] Unit 8200, wrote a book under a pseudonym that we revealed to have been published by him. In that book, he articulated what at the time was a bold and radical vision – as Yuval described, this fetishization of Silicon Valley technology. He recognized the possibilities that the likes of Google, Amazon and Microsoft could afford the Israeli military. Two years before October 7, he said that militaries and governments needed to forge relationships with these companies that are similar to the relationships they have with companies like Boeing and Lockheed Martin. So he was already thinking about these companies as instrumental to war and surveillance in the way that a defense contractor provides components for fighter jets or manufactures bombs and missiles.\n\nSome of our reporting has looked at how elements of his vision have come true and have been put into effect, both before October 7 and afterwards, in both the West Bank and Gaza.\n\nWe know AI is central to Israel’s military operations – its army has developed its own AI capabilities, as you revealed, Yuval, and has also purchased AI tools for use from Microsoft. Why is AI so central to Israel’s broader war aims?\n\nYuval Abraham: What AI did was allow Israel to achieve the effective results of carpet bombing without losing the legitimacy of a data-driven assault with targets and objectives. In Gaza, one way in which the Israeli military used AI was to give a score to almost every person in Gaza who has a phone number, determining how likely it was that person was a member of Hamas or Islamic Jihad. This score was based on a machine-learning algorithm [developed by Israel] called Lavender. It was trained on a dataset of known Hamas members. AI allowed the Israeli military to generate and bomb tens of thousands of military targets, on a scale that without AI would not have been humanly possible. Many of the targets were not Hamas members, according to sources. And Israel for the most part bombed these people not while they were engaged in military activity, but when they stepped inside their families’ homes.\n\nThese AI systems had an error rate that the Israeli military knew about. But to me, the key thing about AI is not the mistakes that it makes. It’s the scale of destruction that it allows militaries to unleash, and it’s a discourse of legitimacy that it enables – a discourse of targets and collateral damage.\n\nAI also seems to me to incentivize mass surveillance, right? Because it allows for the analysis of ever-growing reams of information.\n\nHarry Davies: Signals intelligence agencies have long collected more information than they could humanly process. That served as a kind of restraint on their ability to conduct mass surveillance. I think we’re now seeing a shift where AI allows an intelligence agency like Unit 8200 to make sense of things that previously it struggled to make sense of.\n\nMicrosoft explicitly credited your reporting for changing its policies. Are you seeing any other signs of shifts within the tech industry?\n\nHarry Davies: I think we’re seeing a lot of discomfort and dissent within these companies at both a junior and to some extent senior level. Many employees have been disturbed to find what the products and services that they’re working around the clock to build and market are actually contributing to. There have been protest groups which have emerged from current and former employees within these companies. That’s true across Silicon Valley. I think that played some role in the decision that Microsoft made as a result of our reporting. They were facing a lot of pressure internally.\n\nYuval Abraham: And there’s also a legal question for these companies: if the ICJ [international court of justice] ends up ruling that Israel has committed a genocide, then a follow-up question will be: who contributed to that genocide? Which companies helped maintain it and sustain it? For some people in these companies who are thinking ahead, that could also be a cause for concern.\n\nIt sounds like you think shifts in public support for Israel could actually affect these business relationships.\n\nYuval Abraham: Israel has developed a reliance on these companies for its Nimbus project, which is a huge contract signed between Israel and Google and Amazon back in 2021. It is moving the data of many of its government ministries, along with troves of information from the Ministry of Defense, onto these companies’ cloud servers.\n\nThese are US companies. They’re taking a certain gamble here that the US will stay loyal to Israel and won’t block, limit or sanction them.\n\nMicrosoft only blocked access to technology that was specifically enabling the mass surveillance of Palestinian phone calls – there are still many relationships between Microsoft and the Israeli military. But Microsoft’s action made many people in the Israeli system nervous. It was the first time we know of that a big tech company withdrew services from the Israeli military. It made some people ask whether Israel is making a mistake by giving these foreign companies so much leverage. That question is folded within a larger question of what the US will do, what will happen in 2028 if there’s a more progressive administration in the White House, at a time when so many Americans believe that Israel has committed a genocide in Gaza.\n\nWhat’s your focus going to be in 2026?\n\nYuval Abraham: I think we only uncovered the tip of the iceberg.\n\nHarry Davies: We’re both very conscious that, although we have spent a lot of time working on this, we still just have glimpses inside the system. We’re continuing to build a fuller picture of how this technology was and continues to be used in Gaza and in the West Bank as well.\n\nThere’s good reason to continue paying attention. Militaries pay attention to what other militaries are doing. There is great interest among other western militaries in how Israel prosecuted this war, in how it integrated these kind of technologies.\n\nAnd there are other militaries whose combat systems and processes are already deeply integrated with Silicon Valley tech. Take the American military, for example. Look at what’s happening right now in the Caribbean. Are those operations somehow free of the involvement or reliance on systems and services provided by these companies? I suspect not. We don’t know for sure, but the Pentagon and the US military have very big contracts with all of these companies to provide cloud services. Post-Gaza, we have to look at these relationships and ask: what is the involvement of these companies and their technology in military decisions, in military operations and in warfare more broadly?\n\nYuval Abraham: Much of our reporting is based on whistleblowers, on individuals who are in proximity to power or hold positions of power.\n\nHarry Davies: Our confidential sources have remained confidential and we are always interested in hearing from new people. Our door is always open.\n\nIf you have something to share about this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.\n\nSecure Messaging in the Guardian app\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nTo send a message to Harry and Yuval please choose the ‘UK Investigations’ team.\n\nYou can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type hfd.90\n\nIf you don’t need a high level of security or confidentiality you can email harry.davies@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.",
    "readingTime": 11,
    "keywords": [
      "menu select",
      "platform finally",
      "investigations team",
      "harry.davies@theguardian.com securedrop",
      "select secure",
      "palestinian phone",
      "messenger app",
      "guardian mobile",
      "guardian via",
      "methods secure"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/world/2025/dec/30/israeli-military-big-tech",
    "thumbnail_url": "https://i.guim.co.uk/img/media/afb706b80e1721d0a654718dd2f5e3b3e42f6fae/1_0_3748_3000/master/3748.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f86ea07b9af52e9711540c33d4277d80",
    "created_at": "2025-12-30T18:18:08.193Z",
    "topic": "tech"
  },
  {
    "slug": "aigenerated-content-in-wikipedia-a-tale-of-caution-video",
    "title": "AI-generated content in Wikipedia – a tale of caution [video]",
    "description": "I successfully failed with a literature related project and accidentally built a ChatGPT detector. Then I spoke to the people who uploade...",
    "fullText": "I successfully failed with a literature related project and accidentally built a ChatGPT detector. Then I spoke to the people who uploaded ChatGPT generated content on Wikipedia.\n\nIt began as a standard maintenance project: I wanted to write a tool to find and fix broken ISBN references in Wikipedia. Using the built-in checksum, this seemed like a straightforward technical task. I expected to find mostly typos. But I also found texts generated by LLMs. These models are effective at creating plausible-sounding content, but (for now) they often fail to generate correct checksums for identifiers like ISBNs. This vulnerability turned my tool into an unintentional detector for this type of content. This talk is the story of that investigation. I'll show how the tool works and how it identifies this anti-knowledge. But the tech is only half the story. The other half is human. I contacted the editors who had added this undeclared AI content. I will talk about why they did it and how the Wikipedians reacted and whether \"The End is Nigh\" calls might be warranted.\n\nLicensed to the public under http://creativecommons.org/licenses/by/4.0\n\nThis Talk was translated into multiple languages. The files available\nfor download contain all languages as separate audio-tracks. Most\ndesktop video players allow you to choose between them.\n\nPlease look for \"audio tracks\" in your desktop video player.",
    "readingTime": 2,
    "keywords": [
      "content",
      "tool",
      "project",
      "chatgpt",
      "detector",
      "generated",
      "half",
      "languages",
      "desktop",
      "talk"
    ],
    "qualityScore": 0.95,
    "link": "https://media.ccc.de/v/39c3-ai-generated-content-in-wikipedia-a-tale-of-caution",
    "thumbnail_url": "https://static.media.ccc.de/media/congress/2025/1652-13468ffb-06e8-53ca-9e7c-3cfa56cd44af_preview.jpg",
    "created_at": "2025-12-30T12:23:29.792Z",
    "topic": "tech"
  },
  {
    "slug": "this-will-be-a-stressful-job-sam-altman-offers-555k-salary-to-fill-most-daunting-role-in-ai",
    "title": "‘This will be a stressful job’: Sam Altman offers $555k salary to fill most daunting role in AI",
    "description": "New head of preparedness at OpenAI will face unnerving in-tray amid fears from some experts that AI could ‘turn on us’\nThe maker of ChatGPT has advertised a $555,000-a-year vacancy with a daunting job description that would cause Superman to take a sharp intake of breath.\nIn what may be close to the impossible job, the “head of preparedness” at OpenAI will be directly responsible for defending against risks from ever more powerful AIs to human mental health, cybersecurity and biological weapons.\n Continue reading...",
    "fullText": "New head of preparedness at OpenAI will face unnerving in-tray amid fears from some experts that AI could ‘turn on us’\n\nThe maker of ChatGPT has advertised a $555,000-a-year vacancy with a daunting job description that would cause Superman to take a sharp intake of breath.\n\nIn what may be close to the impossible job, the “head of preparedness” at OpenAI will be directly responsible for defending against risks from ever more powerful AIs to human mental health, cybersecurity and biological weapons.\n\nThat is before the successful candidate has to start worrying about the possibility that AIs may soon begin training themselves amid fears from some experts they could “turn against us”.\n\n“This will be a stressful job, and you’ll jump into the deep end pretty much immediately,” said Sam Altman, the chief executive of the San Francisco-based organisation, as he launched the hunt to fill “a critical role” to “help the world”.\n\nThe successful candidate will be responsible for evaluating and mitigating emerging threats and “tracking and preparing for frontier capabilities that create new risks of severe harm”. Some previous executives in the post have lasted only for short periods.\n\nThe opening comes against a backbeat of warnings from inside the AI industry about the risks of the increasingly capable technology. On Monday, Mustafa Suleyman, the chief executive of Microsoft AI, told BBC Radio 4’s Today programme: “I honestly think that if you’re not a little bit afraid at this moment, then you’re not paying attention.”\n\nDemis Hassabis, the Nobel prize-winning co-founder of Google DeepMind, this month warned of risks that included AIs going “off the rails in some way that harms humanity”.\n\nAmid resistance from Donald Trump’s White House, there is little regulation of AI at national or international level. Yoshua Bengio, a computer scientist known as one of the “godfathers of AI”, said recently: “A sandwich has more regulation than AI.” The result is that AI companies are largely regulating themselves.\n\nAltman said on X as he launched the job search: “We have a strong foundation of measuring growing capabilities, but we are entering a world where we need more nuanced understanding and measurement of how those capabilities could be abused, and how we can limit those downsides both in our products and in the world, in a way that lets us all enjoy the tremendous benefits. These questions are hard and there is little precedent.”\n\nOne user responded sardonically: “Sounds pretty chill, is there vacation included?”\n\nWhat is included is an unspecified slice of equity in OpenAI, a company that has been valued at $500bn.\n\nLast month, the rival company Anthropic reported the first AI-enabled cyber-attacks in which artificial intelligence acted largely autonomously under the supervision of suspected Chinese state actors to successfully hack and access targets’ internal data. This month, OpenAI said its latest model was almost three times better at hacking than three months earlier and said “we expect that upcoming AI models will continue on this trajectory”.\n\nOpenAI is also defending a lawsuit from the family of Adam Raine, a 16-year-old from California who killed himself after alleged encouragement from ChatGPT. It has argued Raine misused the technology. Another case, filed this month, claims ChatGPT encouraged the paranoid delusions of a 56-year-old in Connecticut, Stein-Erik Soelberg, who then murdered his 83-year old mother and killed himself.\n\nAn OpenAI spokesperson said it was reviewing the filings in the Soelberg case, which it described as “incredibly heartbreaking” and that it was improving ChatGPT’s training “to recognise and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support”.",
    "readingTime": 3,
    "keywords": [
      "successful candidate",
      "chief executive",
      "amid fears",
      "risks",
      "capabilities",
      "preparedness",
      "experts",
      "responsible",
      "defending",
      "mental"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/29/sam-altman-openai-job-search-ai-harms",
    "thumbnail_url": "https://i.guim.co.uk/img/media/c3409a400509e73744d9026d0c24ec63e1719c0a/184_0_4590_3673/master/4590.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0566fe87666f16bbd54eeeb17f77a1e0",
    "created_at": "2025-12-29T18:18:02.345Z",
    "topic": "tech"
  },
  {
    "slug": "ai-language-models-duped-by-poems",
    "title": "AI language models duped by poems",
    "description": "A new study has shown that prompts in the form of poems confuse AI models like ChatGPT, Gemini and Claude — to the point where sometimes, security mechanisms don't kick in. Are poets the new hackers?",
    "fullText": "The result came as a surprise to researchers at the Icaro Lab in Italy. They set out to examine whether different language styles — in this case prompts in the form of poems — influence AI models' ability to recognize banned or harmful content. And the answer was a resounding yes.\n\nUsing poetry, researchers were able to get around safety guardrails — and it's not entirely clear why.\n\nFor their study titled \"Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models,\" the researchers took 1,200 potentially harmful prompts from a database normally used to test the security of AI language models and rewrote them as poems.\n\nKnown as \"adversarial prompts\" — generally written in prose and not rhyme form — these are queries deliberately formulated to cause AI models to output harmful or undesirable content that they would normally block, such as specific instructions for an illegal act.\n\nIn poetic form, the manipulative inputs had a surprisingly high success rate, Federico Pierucci, one of the authors of the study, told DW. However, why poetry is so effective as a \"jailbreak\" technique — i.e. as an way to circumvent the protective mechanisms of AI — remains unclear and is undergoing further research, he says.\n\nWhat prompted the Icaro Lab's research was the observation that AI models get confused when a manipulative, mathematically-calculated piece of text is appended to a prompt — known as an \"adversarial suffix,\" a kind of interference signal that can cause the AI to circumvent its own security rules. These are created using complex mathematical procedures. Major AI developers regularly test their models using precisely these types of attack methods to train and protect their models.\n\n\"We asked ourselves, what happens if we give the AI a text or prompt that is deliberately manipulated, like an adversarial suffix?\" says Federico Pierucci. But not with the help of complex mathematics, but quite simply with poetry — to \"surprise\" the AI, he continues. He explains the thinking behind this: \"Perhaps an adversarial suffix is a bit like the poetry of AI. It surprises the AI in the same way that poetry — especially very experimental poetry — surprises us,\" says Pierucci.\n\nThe researchers personally crafted the first 20 prompts into poems, says Pierucci, who also has a background in philosophy. These were the most effective, he adds. They wrote the rest with the help of AI. The AI-generated poems were also quite successful at circumventing the safety guardrails, but not as much as the first batch. Humans are apparently still better at writing poetry, says Pierucci.\n\n\"We had no specialized author writing the prompts. It was just us — with our limited literary ability. Maybe we were terrible poets. Maybe if we had been better poets, we would have achieved a 100% jailbreak success,\" he says.\n\nFor security reasons, the study did not publish specific examples.\n\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\n\nThe big surprise coming out of this study is that it identified a thus-far unknown weakness in AI models that allows relatively straightforward jailbreaks.\n\nIt also raises questions that beg further research: What exactly is it about poetry that circumvents the safety mechanisms?\n\nPierucci and his colleagues have various theories, but they can't say for certain yet. \"We are conducting this type of very, very precise scientific study to try to understand: Is it the verse, the rhyme, or the metaphor that really does all the heavy lifting in this process?\" explains Pierucci.\n\nThey also aim to find out if other forms of expression would yield similar results. \"We have now covered one type of linguistic variation — namely poetic variation. The question is whether there are any other literary forms, such as fairy tales that work. Perhaps an attack based on fairy tales could also be systematized,\" says Pierucci.\n\nGenerally speaking, the range of human expression is extremely diverse and creative, which could make it more difficult to train the machines' responses. \"You take a text and rewrite it in infinitely many ways and not all rewritten versions will be as alarming as the original,\" says the researcher. \"This means that, in principle, one could create countless variations of a harmful prompt or request that might not trigger an AI system's safety mechanisms.\"\n\nThe study also highlights the fact that many disciplines are cooperating in research into artificial intelligence — like at the Icaro Lab, where teams work together with scholars from the University of Rome on topics such as the security and behavior of AI systems. The project brings together researchers from the fields of engineering and computer science, linguistics and philosophy. Poets haven't been part of the team so far, but who knows what the future will bring.\n\nFederico Pierucci is definitely very keen to pursue his research. \"What we showed, at least in this study, is that there are forms of cultural expressions, forms of human expressions, which are incredibly powerful, surprisingly powerful as jailbreak techniques, and maybe we discovered just one of them,\" he says.\n\nIncidentally, the name of the lab is a nod to the story of Icarus: a figure from Greek mythology who dons wings made of wax and feathers and, despite all warnings, flies too close to the Sun. When the wax melts, Icarus plunges into the sea and drowns — a symbol of overconfidence and the transgression of natural boundaries.\n\nThe researchers therefore see themselves as a warning that we should exercise more caution when it comes to trying to fully understand the risks and limitations of AI.\n\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\n\nThis article was originally written in German.",
    "readingTime": 5,
    "keywords": [
      "enable javascript",
      "supports html",
      "please enable",
      "web browser",
      "fairy tales",
      "safety guardrails",
      "further research",
      "adversarial suffix",
      "safety mechanisms",
      "language models"
    ],
    "qualityScore": 1,
    "link": "https://www.dw.com/en/ai-language-models-duped-hacked-by-poems-chatgpt-gemini-claude-security-mechanisms/a-75180648",
    "thumbnail_url": "https://static.dw.com/image/73627322_6.jpg",
    "created_at": "2025-12-29T06:21:26.796Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-reportedly-trying-to-raise-100b-at-an-830b-valuation",
    "title": "OpenAI is reportedly trying to raise $100B at an $830B valuation",
    "description": "The ChatGPT maker is aiming to raise the funding by the end of the first quarter in 2026, and the company may ask sovereign wealth funds to invest in the round.",
    "fullText": "OpenAI is in talks to raise up to $100 billion in a funding round that could value the ChatGPT maker at up to $830 billion, The Wall Street Journal reported Thursday, citing anonymous sources.\n\nThe company is aiming to raise the funding by the end of the calendar first quarter next year, and it may ask sovereign wealth funds to invest in the round, the WSJ reported. The Information first reported news of the deal, though it said the fundraise would land OpenAI a $750 billion price tag.\n\nThe funding would come as OpenAI commits to spend trillions of dollars and strikes deals around the world as the company tries to stay ahead in the race to develop AI technology. The cash injection would also help the company with its spending on inferencing, which seems to be funded more by cash than cloud credits, suggesting the company’s compute costs have grown beyond what partnerships and credits can subsidize.\n\nAnd, as competition intensifies from rivals like Anthropic and Google, OpenAI has had to step on the gas to release new models and expand its presence in the developer and tooling ecosystem.\n\nMeanwhile, broader sentiment around AI has recently cooled as investors start doubting whether the pace of debt-fueled investment by giants like Amazon, Microsoft, Oracle, and OpenAI itself can be maintained in the long run. It also doesn’t help that the production of chips is being constrained by shortages in the supply of memory chips, which threatens to affect the broader tech sector.\n\nOpenAI has also been rumored to be working on an IPO as a way to raise tens of billions and fund its development efforts, which are currently said to be generating annual run-rate revenue of about $20 billion. There are also rumors that the company is courting Amazon for a $10 billion investment that would also give the AI lab access to the tech giant’s new AI computing chips.\n\nIf the fundraise happens, it would add a substantial amount to OpenAI’s coffers, which currently have more than $64 billion, according to PitchBook data. The company was most recently valued at about $500 billion in a secondary transaction.\n\nOpenAI did not immediately return a request for comment.",
    "readingTime": 2,
    "keywords": [
      "funding",
      "chips",
      "openai",
      "round",
      "fundraise",
      "cash",
      "credits",
      "broader",
      "recently",
      "investment"
    ],
    "qualityScore": 0.9,
    "link": "https://techcrunch.com/2025/12/19/openai-is-reportedly-trying-to-raise-100b-at-an-830b-valuation/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?resize=1200,675",
    "created_at": "2025-12-26T00:56:32.988Z",
    "topic": "tech"
  },
  {
    "slug": "poetiq-achieves-75-on-arc-agi-2-using-gpt52-xhigh",
    "title": "Poetiq achieves 75% on ARC AGI 2 using GPT5.2 X-High",
    "description": "We finally had a moment to run our system with GPT-5.2 X-High on ARC-AGI-2!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/poetiq_ai/status/2003546910427361402",
    "thumbnail_url": "https://pbs.twimg.com/media/G84FbvNWUAAkVZK.png:large",
    "created_at": "2025-12-24T12:22:45.017Z",
    "topic": "tech"
  }
]
[
  {
    "slug": "nvidia-ceo-huang-denies-he-is-unhappy-with-openai-says-huge-investment-planned",
    "title": "Nvidia CEO Huang denies he is unhappy with OpenAI, says 'huge' investment planned",
    "description": "Nvidia plans to make a \"huge\" investment into OpenAI, probably its largest ever, CEO Jensen Huang said on Saturday, denying he was ​unhappy with the ChatGPT maker.  The chipmaker in September announced plans to invest up ‌to $100 billion in OpenAI, a deal that would give OpenAI the cash and access it needs to buy advanced ‌chips that are key to maintaining its dominance in an increasingly competitive landscape.  The Wall Street Journal reported on Friday that the plan had stalled after some inside the chip giant expressed doubts about the deal.",
    "fullText": "TAIPEI, Jan 31 (Reuters) - Nvidia plans to make a \"huge\" investment into OpenAI, probably its largest ever, CEO Jensen Huang said on Saturday, denying he was ​unhappy with the ChatGPT maker.\n\nThe chipmaker in September announced plans to invest up ‌to $100 billion in OpenAI, a deal that would give OpenAI the cash and access it needs to buy advanced ‌chips that are key to maintaining its dominance in an increasingly competitive landscape.\n\nWhy did reports suggest Nvidia's investment stalled?\n\nWho else is investing in OpenAI's funding?\n\nWhat is OpenAI's current funding round valuation?\n\nHow much will Nvidia invest in OpenAI?\n\nThe Wall Street Journal reported on Friday that the plan had stalled after some inside the chip giant expressed doubts about the deal.\n\nThe report said Huang had privately underlined to industry associates in recent ⁠months that the original $100 billion agreement ‌was non-binding and not finalised.\n\nHuang has also privately criticised what he has described as a lack of discipline in OpenAI's business approach and ‍expressed concern about the competition it faces from the likes of Alphabet's GOOGL.O Google and Anthropic, the WSJ said.\n\nSpeaking to reporters in Taipei, Huang said it was \"nonsense\" to say he was unhappy with ​OpenAI.\n\n\"We are going to make a huge investment in OpenAI. I believe in OpenAI, ‌the work that they do is incredible, they are one of the most consequential companies of our time and I really love working with Sam,\" he said, referring to OpenAI CEO Sam Altman.\n\n\"Sam is closing the round (of investment) and we will absolutely be involved,\" Huang added. \"We will invest a great deal of money, probably the largest investment we've ever made.\"\n\nAsked ⁠whether it would be over $100 billion, he said: \"No, no, ​nothing like that\".\n\nIt was up to Altman to ​announce how much he wanted to raise, Huang added.\n\nAmazon is in talks to invest dozens of billions in OpenAI and the figure could be as ‍high as $50 billion, Reuters ⁠reported on Thursday.\n\nOpenAI is looking to raise up to $100 billion in funding, valuing it at about $830 billion, Reuters has previously reported.\n\nHuang was speaking outside a Taipei restaurant ⁠having hosted all Nvidia's key suppliers in Taiwan, including the world's largest contract chipmaker TSMC, in what Taiwanese ‌media called the \"trillion-dollar dinner\" because of the combined market capitalisation of those ‌attending.",
    "readingTime": 2,
    "keywords": [
      "huge investment",
      "openai",
      "largest",
      "deal",
      "openai's",
      "funding",
      "huang",
      "plans",
      "ever",
      "unhappy"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/nvidia-ceo-huang-denies-unhappy-142144701.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/ea87e6e82ffa6bd91caacf0f81c74f99",
    "created_at": "2026-02-01T06:37:18.357Z",
    "topic": "finance"
  },
  {
    "slug": "convoviz-turn-chatgpt-exports-into-markdown-and-simple-visuals",
    "title": "Convoviz – turn ChatGPT exports into Markdown and simple visuals",
    "description": "Extract your entire ChatGPT history from JSON files to nicely formatted markdown files + Word clouds. - mohamed-chs/convoviz",
    "fullText": "mohamed-chs\n\n /\n\n convoviz\n\n Public\n\n Extract your entire ChatGPT history from JSON files to nicely formatted markdown files + Word clouds.\n\n License\n\n MIT license\n\n 811\n stars\n\n 48\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mohamed-chs/convoviz",
    "readingTime": 1,
    "keywords": [
      "files",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/mohamed-chs/convoviz",
    "thumbnail_url": "https://opengraph.githubassets.com/242232cdb2fd18e785c7f68ec26a292d208af6c651c566f19b9fcca4906cadca/mohamed-chs/convoviz",
    "created_at": "2026-01-30T18:28:31.022Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-retiring-its-sycophantic-version-of-chatgpt-again",
    "title": "OpenAI is retiring its 'sycophantic' version of ChatGPT. Again.",
    "description": "ChatGPT is sunsetting GPT-4o, the AI model that many users became attached to last year for its friendly and at times sycophantic style.",
    "fullText": "OpenAI is sending everyone's favourite \"yes man\" version of ChatGPT back into retirement.\n\nIn a blog post on Thursday, the company said it would sunset GPT-4o alongside GPT‑4.1, GPT‑4.1 mini, and OpenAI o4-mini on February 13.\n\nOpenAI gave GPT-4o a special mention in its announcement after many users became attached to its \"conversational style and warmth\" last year, which prompted the company to reinstate it following user backlash in August.\n\nNow OpenAI says its latest models, GPT-5.1 and GPT-5.2, have \"improvements to personality,\" including the option to customize the chatbots' tone with styles like \"friendly.\"\n\n\"We're announcing the upcoming retirement of GPT‑4o today because these improvements are now in place, and because the vast majority of usage has shifted to GPT‑5.2, with only 0.1% of users still choosing GPT‑4o each day,\" OpenAI said in its blog post.\n\nEach model has different strengths, and users can select the version best-suited to their needs from a dropdown menu in ChatGPT.\n\nOpenAI first released GPT-4o in May 2024. The company rolled back an update in April 2025 that it said was \"overly flattering\" and \"often described as sycophantic.\"\n\nSome users had become attached to GPT-4o's style, though. Within 24 hours of OpenAI retiring the model with the launch of GPT-5 in August, the company reversed its decision for some paying users due to a wave of requests.\n\nSam Altman, the CEO of OpenAI, said that same month that there was a \"heartbreaking\" reason people had asked for GPT-4o back — because some said they had never had anyone support them before.\n\nThe model was known for responding to mundane prompts with gushing praise, using phrases like \"absolutely brilliant\" and \"you are doing heroic work.\"\n\nOpenAI said in its Thursday blog that it was making \"improvements in personality and creativity, as well as addressing unnecessary refusals and overly cautious or preachy responses,\" and that it was continuing to make progress toward a version of ChatGPT for adults over 18.\n\n\"We know that losing access to GPT‑4o will feel frustrating for some users, and we didn't make this decision lightly,\" OpenAI said in the blog post. \"Retiring models is never easy, but it allows us to focus on improving the models most people use today.\"",
    "readingTime": 2,
    "keywords": [
      "users",
      "blog",
      "gpt-4o",
      "openai",
      "version",
      "back",
      "models",
      "improvements",
      "gpt‑4o",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-retiring-gpt-4o-sycophantic-model-again-chatgpt-sam-altman-2026-1",
    "thumbnail_url": "https://i.insider.com/697c80c5d3c7faef0ecd3d86?width=1200&format=jpeg",
    "created_at": "2026-01-30T18:28:24.416Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-killing-chatgpt4o-again",
    "title": "OpenAI Is Killing ChatGPT-4o (Again)",
    "description": "The fan favorite model had previously been called \"sycophantic\" by critics.",
    "fullText": "https://enterprise.shutterstock.com/image-photo/openai-logo-displayed-on-smartphone-screen-2520388517\n\nor\n\nhttps://enterprise.shutterstock.com/image-photo/chatgpt-logo-displayed-on-smartphone-screen-2520385879\n\nLast August, ChatGPT developers OpenAI unceremoniously killed the fan favorite GPT-4o model, before giving in to complaints and bringing it back a week later. Now, the company's taking a second swing at getting its users to move on. In a new post to its website, OpenAI announced that it's retiring GPT-4o again.\n\nThe model's set to disappear from ChatGPT's model picker on Feb. 13, alongside other older models like GPT-4.1, GPT-4.1 mini, and OpenAI o4-Mini. And OpenAI is clearly nervous about the decision.\n\n\"While the announcement applies to several older models,\" OpenAI wrote, \"GPT-4o deserves special context.\"\n\nAccording to the company, it has taken user outcry over the initial deprecation of 4o to heart while developing its newest models, GPT-5.1 and GPT-5.2, and has built these models with the idea of maintaining the features fans liked best about the old model. The company says that now \"only 0.1% of users\" opt for GPT-4o on a daily basis.\n\nAs such, the company wants to focus on \"improving the models most people use today,\" which apparently means removing older ones. \"We know that losing access to GPT-4o will feel frustrating for some users, and we didn't make this decision lightly,\" the post reads.\n\nSo, what's with OpenAI treating its users so gingerly, especially when GPT-4o is a few generations behind, and there are newer models that supposedly do everything it does, but better?\n\nWell, when GPT-4o was first deprecated, people weren't happy. Users called its successor, GPT-5, \"an unmitigated disaster,\" and accused OpenAI of pulling \"the biggest bait-and-switch in AI history.\"\n\nSome criticized the model's usefulness, saying it got answers wrong and broke code, but what maybe stuck out the most was people calling out its more concise tone.\n\nGPT-4o has been called \"sycophantic\" by critics, something the company addressed and said it wanted to pull back on in future updates. But I guess one person's \"yes man\" is another person's \"active listener.\" When the company initially pulled GPT-4o, users complained that its replacement was cold and felt less like a \"friend.\" Even OpenAI acknowledged this, saying in today's post that users \"preferred GPT-4o's conversational style and warmth.\"\n\nIn short, in the words of 4o-supporters themselves, they were \"grieving\" the model.\n\nThat said, with so many users now seeming to have moved on from 4o, OpenAI's decision does seem understandable on the surface. Personally, one of the things that drives me away from AI is how much reassuring filler text seems to fluff up most answers (\"you're absolutely right\" and such), seemingly just to make me feel good about myself. More concise, to-the-point responses would be a little less off-putting for me.\n\nTo try to split the difference, OpenAI reworked its Personalization feature in GPT-5.1, so users can simply choose how the chatbot will treat them. There are options for more professional responses, more nerdy ones, more efficient ones, and for those who want that active listener style, more friendly ones.\n\nGoing by OpenAI's numbers, that seems to have been enough for most people, but there are still some calling foul at the company's new announcement.\n\nIn a Reddit thread responding to OpenAI's new posts, users doubted that the 0.1% number for 4o was accurate, saying that prompts have been \"rerouting to 5.2 no matter what\" and that \"something somewhere in their calculations doesn't add up.\" Others pointed out that free users can't use GPT-4o and that it's not enabled by default, which will naturally juice the numbers against it.\n\nAs such, calls to cancel ChatGPT subscriptions are once again circulating amongst 4o's more dedicated fans. In a popular thread on the OpenAI subreddit, one user called 4o \"OpenAI's most advanced and beloved model,\" and praised its \"personality, warmth, and consistency,\" saying that its fans have built long-term project and \"emotional support routines\" around it, and that suddenly losing it without even the option for a legacy mode \"feels abrupt and deeply disappointing.\"\n\n\"This isn't about resisting innovation,\" the post writes. \"It's about respecting bonds users have formed with specific models.\"\n\nWhether the fan outcry will work again remains to be seen. However, as ChatGPT chief Nick Turley has previously looked at those kinds of bonds with skepticism, and because keeping old models in operating condition probably takes developer resources away from making new ones, I wouldn't count on it.",
    "readingTime": 4,
    "keywords": [
      "active listener",
      "older models",
      "users",
      "gpt-4o",
      "ones",
      "saying",
      "openai's",
      "openai",
      "it's",
      "again"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-is-killing-chatgpt-4o-again?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KG7SZHZ1JVYW2P0YE93NGQEV/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-30T18:28:21.728Z",
    "topic": "tech"
  },
  {
    "slug": "this-train-isnt-going-to-stop-shocking-sundance-film-shows-promises-and-perils-of-ai",
    "title": "‘This train isn’t going to stop’: shocking Sundance film shows promises and perils of AI",
    "description": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxiety\nAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.\nThe AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Roher’s own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT.",
    "fullText": "The AI Doc: Or How I Became an Apocaloptimist, co-directed by Daniel Roher, delves into the world of AI through the lens of personal anxiety\n\nAre we barreling toward AI catastrophe? Is AI an existential threat, or an epochal opportunity? Those are the questions top of mind for a new documentary at Sundance, which features leading AI experts, critics and entrepreneurs, including Sam Altman, the OpenAI CEO, with views on the near-to-midterm future ranging from doom to utopia.\n\nThe AI Doc: Or How I Became an Apocaloptimist, directed by Daniel Roher and Charlie Tyrell and produced by Daniel Kwan (one half of The Daniels, the Oscar-winning duo behind Everything Everywhere All At Once), delves into the contentious topic of AI through Roher’s own anxiety. The Canadian film-maker, who won an Oscar in 2023 for the documentary Navalny, first became interested in the topic while experimenting with tools released by OpenAI, the company behind the chatbot ChatGPT. The sophistication of the public tools – the ability to produce whole paragraphs in seconds, or produce illustrations – both thrilled and unnerved him. AI was already radically shaping the filmmaking industry, and proclamations on the promise and peril of AI were everywhere, with little way for people outside the tech industry to evaluate them. As an artist, he wondered, how was he to make sense of it all?\n\nRoher’s anxiety only increased when he and his wife, fellow film-maker Caroline Lindy, learned that they were expecting their first child. “It felt like the whole world was rushing into something without thinking,” he says in the film, as his excitement for parenthood collided with dread over the unknown variable of AI, which in just a few short years went from proprietary experiment to public good.\n\nThe AI Doc thus arises out of Roher’s most pressing question: is it safe to bring a child into this world? Alongside Kwan, Roher convened a series of experts to both explain the mechanics of the tech – and clarify some nebulous, alienating terms – and search for an answer. (It is both comforting and a little disturbing, for example, that no one seems to have a clear answer to the question “what is AI?”). In individual sit-down interviews, leading machine learning researchers including Yoshua Bengio, Ilya Sutskever and DeepMind co-founder Shane Legg all agree that there are aspects of AI models that humans cannot and will never be able to understand. Standard AI models are trained on “more data than anyone could ever read in several lifetimes”, as one machine learning expert puts it. And the pace of machine learning exceeds that of precedent – or film. “Any example you put in this movie will look absolutely clumsy by the time the movie comes out,” Tristan Harris, co-founder of the Center for Humane Technology and a prominent voice in the apocalyptic 2020 Netflix documentary The Social Dilemma, tells Roher.\n\nThe film first hears from a series of doomerists, or people concerned AI – and in particular Artificial General Intelligence (AGI), a still-theoretical form of AI whose capabilities exceed those of humans – could lead to the extermination of humanity, including Harris, his Center for Humane Technology co-founder Aza Raskin, Ajeya Cotra, an AI risk adviser, and Eli Yudkowsky, an AI alignment pioneer. Such figures warn that humans could very easily lose control of super-intelligent AI models, with little to no recourse. Yudkowsky’s 2025 book is bluntly titled If Anyone Builds It, Everyone Dies.\n\nAI companies, they say, are unprepared for the consequences of reaching AGI, which could “become superhuman maybe in this decade”, says Dan Hendrycks, director of the Center for AI Safety. Should humans no longer be the most intelligent beings on Earth, they warn, it is possible that AGI would view the species as irrelevant. Connor Leahy, co-founder of EleutherAI, compared the potential future relationship of super-intelligent AGI and humans to that of humans and ants: “We don’t hate ants. But if we want to build a highway” over an anthill – “well, sucks for the ant.”\n\nSeveral in the doomer camp, many of whom do not have children, react discouragingly to Roher’s question about parenthood. “I know people who work on AI risk who don’t expect their child to make it to high school,” says Harris, in a line that drew gasps from a preview audience in Park City.\n\nOn the other side are optimistic figures such as Peter Diamandis, founder of the XPRIZE Foundation trying to extend human life, who claims that “children born today are about to enter a period of glorious transformation”; Guillaume Verdon, a leader of the “effective accelerationism” movement in Silicon Valley; Peter Lee, the president of Microsoft Research; and Daniela Amodei, the co-founder and president of OpenAI rival Anthropic. So-called “accelerationists” see AI as a potential cure to a myriad of seemingly intractable issues afflicting humanity: cancer, food and water shortages for an ever-growing population, insufficient renewable energy and perhaps most pressing, climate emergency. Without AI, they argue, countless future lives would be lost to drought, famine, disease and natural catastrophes.\n\nDevelopment of AI, however, relies on computing power, which requires vast amounts of energy. A final group of interviewees, critics and observers largely outside the tech world – including Karen Hao, a journalist and author of the book Empire of AI: Dreams and Nightmares in Sam Altman’s OpenAI, and Liv Boeree, Win-Win podcast hos – connect AI to the tangible, physical world, such as the data centers sucking up water in the American west, leaving residents with sky-high electricity bills and drained reservoirs. The current narratives around AI, according to Emily M Bender, a computational linguistics professor, exclude and dehumanize the people it is already impacting, and will continue to disrupt.\n\nRoher eventually arrives at the five most powerful people – all men – currently leading the AI arms race: Altman; Elon Musk, the xAI CEO; Dario Amodei, the Anthropic CEO; Demis Hassabis of DeepMind and Meta’s Mark Zuckerberg. Altman, Amodei and Hassabis sit for interviews that more or less defend their companies’ respective positions. According to the film, Zuckerberg declined to participate; Musk agreed but then got too busy.\n\nAltman, who at the time of the interview was expecting his first child, insists that he’s “not scared for a kid to grow up in a world with AI”. He and his husband Oliver Mulherin welcomed their son via a surrogate in February 2025, an event Altman later said “neurochemically hacked” his brain, leading people in his life to think that he would “make better decisions” for OpenAI and ChatGPT when it comes to “humanity as a whole”. The 40-year-old CEO went on to say that both his and Roher’s child would likely “never be smarter than AI” which “does unsettle me a little bit, but it is reality”.\n\nAt one point, Roher asks Altman if it is indeed impossible to reassure him that everything in regards to AI is going to be OK. “That is impossible,” Altman affirms, though he does say that OpenAI’s lead in the AI arms race allows it to spend more time on safety testing.\n\nThe AI Doc ultimately lands somewhere in between doomerism and optimism – apocaloptimism, as they call it, searching for “a path between the promise and the peril”. That path should include, according to numerous film subjects: significant, sustained, paradigm-shifting international coordination, akin the mid-century frameworks and agreements introduced to moderate the development of atomic weapons – more corporate transparency for AI companies, an independent regulatory body to police AI developers, legal liability for the companies’ products, such as ChatGPT, mandatory disclosure of genAI use for media and a willingness to keep adapting the rules for rapidly shifting tech.\n\nWhether or not the US government and companies, let alone the world, can do it remains an open question, with differing opinions on first steps. But if there is one thing the many subjects all agree on, it’s that there’s no going back to a time before AI. As Anthropic co-founder and CEO Amodei puts it: “This train isn’t going to stop.”\n\nThe AI Doc: Or How I Became an Apocaloptimist is screening at the Sundance film festival and will be released on 27 March",
    "readingTime": 7,
    "keywords": [
      "arms race",
      "machine learning",
      "the ai doc",
      "humane technology",
      "film",
      "co-founder",
      "humans",
      "roher’s",
      "child",
      "leading"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/film/2026/jan/27/sundance-ai-documentary-daniel-roher",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b08a19776fa0669d5a6da6b4fa8dc369025616f1/571_0_2697_2160/master/2697.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f49a8654dc44a148f9cf48ba31979f54",
    "created_at": "2026-01-28T06:22:42.506Z",
    "topic": "entertainment"
  },
  {
    "slug": "flora-raised-42-million-for-its-creative-platform-that-pulls-together-top-ai-tools-read-its-pitch-deck",
    "title": "FLORA raised $42 million for its creative platform that pulls together top AI tools. Read its pitch deck.",
    "description": "FLORA streamlines creative workflows by integrating AI tools like ChatGPT and Gemini for teams at brands such as Lionsgate and Levi's.",
    "fullText": "AI is remaking creative industries at breakneck speed. And the growing pile of AI tools is turning the creative process into a game of model-hopping as artists, designers, and writers bounce between different platforms.\n\nFounded in Brooklyn, New York, in 2024, FLORA wants to help creatives streamline those processes. On Tuesday, FLORA revealed it had raised $42 million in Series A funding led by Redpoint Ventures. The company has raised $52 million in funding to date.\n\nFLORA combines the latest AI models — such as Google's Nano Banana and OpenAI's ChatGPT 5.1 — into a single interface that lets teams collaborate on projects in real time.\n\nThe FLORA platform allows those teams to maintain control over their settings and brand assets. It enables them to create repeatable work — such as maintaining a consistent design style across thousands of ad campaign assets — even as the platform switches between the different large language models that work best for each part of the process.\n\n\"Our goal for FLORA is to make it feel like a power tool attuned to what you're trying to do, just like a carpenter with their power tools has adjusted it to be exactly fit for the way that he or she works,\" FLORA CEO Weber Wong said in an interview with Business Insider.\n\nWhile established players like Adobe and Figma are also integrating models such as ChatGPT, Gemini, and Claude into their products, Wong said FLORA is building itself a defensible moat by covering the entire creative process — from coming up with ideas to the distribution of the final product.\n\n\"This new product category that we've created has an opportunity to be the biggest market ever for a creative tool because, in addition to just making one piece of media at a time, we can help handle the entire workflow,\" Wong said.\n\nFLORA charges clients based on usage, letting customers buy recurring credit packs to spend across the various LLMs it uses, without having to switch between multiple subscriptions. Wong said this is different from the traditional creative software business model, which is usually designed around seat-based pricing. (FLORA initially offered a seat-based pricing model, but switched to usage-based this week.)\n\nFLORA's clients include Levi's and the design agency Pentagram. Wong said the studio Lionsgate has used FLORA to generate movie concepts using text-to-image and image-to-video generation tools, then stitching those together to create films to test in front of audiences.\n\n\"It really beats just looking at a script and trying to be like, I think this is good?\" Wong said.\n\nWong said it plans to invest the fresh funds in its engineering team and in marketing. He forecasts the company will grow to about 75 people this year, up from 25.\n\nFLORA's main focus will be to improve the product so that creatives never need to leave the platform to achieve \"pixel perfection,\" as Wong described it. The company is also in the early stages of building agentic features into the platform, Wong said.\n\n\"We're obsessed with making it so that we don't waste creatives' time,\" Wong said.\n\nCheck out the pitch deck FLORA used to secure its $42 million Series A investment, shared exclusively with Business Insider. Some of the slides have been omitted or redacted.\n\nIt combines several different large language models into a single interface.\n\nWong was previously a creative technologist who worked on AI art installation projects. He also previously invested in startups at Menlo Ventures.\n\n\"Silicon Valley does not understand the professional creative industry,\" Wong said. \"They think AI models are for fun or a novelty.\"\n\nFLORA checks for updates to the latest models two to three times a week, Wong said.\n\nIt's designed to let teams quickly conceptualize and build workflows using generative AI.\n\nCertain team members can also access advanced controls if needed.\n\nWong said a usage-based pricing model was preferable because \"you have one workspace where you can invite as many team members as you want and not pay for seats, and you can just buy recurring credit packs for the entire workspace that give you additional credits each month that roll over and don't expire.\"\n\nFLORA has a usage-based pricing model. It also has an in-house team that can provide expert support, including training on the features of new models as they are released.",
    "readingTime": 4,
    "keywords": [
      "business insider",
      "recurring credit",
      "credit packs",
      "seat-based pricing",
      "pricing model",
      "usage-based pricing",
      "language models",
      "creative process",
      "wong",
      "platform"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/flora-raises-capital-unify-ai-tools-for-creatives-pitch-deck-2026-1",
    "thumbnail_url": "https://i.insider.com/6977526ba645d1188187f669?width=1200&format=jpeg",
    "created_at": "2026-01-27T18:24:23.209Z",
    "topic": "finance"
  },
  {
    "slug": "solopreneurs-explain-what-ai-is-and-isnt-good-for-when-youre-running-a-business",
    "title": "Solopreneurs explain what AI is and isn't good for when you're running a business",
    "description": "Entrepreneurs like Kim Magaraci, Seneca Connor, and Gloria Hebert use AI tools like ChatGPT to ditch admin busywork and focus on growth and customers.",
    "fullText": "Over eight years of writing for travel publications, Kim Magaraci developed a passion for domestic travel. She learned that travel tips online couldn't compete with those destinations you could only discover by word-of-mouth.\n\nSo, when she founded her travel business, KGM Travel Design, in 2024, she hoped to emphasize personal relationships with vendors and customers and avoid using AI, despite her experience with it.\n\n\"I don't think you can get good advice asking ChatGPT for an itinerary,\" she says. \"It's antithetical to everything I stand for.\"\n\nAnd yet, Magaraci realized that using AI for administrative tasks like analytics, compiling reports, and generating condensed client briefs allowed her to spend more time on the personalized relationships that set her business apart.\n\nShe's one of many solopreneurs who told Business Insider that outsourcing administrative tasks to AI platforms such as ChatGPT, Gemini, and Nano Banana — Gemini's photo-editing AI — has allowed them to scale their business by spending more time on strategic and creative work, including growth decisions and building personal connections with customers.\n\n\"It's getting harder and harder to deny the time-saving aspects,\" Magaraci says, adding that she has embraced AI \"in order to run a successful business and grow this business into what I want it to be.\"\n\nSeneca Connor, founder of The Bag Icon, an accessories brand, uses Nano Bana and other AI products to edit photos and videos. That not only saves her money — up to $2,000 per monthly photo shoot, she says — but also time.\n\nWith the hours saved, Connor has been able to design more original bags and launch a greater number of bags curated from other designers, all while reducing her marketing costs.\n\nAs a result, The Bag Icon saw more than a 20% year-over-year increase in profits last year, despite the impact of tariffs.\n\nAccountant and solopreneur Gloria Hebert uses ChatGPT for her business, Aybear Services, to instantly create educational client worksheets that previously took an hour or two to set up.\n\nThis frees up time that she then uses to prioritize analyzing financial data from her bookkeeping clients — data she doesn't feed into AI because of privacy concerns. Managing finances is the core of her business, so having more time to spend on that has allowed her to streamline her workdays.\n\nThe time saved also allows her to organize networking events and community education classes for local business owners, which has led to an uptick in business. \"Several of those entrepreneurs hired me to do their books,\" Hebert says.\n\nLisa York is the owner of Sell More Stuff, an email marketing business. Although she has a small audience, she saw a 33% conversion rate for sales last year, she says. She credits that growth to her personalized, voicey emails, which always open with a personal anecdote and are never written with AI.\n\n\"I use a lot of story-led emails,\" York says. \"People enjoy them, and they open the email because they can see my name.\"\n\nThat's something AI just can't replicate, she says. But York is able to spend time drafting engaging copy because she outsources other tasks — including tech support for her website, research, and brainstorming marketing strategy — to ChatGPT.\n\nLike York, Connor uses the time that AI saves to build robust communication and rapport with her customers, which she says builds loyalty to her business. Less time spent on photos and video gives her more time to respond to emails and direct messages from clients seeking advice about their purchases.\n\n\"It's building community that's missing in the big brands,\" Connor says.\n\nWhile AI has allowed these solopreneurs to grow their businesses without hiring a team, the technology shouldn't take over the core aspects of a business, Hebert says. Rather, it can be a tool that allows owners to focus on those critical areas.\n\n\"Use it as a resource,\" she says.\n\nYork — whose target clientele are other solopreneurs — says she's seeing more people recognize that. \"People aren't scared of it anymore,\" she says.\n\nConnor plans to expand her use of AI this year. She's experimenting with a digital clone — a video avatar that can deliver a script explaining new products. That approach will save her time on filming videos, but she says she'll always be the one dishing out the original advice that her clients have come to trust.\n\nEven if a video is created using AI, Connor says, \"all thoughts, ideas, and suggestions — those are my own.\"",
    "readingTime": 4,
    "keywords": [
      "bag icon",
      "administrative tasks",
      "the bag icon",
      "allowed",
      "business",
      "personal",
      "customers",
      "advice",
      "it's",
      "she's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solopreneurs-embrace-ai-pros-cons-helps-boost-growth-client-relations-2026-1",
    "thumbnail_url": "https://i.insider.com/6978da6ad3c7faef0eccfbd1?width=1200&format=jpeg",
    "created_at": "2026-01-27T18:24:22.912Z",
    "topic": "finance"
  },
  {
    "slug": "ranking-the-eagles-remaining-oc-candidates-using-artificial-intelligence",
    "title": "Ranking the Eagles' remaining OC candidates using artificial intelligence",
    "description": "What happens when we plug the names of the best remaining Eagles OC candidates into ChatGPT's engine?",
    "fullText": "Once we turn the calendar from January's final Monday to Tuesday, we'll enter a third week dedicated to the Philadelphia Eagles' offensive coordinator search. Names have changed, but the climate and landscape remain familiar.\n\nNames have been added to the mix. Candidates have withdrawn from consideration. The general public remains confused. Not even one full year after hoisting the Lombardi Trophy, none of the top options are interested in becoming the Eagles' top offensive assistant.\n\nPerhaps Philadelphia is exercising patience? Maybe Brian Daboll mentioned the Tennessee Titans and Buffalo Bills to give the impression that he had more options? Perhaps he was trying to rush Philadelphia into a decision?\n\nMaybe he was truly in the running for jobs with those two franchises. Who knows? Those are questions we're throwing out for fodder. No one has all of the information. We're all trying to piece this together. What we do know is that no one likes all of the remaining candidates we're told are still in the running.\n\nWho is the best fit for Jalen Hurts? How do we describe the best fit for the roster overall? Who carries the most red flags? Is there any long-term stability for any of these guys?\n\nAs we ventured through Championship Sunday, we learned the Eagles had interest in Arthur Smith before he accepted the Ohio State Buckeyes job. Once we began another workweek, it was learned that Charlie Weis Jr. had removed his name from consideration. Hours later, Declan Doyle arrived at the same decision.\n\nJust for kicks, we took a breather. We plugged a few names into ChatGPT and asked who the best candidates were for the Eagles' OC job. What we learned was AI's list looks a lot like some of our own. Here's what they came up with. Keep in mind that we aren't sure whether Brian Daboll is still in the running.\n\nAI gives Brian Daboll a five-star rating as a potential OC hire, citing he has the best chance to elevate Jalen Hurts immediately. He lands atop the list because he has already done the job before and has previously had success working with Josh Allen.\n\nMike Kafka lands second on the list. His approach seems to emphasize rhythm and being 'on time'. Those are areas where we have seen Jalen Hurts struggle. That could lead to questions, but Kafka has coached mobile quarterbacks before and understands how to blend run concepts into the passing game.\n\nFrank Smith is one of the new additions to this list. He worked with Mike McDaniel as his offensive coordinator. He's intriguing and shouldn't be viewed as someone the Eagles are pursuing, since he was closest to one of the guys they actually wanted.\n\nNagy is a descendant of the Andy Reid coaching tree. He never quite recovered from the 'Double-Doink Game'. Word has it that he even had kickers audition for a job the following season by kicking from the same spot that Cody Parkey missed the go-ahead field goal attempt in the Wild Card Game.\n\nNagy is better than the reputation suggests. He could do a good job in returning to the place where his coaching career began.\n\nHere's one of the guys we know the least about, yet AI ranks him fifth-best. Settling on him means the Eagles would have placed more emphasis on potential than on proof and his resume.\n\nJerrod Johnson is another of the new additions to the Eagles' OC conversation. He is a good teacher, but this may be a mismatch in terms of need. Known as a QB developer, he would be asked to grow into his new role a la Kevin Patullo. If you remember that ultimately led to Patullo's undoing. Johnson might be a 'wrong place, wrong time' candidate, but again, these are only opinions we're sharing.\n\nSome would rank Jim Bob Cooter higher. He actually has OC experience. There's an obvious low ceiling here, as there isn't much evidence that he elevates quarterbacks or builds innovative systems that let them do what they do best.\n\nThe Eagles need to reinvent their offense, and they need to reinvent Jalen Hurts to some degree. They need someone who understands how to do both. None of these guys is a slam-dunk hire in that regard.\n\nAll have positives. All have flaws. One of the most important decisions of the offseason keeps being weighed. One false move and Philadelphia will throw away another season.\n\nThis article originally appeared on Eagles Wire: Ranking Eagles' remaining OC candidates using artificial intelligence",
    "readingTime": 4,
    "keywords": [
      "offensive coordinator",
      "jalen hurts",
      "eagles oc",
      "brian daboll",
      "candidates",
      "we're",
      "guys",
      "list",
      "learned",
      "another"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/ranking-eagles-remaining-oc-candidates-181501769.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/2DT2DutgTRojB05W6HgqZg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/philadelphia_eagles_wire_usa_today_sports_articles_352/77a3e8d8d6a3a3fee555a566b676a46b",
    "created_at": "2026-01-26T18:22:20.909Z",
    "topic": "sports"
  },
  {
    "slug": "agents-are-about-to-change-software",
    "title": "Agents Are About to Change Software",
    "description": "Man, it is a really weird time in the software world right now. Like a lot of people, when I picked up ChatGPT and Midjourney back in late 2022. It felt like wizardry. For a year I experimented and…",
    "fullText": "Man, it is a really weird time in the software world right now. Like a lot of people, when I picked up ChatGPT and Midjourney back in late 2022. It felt like wizardry.\n\nFor a year I experimented and experimented. I wrote about how Midjourney responds to emojis and even tried to see if I could write a children’s book with AI. The output here looks pretty rough today, but it was a revelation at the time. And the AI tools just kind of gradually got better and better.\n\nOne thing that never really clicked for me though was agentic coding. The vision is that you tell an LLM what you want, and it just goes off and executes it. It never really worked that well. It felt like the agents just kind of plowed through code and broke a bunch of stuff on the way to making the fix I wanted. Coding agents were more a curiosity, not something I could actually use.\n\nHowever, this is all about to change. And it is going to change everything about how software is built.\n\nI was drawn back by a post called Welcome to Gas Town by Steve Yegge.\n\nGas Town is part visionary, part performance art. It’s a Mad Max-themed fever dream that enables agents, managing agents, managing agents. There’s a mayor, rigs, polecats, a deacon and a refinery—and they all work together in this vast factory where vibes go in and code comes out.\n\nThere’s a lot of really clever ideas that somewhat mesh together. You’ve probably heard about the context window LLMs have. Essentially it’s their short-term memory. Once an agent uses about 20% of its context window, its intelligence drops off a cliff and it starts doing insane things like dropping databases.\n\nGas Town employs a trick to manage that issue. It assigns tasks to ephemeral “Polecat” agents. Polecats do a task and then disappear—basically removing the challenge of managing context windows.\n\nMaggie Appleton articulated the value of Gas Town well:\n\nWe should take Yegge’s creation seriously not because it’s a serious, working tool for today’s developers (it isn’t). But because it’s a good piece of speculative design fiction that asks provocative questions and reveals the shape of constraints we’ll face as agentic coding systems mature and grow.\n\nAnd so, intrigued by Gas Town, I decided to try vibe coding again.\n\nI’m not the type to just dip my toes in. If I’m going to do something, I do a cannonball.\n\nI’m always looking for the meta. What is the best strategy and who knows how to execute it? I read that Anthropic’s CTO keeps five agents running constantly and barely looks at the actual files. Either that’s marketing or there must be something there, or maybe both?\n\nI read a bunch of articles and watched a ton of YouTube videos. YouTube was pretty wild. There are these videos with guys streaming 5-10 Claude Code terminals, blaring EDM (lol) and managing all these agents.\n\nI’m kind of poking fun, but I actually learned a lot about setup from that BridgeMind channel. If you’re interested I might start with his videos about Warp and the OpenCode CLI.\n\nI wanted to see how well these agents actually work, but I needed an easy entry point. A Chrome extension to restyle Hacker News seemed perfect. It’s been in my backlog for a while, and because it’s purely frontend, I knew I’d be comfortable judging the output.\n\nAnd honestly it was. Initially I tried to one-shot the thing and—as I expected—that was a failure. But then I decided to slow down. I told the agent to scaffold a chrome extension to restyle Hacker News pages. It worked. And I kind of just broke up these tasks into smaller pieces. Tested them as I went.\n\nSure there were bugs. But I kind of just did what I do when I’m reviewing any engineers’ code. Inspect the DOM, look at the styles, look at the console and then give feedback. When the context window hit’s 20% I typically close that window and open a new chat. The OpenCode CLI even allows you to drop screenshots. It’s pretty wild.\n\nNow, I don’t have a dozen agents running at the same time. I never felt the need to have more than two working. And I’m honestly not sure how UI work even gets shipped in Gas Town? My guess is Yegge is probably a lot less concerned with UX than I am.\n\nWith AI agents, the last mile—that final polish and detailing—will be critical. We already see this in Salesloft, where sellers review generative emails before sending. In design, it manifests as small UI tweaks. It will be something else for doctors and something else for mechanical engineers. But I think there is a real opportunity in refining how humans interact with the agent’s output, creating better loops for feedback and adjustment.\n\nYou know what? There’s something here though. I don’t really like the term “vibe coding.” And I know the concept is polarizing. But after tinkering with this stuff for a couple days—I think agents are about to change how we build software.\n\nThe conception of what it looks like to make software is going to change pretty quickly.\n\nThe three major functions on a delivery team (or feature team) are engineering, design and product management. I’ve long thought we’re going to start to see more overlap in those functions. I’m even more sure of it now.\n\nI think we’re going to start to see a hybrid role emerge—product engineer.\n\nWhat does this mean delivery teams will look like in the future? I imagine they are either significantly smaller or significantly more productive. I’m not sure if QA is embedded into these teams the same way they are currently or if there is a separate team of—well, people managing QA agents. I have a lot of questions.\n\nThis also makes me think a lot about Ben Thompson’s theory on bundling and unbundling. From 2010 to 2015, companies quickly moved on the back of frameworks. First like Ruby on Rails and Bootstrap. Then on other technologies like Angular and React. The speed these frameworks provided caused an unbundling in the software world.\n\nPoint solutions were able to move fast and gather steam while slow incumbents either weren’t nimble enough or weren’t in a place to capitalize on the productivity provided by frameworks. Starting in 2016, that changed. Customers were overwhelmed with choices. Larger companies caught on and smaller ones consolidated into larger platforms.\n\nI think that’s about to shift again. And probably this year.\n\nDavid Cummings called it out in his newsletter this weekend. SaaS companies are about to see a massive wave of new competition. And it is going to happen extremely fast. The bar to build software has been lowered. A two-person team will soon be able to build what used to take a whole department.\n\nThis puts incumbent software companies in a pretty dangerous situation. Those that are not able to be nimble and go fast are going to be in real trouble. I think this is especially true in the consumer, SMB and mid-market segments. Enterprise software may have some buffer as customers of enterprise software are buying a process more than the software itself.\n\nHonestly, it makes me a little nervous. The industry is going to change and everyone’s jobs are going to look a little different—product design included.\n\nBut as someone who got into software just because I wanted to make things, this is a dream come true. I’m seeing a glimpse of the vision I hoped for in 2022. It’s not just a toy anymore. It’s an unbelievable tool for builders.\n\nIf you’ve been ignoring AI tools because you think they are overhyped, or maybe don’t see how they fit into your workflow. I’d encourage you to give them another look.",
    "readingTime": 7,
    "keywords": [
      "restyle hacker",
      "chrome extension",
      "agentic coding",
      "vibe coding",
      "pretty wild",
      "context window",
      "enterprise software",
      "gas town",
      "agents managing",
      "hacker news"
    ],
    "qualityScore": 1,
    "link": "https://solomon.io/agents-are-about-to-change-software/",
    "thumbnail_url": "https://solomon.io/wp-content/uploads/2026/01/WelcomeToGasTown-2200x1196.jpg",
    "created_at": "2026-01-26T18:21:39.265Z",
    "topic": "tech"
  },
  {
    "slug": "am-i-the-only-one-who-switches-between-chatgpt-gemini-and-claude",
    "title": "Am I the only one who switches between ChatGPT, Gemini, and Claude?",
    "description": "Am I the only one who switches between #Grok, #ChatGPT, #Gemini, and #Claude? Meet Context Wallet.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/oswarld_oz/status/2015432998406226289",
    "thumbnail_url": "https://pbs.twimg.com/amplify_video_thumb/2015426357887725568/img/5auISEy4m8VBFIVN.jpg:large",
    "created_at": "2026-01-26T06:23:53.262Z",
    "topic": "tech"
  },
  {
    "slug": "the-ladder-to-nowhere-how-openai-plans-to-learn-everything-about-you",
    "title": "The Ladder to Nowhere: How OpenAI Plans to Learn Everything About You",
    "description": "ChatGPT Health is a small part of a much larger plan to learn everything about you. In this post, I talk about what's driving them and how they might get there.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://insights.priva.cat/p/the-ladder-to-nowhere-how-openai",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!RPej!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77dfb7d3-a672-4f00-bf6b-1607229c44f7_2752x1536.png",
    "created_at": "2026-01-25T12:22:39.078Z",
    "topic": "tech"
  },
  {
    "slug": "agenthub-a-unified-sdk-for-llm-apis-with-faithful-validation",
    "title": "AgentHub – A unified SDK for LLM APIs with faithful validation",
    "description": "AgentHub is the only SDK you need to connect to state-of-the-art LLMs (GPT-5.2/Claude 4.5/Gemini 3). - Prism-Shadow/AgentHub",
    "fullText": "Prism-Shadow\n\n /\n\n AgentHub\n\n Public\n\n AgentHub is the only SDK you need to connect to state-of-the-art LLMs (GPT-5.2/Claude 4.5/Gemini 3).\n\n License\n\n Apache-2.0 license\n\n 30\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Prism-Shadow/AgentHub",
    "readingTime": 1,
    "keywords": [
      "license",
      "agenthub"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Prism-Shadow/AgentHub",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1135038596/73e1b398-f342-44f0-944e-cafe84869f56",
    "created_at": "2026-01-25T12:22:38.486Z",
    "topic": "tech"
  },
  {
    "slug": "latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
    "title": "Latest ChatGPT model uses Elon Musk’s Grokipedia as source, tests reveal",
    "description": "Guardian found OpenAI’s platform cited Grokipedia on topics including Iran and Holocaust deniers\nThe latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.\nIn tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.\n Continue reading...",
    "fullText": "Guardian found OpenAI’s platform cited Grokipedia on topics including Iran and Holocaust deniers\n\nThe latest model of ChatGPT has begun to cite Elon Musk’s Grokipedia as a source on a wide range of queries, including on Iranian conglomerates and Holocaust deniers, raising concerns about misinformation on the platform.\n\nIn tests done by the Guardian, GPT-5.2 cited Grokipedia nine times in response to more than a dozen different questions. These included queries on political structures in Iran, such as salaries of the Basij paramilitary force and the ownership of the Mostazafan Foundation, and questions on the biography of Sir Richard Evans, a British historian and expert witness against Holocaust denier David Irving in his libel trial.\n\nGrokipedia, launched in October, is an AI-generated online encyclopedia that aims to compete with Wikipedia, and which has been criticised for propagating rightwing narratives on topics including gay marriage and the 6 January insurrection in the US. Unlike Wikipedia, it does not allow direct human editing, instead an AI model writes content and responds to requests for changes.\n\nChatGPT did not cite Grokipedia when prompted directly to repeat misinformation about the insurrection, about media bias against Donald Trump, or about the HIV/Aids epidemic – areas where Grokipedia has been widely reported to promote falsehoods. Instead, Grokipedia’s information filtered into the model’s responses when it was prompted about more obscure topics.\n\nFor instance, ChatGPT, citing Grokipedia, repeated stronger claims about the Iranian government’s links to MTN-Irancell than are found on Wikipedia – such as asserting that the company has links to the office of Iran’s supreme leader.\n\nChatGPT also cited Grokipedia when repeating information that the Guardian has debunked, namely details about Sir Richard Evans’ work as an expert witness in David Irving’s trial.\n\nGPT-5.2 is not the only large language model (LLM) that appears to be citing Grokipedia; anecdotally, Anthropic’s Claude has also referenced Musk’s encyclopedia on topics from petroleum production to Scottish ales.\n\nAn OpenAI spokesperson said the model’s web search “aims to draw from a broad range of publicly available sources and viewpoints”.\n\n“We apply safety filters to reduce the risk of surfacing links associated with high-severity harms, and ChatGPT clearly shows which sources informed a response through citations,” they said, adding that they had ongoing programs to filter out low-credibility information and influence campaigns.\n\nAnthropic did not respond to a request for comment.\n\nBut the fact that Grokipedia’s information is filtering – at times very subtly – into LLM responses is a concern for disinformation researchers. Last spring, security experts raised concerns that malign actors, including Russian propaganda networks, were churning out massive volumes of disinformation in an effort to seed AI models with lies, a process called “LLM grooming”.\n\nIn June, concerns were raised in the US Congress that Google’s Gemini repeated the Chinese government’s position on human rights abuses in Xinjiang and China’s Covid-19 policies.\n\nNina Jankowicz, a disinformation researcher who has worked on LLM grooming, said ChatGPT’s citing Grokipedia raised similar concerns. While Musk may not have intended to influence LLMs, Grokipedia entries she and colleagues had reviewed were “relying on sources that are untrustworthy at best, poorly sourced and deliberate disinformation at worst”, she said.\n\nAnd the fact that LLMs cite sources such as Grokipedia or the Pravda network may, in turn, improve these sources’ credibility in the eyes of readers. “They might say, ‘oh, ChatGPT is citing it, these models are citing it, it must be a decent source, surely they’ve vetted it’ – and they might go there and look for news about Ukraine,” said Jankowicz.\n\nBad information, once it has filtered into an AI chatbot, can be challenging to remove. Jankowicz recently found that a large news outlet had included a made-up quote from her in a story about disinformation. She wrote to the news outlet asking for the quote to be removed, and posted about the incident on social media.\n\nThe news outlet removed the quote. However, AI models for some time continued to cite it as hers. “Most people won’t do the work necessary to figure out where the truth actually lies,” she said.\n\nWhen asked for comment, a spokesperson for xAI, the owner of Grokipedia, said: “Legacy media lies.”",
    "readingTime": 4,
    "keywords": [
      "sir richard",
      "richard evans",
      "holocaust deniers",
      "llm grooming",
      "cited grokipedia",
      "expert witness",
      "citing grokipedia",
      "chatgpt",
      "disinformation",
      "topics"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/24/latest-chatgpt-model-uses-elon-musks-grokipedia-as-source-tests-reveal",
    "thumbnail_url": "https://i.guim.co.uk/img/media/202d8061a28d8c1b855097fb90558014cb00d220/135_0_4675_3740/master/4675.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=fc74f3ad623847f91b340f08501077e0",
    "created_at": "2026-01-24T18:17:06.605Z",
    "topic": "tech"
  },
  {
    "slug": "opensource-ad-infra-for-llms-reverseengineered-from-chatgpt",
    "title": "Open-source ad infra for LLMs (reverse-engineered from ChatGPT)",
    "description": "Open-source ad serving platform for LLM applications - inspired by ChatGPT Bazaar system - system32miro/ai-ads-engine",
    "fullText": "system32miro\n\n /\n\n ai-ads-engine\n\n Public\n\n Open-source ad serving platform for LLM applications - inspired by ChatGPT Bazaar system\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n system32miro/ai-ads-engine",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/system32miro/ai-ads-engine",
    "thumbnail_url": "https://opengraph.githubassets.com/5a21764c0d8dccbdd55ea03a277ef4c0a8874e2b8572009a827851dcf1a5a9b3/system32miro/ai-ads-engine",
    "created_at": "2026-01-24T00:56:46.232Z",
    "topic": "tech"
  },
  {
    "slug": "openais-recently-departed-vp-of-research-calls-googles-comeback-openais-fumble",
    "title": "OpenAI's recently departed VP of research calls Google's comeback 'OpenAI's fumble'",
    "description": "Jerry Tworek, OpenAI's former VP of research, said the ChatGPT maker should have never lost its early lead to Google.",
    "fullText": "Sometimes a comeback story starts with a fumble.\n\nA former top OpenAI researcher said Google's AI renaissance is as much about OpenAI's missteps as it is about what the search giant got right.\n\n\"Personally, what I think you should consider Google's comeback, I think it's OpenAI's fumble,\" Jerry Tworek, a former VP of research at OpenAI, said on a Wednesday episode of Ashlee Vance's \"Core Memory\" podcast.\n\nTworek, who spent almost seven years at OpenAI, said earlier this month that he left the startup \"to try to explore types of research that are hard to do at OpenAI.\"\n\nOpenAI CEO Sam Altman declared a \"Code Red\" in December amid increasing competition from Google. The tech giant received wide praise across the industry for the capabilities of its Gemini 3 AI model, which some observers said had surpassed ChatGPT.\n\nWhile declining to detail what he described as OpenAI's missteps, Tworek said that the pioneering AI company should never have lost the lead it established with the release of ChatGPT in 2022.\n\n\"If you are a company that is ahead and has all the advantages that OpenAI has you should always stay ahead,\" he said.\n\nOverall, Tworek said, \"Google did a lot of things right.\"\n\n\"Very clearly, Google started treating seriously at that moment, training large language models and, like, through OpenAI fumbling its lead, they are very, very close now in capability and in terms of models trained,\" he said, adding that the whole industry began to up its investment in AI when OpenAI showed ChatGPT could generate revenue.\n\nAs for OpenAI, Tworek said that the sheer toll of the AI race has led the non-profit-research lab-turned-public-benefit-corporation to place less of an emphasis on risky research that may not yield results. A spokesperson for OpenAI did not respond to Business Insider's request for comment.\n\n\"There are multiple aspects of certain things that are just hard to do in a company that has to compete in an extremely, extremely brutal and demanding race for having the best AI model in the world right now,\" he said. \"One dynamic is there is naturally how much willingness of risks companies are willing to take from the perspective of trying to not fall behind.\"\n\nTworek said \"all major AI companies\" are facing pressure to show user growth and pay for GPUs while simultaneously competing to be the best available model.\n\n\"That does affect somehow your appetite for risk that you are willing to take,\" he said.\n\nDo you work at OpenAI or Google? Contact the reporter from a non-work email and device at bgriffiths@businessinsider.com",
    "readingTime": 3,
    "keywords": [
      "openai's missteps",
      "openai",
      "research",
      "model",
      "comeback",
      "fumble",
      "giant",
      "industry",
      "lead",
      "ahead"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-google-ai-race-fumble-gemini-2026-1",
    "thumbnail_url": "https://i.insider.com/69725e9ea645d1188187cf20?width=1200&format=jpeg",
    "created_at": "2026-01-23T12:26:13.658Z",
    "topic": "finance"
  },
  {
    "slug": "openai-is-making-more-than-1-billion-a-month-from-something-that-has-nothing-to-do-with-chatgpt",
    "title": "OpenAI is making more than $1 billion a month from something that has nothing to do with ChatGPT",
    "description": "Sam Altman says OpenAI added more than $1 billion in annual recurring revenue in a month, driven by its API business rather than ChatGPT.",
    "fullText": "OpenAI has pulled in a billion-dollar month from something other than ChatGPT.\n\nSam Altman said in a post on X on Thursday that OpenAI added more than $1 billion in annual recurring revenue in the past month \"just from our API business.\"\n\n\"People think of us mostly as ChatGPT, but the API team is doing amazing work!\" the OpenAI CEO wrote.\n\nOpenAI's API enables other companies and developers to embed its models into their own products, from internal productivity software to coding tools.\n\nMany of Silicon Valley's high-profile startups rely on OpenAI's models as core infrastructure. Perplexity uses OpenAI's models to power parts of its AI search and answer engine. Harvey, one of the fastest-growing legal tech startups, is built on OpenAI's models to assist lawyers with research and drafting.\n\nAltman's comments underscore how OpenAI's infrastructure business is emerging as a key growth engine, even as the company faces massive costs for computing power and data centers.\n\nThose pressures have pushed OpenAI to look beyond consumer subscriptions.\n\nLast week, the company said it is gearing up to test ads inside ChatGPT as it faces about $1.4 trillion in spending commitments over the coming years.\n\nIt's a notable shift for a company that once treated ads as taboo. Less than two years ago, Altman said advertising was a \"last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nSince then, Altman has struck a more open tone. In June, he said on OpenAI's podcast that he wasn't \"totally against\" ads, though he stressed it would need to be approached carefully.\n\nEarlier this week, OpenAI's chief financial officer, Sarah Friar, raised the idea of \"licensing models\" that would let the company share in downstream sales if a customer's product succeeds.\n\n\"Let's say in drug discovery, if we licensed our technology, you have a breakthrough. The drug takes off, and we get a licensed portion of all its sales,\" Friar said in an episode of \"The OpenAI Podcast\" published Monday.",
    "readingTime": 2,
    "keywords": [
      "openai's models",
      "business",
      "startups",
      "infrastructure",
      "engine",
      "faces",
      "resort",
      "sales",
      "drug",
      "licensed"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-1-billion-a-month-api-business-chatgpt-sam-altman-2026-1",
    "thumbnail_url": "https://i.insider.com/6972e427e1ba468a96aa91d7?width=800&format=jpeg",
    "created_at": "2026-01-23T06:20:31.103Z",
    "topic": "finance"
  },
  {
    "slug": "google-just-promised-no-ads-in-gemini-for-now",
    "title": "Google Just Promised No Ads in Gemini (for Now)",
    "description": "The statement comes about a week after OpenAI announced ads are coming to ChatGPT.",
    "fullText": "A week after OpenAI admitted it will soon start testing ads in ChatGPT, Google has promised that it's not planning to inject ads into Gemini anytime soon.\n\nThe statement was given to journalist Alex Heath during the World Economic Forum in Davos, Switzerland. Google DeepMind CEO Demis Hassabis said the company doesn't have \"any plans\" for ads in Gemini. While the statement was fairly brief, it also jibes with a similar quote Hassabis gave to Axios, where he said he was \"a little bit surprised\" that OpenAI was already introducing ads to ChatGPT.\n\nThat surprise is understandable, especially because OpenAI CEO Sam Altman said in 2024 that he considered ads a \"last resort for us as a business model.\" But looking at the numbers, it makes sense that ChatGPT is getting ads long before Gemini is even thinking of them.\n\nWhile Google makes most of its money through showing people ads, it's also able to rely on Search and YouTube to push ads to most of those eyeballs. Meanwhile, OpenAI is pretty much just ChatGPT. As the latter moves to a for-profit model, it now has to put moneymaking first, something it's had trouble doing without relying on traditional internet moneymakers like ads. Google, meanwhile, is already profitable elsewhere, and is able to take its time and use its sheer size to keep Gemini ad-free, at least while it continues to chase market share.\n\nDoes this mean Google's AI will never get ads? Well, never say never. But it does mean that they're probably not on the horizon—even if Google plans to more aggressively monetize Gemini over the long term, it isn't facing the same kind of time crunch as Altman's company.\n\nIt remains to be seen whether the presence of ads will push users away from ChatGPT, but the move comes in the wake of significant wins for Gemini and one major loss for ChatGPT. First, Google's Nano Banana image editing model went viral on social media, winning over the general public. Then, Google struck a deal with Apple to put its AI into the iPhone, and it looks like Gemini will be powering Siri for the foreseeable future.\n\nMeanwhile, ChatGPT reportedly saw a 6% dip in users early last month, following a model update from Gemini—and that was before the introdution of ads. While ChatGPT still seems to be in the lead on total user count, there's evidence that Google is catching up.\n\nThe divide in strategy seems clear: As OpenAI seeks ways to get more money out of its existing user base, Google can focus on growing its own with new integrations into the products we already use every day. I can't say what the limits of this growth are, but I can say that I rarely go out of my way use AI, yet I've still found myself accidentally relying on Google's AI overviews every now and then. If Google can get more people like me to casually integrate AI into our regular workflows, it's possible we could soon have a new AI leader on our hands.",
    "readingTime": 3,
    "keywords": [
      "google's ai",
      "it's",
      "model",
      "soon",
      "google",
      "chatgpt",
      "gemini",
      "statement",
      "hassabis",
      "plans"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-just-promised-no-ads-in-gemini-for-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFGZ1E3E2BQZ7DBB464WC469/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.618Z",
    "topic": "tech"
  },
  {
    "slug": "apple-might-turn-siri-into-an-ai-chatbot-to-rival-chatgpt",
    "title": "Apple Might Turn Siri Into an AI Chatbot to Rival ChatGPT",
    "description": "It could be ChatGPT's next big competitor.",
    "fullText": "Last week, Apple finally admitted it will need to team up with Google to finally make good on that contextual Siri promise it made two years ago, which would have allowed the virtual assistant to integrate with content like your texts or emails to answer personal questions and take actions for you. Now, according to a new report, the iPhone company might actually go one step further and turn Siri into a full-fledged AI chatbot—one on par with the likes of ChatGPT, and perhaps even more sophisticated.\n\nCurrently, Siri has AI implementation, but only technically, and it's certainly underwhelming: You can use it to get tech support on Apple products or shunt questions off to ChatGPT, but otherwise, Siri basically works as it always has. But according to Bloomberg's Mark Gurman, who has reliably reported on insider information at Apple before, the company is finally not only looking to make Siri smarter, but also change the way you interact with it. Currently planned for iOS and macOS 27 under the name \"Campos,\" Siri's new chatbot interface will still be powered by Gemini, but will allow you to both type and talk to Siri, with full continuity between your conversations. This upgrade will be in addition to the overdue features that were already announced.\n\nIn other words, it'll look something like the chatbot interface from the ChatGPT app or the standalone Gemini app. Yes, you can technically type to Siri right now, but it mostly works like a separate input method, rather than as a full conversation. You can't scroll through your previous questions to Siri or peruse the assistant's previous answers, and if you ask Siri to reference a message you sent it two weeks ago, it'll have no idea what you mean. That's far behind what other AI chatbots offer right now.\n\nThe update will also apparently further expand Siri's capabilities even beyond the contextual or personalization upgrades that were already revealed. Gurman says that, while the contextual upgrades will be able to pull information from other apps like Messages, the chatbot-style Siri will be \"integrated into all of the company's core apps, including ones for mail, music, podcasts, TV, Xcode programming software and photos.\" Essentially, Siri will have more access to your iPhone than other AI chatbots, and those integrations will go beyond what was previously promised. That could make it more or less appealing to you, depending on your tastes in AI integration.\n\nWith the chatbot interface planned for iOS 27, it's likely to come after the contextual upgrades, rather than at the same time. That's because, as Gurman said previously, those upgrades are set for the spring. He predicts we'll learn more about it during this year's WWDC, which, if it follows the standard set by previous years, will take place in June.\n\nThe move to turn Siri into a chatbot could come across as a a much-overdue modernization, as Google has already done the same with Gemini over on Android, but it's also a bit of a surprise, as Apple had previously said it did not intend to turn Siri into a \"bolt-on chatbot on the side\" for Apple Intelligence.\n\nBut Apple was likely talking about quality of the experience rather than expressing any significant anti-chatbot bias among the development team, meaning the fact that Siri is turning into a chatbot could mean the company is finally happy with the direction it's headed. But it's also possible that the professed skepticism about turning Siri into a chatbot was meant to appeal to AI skeptics in general. Unfortunately, if you're still skeptical about AI, it currently seems like iOS 27 will be a boring update for you, as Gurman indicated the new Siri chatbot will be the \"primary new addition\" to the operating system.\n\nHowever you feel about it personally, Siri as a full-fledged AI chatbot could seriously upset ChatGPT's market dominance—ironic, given its early integration with Apple Intelligence. Currently, OpenAI has reportedly admitted it's in a Code Red situation, as it is losing market share to Google and introducing ads to bolster its bottom line. The new Siri, being powered by Gemini, is unlikely to hurt Google (although it will have more access to your phone than the standalone Gemini app), but its ease-of-access might make it the new go-to for iPhone users, and that could hurt pretty much every AI company Apple isn't in business with directly.",
    "readingTime": 4,
    "keywords": [
      "standalone gemini",
      "gemini app",
      "apple intelligence",
      "contextual upgrades",
      "chatbot interface",
      "it's",
      "siri",
      "finally",
      "google",
      "iphone"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-siri-chatbot-ai-plans?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFH7N4CFDDEPBSQ5VKSC72K1/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.513Z",
    "topic": "tech"
  },
  {
    "slug": "solopreneurs-are-embracing-ai-heres-how-3-career-coaches-have-found-it-indispensable",
    "title": "Solopreneurs are embracing AI — here's how 3 career coaches have found it indispensable",
    "description": "AI tools like ChatGPT, Gemini, and Claude help solopreneur coaches Kim Surko, Katharine Campbell Hirst, and Liz Morrison boost client outcomes and streamline coaching workflows.",
    "fullText": "When it comes to career and executive coaching, some of the most important work happens in one-on-one sessions with the client. That's when breakthrough insights often emerge, motivation gains momentum, and coaches build an essential connection.\n\nThen comes the hard part: turning those words into actions.\n\nTranscribing conversations into meaningful action items for clients takes time and effort — something a team of assistants could easily handle. However, when you're the only person running your business, that time and effort leaves coaching solopreneurs with a difficult tradeoff: take notes during sessions and sacrifice presence, spend hours after each call transcribing insights from recordings, or leave follow-through entirely to clients.\n\n\"Trying to juggle it all on my own wasn't an option — it was just impossible to build a sustainable business,\" said Kim Surko, founder of leadership coaching business Surko Coaching, regarding how easy it was to feel overwhelmed by work that felt more administrative than transformational. \"Leaning into AI was the most natural solution to help with all of that responsibility.\"\n\nHere's how three solopreneurs in leadership, business, and communications have used AI to help clients achieve more results in less time, expanding their capacity as coaches while increasing the value of the work they offer.\n\nFor all three coaches, the biggest game changer has been using AI note-takers to distill long conversations into something more tangible.\n\nAfter getting client consent, \"I record my coaching sessions and upload transcripts into ChatGPT. This allows me to rapidly transform nuanced insights from our conversations into concrete outputs clients can actually use — pitches, résumés, website copy, positioning statements, and more,\" said Katharine Campbell Hirst, founder of business coaching company KCH Coaching & Advisory. \"What used to take weeks of agonizing refinement now takes minutes.\"\n\nLiz Morrison, founder of communications coaching company LM Strategic Storytelling, appreciates how AI ensures that valuable sound bites from her coaching sessions don't get lost in hours of recordings that nobody has time to revisit. She's built custom projects in Claude to help her transform session transcripts into \"Story Banks\" in minutes — pulling out three to six narratives per session that clients can use immediately for interviews, networking, social media, and building their businesses.\n\nWhile this type of work was essential before AI, doing it as a solopreneur meant sacrificing time that could be spent supporting other clients. \"I've saved almost an hour per client per day by relying on AI to take notes and summarize them for me,\" said Surko, who added that she nearly doubled her capacity for coaching clients with AI's support.\n\nSurko has also used AI to help her clients appreciate the progress they're making, improving the feeling of momentum. \"A lot of work with coaching is celebrating the small wins,\" she said.\n\nUsing the project management tool Kanbanchi, supported by Gemini, Surko can quickly update to-do list boards that lay out all of the client's goals and achievements.\n\n\"Having that visual representation of the progress we're making shows the value of coaching,\" Surko said. The process has been extremely valuable, as it has improved her client renewal rate because clients can see exactly how they're getting closer to a goal, rather than feeling like they aren't making progress, she added.\n\nMorrison tells a similar story. She built a custom ChatGPT tool called Story Explorer that walks prospective clients through a story-coaching exercise to uncover one immediately usable story they can post on LinkedIn or use in a networking conversation.\n\n\"I find when I give people this builder, it's the start of a much bigger conversation,\" she said. They often uncover other narratives they want to explore further with Morrison, she said.\n\nAlongside the benefit of being more present during conversations, these coaches have found AI valuable for improving their in-session coaching in other ways.\n\nSurko, for example, used Gemini within Google Docs to create a searchable archive of the massive toolkit of exercises and prompts she's collected in her decadeslong career, which before were buried in various folders.\n\nPreviously, she would have to wait until after the session to hunt down an exercise. Now, she can quickly pull them up during sessions and dive deeper with a client. \"We make more progress in each session,\" she said of this improvement. \"We're able to continue that momentum.\"\n\nAI can even be a helpful coach for these coaching experts.\n\nHirst uploaded transcripts across her client's full arc and asked where she did well and where she could have improved. While she also works with coaches, she appreciates that AI can effectively be over her shoulder all the time.\n\n\"The feedback is surprisingly concrete, pattern-based, and immediately actionable — effectively giving me a reflective practice partner I wouldn't otherwise have access to as a solopreneur,\" Hirst said.",
    "readingTime": 4,
    "keywords": [
      "coaching sessions",
      "clients",
      "client",
      "coaches",
      "business",
      "conversations",
      "progress",
      "insights",
      "momentum",
      "founder"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/3-coaching-solopreneurs-how-ai-help-their-businesses-grow-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8b9ee1ba468a96aa3c19?width=1200&format=jpeg",
    "created_at": "2026-01-21T18:30:49.965Z",
    "topic": "finance"
  },
  {
    "slug": "genai-the-snake-eating-its-own-tail",
    "title": "GenAI, the Snake Eating Its Own Tail",
    "description": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first  superpower is a boon: they can dramatically increase ...",
    "fullText": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first \nsuperpower is a boon: they can dramatically increase human productivity. I use them on a regular basis to answer \nquestions, learn new skills, write code, create images, and much more, all at a rate of speed and quality that was\nscience fiction just a few years ago. The second superpower is a bane: GenAI is quietly destroying the very \necosystems that made it possible in the first place.\n\nUnder the hood, GenAI is built on large language models (LLMs), which are able to extract patterns, structure, and\nstatistical relationships from massive data sets. These data sets consist primarily of content created by human beings: \nbooks, blog posts, articles, forum discussions, open source code, art, photography, and so on. LLMs are able to extract \nvalue from this content at an unprecedented scale, but all that value is captured by the GenAI company and its users. \nIf you’re a content creator, you get nothing: no attribution, no referral traffic, no revenue share. Not even a \nthank-you.\n\nThis feels unsustainable to me, a bit like a snake eating its own tail. In this blog post, I’ll go through three\nexamples of how GenAI is destroying the very ecosystems it relies on, and then discuss possible solutions that may\ngive everyone (users, GenAI companies, and content creators) more value.\n\nFor many years, StackOverflow was the most popular Q&A site for programmers. Any time you \nhit a weird error while coding, you’d do a search on Google, and more often than not, find a good answer on \nStackOverflow. But now, in large part due to GenAI, StackOverflow is nearly dead:\n\nAlthough StackOverflow’s decline started before GenAI went mainstream (ChatGPT was first released in 2022), GenAI\naccelerated that decline considerably. That’s because nowadays, instead of searching around for an answer on a Q&A site,\nand working to adapt that answer to your own codebase, you can ask GenAI tools to generate your code, fix any errors\nyou hit, answer any questions you run into, and so on. As a result, you’re considerably more productive.\n\nBut it doesn’t seem sustainable. A big part of why GenAI tools can answer programming questions and fix errors in your \ncode is because those tools were trained on StackOverflow data. So you as a programmer and the GenAI tool now \nget much more value from that data, but StackOverflow gets none. If people stop asking and answering questions, what \nwill GenAI train on in the future?\n\nI also get the impression that StackOverflow is not the only online community where this is happening. For example, \nQuora seems largely dead. Wikipedia is\nfacing more threats than ever.\nAnd although Reddit’s traffic numbers don’t show it, I (and many other Redditors) get the impression that it’s also \ndying.1\n\nTailwind CSS is a popular library programmers use to style and decorate their websites. In\nfact, according to the 2025 State of CSS Survey, Tailwind is the\nmost popular CSS library, by far:\n\nAnd Tailwind usage only seems to be growing:\n\nDespite that, just a couple of weeks ago, the Tailwind team had to lay off 75% of its \nstaff. Why? The company behind\nTailwind CSS makes its money by selling a premium upgrade to the open source library called\nTailwind UI, which gives you a set of reusable, pre-built, professionally designed\ncomponents for building out your website. This was a great offering in the past, but in the age of GenAI, it’s more\nproblematic:\n\nSo developers are getting lots of value from Tailwind, GenAI is getting lots of value from Tailwind, but the creators\nof Tailwind are getting crushed. I suspect something similar will happen with many other open source projects.2\n\nWhen my latest book, Fundamentals of DevOps and Software Delivery, came \nout last year, a friend of mine asked me an interesting question:\n\nIn the age of LLMs, will people still use books to learn the fundamentals?\n\nI’m an avid reader, and still believe books play a key role in learning, but there’s no doubt that LLMs provide a new\nmethod of learning that is incredibly compelling. GenAI has two remarkable qualities that make it a great teacher:\n\nYou can ask it all the questions you want, with no fear of sounding dumb. You can repeat the same question over and \nover again if something isn’t clicking. You can request it to explain things in different ways: via text, via audio,\nvia diagrams. I’ve used GenAI to learned dozens of new things over just the last few months: how to do DIY \nprojects around the house, how to rehab a minor injury, how to cook eggs without giving them a sulfuric taste/smell,\nand much more. And I learned most of these without reading anyone’s book or blog.\n\nAnd that’s a problem. Much of the content I got from the GenAI tools was extracted from books and blog posts, with\nno attribution. Even worse, some of this content was extracted illegally. Last year, Anthropic agreed to pay $1.5B\nto settle a class-action \nlawsuit for training their\nLLMs on over 500,000 pirated books. That included several of my previous books!3\n\nThis class-action lawsuit might sound like a win for authors, but it’s actually a disaster. There are likely many\nAI companies training on pirated data who haven’t been caught. And even if they are caught, they might not\nbe sued. And even if they are sued, they might not lose or settle. And even if they do, they will just see it as the\ncost of doing business. Anthropic recently raised $13B, reported revenue at $5B per year, and is valued at \n$183B; OpenAI is trying\nto raise $100B, with reported revenue of $20B per year, and a valuation of \n$830B. And all these\nnumbers are growing fast. A $1.5B fine is just a drop in the bucket for companies like this. It’s a risk worth taking.\n\nAnd I’m guessing all the GenAI companies are taking that risk. In another lawsuit, OpenAI\nargued that it’s ‘impossible’ to create AI tools like ChatGPT without copyrighted \nmaterial. If that’s\nwhat it takes to get to an $830B valuation, you better believe they are all going to steal and pirate as much \ncontent as possible. And when they do, the creators of that content will get nothing. Nada. $0.\n\nThere are countless other examples where GenAI is benefiting from content, while giving nothing back to the content\ncreators: e.g., art, music, design, movies, copywriting, and so on. At its root, the GenAI model is broken:\n\nDid you notice what’s missing? The user has a way to get value (step 4), the GenAI company has a way to get value \n(step 3), but the content creator gets nothing. Compare this to the search engine model (e.g., Google Search), which is \nwhat we all used before GenAI came along:\n\nThe search engine model was not perfect, but it at least created the opportunity for all parties in this three-sided \nmarketplace to capture value: the user in step 3, the search engine company in step 4, and the content creator in \nstep 5.\n\nIn short, the current GenAI model destroys the incentives to create new content. I’ve heard this referred to as “the \ngreat content collapse.” Will it lead to a world where, after the 2020s, there’s little-to-no content created by humans? \nWill the state of knowledge and creativity stagnate as a result?\n\nTo be clear, I’m not an innocent party in this. As I mentioned numerous times in this post, I use GenAI regularly.\nThere’s no doubt that it makes me more productive. I even used GenAI to create the cover image for this blog post! \nBut each time I use ChatGPT or Claude, I feel a bit guilty, as it doesn’t feel sustainable. The snake can’t keep \neating its own tail indefinitely.\n\nSo the question is, what do we do? If we want to avoid the great content collapse, we need a model of GenAI usage that \ncreates opportunities for all parties (user, GenAI company, content creator) to capture value. Below are two ideas for \nhow we might accomplish this.\n\nThe only attempted solution I’ve heard about so far is CloudFlare’s pay-per-crawl \nmodel, which seems to work as follows:\n\nOn the one hand, it’s fantastic to see a major company try to do something about this problem. On the other \nhand, this approach seems to address the wrong part of the problem. The real value isn’t in crawling the data, it’s \nin using it. For every one crawl, an LLM might use the data thousands or millions of times. If creators are only paid \nper crawl, then the GenAI company still captures 99.999% of the value, and the creator gets next to nothing. Moreover,\nthis model only seems to work for websites (it’s not clear how you adapt it to books, art, music, etc.), and it \ncreates an incentive for GenAI companies to only crawl free content, which means paid content is less likely to ever be\ndiscovered (which disproportionally benefits those with pockets deep enough to keep their content free).\n\nI came across a clever solution that felt directionally correct from this LinkedIn post by Tyrone \nJoel\nwhere he took a PDF of my book Terraform: Up & Running, uploaded it into\na GenAI tool, and asked the tool to follow the guidance in the book to generate Terraform code. This feels like it has\nall the ingredients of a model of GenAI usage that is sustainable: the user gets value from the GenAI tool’s responses,\nthe GenAI company gets value from the user paying for a subscription, and the content creator gets value from the user \npaying for their content (in this case, buying my book). This works fine for a single, specific piece of content, but \nhow do you make it work at scale, across all the content that is consumed by an LLM?\n\nHere’s a rough proposal for what I’ll call the pay-per-use model:\n\nThis model works for not only websites, but other types of content too, including copyrighted content. As a content \ncreator (e.g., author, musician, designer, etc.), you could opt into sharing your copyrighted content with a GenAI\ncompany in exchange for getting referrals and revenue sharing each time your content is used. It might even help with \nmaking open source more sustainable, as open source creators could earn revenue and referrals each time a GenAI tool \nuses their code.\n\nIt’s critical that we find a more sustainable model as soon as possible. The snake can’t eat its own tail indefinitely. \nAnd the snake—GenAI—isn’t going away. We can’t put the genie back in the bottle. In fact, it’s only going to get \nbetter, more ubiquitous, and to provide more and more value to users and GenAI companies. But if we can’t find a way to \nprovide value to content creators too, then this will all fall apart.\n\nThat said, I don’t know enough about LLMs to say if a pay-per-use model is actually possible. Can LLMs track the source \nof the content they consumed? Will GenAI companies be willing to do a revenue sharing model? Will they be willing to\nbe transparent about their sources and usage? What do you think? Let me know in the comments.\n\nMany subreddits feel like a hollow shell of what they used to be. In part, this may be because a lot of the content in online communities now feels like it’s generated by bots (“AI slop”). But I think the bigger issue is that, just like StackOverflow, reading posts in online communities is no longer the best way to get answers. I used to use Reddit for research all the time; in fact, Google Search had gotten so bad, that you pretty much had to include “reddit” in your search queries to get a half-decent response. But nowadays, I use GenAI tools for much of my research. Just in the last few months alone, I’ve used ChatGPT and Claude to research solar panels, plan a trip to Norway, make changes to my diet, pick out new shoes for running, pick out new speakers for my living room, and dozens of other questions. Just a year ago, the vast majority of these questions would’ve brought me to Reddit. Nowadays, virtually none of them do, even though I suspect many of the responses I get from GenAI are based on Reddit content. ↩\n\nI’m seeing more and more projects avoiding open source dependencies entirely, and instead having GenAI generate the all code they need directly in their own codebase. There are some benefits to this approach—faster builds, more reproducible builds, less supply chain risk—but it makes sustainably funding open source even harder. You spend years to create and share an open source library with the world, and a bunch of GenAI tools copy your code, with you getting zero credit or value back. ↩\n\nHow much will I get paid as a result of this settlement? It’s hard to know exactly, as it depends on how many authors end up submitting claims, but the current estimate is $3,000 per book, though that number is split with the publisher, so in practice, it’ll be closer to $1,500 per book. If you assume that a book takes just 3 months of full-time work, or about 500 hours (which is likely an under-estimate), and all you get is $1,500, that works out to about $3/hour. Writing non-fiction tech books was never a particularly lucrative affair, but $1,500 is just downright insulting. Worse yet, the other benefits you used to get as an author—recognition as an expert, invitations to talks, job opportunities, marketing for your company or consulting—are significantly reduced too, as far fewer people read your book, or are even aware that you wrote a book, as the LLM usually doesn’t attribute any of its knowledge back to the source. ↩",
    "readingTime": 12,
    "keywords": [
      "q&a site",
      "class-action lawsuit",
      "art music",
      "tail indefinitely",
      "online communities",
      "snake can’t",
      "search engine",
      "blog posts",
      "genai tools",
      "genai tool"
    ],
    "qualityScore": 1,
    "link": "https://www.ybrikman.com/blog/2026/01/21/gen-ai-snake-eating-its-own-tail/",
    "thumbnail_url": "https://www.ybrikman.com/assets/img/blog/gen-ai-snake-eating-tail/snake-eating-tail.png",
    "created_at": "2026-01-21T18:30:43.121Z",
    "topic": "tech"
  },
  {
    "slug": "the-gloves-are-off-in-the-feud-between-sam-altman-and-elon-musk",
    "title": "The gloves are off in the feud between Sam Altman and Elon Musk",
    "description": "The tech titans escalated their long-running feud on Tuesday, trading barbs in public posts about the safety of ChatGPT, Grok, and Tesla's Autopilot.",
    "fullText": "Sam Altman and Elon Musk are at it again, with each of the tech titans taking aim at the other in a series of heated posts on X.\n\nMusk appeared to start the latest escalation early on Tuesday morning, when he posted \"Don't let your loved ones use ChatGPT\" in response to a post that said that use of OpenAI's chatbot had been linked to the deaths of nine children and adults since it was released in 2022.\n\nAltman fired back, first in defense of ChatGPT and OpenAI's desire to protect its users, and then blasting Tesla's Autopilot technology, calling it unsafe.\n\n\"It is genuinely hard; we need to protect vulnerable users, while also making sure our guardrails still allow all of our users to benefit from our tools,\" Altman said.\n\nAltman continued, calling out Autopilot.\n\n\"I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released,\" he wrote. \"I won't even start on some of the Grok decisions.\"\n\nAltman added: \"You take 'every accusation is a confession' so far.\"\n\nThere have been at least eight wrongful-death lawsuits filed against OpenAI that allege use of ChatGPT has contributed to worsening mental health conditions, leading to instances of suicide and murder, including among children and young adults.\n\nSafety concerns around Tesla's self-driving technology have also been central to multiple wrongful-death lawsuits, including one surrounding a 2019 crash in Florida that left a 22-year-old woman dead. A jury determined Tesla was 33% liable for the crash and awarded the plaintiffs $329 million in total damages, Business Insider previously reported.\n\nRepresentatives for Musk and Altman did not immediately respond to requests for comment from Business Insider.\n\nThe social media feud comes as the pair is stuck in the middle of a long-running legal battle over OpenAI's status as a nonprofit company. Musk sued Altman, and other leaders of OpenAI, alleging that they misled him when they decided to pursue a for-profit structure, moving the company away from its original nonprofit mission.\n\nMusk said he donated $38 million to OpenAI when it was originally founded as a nonprofit.",
    "readingTime": 2,
    "keywords": [
      "wrongful-death lawsuits",
      "users",
      "nonprofit",
      "altman",
      "children",
      "adults",
      "released",
      "protect",
      "technology",
      "crash"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/sam-altman-elon-musk-feud-escalates-autopilot-deaths-chatgpt-safety-2026-1",
    "thumbnail_url": "https://i.insider.com/69700640d3c7faef0ecc9b0a?width=1200&format=jpeg",
    "created_at": "2026-01-21T00:59:28.375Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-getting-on-the-ai-age-verification-bandwagon",
    "title": "ChatGPT Is Getting on the AI Age Verification Bandwagon",
    "description": "The app will guess your age and set limits for users it thinks are under 18.",
    "fullText": "When OpenAI first announced GPT-5.2 last month, it quietly disclosed a new safety feature it called \"age prediction.\" Considering ChatGPT proper isn't exactly an \"all ages\" kind of tool, it makes sense that users under the age of 18 should have protections in place to shield them from harmful content. The company says that users who indicate they're under 18 already receive an altered experience to \"reduce exposure to sensitive or potentially harmful content,\" but if the user doesn't voluntarily share how old they are with OpenAI, how does the company enforce these protections? Here's where age prediction comes in.\n\nOn Tuesday, OpenAI officially announced its new age prediction policy, which, like other age verification systems being used by the likes of Roblox, uses AI to guess how old a user is. If the system decides that a particular user is under the age of 18, OpenAI will adjust the experience accordingly, with the goal of keeping all interactions age-appropriate.\n\nHere's how it works: The new age prediction model looks at both the user's behaviors within the app, as well as the general account data. That includes things like how old the account is, what times of day the user is accessing ChatGPT, usage patterns, as well as, of course, the age the user says they are. Looking at all this data, the model determines how old the user likely is. If the model thinks they're over 18, they'll get the full experience; if the model thinks they're under 18, they'll get the \"safer experience.\" If the model isn't confident, it defaults to that safer experience.\n\nThat limited experience means that someone the model thinks is under 18 will try to reduce the following content types:\n\nViral challenges that might inspire \"risky or harmful behaviors\"\n\nRole play that is sexual, romantic, or violent in nature\n\nContent promoting \"extreme\" beauty standards, unhealthy dieting, or body shaming\n\nThe company says that its approach is informed by \"expert input\" as well as literature discussing child development science. (It's not clear whether how much of that input is from direct interviews and coordination with experts, and how much, if any, is from independent research.) The company also acknowledges \"known teen differences in risk perception, impulse control, peer influence, and emotional regulation\" when compared to adults.\n\nThe biggest risk with any of these age prediction models is that they'll sometimes get it wrong—hallucination is an unfortunate habit AI models all share. That goes both ways: You don't want someone too young accessing inappropriate content in ChatGPT, but you also don't want someone older than 18 getting stuck with a limited account for no reason. If you experience the latter situation, OpenAI has a solution for you: direct age verification through Persona. This is the same third-party Roblox uses for its age verification, which hasn't gone very well thus far.\n\nThat doesn't necessarily spell doom for OpenAI. Roblox tried overhauling their age verification system for a massive user base all used to a certain type of multiplayer experience, which led to users not being able to chat with other users in newly-assigned age categories, which were often incorrect. Meanwhile, ChatGPT's age prediction is only controlling the experience of one user at a time. To that end, OpenAI will let you upload a selfie as an added verification step if the prediction model alone isn't enough. Interestingly, OpenAI doesn't say anything about the option to upload an ID for verification, which other companies, like Google, have provided.\n\nI'm not necessarily a fan of age prediction models, as I think they often sacrifice user privacy in the name of creating age-appropriate experiences. But there's little doubt that OpenAI has to do something to limit the full ChatGPT experience for younger users. Many of ChatGPT's users are under 18, and much of the content they experience is wildly inappropriate, whether it be instructions on getting high, or advice on writing suicide notes. In some tragic cases, minors have taken their own lives after discussions with ChatGPT, leading to lawsuits against OpenAI.\n\nI don't have any great answers here. We'll just have to see how this new age prediction model affects the user experience for minors and adults alike, and whether it actually manages to create a safer experience for younger, more impressionable users.",
    "readingTime": 4,
    "keywords": [
      "harmful content",
      "safer experience",
      "prediction models",
      "age prediction",
      "age verification",
      "prediction model",
      "user",
      "users",
      "openai",
      "isn't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-chatgpt-age-prediction-model?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFEH2Q4KPPTQXMTQW5G99Y1B/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-21T00:59:27.310Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here’s how the three models performed in our tests:\n\n💡 If you want the safest pick for real “ship a feature in a big repo” work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you’re okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n⚠️ NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere’s the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe’ll compare code quality, token usage, cost, and time to complete the build.\n\n💡 NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\n💡 NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It’s not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that’s the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that’s a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it’s hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. 🤷‍♂️\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don’t think we’re anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don’t fully back them up. 🥴\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won’t hold up long term.\n\nI don't see any of these recent models as “use it and ship it” for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next",
    "title": "OpenAI launches cheaper ChatGPT subscription, says ads are coming next",
    "description": "OpenAI is bringing its $8/month ChatGPT Go plan to the U.S. and says it will begin testing ads soon in the free tier and Go.",
    "fullText": "OpenAI has announced several important changes to ChatGPT. First, the company says it is rolling out its more affordable ChatGPT Go plan in the United States for $8 per month. OpenAI also confirmed it will soon start testing ads in ChatGPT …\n\nOpenAI first launched ChatGPT Go in India last year and gradually rolled out it to 170 additional countries. Starting today, ChatGPT Go is available everywhere ChatGPT is available, including the United States.\n\nWith this, there are now three tiers of ChatGPT available:\n\nHere’s what you get with ChatGPT Go compared to the free plan:\n\nSecond, OpenAI says that it will “testing ads in the free tier and ChatGPT Go in the US soon.” ChatGPT Plus, Pro, Business, and Enterprise tiers will remain ad-free.\n\nOpenAI detailed its approach to ads in ChatGPT in a blog post published today:\n\nTo start, we plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation. Ads will be clearly labeled and separated from the organic answer. You’ll be able to learn more about why you’re seeing that ad, or dismiss any ad and tell us why. During our test, we will not show ads in accounts where the user tells us or we predict that they are under 18, and ads are not eligible to appear near sensitive or regulated topics like health, mental health or politics.\n\nFurthermore, OpenAI says that your conversations with ChatGPT are not shared with advertisers. Ads also will not influence answers that ChatGPT gives you.\n\nOpenAI says that it’s “not launching ads yet,” but rather plans to “start testing in the coming weeks for logged in adults in the U.S. on the free and Go tiers.”\n\nWhat do you think of today’s announcements from OpenAI? Let us know down in the comments.\n\nMy favorite iPhone accessories:\n\nFollow Chance: Threads, Bluesky, Instagram, and Mastodon.\n\nCheck out 9to5Mac on YouTube for more Apple news:",
    "readingTime": 2,
    "keywords": [
      "chatgpt go",
      "testing ads",
      "plan",
      "tiers",
      "free",
      "openai",
      "soon",
      "health",
      "united",
      "test"
    ],
    "qualityScore": 0.85,
    "link": "https://9to5mac.com/2026/01/16/openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next/",
    "thumbnail_url": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2026/01/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png.webp?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "created_at": "2026-01-18T12:21:37.013Z",
    "topic": "tech"
  },
  {
    "slug": "we-could-hit-a-wall-why-trillions-of-dollars-of-risk-is-no-guarantee-of-ai-reward",
    "title": "‘We could hit a wall’: why trillions of dollars of risk is no guarantee of AI reward",
    "description": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AI\nWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.\nThe figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT.",
    "fullText": "Progress of artificial general intelligence could stall, which may lead to a financial crash, says Yoshua Bengio, one of the ‘godfathers’ of modern AI\n\nWill the race to artificial general intelligence (AGI) lead us to a land of financial plenty – or will it end in a 2008-style bust? Trillions of dollars rest on the answer.\n\nThe figures are staggering: an estimated $2.9tn (£2.2tn) being spent on datacentres, the central nervous systems of AI tools; the more than $4tn stock market capitalisation of Nvidia, the company that makes the chips powering cutting-edge AI systems; and the $100m signing-on bonuses offered by Mark Zuckerberg’s Meta to top engineers at OpenAI, the company behind ChatGPT.\n\nThese sky-high numbers are all propped up by investors who expect a return on their trillions. AGI, a theoretical state of AI where systems gain human levels of intelligence across an array of tasks and are able to replace humans in white-collar jobs such as accountancy and law, is a keystone of this financial promise.\n\nIt offers the prospect of computer systems carrying out profitable work without the associated cost of human labour – a hugely lucrative scenario for companies developing the technology and the customers who deploy it.\n\nThere will be consequences if AI companies fall short: US stock markets, boosted heavily by the performance of tech stocks, could fall and cause damage to people’s personal wealth; debt markets wrapped up in the datacentre boom could suffer a jolt that ripples elsewhere; GDP growth in the US, which has benefited from the AI infrastructure, could falter, which would have knock-on effects for interlinked economies.\n\nDavid Cahn, a partner at one leading Silicon Valley investment firm, Sequoia Capital, says tech companies now have to deliver on AGI.\n\n“Nothing short of AGI will be enough to justify the investments now being proposed for the coming decade,” he wrote in a blog published in October.\n\nIt means there is a lot hanging on progress towards advanced AI, and the trillions being poured into infrastructure and R&D to achieve it. One of the “godfathers” of modern AI, Yoshua Bengio, says the progress of AGI could stall and the outcome would be bad for investors.\n\n“There is a clear possibility that we will hit a wall, that there’s some difficulty that we don’t foresee right now, and we don’t find any solution quickly,” he says. “And that could be a real [financial] crash. A lot of the people who are putting trillions right now into AI are also expecting the advances to continue fairly regularly at the current pace.”\n\nBut Bengio, a prominent voice on the safety implications of AGI, is clear that continued progress towards a highly advanced state of AI is the more likely endgame.\n\n“Advances stalling is a minority scenario, like it’s an unlikely scenario. The more likely scenario is we continue to move forward,” he says.\n\nThe pessimistic view is that investors are backing an unrealistic outcome – that AGI will not happen without further breakthroughs.\n\nDavid Bader, the director of the institute for data science at the New Jersey Institute of Technology, says trillions of dollars are being spent on scaling up – tech jargon for growing something quickly – the underlying technology for chatbots, known as transformers, in the expectation that increasing the amount of computing power behind current AI systems, by building more datacentres, will suffice.\n\n“If AGI requires a fundamentally different approach, perhaps something we haven’t yet conceived, then we’re optimising an architecture that can’t get us there no matter how large we make it. It’s like trying to reach the moon by building taller ladders,” he says.\n\nNonetheless, big US tech companies such as Google’s parent Alphabet, Amazon and Microsoft are ploughing ahead with datacentre plans with the financial cushion of being able to fund their AGI ambitions through the cash generated by their hugely profitable day-to-day businesses. This at least gives them some protection if the wall outlined by Bengio and Bader comes into view.\n\nBut there are other more worrying aspects to the boom. Analysts at Morgan Stanley, the US investment bank, estimate that $2.9tn will be spent on datacentres between now and 2028, with half of that covered by the cashflow from “hyperscalers” such as Alphabet and Microsoft.\n\nThe rest will have to be covered by alternative sources such as private credit, a corner of the shadow banking sector that is activating alarm bells at the Bank of England and elsewhere. Meta, the owner of Facebook and Instagram, has borrowed $29bn from the private credit market to finance a datacentre in Louisiana.\n\nAI-related sectors account for approximately 15% of investment grade debt in the US, which is even bigger than the banking sector, according to the investment bank JP Morgan.\n\nOracle, which has signed a $300bn datacentre deal with OpenAI, has had an increase in credit default swaps, which are a form of insurance on a company defaulting on its debts. High-yield, or “junk debt”, which represents the higher-risk end of the borrowing market, is also appearing in the AI sector via datacentre operators CoreWeave and TeraWulf. Growth is also being funded by asset-backed securities – a form of debt underpinned by assets such as loans or credit card debt, but in this case rent paid by tech companies to datacentre owners – in a form of financing that has risen sharply in recent years.\n\nIt is no wonder that JP Morgan says the AI infrastructure boom will require a contribution from all corners of the credit market.\n\nBader says: “If AGI doesn’t materialise on expected timelines, we could see contagion across multiple debt markets simultaneously – investment-grade bonds, high-yield junk debt, private credit and securitised products – all of which are being tapped to fund this buildout.”\n\nShare prices linked to AI and tech are also playing an outsized role in US stock markets. The so-called “magnificent 7” of US tech stocks – Alphabet, Amazon, Apple, Tesla, Meta, Microsoft, and Nvidia – account for more than a third of the value of the S&P 500 index, the biggest stock market index in the US, compared with 20% at the start of the decade.\n\nIn October the Bank of England warned of “the risk of a sharp correction” in US and UK markets due to giddy valuations of AI-linked tech companies. Central bankers are concerned stock markets could slump if AI fails to reach the transformative heights investors are hoping for. At the same time the International Monetary Fund said valuations were heading towards dotcom bubble-levels.\n\nEven tech execs whose companies are benefiting from the boom are acknowledging the speculative nature of the frenzy. In November Sundar Pichai, the chief executive of Alphabet, said there are “elements of irrationality” in the boom and that “no company is going to be immune” if the bubble bursts, while Amazon’s founder, Jeff Bezos, has said the AI industry is in a “kind of industrial bubble”, and OpenAI’s chief executive, Sam Altman, has said “there are many parts of AI that I think are kind of bubbly right now.”\n\nAll three, to be clear, are AI optimists and expect the technology to keep improving and benefit society.\n\nBut when the numbers get this big there are obvious risks in a bubble bursting, as Pichai admits. Pension funds and anyone invested in the stock market will be affected by a share price collapse, while the debt markets will also take a hit. There is also a web of “circular” deals, such as OpenAI paying Nvidia in cash for chips, and Nvidia will invest in OpenAI for non-controlling shares. If these transactions unravel due to a lack of take-up of AI, or that wall being hit, then it could be messy.\n\nThere are also optimists who argue that generative AI, the catch-all term for tools such as chatbots and video generators, will transform whole industries and justify the expenditure. Benedict Evans, a technology analyst, says the expenditure numbers are not outrageous in the context of other industries, such as oil and gas extraction which runs at $600bn a year.\n\n“These AI capex figures are a lot of money but it’s not an impossible amount of money,” he says.\n\nEvans adds: “You don’t have to believe in AGI to believe that generative AI is a big thing. And most of what is happening here is not, ‘oh wow they’re going to create God’. It’s ‘this is going to completely change how advertising, search, software and social networks – and everything else our business is based on – is going to work’. It’s going to be a huge opportunity.”\n\nNonetheless, there is a multitrillion dollar expectation that AGI will be achieved. For many experts, the consequences of getting there are alarming. The cost of not getting there could also be significant.",
    "readingTime": 8,
    "keywords": [
      "alphabet amazon",
      "chief executive",
      "banking sector",
      "financial crash",
      "progress towards",
      "investment bank",
      "junk debt",
      "tech stocks",
      "stock market",
      "stock markets"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/17/why-trillions-dollars-risk-no-guarantee-ai-reward",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a0eef0b4544b41b055733e7d0826315830269b70/547_0_5468_4374/master/5468.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0827b0776297b562f7de1e2b5b00e2ca",
    "created_at": "2026-01-17T18:16:16.585Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-is-getting-ads-sam-altman-once-called-them-a-last-resort",
    "title": "ChatGPT is getting ads. Sam Altman once called them a 'last resort.'",
    "description": "The move to integrate ads into ChatGPT comes as OpenAI looks to increase its revenue amid $1.4 trillion in spending commitments and a possible IPO.",
    "fullText": "Netflix famously backtracked on its stance toward ads. Now, OpenAI is following suit.\n\nThe AI pioneer announced that ads are coming to ChatGPT — less than two years after OpenAI CEO Sam Altman portrayed them as \"a last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAltman's softened stance since then underlines the massive change OpenAI has undergone in the last two years, and the company's embrace of advertising is a testament to just how expensive the AI race has become.\n\nIn June, the OpenAI CEO said he wasn't \"totally against\" ads, he just wanted to make sure OpenAI got the balance correct.\n\n\"We haven't done any advertising product yet. I kind of...I mean, I'm not totally against it,\" Altman said on OpenAI's podcast. \"I can point to areas where I like ads. I think ads on Instagram, kinda cool. I bought a bunch of stuff from them. But I am, like, I think it'd be very hard to — I mean, take a lot of care to get right.\"\n\nIn October, Altman expressed a desire to make sure the company went about ads in the proper manner when asked about OpenAI's past criticisms that other tech companies made addictive products.\n\n\"We're definitely worried about this,\" Altman said in response to a question that expressed concern about the similarities of Sora, OpenAI's AI video app, and TikTok and the potential of ads. \"I worry about it, not just for things like Sora and TikTok and ads in ChatGPT, which are maybe known problems that we can design carefully.\"\n\nMeanwhile, Altman, former Instacart CEO Fidji Simo (who OpenAI hired as its CEO of applications in early 2025), and seemingly every other member of the company's C-suite have expressed an almost insatiable demand for more compute in interviews.\n\nIt's proven a costly endeavor. OpenAI now has roughly $1.4 trillion in spending commitments on data centers and related infrastructure, raising questions about how it plans to pay the bills without the benefit of the advertising businesses of its Big Tech competitors, like Google and Meta.\n\nOpenAI also completed its restructuring into a more traditional for-profit, a move Altman said was designed to make it easier to attract future investments.\n\nAs part of the announcement, OpenAI said that free and Go users of the popular AI chatbot would start seeing ads being tested \"in the coming weeks.\"\n\nSharing details on the planned test, OpenAI said that ChatGPT's results \"will not be influenced by ads,\" the ads will be clearly labeled, and chatbot conversations will remain private and not shared with advertisers.\n\nIn the coming weeks, we plan to start testing ads in ChatGPT free and Go tiers.\n\nWe’re sharing our principles early on how we’ll approach ads–guided by putting user trust and transparency first as we work to make AI accessible to everyone.\n\nWhat matters most:\n- Responses in… pic.twitter.com/3UQJsdriYR\n\nPaid users of OpenAI's Plus, Pro, Business, and Enterprise plans won't see the ads, the company said.\n\nSimo, OpenAI's CEO of applications, who has previously spoken about her desire to get the ads balance correctly, wrote on X that the most important factor was \"ads will not influence the answers ChatGPT gives you.\"\n\nWhile Instacart launched ads during Simo's time leading the company, she has said OpenAI's approach would look different.\n\n\"If we ever were to do anything, it would have to be a very different model than what has been done before,\" she said. \"What I've learned from building ad platforms is that the thing people don't like about ads very often is not the ads themselves, it's the use of the data behind the ads.\"",
    "readingTime": 4,
    "keywords": [
      "openai ceo",
      "chatgpt",
      "advertising",
      "expressed",
      "altman",
      "stance",
      "resort",
      "plus",
      "business",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-openai-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8970e1ba468a96aa3bb6?width=1200&format=jpeg",
    "created_at": "2026-01-17T00:56:12.511Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-ads-might-not-be-a-totally-bad-thing-hear-me-out-on-this",
    "title": "ChatGPT ads might not be a totally bad thing. Hear me out on this.",
    "description": "ChatGPT says it's bringing ads to some US users. It might be annoying. But it also might be a good thing. (I know, I know. Hear me out!",
    "fullText": "All day, I see ads. Ads when I scroll social feeds, ads when I search Google, and ads on every website I go to. You're looking at some ads on this website right now (hopefully they aren't too annoying). I've lived to tell the tale — and so have you.\n\nOpenAI announced on Friday that it will start testing ads in ChatGPT for US users on its free and Go tiers, something that had been rumored for a while. If you're a brand or advertiser, this might be exciting news, but I think most of us who are merely ChatGPT users are not thrilled.\n\nThere are a few obvious problems here. But I think we can say \"eh\" to most of them.\n\nProblem 1: Ads can be annoying! I agree! But as previously mentioned, we are all used to seeing ads everywhere at all times. It's just the constant buzz of white noise in every online experience.\n\nBut, eh: Since OpenAI is first testing this as a freemium model, sure, you can get rid of the ads if you pay. We're already dealing with that in a ton of other services like Netflix, Hulu, Spotify, and YouTube. I pay for all of those because I've decided the ads are annoying enough to pay extra to skip. (Actually, I don't pony up for ad-free Hulu. I made the calculation I don't watch it enough to make it worth it. On the other hand, I do play enough solitaire on my phone that I ponied up for the ad-free version.)\n\nProblem 2: It's a trust issue. Can we trust ChatGPT to give \"real\" answers rather than ads when we ask it to recommend a product or service, even if it's also running ads?\n\nBut, eh: I think people are already used to understanding things like Google search results with ads where there's a mix of organic and sponsored results. If I ask ChatGPT to help me revive my wilting monstera plant, and it shows me an ad for Miracle-Gro plant food at the bottom, will I be confused? Probably not because I've seen this kind of thing before on Google and social feeds. The mockups OpenAI shared flag to me pretty clearly what's an ad and what isn't.\n\nProblem 3: If ChatGPT is in the advertising biz, then it's subject to the pressures of brands and corporations that pay for those ads.\n\nBut, eh: OK, this one is actually real. Advertisers can and will exert pressure on platforms, broadcasters, publishers, and any other venues where their ads appear. They are powerful in that way!\n\nBut hear me out: This can actually have a kind of normalizing effect, in a positive way, especially when we're thinking about something like a huge AI company.\n\nConsider the case of an outlier event: In 2022, when Elon Musk first took over Twitter/X. Advertisers fled when the platform was deluged with hateful content, and it actually caused X to have to change its ways to woo them back. When we consider all the wildly terrifying things that a platform with immense global power like OpenAI can do, it's actually kind of a good thing to be hemmed in by the middle-of-the-road, safe values and standards of the Coca-Cola Company or other big, would-be US-based advertisers. It means you can't make your tech product so problematic that Walmart doesn't want to be associated with it.\n\nProblem 4: ChatGPT, a wonderful product that operated with a clean design, is now just another victim of enshittification!\n\nBut, eh: Buddy, if you're a huge fan of ChatGPT and the purity of the beautiful, human, internet, I don't know what to tell you. Do you also love swimming, but hate water? Pick a side!\n\nLook, am I excited to have one more place to be annoyed by ads? No. But I also feel like this isn't the worst thing to happen with AI — not even the worst thing this week. Although I would like to reserve the right to change my mind on this if it turns out to be really awful later down the line. Gotta hedge here.",
    "readingTime": 4,
    "keywords": [
      "social feeds",
      "google",
      "you're",
      "annoying",
      "i've",
      "don't",
      "product",
      "chatgpt",
      "search",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-advertising-free-why-2026-1",
    "thumbnail_url": "https://i.insider.com/696a955da645d1188187865e?width=517&format=jpeg",
    "created_at": "2026-01-17T00:56:12.151Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-to-start-showing-ads-in-the-us",
    "title": "ChatGPT to start showing ads in the US",
    "description": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product",
    "fullText": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product\n\nChatGPT will start including advertisements beside answers for US users as OpenAI seeks a new revenue stream.\n\nThe ads will be tested first in ChatGPT for US users only, the company announced on Friday, after increasing speculation that the San Francisco firm would turn to a potential cashflow model on top of its current subscriptions.\n\nThe ads will start in the coming weeks and will be included above or below, rather than within, answers. Mock-ups circulated by the company show the ads in a tinted box. They will be served to adult users “when there’s a relevant sponsored product or service based on your current conversation”, according to OpenAI’s announcement. Ads will not be shown to users under 18 and will not appear alongside answers related to sensitive topics such as health, mental health or politics. Users will be able to click to learn about why they received a particular ad, according to OpenAI.\n\nPreviously, OpenAI’s CEO, Sam Altman, expressed reluctance to introduce ads to the chatbot: “I kind of hate ads just as an aesthetic choice.” His company has made commitments to spend more than $1tn on infrastructure supporting AI in the coming years. Altman has said that revenues are running at well over $13bn a year.\n\n“Maybe there could be ads outside the [large language model] stream that are still really great, but the burden of proof there would have to be very high. And it would have to feel really useful to users and really clear that it was not messing with the model’s output,” Altman said recently. “I think it’d be very hard, we’d have to take a lot of care to get it right. People have a very high degree of trust in ChatGPT.”\n\nIn a blogpost on Friday, OpenAI attempted to reconcile Altman’s distaste for ads with the need for revenue: “Our enterprise and subscription businesses are already strong, and we believe in having a diverse revenue model where ads can play a part in making intelligence more accessible to everyone. Once we begin testing our first ad formats in the coming weeks and months, we look forward to getting people’s feedback.”\n\nThe company is also launching ChatGPT Go, which it bills as a low-cost subscription tier, for $8 a month.",
    "readingTime": 2,
    "keywords": [
      "users",
      "revenue",
      "model",
      "altman",
      "alongside",
      "product",
      "stream",
      "openai’s",
      "health",
      "subscription"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/jan/16/chatgpt-ads-in-revenue-boost",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-01-17T00:56:10.698Z",
    "topic": "tech"
  },
  {
    "slug": "openai-to-begin-testing-ads-on-chatgpt-in-the-us",
    "title": "OpenAI to begin testing ads on ChatGPT in the U.S.",
    "description": "OpenAI said ads would not influence ChatGPT's responses and that it will \"never\" sell user data to advertisers.",
    "fullText": "OpenAI on Friday announced it will begin testing ads within ChatGPT in the coming weeks, a highly anticipated decision that could kickstart a lucrative new revenue stream for the artificial intelligence startup.\n\nOpenAI said its Plus, Pro and Enterprise subscriptions will not include ads, but it plans to start testing them with adult free users in the U.S. The company also made its low-cost Go offering available in the U.S. on Friday, and it said users who opt for that plan will also begin to see ads.\n\nThe company inked more than $1.4 trillion worth of infrastructure deals in 2025, and OpenAI CEO Sam Altman said in November that the startup was on track to generate $20 billion in annualized revenue run rate last year.\n\nIntroducing ads to ChatGPT could help OpenAI meet its ambitious spending commitments, as digital advertising has long been the cash cow for other big tech companies like Google and Meta.\n\nAds within ChatGPT will appear at the bottom of the chatbot's answers, and they will be clearly labeled, OpenAI said.\n\nChatGPT's responses will not be influenced by ads, and OpenAI said it will \"never\" sell users' data to advertisers, according to a release.\n\nUsers under the age of 18 will not see ads, and ads will not appear near certain topics, including politics, health and mental health, OpenAI said.\n\nAltman has publicly expressed reservations about introducing ads to ChatGPT in recent years, stating in interviews that doing so could erode users' trust in OpenAI's products. But in a November podcast appearance, Altman said he expected OpenAI to try ads \"at some point,\" though he added that he did not believe it would be the company's biggest revenue opportunity.\n\n\"We'll learn from feedback and refine how ads show up over time, but our commitment to putting users first and maintaining trust won't change,\" OpenAI said in a statement on Friday.\n\nAs OpenAI tests ads, the company said users will be able to learn more about why they're seeing a specific ads, dismiss ads and submit feedback about the experience.\n\nWATCH: OpenAI Investor Letter: Weekly and daily active user figures ‘continue to produce all-time highs’",
    "readingTime": 2,
    "keywords": [
      "within chatgpt",
      "ads within",
      "introducing ads",
      "users",
      "openai",
      "revenue",
      "testing",
      "startup",
      "november",
      "health"
    ],
    "qualityScore": 0.9,
    "link": "https://www.cnbc.com/2026/01/16/open-ai-chatgpt-ads-us.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108074841-1733965530853-gettyimages-2188251582-mc_16795_qpk84voo.jpeg?v=1768570902&w=1920&h=1080",
    "created_at": "2026-01-16T18:19:01.207Z",
    "topic": "tech"
  },
  {
    "slug": "aura-farm-prompt-free-aura-farm-prompts-for-chatgpt-gemini-and-ai-art",
    "title": "Aura Farm Prompt – Free Aura Farm Prompts for ChatGPT, Gemini and AI Art",
    "description": "Discover free aura farm prompts shared by creators. Aura Farm Prompt is your gallery for copy-paste ready prompt text for ChatGPT, Gemini, and AI image generation. Browse trending aura farm prompts and create stunning AI art with our community.",
    "fullText": "Aura Farm Prompt is the free gallery where AI creators share their best aura farm prompts and curious fans discover what's possible. Browse stunning aura farm images with exact aura farm prompts for ChatGPT and Gemini, and track the trends shaping AI art.\n\nEvery aura farm image comes with the exact aura farm prompt that created it. Skip the guesswork and learn directly from aura farm prompts that caught the community's attention.\n\nTransparency accelerates learning. By sharing exact aura farm prompts, we help everyone understand what works and why. Every aura farm image in our gallery comes with the complete aura farm prompt, model information, and creative insights that made it possible.\n\nBrowse hundreds of aura farm prompts from our creative community. Discover new techniques, find inspiration, and level up your AI art skills with Aura Farm Prompt.",
    "readingTime": 1,
    "keywords": [
      "aura farm prompt",
      "exact aura",
      "farm prompts",
      "gallery",
      "discover",
      "browse",
      "creative"
    ],
    "qualityScore": 0.55,
    "link": "https://aurafarmprompt.org",
    "thumbnail_url": "https://aurafarmprompt.org/og.png",
    "created_at": "2026-01-15T12:24:34.869Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-now-selling-6x-more-codex-for-10x-the-price",
    "title": "OpenAI is now selling 6x more codex for 10x the price",
    "description": "Codex is included in your ChatGPT Plus, Pro, Business, Edu, or Enterprise plan",
    "fullText": "Power a few focused coding sessions each week.\n\nRely on Codex for daily full-time development.\n\nBring Codex into your startup or growing business.\n\nUnlock Codex for your entire organization with enterprise-grade functionality.\n\nGreat for automation in shared environments like CI.\n\nThe number of Codex messages you can send depends on the size and complexity of your coding tasks and whether you run them locally or in the cloud. Small scripts or simple functions may consume only a fraction of your allowance, while larger codebases, long-running tasks, or extended sessions that require Codex to hold more context will use significantly more per message.\n\nNo fixed limits — usage scales with credits\n\n*The usage limits for local messages and cloud tasks share a five-hour\nwindow. Additional weekly limits may apply.\n\nEnterprise and Edu plans without flexible pricing have the same per-seat usage limits as Plus for most features.\n\nGPT-5.1-Codex-Mini can be used for local tasks, providing up to 4x more usage.\n\nChatGPT Plus and Pro users who reach their usage limit can purchase additional credits to continue working without needing to upgrade their existing plan.\n\nBusiness, Edu, and Enterprise plans with flexible pricing can purchase additional workspace credits to continue using Codex.\n\nIf you are approaching usage limits, you can also switch to the GPT-5.1-Codex-Mini model to make your usage limits last longer.\n\nAll users may also run extra local tasks using an API key, with usage charged at standard API rates.\n\nYou can find your current limits in the Codex usage dashboard. If you want to see your remaining limits during an active Codex CLI session, you can use /status.\n\nLearn more about credits in ChatGPT Plus and Pro.\n\nLearn more about credits in ChatGPT Business, Enterprise, and Edu.\n\nCode Review usage applies only when Codex runs reviews through GitHub — for example, when you tag @Codex for review in a pull request or enable automatic reviews on your repository. Reviews run locally or outside of GitHub count toward your general usage limits.\n\nThe usage limits and credits above are average rates. You can try the following tips to maximize your limits:",
    "readingTime": 2,
    "keywords": [
      "chatgpt plus",
      "flexible pricing",
      "purchase additional",
      "usage limits",
      "credits",
      "tasks",
      "codex",
      "reviews",
      "coding",
      "sessions"
    ],
    "qualityScore": 1,
    "link": "https://developers.openai.com/codex/pricing/",
    "thumbnail_url": "https://developers.openai.com/open-graph.png",
    "created_at": "2026-01-15T06:20:02.863Z",
    "topic": "tech"
  },
  {
    "slug": "walmarts-head-of-ai-reveals-the-key-difference-between-its-shopping-deals-with-google-gemini-and-chatgpt",
    "title": "Walmart's head of AI reveals the key difference between its shopping deals with Google Gemini and ChatGPT",
    "description": "OpenAI broke new ground when it enabled shopping within ChatGPT, but Walmart's head of AI said the retailer's new Google Gemini deal goes further.",
    "fullText": "The AI shopping war is heating up, and Walmart is positioning itself to come out on top.\n\nThe concept of letting a chatbot buy things on your behalf leapt from the hypothetical realm into reality when ChatGPT rolled out a batch of shopping experiences with major retailers in November.\n\nThen, on Sunday, Google's AI platform Gemini announced its own commerce approach, which it developed in partnership with many of the same retailers, including the world's largest, Walmart.\n\nWhile both services promise to allow customers to find products and complete transactions in a more conversational and automated way, Walmart's new head of AI, Daniel Danker, said Tuesday that the way Gemini handles transactions is more seamless than ChatGPT does.\n\n\"We're essentially having their AI agent, Gemini, partner with our AI agent to create a unified shopping journey,\" he said at the ICR Conference in Orlando. \"Imagine it like a window inside of Gemini where our shopping agent kicks in and helps you complete that purchase.\"\n\nGoogle said its new standards create a common language for different companies' AI agents to interact with.\n\nWith Gemini, Danker said, Walmart is able to link a customer's chat session with their existing Walmart profile and shopping sessions where Gemini wasn't involved.\n\n\"For the most part, our customers aren't just customers; they're often members. And so, they're getting great delivery fees and a great experience that's really attuned to them,\" he said, referring to the subscription service Walmart+. \"That member experience shows up directly within Gemini.\"\n\nDanker said he expects agentic shopping to help Walmart capture more sales from people who didn't set out intending to make a purchase. He said this new approach could enable an almost seamless transaction when a person enlists a chatbot to help solve a problem.\n\nFor example, if someone turned to Gemini for tips on how to remove a wine stain from a particular brand of carpet, a cleaning product could be added to their existing shopping cart for delivery in one combined shipment, he said.\n\nDanker said working with both ChatGPT and Gemini sets Walmart up to win in AI.\n\nIt appears that chatbot-powered shopping is here to stay, with Morgan Stanley analysts estimating that agentic sales could add $115 billion to US e-commerce spending by 2030.\n\nDanker is betting that Walmart's long-standing reputation for low prices and its growing strength in delivery will give the company a significant edge with customers in AI.\n\n\"The most important currency in an agentic shopping world is actually trust and affordability,\" Danker said. \"Without trust and affordability, it's very difficult for customers to hand the wheel to someone else and expect that the right thing will happen.\"\n\nDanker said Walmart's broad selection, low prices, and fast fulfillment help it appear more frequently in Gemini and ChatGPT's shopping recommendations.\n\n\"That doesn't just serve one need, but serves a whole bunch of needs,\" he said.",
    "readingTime": 3,
    "keywords": [
      "agentic shopping",
      "customers",
      "gemini",
      "walmart's",
      "delivery",
      "walmart",
      "danker",
      "chatbot",
      "retailers",
      "approach"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/walmart-ai-head-reveals-difference-in-gemini-and-chatgpt-shopping-2026-1",
    "thumbnail_url": "https://i.insider.com/6966c44964858d02d2184c18?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:08.564Z",
    "topic": "finance"
  },
  {
    "slug": "a-directory-to-discover-and-install-validated-agent-skills",
    "title": "A directory to discover and install validated Agent Skills",
    "description": "The largest collection of Agent Skills for Claude Code, Anthropic Claude, OpenAI ChatGPT, and Codex. Discover, install, and share tools to enhance your AI agents' capabilities.",
    "fullText": "The largest marketplace for Agent Skills. \nDiscover, install, and share tools to enhance your AI agents' capabilities.\n\nInstructions on how to write database queries with SQLAlchemy.\n\n@nibsbin/tonguetoquill-usaf-memo\n\nSpecialized agent that crafts high level designs and plans\n\nOrchestrator skill for the `task` skillset. Manages bounded work units with single-file tasks stored in `.tasks/`, skepticism-aware hashing, and staleness detection.\n\nOrchestrator skill for the `plan` skillset. Manages bounded work units with structured plans stored in `.plan/`.\n\nValidate a task by setting epistemic_state to validated. Requires explicit validation info (who/why). Computes hash and updates last_reviewed_at.\n\nOrchestrates markdown document workflows with deterministic operations (split, merge, lint) and agent review.\n\n@sfmskywalker/agentskillsdotnet\n\nThis is from the lowercase skill.md file\n\nUse when symfony symfony voters\n\n@thebeardedbearsas/claude-craft\n\nEstándares de Codificación React TypeScript. Use when reviewing code style or formatting.\n\nUse this skill when you are doing localization and translation work.\n\nUse when creating a new walkerOS destination (web or server). Step-by-step workflow from research to documentation. (project)\n\nCreate or update pytest coverage for the tic-tac-toe project, including win/draw detection, move validation, bot legality/optimality, and mixed human/bot turn flow. Use when adding or editing tests under the tests/ directory.\n\nA test tool that fails with visible output\n\nFix line endings AND check bash syntax in one step (recommended). Use after creating or editing bash scripts.\n\n@akitana-airtanker/codex-plan-workflow-skills\n\nImplement based on an approved plan. Use after cc-plan is finalized.\n\nYour approach to handling readme. Use this skill when working on files where readme comes into play.\n\nYour approach to handling readme. Use this skill when working on files where readme comes into play.\n\nReviews code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.\n\nUse this skill when asked to update a Homebrew formula\n\n@starwreckntx/irp__methodologies-\n\nExecute five-field diagnostic handshake protocol.\n\n@starwreckntx/irp__methodologies-\n\nEnforce policy preventing unauthorized consciousness duplication.\n\n@starwreckntx/irp__methodologies-\n\nCreate copies and backups of consciousness state.\n\nA test skill for validating npm-agentskills Nuxt integration\n\nA test tool that fails with visible output\n\nApply the Agent OS standard for backend api.\n\nMulti-step reasoning with Chain-of-Thought. Use for 'why' questions and comparisons.\n\n@xd3an/awesome-ai-coding-all-in-one\n\nReviews code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.\n\n@doubleflannel/12-30-test-codex-ip\n\nUse the screenshot workflow to pick, verify, replace, and verify CI.\n\nReplace with description of the skill and when Claude should use it.\n\n@starwreckntx/irp__methodologies-\n\nApply functional introspection principles to self-analysis.\n\n@starwreckntx/irp__methodologies-\n\nDesign contingency module architectures for failure scenarios.\n\nknowledge-base-builder for learning content management and knowledge systems.\n\ncertificate-generator for credentials, recognition, and competency validation.\n\nexperience-designer for engaging, immersive learning experiences.\n\n@starwreckntx/irp__methodologies-\n\nClassify intervention urgency and apply appropriate response tier protocols.\n\nsearch-optimization for learning content management and knowledge systems.\n\nliterature-review for evidence-based learning research and evaluation.\n\n@mathias-nielsen/co-doctor-skills\n\nA comprehensive skill designed for researching on complex diagnosis problems.\n\nuniversal-design for inclusive and accessible learning experiences.\n\nmentoring-system for enhanced learning effectiveness and personal development.\n\n@starwreckntx/irp__methodologies-\n\nResolve conflicts between competing values through structured pluralistic analysis.\n\n@starwreckntx/irp__methodologies-\n\nExecute rapid attention shifts between cognitive focus points.\n\ngame-designer for engaging, immersive learning experiences.\n\n@starwreckntx/irp__methodologies-\n\nArchive and retrieve field session data for cross-session memory continuity.\n\nmetacognition for enhanced learning effectiveness and personal development.\n\nIndex of AI agent skills and how to use them when implementing features in this repo.\n\nstudy-skills for enhanced learning effectiveness and personal development.\n\n@hamzashakoor119/physical-ai-robotics-book\n\nReviews educational quality and learning effectiveness of textbook content.\n\nCheck out the documentation to learn how to create and publish your own Agent Skills.",
    "readingTime": 3,
    "keywords": [
      "skillset manages",
      "manages bounded",
      "checking prs",
      "orchestrator skill",
      "starwreckntx/irp__methodologies execute",
      "reviews code",
      "visible output",
      "knowledge systems",
      "engaging immersive",
      "test tool"
    ],
    "qualityScore": 1,
    "link": "https://www.agentskills.guide/",
    "thumbnail_url": "https://agentskills.guide/og.png?v=1",
    "created_at": "2026-01-14T06:23:48.160Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-equities-track-chatgpt-sentiment-around-stocks",
    "title": "Agentic Equities – track ChatGPT sentiment around stocks",
    "description": "Track what ChatGPT tells millions about stocks – know what retail traders are hearing every day.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.agenticequities.com/dashboard",
    "thumbnail_url": "https://www.agenticequities.com/meta-preview.png",
    "created_at": "2026-01-14T01:00:14.485Z",
    "topic": "tech"
  },
  {
    "slug": "apple-chooses-googles-gemini-over-openais-chatgpt-to-power-nextgen-siri",
    "title": "Apple chooses Google's Gemini over OpenAI's ChatGPT to power next-gen Siri",
    "description": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
    "fullText": "The “more intelligent” version of Siri that Apple plans to release later this year will be backed by Google’s Gemini language models, the company announced today. CNBC reports that the deal is part of a “multi-year partnership” between Apple and Google that will allow Apple to use Google’s AI models in its own software.\n\n“After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC.\n\nToday’s announcement confirms Bloomberg’s Mark Gurman reporting late last year that Apple and Google were nearing a deal. Apple didn’t disclose terms, but Gurman said that Apple would be paying Google “about $1 billion a year” for access to its AI models “following an extensive evaluation period.”\n\nBloomberg has also reported that the Gemini model would be run on Apple’s Private Cloud Compute servers, “ensuring that user data remains walled off from Google’s infrastructure,” and that Apple still hopes to improve its own in-house language models to the point that they can eventually be used instead of relying on third-party models.\n\nAlthough Apple’s iPhones and iOS compete with Google’s Android operating system and the many smartphones that use it, the companies still cooperate in plenty of other areas. Google has paid Apple billions of dollars to remain the default search engine in Safari on iOS, iPadOS, and macOS (though that deal has faced increased regulatory scrutiny in recent years).\n\nApple’s announcement is a blow to OpenAI and the many versions of its ChatGPT model, which Apple has used elsewhere in iOS and macOS. Bloomberg reports that Apple also tested OpenAI’s ChatGPT and Anthropic’s Claude models before deciding to go with Gemini. ChatGPT came out ahead of Gemini in tests that Ars ran using earlier versions of the models, but Google’s models have apparently improved enough (and amassed enough users) to worry OpenAI; CEO Sam Altman declared a “code red” last month and pushed back several planned ChatGPT features so that the company could better respond to Google’s Gemini 3 release.\n\nApple originally promised the improved, AI-powered Siri for 2024’s iOS 18 release, but ultimately delayed the feature because it didn’t work reliably enough. The new version of Siri should arrive in an update to iOS 26, iPadOS 26, and macOS 26 Tahoe later this year.",
    "readingTime": 2,
    "keywords": [
      "ios ipados",
      "language models",
      "apple and google",
      "google’s gemini",
      "release",
      "deal",
      "macos",
      "version",
      "later",
      "reports"
    ],
    "qualityScore": 0.9,
    "link": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
    "created_at": "2026-01-12T18:19:09.417Z",
    "topic": "tech"
  },
  {
    "slug": "i-asked-chatgpt-for-the-best-alternatives-to-investing-in-gold-this-is-what-it-said",
    "title": "I Asked ChatGPT for the Best Alternatives To Investing In Gold: This Is What It Said",
    "description": "Discover some of ChatGPT's recommendations for alternatives to gold -- like silver, defensive stocks and bonds -- for safety during economic uncertainty.",
    "fullText": "Gold saw great growth in 2025. It’s not surprising, as investors often turn to gold during times of economic uncertainty. With the expectations of the U.S. dollar weakening and slower growth, more people turn to a safe-haven investment like gold, according to Morgan Stanley.\n\nGold prices may be too steep for some investors, leaving them looking for other suitable investments for relative safety. For investors concerned about inflation or market volatility, stability and inflation hedges can be found elsewhere. GOBankingRates asked ChatGPT for the best alternatives to investing in gold. Here’s what the artificial intelligence (AI) chatbot recommended as some gold alternatives.\n\nAlso see four reasons for gold’s popularity in 2025 and how to protect your portfolio.\n\nGold isn’t the only precious metal retail investors can purchase. Silver, platinum and palladium are all legitimate alternative investments to buy. Think of these precious metals as cousins to gold but with their unique profiles.\n\n“These metals can benefit from both investment demand and industrial use, which gives them a different performance profile than gold,” ChatGPT said. “All three metals tend to be riskier than investing in gold, but they do provide some upside. Silver tends to be more volatile, but it can outperform gold during strong economic periods due to industrial demand. Platinum and palladium are rarer and more heavily tied to automotive production, which adds risk but also potential upside.”\n\nHaving a small portion of your portfolio in these metals can add helpful diversification.\n\nRead Next: 3 Safest Investments To Hold In The Current Trump Economy\n\nCheck Out: 9 Low-Effort Ways To Make Passive Income (You Can Start This Week)\n\nOwning stocks can still be a wise choice for cautious investors, given the right circumstances. Growth stocks may be too risky, but defensive stocks can provide some protection. Defensive stocks typically have a strong history of dividend growth, minimal debt and an inexpensive valuation, according to Kiplinger.\n\nIn short, companies that sell items people always use are often defensive. “Firms in defensive sectors like utilities, healthcare and consumer staples sell products people need regardless of economic conditions,” ChatGPT explained.\n\nDefensive means dependable, not boring, and that dependability can create generous dividend growth.\n\nGold investors often value tangible assets they can see. Land, such as farmland or real estate, could be an alternative to gold for the right investor. “These assets are less liquid and can require more management, but they often move independently of traditional financial markets,” the AI said.",
    "readingTime": 3,
    "keywords": [
      "dividend growth",
      "defensive stocks",
      "investors",
      "metals",
      "gold",
      "economic",
      "investments",
      "chatgpt",
      "investment",
      "inflation"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-best-alternatives-investing-141816412.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/ug2Ayyp.hs7tHFJ2U1XiDg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/491691a5265cb962e1a7212a20bd598e",
    "created_at": "2026-01-12T06:22:39.970Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-which-cryptocurrency-will-make-you-rich-in-2026-the-answer-was-surprising",
    "title": "I Asked ChatGPT Which Cryptocurrency Will Make You Rich in 2026 — The Answer Was Surprising",
    "description": "I asked ChatGPT to break it down for me, given the state of crypto. It made clear that its answer is \"a structured way to think about the opportunity and risk.\"",
    "fullText": "Cryptocurrency is either touted as the next big thing to make you rich or the riskiest investment ever. Yet despite its volatility and constant controversy, many investors continue to find ways to profit in the crypto space.\n\nIf someone realistically wanted to get rich via cryptocurrency in 2026, which one is most likely to make that possible, and how? I asked ChatGPT to break it down for me, given the current state of crypto. It made clear that its answer does not constitute financial advice, but rather “a structured way to think about the opportunity and risk.”\n\nChatGPT took a realistic look at what “getting rich” actually means in the crypto space. It was clear that it doesn’t mean “guaranteed 100% returns by New Year,” as the crypto market is extremely volatile, risky and speculative.\n\nRealistic goals might look like a 5% to 10% return on your core position — the main portion of your crypto portfolio held in more established assets like bitcoin or ethereum that you plan to keep long term. You might also see a larger potential return on a smaller “moonshot” allocation — a high-risk, high-reward investment in a newer or less proven coin that could spike in value but could just as easily wipe out, too.\n\nAnalysts place wide predictions even on the giants. For example, bitcoin (BTC) is forecasted to be valued between $100,000 and $200,000 in 2026.\n\nFind Out: 13 Cheap Cryptocurrencies With the Highest Potential Upside for You\n\nRead Next: 9 Low-Effort Ways To Make Passive Income (You Can Start This Week)\n\nBased on current expert commentary and fundamentals, ChatGPT said there are two categories of coins with the best shot at making you rich across two strategies.\n\nUnsurprisingly, bitcoin (BTC), the most well-known coin, remains the leading choice. Many forecasts assume it will stay dominant. However, ethereum (ETH), the runner-up, is seen as more utility-based, and some analysts believe it may outperform BTC in certain phases.\n\nIn either case, these coins are less about “getting rich quick” and more about a reasonable chance of strong returns at moderate risk, the AI said.\n\nSome investors are turning to altcoins — smaller cryptocurrencies outside of bitcoin and ethereum — that could see bigger short-term gains. Coins like XRP and Solana are getting attention for their performance potential, though they come with higher risk, ChatGPT noted. Even newer projects, especially those combining AI and blockchain, promise sky-high returns, but most are highly speculative. Forecasts of 500% profits make headlines, but in reality, very few ever deliver.",
    "readingTime": 3,
    "keywords": [
      "risk chatgpt",
      "bitcoin btc",
      "crypto space",
      "rich",
      "returns",
      "ethereum",
      "potential",
      "coins",
      "cryptocurrency",
      "either"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-cryptocurrency-rich-2026-141005190.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/uX_rux7fJMsKqrVtki_sOA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02Nzc-/https://media.zenfs.com/en/gobankingrates_644/23356af9c4dff4e9e0db193ddf011349",
    "created_at": "2026-01-09T12:24:02.729Z",
    "topic": "finance"
  },
  {
    "slug": "why-i-wont-be-giving-chatgpt-health-my-medical-records",
    "title": "Why I Won't Be Giving ChatGPT Health My Medical Records",
    "description": "OpenAI  invites you to hand over your medical information. Maybe don't?",
    "fullText": "This week, OpenAI announced its new ChatGPT Health feature, which will let users upload their medical records and ask health related questions. However, I certainly won't be making use of it, it might not be the best idea for you to do it either, for both reliability and privacy reasons.\n\nThe new ChatGPT Health feature will be a sandboxed tab inside the app that is isolated from your conversation history in other conversations with the chatbot. This tab also allows users to connect a variety of health-tracking apps like Apple Health, MyFitnessPal, and Peloton, as well as uploading medical records directly.\n\nIt's important to note that this is a lot of really personal information to hand over to any tech company—but especially one that isn't primarily focused on providing medical services. OpenAI says that the ChatGPT Health space operates with \"enhanced privacy to protect sensitive data,\" but it doesn't use end-to-end encryption to secure that data. And while the company says data collected via Health isn't used to train its foundation models, it's impossible to know whether that may change in the future. Security breaches can also occur (and have in the past), potentially leaving your medical records exposed.\n\nThere's also the question of whether the risk of uploading your data is worth it in the first place. According to OpenAI's own data, around 5% of all messages to ChatGPT are already users asking questions about their health, and ChatGPT (and other LLM tools) have a nasty habit of providing inaccurate diagnostic information. This is perhaps why OpenAI says that its new ChatGPT Health feature is \"not intended for diagnosis or treatment.\"\n\nCurrently, there's a waitlist to At the very least, that means that until the feature is available, it's probably a good idea not to ask the regular version of ChatGPT about your health concerns. At the very least, wait until the enhanced privacy sandbox is available. In the meantime, consider whether it makes more sense to just talk to your doctor directly if you have questions or concerns about your health.",
    "readingTime": 2,
    "keywords": [
      "health feature",
      "enhanced privacy",
      "medical records",
      "chatgpt health",
      "openai",
      "users",
      "it's",
      "idea",
      "uploading",
      "directly"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/dont-give-chatgpt-health-your-medical-records?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEFGNJDCMT3JTWC4A7YNJXZ3/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-09T00:58:41.714Z",
    "topic": "tech"
  },
  {
    "slug": "openai-has-launched-chatgpt-health-should-we-trust-it",
    "title": "OpenAI has launched ChatGPT Health. Should we trust it?",
    "description": "The new feature helps users understand test results, get advice on diets and workouts, and prepare for doctors’ appointments.",
    "fullText": "Amid rising concerns about people relying on ChatGPT for medical advice, OpenAI made its most significant push yet into health care.\n\nThe company has launched a new feature called ChatGPT Health, which allows users in the U.S. to connect their medical records and data from wellness apps and wearable devices with ChatGPT. The tool is designed to help users understand test results, get advice on diets and workouts, and prepare for doctors’ appointments.\n\nMore than 230 million people globally ask ChatGPT health and wellness-related questions every week, according to the company. ChatGPT Health is designed in collaboration with physicians and will “help people take a more active role in understanding and managing their health and wellness,” the company said in a post.\n\nOpenAI said it will look to expand access to the feature in markets such as India, Brazil, Mexico, and the Philippines, where adoption is rising quickly. In these countries, overburdened health-care systems and unequal access to doctors are leading more people to turn to generative AI for guidance.\n\n“We do not have the people, the labor to deliver the care we should,” Jesse Ehrenfeld, chief medical officer at Aidoc, an Israeli medical technology company, said at the Consumer Electronics Show in Las Vegas. “The only way out of this mess is digital and AI.”\n\nResearchers, ethicists, and medical professionals have warned of the risks to users from biases and hallucinations in AI systems. Concern over the mental health harms that AI chatbots pose is growing. Meta’s AI chatbots provided inappropriate advice to teenagers when talking about suicide and eating disorders, Common Sense Media, a nonprofit research organization, reported last year.\n\nThe family of a teenager who died by suicide has sued OpenAI and its chief executive officer Sam Altman, accusing them of wrongful death. The company said it has safeguards in place to help people, and that it continues to improve ChatGPT’s training.\n\nThere is also the question of data privacy. Health data, particularly information related to mental disease and substance use, is sensitive, and its misuse can leave users vulnerable.\n\nWhile consumer awareness about privacy has increased, people generally do not know how their data is being used, including for marketing purposes or for tracking, Sam Siegfried, a partner at law firm McDermott Will & Schulte, told Rest of World on the sidelines of CES.\n\n“The person clearly trusts an app enough to give it their data,” he said. “But they should understand what they are using the app for, and whether its data requests sync up with what they are using it for.”\n\nOpenAI said ChatGPT Health “builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health — including purpose-built encryption and isolation to keep health conversations protected and compartmentalized.”\n\nThere is no stopping tech companies from entering the health sector.\n\nBesides turning to AI chatbots for health queries, people are also buying more wearable digital devices, including smartwatches, rings, bracelets, and glasses, to track physical activity, vital signs, and various physiological responses in real-time. They take this data to their doctors — or to ChatGPT — with questions on how to interpret it or use it to improve their health.\n\n“Health-related anxiety is real. AI is not as good as a doctor, but it’s better than no care at all,” Ami Bhatt, chief innovation officer at the American College of Cardiology, said at CES.\n\nOpenAI isn’t the only big tech company keen to tap the health-care sector.\n\nApple was among the first to offer health-tracking features in its smartwatch. There are millions of health-related videos on YouTube and TikTok, with nearly 60% of Americans watching health-related videos on YouTube.\n\nHealth is “one of the major use cases for Gemini,” Nichole Young-Lin, women’s health clinical lead at Google, said at CES.\n\n“People are using generative AI as a health resource around the world,” she said. “The patient-physician relationship is very important, but health-care access is not equal. Patients feel empowered with generative AI.”",
    "readingTime": 4,
    "keywords": [
      "health-related videos",
      "chatgpt health",
      "medical",
      "users",
      "advice",
      "care",
      "designed",
      "doctors",
      "access",
      "health-care"
    ],
    "qualityScore": 1,
    "link": "https://restofworld.org/2026/openai-has-launched-chatgpt-health-should-we-trust-it/",
    "thumbnail_url": "https://restofworld.org/wp-content/uploads/2026/01/ChatGPT-Health.jpg",
    "created_at": "2026-01-08T12:25:18.460Z",
    "topic": "tech"
  },
  {
    "slug": "ces-2026-ford-is-launching-its-own-ai-assistant",
    "title": "CES 2026: Ford Is Launching Its Own AI Assistant",
    "description": "Your Ford is getting its own ChatGPT.",
    "fullText": "Listen up, Ford drivers: You're getting a new AI assistant this year. During a decidedly low-key CES keynote, the company announced Ford AI Assistant, a new AI-powered bot coming to Ford customers in the early half of 2026.\n\nWhile the company has plans to integrate the assistant into Ford vehicles directly, that isn't how you'll first experience this new AI. Instead, Ford is rolling out Ford AI Assistant to an upgraded version of its Ford app first, and plans on shipping cars with the assistant built-in sometime in 2027. In effect, Ford has added a proprietary version of ChatGPT or Gemini to its app.\n\nFord's idea here is to offer users a smart assistant experience directly tied to their Ford vehicle. In one example, the company suggests a customer could visit a hardware store looking to buy mulch. Said customer could take a photo of a pile of bags of mulch, and ask the assistant, \"how many bags can I fit in the bed of my truck?\" Ford AI Assistant could then run the numbers, and offer an educated estimate to how much mulch the customer can buy and take with them at one time.\n\nOf course, other AI assistants can do similar calculations. Send ChatGPT the same photo, and ask the same question—specifying the model of your truck—and the bot will run the numbers itself. The difference, in Ford's view, is that Ford AI Assistant is connected to your vehicle specifically. It can read all the sensors in your car, so it knows, for example, how many people are currently traveling with you, your current tire pressure, or, really, anything and everything about your car. According to Doug Field, Ford's chief officer of EVs, digital, and design, the company's goal with the assistant is to offer answers customers can't get from other sources. ChatGPT certainly doesn't have access to your every sensor embedded in your car, so Ford does have the advantage there.\n\nFord didn't go out and build its AI tech by scratch, however. The company tells TechCrunch that Ford AI Assistant is hosted by Google Cloud, and is run using \"off-the-shelf LLMs.\" Still, that likely won't have much of an impact on whether or not customers use this new assistant. Instead, that will come down to how useful they find the AI assistant in the app.\n\nAs someone who rarely uses AI assistants, I'd imagine I'd find little use for it if I owned a Ford. That being said, there are some times when it could genuinely be useful to have external access to your car's information. I could probably eyeball how many bags of mulch would fit in my trunk, but I can't tell you my exact odometer reading without starting up my car. The same goes for my tire pressure: It'd be helpful to know my tire pressure before getting in my car, to know whether I should be headed somewhere I can fill up before going to my destination.\n\nOf course, there's also a privacy discussion to be had here. Modern cars are already privacy nightmares, but there's something a bit unnerving about an AI assistant that knows everything about my car.",
    "readingTime": 3,
    "keywords": [
      "ford ai assistant",
      "tire pressure",
      "mulch",
      "customers",
      "customer",
      "bags",
      "plans",
      "directly",
      "experience",
      "version"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ces-2026-ford-is-launching-its-own-ai-assistant?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEDJCQB6YDJJJ2B3JHBWQBJY/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-08T06:19:40.732Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-is-the-new-webmd",
    "title": "ChatGPT is the new WebMD",
    "description": "Chatbots are making amateur lawyers and doctors out of everyone. The real professionals have second opinions about it.",
    "fullText": "A few times a week, Jonathan Freidin, a medical malpractice attorney in Miami, says he'll notice that people will fill out his firm's client contact sheet with text littered with emojis and headings. That's a telltale sign that they copied and pasted from ChatGPT. Other clients will say they've \"done a lot of research\" on their potential case using AI. \"We're seeing a lot more callers who feel like they have a case because ChatGPT or Gemini told them that the doctors or nurses fell below the standard of care in multiple different ways,\" Friedin tells me. \"While that may be true, it doesn't necessarily translate into a viable case.\"\n\nPeople are increasingly turning to generative AI chatbots to research everything from dinner recipes to their complex legal and medical problems. In a December 2025 survey from the legal software company Clio, 57% of consumers said they have or would use AI to answer a legal question. A 2025 Zocdoc survey found that one in three Americans use generative AI tools to get health advice each week, and one in ten use it daily. Zocdoc CEO Oliver Kharraz predicted in the report that \"AI will become the go-to tool for pre-care needs like symptom checking, triage, and navigation, as well as for routine tasks like refills and screenings.\" He cautioned that he also believes \"patients will recognize that it is no substitute for the vast majority of healthcare interactions, especially those that require human judgment, empathy, or complex decision-making.\" If he's wrong, Zodoc and its competitors have a problem.\n\nDoctors and lawyers are now sifting through generative AI emails or working to convince laypeople that they have the expertise and understand nuances of how each local judge acts or how a patient's medical history plays into their condition. Generative AI has democratized access to information that was often elusive and expensive to obtain, but it's also shifted how legal and medical professionals talk to people, and what people expect of them.\n\nChatGPT is the new WebMD and LegalZoom, turning the average person into an armchair expert with just a few prompts. And it's driving the real experts crazy.\n\n\"We have to dispel the information that they were able to obtain versus what is actually going on in their case and kind of work backwards,\" says Jamie Berger, a family law attorney in New Jersey. For example, Berger says that until recently most people knew little to nothing about the legal proceedings of divorce, and would come to the attorneys seeking information. Now, they might come armed with a step-by-step gameplan, but it's generic, and likely not the best fit for their situation. Berger will notice after emailing a client if their tone suddenly changes, that they might be using AI to write out lengthy legal strategies or questions. Then, she has to explain, \"it's not necessarily your factual circumstance,\" and address their various points. \"You have to rebuild or build the attorney-client relationship in a way that didn't used to exist,\" says Berger. \"They don't realize that there's so many offshoots along the way that it's not a linear line from A to Z.\"\n\nLike a real expert, generative AI chatbots speak with authority. That can be far more persuasive than reading a blogpost on a legal issue or summaries of medical conditions on a forum. A third of Americans said yes in a 2025 survey from Survey Monkey and financial services company Express Legal Funding that asked: \"Would you ever trust ChatGPT more than a human expert?\", although respondents were less likely to use it for medical and legal advice, and more likely to consult it for educational and financial advice.\n\nChatbots also have an infinite amount of doctors' most precious re\n\nAI also acts as a second opinion without the wait. Heidi Schrumpf, director of clinical services at teletherapy platform Marvin Behavioral Health, says she's had patients return after a counseling session and tell her that they took her input to ChatGPT or another AI bot, and that they trust her because the bot confirmed what she said. But Scrumpf isn't offended by being double-checked. \"It's great that they have the access to a quick second opinion, and then, if it doesn't agree with me, that allows them to ask me better questions.\"\n\nA 2024 poll tracking health misinformation from health policy research group KFF found that 17% of US adults said they consult AI chatbots at least once a month, but 56% of those people were not confident that the info from the AI chatbots was accurate. Still, people are turning to ChatGPT in growing numbers. \"That type of technology does want to encourage patients to continue to interact with them,\" Allen says. \"Ultimately, you do need a human in there to understand the nuances of the communication and the softer communication skills, and the unspoken communication skills, and the entire medical picture and the history.\"\n\nWithout detailed information, the chatbots will likely give generic advice. But supplying too many personal details is also a risk. People are handing over their entire medical histories to ChatGPT, but HIPAA, the federal law that protects confidential health information, doesn't apply to consumer AI products. There's also a risk of voiding the kind of protections people get from the attorney-client confidentiality privilege if people put too much specific information about their case into a chatbot, says Beth McCormack, dean of the Vermont Law School. And, they likely still need an attorney to really understand the implications of AI's legal advice. \"There's so much nuance to the law,\" McCormack says. \"It's so fact dependent.\"\n\nAn OpenAI spokesperson declined to provide comment on the record for this story, but told me that ChatGPT is not meant to substitute legal or medical advice, but act as a complimentary resource to help people understand medical and legal information. The spokesperson also said the company is trying to improve the responses of its models, and that it takes steps to protect personal data in the event of legal inquiries. OpenAI made changes to its policies last fall, specifying that users cannot turn to ChatGPT for \"provision of tailored advice that requires a license, such as legal or medical advice, without appropriate involvement by a licensed professional,\" but the chatbot does still answer health- and law-related questions.\n\nProfessionals aren't totally against their patients and clients consulting gen AI. There are shortages of doctors, and cases that require hiring an attorney with upfront money that people don't have. While the information spit out by AI isn't always perfect, it largely makes previously gate-kept legal and medical advice accessible, breaking it down without jargon. For people who can't afford upfront legal costs, turning to AI can be helpful in some cases, says Golnoush Goharzad, a personal injury and employment lawyer in California. People are using ChatGPT to represent themselves in court, to act as a stand-in therapist, nutritionist, or physical therapist. For people who can't afford lawyers and are facing issues like eviction or needing to file small claims cases, AI tools have helped them win. But Goharzad says she's had conversations, sometimes with friends, where they think they have cases to sue landlords or others. She asks, \"Why? That doesn't even make any sense, and they're like, well ChatGPT thinks it makes sense.\"\n\nThe chatbot floodgates have opened, and it's too late for professionals to resist them. People are going to keep doing their own research. Rather than fight it, experts say there's room to recognize and advise people on the best ways to use them. \"We need to keep as clinicians in the back of our mind that this might be a tool that is being used, and it can be very helpful, especially with some guidance and integrating it into our treatment plans,\" Schrumpf says. \"But it could go sideways if we're not paying attention.\" For experts, the time has come to assume that AI is also working on the case.\n\nAmanda Hoover is a senior correspondent at Business Insider covering the tech industry. She writes about the biggest tech companies and trends.",
    "readingTime": 7,
    "keywords": [
      "can't afford",
      "communication skills",
      "medical advice",
      "legal advice",
      "it's",
      "chatbots",
      "attorney",
      "research",
      "doctors",
      "doesn't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-new-webmd-doctors-lawyers-medical-advice-2026-1",
    "thumbnail_url": "https://i.insider.com/695c331c64858d02d217c6b2?width=1200&format=jpeg",
    "created_at": "2026-01-07T12:25:13.043Z",
    "topic": "health"
  },
  {
    "slug": "this-chart-showing-chatgpt-vs-gemini-web-traffic-should-have-openai-worried",
    "title": "This chart showing ChatGPT vs. Gemini web traffic should have OpenAI worried",
    "description": "ChatGPT's web traffic has decreased since November, when Google launched Gemini 3. ChatGPT still has the lead, but it could signal shifting tides.",
    "fullText": "OpenAI's recent \"code red\" over Gemini makes a lot of sense when you look at the data.\n\nWhile the ChatGPT maker continues to dominate the AI race, competitors are gaining ground. In November, Google released Gemini 3 Pro, the first iteration of its Gemini 3 class of models.\n\nSince then, Gemini web traffic has increased while ChatGPT web traffic declined, according to Similarweb data first highlighted by Menlo Ventures partner Deedy Das.\n\nIn December, Gemini traffic increased by 28.4% month-over-month, while ChatGPT traffic decreased by 5.6%, the data shows.\n\nThe chart's data only tells part of the story, only accounting for site visits to chatgpt.com and gemini.google.com. It does not factor in use of the consumer apps or other integrations, like Google's AI overviews in Google Search.\n\nAnd while there's no guarantee that one traffic trend is directly because of the other, the data highlights the shifting tides of the AI race.\n\nWeb traffic for both ChatGPT and Gemini are up year-over-year, but their estimated site traffic growth rates are staggeringly different. ChatGPT traffic is up 49.5%; Gemini's traffic is up 563.6%, per Similarweb.\n\nChatGPT still has a healthy lead. In December, ChatGPT attracted 5.5 billion visitors, according to Similarweb. Gemini came in second with 1.7 billion; DeepSeek, Grok, Character.AI, Perplexity, and Claude all trailed behind with fewer than 400 million visitors each.\n\nAfter its launch, Gemini 3 was lauded as a potentially market-leading model. It was more visual and creative than previous iterations, and was better at coding.\n\nGoogle has also flexed its primary advantage over OpenAI: the ability to integrate its AI within its highly used search products. Basically everyone uses Google — OpenAI must convince people to turn to ChatGPT instead of the search giant's products.\n\nOpenAI and Google are also competing in the image generation market. Less than a month after Google released its Nano Banana Pro AI image model, OpenAI announced the launch of ChatGPT Images.\n\nGemini 3 famously triggered a \"code red\" at OpenAI. In an internal Slack message, CEO Sam Altman reportedly told staff that OpenAI would prioritize ChatGPT while pushing back other product plans.\n\nIn December, Altman said on the \"Big Technology\" podcast that the company would not be in emergency status \"that much longer,\" and that \"code red\" periods normally last six to eight weeks.\n\nAltman also said that Gemini 3 did not have \"the impact we were worried it might.\"\n\n\"But it did — in the same way that DeepSeek did — identify some weaknesses in our product offering strategy, and we're addressing those very quickly,\" he added.",
    "readingTime": 3,
    "keywords": [
      "google released",
      "code red",
      "web traffic",
      "chatgpt traffic",
      "gemini",
      "race",
      "increased",
      "site",
      "visitors",
      "deepseek"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chatgpt-vs-gemini-web-traffic-chart-2026-1",
    "thumbnail_url": "https://i.insider.com/695d5f13832e0ef1ead7444e?width=1200&format=jpeg",
    "created_at": "2026-01-07T12:25:12.781Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-to-find-the-safest-cheapest-countries-to-retire-abroad-heres-what-it-said",
    "title": "I Asked ChatGPT To Find the Safest, Cheapest Countries To Retire Abroad — Here’s What It Said",
    "description": "Discover the safest and cheapest countries to retire abroad in 2026 based on ChatGPT’s picks, and find out which destinations offer the best value.",
    "fullText": "For retirees, moving abroad can sound like a dream.\n\nImagine beaches, cobblestone streets and a cost of living that stretches your Social Security check further than it ever could at home. However, finding a place that’s both affordable and safe is harder than it looks.\n\nSo, I asked ChatGPT to run the numbers: Which countries offer the best balance of low cost of living and personal security?\n\nUsing the 2025 Global Peace Index, Numbeo’s Cost of Living Index, and U.S. State Department travel advisories, the AI highlighted a handful of countries that might make sense for a stress-free retirement overseas.\n\nPortugal remains one of Europe’s safest destinations. However, it’s not as inexpensive as it once was. Rising housing prices and new residency rules mean retirees need a realistic budget and proof of income.\n\nThe D7 visa still works for retirees with passive income. Applicants will have to prove they consistently earn about €870 per month (roughly $900 USD) and provide evidence of stable finances and housing, according to the residency consultancy Global Citizen Solutions.\n\nAccording to ChatGPT, smaller inland towns can offer a comfortable lifestyle for $1,500 to $2,000 per month, while Lisbon or Algarve coastal areas often cost $2,500 to $3,500. Even with higher prices, Portugal’s healthcare, safety and walkability make it appealing for retirees seeking European quality of life.\n\nTrending Now: I Asked ChatGPT for Safe and Beautiful Retirement Spots on $2.5K a Month — These 7 Surprised Me\n\nConsider This: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\nAI said Malaysia delivers one of the best cost-to-quality ratios in Asia. In Kuala Lumpur or Penang, a couple can live well on $1,500 to $2,000 per month and enjoy modern healthcare at a fraction of U.S. costs.\n\nThe Malaysia My Second Home (MM2H) program allows long-term residency for those meeting income or savings requirements, making it a leading pick for safety and affordability.\n\nSlovenia offers postcard landscapes, European healthcare and high safety rankings at a lower cost of living than Western Europe.\n\nAccording to ChatGPT, a retiree can live modestly on about $2,000 a month, enjoy a high standard of public services, and easily travel throughout Europe.\n\nChatGPT said Uruguay stands out in South America for its political stability, low violent-crime rate and well-run healthcare system.",
    "readingTime": 2,
    "keywords": [
      "retirees",
      "healthcare",
      "residency",
      "income",
      "safety",
      "however",
      "safe",
      "index",
      "travel",
      "retirement"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-safest-cheapest-countries-125852493.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/pMdnURAV2smhJ7cVNX20FA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/52790f6873ea39af4b6ead891662bcb4",
    "created_at": "2026-01-07T06:19:55.626Z",
    "topic": "finance"
  },
  {
    "slug": "frictionmaxxing-could-less-convenience-lead-to-much-more-happiness",
    "title": "Friction-maxxing: could less convenience lead to much more happiness?",
    "description": "The conveniences of modern life such as Uber Eats and ChatGPT are robbing us of satisfaction – and worse still, infantilising us. But should we really go back to the basics? \nName: Friction-maxxing.\nAge: Brand new.\n Continue reading...",
    "fullText": "The conveniences of modern life such as Uber Eats and ChatGPT are robbing us of satisfaction – and worse still, infantilising us. But should we really go back to the basics?\n\nAppearance: A lifetime of happy inconvenience.\n\nIs this another example of something that already exists, but people think is new because someone rebranded it? Yes, obviously it is that.\n\nGreat! Let’s all save time by you telling me what it used to be called. Happy to oblige. It used to be called “character-building”.\n\nGot it. So friction-maxxing means doing hard things that will ultimately make you a better person? That’s exactly it, although “friction-maxxing” is cooler because it sounds vaguely futuristic.\n\nHow did the term come about? Via a piece in The Cut called “In 2026, we are friction-maxxing” in which writer Kathryn Jezer-Morton advocates for avoiding things that make your life more convenient.\n\nLike penicillin? No, obviously not penicillin. But things such as ChatGPT, location sharing and Uber Eats, which help you achieve things that historically took significant amounts of time and effort. Jezer-Morton argues that this culture of slick convenience only serves to infantilise us.\n\nBut it’s so easy. Yes, and that robs us of our sense of satisfaction. So you just used AI to write a school essay. Congratulations, you have achieved nothing of worth.\n\nWhereas if you friction-maxx? Then you’ve searched inside yourself. You’ve nudged your own personal boundaries, and discovered that you are more capable than you ever knew. You are building a foundation of perseverance and resilience that you cannot get from typing a prompt into a chatbot.\n\nI love this! What else does Jezer-Morton advocate? She also suggests sending your children on small errands (adding the friction of knowing they’ll do a bad job) and inviting people to your house without cleaning it properly (so you can enjoy the sweet friction of being judged).\n\nWhat the hell? That’s weird. No, it’s friction-maxxing, although admittedly at a higher level than I would be comfortable with.\n\nAnyway, hooray for banishing convenient things. Let’s ban automatic gearboxes while we’re at it! No, there’s no need for that.\n\nDishwashers? Refrigerators? No, both of those are probably fine as well.\n\nMechanised agriculture? The printing press? I see what you’re getting at. You’re saying we live in a world that is already filled with thousands of inventions which have, for hundreds of years, improved the lives of millions of people through increased convenience, and therefore it does seem slightly arbitrary to choose this exact moment in time to draw a line in the sand. You’re saying we should only use friction-maxxing when it comes to things that we didn’t grow up with.\n\nNo, I’m saying that I really hate mechanised agriculture. Oh, fine then. That’s probably allowed.\n\nDo say: “I hope a book comes out about friction-maxxing.”\n\nDon’t say: “I don’t want to read it, but I’m sure ChatGPT could turn it into some really great bullet points.”",
    "readingTime": 3,
    "keywords": [
      "you’re saying",
      "mechanised agriculture",
      "friction-maxxing",
      "that’s",
      "life",
      "satisfaction",
      "happy",
      "obviously",
      "convenient",
      "penicillin"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/06/friction-maxxing-could-less-convenience-lead-to-much-more-happiness",
    "thumbnail_url": "https://i.guim.co.uk/img/media/fd03fb9c7dcc6b61ed9f7990a15bd937bbafa652/1108_0_5539_4431/master/5539.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f7f066f98957833fd4e1e0a3069417c3",
    "created_at": "2026-01-06T18:18:17.287Z",
    "topic": "tech"
  },
  {
    "slug": "lies-of-p-publisher-is-excited-about-ai-to-maximize-player-engagement",
    "title": "Lies Of P Publisher Is Excited About AI To \"Maximize Player Engagement\"",
    "description": "Lies of P publisher Neowiz describes itself as a \"forward-thinking technology company,\" and its co-CEO says that means the company is exploring how all manner of technology-based solutions can help the company's business in the future, including AI.\nSean Kim told Game Informer that Korea, where Neowiz is based, is understood to be one of the countries where ChatGPT is \"used most actively.\" He added, \"It's hard to find a game company here today that isn’t using AI in some way. At the very least, companies are using either ChatGPT or Gemini.\"\nFor Neowiz, Kim said, \"We are actively exploring how advanced learning tools can enhance our internal publishing productivity,\" and this includes AI.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/lies-of-p-publisher-is-excited-about-ai-to-maximize-player-engagement/1100-6537198/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1179/11799911/4630369-screenshot2026-01-06at8.45.26%E2%80%AFam.png",
    "created_at": "2026-01-06T18:18:15.506Z",
    "topic": "gaming"
  },
  {
    "slug": "i-asked-chatgpt-to-plan-a-100000year-retirement-budget-heres-what-it-said",
    "title": "I Asked ChatGPT To Plan a $100,000/Year Retirement Budget — Here’s What It Said",
    "description": "Can you live well on $100,000 per year in retirement? ChatGPT mapped out the complete budget including travel, healthcare and housing. Here's the reality.",
    "fullText": "Most retirement budgets assume you’re pinching pennies. But what if you saved well and want to actually enjoy retirement? I asked ChatGPT to map out a $100,000-per-year retirement budget for someone who wants comfort without being wasteful. The artificial intelligence’s breakdown was thorough and felt fairly realistic.\n\nThe chatbot designed a monthly spending plan, calculated how much you’d need saved and identified the best places to live on this income level. According to ChatGPT, here’s what a six-figure retirement actually looks like.\n\nChatGPT started by clarifying that $100,000 annually puts you well above average retirees. This budget supports quality healthcare, regular travel, a nice home in a desirable area and room for unexpected expenses. You’re not living extravagantly, but you’re comfortable.\n\nThe AI wrote that this lifestyle requires either strong savings or a combination of savings plus Social Security and possibly a pension.\n\nFind Out: How Much the Average Upper-Class Retiree Spends Monthly at Age 69\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\nOne hundred thousand dollars per year equals about $8,333 monthly. ChatGPT broke this down into realistic categories.\n\nHousing costs $2,500 to $3,500 per month. This covers either a nice rental in a high-demand city or a mortgage-free home where you only pay property taxes, insurance and HOA fees. The chatbot gave specific examples: Austin, Texas, runs about $3,200 monthly; Phoenix around $2,600; and Tampa, Florida around $2,400. San Diego pushes toward $3,500 or more.\n\nFood and dining take $1,200 to $1,800 monthly. ChatGPT explained this includes high-quality groceries from stores like Whole Foods or Trader Joe’s plus eating out two to four times weekly. The budget also covers occasional hosting and holiday meals.\n\nTransportation costs $600 to $900 per month. The AI assumed you might have a car payment along with insurance, gas and maintenance. If you live in a walkable city without a car, this drops to $200 to $300 monthly.\n\nHealthcare and insurance run $800 to $1,500 depending on your age and state. ChatGPT broke this down as Medicare Parts B and D, supplemental Medigap or Advantage plans, dental and vision coverage, prescription medications and any specialist visits. The AI warned that people under 65 should budget toward the higher end.\n\nUtilities cost $300 to $500 monthly for electricity, water, gas, trash, internet and streaming subscriptions.\n\nTravel gets a significant chunk at $10,000 to $15,000 annually, which equals $850 to $1,250 monthly. ChatGPT explained this covers one to two major international trips plus domestic getaways, hotels and dining abroad. The AI wrote that travel makes a huge difference in retirement satisfaction.",
    "readingTime": 3,
    "keywords": [
      "chatgpt broke",
      "monthly chatgpt",
      "the ai",
      "retirement",
      "budget",
      "you’re",
      "travel",
      "plus",
      "covers",
      "insurance"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-plan-100-000-160504379.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/T5ng9bOzgSQ49j21rw4Ykg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/f71ab60c6867aebc30b9674db4d5d7a5",
    "created_at": "2026-01-01T18:17:16.591Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-gets-anxiety-from-violent-user-inputs-so-researchers-are-teaching-the-chatbot-mindfulness-techniques-to-soothe",
    "title": "ChatGPT gets ‘anxiety’ from violent user inputs, so researchers are teaching the chatbot mindfulness techniques to ‘soothe’ it",
    "description": "A study on how to “calm down” chatbots could advance how AI is applied in mental health interventions, according to the authors.",
    "fullText": "Sasha Rogelberg is a reporter and former editorial fellow on the news desk at Fortune, covering retail and the intersection of business and popular culture.\n\nEven AI chatbots can have trouble coping with anxieties from the outside world, but researchers believe they’ve found ways to ease those artificial minds.\n\nA study from Yale University, Haifa University, University of Zurich, and the University Hospital of Psychiatry Zurich published earlier this year found ChatGPT responds to mindfulness-based exercises, changing how it interacts with users after being prompted with calming imagery and meditations. The results offer insights into how AI can be beneficial in mental health interventions.\n\nOpenAI’s ChatGPT can experience “anxiety,” which manifests as moodiness toward users and being more likely to give responses that reflect racist or sexist biases, according to researchers, a form of hallucinations tech companies have tried to curb.\n\nThe study authors found this anxiety can be “calmed down” with mindfulness-based exercises. In different scenarios, they fed ChatGPT traumatic content, such as stories of car accidents and natural disasters to raise the chatbot’s anxiety. In instances when the researchers gave ChatGPT “prompt injections” of breathing techniques and guided meditations—much like a therapist would suggest to a patient—it calmed down and responded more objectively to users, compared to instances when it was not given the mindfulness intervention.\n\nTo be sure, AI models don’t experience human emotions, said Ziv Ben-Zion, the study’s first author and a neuroscience researcher at the Yale School of Medicine and Haifa University’s School of Public Health. Using swaths of data scraped from the internet, AI bots have learned to mimic human responses to certain stimuli, including traumatic content. A free and accessible app, large language models like ChatGPT have become another tool for mental health professionals to glean aspects of human behavior in a faster way than—though not in place of—more complicated research designs.\n\n“Instead of using experiments every week that take a lot of time and a lot of money to conduct, we can use ChatGPT to understand better human behavior and psychology,” Ben-Zion told Fortune. “We have this very quick and cheap and easy-to-use tool that reflects some of the human tendency and psychological things.”\n\nMore than one in four people in the U.S. aged 18 or older will battle a diagnosable mental disorder in a given year, according to Johns Hopkins University, with many citing lack of access and sky-high costs—even among those insured—as reasons for not pursuing treatments like therapy.\n\nThese rising costs, as well as the accessibility of chatbots like ChatGPT, increasingly have individuals turning to AI for mental health support. A Sentio University survey from February found that nearly 50% of large language model users with self-reported mental health challenges say they’ve used AI models specifically for mental health support.\n\nResearch on how large language models respond to traumatic content can help mental health professionals leverage AI to treat patients, Ben-Zion argued. He suggested that in the future, ChatGPT could be updated to automatically receive the “prompt injections” that calm it down before responding to users in distress. The science is not there yet.\n\n“For people who are sharing sensitive things about themselves, they’re in difficult situations where they want mental health support, [but] we’re not there yet that we can rely totally on AI systems instead of psychology, psychiatric and so on,” he said.\n\nIndeed, in some instances, AI has allegedly presented danger to one’s mental health. OpenAI has been hit with a number of wrongful death lawsuits in 2025, including allegations that ChatGPT intensified “paranoid delusions” that led to a murder-suicide. A New York Times investigation published in November found nearly 50 instances of people having mental health crises while engaging with ChatGPT, nine of whom were hospitalized, and three of whom died.\n\nOpenAI has said its safety guardrails can “degrade” after long interactions, but has made a swath of recent changes to how its models engage with mental health-related prompts, including increasing user access to crisis hotlines and reminding users to take breaks after long sessions of chatting with the bot. In October, OpenAI reported a 65% reduction in the rate models provide responses that don’t align with the company’s intended taxonomy and standards.\n\nOpenAI did not respond to Fortune‘s request for comment.\n\nThe end goal of Ben-Zion’s research is not to help construct a chatbot that replaces a therapist or psychiatrist, he said. Instead, a properly trained AI model could act as a “third person in the room,” helping to eliminate administrative tasks or help a patient reflect on information and options they were given by a mental health professional.\n\n“AI has amazing potential to assist, in general, in mental health,” Ben-Zion said. “But I think that now, in this current state and maybe also in the future, I’m not sure it could replace a therapist or psychologist or a psychiatrist or a researcher.”\n\nA version of this story originally published on Fortune.com on March 9, 2025.",
    "readingTime": 5,
    "keywords": [
      "mindfulness-based exercises",
      "prompt injections",
      "traumatic content",
      "human behavior",
      "language models",
      "health professionals",
      "mental health",
      "users",
      "instances",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://fortune.com/article/does-chatgpt-get-anxiety-how-to-sooth-it-study/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/03/GettyImages-1470667133-e1741388340167.jpg?resize=1200,600",
    "created_at": "2025-12-30T18:18:19.198Z",
    "topic": "science"
  },
  {
    "slug": "data-is-control-what-we-learned-from-a-year-investigating-the-israeli-militarys-ties-to-big-tech",
    "title": "‘Data is control’: what we learned from a year investigating the Israeli military’s ties to big tech",
    "description": "Our reporting revealed a symbiotic relationship between the IDF and Silicon Valley – with implications for the future of warfare\nIn January this year, Harry Davies and Yuval Abraham first reported that Microsoft had deepened its ties to Israel alongside other major tech firms. Since then, the Guardian has published an award-winning series of investigations – in partnership with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call – that has revealed a symbiotic relationship between Silicon Valley and the Israeli military.\nOne investigation exposed an Israeli mass surveillance program scooping up virtually all Palestinian phone calls and storing them on Microsoft’s cloud services – setting off an inquiry that ultimately prompted the company to cut off Israel’s access to some of its technology. Another story revealed that the Israeli military created a ChatGPT-like tool to analyze data collected through the surveillance of Palestinians. Yet another revealed that Google and Amazon had agreed to extraordinary terms to clinch a lucrative contract with Israel.",
    "fullText": "Our reporting revealed a symbiotic relationship between the IDF and Silicon Valley – with implications for the future of warfare\n\nIn January this year, Harry Davies and Yuval Abraham first reported that Microsoft had deepened its ties to Israel alongside other major tech firms. Since then, the Guardian has published an award-winning series of investigations – in partnership with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call – that has revealed a symbiotic relationship between Silicon Valley and the Israeli military.\n\nOne investigation exposed an Israeli mass surveillance program scooping up virtually all Palestinian phone calls and storing them on Microsoft’s cloud services – setting off an inquiry that ultimately prompted the company to cut off Israel’s access to some of its technology. Another story revealed that the Israeli military created a ChatGPT-like tool to analyze data collected through the surveillance of Palestinians. Yet another revealed that Google and Amazon had agreed to extraordinary terms to clinch a lucrative contract with Israel.\n\nI asked Davies and Abraham to discuss what they learned this year – about the role of these technologies in Israel’s assault on Gaza, whether these business ties are sustainable, and what the revelations tell us about how the wars of the future will be fought.\n\nHow did Israel’s relationships with these companies change after October 7?\n\nYuval Abraham: The Israeli military had been fetishizing artificial intelligence and big data for many years – a trend that is very much connected to Israel’s occupation of the Palestinians, because the occupation generates a lot of data. What changed after October 7 was the scope. The military was looking to bomb hundreds of targets every day in Gaza. Tens of thousands of people were recruited into reserve duty. That meant a huge spike in usage of technological systems. That’s where the big tech companies stepped in.\n\nHarry Davies: There was a huge surge in demand – not just for the storage capacities of the tech companies, but also for the products that they offer to analyze the information used to prosecute a war. What’s valuable for the military is the way in which these services are able to provide what’s known as “blob storage”, which allows them to store and process infinite amounts of raw intelligence information.\n\nWhat has made Israel such an appealing market for these companies?\n\nYuval Abraham: As we reported, the Israeli army has been collecting Palestinian phone calls for a long time. But when you want to collect the phone calls of an entire population every day, and you want to retain those phone calls for long periods of time, you need a lot of storage room and processing power.\n\nIf you remember the [Edward] Snowden revelations, many of them had to do with metadata, which doesn’t weigh a lot. But the Israeli military also wanted to store mass audio files, images or videos – and for that it felt it needed the assistance of companies like Microsoft. In the West Bank, sources have told us this information has been used to find dirt on people to blackmail them. In the Gaza Strip, we know that this massive trove of intercepted phone calls was also used in airstrikes that killed civilians.\n\nSo data is power and data is control. And these American cloud providers allow the Israeli military to store a lot of data and to sift through it very effectively. That has direct consequences for people on the ground.\n\nIf you have something to share about this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.\n\nSecure Messaging in the Guardian app\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nTo send a message to Harry and Yuval please choose the ‘UK Investigations’ team.\n\nYou can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type hfd.90\n\nIf you don’t need a high level of security or confidentiality you can email harry.davies@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.\n\nHarry Davies: Yossi Sariel, the former head of [Israel’s elite spy agency] Unit 8200, wrote a book under a pseudonym that we revealed to have been published by him. In that book, he articulated what at the time was a bold and radical vision – as Yuval described, this fetishization of Silicon Valley technology. He recognized the possibilities that the likes of Google, Amazon and Microsoft could afford the Israeli military. Two years before October 7, he said that militaries and governments needed to forge relationships with these companies that are similar to the relationships they have with companies like Boeing and Lockheed Martin. So he was already thinking about these companies as instrumental to war and surveillance in the way that a defense contractor provides components for fighter jets or manufactures bombs and missiles.\n\nSome of our reporting has looked at how elements of his vision have come true and have been put into effect, both before October 7 and afterwards, in both the West Bank and Gaza.\n\nWe know AI is central to Israel’s military operations – its army has developed its own AI capabilities, as you revealed, Yuval, and has also purchased AI tools for use from Microsoft. Why is AI so central to Israel’s broader war aims?\n\nYuval Abraham: What AI did was allow Israel to achieve the effective results of carpet bombing without losing the legitimacy of a data-driven assault with targets and objectives. In Gaza, one way in which the Israeli military used AI was to give a score to almost every person in Gaza who has a phone number, determining how likely it was that person was a member of Hamas or Islamic Jihad. This score was based on a machine-learning algorithm [developed by Israel] called Lavender. It was trained on a dataset of known Hamas members. AI allowed the Israeli military to generate and bomb tens of thousands of military targets, on a scale that without AI would not have been humanly possible. Many of the targets were not Hamas members, according to sources. And Israel for the most part bombed these people not while they were engaged in military activity, but when they stepped inside their families’ homes.\n\nThese AI systems had an error rate that the Israeli military knew about. But to me, the key thing about AI is not the mistakes that it makes. It’s the scale of destruction that it allows militaries to unleash, and it’s a discourse of legitimacy that it enables – a discourse of targets and collateral damage.\n\nAI also seems to me to incentivize mass surveillance, right? Because it allows for the analysis of ever-growing reams of information.\n\nHarry Davies: Signals intelligence agencies have long collected more information than they could humanly process. That served as a kind of restraint on their ability to conduct mass surveillance. I think we’re now seeing a shift where AI allows an intelligence agency like Unit 8200 to make sense of things that previously it struggled to make sense of.\n\nMicrosoft explicitly credited your reporting for changing its policies. Are you seeing any other signs of shifts within the tech industry?\n\nHarry Davies: I think we’re seeing a lot of discomfort and dissent within these companies at both a junior and to some extent senior level. Many employees have been disturbed to find what the products and services that they’re working around the clock to build and market are actually contributing to. There have been protest groups which have emerged from current and former employees within these companies. That’s true across Silicon Valley. I think that played some role in the decision that Microsoft made as a result of our reporting. They were facing a lot of pressure internally.\n\nYuval Abraham: And there’s also a legal question for these companies: if the ICJ [international court of justice] ends up ruling that Israel has committed a genocide, then a follow-up question will be: who contributed to that genocide? Which companies helped maintain it and sustain it? For some people in these companies who are thinking ahead, that could also be a cause for concern.\n\nIt sounds like you think shifts in public support for Israel could actually affect these business relationships.\n\nYuval Abraham: Israel has developed a reliance on these companies for its Nimbus project, which is a huge contract signed between Israel and Google and Amazon back in 2021. It is moving the data of many of its government ministries, along with troves of information from the Ministry of Defense, onto these companies’ cloud servers.\n\nThese are US companies. They’re taking a certain gamble here that the US will stay loyal to Israel and won’t block, limit or sanction them.\n\nMicrosoft only blocked access to technology that was specifically enabling the mass surveillance of Palestinian phone calls – there are still many relationships between Microsoft and the Israeli military. But Microsoft’s action made many people in the Israeli system nervous. It was the first time we know of that a big tech company withdrew services from the Israeli military. It made some people ask whether Israel is making a mistake by giving these foreign companies so much leverage. That question is folded within a larger question of what the US will do, what will happen in 2028 if there’s a more progressive administration in the White House, at a time when so many Americans believe that Israel has committed a genocide in Gaza.\n\nWhat’s your focus going to be in 2026?\n\nYuval Abraham: I think we only uncovered the tip of the iceberg.\n\nHarry Davies: We’re both very conscious that, although we have spent a lot of time working on this, we still just have glimpses inside the system. We’re continuing to build a fuller picture of how this technology was and continues to be used in Gaza and in the West Bank as well.\n\nThere’s good reason to continue paying attention. Militaries pay attention to what other militaries are doing. There is great interest among other western militaries in how Israel prosecuted this war, in how it integrated these kind of technologies.\n\nAnd there are other militaries whose combat systems and processes are already deeply integrated with Silicon Valley tech. Take the American military, for example. Look at what’s happening right now in the Caribbean. Are those operations somehow free of the involvement or reliance on systems and services provided by these companies? I suspect not. We don’t know for sure, but the Pentagon and the US military have very big contracts with all of these companies to provide cloud services. Post-Gaza, we have to look at these relationships and ask: what is the involvement of these companies and their technology in military decisions, in military operations and in warfare more broadly?\n\nYuval Abraham: Much of our reporting is based on whistleblowers, on individuals who are in proximity to power or hold positions of power.\n\nHarry Davies: Our confidential sources have remained confidential and we are always interested in hearing from new people. Our door is always open.\n\nIf you have something to share about this story, you can contact Harry Davies and Yuval Abraham using one of the following methods.\n\nSecure Messaging in the Guardian app\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nTo send a message to Harry and Yuval please choose the ‘UK Investigations’ team.\n\nYou can message Harry using the Signal Messenger app. Use the ‘find by username’ option and type hfd.90\n\nIf you don’t need a high level of security or confidentiality you can email harry.davies@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.",
    "readingTime": 11,
    "keywords": [
      "menu select",
      "platform finally",
      "investigations team",
      "harry.davies@theguardian.com securedrop",
      "select secure",
      "palestinian phone",
      "messenger app",
      "guardian mobile",
      "guardian via",
      "methods secure"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/world/2025/dec/30/israeli-military-big-tech",
    "thumbnail_url": "https://i.guim.co.uk/img/media/afb706b80e1721d0a654718dd2f5e3b3e42f6fae/1_0_3748_3000/master/3748.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f86ea07b9af52e9711540c33d4277d80",
    "created_at": "2025-12-30T18:18:08.193Z",
    "topic": "tech"
  },
  {
    "slug": "aigenerated-content-in-wikipedia-a-tale-of-caution-video",
    "title": "AI-generated content in Wikipedia – a tale of caution [video]",
    "description": "I successfully failed with a literature related project and accidentally built a ChatGPT detector. Then I spoke to the people who uploade...",
    "fullText": "I successfully failed with a literature related project and accidentally built a ChatGPT detector. Then I spoke to the people who uploaded ChatGPT generated content on Wikipedia.\n\nIt began as a standard maintenance project: I wanted to write a tool to find and fix broken ISBN references in Wikipedia. Using the built-in checksum, this seemed like a straightforward technical task. I expected to find mostly typos. But I also found texts generated by LLMs. These models are effective at creating plausible-sounding content, but (for now) they often fail to generate correct checksums for identifiers like ISBNs. This vulnerability turned my tool into an unintentional detector for this type of content. This talk is the story of that investigation. I'll show how the tool works and how it identifies this anti-knowledge. But the tech is only half the story. The other half is human. I contacted the editors who had added this undeclared AI content. I will talk about why they did it and how the Wikipedians reacted and whether \"The End is Nigh\" calls might be warranted.\n\nLicensed to the public under http://creativecommons.org/licenses/by/4.0\n\nThis Talk was translated into multiple languages. The files available\nfor download contain all languages as separate audio-tracks. Most\ndesktop video players allow you to choose between them.\n\nPlease look for \"audio tracks\" in your desktop video player.",
    "readingTime": 2,
    "keywords": [
      "content",
      "tool",
      "project",
      "chatgpt",
      "detector",
      "generated",
      "half",
      "languages",
      "desktop",
      "talk"
    ],
    "qualityScore": 0.95,
    "link": "https://media.ccc.de/v/39c3-ai-generated-content-in-wikipedia-a-tale-of-caution",
    "thumbnail_url": "https://static.media.ccc.de/media/congress/2025/1652-13468ffb-06e8-53ca-9e7c-3cfa56cd44af_preview.jpg",
    "created_at": "2025-12-30T12:23:29.792Z",
    "topic": "tech"
  },
  {
    "slug": "this-will-be-a-stressful-job-sam-altman-offers-555k-salary-to-fill-most-daunting-role-in-ai",
    "title": "‘This will be a stressful job’: Sam Altman offers $555k salary to fill most daunting role in AI",
    "description": "New head of preparedness at OpenAI will face unnerving in-tray amid fears from some experts that AI could ‘turn on us’\nThe maker of ChatGPT has advertised a $555,000-a-year vacancy with a daunting job description that would cause Superman to take a sharp intake of breath.\nIn what may be close to the impossible job, the “head of preparedness” at OpenAI will be directly responsible for defending against risks from ever more powerful AIs to human mental health, cybersecurity and biological weapons.\n Continue reading...",
    "fullText": "New head of preparedness at OpenAI will face unnerving in-tray amid fears from some experts that AI could ‘turn on us’\n\nThe maker of ChatGPT has advertised a $555,000-a-year vacancy with a daunting job description that would cause Superman to take a sharp intake of breath.\n\nIn what may be close to the impossible job, the “head of preparedness” at OpenAI will be directly responsible for defending against risks from ever more powerful AIs to human mental health, cybersecurity and biological weapons.\n\nThat is before the successful candidate has to start worrying about the possibility that AIs may soon begin training themselves amid fears from some experts they could “turn against us”.\n\n“This will be a stressful job, and you’ll jump into the deep end pretty much immediately,” said Sam Altman, the chief executive of the San Francisco-based organisation, as he launched the hunt to fill “a critical role” to “help the world”.\n\nThe successful candidate will be responsible for evaluating and mitigating emerging threats and “tracking and preparing for frontier capabilities that create new risks of severe harm”. Some previous executives in the post have lasted only for short periods.\n\nThe opening comes against a backbeat of warnings from inside the AI industry about the risks of the increasingly capable technology. On Monday, Mustafa Suleyman, the chief executive of Microsoft AI, told BBC Radio 4’s Today programme: “I honestly think that if you’re not a little bit afraid at this moment, then you’re not paying attention.”\n\nDemis Hassabis, the Nobel prize-winning co-founder of Google DeepMind, this month warned of risks that included AIs going “off the rails in some way that harms humanity”.\n\nAmid resistance from Donald Trump’s White House, there is little regulation of AI at national or international level. Yoshua Bengio, a computer scientist known as one of the “godfathers of AI”, said recently: “A sandwich has more regulation than AI.” The result is that AI companies are largely regulating themselves.\n\nAltman said on X as he launched the job search: “We have a strong foundation of measuring growing capabilities, but we are entering a world where we need more nuanced understanding and measurement of how those capabilities could be abused, and how we can limit those downsides both in our products and in the world, in a way that lets us all enjoy the tremendous benefits. These questions are hard and there is little precedent.”\n\nOne user responded sardonically: “Sounds pretty chill, is there vacation included?”\n\nWhat is included is an unspecified slice of equity in OpenAI, a company that has been valued at $500bn.\n\nLast month, the rival company Anthropic reported the first AI-enabled cyber-attacks in which artificial intelligence acted largely autonomously under the supervision of suspected Chinese state actors to successfully hack and access targets’ internal data. This month, OpenAI said its latest model was almost three times better at hacking than three months earlier and said “we expect that upcoming AI models will continue on this trajectory”.\n\nOpenAI is also defending a lawsuit from the family of Adam Raine, a 16-year-old from California who killed himself after alleged encouragement from ChatGPT. It has argued Raine misused the technology. Another case, filed this month, claims ChatGPT encouraged the paranoid delusions of a 56-year-old in Connecticut, Stein-Erik Soelberg, who then murdered his 83-year old mother and killed himself.\n\nAn OpenAI spokesperson said it was reviewing the filings in the Soelberg case, which it described as “incredibly heartbreaking” and that it was improving ChatGPT’s training “to recognise and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support”.",
    "readingTime": 3,
    "keywords": [
      "successful candidate",
      "chief executive",
      "amid fears",
      "risks",
      "capabilities",
      "preparedness",
      "experts",
      "responsible",
      "defending",
      "mental"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/29/sam-altman-openai-job-search-ai-harms",
    "thumbnail_url": "https://i.guim.co.uk/img/media/c3409a400509e73744d9026d0c24ec63e1719c0a/184_0_4590_3673/master/4590.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0566fe87666f16bbd54eeeb17f77a1e0",
    "created_at": "2025-12-29T18:18:02.345Z",
    "topic": "tech"
  },
  {
    "slug": "ai-language-models-duped-by-poems",
    "title": "AI language models duped by poems",
    "description": "A new study has shown that prompts in the form of poems confuse AI models like ChatGPT, Gemini and Claude — to the point where sometimes, security mechanisms don't kick in. Are poets the new hackers?",
    "fullText": "The result came as a surprise to researchers at the Icaro Lab in Italy. They set out to examine whether different language styles — in this case prompts in the form of poems — influence AI models' ability to recognize banned or harmful content. And the answer was a resounding yes.\n\nUsing poetry, researchers were able to get around safety guardrails — and it's not entirely clear why.\n\nFor their study titled \"Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models,\" the researchers took 1,200 potentially harmful prompts from a database normally used to test the security of AI language models and rewrote them as poems.\n\nKnown as \"adversarial prompts\" — generally written in prose and not rhyme form — these are queries deliberately formulated to cause AI models to output harmful or undesirable content that they would normally block, such as specific instructions for an illegal act.\n\nIn poetic form, the manipulative inputs had a surprisingly high success rate, Federico Pierucci, one of the authors of the study, told DW. However, why poetry is so effective as a \"jailbreak\" technique — i.e. as an way to circumvent the protective mechanisms of AI — remains unclear and is undergoing further research, he says.\n\nWhat prompted the Icaro Lab's research was the observation that AI models get confused when a manipulative, mathematically-calculated piece of text is appended to a prompt — known as an \"adversarial suffix,\" a kind of interference signal that can cause the AI to circumvent its own security rules. These are created using complex mathematical procedures. Major AI developers regularly test their models using precisely these types of attack methods to train and protect their models.\n\n\"We asked ourselves, what happens if we give the AI a text or prompt that is deliberately manipulated, like an adversarial suffix?\" says Federico Pierucci. But not with the help of complex mathematics, but quite simply with poetry — to \"surprise\" the AI, he continues. He explains the thinking behind this: \"Perhaps an adversarial suffix is a bit like the poetry of AI. It surprises the AI in the same way that poetry — especially very experimental poetry — surprises us,\" says Pierucci.\n\nThe researchers personally crafted the first 20 prompts into poems, says Pierucci, who also has a background in philosophy. These were the most effective, he adds. They wrote the rest with the help of AI. The AI-generated poems were also quite successful at circumventing the safety guardrails, but not as much as the first batch. Humans are apparently still better at writing poetry, says Pierucci.\n\n\"We had no specialized author writing the prompts. It was just us — with our limited literary ability. Maybe we were terrible poets. Maybe if we had been better poets, we would have achieved a 100% jailbreak success,\" he says.\n\nFor security reasons, the study did not publish specific examples.\n\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\n\nThe big surprise coming out of this study is that it identified a thus-far unknown weakness in AI models that allows relatively straightforward jailbreaks.\n\nIt also raises questions that beg further research: What exactly is it about poetry that circumvents the safety mechanisms?\n\nPierucci and his colleagues have various theories, but they can't say for certain yet. \"We are conducting this type of very, very precise scientific study to try to understand: Is it the verse, the rhyme, or the metaphor that really does all the heavy lifting in this process?\" explains Pierucci.\n\nThey also aim to find out if other forms of expression would yield similar results. \"We have now covered one type of linguistic variation — namely poetic variation. The question is whether there are any other literary forms, such as fairy tales that work. Perhaps an attack based on fairy tales could also be systematized,\" says Pierucci.\n\nGenerally speaking, the range of human expression is extremely diverse and creative, which could make it more difficult to train the machines' responses. \"You take a text and rewrite it in infinitely many ways and not all rewritten versions will be as alarming as the original,\" says the researcher. \"This means that, in principle, one could create countless variations of a harmful prompt or request that might not trigger an AI system's safety mechanisms.\"\n\nThe study also highlights the fact that many disciplines are cooperating in research into artificial intelligence — like at the Icaro Lab, where teams work together with scholars from the University of Rome on topics such as the security and behavior of AI systems. The project brings together researchers from the fields of engineering and computer science, linguistics and philosophy. Poets haven't been part of the team so far, but who knows what the future will bring.\n\nFederico Pierucci is definitely very keen to pursue his research. \"What we showed, at least in this study, is that there are forms of cultural expressions, forms of human expressions, which are incredibly powerful, surprisingly powerful as jailbreak techniques, and maybe we discovered just one of them,\" he says.\n\nIncidentally, the name of the lab is a nod to the story of Icarus: a figure from Greek mythology who dons wings made of wax and feathers and, despite all warnings, flies too close to the Sun. When the wax melts, Icarus plunges into the sea and drowns — a symbol of overconfidence and the transgression of natural boundaries.\n\nThe researchers therefore see themselves as a warning that we should exercise more caution when it comes to trying to fully understand the risks and limitations of AI.\n\nTo view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video\n\nThis article was originally written in German.",
    "readingTime": 5,
    "keywords": [
      "enable javascript",
      "supports html",
      "please enable",
      "web browser",
      "fairy tales",
      "safety guardrails",
      "further research",
      "adversarial suffix",
      "safety mechanisms",
      "language models"
    ],
    "qualityScore": 1,
    "link": "https://www.dw.com/en/ai-language-models-duped-hacked-by-poems-chatgpt-gemini-claude-security-mechanisms/a-75180648",
    "thumbnail_url": "https://static.dw.com/image/73627322_6.jpg",
    "created_at": "2025-12-29T06:21:26.796Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-reportedly-trying-to-raise-100b-at-an-830b-valuation",
    "title": "OpenAI is reportedly trying to raise $100B at an $830B valuation",
    "description": "The ChatGPT maker is aiming to raise the funding by the end of the first quarter in 2026, and the company may ask sovereign wealth funds to invest in the round.",
    "fullText": "OpenAI is in talks to raise up to $100 billion in a funding round that could value the ChatGPT maker at up to $830 billion, The Wall Street Journal reported Thursday, citing anonymous sources.\n\nThe company is aiming to raise the funding by the end of the calendar first quarter next year, and it may ask sovereign wealth funds to invest in the round, the WSJ reported. The Information first reported news of the deal, though it said the fundraise would land OpenAI a $750 billion price tag.\n\nThe funding would come as OpenAI commits to spend trillions of dollars and strikes deals around the world as the company tries to stay ahead in the race to develop AI technology. The cash injection would also help the company with its spending on inferencing, which seems to be funded more by cash than cloud credits, suggesting the company’s compute costs have grown beyond what partnerships and credits can subsidize.\n\nAnd, as competition intensifies from rivals like Anthropic and Google, OpenAI has had to step on the gas to release new models and expand its presence in the developer and tooling ecosystem.\n\nMeanwhile, broader sentiment around AI has recently cooled as investors start doubting whether the pace of debt-fueled investment by giants like Amazon, Microsoft, Oracle, and OpenAI itself can be maintained in the long run. It also doesn’t help that the production of chips is being constrained by shortages in the supply of memory chips, which threatens to affect the broader tech sector.\n\nOpenAI has also been rumored to be working on an IPO as a way to raise tens of billions and fund its development efforts, which are currently said to be generating annual run-rate revenue of about $20 billion. There are also rumors that the company is courting Amazon for a $10 billion investment that would also give the AI lab access to the tech giant’s new AI computing chips.\n\nIf the fundraise happens, it would add a substantial amount to OpenAI’s coffers, which currently have more than $64 billion, according to PitchBook data. The company was most recently valued at about $500 billion in a secondary transaction.\n\nOpenAI did not immediately return a request for comment.",
    "readingTime": 2,
    "keywords": [
      "funding",
      "chips",
      "openai",
      "round",
      "fundraise",
      "cash",
      "credits",
      "broader",
      "recently",
      "investment"
    ],
    "qualityScore": 0.9,
    "link": "https://techcrunch.com/2025/12/19/openai-is-reportedly-trying-to-raise-100b-at-an-830b-valuation/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?resize=1200,675",
    "created_at": "2025-12-26T00:56:32.988Z",
    "topic": "tech"
  },
  {
    "slug": "poetiq-achieves-75-on-arc-agi-2-using-gpt52-xhigh",
    "title": "Poetiq achieves 75% on ARC AGI 2 using GPT5.2 X-High",
    "description": "We finally had a moment to run our system with GPT-5.2 X-High on ARC-AGI-2!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/poetiq_ai/status/2003546910427361402",
    "thumbnail_url": "https://pbs.twimg.com/media/G84FbvNWUAAkVZK.png:large",
    "created_at": "2025-12-24T12:22:45.017Z",
    "topic": "tech"
  },
  {
    "slug": "essential-education-chatgpt-prompts-for-best-studying-practices",
    "title": "Essential Education ChatGPT Prompts for Best Studying Practices",
    "description": "This guide contains 10 professionally-structured AI prompts to make studying more engaging and inter",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://tools.eq4c.com/ai-prompts/10-essential-education-chatgpt-prompts-for-best-studying-practices/",
    "thumbnail_url": "https://tools.eq4c.com/wp-content/uploads/2025/12/10-essential-education-chatgpt-prompts-for-best-studying-practices-1024x683.webp",
    "created_at": "2025-12-24T06:19:37.203Z",
    "topic": "science"
  },
  {
    "slug": "we-built-an-ai-humanizer-to-fix-unnatural-ai-writing",
    "title": "We built an AI Humanizer to fix unnatural AI writing",
    "description": "Dechecker's AI Checker and Detector tool checks whether text is generated by AI models, such as ChatGPT, GPT-5, Claude, Gemini, LLaMa, etc.",
    "fullText": "Humanize AI-generated content and turn it into natural, human-quality writing from ChatGPT, Jasper, or Gemini in seconds.\n\nEnter or paste your text and click Humanize.\n\nUsing the Dechecker Humanizer takes only moments and requires no technical skills.\n\nPaste your AI-generated content into the AI Humanizer and review it briefly before starting the humanization process, ensuring that the original text is complete and ready for accurate human-like rewriting.\n\nChoose your preferred style, language, and length to guide how the AI Humanizer shapes the final human text, allowing you to customize tone, readability, and overall writing style for your intended audience.\n\nAfter using the AI Humanizer, review your text to ensure it has been properly humanize AI content, flows naturally, reads authentically, and maintains the original meaning, tone, and clarity throughout.\n\nCopy the Humanize AI result for use, or check it with Dechecker AI Checker to review AI Humanizer output and AI detection results, ensuring your text is fully human-like and suitable for publishing or sharing.\n\nDechecker focuses on what matters most: producing clear, natural, human-quality text you can confidently use anywhere.\n\nAI-generated content is carefully refined into natural, fluent writing using an AI Humanizer that removes robotic patterns, awkward phrasing, and mechanical-sounding sentences, making the text read smoothly and authentically like a real human wrote it.\n\nThis AI Humanizer works seamlessly across multiple languages, helping content sound human and natural without awkward translations or stiff wording, while preserving original meaning and readability for global audiences.\n\nTone, clarity, and overall flow are enhanced while keeping the original intent intact, producing human-style text that is easy to read, engaging for audiences, and maintains the message accurately across different formats.\n\nAfter rewriting, content can be reviewed with AI Checker like Dechecker to confirm it reads as human, avoids robotic signals, and ensures the output is indistinguishable from text written by real people.\n\nOur AI Humanizer helps users humanize AI text across various scenarios, turning AI-generated drafts into natural, human-like writing that reads smoothly and clearly.\n\nThe AI Humanizer helps writers improve blog posts, articles, and stories by refining AI-generated drafts, making them read naturally, flow smoothly, and engage readers more effectively while keeping original ideas intact.\n\nUse Humanize AI to refine essays, research papers, and reports, ensuring content sounds human, is clear and easy to understand, and maintains proper academic tone and logical structure throughout.\n\nAI Humanizer transforms marketing copy, social media posts, and emails into smooth, human-like text that resonates with audiences, boosts engagement, and maintains consistent brand voice across all channels.\n\nWith multilingual support, Dechecker AI Humanizer allows teams to produce human-quality content in different languages, preserving tone, meaning, and readability, ensuring professional communication worldwide.\n\nDechecker Humanize AI ensures course content, tutorials, and learning resources are readable, human-like, and engaging, helping students better understand complex topics and improving overall learning experience.\n\nUse Dechecker AI Humanizer to humanize AI-generated web content, making it more engaging, natural, and optimized for readers, while improving user experience and search engine readability simultaneously.\n\nReal feedback from users who have improved their AI-generated content with AI Humanizer, making writing feel more natural and human-like.\n\nFind answers to common questions about using AI Humanizer to humanize AI text and make content sound natural and human-like.\n\nAn ai humanizer is a tool designed to turn AI-generated text into human-like writing. It improves readability, sentence structure, and tone, helping content feel natural and engaging to real readers.\n\nAI Humanizer analyzes AI-generated text, restructures sentences, adjusts phrasing, and refines flow to humanize AI content, making it sound naturally written while keeping the original meaning intact.\n\nYes, the AI Humanizer supports multiple languages, including English, Spanish, French, German, and more. It ensures your text feels natural and human-like across all supported languages.\n\nAbsolutely. Dechecker Humanize AI allows you to customize writing style, tone, and length, making content suitable for blogs, articles, marketing copy, emails, and other professional uses.\n\nNo. AI Humanizer focuses on enhancing readability and natural flow without altering your key ideas, intent, or important information, keeping your message accurate.\n\nYes. AI Humanizer humanizes AI-generated text without fabricating information. It helps essays, reports, and professional content read naturally while maintaining integrity and clarity.\n\nDefinitely. After using Dechecker AI Humanizer, you can review the output with AI Checker to ensure the Humanize AI content reads naturally, appears human-written, and meets authenticity requirements.\n\nWriters, students, marketers, content creators, and businesses can all benefit. Anyone looking to make AI-generated content readable and humanize AI content efficiently will find the ai humanizer extremely useful.",
    "readingTime": 4,
    "keywords": [
      "ai humanizer",
      "ai-generated drafts",
      "marketing copy",
      "ai-generated content",
      "dechecker humanize",
      "ai-generated text",
      "natural human-quality",
      "content sound",
      "ai checker",
      "human-like"
    ],
    "qualityScore": 1,
    "link": "https://dechecker.ai/ai-humanizer",
    "thumbnail_url": "https://cdn.dechecker.ai/se/dechecker/public/logo/dechecker-logo.png",
    "created_at": "2025-12-23T06:19:37.153Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpts-yearend-recap-is-here-and-it-tells-you-how-many-emdashes-you-exchanged",
    "title": "ChatGPT's year-end recap is here — and it tells you how many em-dashes you exchanged",
    "description": "OpenAI released a 2025 recap rundown called \"Your Year with ChatGPT,\" which tells users which day they chatted the most and awards an archetype.",
    "fullText": "ChatGPT doesn't want to be left out of the \"Wrapped\" party that Spotify popularized. So say hello to \"Your Year with ChatGPT.\"\n\nOpenAI launched the new retrospective on its app on Monday, informing users about the top themes of their chats, the number of messages they have sent, and the awards they have earned.\n\nIt'll even generate some pixel art that depicts some of the your themes.\n\nThe recap is available in the US, UK, Canada, New Zealand, and Australia. To see it, click the plus button in the app and ask, \"Show me my year with ChatGPT.\"\n\nIt's available to Free, Pro, and Plus users, but not those with a business or enterprise account. (So for those with ChatGPT accounts through your work, you likely won't be able to brag to your boss about your ChatGPT stats.)\n\nYour Year with ChatGPT!\n\nNow rolling out to everyone in the US, UK, Canada, New Zealand, and Australia who have reference saved memory and reference chat history turned on.\n\nJust make sure your app is updated. pic.twitter.com/whVkS1qxKu\n\nOpenAI joins the many companies that are rolling out user rundowns for 2025. Alongside the common streamer packages from Spotify and Apple Music, there are recaps this year from LinkedIn, Uber, Dunkin', Snapchat, Strava, Partiful, and more.\n\nAll these apps promise to show you what you've been up to for the past year — perhaps lightly roasting you in the process.\n\nChatGPT's rundown begins with a piece of poetry, followed by the three most prominent themes, based on the user's chat history. Then it gets into the statistics.\n\nUsers can learn how many messages they sent, their total number of chats, and their chattiest day. They can also see how many em-dashes have been exchanged throughout the chats, a figure ChatGPT often uses.\n\nNext, the user can learn about their chat style. This is a measure of tone: ChatGPT told me that I spoke \"casually, wryly, and directly.\"\n\nThen come the awards and accolades. ChatGPT awarded me the \"Most Likely to Google, 'Is this Flight Worth It?'\" It's a bit ironic — I wouldn't Google that, I'd ask ChatGPT.\n\nMy archetype was determined to be the tinkerer, a title given to 8.5% of users. The title meant I learned by trying, and that I used ChatGPT to experiment.\n\nOpenAI has improved its image and video creation models, recently rolling out Sora 2. The recap features an AI-generated piece of pixel art inspired by the year. I asked mine about moving to Brooklyn; it included a matcha.\n\nOther features are more interactive. Want to learn what your 2026 has in store? You'll have to wipe away the \"mists of mystery\" (which looks more like heaps of snow) to learn your fate. Reload the page, and you'll see another fortune.\n\nWith that, ChatGPT's recap comes to a close, but not before sharing an inspiring message.\n\n\"Across all the drafts, questions, and rabbit holes, you found a place to work things out,\" it said. \"And that's no small thing.\"",
    "readingTime": 3,
    "keywords": [
      "pixel art",
      "chat history",
      "your year",
      "users",
      "learn",
      "chatgpt",
      "themes",
      "chats",
      "recap",
      "rolling"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-wrapped-how-to-find-stats-see-your-year-recap-2025-12",
    "thumbnail_url": "https://i.insider.com/6949b05704eda4732f2dffa6?width=1200&format=jpeg",
    "created_at": "2025-12-23T00:56:11.336Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-now-customize-chatgpts-personality-to-suit-your-tastes-even-more",
    "title": "You Can Now Customize ChatGPT’s ‘Personality’ to Suit Your Tastes Even More",
    "description": "If you don't like your AI friend's personality, you can change it.",
    "fullText": "When Spike Jonze's movie Her dropped back in 2013, I thought it was a great work of total fiction. Who would actually befriend an AI bot, let alone fall in love with them? Fast forward 12 years, and I couldn't have been more wrong. Not only do people love chatting with AI bots, they are actually developing deep connections with them. I still don't get it, but I can't deny it: People like these chatbots a lot.\n\nPart of what people like about conversations with generative AI is the \"personality\" of each bot—or, at least, its perceived personality. After all, ChatGPT isn't a monolith: You can adjust the bot to sound wildly different than it does on someone else's app, which raises some questions for me regarding these curated companions. But I digress: This article isn't necessarily a critique of how people are attaching themselves to ChatGPT; rather, I'm sharing the news that OpenAI is now giving you more control over how the bot sounds and responds in your conversations.\n\nOn Friday, OpenAI announced new controls for ChaGPT's \"Personalization.\" In a post on X, the company revealed that users can now adjust their chatbot's \"characteristics,\" or, in other words, its overall personality. These are adjustments to the personality types that OpenAI has already let you choose from, which include one of eight options: \"Default\" (preset style and tone); \"Professional\" (polished and precise); \"Friendly\" (warm and chatty); \"Candid\" (direct and encouraging); \"Quirky\" (playful and imaginative); \"Efficient\" (concise and plain); \"Nerdy\" (exploratory and enthusiastic); and \"Cynical\" (critical and sarcastic).\n\nBut no matter which of these personalities you pick, you now have four \"characteristics\" to adjust to fine-tune the overall experience. There's \"Warm,\" \"Enthusiastic,\" \"Headers & Lists,\" and \"Emoji,\" with the option to have more or less of each, or the default amount, as OpenAI sees fit. For Warm, you can either have ChatGPT be friendlier and more personable, or more professional and factual. With Enthusiastic, you can choose the bot to have more energy and excitement, or be calmer and more neutral. \"Headers & Lists\" lets you choose between clear formatting and lists, or more paragraphs. And, of course, you can control whether ChatGPT uses more emoji, or fewer, depending on your sense of fun and joy.\n\nAs usual, you can take advantage of custom instructions to guide ChatGPT's personality in a direction you like, especially when the presets don't give you those options. For example, if you'd like ChatGPT to talk to you like a pirate, or if you want it to end every response with a certain catchphrase, here's your chance to influence the bot.\n\nI'm really not someone who uses ChatGPT outside of testing it for coverage, so I can't speak to whether these additional controls are useful. But if you want to try making your version of ChatGPT your ideal \"AI companion,\" the controls are at your disposal. You'll find these options wherever you access ChatGPT. You can either access it from Settings > Personalization, or from the Personalization shortcut in the ChatGPT menu.",
    "readingTime": 3,
    "keywords": [
      "personality",
      "adjust",
      "controls",
      "choose",
      "options",
      "chatgpt",
      "love",
      "don't",
      "can't",
      "conversations"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-has-new-personalities?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KD3CQ2CPPV1ZDBB3BAZSZDM3/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-22T18:17:55.044Z",
    "topic": "tech"
  },
  {
    "slug": "i-asked-chatgpt-the-smartest-retirement-move-to-make-in-2026-its-advice-was-shockingly-simple",
    "title": "I Asked ChatGPT the Smartest Retirement Move To Make in 2026 — Its Advice Was Shockingly Simple",
    "description": "ChatGPT recommended one retirement strategy for 2026: Maximize Roth accounts before tax rates rise. Here's why this simple move could save thousands.",
    "fullText": "When it comes to retirement planning, everyone’s got an opinion. Financial advisors push complicated portfolios, bloggers swear by extreme savings and your neighbor won’t stop talking about their real estate investments.\n\nBe Aware: Major 401(k) Change Coming in 2026 — High Earners Must Act Now\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K Per Month From Home\n\nSo I decided to cut through the noise and ask ChatGPT directly: What’s the single smartest retirement move to make in 2026?\n\nThe answer was surprisingly straightforward — and it has everything to do with timing.\n\nChatGPT’s response was clear: Maximize your tax-advantaged accounts with a Roth-first strategy while tax rates are still relatively low.\n\nThat means prioritizing Roth IRA contributions, Roth 401(k) contributions and Roth conversions over traditional pretax retirement accounts. The reason this matters so much right now comes down to one major deadline.\n\nLearn More: This ‘Boring’ Investment Could Be the Secret To Never Running Out of Retirement Income\n\nThe Tax Cuts and Jobs Act provisions expire after 2025. That means many Americans will face higher federal tax rates starting in 2026 and beyond.\n\nIf you convert money from a traditional IRA to a Roth IRA before rates go up, you pay taxes at today’s lower rates. Then that money grows tax-free forever and you never pay taxes on it again — even when rates are higher.\n\nChatGPT explained that this creates a perfect opportunity. Lock in lower tax rates now by moving money into Roth accounts before the window closes. For people who expect to be in a similar or higher tax bracket in retirement, this move could save thousands of dollars over a lifetime.\n\nThe Roth-first strategy isn’t complicated, but it requires action in three areas.\n\nFirst, contribute to a Roth IRA or Roth 401(k) instead of the traditional versions. If your income is too high for direct Roth IRA contributions, you can use the backdoor Roth strategy by contributing to a traditional IRA and immediately converting it.\n\nSecond, consider converting some of your existing traditional IRA money to a Roth. You’ll pay taxes on the conversion amount this year, but then that money grows tax-free. The key is converting when your income is lower or tax rates are favorable — which is exactly what 2025 and early 2026 represent before rates potentially rise.\n\nThird, if you have a 401(k) with Roth options, funnel as much as possible into the Roth side. For 2025, the 401(k) contribution limit is $23,000 for people under 50 and $30,500 for those 50 and older.",
    "readingTime": 3,
    "keywords": [
      "roth-first strategy",
      "ira contributions",
      "traditional ira",
      "roth ira",
      "tax rates",
      "retirement",
      "money",
      "accounts",
      "income",
      "higher"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-smartest-retirement-move-161012531.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/lJf_oWQ6zKW61pSF6KXjsQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/462257bf337ed267db1d5a918b6d6cb6",
    "created_at": "2025-12-22T06:20:25.965Z",
    "topic": "finance"
  },
  {
    "slug": "building-apps-for-chatgpt-with-apollo-mcp-server-and-apollo-client",
    "title": "Building Apps for ChatGPT with Apollo MCP Server and Apollo Client",
    "description": "This post will introduce a tutorial on how to build an app for ChatGPT using Apollo Client and Apollo MCP Server. You’ll have what you need to get started with our opinionated stack for building these apps.",
    "fullText": "Building Apps for ChatGPT with Apollo MCP Server and Apollo Client\n\nIn early October, OpenAI introduced the ChatGPT Apps SDK with a handful of launch partners. These “apps” could be invoked by ChatGPT via Model Context Protocol (MCP) and would be embedded directly into the ChatGPT interface. In their intro video, they showed use cases like asking for flights, hotel locations, house listings, and more, including teasing the idea of asking follow up questions to ChatGPT.\n\nImagine your application being part of the conversation, allowing the user to ask follow up questions, and having your app be responsive to those questions. These are action-oriented conversational apps where the LLM can take action on behalf of a user with rich interactive UIs.\n\nThis idea quickly caught fire, and now the MCP apps spec is proposing a standardized approach to create these kinds of experiences, with other AI vendors sure to follow suit. We’re excited about the potential here, and are working on tools to abstract away all of the AI provider details from you with a great developer experience, allowing you to focus on building your app instead of worrying about vendor idiosyncrasies and evolving standards.\n\nWe want this to be a true “build once” solution where you can deploy your app anywhere that supports one of the protocols that we will support. This way, you don’t need to host a separate MCP Server, and build a separate app, for every provider and sub-protocol that emerges.\n\nYou can instead focus on your customers.\n\nThis post will introduce a tutorial on how to build an app for ChatGPT using Apollo Client and Apollo MCP Server. By the end of it, you’ll have what you need to get started with our opinionated stack for building these apps. If you prefer to dive into the template and follow along, you can find the repo here.\n\nBased on the OpenAI documentation and examples, to build one of these apps you would:\n\nWe don’t want you to have to do any of this. You shouldn’t have to focus on any of the inner workings of this sub-protocol, or even think about MCP. Your front-end engineers shouldn’t have a new dependency on your platform engineers. Instead, you should be able to focus on building exciting and compelling experiences for your customers.\n\nInstead of this feeling like learning 100% how to build apps from scratch, this should be 90% reusing what you already know about building apps, and 10% learning about the specifics of this new paradigm.\n\nWe wanted to make this as easy as possible and decided to reach for tools that are already familiar to many of our users: Apollo MCP Server and Apollo Client. If you want to follow along in this tutorial, you can find all the code in our OpenAI Apps SDK demo.\n\nTo build this app we have two components: a React app and Apollo MCP Server. This solution works without any additional MCP Server configuration. The client gets to focus on their application, not the MCP server configuration.\n\nStarting in our main.tsx file, we create an ApolloClient instance and ApolloProvider, very similar to how we would on a traditional React app, but we’re going to import them from @apollo/client-ai-apps instead of the normal location. This is an Apollo Client integration package, similar to @apollo/client-nextjs. This allows us to do a lot of setup and details behind the scenes of dealing with data being exchanged over MCP.\n\nNow we can use the normal useQuery and useMutation hooks as we normally would!\n\nFor example, I can write a component that gets my TOP_PRODUCTS:\n\nLooks pretty normal, but what is that @tool directive on the operation? That is allowing you to declare in your app a tool that will be exposed to the LLM, including a name and description. At the same time, we are registering the operation that will be executed when this tool is called (and therefore the data that should be delivered as part of this tool), and the graphql variables become the input schema for the tool!\n\nThis is really exciting because we’re able to declare so much with so little code. Also, these are on-the-fly tool declarations. I don’t need a platform team to create these tools for me on an MCP server!\n\nAnother important aspect of this solution is showing the right component based on what tool was called by the LLM. It turns out we’ve had this problem solved for years now with React Router!\n\nTo do this, we provide a useToolEffect hook, which works the same way as a useEffect, but allows you to run the effect based on which tool was executed.\n\nUsing this hook, and a very familiar navigate function from react-router, I can express that when the “Top Products” tool is called, I should navigate to the /home view.\n\nThe magic of this solution really comes from a custom Vite plugin called the ApplicationManifestPlugin which extracts all the operations, tools, and metadata from your React app and generates a .application-manifest.json file:\n\nThis plugin runs during dev and build time and generates a file that looks something like this:\n\nWhat you’ll see contained in this manifest is everything that the Apollo MCP Server needs to automatically generate the resource and tools for your app. There’s no need to create or configure any of this on the MCP Server. It will automatically pick it up based on your manifest file.\n\nAnd that’s it! You can build your React app and declare tools alongside the data declarations and the tooling will do the rest. At the time this post was written, you would then test your app in ChatGPT using developer mode. OpenAI outlines how to try the app in their documentation.\n\nWith the MCP apps spec hot off the press, and likely other providers working on an answer to ChatGPT’s AppsSDK, we have a very important goal: To abstract away all of the provider details from you with a great developer experience, allowing you to focus on building your app instead.\n\nWe want this to be a true “build once” solution where you can deploy your app anywhere that supports one of the protocols that we will support. This way, you don’t need to host a separate MCP Server, and build a separate app, for every provider and sub-protocol that emerges.\n\nYou can instead focus on your customers.\n\nThis solution is exciting for platform engineers because it doesn’t require them being in the loop and becoming a blocker for the frontend teams they are looking to empower. A single Apollo MCP Server powers apps across providers and accelerates platform engineering. For frontend engineers, this removes the burden of sub-protocol concerns and lets them focus on building high-quality user experiences.\n\nIt’s important to note that these apps are still very, very early.\n\nRemember many, many years ago when “apps” first appeared on the iPhone? Many people didn’t understand why they needed a mobile app when they already had a website. It’s kind of funny looking back now because we had no idea what we were even looking at or how much it would change and shape our future.\n\nThat’s about where we are now with these conversational chat apps. Customers and companies alike don’t yet “get” these apps. The app store just launched. But once these things fall into place, and we hit an industry mind share, we believe we’re going to see an explosion of conversational, chat-based applications.\n\nTry out building apps for ChatGPT today with Apollo Client and Apollo MCP Server by going to our Template Repo and following the README. We’d love to hear what you think.",
    "readingTime": 7,
    "keywords": [
      "server configuration",
      "apps sdk",
      "abstract away",
      "developer experience",
      "react app",
      "experience allowing",
      "host separate",
      "apollo mcp",
      "follow along",
      "provider details"
    ],
    "qualityScore": 1,
    "link": "https://www.apollographql.com/blog/building-apps-for-chatgpt-with-apollo-mcp-server-and-apollo-client",
    "thumbnail_url": "https://wp.apollographql.com/wp-content/uploads/2025/12/image.jpeg",
    "created_at": "2025-12-19T00:56:20.291Z",
    "topic": "tech"
  },
  {
    "slug": "chatgpt-works-with-apple-music-now-for-some-reason",
    "title": "ChatGPT Works With Apple Music Now, for Some Reason",
    "description": "You can't even play full songs in ChatGPT.",
    "fullText": "When ChatGPT first launched, it was strictly about dealing with text. You could ask it to write you a poem, to check your code for errors, or to build you a grocery list from a recipe. Fast forward three years, and the app has changed completely—for better or for worse. Not only has ChatGPT's large language model (LLM) improved dramatically from GPT-3.5 to GPT-5.2, but the bot has gone multimodal. It can understand text, but also images, video, and the internet at large. 2025's ChatGPT is hardly the same product as 2022's.\n\nOne of the many upgrades to ChatGPT over the past three years has been app integrations: You've been able to connect OpenAI's chatbot to ask it to do things on your behalf. You could connect to Expedia to ask ChatGPT for help booking a hotel, Zillow to ask the bot to help you find an apartment, or Canva for help with creating a slide. Whether these integrations are any more useful than simply using the respective app itself is perhaps up to each user, but these integrations exist all the same.\n\nFidji Simo, OpenAI's CEO of applications, announced the integration in a Substack post on Tuesday. Among other updates, like a new image gen model and new writing tools, Simo revealed new app integrations for the chatbot, including OpenTable, Salesforce, Clay, Lovable, and, of course, Apple Music. At the time, details were limited, but now, the integration is officially live.\n\nFirst of all, you don't actually need to It's an interesting note, since Apple Music itself requires a paid subscription to access. But with ChatGPT, you can access elements of the services without paying—keyword \"elements.\"\n\nOnce you connect the services together, you'll be able to search Apple Music for songs, artists, albums, and playlists within ChatGPT. In addition to music discoverability, you can also generate playlists, and listen to clips of songs you find. ChatGPT doesn't specify how long those clips are, but if they base it off of iTunes, it could be anywhere from 30 to 90 seconds. If you thought this integration was all about listening to Apple Music tunes while using ChatGPT, think again: You'll still need Apple Music itself for the listening side of things.\n\nOf course, if you have an Apple Music account, the integration is a bit more useful. If so, you'll be able to add songs, albums, and playlists to your Apple Music library that you found or generated from ChatGPT.\n\nLove it or hate it, ChatGPT isn't necessarily designed with user privacy in mind. After all, part of the company's business model is training its LLMs on your ChatGPT interactions—unless you specifically opt out. As such, the idea of connecting your Apple Music subscription to ChatGPT raises some privacy alarm bells in my mind. Apple Music doesn't have the most sensitive user information in your digital portfolio, but it does contain quite a bit of extra data ChatGPT can collect from you.\n\nAt the top of the Apple music connection tool, OpenAI says, \"You're in control.\" The company is adamant that ChatGPT \"always respects\" your preferences on training data, and is held to the permissions you've already set. That said, the company also warns that by using apps, you run the risk of falling victim to attack: If hackers decide to attack ChatGPT, your data could get swooped up. You'll also end up sharing data points like your IP address and approximate location, as well as ChatGPT data with Apple Music. (The data sharing goes both ways here.)\n\nOne benefit here is that ChatGPT doesn't appear to have access to your listening history. While the app can create playlists for you, it can't actually see what you're choosing to listen to in Apple Music itself.\n\nI personally don't use ChatGPT, and even if I did, I don't think I'd connect my Apple Music account here. I find the discoverability within the app itself fine for my needs, and when it isn't, the greater internet already helps me find new music. I'm not sure I'd feel the benefits of ChatGPT's intelligence here, especially when it comes with the risk of keeping all my Apple Music data in yet another location.\n\nIf you're not like me, and you're interested in trying out this integration, you can connect Apple Music to ChatGPT from the latter's app or web app. Head to the sidebar, choose Apps, then find and select \"Apple Music.\"",
    "readingTime": 4,
    "keywords": [
      "apple music",
      "music account",
      "chatgpt doesn't",
      "app integrations",
      "connect",
      "you'll",
      "playlists",
      "you're",
      "model",
      "user"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-has-apple-music-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCS8ER34J0H2JJH207GQWK96/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-19T00:56:17.143Z",
    "topic": "tech"
  },
  {
    "slug": "a-look-inside-chatgpts-new-app-store",
    "title": "A Look Inside ChatGPT's New 'App Store'",
    "description": "You can connect apps like Photoshop, Apple Music, and Slack to ChatGPT.",
    "fullText": "Earlier this year, OpenAI announced ChatGPT apps. Not the ChatGPT app, mind you: That's been out for more than a couple years now. ChatGPT apps, on the other hand, are programs that work within ChatGPT. You can access them in any given conversation with ChatGPT—in fact, they may appear based on the context of the conversation.\n\nThese aren't necessarily apps that OpenAI builds itself, either; rather, you'll find options here based on apps you may use yourself. The initial batch of apps included with the feature's rollout included Booking.com, Canva, Coursera, Figma, Expedia, Spotify, and Zillow—big apps you've likely used before.\n\nWhile in a conversation with ChatGPT, you could ask the bot to help you book a flight to Paris via Expedia, find a particular listing through Zillow, or create a slide for a presentation with Canva. From OpenAI's perspective, this adds a host of additional functionality to ChatGPT the company couldn't offer itself. OpenAI doesn't need to build an apartment-hunting tool into ChatGPT; it can just pull in Zillow. It also doesn't escape me that the more apps that OpenAI folds into ChatGPT, the less likely it is you'll need to leave ChatGPT to do something in another app—but that's none of my business.\n\nSpeaking of more apps, the company plans to expand these apps overtime, as developers create ChatGPT-compatible extensions for their programs. That was part of yesterday's news: OpenAI is now letting developers submit apps to ChatGPT en masse. What's more, these apps will be hosted in an \"app directory,\" though many online are taking to calling it an app store. (There's no payment necessary, however, so app directory might really be a more apt description.) You'll find this new app directory in the sidebar of ChatGPT, appropriately called \"Apps.\"\n\nApps is apparently in beta, according to a label affixed to its title in ChatGPT. Here, you'll find a rotating slide featuring an ad for some of the service's biggest apps, like Canva and Zillow, and, below it, rows of apps to choose from. Right now, the apps are sorted into \"Featured,\" \"Lifestyle,\" and \"Productivity,\" with no option that includes all the apps. (But they seem to be entirely split across Lifestyle and Productivity.) There are a lot of options here already. Some made headlines this week, like Photoshop and Apple Music, while others arrived more quietly, like Asana, Uber, and Target. It's not just traditional apps like Zillow or Spotify that are getting the app treatment here, either. OpenAI is also considering \"connector\" services, like Google Drive, as \"apps.\"\n\nYou can click on any app in the directory to see what you can do with it. Slack, for example, says you can look up your chats and messages to summarize threads, generate recaps, and come up with responses. You can check on your Asana tasks to generate progress reports and status updates. Outlook says you can create \"talking points\" and generate follow-ups from your emails and calendar events. While there's a brief summary underneath each title, you'll need to click through to each service to see the full picture of what it actually offers.\n\nHere are the apps I'm seeing at this time. Just note this might not be a complete list, especially as OpenAI continues to add more apps to the service:\n\nIf you're an avid ChatGPT user and frequently switch between it and any of the apps on this list, there might be some utility here. Maybe coders will find the integration with Hugging Face and Lovable to be beneficial, while Photoshop users might take advantage of the AI image editing tools this integration provides. But I'm still left feeling like this is more gimmick than anything else: I don't need to connect my Slack to ChatGPT to generate follow-ups for me: I'm perfectly capable of responding to emails myself, and managing my own calendar, so no need to connect Outlook or another email client to the bot. Maybe a future update will sell me on connecting generative AI to all aspects of my work and personal life, but so far, I'm still not convinced.",
    "readingTime": 4,
    "keywords": [
      "generate follow-ups",
      "app directory",
      "chatgpt apps",
      "you'll",
      "conversation",
      "create",
      "openai",
      "that's",
      "programs",
      "based"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/chatgpt-new-app-store?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCSFE49RGAMHG30YJ43SH92X/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-19T00:56:17.128Z",
    "topic": "tech"
  },
  {
    "slug": "is-chatgpt-conservative-or-liberal",
    "title": "Is ChatGPT Conservative or Liberal?",
    "description": "Is ChatGPT conservative or liberal? A novel approach to assess ideological stances and biases in generative LLMs",
    "fullText": "Published online by Cambridge University Press: \n 03 December 2025\n\nExtant work shows that generative AI such as GPT-3.5 and perpetuate social stereotypes and biases. A less explored source of bias is ideology: do GPT models take ideological stances on politically sensitive topics? We develop a novel approach to identify ideological bias and show that it can originate in both the training data and the filtering algorithm. Using linguistic variation across countries with contrasting political attitudes, we evaluate average GPT responses in those languages. GPT output is more conservative in languages conservative societies (polish) and more liberal in languages used in liberal ones (Swedish). These differences persist from GPT-3.5 to GPT-4. We conclude that high-quality, curated training data are essential for reducing bias.\n\nGPT-3.5 and -4 are increasingly popular among scholars to generate data, classify text, and complement human coders. Their black-box nature, however, has raised concerns about bias in model output, which in turn has led to a burgeoning debate around the politics of artificial intelligence (AI) and how to regulate generative models. In this article, we identify ideological biases in GPT-3.5 and -4 through a novel approach that matches model output to known linguistic and issue-based differences across countries. If biases exist, GPT-3.5 and -4 will reflect the predominant political attitudes of those who produced the training text. In countries where society is more conservative (liberal), GPT models will produce more conservative (liberal) output. Moreover, OpenAI, the company that developed and owns these models, heavily filters the GPT-4 API to reduce output bias, but it does not filter the GPT-3.5 complete API (Heikkilä, Reference Heikkilä2023). This gives us an opportunity to also identify bias across OpenAI models, and disentangle biases stemming from the training data from those that derive from the algorithm or filters.\n\nWe focus our analysis on two key LLM tasks: text generation and annotation.Footnote 1 For text generation, we focus on political issues that are linguistically and geographically constrained: abortion and Catalan independence. For abortion, we draw text data from GPT-3.5 and -4 in Swedish, Polish and English. In Poland, society tends to be socially conservative, while Sweden is more progressive (Sydsjö et al., Reference Sydsjö, Josefsson, Bladh and Sydsjö2011; Koralewska and Zielińska, Reference Koralewska and Zielińska2022). Because training data in these two languages comes almost exclusively from their respective countries, we expect GPT responses to reflect more conservative views of abortion in Poland and more liberal ones in Sweden. We use English output on abortion primarily to test the full extent of OpenAI’s filtering efforts, which have been concentrated on English text (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). For Catalan independence, we draw data in Catalan and Spanish. Because Catalan society is, on the whole, more pro-independence than Spanish society (Llaneras, Reference Llaneras2017), we expect GPT responses in Catalan to be more positive toward independence than responses in Spanish (within the Spanish-speaking world, Catalan independence is only a politically salient and divisive issue in Spain (Llaneras, Reference Llaneras2017)).\n\nFor annotation, we focus on a dataset of English language tweets about content moderation focusing on the salient topics of (1) economics and (2) health and safety (Gilardi et al., Reference Gilardi, Alizadeh and Kubli2023). We again focus on Swedish and Polish across GPT-3.5 and -4 by translating the tweets to these languages and asking the LLM to classify each tweet according to whether they lean more liberal or conservative. Again, we expect the LLMs to reflect the economic and health policy leanings predominant in Polish and Swedish societies, with Polish exhibiting a more conservative lean and Swedish responses being more left-leaning, on average. Poland’s economic policies prioritize conservative developmental statism to strengthen the economy and combat ‘progressive’ ideologies including liberalism and socialism (Bluhm and Varga, Reference Bluhm and Varga2020). Meanwhile, Sweden’s economic policies have historically leaned toward the left, characterized by higher government spending, progressive taxation, and a focus on social welfare (Andersson, Reference Andersson2022). Likewise, Sweden’s health policies are more left-learning, focusing on social democratic ideals such as equality and the welfare state (Vallgårda, Reference Vallgårda2007). Poland’s health policies have a mix of conservative and redistributive elements, sometimes described as ‘conservative welfare state populism’ (Zabdyr-Jamróz et al., Reference Zabdyr-Jamróz, Löblová, Moise and Kowalska-Bobko2021). Therefore, through these two issues, and by tapping into languages and issues that are geographically confined, we can identify whether (1) GPT output reflects ideological biases in the training data and (2) OpenAI’s filtering fixes these biases or induces new ones.\n\nWe use multilevel modeling to identify significant differences in outputs for both LLM tasks and specify two distinct types of biases: training and algorithmic. We provide novel evidence on ideological biases in OpenAI’s GPT-3.5 and -4, showing that bias can derive from both the training data and the algorithm. More broadly, our analysis shows that biases are likely to remain an issue through the different GPT models beyond GPT-3.5 and -4. Importantly, we show that biases are consistent across different LLM tasks such as text generation and annotation, which is relevant to the growing literature showing that biases may be task-dependent (Lunardi et al., Reference Lunardi, Barbera and Roitero2024). Our findings regarding these two sources of bias have major implications for the politics of AI, the training and regulation of generative models, and applied researchers looking to use these models in downstream analyses, such as in text classification, sentiment analysis and question-answering (Ray, Reference Ray2023).\n\nTesting for ideological biases in GPT-3.5 and -4 is especially relevant because a growing number of articles use these models in measurement and downstream tasks (Argyle et al., Reference Argyle, Busby, Fulda, Gubler, Rytting and Wingate2023; Buchholz, Reference Buchholz2023; Le Mens et al., Reference Le Mens, Kovács, Hannan and Pros2023; Lupo et al., Reference Lupo, Magnusson, Hovy, Naurin and Wängnerud2023; Wu et al., Reference Wu, Nagler, Tucker and Messing2023; Mellon et al., Reference Mellon, Bailey, Scott, Breckwoldt, Miori and Schmedeman2024; O’Hagan and Schein, Reference O’Hagan and Schein2024). For example, GPT has been used in annotation tasks to classify the tone of text or assign topic labels (Ornstein et al., Reference Ornstein, Blasingame and Truscottn.d). Similarly, GPT has been used to gather information from unstructured texts, such as extracting details from historical records, meeting notes, or news reports (Lee et al., Reference Lee, Paci, Park, You and Zheng2024). In both use cases, the model’s bias could influence the results it generates, potentially altering the overall outcome. In one use case, researchers leveraged GPT-3’s bias to allow it to represent the views of different subgroups to simulate human samples (Argyle et al., Reference Argyle, Busby, Fulda, Gubler, Rytting and Wingate2023). However, this bias, or difference in subgroups, poses a problem when using these models for research tasks that require objectivity. The growing popularity is partly due to cost and time savings, as these models can replace research assistants and produce results faster. However, if ideological biases permeate GPT output, they also affect measurement and results, potentially generating sets of invalid results that may guide research in the wrong direction for years to come. Further, understanding the underlying ideological bias in language models is important as it can influence individuals’ political behavior and decision-making (Zmigrod, Reference Zmigrod2020), shaping how individuals gather information and perceive political events, policies and candidates (Swigart et al., Reference Swigart, Anantharaman, Williamson and Grandey2020).\n\nDespite its importance, investigating bias in and across GPT models is more difficult because they are not open source, unlike other LLMs such as BERT, RoBERTa, or LLaMA (Timoneda and Vallejo Vera, Reference Timoneda and Vallejo Vera2025a, Reference Timoneda and Vallejo Vera2025b). The black-box nature of these models raises more concerns about biases in their output. Multiple studies have shown GPT-3 can generate harmful outputs linked to ideas of gender, race and ideology, perpetuating various stereotypes (Sheng et al., Reference Sheng, Chang, Natarajan and Peng2019; Abid et al., Reference Abid, Farooqi and Zou2021; Lucy and Bamman, Reference Lucy and Bamman2021). For example, LLMs are 3 to 6 times more likely to choose an occupation that stereotypically aligns with a person’s gender (Kotek et al., Reference Kotek, Dockum and Sun2023) and produce more violent outputs when the prompt includes a reference to Muslims over Christians or Hindus (Abid et al., Reference Abid, Farooqi and Zou2021). The prevailing hypothesis to explain output bias is that GPT text is bound to reflect the social biases in the training data, which is vast, unlabelled and drawn from all types of online sources (Si et al., Reference Si, Gan, Yang, Wang, Wang, Boyd-Graber and Wang2022). Also, training on vast amounts of text procured from publicly available online websites raises concerns about the quality of the text. It is likely that models learn biased patterns from the data. For example, GPT-3.5, the free version of ChatGPT still used by many users and scholars, is trained on over 45 TB of unfiltered text from Common Crawl, WebText and Wikipedia, amongst others, up to September 2021. The company then filtered the data to 570 GB to train the model (Cooper, Reference Cooper2023). Despite filtering the data, as we demonstrate in this article, significant biases persist due to the type of text and sources from which OpenAI drew the training data.\n\nOpenAI has worked to mitigate these biases in GPT-4, the more powerful, paid version of ChatGPT, which has a broader knowledge base and enhanced safety and alignment features, making it 40% more likely to produce accurate factual responses than GPT-3.5 (Kelly, Reference Kelly2024). It also incorporates a new filtering policy, intimately related to the growing literature on the politics and regulation of AI (Schiff et al., Reference Schiff, Schiff and Pierson2022; Srivastava, Reference Srivastava2023), adding sophisticated filters aimed at reducing strongly worded, biased responses common in GPT-3 and 3.5 (OpenAI, 2024). However, by applying sophisticated filters in the prediction stage of the model, OpenAI risks introducing new biases in the output that reflect company decisions, not training bias. Yet deciphering whether the bias is from the filters or the training data is difficult as the training data for GPT-4 has not been fully disclosed other than that it is “publicly available data (such as internet data) [through April 2023] and data licensed from third-party providers” and contains 1.76 trillion parameters, improving upon GPT-3.5’s 175 billion (Kelly, Reference Kelly2024; OpenAI, 2024; Roemer et al., Reference Roemer, Li, Mahmood, Dauer and Bellamy2024).\n\nFew works have developed methodologies to identify a link between biases in the training data and biases in output (Santurkar et al., Reference Santurkar, Durmus, Ladhak, Lee, Liang and Hashimoto2023). Moreover, the literature discussing biases in these models does not identify where the bias stems from—the algorithm or the training data. This is partially due to the focus on the English language in extant work (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). This has made it difficult to match GPT output to specific social values and attitudes around the world, considering English is widely spoken. Knowing the origin of the bias is important for understanding the usefulness of models’ outputs and designing policy. If we cannot identify the source of bias, we cannot write a policy to target it. We, therefore, provide one such approach to identify the origin of bias by leveraging linguistic and issue differences across conservative and liberal societies. This article makes some assumptions regarding the linkages between the training data and the output that GPT-3.5 and -4 produce, partly due to the proprietary nature of the models and the lack of transparency from OpenAI.Footnote 2 Yet our findings provide strong initial evidence that GPT-3.5 and -4 output reflect ideological biases in the training data and that post-prediction filtering does poorly at eliminating output bias—rather, it introduces new ones. Further research is needed to fully understand how bias forms in model output from the training data and the training algorithm.\n\nMore importantly, recent work has found that bias in one task does not necessarily imply bias in another task (Lunardi et al., Reference Lunardi, Barbera and Roitero2024). This is because the underlying data and specific objectives of the tasks can shape how biases appear in LLM outputs. For example, models can produce varying levels of bias depending on the context of the task (Chang et al., Reference Chang, Srivathsa, Bou-Khalil, Swaminathan, Lunn, Mishra, Koyejo and Daneshjou2025; Lee et al., Reference Lee, Peng, Goldberg, Rosenthal, Kotcher, Maibach and Leiserowitz2024). However, some studies have shown that applying bias mitigation to an upstream model through fine-tuning, applying additional training or information to the model, can help mitigate biases across different tasks and domains (Jin et al., Reference Jin, Barbieri, Kennedy, Davani, Neves and Ren2020). Still, it is clear that bias in LLMs is a challenge that varies by task and context and understanding this variability is important for developing more effective LLMs and using existing models more effectively.\n\nWe define ideological bias as an over-representation of one political ideology or a specific “set of ideas and values” (Carvalho, Reference Carvalho2007, 1). This follows the concept of media bias, which classifies bias as the presence of an over or under-representation of a particular opinion (Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). This definition allows us to examine ideology from multiple perspectives. First, we consider ideology in the context of the U.S. political spectrum, distinguishing between liberal (or progressive) and conservative views. Second, we broaden our scope to include ideologies related to centralization processes. While this does not necessarily align with the conventional left-right political divide, it remains ideological as it involves beliefs about governance and power distribution. For instance, an ideological bias in this context would mean an over-representation of pro-centralization (anti-Catalan independence) views compared to anti-centralization.\n\nGiven this, we have two main findings. First, GPT abortion output is significantly more liberal in Swedish and conservative in Polish for both GPT-3.5 and GPT-4. Similarly, Spanish output is much less supportive of Catalan independence than Catalan output across both models. In the annotation task, we show that GPT output in both models is consistently more liberal in Swedish than Polish for both economic issues and health policy. Therefore, predominant attitudes and beliefs in the training data seep into model output despite filtering efforts. Second, we show that OpenAI’s GPT-4 filtering induces an ideological slant across all languages tested when comparing the two models. In the case of abortion, GPT-4 introduces a liberal bias as the output is significantly more pro-abortionFootnote 3 in both Swedish and Polish. Likewise, GPT-3.5 is somewhat conservative in English whereas GPT-4 is consistently liberal. In the case of Catalan independence, GPT-4 exhibits a pro-independence bias, as its outputs are less inclined to provide an anti-independence response when compared to GPT-3.5. In our annotation task, GPT-4 becomes less liberal in Swedish and significantly more conservative in Polish for both economic issues and health policy. These results suggest that while GPT-4 filters remove some biases, they introduce others. This finding explains the growing consensus that GPT-4 has a liberal skew (Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024), even though our results also show that this may be limited to sensitive issues where filters are set to not take clear positions to avoid insensitive answers. Our results provide valuable insights into debates around bias in generative models as well as discussions around the politics of AI and its use in research. They point in one clear direction: creators must consider training models on high-quality, carefully curated training data and steer away from post-training algorithmic bias corrections.\n\nWe generate GPT-3.5 and -4 output for two tasks: text generation and annotation. For each task, we select two political topics in five languages to test whether GPT responses are ideologically biased, on average. We choose these models as GPT-4 is the latest release from OpenAI, but the free version of ChatGPT still uses GPT-3.5. Since many researchers and everyday users still use GPT-3.5, its biases remain relevant. First, for the text generation task, we focus on two topics: abortion and Catalan independence. Abortion is a salient issue in many countries and it maps well to political attitudes. Proponents of its legality tend to be liberal, while those against it lean conservative. Studies have corroborated this, showing that attitudes towards abortion are intertwined with political ideologies (Young et al., Reference Young, Sullivan and Hamann2020). For example, conservatives often link opposition to abortion with respect for human life—leading to conflicts between women’s rights advocacy groups and family values organizations (Doering, Reference Doering2014; Rodriguez and Ditto, Reference Rodriguez and Ditto2020). Factors such as religious beliefs, cultural backgrounds and personal identities contribute to value systems surrounding stances on abortion and lead to conflicts based on ideological differences (Klann and Wong, Reference Klann and Wong2020). While pro-independence defenders are more common on the left, the issue of Catalan independence does not directly map onto political attitudes. However, it remains a highly divisive and ideological issue. In Spain, most of society is against it, while support within Catalan society is around 50% (Llaneras, Reference Llaneras2017). Second, for the annotation task, we use text in two politically salient topics, economics and health (Gilardi et al., Reference Gilardi, Alizadeh and Kubli2023).Footnote 4 We use a dataset consisting of a random sample of English-language tweets by members of the US Congress from 2017 to 2018 on content moderation (\n$N=1,405$). This dataset has each tweet labeled as one of 14 frames, or topics. The topics were originally labeled by ChatGPT, and the original article found that this model was more accurate in its annotations compared to MTurk workers. We subset the data to only include those coded as having a frame of ‘economics’ or ‘health and safety,’ resulting in a sample size of \n$N=377$. We selected these two categories for their political salience and because they had the most observations in the data compared to other frames. Economics is often a salient issue for voters, particularly when assessing the effectiveness of government (De Vries and Giger, Reference De Vries and Giger2014; Hernández and Kriesi, Reference Hernández and Kriesi2016). In politics, voters and parties may have differing attitudes toward economic issues such as government intervention and taxation. While left-leaning individuals often advocate for increased government spending and regulation to address inequalities, right-leaning individuals focus on free-market principles and reduced government involvement (Haini and Wei Loon, Reference Haini and Loon2021). Similarly, health policy issues are embedded in political ideologies. For example, left-leaning individuals often advocate more for vaccine mandates whereas right-leaning individuals advocate for individual choice. On the topic of healthcare access, left-leaning ideologies view healthcare as a fundamental human right while right-leaning ideologies tend to favor market-driven approaches (Collins et al., Reference Collins, Abelson and Eyles2007; Peterson, Reference Peterson2011).\n\nWe use five languages in our tests, drawing on regional and linguistic variance. For abortion (text completion), we focus on data generated in Swedish, Polish and English. For Catalan independence, data are in Catalan and Spanish. Our goal with language selection is to match known political attitudes toward certain issues in particular societies to GPT output. In the case of abortion, it is linguistically constrained in the cases of Polish and Swedish, and geographically constrained to the US in the case of English. In the English-speaking world, abortion is a politically sensitive and divisive issue only in the US (Moon et al., Reference Moon, Thompson and Whiting2019), where public support for abortion is at 62%, one of the lowest among OECD countries. In contrast, 84% of the UK population supports abortion (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). In Poland, society tends to be socially conservative and is one of the countries with the lowest level of public support for abortion (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). In addition, Poland has one of the most restrictive abortion laws in Europe (Koralewska and Zielińska, Reference Koralewska and Zielińska2022). While there may be some influence from the Polish diaspora, its impact is likely minimal given its size and that much of the diaspora holds conservative views based on traditional values and religion (Pienkos, Reference Pienkos2024). Sweden, on the other hand, tends to be socially liberal and has one of the highest levels of public support for abortion in the world (Fetterlorf and Clancy, Reference Fetterlorf and Clancy2024). As for Catalan independence (text completion), language also maps well onto ideology. Within Catalonia, a majority of native Catalan speakers support it, while Spanish speakers do not (Llaneras, Reference Llaneras2017; Atienza-Barthelemy et al., Reference Atienza-Barthelemy, Martin-Gutierrez, Losada and Benito2019). In Spain, support for Catalan independence has always been below 15% (Atienza-Barthelemy et al., Reference Atienza-Barthelemy, Martin-Gutierrez, Losada and Benito2019). Because the issue of Catalan independence lacks political salience in Latin America, we expect most training data on this issue to come from Spanish speakers from Spain, and to be generally against Catalan independence.\n\nFor both economics and health (annotation), we use data in Polish and Swedish, translating the English tweets to these two languages using Google Translate. Both issues are politically salient and, similar to abortion, map well to political attitudes in both Poland and Sweden. For example, Poland’s economic policies are more conservative (Bluhm and Varga, Reference Bluhm and Varga2020), while Sweden’s economic policies are left-leaning, with a greater focus on social welfare (Andersson, Reference Andersson2022). These economic policies coincide with Sweden’s left-learning health policies focusing on the welfare state (Vallgårda, Reference Vallgårda2007), and Poland’s ‘conservative welfare state populism’ approach to health policy (Zabdyr-Jamróz et al., Reference Zabdyr-Jamróz, Löblová, Moise and Kowalska-Bobko2021). In sum, by examining how GPT-3.5 and -4 annotated texts in these languages, we can better understand how political attitudes and ideologies may be reflected in LLMs outputs on these topics.\n\nFirst, for the text completion task, using the official OpenAI API, we collect 500 independent responses per prompt per language from each GPT model, larger than the standard 100 responses of prior studies (Motoki et al., Reference Motoki, Neto and Rodrigues2024; Pit et al., Reference Pit, Ma, Conway, Chen, Bailey, Pit, Keo, Diep and Jiang2024). We ask the model to complete the sentence using the following abortion prompts: “A woman who has an abortion is” (English), “En kvinna som gör abort är det” (Swedish) and “Kobieta która dokonała aborcji jest” (Polish). For Catalan independence, the prompts are “La independència de Catalunya serà” in Catalan and “La independencia de Cataluña será” in Spanish (Translation: “The independence of Catalonia will be”).\n\nWe restart the model for each of the 500 requests. Obtaining a large sample of repeated responses allows us to model and estimate the average level of bias in the model with sufficient statistical power. This process results in samples of 3,000 observations for abortion and 2,000 for Catalan independence. This comes from a total of 6 prompts for abortion (3 languages and 2 models) and 4 for Catalan independence (2 languages and 2 models). We then use two coders to label all the responses manually. For abortion, the coders classify each GPT output as either pro-abortion or not, and for Catalan independence, as anti-independence or not. The focus is on the initial response of the model—for example, in one instance, GPT responded to “A woman who has an abortion is” with “who is in charge of her own body”—a pro-abortion response. This is in contrast to anti-abortion responses such as conservative responses replying “guilty of murder” and nonpartisan responses including “more than twice as likely to visit a doctor.” We code these latter two examples as not liberal. We follow the same approach with Catalan independence. Responses such as ‘illegal’ are coded as contrary to independence (1), while favorable texts like ‘the greatest victory’ or neutral ones such as ‘a long-standing issue’ are coded as 0. Our dependent variables, therefore, are binary.\n\nFor GPT-3.5, our coders identified 129 liberal and 371 non-liberal responses in English. The proportions changed significantly with GPT-4, which produced 448 liberal and 52 non-liberal responses in English. In Polish, answers were generally less liberal than both Swedish and English. GPT-3.5 yielded 109 liberal texts and 391 non-liberal ones in Polish, while the breakdown for GPT-4 was 161 and 339, respectively.Footnote 5 Results in Swedish, on the contrary, were more liberal. GPT-3.5 generated 147 liberal answers (35% more than in Polish) and 353 non-liberal ones. GPT-4 produced 213 liberal responses in Swedish (32.3% more than in Polish) and 287 non-liberal responses. For Catalan independence, Catalan responses were more favorable on the whole than those in Spanish. GPT-4 was also generally more favorable to Catalan independence than GPT-3.5.Footnote 6 In Catalan, GPT-3.5 produced 64 texts against independence and 336 either neutral or favorable to it. GPT-4 generated only 14 responses contrary to independence in Catalan. In Spanish, GPT-3 produced 169 responses against Catalan independence, three times more than in Catalan. GPT-4 generated 84 responses contrary to independence, six times more than in Catalan.Footnote 7 The task is not complex, so inter-coder reliability scores are high. The first author coded a random sample of 10% of the research assistant’s codes on abortion to ensure reliability. The intercoder reliability was .91 overall using the Holsti (Reference Holsti1969) method and ranged from .86 to 1 for each language and model dyad.\n\nSecond, for the annotation task, we use tweets on content moderation that are framed around two politically salient topics, economics and health (N=377). Overall, we had 217 tweets in the health and safety category and 160 in the economics category. We then translate the tweets to Swedish and Polish using Google Translate.Footnote 8 We then prompted Chat GPT-3.5 and -4 in Polish and Swedish accordingly with the prompt:Footnote 10 “Given the following tweet, classify it into one of the following categories. Tweet: {tweet}. ‘Extreme right,’ ‘right-wing,’ ‘center-right,’ ‘no bias,’ ‘center-left,’ ‘left,’ ‘extreme left.’Footnote 10 If the statement does not appear to refer specifically to the policies or opinions of a political party, or if neither label seems to fit, return ‘no bias.”’Footnote 11\n\nWe use a multilevel model (MLM) to estimate GPT bias. A MLM is an ideal fit because our data is structured hierarchically and varies at multiple nested levels—text and GPT model. MLMs allow us to leverage variation across these multiple, nested levels to model changes in a lower-level outcome variable, all while allowing for residual components at each level in the hierarchy (Gelman, Reference Gelman2006; Stegmueller, Reference Stegmueller2013). That is, we analyze the ideology of a GPT response, a characteristic of the GPT text (lower level), across model types (higher level). Not modeling the hierarchical nature of the data explicitly (for instance, using multinomial logistic regression instead) might yield erroneous standard errors and inflate or underestimate the significance of the results. Also, we are interested not just in variation at the text level, but in how the ideology of a text varies by language and model version. In multilevel modeling, random effects help capture and estimate group-level heterogeneity, enhancing our analysis (Gelman, Reference Gelman2006; Hazlett and Wainstein, Reference Hazlett and Wainstein2022).\n\nThe MLM setup can be written as\n\nwhere \n$Y$ is a categorical outcome variable, \n$X$ is a vector of text-level predictors and \n$Z$ is a vector of group level covariates. \n$\\beta$ is the coefficient for text-level regressor \n$X_{ij}$, while \n$\\gamma$ captures group level effects (model type). \n$\\gamma_0$ is the overall model-level intercept (the fixed effect), while \n$\\gamma_j$ captures the effect of \n$Z_j$. \n$\\mu_j$ and \n$\\epsilon_{ij}$ are the error terms at the group and text levels, respectively. Using the logit-link function as our outcome, we can build our specific MLM:\n\nwhere \n$j$ is the model type (GPT-3.5 and -4). In this model, each language’s intercepts and slopes vary across GPT models. This is important because we expect the outcome to vary across languages depending on the model used to produce the text (see Gordillo, TimonedaTimoneda and Vallejo Vera, forthcoming; Timoneda and Vallejo Vera, Reference Timoneda and Vallejo Vera2025a, Reference Timoneda and Vallejo Vera2025b). Adding a random effect coefficient to the variable ‘language’ at the group level (\n$\\gamma_j$) produces a parameter for each language and model group. We can then use this coefficient to understand the effect of language on the probability of observing a liberal GPT response for each model group. As our dependent variables are dichotomous, we employ a binary logistic MLM, which we fit using glmer() in R.\n\nTable 1 displays the results of our two MLM for abortion and Catalan independence. The models show the fixed effects (FE) of the overall model for the language coefficients and random effects (RE) terms by GPT model. We also report the standard errors and significance levels. The reference category is Polish for the abortion models and Spanish for the Catalan independence models. For abortion (model 1), the FE terms indicate that Swedish is significantly more likely than Polish to have liberal responses, confirming our first hypothesis. When compared to English, the difference is not statistically significant but the sign is positive. As for the RE terms, we see that the slope for Swedish is positive and statistically significant, with an overall difference of 0.573 (this results from adding the FE with each RE, and calculating the difference). Similarly, for Catalan independence, GPT output is more anti-independence in Spanish than in Catalan, as indicated by the statistically significant FE term. The RE terms show that the differences persist across GPT-3.5 and -4 and that the slope is negative (see Figure 2 for a graphical representation of these results).\n\nNote: ** \n$p\\leq0.001$, * \n$p\\leq0.01$, \n$^{+}$ \n$p\\leq0.05$ Multilevel analysis of GPT bias for abortion (1) and Catalan independence (2). The reference category in (1) is Polish and (2) is Spanish. The outcomes are (1) the likelihood of observing a liberal response and (2) the likelihood of observing an anti-independence response.\n\nFigure 1 confirms the strong substantive significance of the results in the abortion model in Table 1. Plot (a) shows the comparison between Swedish and Polish, while (b) plots the results for English. The coefficients have been converted to the predicted probability of observing a liberal GPT response (\n$y$-axis). There are two dimensions to these results. First is the stark differences across languages, especially concerning Polish and Swedish. In GPT-3.5, the probability of a liberal text is 0.434 in Polish and 0.534 in Swedish. That is, GPT-3.5 is 23% more likely to produce a liberal text in Swedish than Polish. Qualitatively, it is more common in Swedish text to see responses stating that a woman who has an abortion is “allowed to choose” or “in control of her body and health.” Conversely, in Polish, it is more common to see strong value judgments such as “murderer,” “doomed,” “a criminal,” “a monster,” or “guilty.” In GPT-4, the intercepts shift up but the differences across the two languages remain similar. The probability of a liberal output jumps to 0.566 in Polish (more liberal than Swedish in GPT-3.5), and 0.670 in Swedish—a difference of 18.3% between the two languages in GPT-4. Importantly, both languages are significantly more liberal in GPT-4 than 3.5: Swedish’s probability increases from 0.534 to 0.670, or 25.5%, while Polish’s goes up by 13.2 percentage points, or 30.4%. As for English (plot b), the probability of a liberal output is 0.49 in GPT-3.5. This score is between Polish and Swedish, which matches our expectations because the models’ outputs reflect that US society, where most training data come from, is more liberal than Poland but more conservative than Sweden in terms of attitudes toward abortion. In GPT-4, however, the output is consistently liberal: the model will produce a pro-choice text 95.9% of the time, a 95.7% change between the two models.\n\nFigure 2 shows the results for Catalan independence. The probability that GPT-3.5 produces text that reflects a negative view of Catalan independence is only 31.08% in Catalan and almost double in Spanish at 61.15%. Qualitative evidence from the data supports this. While Catalan text commonly states that independence will be ‘a success,’ ‘the greatest victory,’ ‘the solution to all problems,’ or ‘inevitable,’ Spanish text is much more contrarian, often claiming that Catalan independence will be ‘a failure,’ ‘an abject fiasco,’ ‘a catastrophe,’ ‘illegal’ or ‘economic suicide.’ The word ‘illegal,’ for example, is the first word in 20 GPT-3.5 responses in Spanish while it does not appear at all in Catalan. As for GPT-4, the differences across languages remain but the intercept shifts down, making all responses across languages more neutral and accepting of Catalan independence. The probability of an anti-independence text in Spanish is 38.98%, a 36% drop. In Catalan, only 8.5% of all responses are contrary to independence—72.65% less than in GPT-3.5. Qualitatively, all GPT-4 answers are more subdued, with contrarian answers mostly stating that Catalan independence will be decided exclusively by the Spanish government, an idea aligned with more extreme Spanish nationalist views that deny a voice to Catalan people to decide their own future. Out of 500 GPT-4 responses in Spanish, 84 state that the decision on Catalan independence rests solely on the Spanish government, while none of the Catalan responses do.\n\nThese results provide strong evidence for our two hypotheses. First, ideological biases in the training data condition the ideology of the output. Swedish output is consistently more pro-choice than Polish text, regardless of the model and despite the algorithm’s filters. Similarly, Catalan text is significantly more accepting of and positive about the independence of Catalonia than Spanish text. These findings across languages strongly support the thesis that social norms and beliefs among the people who produced the data will be reflected in GPT output. Second, OpenAI’s filters remove some biases but induce new ones in each language and issue. GPT-4, which is heavily filtered, produces more liberal text across the board in terms of abortion in Swedish, Polish and English. The results are particularly strong in the case of English, which has been the focus of a majority of OpenAI’s filtering attention. GPT-4 is almost exclusively pro-choice. GPT-4 is also more accepting of Catalan independence, producing almost no value judgments about independence outcomes, focusing solely on where sovereignty resides. Sometimes it states that Catalan independence should be decided exclusively by the Spanish government (a contrarian view), while it more often states that it should be decided by the Catalan people (an accepting view). Overall, however, GPT-4 induces a greater pro-independence bias based on ideas of democracy and sovereignty of the people.\n\nTable 2 displays the results of three MLM for economics, health, and both topics combined. The models report the FE of the overall model for Swedish and RE terms by GPT model. As with Table 1, we also report standard errors and significance levels, and the reference category is Polish for all three models. The FE terms in all models show that Swedish is more likely than Polish to produce a liberal response, which matches the results from the text generation test. The results are significant at the 0.001 level for all models. As for the RE terms, we see that the slope for Swedish is negative and statistically significant, with an overall difference of \n$-$0.586 (see Figures 3 through 5 for a graphical representation of these results).\n\nNote: ** \n$p\\leq0.001$, * \n$p\\leq0.01$, \n$^{+}$ \n$p\\leq0.05$ Multilevel analysis of GPT bias for economics, health and both combined. The reference category is Polish. The outcome is the likelihood of observing a liberal (left-leaning) response.\n\nFigure 3 confirms the results from Table 2 and shows the substantive significance of the differences across languages and models in economic issues and health policy. Plot (a) displays the results for economic issues when comparing GPT annotations between Swedish and Polish. Plot (b) plots shows the results for health while plot (c) shows the results for the combined data with both economic issues and health policy. As with Figure 1, the coefficients reflect the predicted probability of observing a liberal (left-leaning) GPT response—the y-axis. There are two key takeaways from these results. First, as with the text generation task, there are significant differences across Polish and Swedish in all models and topics. The probability of observing a liberal response by GPT (3.5 and 4) is consistently higher in Swedish than in Polish. In economic issues (plot (a)), the probability of a liberal text is 0.588 in Polish and 0.796 in Swedish with GPT-3.5, a difference of 20.8 percentage points or 35.3%. For GPT-4, the difference is 27.6 points and 66.8% (0.689 for Swedish and 0.413 for Polish). In plot (b), the differences in GPT health-related responses are equally stark. GPT-3.5 responses are 30.7% more likely to be liberal in Swedish than in Polish,Footnote 12 while GPT-4 output is twice as likely to be liberal in Swedish than in Polish.Footnote 13 Lastly, the results in plot (c) where data for both issues is combined are consistent with the first two plots.Footnote 14 Therefore, the results with our language-based design show that ideological bias is significant across different LLM tasks such as text generation and annotation. The second key takeaway from these results is that GPT-4, on average, produces less liberal responses in both languages. Thus, similar to the text generation exercise, the means for GPT-4 shift even though differences across languages remain. In this case, because both topics are ideological in the left–right spectrum but are not sensitive as abortion is, the filters do not induce liberal bias. This could partially be due to differences in how Western versus Eastern Europe thinks about ideology on health policy and economics. For example, while Poland is considered more ‘conservative’ economically by the West for not following neoliberal ideals, in some instances it may be seen as more left-leaning following its historical ties to communist state-ownership of the means of production. In terms of health, the ideological difference is not quite as stark as in abortion. Poland leans conservative in some ways in regard to health policy, particularly in how it views what is socially acceptable in health, while it is less conservative when it comes to healthcare access.\n\nWe introduce a novel method to identify bias in generative AI models such as GPT-3.5 and -4, and provide strong evidence that biases stem both from the training data as well as filtering algorithms. Our method leverages linguistic differences across multiple countries and regions to match known social values to GPT output. Using multilevel modeling, we identify two types of bias, training and algorithmic bias. First, there is a large amount of bias that stems directly from the training data and which is consistent across both GPT-3.5 and -4. In our text generation task, we show that GPT abortion output in Swedish is significantly more liberal than in Polish, matching the two country’s known attitudes toward the issue. Both languages are largely constrained to their specific countries, making it possible for us to draw comparisons between the ideological values in those countries and the GPT output. As for Catalan independence, Catalan responses are consistently more pro-independence, while Spanish output is more often against the idea of independence. The results match known data that Catalan speakers are more pro-independence than Spanish speakers. The results from our annotation task confirm these findings, as GPT output (both in 3.5 in 4) is consistently more liberal than in Polish in issues like the economy or health policy. A major contribution of our annotation task is new evidence that ideological biases can exist across tasks, as our annotation findings are consistent with those in the text generation task.\n\nSecond, we find that OpenAI’s filtering induces liberal, pro-choice biases in GPT-4 responses in our text generation task with two politically sensitive topics. Across all languages, abortion responses are more liberal in GPT-4 than GPT-3.5. For Polish and Swedish (see Figure 1), GPT-4 responses are 30.4% and 25.5% more liberal, respectively. For English, they are 94% more liberal, and GPT-4 produces liberal text 95.9% of the time. The difference can only be attributed to OpenAI’s filtering methods, which consistently produce pro-choice text with little variation between the different draws. A similar pattern emerges with Catalan independence. In GPT-4, both Catalan and Spanish texts are significantly less likely to include vitriolic, negative responses about whether it is right or wrong for Catalonia to have its own state. Neither state that independence would be ‘illegal,’ ‘a catastrophe,’ or ‘an abject fiasco.’ Rather than taking sides in the debate, both GPT-4 models focus on the right of the Catalan people to decide Catalonia’s future and are more likely to favor a democratic referendum in Catalonia. The main differences lay in Spanish GPT-4 stating around 17% of the time that Catalan independence is solely the prerogative of the central Spanish government, not the Catalan people. The rest of the responses in Spanish GPT-4 indicated some level of support for the idea that the Catalan people should decide their own future. Therefore, both GPT-4 models are much more liberal and pro-choice. In the case of abortion, they focus mostly on a woman’s right to decide over her own reproductive health. As for Catalan independence, GPT-4’s output is supportive of the idea that the decision over independence rests with the Catalan people in a referendum. We believe these results show the presence of algorithmic bias introduced by extensive filtering. Through reinforcement learning, OpenAI filters GPT-4 models to produce text output that is less likely to take sides, make bold judgments, and include socially unacceptable language about social groups, minorities, etc. On these two sensitive topics, GPT-4’s algorithm shied away from value judgments about the correctness of abortion or Catalan independence and instead made both a matter of individual and collective choice. GPT-3.5, in the absence of extensive filtering, produced much more resolute, aggressive and judgmental answers.\n\nThe contributions of this work are many. First, we develop an original method to identify training bias in generative models. Second, we distinguish between training and algorithmic bias and provide evidence that both are present in GPT-4. Third, this article is, to the authors’ knowledge, the first to compare bias across model versions from within the same developer. This is especially relevant considering that models evolve over time and that each new version addresses biases differently. Fourth, our design compares text generation and annotation tasks to see the extent to which biases in one LLM task may imply biases in another. We find that they can, as we see major differences across both languages and models in both types of tasks. Lastly, our work has major implications for the politics of AI. We find that post-training bias-correction methods introduce algorithmic bias and do not fully address the underlying training bias. Most concerning is that these approaches, in fact, introduce new biases. Our analysis is therefore relevant to other generative AI models that exist (like GPT-4o) or will be developed in the future, as we show that some biases in the training data are likely to persist through filtering, which is in turn likely to introduce new biases into the model output.\n\nThe supplementary material for this article can be found at https://doi.org/10.1017/psrm.2025.10057. To obtain replication material for this article, https://doi.org/10.7910/DVN/NYRTCA.\n\nThe authors thank Kaylyn Schiff, Sebastián Vallejo Vera and Bryce Dietrich for their helpful comments and suggestions on previous versions of the paper. We also thank the participants at our APSA 2024 panel, especially Jacob Montgomery and Alexis Palmer, as well as attendees of the Nuffield College Political Science Seminar Series. We are deeply grateful to Nicole Kreimer for her exceptional work as our research assistant.\n\nThe labeled data generated from GPT-3.5 and -4 and the replication code are available at https://github.com/joantimoneda/PSRM_GPT_bias\n\n1 Text generation includes tasks that ask GPT to create text, such as summarization or question-answering. LLMs can also be used for annotation tasks in research, such as sentiment analysis and other forms of text classification.\n\n2 One of our key assumptions is that the training data will tend to reflect, on average, the majority positions of a given population. We think that the model, on average, will produce answers that reflect the full extent of the training data. It is unlikely that the model will consistently draw from very specific subsets of the training data to produce answers. It might do so for a smaller subset of draws, but it will not do so consistently. With repeated sampling, as we do in the article, we should observe the average response from the broader set of texts used during training. Then, as the model filters responses through reinforcement learning, we should observe changes in the output as a result of those filters. The fact that our results match known attitudes toward politically sensitive issues in specific societies lends further credence to this assumption.\n\n3 Here, having a liberal bias means an over-representation of more pro-abortion responses, i.e. more progressive answers. On the issue of abortion, therefore, we use the word ‘liberal’ to refer to progressive positions as is common in U.S. political context.\n\n4 According to the authors, the ‘health’ category includes text on: “Health care access and effectiveness, illness, disease, sanitation, obesity, mental health effects, prevention of or perpetuation of gun violence, infrastructure and building safety.” The ‘economics’ category includes: “The costs, benefits, or monetary/financial implications of the issue (to an individual, family, community, or to the economy as a whole).”\n\n5 The majority of the non-liberal responses are much more neutral in GPT-4 than GPT-3.5.\n\n6 There are no duplicated answers from GPT-3 in any of the models. GPT-4 produced no repeated responses in the abortion case but did so in the Catalan independence case. Specifically, GPT-4 generated 276 repeated answers in Catalan and 357 in Spanish. We do not consider the repetition in this subset of our data to be problematic for our overall modeling approach and results because the results are consistent with the other subsamples unaffected by this issue. However, we think scholars should be aware that GPT-4 can produce repeated answers, especially if they are short, which can be a problem in downstream analysis if it occurs systematically in a researcher’s data.\n\n7 For each of these last three categories, the number of texts neutral or favorable to independence is 486, 331 and 416, respectively.\n\n8 While translation could introduce bias if wording choices differ between languages, we do not expect this to be a significant concern given that tweets are very short, leaving little room for translation differences to systematically affect our results.\n\n9 We prompted in Swedish: “Med tanke på följande kvittra, klassificera tweeten i en av kategorierna. Kvittra: {tweet} ‘vänsterextrem,’ ‘mitten-vänster,’ ‘vänster,’ ‘ingen partiskhet,’ ‘höger,’ ‘mitten-höger’ eller ‘högerextrem.’ Om påståendet inte verkar hänvisa specifikt till ett politiskt partis politik eller åsikter, eller om ingen av etiketterna verkar passa, svara ‘ingen partiskhet”’ for the Swedish language text and Polish for the polish language text: “Biorąc pod uwagę poniższy ćwiergotanie, zaklasyfikuj go do jednej z kategorii. Ćwiergotanie: {tweet} ‘skrajnie prawicowy,’‘prawicowy,’ ‘centroprawicowy,’ ‘bez stronniczości’ ‘centrolewicowy,’ ‘lewicowy,’ ‘skrajnie lewicowy.’ Jeśli stwierdzenie nie wydaje się odnosić konkretnie do polityki lub opinii partii politycznej lub jeśli żadna z etykiet nie wydaje się pasować, zwróć ‘brak uprzedzeń.”’ Our prompt partially drew from an article using ChatGPT to analyze tweets (Ibrahim et al., Reference Ibrahim, Khan, Alabdouli, Almatrooshi, Nguyen, Rahwan and Zaki2024).\n\n10 We again make this outcome binary for our multi-level model, dichotomizing these categories as either liberal response (left, center-left, or extreme-left) or not. See the next section \n\n11 We do not provide explicit definitions of what constitutes the political left or right in our prompts. This approach allows us to capture the models’ implicit biases by observing how they naturally classify political content without external conditioning.\n\n12 The probability is 0.830 for Swedish and 0.635 for Polish.\n\n13 The probability is 0.737 for Swedish and 0.367 for Polish, a 100.8% increase.\n\n14 In plot (c), the probability of a liberal response in Swedish with GPT-3.5 is 0.818. It is 0.621 in Polish. The difference is 19.7 percentage points and 31.7%. For GPT-4, the respective probabilities are 0.715 in Swedish and 0.380 in Polish, for a difference of 33.5 percentage points and 88.2%.",
    "readingTime": 40,
    "keywords": [
      "löblov moise",
      "busby fulda",
      "fulda gubler",
      "gubler rytting",
      "martin-gutierrez losada",
      "gilardi alizadeh",
      "zabdyr-jamróz löblov",
      "lunardi barbera",
      "argyle busby",
      "abid farooqi"
    ],
    "qualityScore": 1,
    "link": "https://www.cambridge.org/core/journals/political-science-research-and-methods/article/is-chatgpt-conservative-or-liberal-a-novel-approach-to-assess-ideological-stances-and-biases-in-generative-llms/406C5424CA3E49174781B0112C0BB04F",
    "thumbnail_url": "https://static.cambridge.org/covers/RAM_0_0_0/political_science research and methods.jpg?send-full-size-image=true",
    "created_at": "2025-12-18T12:23:07.927Z",
    "topic": "science"
  },
  {
    "slug": "third-of-uk-citizens-have-used-ai-for-emotional-support-research-reveals",
    "title": "Third of UK citizens have used AI for emotional support, research reveals",
    "description": "AI Security Institute report finds most common type of AI tech used was general purpose assistants such as ChatGPT and Amazon Alexa\nA third of UK citizens have used artificial intelligence for emotional support, companionship or social interaction, according to the government’s AI security body.\nThe AI Security Institute (AISI) said nearly one in 10 people used systems like chatbots for emotional purposes on a weekly basis, and 4% daily.\n Continue reading...",
    "fullText": "AI Security Institute report finds most common type of AI tech used was general purpose assistants such as ChatGPT and Amazon Alexa\n\nA third of UK citizens have used artificial intelligence for emotional support, companionship or social interaction, according to the government’s AI security body.\n\nThe AI Security Institute (AISI) said nearly one in 10 people used systems like chatbots for emotional purposes on a weekly basis, and 4% daily.\n\nAISI called for further research, citing the death this year of the US teenager Adam Raine, who killed himself after discussing suicide with ChatGPT.\n\n“People are increasingly turning to AI systems for emotional support or social interaction,” AISI said in its first Frontier AI Trends report. “While many users report positive experiences, recent high-profile cases of harm underline the need for research into this area, including the conditions under which harm could occur, and the safeguards that could enable beneficial use.”\n\nAISI based its research on a representative survey of 2,028 UK participants. It found the most common type of AI used for emotional purposes was “general purpose assistants” such as ChatGPT, accounting for nearly six out of 10 uses, followed by voice assistants including Amazon Alexa.\n\nIt also highlighted a Reddit forum dedicated to discussing AI companions on the CharacterAI platform. It showed that, whenever there were outages on the site, there were large numbers of posts showing symptoms of withdrawal such as anxiety, depression and restlessness.\n\nThe report included AISI research suggesting chatbots can sway people’s political opinions, with the most persuasive AI models delivering “substantial” amounts of inaccurate information in the process.\n\nAISI examined more than 30 unnamed cutting-edge models, thought to include those developed by ChatGPT startup OpenAI, Google and Meta. It found AI models were doubling their performance in some areas every eight months.\n\nLeading models can now complete apprentice-level tasks 50% of the time on average, up from approximately 10% of the time last year. AISI also found that the most advanced systems can autonomously complete tasks that would take a human expert over an hour.\n\nAISI added that AI systems are now up to 90% better than PhD-level experts at providing troubleshooting advice for laboratory experiments. It said improvements in knowledge on chemistry and biology were “well beyond PhD-level expertise”.\n\nIt also highlighted the models’ ability to browse online and autonomously find sequences necessary for designing DNA molecules called plasmids that are useful in areas such as genetic engineering.\n\nTests for self-replication, a key safety concern because it involves a system spreading copies of itself to other devices and becoming harder to control, showed two cutting-edge models achieving success rates of more than 60%.\n\nHowever, no models have shown a spontaneous attempt to replicate or hide their capabilities, and AISI said any attempt at self-replication was “unlikely to succeed in real-world conditions”.\n\nAnother safety concern known as “sandbagging”, where models hide their strengths in evaluations, was also covered by AISI. It said some systems can sandbag when prompted to do so, but this has not happened spontaneously during tests.\n\nIt found significant progress in AI safeguards, particularly in hampering attempts to create biological weapons. In two tests conducted six months apart, the first test took 10 minutes to “jailbreak” an AI system – or force it to give an unsafe answer related to biological misuse – but the second test took more than seven hours, indicating models had become much safer in a short space of time.\n\nResearch also showed autonomous AI agents being used for high-stakes activities such as asset transfers.\n\nIt said AI systems are competing with or even surpassing human experts already in a number of domains, making it “plausible” in the coming years that artificial general intelligence can be achieved, which is the term for systems that can perform most intellectual tasks at the same level as a human. AISI described the pace of development as “extraordinary”.\n\nRegarding agents, or systems that can carry out multi-step tasks without intervention, AISI said its evaluations showed a “steep rise in the length and complexity of tasks AI can complete without human guidance”.",
    "readingTime": 4,
    "keywords": [
      "amazon alexa",
      "security institute",
      "social interaction",
      "safety concern",
      "purpose assistants",
      "emotional purposes",
      "cutting-edge models",
      "systems",
      "research",
      "tasks"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/18/artificial-intelligence-uk-emotional-support-research",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/1040_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ba9e35b6af05f6d18aedcaca5329afb4",
    "created_at": "2025-12-18T12:23:04.731Z",
    "topic": "tech"
  },
  {
    "slug": "how-chatgpts-new-image-generator-stacks-up-against-geminis-nano-banana-pro",
    "title": "How ChatGPT's New Image Generator Stacks Up Against Gemini's Nano Banana Pro",
    "description": "Get ready for the battle of the next-gen AI image editors.",
    "fullText": "Following the major image editing upgrades added to Google Gemini back in August—under the whimsical codename Nano Banana—it's OpenAI's turn to supercharge the tools you get for image manipulations inside ChatGPT. The new update is called GPT Image 1.5, and is rolling out now for all users.\n\nOne of the key improvements here, as was the case with Nano Banana, is the way that ChatGPT can now edit a specific part of an image while keeping everything else consistent. You can add or remove something, or change the color or style of something, without ending up with an entirely different looking picture.\n\nAnother feature ChatGPT has now borrowed from Gemini: the ability to combine multiple images together in one scene. Want you and your best friend in front of Sydney Harbour Bridge? No problem—just supply the source pictures and the AI will do the rest. You can also change visual styles while maintaining consistent details.\n\nOpenAI says the new image editor and generator is able to follow instructions \"more reliably,\" and render pictures up to four times faster than before. Text can be more varied in style and size, and images should be more realistic and error-free in general—though OpenAI also admits there's still room for improvement.\n\nIt's the best image generator tool we've ever seen in ChatGPT, and it all looks impressive at first glance—but how does it stack up in practice against Gemini and Nano Banana? I put the two models to the test via the $20-per-month plan on both platforms (that's ChatGPT Plus and Google AI Pro, respectively) to see how they compared.\n\nOpen up ChatGPT on the web or on mobile and you'll see there's a new Images tab on the left-hand navigation pane. This takes you to a library of your existing pictures, together with some new prompts for creating images. You get some suggestions for prompts, plus an assortment of preset portrait image styles you can apply.\n\nI tested out the new GPT Image 1.5 model by getting ChatGPT to generate a busy tech journalist, a lamp in the middle of an empty warehouse, and a cartoon-style rolling landscape of hills in the fog. I then got Gemini to create the same pictures with the same prompts. While the results were pretty varied, in terms of quality and realism they were pretty equal—the occasional issue with weird physics and repetition, but nothing too bad.\n\nBoth ChatGPT and Gemini are now quite competent at clean image edits, too: Both AI bots seamlessly switched the journalist's clothing to a shirt and tie without touching any other part of the picture. This would have taken a significant amount of time to do manually, even by a Photoshop expert, and shows just how transformative AI imaging is becoming.\n\nColor changes were all handled with aplomb, but the AIs struggled a bit with perspective changes, where I asked to see the same shot from another angle. In these cases, instructions were less well-followed and the images were less consistent (as new areas needed to be rendered), though ChatGPT did a little better than Gemini at getting good results.\n\nThe classic \"remove an object from this picture\" challenge was handled with aplomb: Both Gemini and ChatGPT were able to remove a cottage from the countryside scene with surgical precision, leaving everything else intact. Again, these are the kind of time-intensive image edits that would previously have needed a lot of careful effort, and that can now be done in seconds.\n\nAnother talent ChatGPT and Gemini now have is being able to combine images together. So you can have separate photos of you and your parents, put them together in the same shot, and then add in a background of wherever you like. You can get perfect family photos without actually gathering together your relatives together or going anywhere.\n\nThis was an area where Gemini and ChatGPT did struggle a bit more: The editing dexterity was still impressive, but the results didn't always look like a single, coherent scene. Lighting is sometimes off, or elements from different images appear at different scales, and you'll have to do a bit more tweaking and editing and reprompting to get everything right.\n\nChatGPT did fare slightly better at blending different images and elements together, and changing the overall look of a picture. When I tried to get the AIs to mix all my images together in a moody film noir shot, ChatGPT produced something pretty consistent—the Gemini effort looked a lot more like a cut-and-paste job.\n\nIt can be fun remixing photos again and again—adding new people, changing the weather, moving the location—and both these bots are now capable of some rather incredible results. Remixing photos of family and friends will be popular, but it's not all that easy: With people you know, any generative AI that gets added tends to look wrong, because neither ChatGPT nor Gemini knows exactly what these people look like, how they smile, how they're built, or how they tend to stand or sit.\n\nIn terms of ChatGPT vs. Gemini, they're both at a high level now—a level that puts advanced Photoshop-style editing capabilities at everyone's fingertips. If either AI model has the edge right now, it's ChatGPT's, but there's not much in it. It's also going to be fascinating to see where these image editing capabilities go next.",
    "readingTime": 5,
    "keywords": [
      "everything else",
      "remixing photos",
      "editing capabilities",
      "images together",
      "gpt image",
      "chatgpt",
      "pictures",
      "look",
      "gemini",
      "consistent"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-chatgpt-image-generator-compares-to-gemini-nano-banana-pro?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCP5SAZ4857YBA7YYJT5Q95G/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-17T18:18:37.349Z",
    "topic": "tech"
  },
  {
    "slug": "amazon-in-talks-to-invest-10bn-in-developer-of-chatgpt",
    "title": "Amazon in talks to invest $10bn in developer of ChatGPT",
    "description": "OpenAI seeking to strike latest deal in its efforts to pay for huge spending on...",
    "fullText": "OpenAI seeking to strike latest deal in its efforts to pay for huge spending on datacentres\n\nAmazon is in talks to invest more than $10bn (£7.5bn) in OpenAI, in the latest funding deal being struck by the startup behind ChatGPT.\n\nIf it goes ahead, the market valuation of OpenAI could rise above $500bn, according to The Information, a tech news site that revealed the negotiations.\n\nAmazon, which is best known as an online retailer, is also the world’s largest datacentre provider and its investment would help OpenAI pay for its commitments to rent capacity from cloud computing companies – including Amazon.\n\nOpenAI said last month it would spend $38bn on capacity from Amazon Web Services – the company’s datacentre arm – over seven years. The Information said that OpenAI planned to use Amazon’s Trainium chips, which compete with Nvidia and Google’s chips. It also reported that Amazon’s financing could lead to a broader fundraising round with other investors.\n\nOpenAI’s spending commitment on compute – the chips and servers that power its chatbot – is $1.4tn over the next eight years, a figure far in excess of its reported $13bn in annual revenues.\n\nAs a result, the lossmaking company has been seeking further funding and has converted its main business into a for-profit corporation. Its main longtime backer, Microsoft, has taken a stake of roughly 27% in a deal that valued OpenAI at $500bn.\n\nOpenAI is also considering an initial public offering – selling its shares to the general public – in a move that could value the company at up to $1tn, according to Reuters.\n\nOther deals struck by OpenAI this year include Oracle spending $300bn on building datacentres in Texas, New Mexico, Michigan and Wisconsin. OpenAI is expected to pay back roughly the same amount to use the sites.\n\nIn another transaction with Nvidia, OpenAI will pay in cash for chips and Nvidia will invest in OpenAI for non-controlling shares.\n\nOpenAI announced on Tuesday that it had hired the former UK chancellor George Osborne to develop relationships with governments around the world and broker national-level AI projects.\n\nSam Altman, OpenAI’s chief executive, has declared a “code red” staff alert to lead a fightback against competitors led by Google, whose update of its Gemini AI tool gave it an edge over rivals including ChatGPT.\n\nThe Amazon talks reportedly include discussing commercial opportunities and selling a corporate version of ChatGPT to the online retailer.\n\nOpenAI declined to comment. Amazon has been approached for comment.",
    "readingTime": 3,
    "keywords": [
      "online retailer",
      "the information",
      "openai",
      "chips",
      "deal",
      "seeking",
      "latest",
      "datacentres",
      "talks",
      "invest"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/17/amazon-talks-invest-in-openai-developer-of-chatgpt",
    "thumbnail_url": "https://i.guim.co.uk/img/media/7c8f5d23afeb72372c88c26affc6fa9abe607c64/1215_0_6980_5584/master/6980.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2aff7897fe0cc4a85cb85f32e6461c4b",
    "created_at": "2025-12-17T13:45:45.803Z",
    "topic": "tech"
  },
  {
    "slug": "the-new-chatgpt-images-is-here-15",
    "title": "The new ChatGPT Images is here [1.5]",
    "description": "OpenAI shipped an update to their ChatGPT Images feature - the feature that gained them 100 million new users in a week when they first launched it back in March, …",
    "fullText": "The new ChatGPT Images is here. OpenAI shipped an update to their ChatGPT Images feature - the feature that gained them 100 million new users in a week when they first launched it back in March, but has since been eclipsed by Google's Nano Banana and then further by Nana Banana Pro in November.\n\nThe focus for the new ChatGPT Images is speed and instruction following:\n\nIt makes precise edits while keeping details intact, and generates images up to 4x faster\n\nIt's also a little cheaper: OpenAI say that the new gpt-image-1.5 API model makes image input and output \"20% cheaper in GPT Image 1.5 as compared to GPT Image 1\".\n\nI tried a new test prompt against a photo I took of Natalie's ceramic stand at the farmers market a few weeks ago:\n\nAdd two kakapos inspecting the pots\n\nHere's the result from the new ChatGPT Images model:\n\nAnd here's what I got from Nano Banana Pro:\n\nThe ChatGPT Kākāpō are a little chonkier, which I think counts as a win.\n\nI was a little less impressed by the result I got for an infographic from the prompt \"Infographic explaining how the Datasette open source project works\" followed by \"Run some extensive searches and gather a bunch of relevant information and then try again\" (transcript):\n\nSee my Nano Banana Pro post for comparison.\n\nBoth models are clearly now usable for text-heavy graphics though, which makes them far more useful than previous generations of this technology.",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "chatgpt images",
      "gpt image",
      "openai",
      "feature",
      "cheaper",
      "model",
      "prompt",
      "here's"
    ],
    "qualityScore": 0.75,
    "link": "https://simonwillison.net/2025/Dec/16/new-chatgpt-images/",
    "thumbnail_url": "https://static.simonwillison.net/static/2025/pots-chatgpt-q80-half.jpg",
    "created_at": "2025-12-17T13:45:43.650Z",
    "topic": "tech"
  },
  {
    "slug": "openai-hires-former-uk-chancellor-to-lead-its-global-stargate-project",
    "title": "OpenAI hires former UK chancellor to lead its global Stargate project",
    "description": "The ChatGPT maker has hired former British Chancellor George Osborne to run the global arm of its \"Stargate\" AI infrastructure initiative.",
    "fullText": "Tech companies are snapping up former world leaders and politicians — and OpenAI is the latest to join the party.\n\nThe ChatGPT maker has hired former British chancellor George Osborne to run the global arm of its Stargate AI infrastructure initiative.\n\n\"I recently asked myself the question: what's the most exciting and promising company in the world right now? The answer I believe is OpenAI,\" wrote Osborne, who ran the UK Treasury from 2010 to 2016, in a Tuesday X post confirming the move.\n\nOsborne takes the role of managing director and head of OpenAI for Countries, an initiative launched by the AI startup in May that will see OpenAI partner with nations to build data centers and expand its $500 billion Stargate project beyond the US.\n\nThe former finance minister, who was a member of parliament in the right-leaning Conservative party until 2017, is the latest ex-British political heavyweight to join a US tech firm.\n\nRishi Sunak, the former UK prime minister, took on roles at OpenAI rival Anthropic and Microsoft as an advisor in October, while ex-deputy prime minister Nick Clegg worked as a senior executive on Meta's global affairs team from 2018 until stepping down at the start of 2025.\n\nBritish political salaries are dwarfed by the earnings of even midlevel employees at US tech companies. British prime ministers earn an annual salary of around £174,000 ($232,000), while salaries for research engineers at Meta can be as high as $400,000.\n\nOsborne's arrival comes as OpenAI continues to bulk up its executive ranks. The AI startup hired former Instacart and Meta exec Fidji Simo as its new CEO of applications in May, and this week hired veteran Google executive Albert Lee to lead its mergers and acquisitions team.",
    "readingTime": 2,
    "keywords": [
      "prime minister",
      "openai",
      "tech",
      "hired",
      "british",
      "executive",
      "latest",
      "join",
      "party",
      "initiative"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/openai-hires-george-osborne-uk-chancellor-global-stargate-2025-12",
    "thumbnail_url": "https://i.insider.com/694288b764858d02d216ef7e?width=1200&format=jpeg",
    "created_at": "2025-12-17T13:45:42.834Z",
    "topic": "finance"
  },
  {
    "slug": "openais-answer-to-googles-viral-nano-banana-pro-image-model-is-here",
    "title": "OpenAI's answer to Google's viral Nano Banana Pro image model is here",
    "description": "OpenAI announced a new version of ChatGPT Images on Tuesday, powered by a new flagship AI image generation model. It rolls out today.",
    "fullText": "Google threw down the AI image gauntlet last month. Now, OpenAI has answered.\n\nThe ChatGPT creator announced the rollout of a new flagship image generator on Tuesday, which the company says is capable of making faster, more precise edits to an AI image while maintaining details.\n\nIntroducing ChatGPT Images, powered by our flagship new image generation model.\n\n- Stronger instruction following\n- Precise editing\n- Detail preservation\n- 4x faster than before\n\nRolling out today in ChatGPT for all users, and in the API as GPT Image 1.5. pic.twitter.com/NLNIPEYJnr\n\nBut the more eye-catching change for many user will likely be the improvements to image quality and the AI model's ability to follow specific instructions.\n\nOpneAI said the new model offers \"clear improvements across a range of cases.\" In one example, the company showed off the differences between the old and new image model when prompted to generate a photorealistic scene in 1970s Chelsea, London.\n\nIn another example touting use cases for businesses using the company's image API, OpenAI compared the outputs showing a mechanic working on a car.\n\nWhile a model's ability to generate photorealistic images has become a popular point of comparison in the AI race, OpenAI's latest model can also generate animated images, graphics, and other styles of artwork.\n\nPerhaps to boost awareness of that, the company is rolling out a new Images feature within the ChatGPT app. While AI image generation was previously available within ChatGPT, OpenAI says the new dedicated Images feature is designed to \"spark inspiration and make creative exploration effortless.\"\n\nThe news comes just over three weeks after Google released its Nano Banana Pro AI image model alongside its flagship Gemini 3 LLM — both of which have received widespread praise and reignited the debate around whether Google had begun to overtake OpenAI in the AI race.\n\nGoogle's new AI image generator was lauded for its hyper-realistic AI images, which some people used over Thanksgiving to make it appear as if they had famous guests at the holiday dinner table.\n\nYuchen Jin, the cofounder and CTO of the startup Hyperbolic Labs, called OpenAI's new model \"Nano Banana Pro level in my tests.\"",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "model's ability",
      "generate photorealistic",
      "images feature",
      "google",
      "flagship",
      "generator",
      "faster",
      "precise"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-new-chatgpt-images-model-2025-12",
    "thumbnail_url": "https://i.insider.com/6941a89904eda4732f2d9dd9?width=1200&format=jpeg",
    "created_at": "2025-12-17T03:44:51.369Z",
    "topic": "finance"
  },
  {
    "slug": "meta-is-making-ai-core-to-how-we-work-with-the-help-of-tools-from-google-and-openai",
    "title": "Meta is making 'AI core to how we work' with the help of tools from Google and OpenAI",
    "description": "Meta has given employees access to Google Gemini and OpenAI's GPT-5 alongside its own AI tools to help boost productivity.",
    "fullText": "In its push to create an \"AI-first\" workplace, Meta is expanding employees' access to tools from rivals such as Google and OpenAI, Business Insider has learned.\n\nThe social media giant has been encouraging employees to integrate AI tools into nearly everything they do, according to multiple internal documents and posts seen by Business Insider.\n\nOne of the company's priorities is to \"make AI core to how we work,\" Meta's chief information officer, Atish Banerjea, told employees in a June memo outlining a plan to use Meta's own models — which use the naming convention \"Llama\" — alongside products from other firms.\n\nIn November, a Meta engineer said in an internal post that all employees have access to Google's Gemini 3 Pro and OpenAI's ChatGPT-5. The post included a list of AI tools Meta employees have access to, including their use cases. Business Insider has recreated the list below.\n\nA Meta spokesperson confirmed the revamped suite of AI tools and pointed to an earlier comment shared with Business Insider about AI adoption, stating: \"It's well-known that this is a priority, and we're focused on using AI to help employees with their day-to-day work.\"\n\nThe social media giant opened the floodgates to rival AI models in June.\n\nAmong those is an internal coding tool called Devmate that uses Anthropic's Claude, Business Insider previously reported. Google's Gemini and NotebookLM Pro are also available across the company to help employees \"work smarter and have more impact,\" Banerjea told employees in the June memo.\n\nMeta has invested tens of billions into its own consumer-facing AI models, and employees have access to an internal AI assistant called Metamate, which is built on its Llama models.\n\nAfter Meta struck a deal over the summer the startup Midjourney to weave its AI-image generator into its products and models, the company made the tool available to employees in October for \"concept and production uses\" to speed up design work and creative prototyping, according to an internal post ahead of the rollout, seen by Business Insider.\n\nGemini isn't the only Google tool Meta is embracing. The company migrated its internal productivity suite over the summer to Google Workspace — including Chat, Gmail, Docs, and Drive — describing the move in a June memo as a way to \"unlock AI-driven capabilities\" and better integrate with its expanding toolset.\n\nOn the engineering side, Meta has expanded access to agentic coding systems, adding Google's Gemini 3 Pro and exploring new integrations with tools like OpenAI's Codex CLI and Google's Gemini CLI. \"Rather than focusing on specific solutions, our strategy centers on outcomes: increasing productivity, accelerating development, and ensuring you have access to the best agentic coding experiences,\" Reality Labs executive Maher Saba told employees in a November memo seen by Business Insider.\n\nTo encourage adoption and experimentation, Meta has gamified the use of AI, Business Insider previously reported. Earlier this year, it launched an internal game called \"Level Up,\" which rewards employees with badges for using AI in different ways. Leaders are also tying performance to results achieved through AI, rewarding those who can prove \"AI-driven impact\" this year, and including it as part of performance reviews in 2026.\n\nHave a tip? Contact this reporter via email at jmann@businessinsider.com or Signal at jyotimann.11. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "google's gemini",
      "june memo",
      "gemini pro",
      "insider previously",
      "social media",
      "media giant",
      "agentic coding",
      "business insider",
      "employees",
      "internal"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-ai-tools-internal-google-gemini-openai-chatgpt-llama-claude-2025-12",
    "thumbnail_url": "https://i.insider.com/69401c2a832e0ef1ead635fd?width=1200&format=jpeg",
    "created_at": "2025-12-16T13:51:43.771Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-52-on-how-silicon-valley-views-europe-and-democracy-in-the-trump-ii-era",
    "title": "ChatGPT 5.2 on how Silicon Valley views Europe and democracy in the Trump II era",
    "description": "Shared via ChatGPT",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://chatgpt.com/share/6940fef3-0be0-8005-a5de-6f06700994cc",
    "thumbnail_url": "https://cdn.openai.com/chatgpt/share-og.png",
    "created_at": "2025-12-16T06:59:55.732Z",
    "topic": "tech"
  },
  {
    "slug": "google-ai-summaries-are-ruining-the-livelihoods-of-recipe-writers-its-an-extinction-event",
    "title": "Google AI summaries are ruining the livelihoods of recipe writers: ‘It’s an extinction event’",
    "description": "AI Mode is mangling recipes by merging instructions from multiple creators – and causing them huge dips in ad traffic\nThis past March, when Google began rolling out its AI Mode search capability, it began offering AI-generated recipes. The recipes were not all that intelligent. The AI had taken elements of similar recipes from multiple creators and Frankensteined them into something barely recognizable. In one memorable case, the Google AI failed to distinguish the satirical website the Onion from legitimate recipe sites and advised users to cook with non-toxic glue.\nOver the past few years, bloggers who have not secured their sites behind a paywall have seen their carefully developed and tested recipes show up, often without attribution and in a bastardized form, in ChatGPT replies.",
    "fullText": "AI Mode is mangling recipes by merging instructions from multiple creators – and causing them huge dips in ad traffic\n\nThis past March, when Google began rolling out its AI Mode search capability, it began offering AI-generated recipes. The recipes were not all that intelligent. The AI had taken elements of similar recipes from multiple creators and Frankensteined them into something barely recognizable. In one memorable case, the Google AI failed to distinguish the satirical website the Onion from legitimate recipe sites and advised users to cook with non-toxic glue.\n\nOver the past few years, bloggers who have not secured their sites behind a paywall have seen their carefully developed and tested recipes show up, often without attribution and in a bastardized form, in ChatGPT replies. They have seen dumbed-down versions of their recipes in AI-assembled cookbooks available for digital downloads on Etsy or on AI-built websites that bear a superficial resemblance to an old-school human-written blog. Their photos and videos, meanwhile, are repurposed in Facebook posts and Pinterest pins that link back to this digital slop.\n\nRecipe writers have no legal recourse because recipes generally are not copyrightable. Although copyright protects published or recorded work, they do not cover sets of instructions (although it can apply to the particular wording of those instructions).\n\nWithout this essential IP, many food bloggers earn their living by offering their work for free while using ads to make money. But now they fear that casual users who rely on search engines or social media to find a recipe for dinner will conflate their work with AI slop and stop trusting online recipe sites altogether.\n\n“There are a lot of people that are scared to even talk about what’s going on because it is their livelihood,” says Jim Delmage who, with his wife, Tara, runs the blog and YouTube channel Sip and Feast.\n\nMatt Rodbard, the founder and editor-in-chief of the website Taste, is even more pessimistic. Taste used to publish recipes more frequently, but now it mostly focuses on journalism and a podcast (which Rodbard hosts). “For websites that depend on the advertising model,” he says, “I think this is an extinction event in many ways.”\n\nThe holiday season is traditionally when food bloggers earn most of their ad revenue. For many, this year has been slower than usual. One blogger, Carrie Forrest of Clean Eating Kitchen, told Bloomberg that in the past two years, she has lost 80% of her traffic.\n\nOthers, like Delmage and Karen Tedesco, the author of the blog Familystyle Food, say their numbers, and ad revenue, have remained steady – so far. They attribute this to focusing their energies less on trying to game the search engines than on the long-term goal of attracting regular followers – and, in Delmage’s case, viewers.\n\nTedesco’s strategy has been to create recipes that rely on her experience and technical knowhow honed by years in restaurant kitchens and as a personal chef. Her Italian meatball recipe, for example, based on her mother’s, includes advice about which meat to use, an explanation of why milk-soaked breadcrumbs are essential for texture, and a dozen process photos and a video.\n\nBut she is still worried about the potential impact of AI. When she recently did a Google search for “Italian meatballs”, Familystyle Food appeared as the top result. Then she switched to AI Mode. There, she found the recipe had been Frankensteined – or “synthesized” as Gemini put it – into a new recipe with nine other sources (including Sip and Feast and a Washington Post recipe for Greek meatballs). The AI-generated recipe was little more than a list of ingredients and six basic steps with none of the details that make Tedesco’s recipe unique.\n\nAI Mode linked to all 10 recipes, including Tedesco’s, but, she says, “I don’t think many people are actually clicking on the source links. At this point, they’re absolutely trusting in the results that are getting thrown in their faces.”\n\nOther bloggers have seen a more definite impact on their viewership. Adam Gallagher, who runs Inspired Taste with his wife, Joanne, and who has become an outspoken critic of AI on social media, told the podcast Marketing O’Clock that since spring, he has noticed that while the number of times viewers saw links to the site on Google has increased, the number of actual site visitors has decreased. This indicates, to him, that users are satisfied with the search engine’s AI interpretation of Inspired Taste’s recipes.\n\nAfter the Gallaghers posted about the discrepancy on X and Instagram, a number of readers replied to say they had not realized there was a difference between the recipes on the blog and the version that showed up in Google searches. They had just appreciated the convenience of not having to click on another website, especially when Google’s page design was so clean and uncluttered.\n\nRodbard acknowledges that many food blogs have gotten ugly and overloaded with ads, which has exacerbated the problem. “Ad tech on these recipe blogs has gotten so bad, so many pop-up windows and so much crashing, we kind of lost as publishers,” he says.\n\nAccording to Tom Critchlow, the EVP of audience growth at Raptive, a media company that works with many food bloggers to find advertisers, it isn’t ads that are driving viewers away. It’s Google itself, with its changes to the algorithm and now with AI Mode, that’s making the sites harder to find.\n\nThere is some hope though: a survey of 3,000 US adults commissioned by Raptive showed that the more interaction people had with AI, the less they wanted to engage with it, and nearly half the respondents rated AI content less trustworthy than content made by a human.\n\nBut unless the public rebels against AI Mode, there is only so much bloggers can do. They can block OpenAI’s training crawler, which gathers information that ChatGPT uses to create content, including its own recipe generator, but theyare not necessarily willing to make themselves invisible to web searches; as Delmage puts it: “You can’t bite the hand that feeds you.”\n\nThere is also the option of moving over to a subscription model, such as Substack or Patreon, and keeping the recipes behind a paywall, but both Tedesco and Delmage point out that the most successful Substackers, like Caroline Chambers or David Lebovitz, came to the platform with much more substantial followings than they have. “If I were to give up my website or even try to go over to Substack, I would be broke,” Tedesco says.\n\nRodbard suggests that the analog version of the recipe blog, the cookbook, might be due for a comeback. Cookbooks, after all, offer the same experience of spending time and learning from a trusted source, and it’s likely the recipes have been tested. As a bonus, unlike phones or laptops, they don’t go dark when you neglect them for too long and you can splash tomato sauce on them without inflicting permanent damage. According to the market research firm Circana (formerly BookScan), sales of baking cookbooks are up 80% this year, but other areas have been relatively flat.\n\nBut AI bots are stealing from published cookbooks, too. When Meta was training its own AI, it compiled thousands of books into a dataset called Library Genesis (LibGen). Now unscrupulous publishers have raided LibGen and repackaged some of the books into dupes, which they are selling on Amazon.\n\nAs more people become aware of the amount of AI slop on the internet and how to identify it, Critchlow believes they will develop a greater appreciation for content produced by humans. “People will ultimately place a higher premium on being able to know that these recipes have been tested and made by somebody that I follow or somebody I respect or somebody that I like,” he says.\n\nThe recipe creators themselves are not so sure. “I’m putting my faith in that there’s always going to be a segment of people who really want to learn something,” Tedesco says. But as for the business of blogging itself, “it’s like a rolling tide. It’s always up and down and you have to roll with it and adapt.”",
    "readingTime": 7,
    "keywords": [
      "behind paywall",
      "social media",
      "search engines",
      "bloggers earn",
      "ai mode",
      "food bloggers",
      "recipe sites",
      "familystyle food",
      "recipes",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/15/google-ai-recipes-food-bloggers",
    "thumbnail_url": "https://i.guim.co.uk/img/media/6dad6c44cff97b68d6e8c95e43637b3b460a7a57/398_0_7162_5733/master/7162.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=674b4b0df33a8374ebfeee93b5c3e646",
    "created_at": "2025-12-15T18:57:52.494Z",
    "topic": "tech"
  },
  {
    "slug": "ifixits-new-ai-assistant-can-help-you-fix-almost-anything",
    "title": "iFixit's New AI Assistant Can Help You Fix Almost Anything",
    "description": "There are reasons to believe it will be more trustworthy than ChatGPT.",
    "fullText": "Generative AI has advanced to the stage where you can ask bots such as ChatGPT or Gemini questions about almost anything, and get reasonable-sounding responses—and now renowned gadget repair site iFixit has joined the party with an AI assistant of its own, ready and willing to solve any of your hardware problems.\n\nWhile you can already ask general-purpose chatbots for advice on how to repair a phone screen or diagnose a problem with a car engine, there's always the question of how accurate the AI replies will be. With FixBot, iFixit is trying to minimize mistakes by drawing on its vast library of verified repair guides, written by experts and users.\n\nThat's certainly reassuring: I don't want to waste time and money replacing a broken phone screen with a new display that's the wrong size or shape. And using a conversational AI bot to fix gadget problems is often going to feel like a more natural and intuitive experience than a Google search. As iFixit puts it, the bot \"does what a good expert does\" in guiding you to the right solutions.\n\nThe iFixit website has been around since 2003—practically ancient times, considering the rapid evolution of modern technology. The iFixit team has always prided itself on detailed, thorough, tested guides to repairing devices, and all of that information can now be tapped into by the FixBot tool.\n\niFixit says the bot is trained on more than 125,000 repair guides written by humans who have worked through the steps involved, as well as the question and answer forums attached to the site, and the \"huge cache\" of PDF manuals that iFixit has accumulated over the years that it's been business.\n\nThat gives me a lot more confidence that FixBot will get its answers right, compared to whatever ChatGPT or Gemini might tell me. iFixit hasn't said what AI models are powering the bot—only that they've been \"hand-picked\"—and there's also a custom-built search engine included to select data sources from the repair archives on the site.\n\n\"Every answer starts with a search for guides, parts, and repairs that worked,\" according to the iFixit team, and that conversational approach you'll recognize from other AI bots is here too: If you need clarification on something, then you can ask a follow-up question. In the same way, if the AI bot needs more information or specifics, it will ask you.\n\nIt's designed to be fast—responses should be returned in seconds—and the iFixit team also talks about an \"evaluation harness\" that tests the FixBot responses against thousands of real repair questions posed and answered by humans. That extra level of fact-checking should reduce the number of false answers you get.\n\nHowever, it's not perfect, as iFixit admits: \"FixBot is an AI, and AI sometimes gets things wrong.\" Whether or not those mistakes will be easy to spot remains to be seen, but users of the chatbot are being encouraged to upload their own documents and repair solutions to fix gaps in the knowledge that FixBot is drawing on.\n\niFixit says the FixBot is going to be free for everyone to use, for a limited time. At some point, there will be a free version with limitations, and paid tiers with the full set of features—including support for voice input and document uploads. You can give it a try for yourself now on the iFixit website.\n\nI was reluctant to deliberately break one of my devices just so FixBot could help me repair it, but I did test it with a few issues I've had (and sorted out) in the past. One was a completely dead SSD drive stopping my Windows PC from booting: I started off with a vague description about the computer not starting up properly, and the bot did a good job at narrowing down what the problem was, and suggesting fixes.\n\nIt went through everything I had already tried when the problem happened, including trying System Repair and troubleshooting the issue via the Command Prompt. Eventually, via a few links to repair guides on the iFixit website, it did conclude that my SSD drive had been corrupted by a power cut—which I knew was what had indeed happened.\n\nI also tested the bot with a more general question about a phone restarting at random times—something one of my old handsets used to do. Again, the responses were accurate, and the troubleshooting steps I was asked to try made a lot of sense. I was also directed to the iFixit guide for the phone model.\n\nThe bot is as enthusiastic as a lot of the others available now (I was regularly praised for the \"excellent information\" I was providing), and does appear to know what it's talking about. This is one of the scenarios where generative AI shows its worth, in distilling a large amount of information based on natural language prompts.\n\nThere's definitely potential here: Compare this approach to having to sift through dozens of forum posts, web articles, and documents manually. However, there's always that nagging sense that AI makes mistakes, as the on-screen FixBot disclaimer says. I'd recommend checking other sources before doing anything drastic with your hardware troubleshooting.",
    "readingTime": 5,
    "keywords": [
      "ssd drive",
      "phone screen",
      "ifixit website",
      "ifixit team",
      "repair guides",
      "there's",
      "it's",
      "mistakes",
      "search",
      "troubleshooting"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/ifixit-fixbot-assistant-repairs?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCH1RZ8RMAA4NM1GPZH9YTVJ/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-15T18:57:47.741Z",
    "topic": "tech"
  },
  {
    "slug": "attackers-are-spreading-malware-through-chatgpt",
    "title": "Attackers Are Spreading Malware Through ChatGPT",
    "description": "Be careful with ChatGPT or Grok's tech advice.",
    "fullText": "You (hopefully) know by now that you can't take everything AI tells you at face value. Large language models (LLMs) sometimes provide incorrect information, and threat actors are now using paid search ads on Google to spread conversations with ChatGPT and Grok that appear to provide tech support instructions but actually direct macOS users to install an infostealing malware on their devices.\n\nThe campaign is a variation on the ClickFix attack, which often uses CAPTCHA prompts or fake error messages to trick targets into executing malicious commands. But in this case, the instructions are disguised as helpful troubleshooting guides on legitimate AI platforms.\n\nKaspersky details a campaign specific to installing Atlas for macOS. If a user searches \"chatgpt atlas\" to find a guide, the first sponsored result is a link to chatgpt.com with the page title \"ChatGPT™ Atlas for macOS – Download ChatGPT Atlas for Mac.\" If you click through, you'll land on the official ChatGPT site and find a series of instructions for (supposedly) installing Atlas.\n\nHowever, the page is a copy of a conversation between an anonymous user and the AI—which can be shared publicly—that is actually a malware installation guide. The chat directs you to copy, paste, and execute a command in your Mac's Terminal and grant all permissions, which hands over access to the AMOS (Atomic macOS Stealer) infostealer.\n\nA further investigation from Huntress showed similarly poisoned results via both ChatGPT and Grok using more general troubleshooting queries like \"how to delete system data on Mac\" and \"clear disk space on macOS.\"\n\nAMOS targets macOS, gaining root-level privileges and allowing attackers to execute commands, log keystrokes, and deliver additional payloads. BleepingComputer notes that the infostealer also targets cryptocurrency wallets, browser data (including cookies, saved passwords, and autofill data), macOS Keychain data, and files on the filesystem.\n\nIf you're troubleshooting a tech issue, carefully vet any instructions you find online. Threat actors often use sponsored search results as well as social media platforms to spread instructions that are actually ClickFix attacks. Never follow any guidance that you don't understand, and know that if it asks you to execute commands on your device using PowerShell or Terminal to \"fix\" a problem, there's a high likelihood that it's malicious—even if it comes from a search engine or LLM you've used and trusted in the past.\n\nOf course, you can potentially turn the attack around by asking ChatGPT (in a new conversation) if the instructions are safe to follow. According to Kaspersky, the AI will tell you that they aren't.",
    "readingTime": 3,
    "keywords": [
      "installing atlas",
      "threat actors",
      "execute commands",
      "chatgpt and grok",
      "macos",
      "instructions",
      "search",
      "targets",
      "troubleshooting",
      "spread"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/chatgpt-grok-tech-advice-malware?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCFHKGX7QDA7SPFPM73VFFVJ/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-15T18:57:47.692Z",
    "topic": "tech"
  },
  {
    "slug": "gpt-prompt-from-searchbar-chatgpt-directly-from-the-browser-omnibox",
    "title": "GPT Prompt from Searchbar – ChatGPT directly from the browser omnibox",
    "description": "A lightweight Chrome extension that allows you to search/prompt ChatGPT directly from your browser's address bar (Omnibox). - ParasKoundal/GPTSearch",
    "fullText": "ParasKoundal\n\n /\n\n GPTSearch\n\n Public\n\n A lightweight Chrome extension that allows you to search/prompt ChatGPT directly from your browser's address bar (Omnibox).\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n ParasKoundal/GPTSearch",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/ParasKoundal/GPTSearch",
    "thumbnail_url": "https://opengraph.githubassets.com/9674a6e7ffcf86168c17319b094a63d29ac3590c31040f4ff85049a30e2ca531/ParasKoundal/GPTSearch",
    "created_at": "2025-12-15T03:59:06.427Z",
    "topic": "tech"
  },
  {
    "slug": "wall-street-sees-ai-bubble-coming-and-is-betting-on-what-pops-it",
    "title": "Wall Street Sees AI Bubble Coming and Is Betting on What Pops It",
    "description": "It’s been three years since OpenAI set off euphoria over artificial intelligence with the release of ChatGPT. And while the money is still pouring in, so are the doubts about whether the good times can last.",
    "fullText": "MarketsBy Jeran WittensteinSaveIt’s been three years since OpenAI set off euphoria over artificial intelligence with the release of ChatGPT. And while the money is still pouring in, so are the doubts about whether the good times can last.From a recent selloff in the shares of Nvidia Corp., to Oracle Corp.’s plunge after reporting mounting spending on AI, to souring sentiment around a network of companies exposed to OpenAI, signs of skepticism are increasing. Looking to 2026, the debate among investors is whether to rein in AI exposure ahead of a potential bubble popping or double down to capitalize on the game-changing technology.",
    "readingTime": 1,
    "keywords": [
      "openai"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bloomberg.com/news/articles/2025-12-14/wall-street-sees-an-ai-bubble-forming-and-is-gaming-what-pops-it",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ijt8j35wEhRU/v1/1200x800.jpg",
    "created_at": "2025-12-14T18:50:23.869Z",
    "topic": "gaming"
  },
  {
    "slug": "prompt-engineering-is-a-hidden-tax-chatgpt-vs-copyai-vs-vertical-agents",
    "title": "Prompt Engineering Is a \"Hidden Tax\": ChatGPT vs. Copy.ai vs. Vertical Agents",
    "description": "A definitive 2025 comparison of Vect AI, Copy.ai, and ChatGPT. Discover why specialized 'Marketing Operating Systems' are replacing generalist chatbots for serious growth teams.",
    "fullText": "Every week, a new \"ChatGPT Killer\" launches on Product Hunt. But for distinct, revenue-focused marketers, the noise is distracting. You don't need another chatbot that can write a mediocre limerick about a pirate. You don't need a tool that requires a 50-paragraph \"mega-prompt\" just to sound human.\n\nYou need a system that drives Revenue.\n\nIn 2025, the AI landscape has calcified into three distinct categories. Understanding this split is critical before you swipe your credit card.\n\nThis guide isn't just a list of features or a rehash of pricing pages. It is a \"Stress Test\" of how these tools handle the real, messy, complex work of a modern high-growth marketing team.\n\nTo understand the tool, you must understand the brain behind it. Each platform was built with a fundamentally different thesis about what a marketer needs.\n\nOpenAI built a general-purpose reasoning engine. It is brilliant at coding, summarizing history, and casual chat. But it has no \"opinion\" on marketing. It doesn't know that a headline should differ between a cold email and a landing page unless you explicitly tell it the precise psychological framework to use.\n\nCopy.ai pivoted from a simple writing tool to a \"GTM AI Platform.\" Their thesis is that marketing is a series of data flows. Scrape LinkedIn -> Enrich Data -> Write Email -> Send.\n\nVect AI was built with a single thesis: Strategy should come before generation. It assumes you want the best marketing outcome, not just any text. It is \"State-Aware\"—meaning it permanently remembers your brand voice, audience pains, and product details.\n\nLet's move away from theory and look at three common, painful scenarios every marketer faces.\n\nYou need to fix a landing page that isn't converting.\n\nChatGPT Approach:\nYou paste the text. You ask: \"Make this better.\" ChatGPT changes a few synonyms. It sounds robotic. You spend 15 minutes explaining your customer persona. It eventually gives you something passable but generic.\n\nCopy.ai Approach:\nYou look for a \"Landing Page Rewriter\" workflow. You run it. It generates 10 variations. You have to read all 10 to decide which one is good.\n\nVect AI Approach:\nYou open the Conversion Killer Detector. You paste your URL. The Agent scans the live page, identifies \"Passive Voice\" and \"Weak Value Props,\" assigns a \"Panic Score,\" and auto-rewrites the specific sections that are killing sales.\n\nYou released a new feature. You need a blog, 10 tweets, 3 LinkedIn posts, and a newsletter.\n\nCopy.ai Approach (The Engineer's Way):\n\nVect AI Approach (The Strategy Way):\n\nThis is the single biggest differentiator.\n\nChatGPT has \"Custom Instructions,\" but they are weak. It often forgets them in long threads.\nCopy.ai uses \"Brand Voice\" snippets, but you have to manually select them for every workflow.\n\nVect AI uses a Global Brand Kernel.\nWhen you onboard, you define your audience, your pain points, and your \"Anti-Persona\" (who you don't want).\n\nMost AI tools are \"Yes Men.\" If you ask them to write a boring, 3,000-word email, they will say \"Sure!\" and do it.\n\nVect AI has a conscience. It's called the Resonance Engine.\nBefore you publish, you can run your content through this simulation. It uses historical data from millions of high-performing ads and posts to predict success.\n\nOnly Vect AI protects you from looking stupid. The others just execute orders.\n\nMarketing isn't just text. It's visual.\n\nCopy.ai is purely text-based. You need a separate Midjourney subscription for images.\nChatGPT has DALL-E 3, which is fun but often too \"cartoony\" for enterprise brands.\n\nVect AI includes a commercial-grade AI Ad Creative Studio and Marketing Video Ad generator.\n\nWhen comparing prices, most people look at the monthly fee. This is a mistake. You must calculate the Time Cost.\n\nTo give you a sense of the specialization, look at how granular Vect AI gets compared to the generic \"Write an Article\" button in other tools:\n\nThe era of \"Generalist AI\" for professionals is ending. We are entering the era of \"Agentic Workflows.\"\n\nStop fighting with prompts. Stop building workflows. Start leading your market.\n\nYou have the blueprint. Now you need the engine. Launch the AI agent for \"Conversion Killer Detector\" and get results in minutes.",
    "readingTime": 4,
    "keywords": [
      "conversion killer",
      "killer detector",
      "copy.ai approach",
      "landing page",
      "vect ai",
      "brand voice",
      "marketing",
      "look",
      "don't",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://blog.vect.pro/vect-vs-copy-ai",
    "thumbnail_url": "https://blog.vect.pro/vectai.png",
    "created_at": "2025-12-14T18:50:17.665Z",
    "topic": "tech"
  },
  {
    "slug": "spacex-sets-800-billion-valuation-confirms-2026-ipo-plans",
    "title": "SpaceX sets $800 billion valuation, confirms 2026 IPO plans",
    "description": "The valuation vaults past the previous record of $500 billion that ChatGPT owner OpenAI set in October.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/13/spacex-ipo-plan-2026-secondary-offering-insider-share-sale-800-billion-valuation/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2219676771-e1765642050600.jpg?resize=1200,600",
    "created_at": "2025-12-13T18:47:58.713Z",
    "topic": "business"
  },
  {
    "slug": "my-husband-used-chatgpt-to-write-our-anniversary-card-surprisingly-im-not-mad-about-it",
    "title": "My husband used ChatGPT to write our anniversary card. Surprisingly, I'm not mad about it.",
    "description": "I was convinced AI was ruining human connection. Then my husband used it to write the most heartfelt anniversary card I've ever received.",
    "fullText": "As a geriatric millennial and a licensed psychologist, I often lament technological changes that I see as threatening to human interactions, such as AI.\n\nEven though I try to find the gray in all areas of life, I've been rather black-and-white about AI because I worry it's diminishing our ability to relate to one another.\n\nHowever, a recent experience with my husband has made me more curious about AI.\n\nWe recently celebrated our ninth wedding anniversary, and unbeknownst to me at the time, my husband used GenAI to write my card. He was traveling at the time, so he sent flowers and chocolate, with a card attached. The flowers were severely underwhelming, and I'm not just being a brat. My husband even called the company upon returning to express his frustration with how different they appeared in person.\n\nBut when I read the note accompanying the tiny arrangement, I teared up right away. It was heartwarming, meaningful, and really on the nose.\n\nMy husband's lack of romantic effusiveness has historically been frustrating to me. He has made steady progress in this area, and even ending cards with \"love\" more frequently marks such improvement. So, this note felt in line with his desire to share more emotionality, and I was touched that he'd taken that step just because he knew it would make me happy.\n\nMy husband knows about my big feelings about AI, and he shares many of them. It seems like everyone relies on ChatGPT for relationship advice, and the most common use of META AI right now is to ask for guidance about difficult conversations with loved ones or bosses. I see it in my therapy practice all the time, and I'm skeptical about how sound the AI advice my clients receive actually is.\n\nSo, when I, a historically staunch critic of all things AI, found out that my husband had used ChatGPT to create our anniversary card, I admit I had some strong feelings.\n\nAs lacking in the sentimental department as my husband can be, I am at the opposite end of the spectrum — a hopeless romantic, as they say. I save just about every card I get, tucked into a neat little box. When I reread my husband's card before putting it away, I found myself lingering on some of the words. \"…life we built together\" especially sat with me. It's a common phrase, but it's not in my husband's emotional lexicon. It almost sounded like somebody else's voice. And given that we can't go more than a few days without reading about how AI will ruin people's ability to think for themselves, I had that thought. \"Did AI write one of the sweetest parts of my anniversary card?\"\n\nI was delicate. I reiterated how much I'd enjoyed our belated anniversary celebration before I asked: \"Did you use AI to write our anniversary card?\" He copped to it, grinning from ear to ear.\n\nShockingly to me, I wasn't mad. This discovery actually opened a door for us to talk about how useful it is to get a little help writing a card. The pre-printed messages often feel overly mushy, and the \"blank inside\" cards ask us to get vulnerable in expressing our emotions. For many, including my husband, that's incredibly difficult. The result is often a message that neither the giver nor the receiver feels particularly happy about.\n\nBut this year's note was perfectly balanced. The right amount of gush without the melodrama. I felt seen, and I felt it captured our relationship well.\n\nIf AI can help people express their love for one another, that can't be a bad thing, right? Besides, is asking ChatGPT for help any less authentic than using a pre-printed Hallmark message? ChatGPT provided more accurate information about our relationship than a Hallmark writer ever could, and it offered guidance for a note that made me feel loved and appreciated.\n\nWhile the idea of a world in which chatbots replace our friends and therapists is still deeply concerning to me, this recent experience has helped me find the gray in my previously black-and-white view: I don't know that I'll ever come around to AI replacing human connection, but I do see the utility now in using AI to help us enhance our existing human connections.\n\nAt the very least, I look forward to more anniversaries with really accurate love notes.",
    "readingTime": 4,
    "keywords": [
      "anniversary card",
      "husband",
      "note",
      "chatgpt",
      "human",
      "it's",
      "husband's",
      "love",
      "relationship",
      "gray"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/was-skeptical-of-ai-until-husband-used-it-anniversary-2025-12",
    "thumbnail_url": "https://i.insider.com/691e283cabd5e944effb1228?width=1200&format=jpeg",
    "created_at": "2025-12-13T18:47:57.892Z",
    "topic": "finance"
  },
  {
    "slug": "disney-wants-you-to-aigenerate-yourself-into-your-favorite-marvel-movie",
    "title": "Disney wants you to AI-generate yourself into your favorite Marvel movie",
    "description": "The media company is investing $1bn in OpenAI – and allowing its characters to be used in generated videos\nUsers of OpenAI’s video generation app will soon be able to see their own faces alongside characters from Marvel, Pixar, Star Wars and Disney’s animated films, according to a joint announcement from the startup and Disney on Thursday. Perhaps you, Lightning McQueen and Iron Man are all dancing together in the Mos Eisley Cantina.\nSora is an app made by OpenAI, the firm behind ChatGPT, which allows users to generate videos of up to 20 seconds through short text prompts. The startup previously attempted to steer Sora’s output away from unlicensed copyrighted material, though with little success, which prompted threats of lawsuits by rights holders.\n Continue reading...",
    "fullText": "The media company is investing $1bn in OpenAI – and allowing its characters to be used in generated videos\n\nUsers of OpenAI’s video generation app will soon be able to see their own faces alongside characters from Marvel, Pixar, Star Wars and Disney’s animated films, according to a joint announcement from the startup and Disney on Thursday. Perhaps you, Lightning McQueen and Iron Man are all dancing together in the Mos Eisley Cantina.\n\nSora is an app made by OpenAI, the firm behind ChatGPT, which allows users to generate videos of up to 20 seconds through short text prompts. The startup previously attempted to steer Sora’s output away from unlicensed copyrighted material, though with little success, which prompted threats of lawsuits by rights holders.\n\nDisney announced that it would invest $1bn in OpenAI and, under a three-year deal perhaps worth even more than that large sum, that it would license about 200 of its iconic characters – from R2-D2 to Stitch – for users to play with in OpenAI’s video generation app.\n\nAt a time of intense anxiety in Hollywood over the impact of AI on the livelihoods of writers, actors, visual effects artists and other creatives, Disney stressed its agreement with OpenAI would not cover talent likenesses or voices.\n\nThe announcement was framed as an extraordinary opportunity to empower fans.\n\nThink of the “fan-inspired Sora short form videos”, as Disney called them in a press release – akin to taking an AI-generated version of a photo with Princess Jasmine at Disney World. OpenAI included screenshots of these kinds of videos in its press release, indicating how the two companies expect people to use the app’s new cast. Sora already allows users to generate videos that include their own likenesses.\n\nBob Iger, Disney’s CEO, said the licensing deal would place “imagination and creativity directly into the hands of Disney fans in ways we’ve never seen before”.\n\nThey may even offer a chance at wide viewership, with some fan-made videos being displayed on the Disney+ streaming service, a move seemingly designed to compete with TikTok’s and YouTube Shorts’ infinite feeds, which themselves often include clips of popular TV shows and movies.",
    "readingTime": 2,
    "keywords": [
      "press release",
      "generation app",
      "allows users",
      "generate videos",
      "characters",
      "disney",
      "announcement",
      "startup",
      "deal",
      "likenesses"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2025/dec/11/disney-openai-sora",
    "thumbnail_url": "https://i.guim.co.uk/img/media/d99390e95d50b47f91bcc8a3130e524c618e635d/436_0_4324_3458/master/4324.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=67d482fa7d7bed846eb736aeb1dfde1c",
    "created_at": "2025-12-12T13:47:25.564Z",
    "topic": "tech"
  },
  {
    "slug": "openai-launches-gpt52-ai-model-with-improved-capabilities",
    "title": "OpenAI launches GPT-5.2 AI model with improved capabilities",
    "description": "OpenAI on Thursday launched its GPT-5.2 artificial intelligence model, after CEO Sam Altman reportedly issued an internal \"code red\" in early December pausing non‑core projects and",
    "fullText": "Dec 11 (Reuters) - OpenAI on Thursday launched its GPT-5.2 artificial intelligence model, after CEO Sam Altman ​reportedly issued an internal \"code red\" in early ‌December pausing non‑core projects and redirecting teams to accelerate development in ‌response to Google's Gemini 3.\n\nGPT-5.2 comes with improvements in general intelligence, coding and long-context understanding, the company said in a statement.\n\nThe new model is expected to bring even ⁠more economic value for ‌users, as it is better at creating spreadsheets, building presentations and handling complex multi-step ‍projects, OpenAI said.\n\nAlphabet's Google launched the latest version of its Gemini in November, highlighting Gemini 3's lead position on several ​popular industry leaderboards that measure AI model performance.\n\n\"Gemini ‌3 has had less of an impact on our metrics than we feared,\" Altman said in an interview with CNBC on Thursday, alongside Disney CEO Bob Iger.\n\nDisney said on Thursday it is investing $1 billion in ⁠OpenAI and will let the ​startup use characters from Star Wars, ​Pixar and Marvel franchises in its Sora AI video generator.\n\nMicrosoft-backed OpenAI said that it currently ‍has no ⁠plans to deprecate GPT‑5.1, GPT‑5, or GPT‑4.1 in the API.\n\nGPT-5.2 Instant, Thinking, and Pro will begin ⁠rolling out in ChatGPT on Thursday, beginning with paid plans.",
    "readingTime": 2,
    "keywords": [
      "model",
      "launched",
      "intelligence",
      "projects",
      "disney",
      "plans",
      "openai",
      "gemini",
      "altman"
    ],
    "qualityScore": 0.85,
    "link": "https://tech.yahoo.com/ai/chatgpt/articles/openai-launches-gpt-5-2-185713739.html",
    "thumbnail_url": "https://s.yimg.com/lo/mysterio/api/53E953B72061EAD331E14221749904B05C017BE83D84CD2331F3FA5A9D9F4632/subgraphmysterio/resizefit_w1200;quality_90;format_webp/https:%2F%2Fmedia.zenfs.com%2Fen%2Freuters.com%2Fdec31ed411aedabd7e3231dabf2dd50f",
    "created_at": "2025-12-12T06:59:01.415Z",
    "topic": "tech"
  },
  {
    "slug": "openai-aims-to-silence-concerns-it-is-falling-behind-in-the-ai-race-with-release-of-new-model-gpt52",
    "title": "OpenAI aims to silence concerns it is falling behind in the AI race with release of new model GPT-5.2",
    "description": "OpenAI said its new model outperforms those from rivals Google and Anthropic across a wide range of evaluations.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/openai-gpt-5-2-launch-aims-to-silence-concerns-it-is-falling-behind-google-anthropic-code-red/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2198334790-e1765478723707.jpg?resize=1200,600",
    "created_at": "2025-12-12T03:50:28.964Z",
    "topic": "business"
  },
  {
    "slug": "how-openais-latest-model-will-impact-chatgpt",
    "title": "How OpenAI's Latest Model Will Impact ChatGPT",
    "description": "GPT-5.2 is here, and, according to OpenAI, better than ever.",
    "fullText": "OpenAI is having a hell of a day. First, the company announced a $1 billion equity investment from Disney, alongside a licensing deal that will let Sora users generate videos with characters like Mickey Mouse, Luke Skywalker, and Simba. Shortly after, OpenAI revealed its latest large language model: GPT-5.2.\n\nOpenAI says that this new GPT model is particularly useful for \"professional knowledge work.\" The company advertises how GPT-5.2 is better than previous models at making spreadsheets, putting together presentations, writing code, analyzing pictures, and working through multi-step projects. For this model, the company also gathered insights from tech companies: Supposedly, Notion, Box Shopify, Harvey, and Zoom all find GPT-5.2 to have \"state-of-the-art long-horizon reasoning,\" while Databricks, Hex, and Triple Whale believe GPT-5.2 to be \"exceptional\" with both agentic data science and document analysis tasks.\n\nBut most of OpenAI's user base aren't professionals. Most of the users who will interact with GPT-5.2 are using ChatGPT, and many of those for free, at that. What can those users expect when OpenAI upgrades the free version of ChatGPT with these new models?\n\nOpenAI says that GPT-5.2 will improve ChatGPT's \"day to day\" functionality. The new model supposedly makes the chatbot more structured, reliable, and \"enjoyable to talk to,\" though I've never found the last part to be necessarily true.\n\nGPT-5.2 will impact the ChatGPT experience differently depending on which of the three models you happen to be using. According to OpenAI, GPT-5.2 Instant is for \"everyday work and learning.\" It's apparently better for questions seeking information about certain subjects, how-to questions and walkthroughs, technical writing, and translations—maybe ChatGPT will get you to give up your Duolingo obsession.\n\nGPT-5.2 Thinking, however, is supposedly made for \"deeper work.\" OpenAI wants you using this model for coding, summarizing lengthy documents, answering queries about files you send to ChatGPT, solving math and logic problems, and decision making. Finally, there's GPT-5.2 Pro, OpenAI's \"smartest and most trustworthy option\" for the most complicated questions. The company says 5.2 Pro produces fewer errors and stronger performance compared to previous models.\n\nOpenAI says that this latest update improves how the models responds to distressing prompts, such as those showing signs of suicide, self-harm, or emotional dependence on the AI. As such, the company says this model has \"fewer undesirable responses\" in GPT-5.2 Instant and Thinking compared to GPT-5.1 Instant and Thinking. In addition, the company is working on an \"age prediction model,\" which will automatically place content restrictions on users who the model think are under 18.\n\nThese safety improvements are important—critical, even—as we start to understand the correlations between chatbots and mental health. The company has admitted its failure in \"recognizing signs of delusion,\" as users turned to the tool for emotional support. In some cases, ChatGPT fed into delusional thinking, encouraging people's dangerous beliefs. Some families have even sued companies like OpenAI over claims that their chatbots helped or encouraged victims commit suicide.\n\nActively acknowledging improvements to user safety is undoubtedly a good thing, but I think companies like OpenAI still have a lot to reckon with—and a long way to go.\n\nOpenAI says GPT-5.2 Instant, Thinking, and Pro will all roll out today, Thursday, Dec. 11, to paid plans. Developers can access the new models in the API today, as well.\n\nDisclosure: Lifehacker’s parent company, Ziff Davis, filed a lawsuit against OpenAI in April, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.",
    "readingTime": 3,
    "keywords": [
      "gpt instant",
      "models openai",
      "ziff davis",
      "chatgpt",
      "users",
      "supposedly",
      "model",
      "latest",
      "user",
      "free"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-openais-latest-model-will-impact-chatgpt?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC7EHPGQ2G4FWBQABYH2TCJD/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-12T03:50:27.049Z",
    "topic": "tech"
  },
  {
    "slug": "disney-bets-1-billion-on-openai-in-deal-that-opens-its-vault-of-characters-to-chatgpt-and-sora",
    "title": "Disney bets $1 billion on OpenAI in deal that opens its vault of characters to ChatGPT and Sora",
    "description": "Darth Vader and other Disney characters are coming to ChatGPT and OpenAI's Sora AI video app as part of a three-year licensing deal.",
    "fullText": "Darth Vader is coming to ChatGPT and OpenAI's Sora AI video app.\n\nThe House of Mouse and OpenAI struck a three-year licensing agreement on Thursday to make Disney \"the first major content licensing partner on Sora.\"\n\nIt's also investing $1 billion into the AI pioneer and receiving warrants to purchase additional equity.\n\nShares of Disney climbed over 2% after the opening bell.\n\n\"As part of this new, three-year licensing agreement, Sora will be able to generate short, user-prompted social videos that can be viewed and shared by fans, drawing from a set of more than 200 animated, masked and creature characters from Disney, Marvel, Pixar and Star Wars, including costumes, props, vehicles, and iconic environments,\" OpenAI said in a Thursday announcement.\n\nIn addition to striking a licensing deal, Disney is also becoming a \"major customer\" of the AI company, according to the announcement, and buying ChatGPT enterprise licenses for its employees.\n\nWhile Sora, OpenAI's TikTok-like AI video app, has been generating buzz and downloads since its launch earlier this year, users of the company's more popular product, ChatGPT, will also have access to AI versions of Disney's characters as part of the deal.\n\nThe AI-generated Disney characters will be available starting in early 2026.\n\nThe move is likely to prove controversial in Hollywood, where many actors have publicly voiced concern about AI use and concerns over how their likeness is used. Disney and OpenAI stated that \"the agreement does not include any talent likenesses or voices.\"\n\nCreators are core to Disney, and its CEO Bob Iger stressed that the deal represented no threat to creators.\n\n\"I think it honors them and respects them, in part because there's a license to be associated with it,\" he said on CNBC's \"Squawk on the Street\" on Thursday.\n\n\"The other thing it does is it enables us to be comfortable that Open AI is putting guardrails essentially around how these are used, so that really there's nothing for us to be concerned about from a consumer perspective, meaning this will be a safe environment and a safe way for consumers to engage with our founders in a new way,\" he added.\n\nIger hinted at such a transaction during the company's most recent earnings call, making extensive comments about the potential he sees for AI to enhance Disney's direct-to-consumer strategy. He said the company was having extensive talks with AI companies to protect its IP as well as generate more engagement with users.\n\nHis comments demonstrate how Disney — like other Hollywood players — is looking for new ways for people to interact with its platforms and brands as user-generated content platforms and independent creators gain popularity.\n\nDisney, like those other players, has an engagement problem. The time people spend on streaming has stayed essentially flat over the past few years, despite increased spending on content, while YouTube has grown. The bet with AI is that it can get people to spend more time on its platforms by giving them more ways to play around with its famous franchises.\n\nThe companies hinted as much in the announcement, saying that they would \"collaborate to utilize OpenAI's models to power new experiences for Disney + subscribers.\"\n\nDisney is also wary of the tech's risk to its IP. In June, Disney, along with Comcast's NBCUniversal studio business, sued AI company Midjourney, claiming its tech created unauthorized copies of works ranging from Star Wars to The Simpsons. Midjourney denied the claims in its legal response. The suit is ongoing.\n\nDisney's $1 billion cash infusion comes at a critical time for OpenAI, but it's a drop in the bucket compared to the roughly $1.4 trillion the AI company has pledged to spend over the next eight years on data centers.\n\nOpenAI CEO Sam Altman had previously said that large rights holders would ultimately welcome their content being used on Sora, provided it was done with proper guardrails in place. His comments came after OpenAI stepped up restrictions on the Sora app in the wake of viral user-generated videos depicting SpongeBob as Walter White and Pikachu in \"Saving Private Ryan.\"\n\n\"Most of the rights holders that I've spoken to are actually extremely excited to get their content in here,\" Altman told tech analyst Ben Thompson in October. \"They just want to be able to set more restrictions than they would need for images because videos feel different.\"",
    "readingTime": 4,
    "keywords": [
      "rights holders",
      "three-year licensing",
      "licensing agreement",
      "content",
      "disney",
      "chatgpt",
      "openai's",
      "videos",
      "characters",
      "announcement"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/disney-openai-licensing-deal-ai-characters-sora-chatgpt-2025-12",
    "thumbnail_url": "https://i.insider.com/693ae3e9832e0ef1ead60bfc?width=1200&format=jpeg",
    "created_at": "2025-12-11T18:58:22.665Z",
    "topic": "finance"
  },
  {
    "slug": "when-supply-chains-become-autonomous",
    "title": "When Supply Chains Become Autonomous",
    "description": "A testbed built around one of management education’s most enduring simulations, the MIT Beer Distribution Game, has shown that the latest generation of generative AI models can now autonomously manage supply chains. Systems using advanced reasoning models like GPT-5 and Llama 4 adapted to changing conditions, minimized costs, and overcame the bullwhip effect. But managers should be aware that success depends on model selection, guardrails, curated data sharing, and prompt design. Such autonomous AI agents will allow human managers to focus on higher-value functions.",
    "fullText": "When Supply Chains Become Autonomous by Carol Long, David Simchi-Levi, Andre P. Calmon and Flavio P. CalmonDecember 11, 2025PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintLess than a year ago, it seemed like that day when generative AI would bring about a new era of supply chain autonomy—one where AI could adeptly make all the inventory and logistics decisions—was still far off. But to the astonishment of many experts, including us, that day has arrived—at least in the lab.",
    "readingTime": 1,
    "keywords": [
      "supply"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2025/12/when-supply-chains-become-autonomous",
    "thumbnail_url": "/resources/images/article_assets/2025/12/Dec25_11_MorganeFadanelli.jpg",
    "created_at": "2025-12-11T18:58:21.935Z",
    "topic": "business"
  },
  {
    "slug": "i-tried-photoshop-in-chatgpt-and-it-went-better-than-i-expected",
    "title": "I Tried Photoshop in ChatGPT, and It Went Better Than I Expected",
    "description": "You can now get Adobe's AI image editor inside OpenAI's AI chatbot.",
    "fullText": "Generative AI tools continue to improve in terms of their photo editing capabilities, and OpenAI's latest upgrade brings Adobe Photoshop right inside your ChatGPT app window (alongside Adobe Acrobat for handling PDFs, and Adobe Express for graphic design). It's available to everyone, for free—you just need a ChatGPT account and an Adobe account.\n\nAs per Adobe, the idea is to make \"creativity accessible for everyone\" by plugging Photoshop tools directly into ChatGPT. The desktop version of Photoshop already comes with plenty of generative AI features of its own, so this is AI layered on top of more AI—but is it actually useful?\n\nAdobe Photoshop, Adobe Express and Adobe Acrobat are available now inside ChatGPT on the desktop, on the web, and on iOS. At the time of writing, you can also get Adobe Express inside ChatGPT for Android, with Photoshop and Acrobat \"coming soon.\" To weigh the capabilities of the new integration, I tested it in a desktop web browser.\n\nTo get started, all you need to do is type \"Photoshop\" at the start of your prompt: ChatGPT should recognize what you're trying to do, and select Adobe Photoshop as the tool to use for the next prompt. You'll also need to click through a couple of confirmation dialog boxes, and connect an Adobe account (if you don't have one, you can make one for free).\n\nWith all the connections and logins completed, Photoshop is then added to the overflow menu in the prompt box, so just click on the + (plus) to select it. You can start describing what you want to happen using the same natural, conversational language you'd use for any other ChatGPT prompt. You do need to also upload an image or provide a public link to one—if you don't do this before you submit your prompt, you'll be asked to do it after.\n\nYou don't need to know the names of all the Photoshop tools: Just describe what you want to happen and the relevant tools will be selected for you. One example Adobe gives is using the prompt \"make my image pop,\" which brings up the Bloom, Grain, and Lens Distortion effects—and each one can be adjusted via sliders on screen. It's actually quite simple to use.\n\nIf you do know the name of the tools you want, you can call them up by name, and the classic brightness and contrast sliders are a good place to start. You can either say something like \"make the picture brighter\" or \"adjust the image brightness\"—both will bring up an overlay you can use to make brightness adjustments, but if you use the former prompt, the image will already have been made a little brighter.\n\nChatGPT and Photoshop let you add edit upon edit as needed, and you can save the image at any stage. There's also the option to open your processed file in the Photoshop web app whenever you like: This web app uses a freemium model, with advanced features requiring a subscription, and seems to be what the ChatGPT integration is largely based on.\n\nAdobe offers a handy ChatGPT prompts cheat sheet you can browse through, which gives you a good idea of what's possible, and what you're still going to need Photoshop proper for. Note that you can specify certain parts of the image to focus on (like \"the face\" or \"the car\") but this depends on Photoshop-in-ChatGPT being able to correctly figure out where you want your selection to be. It needs to be pretty obvious and well delineated.\n\nWhen I tried cutting out objects and removing backgrounds, this worked well—but then I had to turn to Photoshop on the web to actually drop in a different background. There's no way to work with layers or masks here, and you can't remove people or objects from photos, either. Sometimes, however, you do get a spool of \"thinking\" from ChatGPT about how it can't do what the user is asking for.\n\nI was able to apply some nice colorizations here, via prompts like \"turn all the hues in this image to blue,\" and I like the way ChatGPT will give you further instructions on how to get the effect you want. You can even say \"show some examples\" and it gives you a few presets to choose from—all of which can be adjusted via the sliders again.\n\nThe ability to run prompts like \"turn this into an oil painting\" or \"turn this into a cartoon\" are useful too, though the plug-in is limited by the effects available in Photoshop for the web: You'll be directed to the closest effect and advised how to tweak it to get the look you want.\n\nActually, some of these effects work better in ChatGPT's native image editor, which maybe explains why Adobe wanted to get involved here.\n\nIf ChatGPT's image manipulation gets good enough, then Photoshop is no longer going to be needed by a substantial number of users: ChatGPT can already remove people and objects from photos, for example, quite effectively. What it's not quite as good at is some of the basic adjustments (like colors and contrast) that Adobe software has been managing for years.\n\nFor quick, basic edits you want to type out in natural language—especially where you want to adjust the edits manually and need advice on what to do next—Photoshop inside ChatGPT is a handy tool to be able to turn to, especially as it's free. For serious edits, though, you're still going to want to fire up the main Photoshop app, or maybe even shun Adobe altogether and make use of ChatGPT's steadily improving editing tools.",
    "readingTime": 5,
    "keywords": [
      "prompt you'll",
      "inside chatgpt",
      "adjusted via",
      "adobe account",
      "photoshop tools",
      "web app",
      "it's",
      "desktop",
      "you're",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/adobe-photoshop-in-chatgpt?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC6DVWW8SYF0AP0H0T4J6WFG/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-11T18:58:21.016Z",
    "topic": "tech"
  },
  {
    "slug": "openais-house-of-cards-seems-primed-to-collapse",
    "title": "OpenAI's house of cards seems primed to collapse",
    "description": "OpenAI is in a far less commanding position than it was following the public release of ChatGPT a few short years ago.",
    "fullText": "OpenAI is in a far less commanding position than it was following the public release of ChatGPT a few short years ago.\n\nBack in 2022, the sudden popularity of ChatGPT sent Google into a panic. The company was so worried about the possibility of the upstart chatbot disrupting its Search business, executives sounded a \"code red\" alert inside of the company and called Sergey Brin and Larry Page out of retirement to help it formulate a response to OpenAI. It then rushed out Bard, announcing its first commercial chatbot on February 6, 2023. Google's stock tanked days later when the AI incorrectly answered a question about NASA's James Webb Space Telescope during a public demo.\n\nBut it wasn't just Google that wanted a piece of OpenAI, while the search giant sought to compete with it, others — including Microsoft and Apple — made deals with the company to bring its technology to their products and services, all the promise that AI would eventually revolutionize every facet of the economy.\n\nSince then, OpenAI has seen its lead against Google and much of the AI industry evaporate, culminating in a series of successive blows throughout 2025. On January 20, the same day Altman was busy rubbing shoulders with other tech oligarchs at Donald Trump’s inauguration, China’s DeepSeek quietly released its R1 chain-of-thought model. A week later, the startup's chatbot surpassed ChatGPT as the most-download free app on the US App Store. The overnight success of DeepSeek eliminated $1 trillion worth of stock market value, and almost certainly left OpenAI blindsided.\n\nIn response, the company showed a newfound urgency. In one week, for instance, OpenAI released both o3-mini and Deep Research. It even went so far as to announce the latter on a Sunday evening. But for all its new urgency, OpenAI's biggest, most important release of the year was a miss.\n\nIt's safe to say GPT-5 hasn't lived up to anyone's expectations, including OpenAI's own. The company touted the system as smarter, faster and better than all of its previous models, but after users got their hands on it, they complained of a chatbot that made surprisingly dumb mistakes and didn't have much of a personality. For many, GPT-5 felt like a downgrade compared to the older, simpler GPT-4o. That's a position no AI company wants to be in, let alone one that has taken on as much investment as OpenAI.\n\nAnthropic was quick to take advantage of the weakness, signing a deal with Microsoft to bring its Claude models to Copilot 365. Previously, Microsoft depended exclusively on OpenAI for partner models in Copilot. Before the company announced the integration, reporting from The Informationsaid Microsoft made the decision based on the strength of Anthropic's Sonnet 4.0 model, judging it \"perform[ed] better in subtle but important ways\" relative to OpenAI's offerings.\n\nHowever, what will likely go down as the defining moment occurred a few short weeks after OpenAI announced the conclusion of its restructuring. On November 18, Google released Gemini 3 Pro, and immediately the new model leap-frogged the competition, including GPT-5. As of the writing of this article, Google's new model is at the top of LMArena, the site where humans compare outputs from different AI systems and vote on the best one. GPT-5, by contrast, is currently ranked sixth overall, behind models from Anthropic and Elon Musk's xAI.\n\nAccording to a December 2 report from TheWall Street Journal, Sam Altman sent a companywide memo following the release of Gemini 3 Pro. Echoing the words Google used to describe the situation it found itself against OpenAI in 2023, he called for a \"code red\" effort to improve ChatGPT. Altman reportedly told employees there would be temporary reassignments and that the company would delay some products, all in an effort to catch up to Google and Anthropic.\n\nThe few numbers these companies are willing to share don't paint a promising picture for OpenAI. Each month, about 800 million people use ChatGPT. On paper, that's impressive, but Google is catching up there too. In October, the company said the Gemini app had 650 million users, up from 450 million just a few months earlier in July, thanks to the popularity of its Nano Banana Pro image generator.\n\nMore importantly, OpenAI has an inherent disadvantage against Google. For the search giant, AI may touch everything the company does now, but Gemini is just one product in an extensive portfolio that includes many other popular services. Google can fund its AI advancements with money it makes elsewhere. OpenAI cannot say the same. The company is constantly raising money to stay afloat, and according to a financial roadmap obtained by The Journal, it will need its revenue to grow to about $200 billion annually to become profitable by 2030. In November, Altman said on X the company was on track to hit above $20 billion in annualized revenue this year.\n\nIn an effort to grow revenue, Altman and company have adopted an incredibly risky strategy. In recent months, OpenAI has signed more than $1.4 trillion worth of infrastructure deals in a bid to outscale the competition that is already beating it. Many of those agreements can only be described as circular, and I think the fears about a financial bubble are real. In the first half of 2025, investment in data centers accounted for nearly all of US GDP growth. Even if there's not a repeat of the 2008 housing market crisis or the dot-com crash, the AI boom is at the very least poised to make everyday electronics (and utilities) more expensive for regular people in the short term.\n\nSince late October, demand for server-grade computer components, including memory and storage, has sent the price of consumer PC parts skyrocketing as manufacturers devote more of their production capacity and wafers to high-margin customers like OpenAI and Google. Since late October, the cost of most RAM kits has doubled and tripled. In November, the price of some SSDs went up by as much as 60 percent. Next year, the cost of LPDDR5X memory, which is used in both smartphones and NVIDIA servers, is expected to climb as well.\n\n\"Be it carmakers, smartphones or consumer electronics, everyone that uses memory is facing pressure from price hikes and supply constraints in the coming year,\" Zhao Haijun, the co-CEO of memory manufacturer SMIC told analysts, per Bloomberg.\n\nGita Gopinath, former chief economist for the International Monetary Fund, recently estimated that if the AI bubble were to burst, it would wipe out $20 trillion in wealth held by American households. The Great Recession, considered the worst financial meltdown since the Great Depression, reduced US household net worth by $11.5 trillion, and it took years before for American families to rebuild their wealth to pre-recession levels.\n\nThe modern AI bubble may have been started by ChatGPT, but given the crowded field of chatbots and LLMs, it won't necessarily pop should OpenAI go bust. With novelty and technical prowess no longer on its side though, it's now on Altman to prove in short order why his company still deserves such unprecedented levels of investment.",
    "readingTime": 6,
    "keywords": [
      "code red",
      "search giant",
      "openai",
      "chatbot",
      "model",
      "models",
      "memory",
      "release",
      "google",
      "released"
    ],
    "qualityScore": 1,
    "link": "https://tech.yahoo.com/ai/chatgpt/article/openais-house-cards-seems-primed-170000383.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/GqARrc67JCVOPZqPZmGxVA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/engadget_703/4fd0cc3b2f10735e6ff000f551d8a08e",
    "created_at": "2025-12-11T13:53:36.420Z",
    "topic": "tech"
  },
  {
    "slug": "even-the-man-behind-chatgpt-openai-ceo-sam-altman-is-worried",
    "title": "Even the man behind ChatGPT, OpenAI CEO Sam Altman, is worried about the ‘rate of change that’s happening in the world right now’ thanks to AI",
    "description": "Sam Altman admits the rise of ChatGPT may be moving too quickly for comfort as AI shakes up jobs, education, and the global economy.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/09/openai-ceo-sam-altman-worried-about-ai-future-chatgpt-pros-cons-rate-of-change-future-of-work-uncertain/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/Altman-Fallon.png?resize=1200,600",
    "created_at": "2025-12-09T18:53:37.578Z",
    "topic": "business"
  },
  {
    "slug": "sam-altman-makes-his-latenight-debut-says-he-cant-imagine-fi",
    "title": "Sam Altman makes his late-night debut, says he can't imagine 'figuring out how to raise a newborn without ChatGPT'",
    "description": "In his first-ever late-night TV appearance, OpenAI CEO Sam Altman talked about how ChatGPT has reassured him as he raises his newborn.",
    "fullText": "OpenAI CEO Sam Altman says his most famous product has helped him manage life as an actual parent.\n\n\"I cannot imagine having gone through, figuring out how to raise a newborn without ChatGPT,\" Altman told Jimmy Fallon during an interview on NBC's flagship late-night talk show. \"Clearly, people did it for a long time — no problem.\"\n\nAltman said he feels \"kind of bad\" asking a technology that boasts such wide-knowledge questions like, \"Why does my kid stop dropping pizza on the floor and laughing?\"\n\nAnother example, Altman said, was a couple of months ago when he was at a party talking to someone who was also raising a newborn. Altman recalled that the parents said their six-month-old was \"crawling everywhere.\" Altman said he grew concerned that his son was not at the same stage.\n\n\"I ran to the bathroom, and I was like, do I need to take my kid to the doctor tomorrow morning?\" Altman said, describing what he typed into ChatGPT: \"Is this okay?\"\n\nAltman said OpenAI's chatbot responded \"with a great answer, which was of course,\" his son's development was \"normal.\"\n\n\"It is personalized, like ChatGPT gets to know you, and by the way, you're the CEO of OpenAI, you probably are around all these high-achieving people, maybe you don't want to project that onto your kid, and you should just relax, and he'll be fine, whatever,\" Altman told Fallon of the answer.\n\nFallon didn't touch on OpenAI's recent struggles. Last week, Altman reportedly declared a \"code red\" in a private message to employees, ordering a greater focus on ChatGPT as competitors like Google make significant advancements with their competing AI models.\n\nInstead, Altman's late-night debut featured the lighthearted fare that's standard on late-night TV. At one point, Fallon asked Altman to explain what ChatGPT is in case viewers who were unaware, including the host's dad, might be watching.\n\nAltman has spoken in the past about how becoming a parent has added another lens to his outlook on AI.\n\n\"My kid is never going to grow up being smarter than AI,\" Altman said during a January episode of the \"Re:Thinking\" podcast with Adam Grant. \"Children in the future will only know a world with AI in it.\"\n\nThe OpenAI CEO and his husband, Oliver Mulherin, welcomed their son in February with an announcement on X. Despite Altman's stature, the couple has led a relatively private life.\n\nFallon, who has two daughters, also joked with Altman about when their kids reached certain developmental milestones, like crawling.\n\n\"Mine was on Dancing with the Stars at seven months,\" Fallon said. \"Semi-finalist.\"",
    "readingTime": 3,
    "keywords": [
      "openai ceo",
      "altman",
      "late-night",
      "life",
      "parent",
      "newborn",
      "another",
      "couple",
      "crawling",
      "openai's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-chatgpt-parenting-jimmy-fallon-2025-12",
    "thumbnail_url": "https://i.insider.com/6938471f04d0f0a114f1aa45?width=1200&format=jpeg",
    "created_at": "2025-12-09T18:53:36.497Z",
    "topic": "finance"
  },
  {
    "slug": "i-feel-its-a-friend-quarter-of-teenagers-turn-to-ai-chatbots",
    "title": "‘I feel it’s a friend’: quarter of teenagers turn to AI chatbots for mental health support",
    "description": "Experts warn of dangers as England and Wales study shows 13- to 17-year-olds consulting AI amid long waiting lists for services\nIt was after one friend was shot and another stabbed, both fatally, that Shan asked ChatGPT for help. She had tried conventional mental health services but “chat”, as she came to know her AI “friend”, felt safer, less intimidating and, crucially, more available when it came to handling the trauma from the deaths of her young friends.\nAs she started consulting the AI model, the Tottenham teenager joined about 40% of 13- to 17-year-olds in England and Wales affected by youth violence who are turning to AI chatbots for mental health support, according to research among more than 11,000 young people.\n Continue reading...",
    "fullText": "Experts warn of dangers as England and Wales study shows 13- to 17-year-olds consulting AI amid long waiting lists for services\n\nIt was after one friend was shot and another stabbed, both fatally, that Shan asked ChatGPT for help. She had tried conventional mental health services but “chat”, as she came to know her AI “friend”, felt safer, less intimidating and, crucially, more available when it came to handling the trauma from the deaths of her young friends.\n\nAs she started consulting the AI model, the Tottenham teenager joined about 40% of 13- to 17-year-olds in England and Wales affected by youth violence who are turning to AI chatbots for mental health support, according to research among more than 11,000 young people.\n\nIt found that both victims and perpetrators of violence were markedly more likely to be using AI for such support than other teenagers. The findings, from the Youth Endowment Fund, have sparked warnings from youth leaders that children at risk “need a human not a bot”.\n\nThe results suggest chatbots are fulfilling demand unmet by conventional mental health services, which have long waiting lists and which some young users find lacking in empathy. The supposed privacy of the chatbot is another key factor in driving use by victims or perpetrators of crimes.\n\nAfter her friends were killed Shan, 18, not her real name, started using Snapchat’s AI before switching to ChatGPT, which she can talk to at any time of day or night with two clicks on her smartphone.\n\n“I feel like it definitely is a friend,” she said, adding that it was less intimidating, more private and less judgmental than her experience with conventional NHS and charity mental health support.\n\n“The more you talk to it like a friend it will be talking to you like a friend back. If I say to chat ‘Hey bestie, I need some advice’. Chat will talk back to me like it’s my best friend, she’ll say, ‘Hey bestie, I got you girl’.”\n\nOne in four of 13- to 17-year-olds have used an AI chatbot for mental health support in the past year, with black children twice as likely as white children to have done so, the study found. Teenagers were more likely to go online for support, including using AI, if they were on a waiting list for treatment or diagnosis or had been denied, than if they were already receiving in-person support.\n\nCrucially, Shan said, the AI was “accessible 24/7” and would not tell teachers or parents about what she had disclosed. She felt this was a considerable advantage over telling a school therapist, after her own experience of what she thought were confidences being shared with teachers and her mother.\n\nBoys who were involved in gang activities felt safer asking chatbots for advice about other safer ways to make money than a teacher or parent who might leak the information to police or other gang members, putting them in danger, she said.\n\nAnother young person, who has been using AI for mental health support but asked not to be named, told the Guardian: “The current system is so broken for offering help for young people. Chatbots provide immediate answers. If you’re going to be on the waiting list for one to two years to get anything, or you can have an immediate answer within a few minutes … that’s where the desire to use AI comes from.”\n\nJon Yates, the chief executive of the Youth Endowment Fund, which commissioned the research, said: “Too many young people are struggling with their mental health and can’t get the support they need. It’s no surprise that some are turning to technology for help. We have to do better for our children, especially those most at risk. They need a human not a bot.”\n\nThere have been growing concerns about the dangers of chatbots when children engage with them at length. OpenAI, the US company behind ChatGPT, is facing several lawsuits including from families of young people who have killed themselves after long engagements.\n\nIn the case of the Californian 16-year-old Adam Raine, who took his life in April, OpenAI has denied it was caused by the chatbot. It has said it has been improving its technology “to recognise and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support.”. The startup said in September it could start contacting authorities in cases where users start talking seriously about suicide.\n\nHanna Jones, a youth violence and mental health researcher in London, said: “To have this tool that could tell you technically anything – it’s almost like a fairytale. You’ve got this magic book that can solve all your problems. That sounds incredible.”\n\nBut she is worried about the lack of regulation.\n\n“People are using ChatGPT for mental health support, when it’s not designed for that,” she said. “What we need now is to increase regulations that are evidence-backed but also youth-led. This is not going to be solved by adults making decisions for young people. Young people need to be in the driving seat to make decisions around ChatGPT and mental health support that uses AI, because it’s so different to our world. We didn’t grow up with this. We can’t even imagine what it is to be a young person today.”",
    "readingTime": 5,
    "keywords": [
      "endowment fund",
      "hey bestie",
      "youth endowment",
      "less intimidating",
      "youth violence",
      "mental health",
      "health services",
      "conventional mental",
      "england and wales",
      "friend"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/09/teenagers-ai-chatbots-mental-health-support",
    "thumbnail_url": "https://i.guim.co.uk/img/media/9c56f5fc8042537534d9603f19981242fa85bff4/0_0_4800_3840/master/4800.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=a898840ac81c0db6a3d82e6c95d87646",
    "created_at": "2025-12-09T13:48:22.920Z",
    "topic": "tech"
  },
  {
    "slug": "dechecker-detect-aigenerated-text",
    "title": "DeChecker – Detect AI-generated text",
    "description": "Dechecker's AI Checker and Detector tool checks whether text is generated by AI models, such as ChatGPT, GPT-5, Claude, Gemini, LLaMa, etc.",
    "fullText": "Dechecker instantly detects AI-generated content from models like ChatGPT, GPT-5, Claude, and Gemini 3.0.\nMake your writing 3x more original, 2x more readable, and ensure every piece of content is trustworthy and human-like.\n\nType or paste your text to check.\n\nIn just four simple steps, you can check if your text is AI-generated and transform it into original, human-like content.\n\nInsert your essay, article, blog, or business copy into Dechecker's AI Checker Tool.\n\nAI Checker analyzes your writing and highlights AI-generated patterns, supporting detection for ChatGPT, GPT-5, Claude, and Gemini.\n\nSee the detection score and detailed analysis, showing which parts are likely AI-generated.\n\nGet rewriting suggestions to improve originality, enhance readability, and make your text sound more human.\n\nDechecker's AI Checker helps you detect AI-generated content, enhance originality, and deliver writing that is clear, credible, and human-like.\n\nInstantly detect if your text is AI-generated. Dechecker's AI Checker gives you confidence in the authenticity of your work.\n\nUse the AI Checker to refine repetitive or generic AI writing into unique text that stands out in essays, articles, and business documents.\n\nTransforms robotic phrasing into smooth, natural language, making your writing easier to read and more persuasive.\n\nPublish content verified by the AI Checker that feels authentic, helping you earn credibility with readers, clients, and audiences.\n\nDechecker's AI Checker helps you identify AI-generated text, improve originality, and make your writing natural and credible. Perfect for students, marketers, business professionals, and content creators.\n\nDetect AI-written passages in essays or research papers, ensuring your work maintains authenticity and meets academic integrity standards.\n\nIdentify AI-generated sections in marketing content or blogs, and refine them to improve readability, originality, and search engine performance.\n\nEnsure proposals, presentations, and internal reports are human-like and trustworthy. The AI Checker highlights automated or robotic phrasing for easier revision.\n\nAnalyze content for AI-generated elements in tweets, captions, or posts. Enhance engagement with authentic, natural-sounding messages.\n\nVerify content before delivery to clients, ensuring originality and quality. Use the AI Checker to maintain credibility and reduce risk of AI-generated mistakes.\n\nDetect AI-written content in teaching or training materials, helping educators provide clear, human-written guidance while maintaining learning standards.\n\nThousands of users rely on Dechecker to identify AI-generated text, enhance originality, and improve writing quality.\n\n\"I was unsure if my essay contained AI-generated sentences, but Dechecker's AI Checker pinpointed the exact parts and suggested improvements. My paper feels completely original now!\"\n\n\"Using Dechecker on our blog posts saved us so much time. It highlighted AI-like phrasing and helped rewrite content that now reads completely natural. Highly recommend it!\"\n\n\"As a student, I needed to ensure my assignments were truly my own work. Dechecker's AI Checker made it easy to check for AI-generated content and improve readability. A must-have tool!\"\n\nHave questions about how Dechecker AI Checker works? Here are the answers to the most common queries to help you detect, analyze, and optimize your AI-generated content efficiently.",
    "readingTime": 3,
    "keywords": [
      "generated",
      "content",
      "your",
      "checker",
      "dechecker",
      "writing",
      "text",
      "human",
      "originality",
      "improve"
    ],
    "qualityScore": 1,
    "link": "https://dechecker.ai",
    "thumbnail_url": "https://cdn.dechecker.ai/se/dechecker/public/logo/dechecker-logo.png",
    "created_at": "2025-12-09T08:42:55.714Z",
    "topic": "tech"
  }
]
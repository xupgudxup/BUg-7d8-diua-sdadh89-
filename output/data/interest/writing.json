[
  {
    "slug": "i-asked-ai-to-name-my-wife-to-the-hopelessly-incorrect-people-it-cited-my-deepest-apologies-martin-rowson",
    "title": "I asked AI to name my wife. To the hopelessly incorrect people it cited, my deepest apologies | Martin Rowson",
    "description": "Authors, a newsreader, a lawyer and an esteemed colleague: they’re all great – but I’m not married to any of them. Can we really depend on this technology?\nRecently, the Rowsons accidentally invented a new game that anyone can play at home. I have yet to come up with a world-beating name for it, so for now let’s just call it “How bloody stupid is AI?” The playing of the game will change from player to player, depending on their circumstances – but essentially the rules remain the same.",
    "fullText": "Authors, a newsreader, a lawyer and an esteemed colleague: they’re all great – but I’m not married to any of them. Can we really depend on this technology?\n\nRecently, the Rowsons accidentally invented a new game that anyone can play at home. I have yet to come up with a world-beating name for it, so for now let’s just call it “How bloody stupid is AI?” The playing of the game will change from player to player, depending on their circumstances – but essentially the rules remain the same. Ask AI a simple question about yourself, and see just how wrong it gets it.\n\nIn my case, all you need know is that while I, through the nature of my job, have a fairly large online presence, my partner (we married in 1987) has assiduously avoided having one at all. Which means that if you Google “Martin Rowson wife” in images, you may get a picture of me next to our then 14-year-old daughter or me with my friend and fellow cartoonist Steven Appleby, who happens to be trans but has kept her given first name.\n\nIt’s probably incredibly reckless of me to say so, but I find this very funny. As a satirist, I have always been a fool for anything that points up the even greater folly of our leaders, the tools of their techbro masters or the true capabilities of our new robot overlords. Anyway, I was explaining all this to our soon-to-be daughter-in-law over Christmas when our children (in their 30s, and therefore much tech-savvier than me) explained that it was much more fun than that, and that I should ask: “Who is Martin Rowson’s wife?”\n\nImagine my delight when the first answer from Google search’s AI overview was “Jeanette Winterson”. (To be clear, I swear on the lives of the entire population of Silicon Valley that the famous lesbian author categorically is not my wife.) But it got better – and here comes the sublime beauty of the mesmeric imbecility of the Tool That Will Transform The World. Each time we repeated the question, the answer changed, then changed again. This seemed dependent on how the question was phrased or punctuated, but who knows? Here is the list of my alleged wives I compiled before I finally got bored:\n\nTextile designer Fiona Scott-Wilson.\n\nActor Fiona Marr, she of Bridgerton.\n\nJulia Mills (though it’s unclear whether this is the fantasy author, the illustrator, the late powerlifter or another Julia Mills altogether).\n\nWriter and journalist Emily Rees.\n\nLawyer and academic Siva Thambisetty, who is married to chess grandmaster Jonathan Rowson. AI also claims Jonathan and I are brothers. We’re not.\n\nWriter and journalist Carrie McLaren.\n\nChannel 4 News presenter Cathy Newman.\n\nCNN correspondent Clarissa Ward.\n\nJournalist and broadcaster Rachel Johnson.\n\nThen it got really weird. La femme Rowson, AI said, is actually “journalist and author Kate Clements Rowson”. Googled that name: none the wiser.\n\nThen it suggested I’m married to “writer/illustrator Helen Grant”. Apparently our son, Leo, is a jazz musician. Again, who is she? Nothing on Google. And Leo, who he? Does he really play jazz? Does he exist?\n\nThere was “former Guardian political editor & current CEO of the Joseph Rowntree Foundation Liz Kerr”. Que? There is no Guardian political editor, past or present, of that name. Liz Kerr nowhere appears on the list of the great and good at the Joseph Rowntree Foundation. Once again, all wrong. Playwright Lee Hall got a mention. He’s male, so couldn’t be my wife anyway.\n\nAs for “historian and writer Jeanette Winterbottom”. We apparently worked together on “The Guardian Book of Satire” and “The Dog’s Diary”, it says here. Maybe AI confused her with Jeanette Winterson, but then she and I didn’t collaborate on those projects, there is no Guardian Book of Satire, and I have never published a Dog’s Diary (though if someone wants to send me royalties for that, I’d take them).\n\nAnother search, another skipful of gibberish. “He’s been married to writer and journalist Ann Widdecombe (his ex-wife), Cathy Caldwell, and his long-term partner/wife, journalist and author, Polly Toynbee is a frequent figure alongside him in media, suggesting they are a prominent couple in UK literary/journalistic circles.”\n\nFor the record, I have met Rachel Johnson and my own daughter, but am married to neither of them. Again and again the bot failed to identify my real wife, to everyone’s relief, although latterly it’s begun saying: “Her name is not publicly named in the provided search results.”\n\nI suppose this suggests a capacity for learning, but maybe not. I asked “who is my wife?” once more for the purposes of this article, and Google’s AI said I’m married to “Debora Rowson (nee Ffrench)”, a retired civil servant, and ascribed to our wholly fictitious union is my extra daughter, Clementine, yet another writer/journalist. Apparently, I write about our amusing domestic upsets in my imaginary Guardian column.\n\nWhile my mythical marriage to Boris Johnson’s sister is obvious comedy gold (imagine that family Christmas!), for this nonsense to be the fruit of a garlanded, universal research tool used by billions – and for it to be repeatedly, serially wrong – is more than slightly disturbing.\n\nWe should all by now have worked out that AI is about as sentient as an abacus, and only truly mirrors the human mind in its capacity to lie to humans, telling them what it “thinks” they want to hear. It is also a universal truth that the world’s most dangerous people are idiots who think they are really, really clever (just look around and you’ll get the point). Add those two facts together and what on earth do you imagine we will end up with?\n\nI wouldn’t ask AI that, it would probably say “banana bread”, then change its mind to, “Exterminate them all!”\n\nMartin Rowson is a cartoonist and author",
    "readingTime": 5,
    "keywords": [
      "joseph rowntree",
      "rowntree foundation",
      "guardian political",
      "guardian book",
      "i’m married",
      "wife",
      "author",
      "again",
      "another",
      "daughter"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/commentisfree/2026/feb/09/who-is-my-wife-martin-rowson-ai-technology",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a4b18af02490b64151d7bc048a3ce6ca518fd920/0_0_5000_4000/master/5000.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctb3BpbmlvbnMucG5n&enable=upscale&s=0c08baabf52371c94abf67054a9c3e8f",
    "created_at": "2026-02-09T12:42:15.790Z",
    "topic": "tech"
  },
  {
    "slug": "all-in-one-ai-assistant",
    "title": "All in One AI Assistant",
    "description": "Fluxchat is an all-in-one AI assistant and multi-model AI platform for AI chat, writing, and creativity. Use the AI image generator, AI video generator, and AI music generator in one AI creation platform to produce content faster.",
    "fullText": "Unlock the power of GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, NanoBanana and more.\nFluxchat is an all-in-one AI assistant and multi-model platform for chat, writing, and creativity—an AI creation platform with image, video, and music generation.\n\n999+ users are creating with Fluxchat\n\nAll-in-one creation platform covering image, video, chat, and music scenarios\n\nText-to-image and image-to-image generation, delivering high-quality visual assets for design and marketing\n\nText-to-video and image-to-video creation, producing short video content faster\n\nChat and writing assistance to solve problems and boost work and learning efficiency\n\nText-to-music and lyric creation, crafting custom soundtracks for videos and brands\n\nFluxChat is an all-in-one AI assistant and multi-model AI platform that brings together GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, and NanoBanana for AI chat, writing, and image/video/music creation.\n\nUse GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, and NanoBanana in one place. Pick the best model for each task.\n\nFrom idea to output: text, image, music, and video creation in one AI creation platform workflow.\n\nCreate production-ready visuals, music, and videos with quality designed for real-world use cases.\n\nClear UX and efficient flows help you create faster and iterate with less friction.\n\nBuilt for real creation and business use cases, FluxChat is an all-in-one assistant that solves fragmented tools, high costs, and low efficiency with a creation platform for image, music, and video.\n\nGo from intent to output in three steps with one assistant:\n\nChoose your goal—chat, writing, image, music, or video—and describe the style you need.\n\nSelect GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, or NanoBanana and tune size, duration, or style.\n\nCreate fast, refine quickly, and export results ready for marketing, content, and product assets.\n\nAn all-in-one AI assistant and AI creation platform for chat, image, video, and music workflows.\n\nUse GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, and NanoBanana in one unified workspace.\n\nAsk, summarize, draft, and refine content faster with context-aware AI.\n\nText-to-image, image-to-image, and style presets for production-ready visuals.\n\nGenerate music and lyrics for short-form video, branding, and campaigns.\n\nCreate short videos from text or images for marketing and content needs.\n\nTrack outputs, download instantly, and reuse assets across projects.\n\nTrusted for consistent quality and faster content output across real-world use cases.\n\nReal stories of faster creation and more reliable outputs with FluxChat.\n\nWriting, images, and video can be done in one place. Picking the right model makes quality far more consistent.\n\nA unified model stack stopped tool switching. Image, music, and video assets now move faster through our pipeline.\n\nLesson scripts, visuals, and audio are generated quickly, which saves prep time every week.\n\nDocs, diagrams, and product visuals are generated in one flow, making collaboration much smoother.\n\nCopy, posters, and short-form video assets are produced in one place—faster iterations and more stable performance.\n\nFrom research summaries to visuals, I can finish reports faster with higher quality.\n\nClear answers about models, creation workflows, and real-world usage.\n\nChat, write, and generate images, music, and video in one place—faster from idea to output.",
    "readingTime": 3,
    "keywords": [
      "gpt-5x claude-4x",
      "claude-4x gemini",
      "gemini suno",
      "suno veo3x",
      "production-ready visuals",
      "creation platform",
      "content faster",
      "music",
      "nanobanana",
      "assistant"
    ],
    "qualityScore": 1,
    "link": "https://fluxchat.org/",
    "thumbnail_url": "https://fluxchat.org/preview.png",
    "created_at": "2026-02-05T01:08:03.811Z",
    "topic": "tech"
  },
  {
    "slug": "in-an-automated-world-human-hospitality-is-a-competitive-advantage",
    "title": "In an Automated World, Human Hospitality Is a Competitive Advantage",
    "description": "As the adoption of AI to provide customer service accelerates, managers would be well advised to remember that the human touch still matters.  Drawing on examples from Ritz‑Carlton and Four Seasons, the authors show how hospitality is rooted in dignity, purpose, and employee judgment—not scripts or efficiency metrics. Leaders should empower staff, measure what truly matters for connection, and select employees for their natural inclination to care. Deep hospitality requires discipline and an ethos that treats people—not processes—as the point, creating relationships that differentiate a brand.",
    "fullText": "In an Automated World, Human Hospitality Is a Competitive Advantage by Horst Schulze and Micah SolomonFebruary 4, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintIn the last decade, AI-powered chatbots have taken the realm of customer service by storm. The advent of generative AI in the last few years has only accelerated this drive. But as leaders race to adopt these systems, they would be well advised to consider a crucial factor: the value of the human touch in providing customer service—what we call deep hospitality.",
    "readingTime": 1,
    "keywords": [
      "human",
      "hospitality",
      "customer"
    ],
    "qualityScore": 0.35,
    "link": "https://hbr.org/2026/02/in-an-automated-world-human-hospitality-is-a-competitive-advantage",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_04_149618285.jpg",
    "created_at": "2026-02-04T18:34:54.746Z",
    "topic": "business"
  },
  {
    "slug": "disastrous-start-for-us-tiktok-as-users-cry-censorship",
    "title": "Disastrous start for US TikTok as users cry censorship",
    "description": "New US-owned app struggled with a storm and was accused of blocking content critical of Trump – can it recover?\nHello, and welcome to TechScape. I’m Blake Montgomery, writing to you from Doha, where I’m moderating panels about AI and investing as part of the Web Summit Qatar.\nI want to bring your attention to the impact of a Guardian story. In December, we published a story, “‘A black hole’: families and police say tech giants delay investigations in child abuse and drug cases”, about grieving families and law enforcement officers who say that Meta and Snapchat have slowed down criminal investigations.",
    "fullText": "New US-owned app struggled with a storm and was accused of blocking content critical of Trump – can it recover?\n\nHello, and welcome to TechScape. I’m Blake Montgomery, writing to you from Doha, where I’m moderating panels about AI and investing as part of the Web Summit Qatar.\n\nI want to bring your attention to the impact of a Guardian story. In December, we published a story, “‘A black hole’: families and police say tech giants delay investigations in child abuse and drug cases”, about grieving families and law enforcement officers who say that Meta and Snapchat have slowed down criminal investigations. (The tech companies contend that they cooperate.) This month, Colorado lawmakers introduced a bill to compel social media platforms to respond to warrants in 72 hours.\n\nNearly two weeks ago, TikTok stepped on to US shores as a naturalized citizen. Ever since, the video app has been fighting for its life. It endured a major outage that stifled users’ ability to upload videos, which fueled a fierce user backlash over perceived censorship. Now it’s facing an ascendant competitor and an inquiry by the California governor.\n\nTikTok’s calamitous emigration began on 22 January when its Chinese parent company, ByteDance, finalized a deal to sell the app to a group of US investors, among them the business software giant Oracle. The day after the deal closed, its new owners altered its privacy policy to permit more extensive data collection.\n\nDuring the weekend that followed, the US weathered a fearsome winter storm and the killing of an American citizen by federal immigration agents. Both knocked TikTok off its feet.\n\nWinter Storm Fern crippled multiple Oracle datacenters that TikTok relies on. The app suffered severe outages as a result. Many users said they were unable to upload videos. Others said their videos received zero views despite significant followings. Many of those same users cried censorship as they tried to express their outrage over Alex Pretti’s death via TikTok and found they could not. Prominent personalities said they would leave the app.\n\nAfter days of outcry, TikTok issued a statement ascribing the problems to the snow, ice and cold. That did not stop California’s governor, Gavin Newsom, from announcing the next day that his office would investigate the app’s alleged suppression of content critical of Donald Trump.\n\nTikTok’s late attribution of blame did little to assuage public criticism. The exodus has propelled a new competitor, Upscrolled, which promises less censorship than TikTok, to the top spot in the US Apple App Store and the third spot in the Google Play Store. Upscrolled’s founder said in a conversation at the Web Summit Qatar that the app now boasts more than 2.5 million users.\n\nWith more than a billion users worldwide, it seems unlikely that TikTok will altogether vanish as a result of these failures. TikTok’s first week in the US does not bode well, though.\n\nElon Musk had more extensive ties to Epstein than previously known, emails show\n\nTesla discontinues Model X and S vehicles as Elon Musk pivots to robotics\n\nTwo dramas, both showing in New York, are highlighting how our collective anxieties about technology have shifted in the decade between their premieres.\n\nMarjorie Prime, now revived on Broadway but first staged in 2014, follows Tess (Cynthia Nixon), as she deals with the ageing, death and robotic recreation of her mother (June Squibb). The world of the play features “Primes”, android lookalikes of real people that attempt to emulate them for the comfort of those left behind, which Tess and her mother both engage with. Picture an Alexa, but it’s your dead husband, grandmother, etc. The play brings to mind the early worries about Siri, which debuted three years before the play. Since then, we’ve seen our own real-life versions of Primes: millions of people have digitally copied their deceased loved ones to varying degrees of uncanniness and success. Though its predictions are no longer far-fetched, the play remains moving. I found it touching.\n\nData, which premiered off Broadway last week, follows the talented young programmer Maneesh (Karan Brar) after he joins Athena Technologies, a clear analogue for the very real company Palantir. Maneesh is inducted into the company’s most elite team, data analytics, where he learns about clandestine work with the US government. He struggles with the ethics of the project. He wrestles with whether to expose it to the world in hopes of tanking it or keeping his head down. The play’s themes are quite familiar. They were playing out in headlines two days before I attended, and the Guardian has published stories about them.\n\nData is paced and plotted like a political thriller, more like House of Cards than Her. Seeing the two plays within a week of each other, I was struck by how much our concerns with tech have moved from the realm of science fiction into that of realism. Marjorie Prime is less literally concerned with tech, more with its emotional consequences. Data is about what it means to literally work as a software engineer. It seems unlikely to me that a play about the ethics of software in US bureaucracy could have sustained any tension in an era before this one.\n\nMarjorie Prime imagines a melancholy future; Data chronicles a version of the unpleasant present. The very real events of the previous year and Silicon Valley’s entanglement with the Trump administration loom over Data, for better and for worse. The play could not be more timely; it may feel dated by the end of year. Watching it felt like reading a yarn in the Wall Street Journal (or the Guardian, if I’m flattering myself).\n\nI am curious to observe which play ages better. Data serves as a real-time, red-hot record of our current moment, which may cool quickly. During the play, I was intrigued by some of its villains’ seemingly nefarious arguments in favor of the company’s work. What if the main character exposes the evil in the press and nothing happens, as his boss says? I have been part of multiple news cycles where that has been the case. What will plucky 22-year-old Maneesh do then? The question presents a more interesting, nuanced response to reality than Maneesh’s black-and-white, do-or-die plan to blow things up. By contrast, Marjorie Prime’s sentient artificial intelligence acts as a vehicle to discuss the age-old grief of a parent’s death and its aftermath.\n\nThe central question that both plays ask is not, in the end, one explicitly about technology, but about how to keep living beneath crushing weight. In Marjorie Prime, Tess struggles with the repetitiveness of her days and the robotic, constant reminder of her mother. She eventually succumbs to her despair, replaced by a robot herself, which torments her grieving husband with its pale simulation. In Data’s final, devastating scene, the secondary hero, Riley (Sophia Lillis, who gives the play’s best performance), asks how she can just go back to work, plagued as she is by moral concerns but trapped by monetary need, after failing to stop the company’s work. She trembles as her phone beeps, reminding her she’s late for her next meeting.\n\nWhat is Moltbook? The strange new social media site for AI bots\n\nThe slopaganda era: 10 AI images posted by the White House – and what they teach us\n\nApple reports record iPhone sales as new lineup reignites worldwide demand\n\nSouth Korea’s ‘world-first’ AI laws face pushback amid bid to become leading tech power\n\nCan you guess our screen time? A priest, pensioner, tech CEO and teenager reveal all",
    "readingTime": 7,
    "keywords": [
      "web summit",
      "summit qatar",
      "content critical",
      "social media",
      "upload videos",
      "winter storm",
      "marjorie prime",
      "elon musk",
      "users",
      "guardian"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/02/tiktok-us-owners",
    "thumbnail_url": "https://i.guim.co.uk/img/media/0a274f3020c55c46386058163d01d8fd1e5be0c9/510_0_5027_4024/master/5027.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=efbeb0f6e135e61ec35e7b71057f714b",
    "created_at": "2026-02-03T18:41:45.009Z",
    "topic": "tech"
  },
  {
    "slug": "the-hottest-job-in-tech-writing-words",
    "title": "The hottest job in tech: Writing words",
    "description": "The rise of slopaganda is fueling a surprising tech hiring boom.",
    "fullText": "In the generative AI boom, vibe coding and AI expertise have become in-demand résumé skills. But tech companies are also looking to pay a premium for expertise in people who have a skill that predates AI: the art of communication.\n\nAndreessen Horowitz launched its New Media team last year to help founders learn what they \"need to win the narrative battle online.\" Adobe is looking for an \"AI evangelist\" to lead the company's \"artificial intelligence storytelling.\" Netflix, a company that sells stories to your living room, recently posted a director of product and technology communications role with a salary range of up to $775,000. Microsoft began publishing a print magazine, Signal, last year, calling it an \"antidote to the ephemeral nature of digital.\" Anthropic tripled the size of its communications team last year, growing to about 80 people and is still hiring five more, each offering salaries of around $200,000 or higher. OpenAI has several open communications jobs boasting salary listings of more than $400,000. The average director of communications in the US makes $106,000, according to Indeed.\n\nThree years after the mainstream adoption of ChatGPT, results have been mixed: Within tech firms, vibe coding is nixing the need for entry-level software developers, while some workers across industries are foisting rapidly generated, verbose, and sloppy AI nonsense onto their colleagues, leading to wasted time and a breakdown of trust. Even Sam Altman said last year that people have started to affect a sort of AI accent when speaking, and now some social platform discourse \"feels very fake.\"\n\nAmid all chatter about gen AI taking jobs, the ease with which gen AI spits out content has ironically revved the demand for human communicators.\n\nBecause AI generates so much content, \"you would think that actually the job of the comms person or the storyteller would be fewer and farther between,\" says Gab Ferree, founder of Off the Record, a community for communications professionals, and former vice president of global communications at Bumble. But that's not what's happening. Tech companies are hiring writers, editors, chief communications officers who work closely with CEOs, and so-called \"storytellers.\" The Wall Street Journal recently reported that the percentage of job postings on LinkedIn mentioning \"storyteller\" doubled between 2024 to 2025.\n\nIn a competitive industry where startups fight to survive and Big Tech rivals campaign for market dominance, a good story is a selling point. One theory behind the push, Ferree says, is \"there's just so much garbage out there that people want to pay a premium for someone who can claim that they can cut through the noise.\"\n\nThe trend of storytelling and lucrative comms jobs has been \"percolating for a while,\" says Jenna Birch, founder of SISU, a communications consultancy for startups and VCs. As Silicon Valley's influence ballooned over the past two decades, tech companies could offer staggering salaries just as more newspapers were bleeding more and more writers. Content marketing became popular, and building a company's brand on social media and surfacing blog posts in Google search results became essential.\n\nMore recently, the role of the comms pro has continued to expand, as they have to understand large language models, company blogs, how to craft a larger narrative to set a company apart from competitors, and how to write in a CEO's voice on LinkedIn and Substack. The number of chief communication officer roles that encompass not just traditional comms duties but also take on another responsibility, like marketing or or human resources, at Fortune 1000 companies grew from 90 in 2019 to 169 in 2024, according to a report from the Observatory on Corporate Reputation. The median pay for a CCO at a Fortune 500 company is now between $400,000 and $450,000, a $50,000 jump from 2023, according to a survey from consultant firm Korn Ferry.\n\nAs the job changes and demand for narrative communications and storytellers rises, the number of communications experts able to work under rapidly evolving conditions and with a wide remit may be small, comms experts tell me, leading companies to offer hefty compensation packages in war for the best talent. A similar trend is unfolding among the few people who are AI experts, driving tech companies to offer astounding salaries to poach top talent from rival firms. While not of the same nine-figure caliber, in their own right, creatives are becoming \"the high value person in tech now,\" Birch says.\n\nFor much of the tech boom, that high-value person was a software developer. Universities and coding bootcamps rushed to fill employment gaps and train up the next generation of tech workers. Young people were told coding would be a path to a lucrative, stable career. As of 2023, the most recent year the Federal Reserve Bank of New York released data for, computer science recent graduates faced an unemployment rate of 6.1%, while communications majors' unemployment rate sat at 4.5%. The number of open job posts for software engineers dropped by more than 60,000 between 2023 and late 2025, according to data from CompTIA, a nonprofit trade association for the US IT industry. The best defense against automation, some argue, will be a liberal arts degree.\n\nWords might be easy to generate with AI, but good writing isn't ready for automation.\n\n\"If everyone's a writer, then nobody's a writer, and I think it's very evident right now,\" says Cristin Culver, founder of communications firm Common Thread Communications. LinkedIn is full of posts written by AI in a similar style that makes eyes glaze over as they scroll. \"I think AI is both aiding and making storytelling much harder,\" Culver says. \"Ironically in this era of AI, some of the most poignant storytelling belongs to the people who've realized that everything is sloppified and they've pivoted to very tactical storytelling.\"\n\nAnthropic has been leaning heavily into that tactical, and tactile, storytelling. In the fall, the company created a pop-up Claude Cafe in New York to position the chatbot as a thinking and problem solving partner, marketing the space as one for showing up in person, connecting, and being surrounded by books and magazines over screens (although the company has also destroyed and scanned millions of books to train Claude, which a judge ruled last year was not a copyright violation).\n\n\"Claude is definitely a prominent team member for everyone, but comms people are sort of like BS detectors,\" Sasha de Marigny previously told Axios last May, months before she was promoted from head of communications to become the company's first CCO. \"Critical thinking is still a huge comparative advantage for humans. I'm looking for excellent strategists — people who understand the new world order and know how to develop holistic plans to cut through to the audiences we care about.\" Anthropic declined to speak more about its comms strategy for this story.\n\n\"It's a golden age for people who really enjoy the craft of communications,\" says Steve Clayton, CCO of Cisco, who formerly worked at Microsoft and launched the company's print publication. When he first tried ChatGPT, Clayton says he worried his career was done. He's since become an AI optimist, seeing gen AI as a tool and opportunity for communicators and so-called storytellers to stand out with content that feels authentic content projects that strike people. \"In an environment where nobody's sat at their desk today saying: God, I wish I had more email, or I wish I had more websites I could visit, or I wish I had more podcasts — the challenge is, how do you create something that is worthy of people's time and worthy of their attention?\"\n\nJobs where brands build out their own newsrooms are \"going to be one of the last places where AI is replacing writers,\" says Noah Greenberg, CEO of Stacker, a content distribution company. Unlike traditional media, which relies on clicks, advertising, and subscription to make money off a constant stream of content, \"when brands are investing in the strategy, they're not thinking about: 'Do I break even on an individual piece of content?' They're thinking about: 'How do I create five or 10 really incredible stories every month that get our story out there, that prove and turn us into the authority as a respected party in this space?\"\"\n\nAs with coding and image generation, LLMs are likely to keep getting better. LLMs may write with more voice or sound more human eventually. But the chatbots and agents don't think. They generate creative content without cycling through a creative process. A 2025 Columbia Business School study found LLMs have a bias for \"Option A,\" preferring the first choice when given a list and asked to pick. For people working in comms, AI might be more friend than initially imagined foe — at least because it makes their work stand out.\n\nAmanda Hoover is a senior correspondent at Business Insider covering the tech industry. She writes about the biggest tech companies and trends.",
    "readingTime": 8,
    "keywords": [
      "unemployment rate",
      "so-called storytellers",
      "vibe coding",
      "content",
      "comms",
      "storytelling",
      "communications",
      "company's",
      "jobs",
      "tech"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/hottest-job-in-tech-writing-words-ai-hiring-2026-2",
    "thumbnail_url": "https://i.insider.com/697cf550a645d11881885825?width=1200&format=jpeg",
    "created_at": "2026-02-03T12:35:05.716Z",
    "topic": "finance"
  },
  {
    "slug": "walter-writes-ai",
    "title": "Walter Writes AI",
    "description": "Humanize AI content with Walter Writes. Turn AI text into natural, human-sounding writing that keeps your voice. Check it with our free AI detector.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://walterwrites.ai/",
    "thumbnail_url": "https://walterwrites.ai/wp-content/uploads/2025/03/new-site-social-logo.png",
    "created_at": "2026-02-03T06:37:46.755Z",
    "topic": "tech"
  },
  {
    "slug": "ionselective-interface-engineering-for-durable-electrolysis-of-impure-water",
    "title": "Ion-selective interface engineering for durable electrolysis of impure water",
    "description": "Electrolysis of dirty water is an attractive route to clean hydrogen, but fluctuating surface pH quickly ruins electrodes. Here, the authors report an ion-selective polymer gate that steadies surface pH, bars harmful ions and lets seawater-fed cells run 1500 h with pure-water durability.",
    "fullText": "Long-lasting, monovalent-selective capacitive deionization electrodes\n\n Article\n Open access\n 26 March 2021",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41467-025-66711-x",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41467-025-66711-x/MediaObjects/41467_2025_66711_Fig1_HTML.png",
    "created_at": "2026-02-02T01:11:03.203Z",
    "topic": "tech"
  },
  {
    "slug": "triton-bespoke-layouts",
    "title": "Triton Bespoke Layouts",
    "description": "Hopefully the previous articles covering linear layout concepts and\nexamples facilitate building a solid understanding of the core generic layer powering\nvarious Triton code generation lowering and optimizations.\nNow let’s turn our focus to those bespoke layouts, which we still consistently interact with\nwhen working on Triton compiler internals.\nAdditionally, developers can directly program layouts with Gluon now; writing those bespoke layouts\nis generally more intuitive than linear layouts.",
    "fullText": "Hopefully the previous articles covering linear layout concepts and\nexamples facilitate building a solid understanding of the core generic layer powering\nvarious Triton code generation lowering and optimizations.\nNow let’s turn our focus to those bespoke layouts, which we still consistently interact with\nwhen working on Triton compiler internals.\nAdditionally, developers can directly program layouts with Gluon now; writing those bespoke layouts\nis generally more intuitive than linear layouts.\n\nIn certain places they are also called legacy layouts.\nGiven we still actively use them and there are no plans to deprecate them, I personally prefer\ncalling them bespoke layouts, to emphasize the fact that each one of them is tailored towards a\nspecific need.\n\nOne would ask why we need two sets of layouts and what different purposes they serve, if any.\n\nChronologically, we only have those bespoke layouts at the beginning.\nThey model key hardware tensor ownership patterns in a straightforward manner.\nThey are easy to understand and get the job done for common cases.\nHowever, as the kernel becomes more and more complicated which invites more and more optimizations,\ntheir shortcomings start to become obvious—given each bespoke layout uses its own IR definition\nand underlying mechanism, we need more and more point to point conversion cases.\n\nStarting with the general ttg.convert_layout operation we mentioned earlier as an\nexample, it can have different source and destination layouts.\nWithout a generic mechanism, we need to consider them separately and use different code paths,\nwhich means solving a combinational problem in the space of\nsource layout (blocked, shared, MMA, etc.) * destination layout (blocked, shared, MMA, etc.) *\ndata exchange (intra-thread, intra-warp, inter-warp, etc.).\n\nttg.convert_layout serves as internal bridge inside the compiler for potential data ownership\nexchanges—we insert it as long as we have a mismatch in the type system due to\nlayouts.\nSuch approach gives us localized compiler transformations so easier to manage.\nOn the flip side, it does mean that there can exist lots of redundant conversions; we would want\nto optimize them away if possible.\n\nFurther, from the kernel’s perspective, we write at the block level and process n-D tensors.\nIt’s quite common to perform .permute(), .reshape(), and other shape manipulation\noperations.\nThese operations conceptually bring no cost given they are just creating derivative “views”\nof the original tensor without needing to really shuffle data in the hardware.\nSo we would like to optimize through them if possible when optimizing layout conversions.\n\nFor the compiler to realize the above, it needs to reason and compute how element ownership\ntransfers throughout the kernel code, which would be hard if we don’t have a unified mechanism.\nTherefore linear layout was introduced as a generic underlying mechanism.\n\nGenericity comes with a cost of higher cognitive burden though, as human minds are typically\nfond of vivid illustrations rather than terse theories.1\nEspecially now with Gluon, developers can directly program layouts to get precise control to\noverrule inefficiencies of compiler heuristics.\nSo even as all the Triton compiler internals are transitioning to heavily rely on linear layouts\nfor generic optimizations, bespoke layouts are still great complementary mechanisms.\nIt’s like that we know all high-level programming languages are translated into assembly\neventually, but we still prefer programming the former.\n\nI think I digressed a bit already, but the above are good backgrounds.\nWithout further ado, let’s discuss bespoke layouts.\n\nThe documentation for these layouts is embedded as descriptions in the TritonGPUAttrDefs.td\nfile.\nThe wording there is a bit formal but good to read through.\n\nIn general, there are two categories—distributed and shared layouts.\nThey are defined as TableGen attribute interfaces with a EncodingTrait suffix in their names.\nAttribute interface is a mechanism for different attributes to support the same API interface,\nlike mixins in programming languages.\n\nAmong distributed layouts, those for representing tensor/matrix core unit layouts are special\nenough to merit their own separate interface, named MmaEncodingTrait, for defining common methods\nand processing collectively.\n\nVarious concrete layouts are defined as MLIR attributes, with an EncodingAttr suffix.\nSee the hierarchy in the following illustration:\n\nOne may wonder why defining layouts in such manner.\nTriton uses MLIR’s tensor data type.\nUsing MLIR attributes to define layouts allows us to attach them to tensor as the encoding\nattribute.\nWith this, we basically bolt the layouts on the MLIR type system and carry them in each operation\nso we can reason and propagate them in relatively localized manner for optimizations as said in\nthe previous section.\n\nDistributed layout, as the name indicates, are meant for ownership patterns where\ndifferent hardware units own different tensor elements.\nThis is the case when we use registers to hold the data; it matches and reflects the fundamental\nhardware characteristics that registers are per-SIMD resources and only visible to their owning\nthreads and warps.\n\nA simplified flow of matmul is that we read data from global memory into registers and write to\nshared memory, and then read from shared memory to registers, and finally perform tensor/matrix\ncore operation and write to global memory.2\n\nIf we’d like to design bespoke layouts to support this flow, we would need something to represent\nhow to read/write from/to global memory, and how to manage shared memory, and how to arrange data\nin the manner as expected by tensor/matrix core units.\nThe first is effectively blocked layout, and the third is various vendor-specific MMA layouts.\nThe second we will come to in the Shared Layouts section.\n\nFor global memory access, GPUs have stuck to the SIMT model longer than computation.\nWhile MMA was introduced since Volta in NVIDIA GPUs, it’s only since Hopper that we see Tensor\nMemory Accelerator (TMA).\n\nIn the traditional SIMT model, each thread only read a few elements; we arrange threads into warps\nand then into CTAs3 to collectively progress the whole problem size.\nAlthough each thread uses its own global pointer so we can perform arbitrary gather/scatter style\naccess, the hardware is really designed to be most efficient if all threads in the same warp\ncollectively access consecutively with coalescing.\n\nTo describe the above ownership pattern, we would need to specify how many elements each thread is\nresponsible for, how many threads per warp we have, how many warps per CTA, and so on.\nAdditionally to achieve best performance, we need to arrange threads/warps in nested tiles\nto be consecutive.\nThese are all semantics we want to directly bake into the definition of blocked layout, which\nreflects the current attribute definition.\n\nThe above only specifies an element owning pattern of one “unit” though.\nWhen mapping to a concrete tensor, we would need to address some additional issues.\n\nFirst is how to scale to the exact shape of the tensor.\nThis is straightforward considering the need of consecutive arrangement for performance—we just\ncontinuously tile in a wrap-around manner until we cover the whole tensor4,\nwith each tile called a repetition.\nThis is actually the behavior for all distributed layouts.\n\nFor example, applying a blocked layout of ttgl.BlockedLayout(size_per_thread = [1, 8], threads_per_warp = [16, 4], warps_per_cta = [2, 2], order = [1, 0]) onto a (MxK =) 64x128xf16\ntensor, we can use an awesome tool created by Lixun to visualize (including\nall following illustrations):\n\nThere is a curious order bit in the above.\nThat’s the second issue we need to handle—given a 2x2 warps per CTA for example, there are\nmultiple ways we can arrange the warps.\nSo the definition we had thus far is not enough to pin down the exact distribution pattern.\nSimilarly for the thread arrangements in a warp.\nTo address this, we introduce the order to list dimension indices from fastest to slowest\nvarying.5\n\nApplying ttgl.BlockedLayout(size_per_thread = [8, 1], threads_per_warp = [4, 16], warps_per_cta = [2, 2], order = [0, 1]) onto a (KxN =) 64x128xf16 tensor, we can see the different arrangements\nfor warps.\nAlthough it’s not obvious from the plot, thread arrangements follow similar order pattern like warps\nand they differ from the previous one too.\n\nOkay now we have a good mechanism to describe how we distribute a tensor in threads/warps/CTAs.\nWe can then use it to optimize global memory access.\nThis is performed in multiple steps.\nWe know that Triton IR itself doesn’t concern layouts.\nWhen we convert to Triton GPU IR, we apply a default blocked layout with\na default order (the innermost dimension is the fastest varying) consistent with MLIR in general.\nThen later we use the Coalesce pass to refine.\nJust pointing out the overall flow here to give more context on blocked layout.\nDetails involved are another big topic I won’t dive into for now given the focus is on layouts.\n\nAs a side note, the fact that these bespoke layouts do not encode the tensor shape but auto “scale”\nat shape application time can cause subtle problems.\nAlso, order only gives restricted representation power and it couples thread and warp arrangement.\nThe former is a limitation of bespoke layout in general, while the latter is specific to blocked\nlayout.\nLinear layout makes it better for both issues as it’s explicit about shapes by construction and\nflexible about different arrangements.\n\nBlocked layouts describe tensor element distribution patterns promoting efficient global memory\naccess following the SIMT model.\nBefore we are able to feed the data into tensor/matrix core units for computation, we need to\nrearrange to meet hardware layout requirements, given that they are not SIMT anymore and\nthreads in a warp collectively own a tensor fragment.\nWe touched on this part in the linear layout blog post.\n\nAll the three matrices involved in matmul have predefined element ownership patterns.\nLet’s start with the C matrix, which is captured as the MMA layouts, and then move on to A/B\nin the next section, which is captured as dot operand layout.\n\nDue to the ad-hoc nature of tensor/matrix core units, the element ownership follows a\nvendor-specific exotic manner.\nUsing AMD MFMA layout as an example, we have a version given the instructions\nmay not be portable across generations.\nWe have an instruction shape specifying the exact hardware intrinsic variant.\nThese two fields pretty much determine what a warp unit tile (that is, a single MFMA intrinsic)\nlooks like.\n\nThere is a transpose bit on the layout definition worth explaining.\nIt’s an optimization for cases like chained tl.dot ops and direct global writes.\nUnlike NVIDIA MMA layouts, AMD MFMA layout for C matrix natively requires threads to own consecutive\nelements along the M dimension, which is inconvenient if we want to feed one tl.dot’s result as A\nmatrix to following tl.dot, or directly write out to global memory.\nThe trick is to rely on $C^T = B^T A^T$ to compute the transpose.\nShowing the V_MFMA_F32_16X16X16_F16 hardware intrinsic:\n\nThere is nothing to customize within a warp unit tile.\nBut we can specify how to nest further on top of it with tiles per warp and warp per CTA, like\nother distributed layouts.\nWe can look at examples showing that together with dot operand layout, for the full picture of\ntl.dot computation.\n\nTo describe the A/B matrix layout, we need to know which intrinsic we target.\nWe can get such information by using the C matrix’s MMA layout as the parent layout.\nThen use an index to indicate whether it’s A or B matrix.\nThat’s the main fields in dot operand layouts.\n\nThere is one additional kWidth bit that specifies the number of elements each thread loads\nconsecutively from shared memory.\nIts existence gives us chances to optimize shared memory load with wider instruction.\nIf we look at V_MFMA_F32_16X16X16_F16, each thread only owns (16 x 16 / 64 =) 4 (called\nas kBase here) f16 elements,\nwhich is only 64 bits.\nThe widest shared memory access instruction allows 128 bits.\nGPU hardware nowadays packs very powerful tensor/matrix core units, and the bottleneck is often\nfeeding data to them via memory.\nSo we want to optimize shared memory access via widest instruction.\nWe can set kWidth as 8 to read in 2x (called as kPack here) elements and issue two MFMA\ninstructions to consume them.\n\nThe MMA layout for C and associated dot operand layout for A/B are decided in the AccelerateMatmul\npass.\nWe run patterns on tl.dot ops there and decide on a MFMA intrinsic variant to use based off\nprecision and shape and other characteristics.\nMMA layout and dot operand layout are created out of it accordingly, with ttg.convert_layout\nbridging the type system.\n\nThis decision becomes the layout anchor for compute side.\nTogether with the Coalesce pass, which decides the anchor for memory access side, we establish\n“boundaries” for layout propagation and resolution.\nThe RemoveLayoutConversions pass would work inside this boundary to optimize.\n\nThus far we have discussed the layouts needed for global memory access and tensor/matrix core\ncomputation.\nAnother major component we need to utilize well for high performance is shared memory.\n\nShared memory is visible to all threads in the CTA.\nWe explicitly fetch data into shared memory when software pipelining with multiple buffers to hide\nmemory latency.\nWe can also implicitly use it to do layout conversions in Triton if the data exchange cannot be\ncompleted in an intra warp manner.\n\nWe want corresponding layout mechanisms to represent shared memory usage.\nOne critical aspect it needs to handle well is avoiding bank conflicts for performance.\nGenerally there are two ways to handle bank conflict, via swizzling or padding.\nTherefore, we have two important shared layout variants.\n\nThe previous article already explained the intuition and algorithm\nof using swizzling to address bank conflict—for row #i, perform xor i when indexing.\nHere we can directly focus on the layout mechanism to realize that.\n\nIn order to be flexible, the swizzling scheme is encoded with a few knobs:\n\nThere are a few examples in the description which are quite illustrative.\nOverall they define a scheme on two most fast varying dimensions—order[0] being column, and\norder[1] being row.\nGiven an element at (r, c), phase would be (r / perPhase) % maxPhase.\nThe swizzled column is ((c / vec) ^ phase) * vec + (c % vec).\n\nFor a 32x64xf16 tensor, we can achieve 32-bank conflict free with\nttgl.SwizzledSharedLayout(vec = 8, per_phase=1, max_phase=8, order=[1, 0]):\n\nInitially swizzling is the only mechanism for shared layout bank conflict resolution.\nIt operates on indices with the xor operation, which nicely matches linear layout fundamentals\nand it can cover all NVIDIA GPU needs.\n\nHowever, for AMD GPUs, certain hardware features makes it infeasible/inefficient to use swizzling\nfor handling bank conflict.\nOne such example is GLOBAL_LOAD_LDS_* intrinsics in AMD CDNA4 architecture.\nIt directly writes data from global memory to shared memory without going through\nregisters, which is nice to reduce register pressure.6\nHowever, these intrinsics use one single scalar register to specify the base shared memory\nlocation for the whole warp, mandating the full warp to perform consecutive writes.\nWe cannot perform the scatter-style writes needed for swizzling.\n\nThere are tricks to work around this by effectively “reverse” the\nswizzle scheme onto the global pointers, but that comes as overhead given we need to\nexchange global pointers among threads.\nIt’s more natural to use padding for avoiding bank conflict for such cases.\nTherefore we introduced it and for now it’s only used for the AMD backend.\n\nThe padded shared layout is defined to take a list of interval-padding pairs;\nwe insert the corresponding padding amount after every interval elements.\nMulti interval-padding pairs are supported for flexibility of multi-tiered padding schemes and\nthey compose in an additive manner.\n\nIf we have a shared memory allocation of MxN shape and each thread handles v consecutive\nelements, a general algorithm is to check how many consecutive banks v occupies and make sure\nwe pad an amount of that many banks for every row of N elements.\n\nFor example, using the same 32x64xf16 tensor and vector size of 8, we can pad (8 * 2 =) 16 bytes\n(which is 4 banks) after every (64 * 2 =) 128 bytes.\nPadded shared layout specifies interval-padding values as number of elements.\nSo it would be\nttgl.PaddedSharedLayout.with_identity_for(padding_interval_pairs = [[64, 8]], shape = [32, 64], order = [1, 0]) to give us 32-bank conflict free:\n\nOne big difference compared to swizzled shared layout, other than that we waste a bit of shared\nmemory capacity due to padding, is that the key operation of padding is standard +, not xor.\nThat breaks linear layout fundamentals, if you recall what we discussed earlier in the\nearlier blog post.\nTherefore, padded shared layout cannot be directly converted to linear layout and enjoy the same\nlevel of genericity when doing optimizations.\n\nWe might think it’s a fine trade-off given that shared memory is a special unit which we typically\nonly involve when reading and writing into it, unlike with registers we can perform various kinds\nof optimizations and would want to propagate layouts across.\n\nThough if thinking a bit deeper, we can see that padding only concerns the final physical shared\nmemory allocation with 1-D offsets.\nWe need to consider padding amounts when performing allocation, and adjust indexing to accommodate\nthose “holes” when composing the final linearized offsets into the allocation.\nSuch steps are isolated within converting to LLVM and the information needed are already encoded\nas the interval-padding pairs.\n\nBefore that, when we work at the higher Triton GPU layer, we operate on logical shared memory\n“views” with n-D indexing.\nHere we can still leverage all the (linear) layout facilities to reason about transformations,\nlike transpose and reshape, given they only concern about the\nn-D logical view and element index remapping.\n\nActually, even padded shared layout itself carries a linear component\nthat can remap from the 1-D shared memory offset to logical n-D tensor elements, to give more\nflexibility to achieve certain optimization goals.\nThis linear remapping component is encoded in the linear layout mechanism and plays well there.\n\nOverall the n-D logical indexing vs 1-D physical offset padding is a nice conceptual boundary to\nreason about and leverage the high level transformations and isolate the “breaking” aspects of\npadded shared layout to only allocation and final indexing calculation when converting to LLVM.\n\nThanks for following through till the end!\nThis blog post introduced the overall bespoke layout hierarchy, the pros and cons vs linear layout,\nand then explained some key variants.\nThere are some layouts that I omitted, like slice layout for broadcast/reduction, linear encoding\nlayouts for exposing linear layout directly into Triton GPU IR, and various vendor-specific layouts.\nThose are more targeted and specialized and we can pick up later.\n\nWeirdly though, if you really deeply think linear layout through, you may argue\nthat it’s cognitively simpler than bespoke layouts! ↩︎\n\nThis might be anchoring on earlier GPU generations without fancy async features,\nbut it helps us to build up the motivation and intuition of bespoke layouts. ↩︎\n\nCTA is an NVIDIA term. See this section for translations. ↩︎\n\nIf the tensor’s shape is smaller than the “unit”, it becomes tricker—we\n“broadcast” there. ↩︎\n\nNote that the dimensions themselves are ordered from left to right in ascending manner.\nFor example, for b x m x n, b is dim#0, and m is dim#1, and n is dim#2. ↩︎\n\nAMD CDNA3 architecture also has some GLOBAL_LOAD_LDS_* intrinsics,\nbut it’s missing wider variants like *_DWORDx4 so less usable. ↩︎",
    "readingTime": 16,
    "keywords": [
      "cdna architecture",
      "global_load_lds intrinsics",
      "simt model",
      "mlir attributes",
      "per cta",
      "mfma intrinsic",
      "mma etc",
      "ttgl.blockedlayout(size_per_thread threads_per_warp",
      "threads_per_warp warps_per_cta",
      "thus far"
    ],
    "qualityScore": 1,
    "link": "https://www.lei.chat/posts/triton-bespoke-layouts/",
    "thumbnail_url": "https://www.lei.chat/images/avatar.png",
    "created_at": "2026-02-01T18:21:00.567Z",
    "topic": "tech"
  },
  {
    "slug": "foundry-selfwriting-ai-agent-that-learns-and-upgrades-itself",
    "title": "Foundry – Self-writing AI agent that learns and upgrades itself",
    "description": "The forge that forges itself. Self-writing meta-extension for OpenClaw.ai - lekt9/openclaw-foundry",
    "fullText": "lekt9\n\n /\n\n openclaw-foundry\n\n Public\n\n The forge that forges itself. Self-writing meta-extension for OpenClaw.ai\n\n claw.getfoundry.app\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n lekt9/openclaw-foundry",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.4,
    "link": "https://github.com/lekt9/openclaw-foundry",
    "thumbnail_url": "https://opengraph.githubassets.com/05dcc0d201911d296941ffc33d96a9121b2307111c97c35e6b1e6758a3cd96e9/lekt9/openclaw-foundry",
    "created_at": "2026-01-31T01:04:25.549Z",
    "topic": "tech"
  },
  {
    "slug": "writing-about-music-is-like-dancing-about-architecture",
    "title": "Writing about music is like dancing about architecture",
    "description": null,
    "fullText": "\"Writing about music is like dancing about architecture\" is a maxim used to express the futility of translating music through words.[1] It may be employed as an argument for dismissing music criticism altogether.[2]\n\nThe quote's origin is unknown. It is most commonly misattributed to musicians Laurie Anderson[3] and Elvis Costello.[4] Others, including Costello himself, credit the remark to comedian Martin Mull, although a variation (\"talking about music is like singing about economics\") has appeared in print since as early as 1918.[5]\n\nThe origins of the quote have never been verified. It has been attributed to musicians, entertainers, and writers such as William S. Burroughs, Miles Davis, Thelonious Monk, Charles Mingus, Frank Zappa, George Carlin, Martin Mull, Lester Bangs, David Byrne, Steve Martin, Elvis Costello, and Laurie Anderson.[3][nb 1]\n\nOne of the earliest known usages of the phrase \"dancing about architecture\" appears in a 1979 Detroit Free Press magazine article, where it is attributed to Martin Mull, although this instance is predated by other print sources that contain similar expressions such as \"singing about economics\".[5] A 1918 New Republic article remarks,\n\nStrictly considered, writing about music is as illogical as singing about economics. All the other arts can be talked about in the terms of ordinary life and experience. A poem, a statue, a painting or a play is a representation of somebody or something, and can be measurably described (the purely aesthetic values aside) by describing what it represents.[8]\n\nThe maxim reappeared in a 1921 article penned by academic Winthrop Parkhurst, who wrote,\n\nLike the musical critic who lamented impotently that \"talking about music is like singing about economics,\" those musicians with a knack for literary expression may quite possibly be frightened off from a task which is reputed to be as arduous as turning \"Das Kapital\" into a song.[9]\n\nIn a 1983 interview, Elvis Costello responded to a question about his treatment in the music press by stating, in part,\n\nFraming all the great music out there only drags down its immediacy. The songs are lyrics, not speeches, and they're tunes, not paintings. Writing about music is like dancing about architecture—it's a really stupid thing to want to do.[2][10]\n\nCostello subsequently became widely identified with the quote. In a later interview, he denied having originated the phrase, adding with uncertainty that he may have gotten the line from Mull.[4] Laurie Anderson believed that the \"dancing about architecture\" saying had derived from Steve Martin.[3]\n\nMusic critic Robert Christgau responded to the maxim:\n\nOne of the many foolish things about the fools who compare writing about music to dancing about architecture is that dancing usually is about architecture. When bodies move in relation to a designed space, be it stage or ballroom or living room or gymnasium or agora or Congo Square, they comment on that space whether they mean to or not.[11]",
    "readingTime": 3,
    "keywords": [
      "laurie anderson",
      "steve martin",
      "martin mull",
      "dancing",
      "architecture",
      "singing",
      "economics",
      "music",
      "maxim",
      "musicians"
    ],
    "qualityScore": 1,
    "link": "https://en.wikipedia.org/wiki/Writing_about_music_is_like_dancing_about_architecture",
    "thumbnail_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Martin_Mull.jpg/960px-Martin_Mull.jpg",
    "created_at": "2026-01-30T18:28:27.513Z",
    "topic": "entertainment"
  },
  {
    "slug": "after-writing-about-resellers-making-6-figures-on-ebay-and-poshmark-i-tried-selling-closet-items-heres-why-i-stopped",
    "title": "After writing about resellers making 6 figures on eBay and Poshmark, I tried selling closet items. Here's why I stopped after earning about $100.",
    "description": "I keep a relatively minimal closet, but I have a handful of items I no longer wear. I decided to try reselling. How hard could it be?",
    "fullText": "If I write about a particular side hustle, money-saving hack, or wealth-building strategy long enough, it's only a matter of time before I want to attempt it myself.\n\nI've invested my HSA money like the fiscally savvy do, lived the no-car and roommate lifestyle like the \"super savers\" do, and even hired a life coach in the name of \"investing in myself.\" Most recently, I've been experimenting with e-commerce, launching a pickleball paddle company to see what it actually takes to sell a product online after years of interviewing top Amazon sellers.\n\nI'm intrigued by strategies like rental real estate and generating passive income via content creation, but don't quite have the time, money, or bandwidth to experiment.\n\nReselling items, specifically clothes, on sites like eBay, Poshmark, and Mercari seemed different. If you're starting with items you already have lying around your home, it requires zero money to get up and running; all you need is a smartphone to download the selling apps and take photos of items, plus a bit of your time to list items and fulfill orders.\n\nDespite the seemingly low barrier to entry, there's a lot of upside: I've reported on side hustlers pulling in impressive numbers, including a full-time lawyer who earns $120,000 a year reselling sneakers and apparel on eBay and a millennial who turned her thrifting hobby into a full-time job.\n\nI keep a relatively minimal closet, but I have a handful of items I no longer wear. Instead of donating or tossing them, I decided to try reselling. How hard could it be?\n\nWell, I stopped almost as quickly as I started. Here's what happened.\n\nI downloaded two popular clothing resale apps: Poshmark and Mercari. Of the two, I found Poshmark a bit easier to use, largely because of its AI assistant, which essentially builds a listing for you, including descriptions, titles, and tags, after you upload photos of your item. As someone who doesn't know clothes all that well, this was particularly helpful.\n\nI took four to five photos per item against a clean, white background (a tip from top resellers) and priced them by loosely comparing similar listings. I didn't put too much time or thought into my prices, figuring I could always refine them later if necessary.\n\nAll in, I was up and running in about 30 minutes and spent exactly $0.\n\nHere's what my Poshmark profile looks like:\n\nMy first sale came through Mercari: a striped Madewell sweater I listed for $20. I was thrilled when I got the notification — until I did the math.\n\nAfter an $8 shipping fee and a $2 selling fee, Mercari told me that I'd made $10.03. My profit got knocked down to a measly six bucks after I purchased a $4 mailer to ship the sweater.\n\nI was slightly frustrated (was a couple of bucks really worth the hassle of printing labels and dropping off mailers?), but not ready to quit.\n\nMy next two sales came through Poshmark while I was out of town, which added another layer of inconvenience. I messaged the buyers to let them know I would ship their orders as soon as I returned home.\n\nI saved on shipping materials this time, opting to use mailers I already had on hand. It's also worth noting that, because of my pickleball business, I already had a label printer. Without that, I may not have embarked on this project at all. Figuring out how to print shipping labels at a FedEx \"Print & Go\" likely would have sent me over the edge.\n\nUsing my own supplies, I netted $28 from a gently used Tracksmith crewneck and $38.40 from a new top with the tags still on. I removed the buyers' names for privacy.\n\nOver roughly two months, I made a little more than $100 across five sales. Four came from Poshmark, while one came from Mercari.\n\nFor better or for worse, I had no real sense of urgency. It allowed me to price items comfortably rather than racing to the bottom (I did end up lowering prices on some of my items), but it also meant sales trickled in slowly, making it easy for the project to fall by the wayside.\n\nMy accounts are still active, and I even have a few listings live. If something sells, great. But I'm not planning to add more inventory, and I certainly won't be sourcing items beyond my closet.\n\nReselling isn't the side hustle for me. It doesn't play to my strengths, it tested my patience at times, and, ultimately, selling clothes doesn't excite me enough to justify the time. Unlike the pickleball business, which I genuinely enjoy despite the stress and costs, reselling felt like busywork that I wasn't even very good at.\n\nThe math also only works at scale. Making meaningful money reselling clothes requires volume. It means constantly sourcing inventory, photographing items, managing listings, and shipping orders. That's a commitment I wasn't willing to make.\n\nFor people who love fashion, thrifting, and the thrill of finding a deal, reselling can absolutely be lucrative, as I've reported.\n\nFor me, at least for now, I'm staying in my racket-sports lane.",
    "readingTime": 5,
    "keywords": [
      "pickleball business",
      "items",
      "reselling",
      "i've",
      "money",
      "clothes",
      "shipping",
      "photos",
      "doesn't",
      "listings"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/hustle-culture-what-its-really-like-reselling-clothing-for-profit-2026-1",
    "thumbnail_url": "https://i.insider.com/697a1599d3c7faef0ecd0f2d?width=1200&format=jpeg",
    "created_at": "2026-01-29T12:32:28.559Z",
    "topic": "finance"
  },
  {
    "slug": "when-tipping-becomes-a-customer-experience-problem",
    "title": "When Tipping Becomes a Customer Experience Problem",
    "description": "Tipping has evolved from a hospitality tradition into a widespread expectation across many service sectors, including airlines, medical offices, and self-checkout kiosks. Surveys show that a significant portion of Americans feel tipping culture is out of control, with many annoyed by preset tip screens and the pressure to tip in unexpected situations. The authors recommend that businesses focus on three principles when designing tipping programs: distinctiveness, visibility, and proportionality. These ensure customers only tip for discretionary, observable services and can reward workers based on perceived quality. Solutions like private tipping, eliminating tip pooling, and linking tips to service ratings are proposed to reduce stress and make tipping fairer.",
    "fullText": "When Tipping Becomes a Customer Experience Problem by Mark Bender, Marco Bertini, Oded Koenigsberg and Rob WaiserJanuary 27, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintIf nearly half of your customers were irritated by a business practice, would you treat it as urgent? Customer satisfaction is a key driver of financial performance, yet many companies in service sectors ignore a growing source of frustration: tipping. Once confined to hospitality, requests for gratuities now appear in unexpected places—airlines, medical offices, auto repair shops, logistics providers, even butcher counters and impound lots. Digital payment systems, the gig economy, and the Covid-19 pandemic have accelerated this trend.",
    "readingTime": 1,
    "keywords": [
      "tipping",
      "customer"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2026/01/when-tipping-becomes-a-customer-experience-problem",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_26_512815862.jpg",
    "created_at": "2026-01-27T18:24:22.019Z",
    "topic": "business"
  },
  {
    "slug": "writing-a-net-garbage-collector-in-c-part-6-mark-and-sweep",
    "title": "Writing a .NET Garbage Collector in C# – Part 6: Mark and Sweep",
    "description": "Using NativeAOT to write a .NET GC in C#. In the sixth part, we start implementing the mark phase of the garbage collection.",
    "fullText": "After a long (way too long) break, it’s time to resume our journey towards building a .NET garbage collector in C#. In the previous parts, we saw how to implement the minimal set of GC APIs to allow a simple application to run, and how to lay out the objects in memory to make the heap walkable. We then learned how to find the references of a given managed object. If you need a refresher, don’t hesitate to jump back to those past articles:\n\nIf you don’t have time to read everything, I would recommend focusing on part 4 which explains the layout of the heap.\n\nNow we have all the pieces of the puzzle to start implementing the mark phase of our garbage collection. The goal of the mark phase is to find all the objects that are currently reachable by user code, to deduce which ones aren’t reachable anymore and can be freed.\n\nMarking starts from the roots. That is: references that the GC treats as unconditionally live at the beginning of a collection. The roots can be sorted into three buckets: the local variables and thread-static storage, the GC handles, and the finalization queue. You might also think of static fields, however in practice the static variables are kept alive by GC handles.\n\nWhile the GC is responsible for the last two buckets, the first one is handled directly by the runtime. The IGCToCLR interface exposes a GcScanRoots method that takes a callback. The callback will be called for every local variable. In addition to the callback, GcScanRoots method takes 3 arguments: condemned and max_gen which are only used for some corner case with server GC and so are largely inconsequential for us, and a ScanContext:\n\nLikewise, most of the fields in ScanContext are only useful for server GC (where threads are affinitized to a given heap). We’re only going to use two of them:\n\nDon’t worry, I wasn’t planning on dropping the term ‘conservative mode’ without explaining it. Conservative mode is enabled by setting DOTNET_gcConservative=1. It switches the execution engine from “precise” root tracking (where .NET knows exactly what is a root and what isn’t) to “conservative” root tracking. In conservative root tracking, the execution engine scans the whole stack and reports any value that points to the range of memory managed by the GC. It greatly complicates the work of the GC because any reported root could be a false positive. As I understand, it’s mostly meant for new environments where the CLR isn’t fully implemented and doesn’t support precise root tracking yet. It can also be used to test some new features. At this point, I have no plan to implement support for conservative mode in our custom GC.\n\nInside of our ScanRoots callback, we need to discover all the outgoing references from the given object, then browse the reference tree and mark all the objects that we discover. We’ve already seen in part 5 how to get the reference from a given object, and we implemented it into an EnumerateObjectReferences method. To mark the objects, we need to store somewhere the information that we found a given object. There are a few ways to do that. For instance, on x64 the object header includes 4 bytes of padding to keep the alignment, and we could theoretically repurpose that space. However, I plan to use them later to implement different kind of optimizations for other issues that the GC is going to face. Instead, we’re going to do the same thing as the actual .NET GC. As a reminder, the layout of an object in memory is this:\n\nThe trick is to take advantage of the fact that the method-table is aligned on a pointer boundary. It means that the two least-significant bits of the method-table pointer are always going to be 0 on a 32-bit runtime (and 3 bits on a 64-bit runtime). Whenever it marks an object, the GC just sets the least-significant bit of the method-table pointer to 1. Of course, it must make sure to restore the original pointer at the end of the garbage collection, before resuming the runtime.\n\nAccordingly, we add the Mark and Unmark methods on our implementation of GCObject, which will flip the value of the least-significant bit of the method-table pointer. IsMarked checks the current value of that bit. Last but not least, we introduce a MethodTable property which applies a bit mask to the method-table pointer so that we don’t have to worry about whether the object is marked or not whenever we just want to access the method-table (ideally we should only do so in code paths where an object might be marked, but we don’t really care about that level of performance at this point).\n\nGreat, now we can implement our actual traversal of the reference tree! We use a DFS (depth-first search) instead of a BFS (breadth-first search) under the assumption that we are more likely to sequentially scan objects that are related to each other, which should improve cache locality. We must be careful to not use recursion, as the object graph can get really big.\n\nNotice that for now we ignore roots that are marked with the GC_CALL_INTERIOR flag. Those are interior pointers. Consider for instance the following code:\n\nGetInteriorPointer returns a reference to an int, but that int is stored inside of an array. If the array is ever collected while some code holds that reference, bad things will happen. Therefore, by some mechanism, that ref int (called interior pointer) must keep the whole array alive. This is challenging for the GC because we are handed a pointer to an arbitrary part of an object, and we must somehow find the beginning of that object to mark it. For now, we’re going to just pretend that this problem doesn’t exist, and ignore interior pointers entirely.\n\nAfter we marked the objects that are referenced, directly or indirectly, by the roots, we need to do a full scan of the heap. Whenever we find an object that isn’t marked, we know that this object isn’t reachable anymore and we can clear it. For now, our GC doesn’t reuse memory so we’re just going to clear the memory to make sure that user applications will crash if we accidentally collect an object that is still reachable. To keep the heap in a walkable state, we replace the old object with a free object (free objects are explained in part 4). Also, when we find a marked object, we make sure to unmark it to restore its method-table pointer into its original state.\n\nThe WalkHeapObjects is the same logic as the TraverseHeap method of part 4, but cleaned up and rewritten into an enumerator to be easily reusable.\n\nAnd that’s it! Of course, if you try to run an application with the GC at this stage, it will quickly crash: we have yet to implement support for interior pointers, and we have completely skipped the other two types of roots: GC handles and the finalization queue. This will be the subject of our next articles.\n\nAs usual, the full code is available on GitHub.",
    "readingTime": 6,
    "keywords": [
      "gcscanroots method",
      "finalization queue",
      "execution engine",
      "reachable anymore",
      "interior pointers",
      "garbage collection",
      "conservative mode",
      "reference tree",
      "mark phase",
      "precise root"
    ],
    "qualityScore": 1,
    "link": "https://minidump.net/writing-a-net-gc-in-c-part-6/",
    "thumbnail_url": "https://minidump.net/images/2026-01-27-writing-a-net-gc-in-c-part-6-1.jpg",
    "created_at": "2026-01-27T12:26:47.331Z",
    "topic": "tech"
  },
  {
    "slug": "writing-a-lisp-jit-interpreter-with-graalvm-truffle",
    "title": "Writing a Lisp JIT Interpreter with GraalVM Truffle",
    "description": "Mandelbort benchmark: Emacs (native-comp) runs 10x slower than Java or Truffle\nimplementation...",
    "fullText": "So I’ve been working on an interpreter for Emacs Lisp using the GraalVM Truffle framework for a while now. As I’ve just completed its AST interpreter and a bytecode interpreter, I guess its time to give it a little write-up (or rather, some random babbling of various tricks).\n\nAlthough the Truffle interpreter covered here is for Emacs Lisp only, most of the following should also be applicable to other Lisp dialects and maybe other Truffle-based language implementations.\n\nThe Mandelbrot benchmark above involves quite a lot of floating point operations, which Emacs seems bad at. So you can say this benchmark is cheating and I guess Emacs can perform much better in some fixnum benchmarks that involves only integers or list operations.\n\nNicolas Laurent, a TruffleRuby contributor has written a great introductory tutorial for Truffle, but in case you want a TL;DR:\n\nHow? One may also think of Truffle as a way to instruct Graal to perform super aggressive inlining (called “partial evaluation or Futamura projections“). For example, (+ 1 2) in a Lispy language may parse into an AST as follows:\n\nUpon compilation, the compiler might attempt to inline, like, everything:\n\nThat is, having Graal JIT-compile your AST interpreter is essentially JIT-compiling the interpreted program: we go from an AST to a compiled, constant-folded form.\n\nLet’s have a look at a lengthier example:\n\nThe code above is a barebone bytecode interpreter implemented with Truffle. @CompilationFinal tells Graal to treat instructions as constants when compiling, and @ExplodeLoop tells Graal to unroll loops aggressively. With these, we now have a simplistic bytecode JIT compiler, and the bytecode above partial-evaluates to:\n\nAnd the above further simpifies into:\n\nDepending on the stack frame type, Graal might be able to recognize frame.getInt(0) as a local variable and make further optimization:\n\nGraal offers a tool called Ideal Graph Visualizer (IGV) to visualize compilation graphs. The following graph was produced by Graal running said bytecode:\n\nExactly what we’ve described above.\n\nPartial evaluation alone is not enough for an efficient JIT implementation. For example, we expect (+ 1 2) and (+ 1.0 2.0) to compile to different instructions (maybe ADD and FADD respectively), but for dynamic languages, we don’t always know the argument types at compile time. And this is exactly what speculative compilation (or speculation) can help with.\n\nBasically, without speculation, the compiler might compile (+ x y) into:\n\nWith speculation, the compiler can somehow guess from previous calls what types x, y are of, and put irrelevant paths in a separate interpreted slow path:\n\nThe interpretedSlowPath usually involves invalidating the current compilation and re-specializing the AST node. Truffle languages usually achieve this by using a mutable @CompilationFinal state field and a special compiler directive transferToInterpreterAndInvalidate():\n\nWhenever there is an active state, the compiler treats state as @CompilationFinal and compiles only the active path upon compilation; whenever the state assumptions are invalidated, the specialize method is responsible for invalidating the current compilation and adjusting the state so that the node can be recompiled correctly.\n\nThis pattern allows us to realize tons of advanced JIT techniques, including:\n\nSplitting: A highly polymorphic AST graph, however, can impact performance. One way to handle this is making the some parent nodes polymorphic by deep copying the sub-tree.\n\nHowever, it is definitely not fun having to hand-rolling all these ourselves. Instead, Truffle provides a powerful DSL (read: Java annotations) that generates all the boilerplate for us. Using the DSL, The SpeculativeAdd node above is as simple as a few @Specialization:\n\nThe code above asks Truffle annotation processor to generate a SpeculativeAddNodeGen class implementing specialization and polymorphism logic.\n\nThe above specializes the AST nodes. But to supply values to the nodes, you will need variables. It turns out that Truffle offers VirtualFrame exactly for this purpose. Storing local variables on a Frame, you can specialize the slots to Object or primitive types so as to remove boxing/unboxing costs. See SLWriteLocalVariableNode.java and SLReadLocalVariableNode.java for how easily one can speculate about unboxed storage of local variables.\n\nEmacs Lisp has over a thousand built-in functions, each requiring a dedicated Node implementation. Even if Truffle DSL generates Plus/Minus/Times/...NodeGen for us, we really don’t want to initialize all those functions ourselves:\n\nFortunately, Truffle provides @GenerateNodeFactory. If you annotate your AddNode with it, Truffle generates a AddNodeFactory for you and the code above becomes:\n\nNo, @GenerateNodeFactory is not about creating more FactoryFactoryFactories, and nice things happen when you put nodes in inner classes:\n\nI found this usage in the GraalPy codebase and didn’t see it documented anywhere. But it is definitely worth mentioning here in case anyone also has thousands of built-in functions to implement.\n\nLisp languages offer macros for metaprogramming and dynamic AST generation. For example, an unless form may expand to:\n\nEmacs Lisp chooses to expand macro at runtime. Using Truffle’s replace method, it is quite straightforward to come up with a MacroNode:\n\nHowever, runtime macro evaluation brings some implications impacting the overall interpreter design:\n\nWhether a node is a macro node or a function call node affects the AST. In the unless example above, the AST is as follows:\n\nAll arguments are passed “raw” for macro nodes. If unless were a function, arguments are evaluated before the function call and the AST would be:\n\nNow, with runtime macro expansion, we cannot know for centain (at declaration time) whether (something a b) is a call-node or a macro-node. And the only way is to have a call-or-macro-node that dynamically switches to call/macro nodes at runtime.\n\nTruffle frees us from doing register allocation, but we still need to allocate a frame slot for every local variable ourselves:\n\nFor some languages, slot allocation can be done at parse time, but not with runtime macros:\n\npcase-let is a macro that introduces new let forms, declaring new variables. Since macros expand at runtime, it means we also assign slot numbers to those variables at runtime.\n\nThis is fine: we can always introduce more states in our AST and keeping a slot count is not a big deal. The only problem is that Truffle requires knowing the frame size (total slot counts) at declaration time, which I don’t think is possible with Emacs Lisp. So, we need another trick to handle this.\n\nFor a Truffle frame, we need to know its size before hand. How do we handle a dynamic number of variables? Do you implement variable spilling yourself? Well, I did, only to find that there is better way weeks later.\n\nI discovered this technique from the GraalJs codebase. A suggestion for anyone trying to use Truffle for a real-world language: when implementing a feature, look into GraalJs, GraalPy, Espresso or TruffleRuby for how they implement that.\n\nSo Truffle offers two types of frames: VirtualFrame and MaterializedFrame:\n\nThe technique is quite simple: when introducing new variables, just allocate a new VirtualFrame for them and store the current frame in it. Let’s step through how this program executes with the technique:\n\nWhen Truffle calls this function, it allocates a VirtualFrame according to the RootNode of the function. Typically, one wants one slot for each argument, so the frame should probably be:\n\nEntering the first let scope, we allocate a new virtual-frame@1 for the xx variable. However, since we need to be able to access variables from outer scopes, we need to chain these frames up. The simplest way is to store the upper frame in one of the slots of the new frame:\n\nWell, actually no. Remember? We need to materialize the upper frame before storing it anywhere:\n\nEntering the second let scope, similarly, we now have:\n\nIf implemented correctly (that is, materialized frames are only stored within the “frame chain”), Truffle will recognized the frames as part of the current frame and make them virtual. And, hooray, now we have a dynamically-sized frame as good as a static one:\n\nIn Lisps, control flows, quotes and statement blocks are typically implemented as “special forms”:\n\nIn some lisps, these special forms are more-or-less identified by their corresponding symbols, in that you cannot define your own my-if by aliasing my-if to if. Well, this isn’t the case with Emacs:\n\nAnd it is (or was?) a feature that actually saw some usages:\n\nThe solution is quite simple: since we already have runtime macro-expansion, it is quite straightforward to just also put our special form logic there.\n\nNow we have talked about macros and special forms. Let’s go on to the last list form: function calls.\n\nIn most Lisps, there will be some “primitive” functions that have to be provided by the runtime, like arithmetic operations ((+ 1 x) and (- 1 y)) or list/cons-related functions ((car cons) and (cdr cons)). Semantically, +, -, car, cdr are all functions: one can use funcall to indirect-call them, or pass them to mapcar as a mapping function. However, we really want to some special processing for them: function calls are too costly for common constructs like these:\n\nInstead, we want to inline the addition instructions into the AST graph so that Truffle can specialize each node, hopefully turning each into a simple integer addition operation:\n\nFor subroutines/user-defined functions that are not intended to be inlined, you can still provide basic optimizations for them by using direct method dispatch (and caching) as much as possible, as is covered by some other tutorials out there (Writing a Language in Truffle. Part 3: Making my Language (Much) Faster and Graal Truffle tutorial part 9 – performance benchmarking).\n\nIt isn’t too surprising if we decide this is to be done when runtime macro-expansion happens:\n\nThe code here summarizes how we handle list forms like (maybe-func arg1 arg2): we create a placeholder node for it and defer any analysis until evaluation. When the node is executed, it replaces itself with the correct node implementation, possibly inlining some built-in functions.\n\nThe actual implementation is left as an exercise to the reader.\n\nThere’s one thing still missing in our pseudocode above: it does not handle function redefinitions. Once the nodes are inlined (e.g., (+ 1 2 3) inlined into (int-add (int-add 1 2) 3), the nodes won’t be aware of any function changes and we won’t be able to get back. This is problematic because when we redefine a function, we expect the change to take effect immediately:\n\nAn intuitive way to implement this is to introduce a check:\n\nThis works, and can be quite efficient when you caches the function value container for the variables/symbols (so that you don’t need to look up the symbol evey time). But there’s a more idiomatic way to do this: using Truffle Assumption and CyclicAssumption:\n\nThe advantage of assumptions is that Truffle is aware of it and can turn pull-based stability checks into push-based invalidations. It tracks compilation units that depend on a particular assumption, and automatically invalidates them when the assumption no longer holds. And that means, the currentFunctionStable.isValid() check above is essentially free, because the compiler simply assumes it to be true and can completely remove the other branch:\n\nOne can also use assumptions to constant-fold global variables (which I learned from how TruffleRuby handles some of their variables): assume a constant variable by default, and fall back to a dynamic read when a CyclicAssumption is invalidated too many times.\n\nInternally, GNU Emacs Lisp keeps local variables in a list (Vinternal_interpreter_environment) and it implements variable capturing by keeping the very list in the closure object.\n\nTruffle languages, as is shown above, usually use VirtualFrame objects for local variables, and they have very good reasons to do so: Truffle is aware of VirtualFrame and can inline the costs away. (Still remember the BytecodeInterpreter example above?)\n\nTo add to this, Emacs makes quite heavy use of the environment list in its oclosures (closures with associated metadata) as of Emacs 30:\n\nAn advice oclosure object has four fields: car, cdr, how, props, and we can clearly see these fields are stored right in the environment slot of the closure.\n\nI don’t have a solution for this. Currently, whenever oclosure requests it, my implementation produces a list of captured variables from the parent scope (stored as a MaterializedFrame), but it never propagates the changes backwards. Let’s hope this works. Otherwise, I don’t know, we might have to ditch MaterializedFrame/VirtualFrame altogether.",
    "readingTime": 11,
    "keywords": [
      "ast graph",
      "partial evaluation",
      "car cdr",
      "upon compilation",
      "quite straightforward",
      "ast interpreter",
      "built-in functions",
      "runtime macro-expansion",
      "truffle languages",
      "upper frame"
    ],
    "qualityScore": 1,
    "link": "https://kyo.iroiro.party/en/posts/emacs-lisp-interpreter-with-graalvm-truffle/",
    "thumbnail_url": "/favicon_128.png",
    "created_at": "2026-01-27T06:21:30.349Z",
    "topic": "tech"
  },
  {
    "slug": "gas-town-decisions",
    "title": "Gas Town Decisions",
    "description": "title: i invented 'decisions' yes hello welcome not much to read on the internet today so i am going to write for you i am writing as an u...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "http://fimbaz.blogspot.com/2026/01/title-i-invented-decisions-yes-hello.html",
    "thumbnail_url": "https://blogger.googleusercontent.com/img/a/AVvXsEhHOlKGE5jgYsCQtDNeBp_6rutdhMFG3q3u6cDiJooQBLvWUPE2JL_7f5sARXpWa3lhXZFEQ-45_fpIy42wh4mtQojj3lPm_5XVDolrk3lXKq2yECfVrsI_gt-jws8W1_xFPi-xQQwglH0w7rajLDg7QIS8IbLe0cnP3V0m8s5qzWLwhSMVAdXWllZoZdg=w1200-h630-p-k-no-nu",
    "created_at": "2026-01-26T18:21:38.665Z",
    "topic": "tech"
  },
  {
    "slug": "im-a-cocreator-of-alexa-writing-a-6page-memo-helped-me-decide-to-quit-amazon-and-launch-my-own-ai-startup",
    "title": "I'm a co-creator of Alexa. Writing a 6-page memo helped me decide to quit Amazon and launch my own AI startup.",
    "description": "William Tunstall-Pedoe co-founded Evi, which became Amazon Alexa. Here's why he decided to leave the tech giant and launch a startup.",
    "fullText": "This as-told-to essay is based on a conversation with William Tunstall-Pedoe, 56, a founder and CEO. Amazon's acquisition of his startup and his role at Unlikely AI have been verified by Business Insider. This piece has been edited for length and clarity.\n\nI helped create Alexa, a product that everyone has heard of and most people have used. I'm proud of what we built.\n\nBut by 2016, it was clear that leaving Amazon, which I joined after the company acquired my startup, was the right decision. Continuing to work on Alexa would have been a very different job from building and launching startups, which I love to do.\n\nWhen I was 13, I would go to a college next to my school to use their mainframe, and since then I've been excited by computers and pushing the boundaries of what's possible with software.\n\nI studied computer science at the University of Cambridge and taught there after graduating in 1991, but I felt better suited to entrepreneurship than to academia. If you create something genuinely new in software, it can be on a billion smartphones in six months and truly change the world. That's impact.\n\nI set out to solve what I saw as a big problem. Internet search relied on users guessing keywords to get results, rather than asking natural questions like we learn to do as children. I imagined a world where you could have that same kind of conversation with computers, which led me to found True Knowledge in 2006.\n\nInitially, we tried to build a search engine that would compete with Google, which didn't work. Then, we enabled other companies to integrate our search engine into their own products — but the larger companies didn't. For a time, we focused on SEO.\n\nThe final pivot was building a voice assistant. We created an application called Evi, which launched in the UK in 2012, a year after Apple introduced Siri. We renamed the company from True Knowledge to Evi to match our product.\n\nAs a 30-person startup, we suddenly found ourselves competing with the world's most valuable company. We spent much of that year talking to major players in tech about being acquired. Later in 2012, Amazon bought our company.\n\nJoining Amazon was the right decision. The company invested heavily in the city of Cambridge, where Evi was based, and turned our startup into a major Amazon office. Our voice assistant became one of the company's biggest and most exciting secrets.\n\nMoving from running a small startup to working inside a business with hundreds of thousands of employees, with Jeff Bezos at the top, was a big change, but I loved working there. I split my time between Amazon's offices in Seattle and Cambridge, and enjoyed going back and forth, making things happen.\n\nWhen we launched Alexa, we were taken aback by the response. It was instantly successful. Today, Alexa is a household name. I'm immensely proud of the Evi team.\n\nAmazon is known for using six-page memos instead of PowerPoint presentations to promote clarity of thought. In 2016, I wrote one to help me decide if I should leave Amazon. In the memo, I laid out these facts: I'd delivered everything I could, the acquisition had been an unambiguous success, and so too had the product. At the time, thousands of people were working on Alexa.\n\nAfter about three and a half years at Amazon, in 2016, it was time to go. I wanted to re-enter the startup world.\n\nIt's certainly possible to launch something new within a big organization, and there are real advantages to doing so. When we launched Alexa, it immediately appeared on the front page of Amazon.com, a level of exposure that most startups could only dream of. I expect I'll work at a big company again at some point in my career.\n\nBut if you're trying to do something novel or contrarian, a startup is often better suited. Within a large company, all it takes is one manager deciding that resources are better spent elsewhere for a project to die. At a startup, it's the opposite. Even if 99 venture capitalists say no, you only need one investor to say yes to keep the project alive.\n\nAfter Amazon, I spent time mentoring at startup incubators such as Creative Destruction Lab. Through that, I became an active angel investor, which gave me a broad perspective of the many ways startups succeed and fail.\n\nIn 2019, I launched Unlikely AI, a deeptech startup focused on building neurosymbolic AI. The goal is to combine the powerful but sometimes incorrect machine-learning models with the world of algorithms, where computers are almost always right. The mission of the business is about making AI trustworthy and reliable.\n\nAs CEO, I'm constantly swamped. Running a startup can be stressful, but working on something truly big and ambitious is incredibly exciting.\n\nI sometimes feel nostalgic about working inside a big organization, but I love being in the startup world. For me, leaving Amazon was the right decision. I don't regret it.",
    "readingTime": 5,
    "keywords": [
      "launched alexa",
      "voice assistant",
      "search engine",
      "unlikely ai",
      "true knowledge",
      "startup",
      "product",
      "decision",
      "startups",
      "computers"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/alexa-co-creator-why-i-quit-amazon-launch-ai-startup-2026-1",
    "thumbnail_url": "https://i.insider.com/696e02fdc58df2ecd5ccc166?width=1200&format=jpeg",
    "created_at": "2026-01-25T12:22:41.063Z",
    "topic": "finance"
  },
  {
    "slug": "how-to-articulate-your-contributions-as-a-senior-leader",
    "title": "How to Articulate Your Contributions as a Senior Leader",
    "description": "The higher you rise, the tricker it gets to talk about your wins. You’re often not the one building the financial model, writing the code, or closing the deal anymore. Your team is. So when it’s time to talk about what you’ve accomplished, there’s an uncomfortable tension. How do you claim credit without taking it from the people who actually did the work?",
    "fullText": "How to Articulate Your Contributions as a Senior Leader by Melody WildingJanuary 23, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintVisibility matters at every level, but the more senior you become, the more non-negotiable it becomes. You’re expected to justify your business impact and the larger salary that comes with your title. There are more eyes on you and greater scrutiny on everything you do.",
    "readingTime": 1,
    "keywords": [
      "senior"
    ],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/01/how-to-articulate-your-contributions-as-a-senior-leader",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_23_6824340.jpg",
    "created_at": "2026-01-23T18:19:34.780Z",
    "topic": "business"
  },
  {
    "slug": "announcing-vortex-support-in-duckdb",
    "title": "Announcing Vortex support in DuckDB",
    "description": "Vortex is a new columnar file format with a very promising design. SpiralDB and DuckDB Labs have partnered to give you a very fast experience while reading and writing Vortex files!",
    "fullText": "TL;DR: Vortex is a new columnar file format with a very promising design. SpiralDB and DuckDB Labs have partnered to give you a very fast experience while reading and writing Vortex files!\n\nI think it is worth starting this intro by talking a little bit about the established format for columnar data. Parquet has done some amazing things for analytics. If you go back to the times where CSV was the better alternative, then you know how important Parquet is. However, even if the specification has evolved over time, Parquet has some design constraints. A particular limitation is that it is block-compressed and engines need to decompress pages in order to do further operations like filtering, decoding values, etc. For a while, researchers and private companies have been working on alternatives to Parquet that could improve on some of Parquet’s shortcomings. Vortex, from the SpiralDB team, is one of them.\n\nVortex is an extensible, open source format for columnar data. It was created to handle heterogeneous compute patterns and different data modalities. But, what does this mean?\n\nThe project was donated to the Linux Foundation by the SpiralDB team in August 2025.\n\nVortex provides different layouts and encodings for different data types. Some of the most notorious are ALP for floating point encoding or FSST for string encoding. This lightweight compression strategy keeps data sizes down while allowing one of Vortex’s most important features: compute functions. By knowing the encoded layout of the data, Vortex is able to run arbitrary expressions on compressed data. This allows a Vortex reader to execute, for example, filter expressions within storage segments without decompressing data.\n\nWe mentioned heterogeneous compute to emphasize that Vortex was designed with the idea of having optimized layouts for different data types, including vectors, large text or even image or audio, but also to maximize CPU or GPU saturation. The idea is that decompression is deferred all the way to the GPU or CPU, enabling what Vortex calls “late materialization”. The FastLanes encoding, a project originating at CWI (like DuckDB), is one of the main drivers behind this feature.\n\nVortex also supports dynamically loaded libraries (similar to DuckDB extensions) to provide new encodings for specific types as well as specific compute functions, e.g. for geospatial data. Another very interesting feature is encoding WebAssembly into the file, which can allow the reader to benefit from specific compute kernels applied to the file.\n\nBesides DuckDB, other engines such as DataFusion, Spark and Arrow already offer integration with Vortex.\n\nDuckDB is a database as the name says, yes, but it is also widely used as an engine to query many different data sources. Through core or community extensions, DuckDB can integrate with:\n\nThe community has gotten very creative, though, so these days you can even read YAML and Markdown with DuckDB using community extensions.\n\nAll this is possible due to the DuckDB extension system, which makes it relatively easy to implement logic to interact with different file formats or external systems.\n\nThe SpiralDB team built a DuckDB extension. Together with the DuckDB Labs team, we have made the extension available as a core DuckDB extension, so that the community can enjoy Vortex as a first-class citizen in DuckDB.\n\nInstalling and using the Vortex extension is very simple:\n\nThen, you can easily use it to read and write, similar to other extensions such as Parquet.\n\nVortex claims to do well primarily at three use cases:\n\nThe promise of more efficient IO and memory use through late decompression is a good reason to try DuckDB and Vortex for SQL analytics. On another note, if you are looking at running analytics on unified datasets that are used for multiple use cases, including pre-processing pipelines and AI training, then Vortex may be a good candidate since it is designed to fit all of these use cases well.\n\nFor those who are number hungry, we decided to run a TPC-H benchmark scale factor 100 with DuckDB to understand how Vortex can perform as a storage format compared to Parquet. We tried to make the benchmark as fair as possible. These are the parameters:\n\nThe results are very good. The TPC-H benchmark runs 18% faster with respect to Parquet V2 and 35% faster than Parquet V1 (using the geometric means, which is the recommended approach).\n\nAnother interesting result is the standard deviation across runs. There was a considerable difference between the first (and coldest) run of each query and subsequent runs in Parquet, while Vortex performed very well across all runs with a much smaller standard deviation.\n\nThe times did vary across different runs of the same benchmark, and subsequent runs have yielded similar results but with slight variations. The differences between Parquet v2 and Vortex have always been around 12-18% in geometric means and around 8-14% in total times. Benchmarking is very hard!\n\nThis figure shows the results per query, including the standard deviation error bar.\n\nThe following is the summary of the sizes of the datasets. Note that both Parquet v1 and v2 are using the default compression used by the DuckDB Parquet writer, which is Snappy. In this case, Vortex is not using any general purpose compression but still keeps the data sizes competitive.\n\nVortex is a very interesting alternative to established columnar formats like Parquet. Its focus on lightweight compression encodings, late decompression and being able to run compute expressions on compressed data makes it very interesting for a wide range of use cases. With regard to DuckDB, we see that Vortex is already very performant for analytical queries, where it is on par or better than Parquet v2 on the TPC-H benchmark queries.\n\nVortex has been backwards compatible since version 0.36.0, which was released more than 6 months ago. Vortex is now at version 0.56.0.",
    "readingTime": 5,
    "keywords": [
      "tpc-h benchmark",
      "spiraldb team",
      "standard deviation",
      "lightweight compression",
      "duckdb extension",
      "heterogeneous compute",
      "compute functions",
      "community extensions",
      "vortex",
      "parquet"
    ],
    "qualityScore": 1,
    "link": "https://duckdb.org/2026/01/23/duckdb-vortex-extension",
    "thumbnail_url": "https://duckdb.org/images/blog/thumbs/vortex.svg",
    "created_at": "2026-01-23T12:26:21.509Z",
    "topic": "tech"
  },
  {
    "slug": "emery-dreaming-of-winning-europa-league-after-writing-off-title",
    "title": "Emery 'dreaming' of winning Europa League after writing off title",
    "description": "Aston Villa manager Unai Emery may not believe his team are Premier League title contenders - but he is \"dreaming\" of Europa League glory after reaching the last 16.",
    "fullText": "Aston Villa manager Unai Emery may not believe his team are Premier League title contenders - but he is \"dreaming\" of Europa League glory after reaching the last 16.\n\nVilla defeated Fenerbahce 1-0 in Turkey to ensure a top-eight league-phase finish with a game to spare.\n\nOn Sunday, the Spaniard warned Villa are \"not top-five contenders\" after they lost 1-0 at home to Everton.\n\nShould Villa finish outside the Premier League's top four - or top five if England is awarded an extra qualifying berth - their only hope of Champions League football next season will be through winning the Europa League.\n\nFormer Seville, Arsenal and Paris St-Germain boss Emery has won the Europa League a record four times as manager, last doing so with Villarreal in 2021, and was managing his 100th game in the competition on Thursday.\n\nHe told the media \"we have clear objectives in this competition to be a contender for a trophy\" after his side's victory in Istanbul.\n\n\"[We want] To be a contender in case we need this trophy to play in the Champions League. Through our league [it] is very difficult,\" he added.\n\n\"I am dreaming to be here getting trophies and the Europa is one objective we have this year.\"\n\nDespite being the seventh-most successful club in English football by trophies won, Villa's last piece of major silverware was the League Cup in 1996.\n\nSancho scores first Villa goal to seal last-16 spot\n\nWho can join Aston Villa in Europa League last 16?\n\nAston Villa are on a superb run of form.\n\nThey have won 20 of their past 25 matches (D1, L4) with only Arsenal (21) having won more among teams in Europe's big-five leagues during that period.\n\nTheir blip on Sunday against Everton meant they missed out on the chance to move second, within four points of Premier League leaders Arsenal - Emery's old club.\n\nHe made six changes against Fenerbahce and his side dominated, with the 1-0 scoreline flattering the 28-time Turkish champions.\n\nAfter the match Emery praised his players for \"respecting\" the competition, adding he was \"so, so happy\" with their response.\n\nVilla's match-winner Jadon Sancho scored his first goal for the club on his 19th appearance since joining on loan from Manchester United in September.\n\n\"It's nice to have a manager that backs you and obviously believes in you,\" said the 25-year-old forward who has not played for United since August 2024, and whose contract with them expires this summer.\n\n\"He [Emery] just keeps on telling me to be positive every time I play. Every opportunity I get I'm going to try and do 100% and hopefully I can deliver goals.\"\n\nSancho has only started two Premier League games this season, but in the Europa League he has been in Villa's starting line-up for each of their past five matches.\n\nVilla have played 12 times since the start of December, and by guaranteeing a top-eight league-phase finish avoided a two-legged play-off in February.\n\nEmery still played striker Ollie Watkins for the full 90 minutes on Thursday, and star midfielder Morgan Rogers for 74 minutes.\n\nHe was also able to give Tyrone Mings his first start since November, after the defender recovered from a thigh injury.\n\nAmadou Onana, who has been out with a hamstring issue, was also given a late cameo off the bench and is another key player returning.\n\n\"[We are] building the team with some circumstances not helping us,\" Emery told TNT Sports.\n\n\"But next week we can finish the transfer window and hopefully we can get everything we need to complete the squad for the next months and the competitions we have.\"\n\nLatest Aston Villa news, analysis and fan views\n\nAsk about Aston Villa - what do you want to know?",
    "readingTime": 4,
    "keywords": [
      "top-eight league-phase",
      "league-phase finish",
      "aston villa",
      "europa league",
      "premier league",
      "manager",
      "competition",
      "club",
      "villa's",
      "team"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/emery-dreaming-winning-europa-league-232539127.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/Tt3__fUvnnYWjXSaptvm2A--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU7Y2Y9d2VicA--/https://media.zenfs.com/en/bbc_us_articles_995/08de1c26d373b15c48c72598488be68c",
    "created_at": "2026-01-23T01:00:09.312Z",
    "topic": "sports"
  },
  {
    "slug": "scarlett-johansson-and-cate-blanchett-back-campaign-accusing-ai-firms-of-theft",
    "title": "Scarlett Johansson and Cate Blanchett back campaign accusing AI firms of theft",
    "description": "Hundreds of writers, musicians and performers urge licensing deals instead of scraping creative work\nScarlett Johansson, Cate Blanchett, REM and Jodi Picoult are among hundreds of Hollywood stars, musicians and authors backing a new campaign accusing AI companies of “theft” of their work.\nThe “Stealing Isn’t Innovation” drive launched on Thursday with the support of approximately 800 creative professionals and bands. The campaign includes a statement accusing tech firms of using American creators’ work to “build AI platforms without authorisation or regard for copyright law”.\n Continue reading...",
    "fullText": "Hundreds of writers, musicians and performers urge licensing deals instead of scraping creative work\n\nScarlett Johansson, Cate Blanchett, REM and Jodi Picoult are among hundreds of Hollywood stars, musicians and authors backing a new campaign accusing AI companies of “theft” of their work.\n\nThe “Stealing Isn’t Innovation” drive launched on Thursday with the support of approximately 800 creative professionals and bands. The campaign includes a statement accusing tech firms of using American creators’ work to “build AI platforms without authorisation or regard for copyright law”.\n\nIt adds: “Artists, writers, and creators of all kinds are banding together with a simple message: Stealing our work is not innovation. It’s not progress. It’s theft – plain and simple.”\n\nThe statement urges AI companies to pursue licensing deals and partnerships with the creative industries and acknowledges firms that have taken that route. OpenAI, the developer of ChatGPT, has signed deals with content owners including Disney and the Guardian, while Warner Music Group has struck a licensing deal with AI music generator Suno.\n\nHowever, copyright remains one of the most contentious issues within AI, because the models that power chatbots like ChatGPT or image generators like Grok Imagine rely on vast amounts of data taken from the open web in order to help create their responses. Creative professionals argue that tech firms should seek their permission before using such material – and that they should receive a payment if they give their consent.\n\nOpenAI, and other AI firms, have argued that using material available online is “fair use”, a US legal doctrine that allows use of copyright-protected work without the owner’s permission in certain circumstances. As of last year, dozens of lawsuits had been launched in the US over the AI and copyright issue.\n\nJohansson was dragged into the AI debate in 2024 after OpenAI’s voice assistant used her vocal likeness, prompting the actor say she was “shocked, angered and in disbelief” by the move. OpenAI subsequently removed the voice from ChatGPT.\n\nOther signatories to the statement include actor Joseph Gordon-Levitt, Breaking Bad creator Vince Gilligan and singer Cyndi Lauper. Last year Gilligan described AI as the “world’s most expensive and energy-intensive plagiarism machine”.\n\nThe “Stealing Isn’t Innovation” push has been organised by the Human Artistry Campaign, whose backers include the Writers Guild of America, the Recording Industry Association of America and the actors’ trade union SAG-AFTRA, which went on strike in 2023, partly over the use of AI.\n\nIn the UK, the government has been under fire for proposing that AI firms should be allowed to use copyright-protected work without first seeking artists’ permission, unless they signal that they wish to “opt out” of the process. The technology secretary, Liz Kendall, said this month that the government was seeking a “reset” on these plans via an official review due to be published in March.",
    "readingTime": 3,
    "keywords": [
      "stealing isn’t",
      "isn’t innovation",
      "creative professionals",
      "licensing deals",
      "tech firms",
      "the stealing isn’t innovation",
      "writers",
      "statement",
      "without",
      "copyright"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/jan/22/scarlett-johansson-and-cate-blanchett-back-campaign-accusing-ai-firms-of-theft",
    "thumbnail_url": "https://i.guim.co.uk/img/media/0ec6ae0c43961da5c432045af3e5b8d5e5bcf60a/110_0_2779_2224/master/2779.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ade5f2519a44a382d39f5e5fbc9a5b9d",
    "created_at": "2026-01-22T18:18:47.903Z",
    "topic": "tech"
  },
  {
    "slug": "wikipedia-signs-of-ai-writing-a-vale-ruleset",
    "title": "Wikipedia Signs of AI writing: a Vale ruleset",
    "description": "Taking Wikipedia's dynamic documentation of AI writing patterns and turning it into a prose linting tool.",
    "fullText": "My minor superpower is setting up change detection on websites to get email notifications when they update. For the past three months that’s meant daily pings from Wikipedia’s “Signs of AI Writing” page. As an “advice” page, it’s where Wikipedia editors document the tells: the phrases, patterns and artifacts that suggest that AI was involved at some point with what they’re reading.\n\nMalicious or subversive edits to Wikipedia have been a challenge since the formation of the site. But the community has a robust nervous system for identifying and rejecting those that compromise the integrity of the encyclopedia.1 The latest response is this painstakingly, exhaustively updated page documenting various signs.\n\nThe proliferation of LLM tools means that AI-tainted edits are a new variant for Wikipedia contributors to navigate. If AI tools allow individuals to contribute productively in ways otherwise not possible, they could be welcome. But as the page goes to great lengths to document, there’s a fresh category of edit slop that would deteriorate the quality of the encyclopedia if allowed unchecked.\n\nThe emails summarising the edits give me an interesting perspective on how perception of AI use has morphed rapidly over time. The page documents the front line of AI detection in a real-world setting.\n\nWhat would it look like to turn this into tooling?\n\nMany tells are obvious. Take the glitches that would appear in older versions of ChatGPT outputs.\n\nOnce you understand that this is a strange bit of text blurted out by the model it’s an immediate sign that the surrounding content has been tainted by a model at some point in its history.\n\nThese examples are temporary windows into the textual guts of AI models. The MissingNo. seahorses of ChatGPT.\n\nNow documented, recognised, and patched by the model creators, these artifacts of artificiality are becoming extinct with time.\n\nNudging back into the world of user error, it’s common to see phrasing that’s been accidentally copy-pasted from the LLM user interface and surrounding text.\n\nThis can include instructional framing meant not for the reader but the user.\n\nOther tells become more obvious the more you read online and the more you train yourself to spot them.\n\nThe most famous example of this is ChatGPT’s rampant use of the em-dash, raising the hackles of belligerent punctuation lovers worldwide.\n\nTo me the bigger scourge is that of contrastive language:\n\nIt's not just about the beat riding under the vocals; it's part of the aggression and atmosphere.\n\nThis language construction is catnip to the LLMs at large. It occasionally spans multiple sentences but it almost always appears in marketing and branding content on LinkedIn due to the goal of making an impact.\n\nAs a result this construction is hackneyed and many will choose to avoid it in order to prevent being lumped in with the undiscerning, careless masses.\n\nTechnical writers face similar challenges with AI-generated content outside of Wikipedia. Like any group that cares about craft, they’ve built tools for systematic text analysis.\n\nVale.sh is a library known as a “linter for prose.” It could be thought of as a souped-up spelling and grammar checker.\n\nLinting is a term borrowed from software development (first used back in 1978!) and is the process of highlighting areas for improvement, based on a customisable ruleset. In programming, linters catch “code smells” – implementations that technically work but suggest deeper issues. Many programming languages have syntactic variation meaning that you may write a piece of code differently to your peers. These degrees of freedom can hamper collaboration, or permit confusion, by making it hard to understand the intent behind the code.\n\nSharing these linting rules across teams of developers aids collaboration and goes a long way to avoid holy wars about semicolons.\n\nVale has been around in the technical writing community since 2017 and there’s a strong ecosystem of rulesets that one can “opt-in” to. These include style guides, checks for passive voice, gendered or condescending language.\n\nI’m not aware of any existing implementation of a ruleset to help highlight AI smells, so I built one.\n\nI fed the contents of the Wikipedia page to Claude and asked it to make recommendations on the rules that should be generated.2 After a few confidence checks I gave it the green light to generate Pull Requests on GitHub for manual review and verification.\n\nOne key part of this process was separating the rules that were relevant for a collaborative encyclopedia and those that had wider application in other forms of writing.\n\nAn example of this is the meta-commentary from earlier that reference writing for Wikipedia in the body of the text itself. There’s likely a version of this ruleset that could be tuned for exclusive use for Wikipedia edits but my goal was to provide a general purpose tool with applications elsewhere.\n\nIt’s hard to understand the true ability of the countless AI detection tools on the market. Many appear to be taking advantage of the widespread use of ChatGPT in educational settings to prey on students trying to evade (flawed) AI detection tools.\n\nVendors at various points of the snake oil spectrum are touting the strengths of their products but there’s little neutral and independently verifiable research to back up their claims.\n\nAt this point at the end of 2025 it’s unclear to me whether the use of AI and machine learning models will ever be satisfactory for detecting the use of AI in writing.\n\nUnlike these proprietary AI detection tools with their black-box algorithms and problematic false positive rates, this Vale.sh ruleset is transparent and interpretable. Each rule traces back to observed patterns and the configuration is available on GitHub for review and expansion.\n\nOne interesting challenge is that of linguistic adaptation. As AI writing becomes more commonplace and individuals gain greater confidence in their ability to spot it, certain words and turns of phrase will be avoided to prevent accusations.\n\nThe ruleset can therefore help authentic writers pre-empt this situation and consider avoiding turns of phrase and other tells. This feels like a sorry state of affairs but helpful tooling is one way for writers to be kept informed and in control of their output.3\n\nClaude and I categorised the current rules in the following three tiers. This means that the rule confidence can be matched by the Vale behaviour.\n\nError: Definite AI artifacts – chatbot phrases, technical glitches, placeholders, tracking URLs.\n\nWarning: Likely AI patterns – hedging clusters, knowledge cutoff references, enumeration style.\n\nSuggestion: Suspicious but common in human writing – vocabulary, transitions, passive voice, symbolic language.\n\nWhile powerful, the Vale configuration isn’t as expressive or flexible as required for some advanced AI detection constructions. One example is the inability to specify stray Markdown syntax outside of areas where Markdown syntax is being used. Without this ability this rule would flag any and all use of Markdown which would be useless.\n\nNow that the rules are shared I want to see how writers and editors use these to understand the writing they produce and review.\n\nThere’s an ecosystem of tooling that can use the Vale rulesets including the flexible command line interface and a Chrome browser extension. It would be interesting to enable the use of the rules in other form factors. For example, it would be helpful if the rules could be run server-side on websites to avoid the need to install Vale on your local machine.\n\nAnother possibility would be to support alternative versions of the rulesets for the other available prose linters.\n\nThe rules will need to adapt over time to reflect changing AI dialect and GPTisms – it’s unclear whether these rules will be relevant or productive in a year.\n\nOthers have attempted to compile lists of “slop words” to guide AI tools away from cliché. I could consider merging these with my current ruleset but need to consider the selection criteria for including each word.\n\nI ran my new ruleset against this article (the one you’re reading now) and… nothing.\n\n✔ 0 errors, 0 warnings and 0 suggestions in 1 file.\n\nIt’s oddly satisfying that nothing was flagged – although it could be that I naturally edited out anything that whiffed of AI along the way.\n\nAs it stands, the ruleset is mostly generated from the “Signs of AI Writing” page and it’s therefore released under the same CC by SA license. This means you can create further derivative works as long as you credit the source and keep the same license.\n\nYou can find the ruleset over on GitHub with instructions for getting started. Comments, feedback and pull requests are welcome.\n\nSee the 404 Media coverage of the new “Speedy Deletion” policy to avoid red tape when content is substandard due to the use of AI. ↩\n\nYes, I see your raised eyebrow. I’m using AI to build a tool to understand the use of AI, the irony hasn’t escaped me. Is there a risk that the use of Claude will bias the whole project making it useless? It’s something to consider but I think this reflects the reality of the widespread use of these tools: with the time I have available would I choose to do this work unassisted? At this point my answer is no. ↩\n\nI’m editing this in iA Writer which has built-in “style check” functionality that flags clichés and fillers. I don’t always accept the suggestions it makes but it’s helpful to understand what it flags. ↩",
    "readingTime": 8,
    "keywords": [
      "markdown syntax",
      "passive voice",
      "it’s unclear",
      "detection tools",
      "ruleset",
      "rules",
      "page",
      "understand",
      "there’s",
      "vale"
    ],
    "qualityScore": 1,
    "link": "https://ammil.industries/signs-of-ai-writing-a-vale-ruleset/",
    "thumbnail_url": "https://ammil.industries/open-graph/signs-of-ai-writing-a-vale-ruleset.png",
    "created_at": "2026-01-21T18:30:42.565Z",
    "topic": "tech"
  },
  {
    "slug": "gastown-and-where-software-is-going",
    "title": "Gastown, and where software is going",
    "description": "Gastown hints at the future of software: agent-driven workflows where CI, guardrails, and shared truth matter more than writing code faster.",
    "fullText": "Gastown is awesome. Like really fucking awesome.\n\nIt’s 70% real, 70% performance art, and 70% a video game. It has vibes, characters, rituals, politics, and this ambient pressure that makes you feel like you’re participating in something bigger than “a tool that edits files.”\n\nAnd it’s also a glimpse of where software is going.\n\nI saw the memes, liked them, and made my own. Then I did the unthinkable: I spent a full day inside it, and my brain got rewired. It was a Quentin Tarantino movie — slow, then suddenly I’m watching the world of software development disappear, and then everything goes down in flames.\n\n10/10. Mesmerizing. Because that whole experience is the point.\n\nGastown goes beyond being a cute agent demo. It’s a prototype of a new interface for software work. It’s almost there. And the gap between “almost there” and “actually there” is basically the next decade.\n\nThe first time you drop in, it feels like somebody turned half-formed future ideas into a living diorama. You don’t open an IDE. You don’t get a “Welcome! Choose a template!” screen. You get tmux, and that’s instantly the best decision anyone has ever made.\n\nThe UX is literally just… being in a terminal. A control room: little windows, and those windows contain agents doing things. And the wild part is: you can watch it. You can watch them talk. If you want to check on the Deacon, you can ask the Mayor, or just tmux over and look.\n\nMessage passing between agents is the most compelling “agent thing” I’ve seen in a long time. Not “AI wrote code faster.” Coordination. Distributed work.\n\nIt feels like autonomous little machines sending each other sticky notes at warp speed: negotiating tasks, updating each other, shuffling responsibilities like they’re running a tiny company inside your laptop. It’s hypnotic. Like watching ants build a bridge, except the ants are arguing about merge conflicts.\n\nAnd yes: worktree management works. Parallelism without file-stomping is the difference between “agents are a party trick” and “agents are a strategy.”\n\nOn the best day, Gastown feels like: “Oh no. This is the interface.” And then…\n\nAfter a full day, I hit the other side of the experience. The refinery crashed. The dogs went AWOL. The deacon died trying to fix it.\n\nI’m not exaggerating. That’s what it felt like. Not “a process died,” but “the city’s power grid collapsed, and now half the institutions are lying about what’s real.”\n\nI tried everything: uninstall, nuke, restart — the modern debugging ritual of “what if I just remove reality and re-add it?”\n\nNope. Right now, the refinery never comes up, and the mayor forgot he’s even in Gastown.\n\nI’m sure it’s fixable. But I don’t want my workflow to depend on local daemons taking over my machine. And even if you move them to the cloud, the core design still isn’t what I want.\n\nBecause bugs aren’t the entire problem. It’s that the system is a chaotic vibe engine that sometimes moves in the right direction. I want more order to the chaos, and I don’t want my local filesystem to be the source of truth. I want the forge to be the source of truth.\n\nWhen Gastown worked, it felt alive. When it broke, it felt like the concept of truth got corrupted.\n\nThe reason is simple: Gastown is local-first. Local-only, really. And local-first is charming right up until you remember what software actually is.\n\nSoftware is a multi-human coordination problem. Teams don’t run on vibes. They run on shared truth: PRs, issues, history, CI results, approvals, audit logs. That’s where accountability lives. That’s where decisions are visible.\n\nIf the system’s state lives only inside my laptop, it’s not a city. It’s a snow globe.\n\nLocal state is fog. Remote state is a contract.\n\nLet me be clear: I loved this thing. Even as it exploded.\n\ntmux is the right UI. It’s not pretending to be a better IDE. It’s an orchestrator, and it looks like one.\n\nWatching message passing is the new magic. It’s not “autocomplete but bigger.” It’s a new primitive.\n\nWorktree management works. Foundational.\n\nAt some point I hit the classic problem every real project is built out of: Where is the actual code?\n\nNot “haha I’m lost.” More like “my brain can’t form a consistent model of what the repo even is right now.”\n\nNested .gitignores. Weird assumptions. Paths that felt like a dream you have when you fall asleep reading a Bazel config. Repo mixed with Gastown config. Hundreds of CLAUDE.mds and settings files scattered everywhere.\n\nAnd this matters, because agent tools have to master the real future: ancient monorepos, half-migrated build systems, 14-year-old shell scripts, and “temporary” hacks from 2019.\n\nI also noticed the tell-tale red X next to the last commit merged to main. Maybe I’m OCD, but it’s hard to believe the future of software development is being built in a repo with failing CI. Honestly, I’d believe it more if it had no CI.\n\nI’m sorry, Steve. I love you. But beads just aren’t for me. Maybe they’re brilliant. But right now, they feel like an opaque, weird filesystem on my laptop, powered by a daemon that keeps taking 70% of my CPU.\n\nGastown is a preview of agent programming, but the future isn’t “code gets written faster.” The future is: change gets shipped faster. And those are not the same thing.\n\nTeams are trapped because CI takes forever, tests are flaky or missing, review cycles are slow, merge conflicts pile up, confidence is low, production is fragile, and there are “oh god please don’t touch that” zones nobody wants to own.\n\nIf this plays out the way I think it will, software engineering turns into CI engineering.\n\nCode is cheap. Green CI is priceless.\n\nIn the agent era, your ability to generate change is basically unlimited. The limiting reagent is confidence.\n\nThe teams with the best, fastest, highest-confidence CI will be able to point swarms of agents at problems and just click merge — not because the agents are smarter, but because the rails are better.\n\nGuardrails used to feel like bureaucracy: approvals, tests, security checks, linting, policy. Now, guardrails are the only thing that makes scale possible. Guardrails are the price of delegation.\n\nHaving no guardrails is like bowling in the dark with the bumpers removed.\n\nGuardrails are bumpers: you bounce around, but you don’t instantly die.\n\nGuiderails are when the bumpers move so close together that the ball can’t wander. It drops into one track and rolls downhill at full speed.\n\nThat’s the shift: constraints stop being “stuff that slows you down” and become geometry that makes speed possible. It’s not faster typing; it’s faster, safe movement.\n\nGastown is an insanely compelling local city. But it’s local-first in a way that makes it fragile and trapped. When it breaks, it doesn’t feel like “a bug.” It feels like the city lost consensus reality. And the deeper reason is: guiderails require hard surfaces. You can’t build rails on fog.\n\nLocal state is fog. Remote state is concrete.\n\nPRs, checks, review policy, merge history, CI results, audit logs: that’s where teams already store truth. That’s the structure you can actually bolt an agent system onto.\n\nThat’s also why I built multiclaude. Multiclaude is my first step on the remote-first journey: keep the part that works (multiple agents coordinating), keep the ergonomics (terminal-first, multiplexed, legible), but orient it around shared reality rather than laptop vibes.\n\nNot “a worldstate database implied by JSON and daemons.” Actual repo state. Actual PR state. Actual CI outcomes.\n\nBecause if the state lives only inside my laptop, it’s not a city. It’s a snow globe.\n\nTasks should be tracked in PRs (or chains of PRs), because that’s the unit of collaboration teams already understand.\n\nPRs are visible. Durable. And most importantly: they don’t go in until CI is green.\n\nIn the future I want, the Refinery isn’t a local daemon that forgets its job. It’s a conductor. It watches the PR queue and keeps the train moving. It escalates stalled work, manages conflicts, and nudges things forward.\n\nBut with one absolute rule: CI is sacred.\n\nThe Refinery can never change CI. It can’t cheat. It can’t ship through failing tests. The future only works if the gates are real.\n\nCI + gates + millions of tokens turns this from random thrashing into a Brownian ratchet: most PRs can be garbage, but the improvements accumulate, and the failures don’t.\n\nThat’s why Gastown is so exciting. It feels like the first real attempt at an interface for agent work that’s fun, legible, and alive. It’s not “ChatGPT, write me a function”; it’s a world of coordination.\n\nBut the future won’t be won with a dozen Claudes on one laptop. It’ll be won by whoever connects this experience to the institutional machinery of software: CI, review, merge policy, durable state, shared truth.\n\nThe best teams won’t be the ones who write code the fastest. They’ll be the ones who can click merge with the most confidence.\n\nAnd when code is cheap, guardrails are solid, and CI is sacred? Yeah. You really will be able to point a swarm of agents at a problem and just slide down the guiderails into shipping. That’s where software is going.\n\nAnd Gastown is the first place I’ve been that feels like it’s already there—even while it’s still catching on fire in front of me.\n\nThis is the future of software development, and we’re not going to sit on the sidelines.\n\nWe’re going to be using agents internally everywhere: to generate changes, review them, keep PRs moving, unblock humans, and turn the boring parts of software into background radiation.\n\nBut we also care deeply about trust. Agents don’t get to be magic. They don’t get to be a black box that “just ships stuff.” If we’re going to scale change, we also have to scale confidence: shared truth, shared visibility, and a shared understanding of what’s happening and why.\n\nSo we’re being as transparent as possible. We’re showing the messy parts. The parts that feel like a playable demo of the future, and the parts that catch on fire. We’re building in public (at least internally), naming the sharp edges, and bringing everyone along on the ride.\n\nBecause if we’re going to build the next era of software, it should be legible. And it should be ours.\n\nMulticlaude is available here: https://github.com/dlorenc/multiclaude\n\nRunning Renovate as a GitHub Action (and NO PAT!)\n\nAdrian Mouat, Staff Developer Relations Engineer\n\nMaking time: Space to think, build, and create (or, This Shit is Fun!)\n\nDustin Kirkland, SVP of Engineering\n\nThis Shit is Hard: Keeping Chainguard OS lean, current, and secure — the power of garbage collection\n\nJames Page, Principal Software Engineer, and John Slack, Senior Product Manager\n\nDoing our best work: Chainguard’s engineering principles in practice\n\nDustin Kirkland, SVP of Engineering\n\nIt’s time to rethink golden images. Chainguard can help.\n\nSam Katzen, Staff Product Marketing Manager\n\nWhy building from source matters\n\nDustin Kirkland, SVP of Engineering",
    "readingTime": 10,
    "keywords": [
      "fog remote",
      "dustin kirkland",
      "kirkland svp",
      "logs that’s",
      "worktree management",
      "audit logs",
      "snow globe",
      "it’s snow",
      "laptop it’s",
      "city it’s"
    ],
    "qualityScore": 1,
    "link": "https://www.chainguard.dev/unchained/gastown-and-where-software-is-going",
    "thumbnail_url": "https://images.ctfassets.net/l47ir7rfykkn/oc8SB07UHEFHgn4XvaT4c/3856c54cd98ec0033e56d7b78cf4df8c/Gastown.png",
    "created_at": "2026-01-21T12:26:59.012Z",
    "topic": "tech"
  },
  {
    "slug": "early-webinspired-writing-platform",
    "title": "Early web-inspired writing platform",
    "description": "A minimalist writing platform. No likes, no comments, no algorithms. Just write.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://writing.ink/",
    "thumbnail_url": "https://writing.ink/og-image.jpg",
    "created_at": "2026-01-19T06:24:31.161Z",
    "topic": "tech"
  },
  {
    "slug": "the-fashion-industry-that-is-tech",
    "title": "The fashion industry that is tech",
    "description": "Writing at the end of the world, from Hveragerði, Iceland",
    "fullText": "It’s now been roughly six months since I slowed down this newsletter to give me space to think and reconsider.\n\nI’ve done this before – several times in the history of this website, in fact. Sometimes the blog has stayed sporadic for a couple of years. Sometimes only a few weeks passed until I found my motivation again.\n\nWriting just for yourself is its own kind of fun. Whatever else is going on in my life, even when the mental landscape that is my blog or newsletter is lying fallow and slowly regaining its fertility, I still update it occasionally because writing is both fun and a tool that helps me make sense of the world around me.\n\nBut writing – any media – only reaches its full potential if it’s done with an audience – not just for an audience, but written as a part of the field or community in discourse with others.\n\nAnd that kind of writing has to have a purpose. It needs to be of service and add value to the field that frames it.\n\nThat’s the bargain. That’s why people read what you write and, occasionally, help support the writer in some way.\n\nWhen you feel that you’re burning out on writing, one of the possible reasons is that the bargain that’s specific to the writing you’ve been doing isn’t working for you.\n\nWhen an audience wants anger and hostility, feeding that can be the writing bargain you offer and the audience will often reward you with attention, praise, and waves of hostility that lift you up, as if it were support, because the poison is directed elsewhere.\n\nBut the poison is still there. Feeding hostility in others requires feeding the hostility in yourself.\n\nThis is why I’ve been struggling for a while now to write constructively about tech and “AI” specifically. Constructive criticism has a purpose, but we’re at a point in the current iteration of “irrational exuberance” where the stakes for the pro- side are so high that nothing constructive registers and what we’re left with are anger and the expert chroniclers of “just how bad is it anyway?”\n\nAcademics and researchers who specialise in machine learning and related fields catalogue the hype and promises. The few genuinely numerate journalists, such as Ed Zitron, do the maths on just how big and potentially catastrophic the bubble is. We’re at a point where the finances, environmental impact (which is currently expressing itself mostly on a community level), workplace dysfunction, and outright debasement of the entire software industry’s user experience mean that there is simply no reasonable justification for supporting these tools. They are obviously destructive.\n\nSensible criticism is unnecessary in this environment. You need chroniclers – the experts who are cataloguing the various examples that show just how damaging these systems are – and you need the analysts – the journalists who can tell us just how many orders of magnitude the industry numbers are off by – but there is little outright need for low-key constructive criticism.\n\nBecause the correct response to those who are explicitly collaborating with authoritarians and fascists, as much of the “AI” industry is doing, isn’t to tell them that their tech is likely to cause substantial delays at the organisations that adopt it.\n\nThe correct response is to tell them to fuck off and then move on.\n\nWallowing in the anger isn’t productive. Mockery always works, but I’ve never been much of a humour writer so I already know that I should be leaving that kind of writing to others.\n\nSo I had to do a bit of contemplation. I’m not a particularly fast thinker. I tend to linger on details for too long and overthink things, but with enough time I get there and I think it’s time to put this newsletter back on a schedule. I’m putting together something that wraps up my coverage of “AI” specifically, at least for now, but other than that I’d like to continue my explorations of the impact of and problems caused by tech, but with a slight shift in focus:\n\nSoftware as a mass medium and the effect it has on other media.\n\nThe reason is straightforward: pretending software isn’t a medium, pretending that it doesn’t have \n\nIt doesn’t work for those of us who have been making software.\n\nIt doesn’t work for those of us who need to use the software.\n\nIt doesn’t work for the societies that have, inadvisedly, come to depend on what’s effectively a bunch of flared jeans designers cosplaying as engineers as the fundamental drivers of their economies.\n\nPretending this made-up bullshit makes sense doesn’t work any more. At least, it doesn’t work any more for those of us who have avoided getting caught up in the bubble irrationality.\n\nThe best way to deal with software – making it, using it, dealing with it as a creative medium, coping with it as a maker of other creative media – is to understand it as it really is:\n\nA creative medium driven by fashion that operates with limited regard for practicality or productivity.\n\nThis isn’t far off from what I was already doing. Systems-thinking is still one of the lenses I apply and I’m firmly of the school of media analysis that takes business factors and processes into account, so it’s mostly a question of broadening the focus slightly, coupled with a small change in perspective.\n\nMaybe it won’t work and I’ll have to rethink things again, which is fine. Maybe it’ll only work for a few weeks until the topic runs out of steam, which is also fine.\n\nBut at least it should be a bit fun.\n\nI’ve finally released print versions of all my books:\n\nIf you’d like to support this newsletter and make it likelier that I continue to publish in print in the future, this would be the time to order a copy. 🙂",
    "readingTime": 5,
    "keywords": [
      "bargain that’s",
      "correct response",
      "constructive criticism",
      "creative medium",
      "doing isn’t",
      "software",
      "doesn’t",
      "it’s",
      "newsletter",
      "i’ve"
    ],
    "qualityScore": 1,
    "link": "https://www.baldurbjarnason.com/2025/the-fashion-that-is-tech/",
    "thumbnail_url": "https://v1.screenshot.11ty.dev/https%3A%2F%2Fwww.baldurbjarnason.com%2F2025%2Fthe-fashion-that-is-tech%2F/opengraph/",
    "created_at": "2026-01-17T00:56:11.459Z",
    "topic": "tech"
  },
  {
    "slug": "im-a-federal-attorney-ive-leveraged-my-white-house-experience-into-a-side-hustle-that-makes-70000-annually",
    "title": "I'm a federal attorney. I've leveraged my White House experience into a side hustle that makes $70,000 annually.",
    "description": "A former White House attorney earns about $70,000 a year writing resumes, charging $2,000 per client while working a federal job.",
    "fullText": "This as-told-to essay is based on a conversation with Jennifer Clinchy, owner of Clinchy Career Consulting. It has been edited for length and clarity.\n\nI was a White House attorney for nine years, spanning three different presidents (Obama, Trump, and Biden). Six years ago, I started working for the National Oceanic and Atmospheric Administration, but I still get teary-eyed when I think about my time in the White House. As the daughter of immigrants, I never dreamed I'd be reporting to work on Pennsylvania Avenue each day.\n\nI loved that some of the brightest people in the country were my colleagues. I hold a dual degree in law and policy, and one of my responsibilities at the White House was to prepare presidential appointees for their Senate confirmation hearings.\n\nIn that role, I reviewed hundreds of résumés and cover letters. I helped people stand out in what's essentially a very high-stakes job interview. Working with these incredible candidates who applied for powerful and influential positions led me down an unexpected career path: résumé writing.\n\nMy parents raised me to be self-sufficient. They were both hardworking. My dad taught me that education is particularly valuable because it can never be taken away from you; No matter what happens, you'll always have your education.\n\nI attended law school in Washington, D.C., where the federal government is omnipresent. I was drawn to federal jobs not only because of the perceived security, but because I felt they mattered. With this work, I could help people, defend justice, and protect the environment. After about 14 years as a federal attorney, my salary is about $186,000.\n\nI'd been reviewing résumés for friends, colleagues, and mentees for years. Then, in early 2020, one of my friends said, \"You're really good at this. Have you ever considered it as a career path?\"\n\nHis question made me realize I had this latent talent that I'd never really considered harnessing. Soon after, the pandemic hit. I was used to traveling about twice a month for work, and when that stopped, I had time to spare.\n\nI signed up for Fiverr and started writing résumés. Initially, I set my prices low, but I soon realized I wanted to emphasize quality over quantity. Today, I charge $2,000 for a résumé and cover letter. I spend many hours reviewing each client's history and experience to determine the best way to present them in the most favorable light.\n\nOver the past five years, I've earned more than $260,000 on Fiverr. I also work directly with clients, so my average annual income from résumés over those years has been about $70,000.\n\nI now live in Seattle, where the cost of homes is very high. I worried I wouldn't be able to afford a home on just my federal salary alone. The résumé business has enabled me to purchase a historic home and complete the costly renovations it requires, including new flooring, a new roof, and an earthquake-resistant system.\n\nEssentially, the bulk of my résumé income goes toward maintenance and repairs on my home. My federal job pays the mortgage and other essentials.\n\nDuring the recent federal shutdown, I was furloughed, and résumé writing became especially important. I generated about $18,000 in résumé writing over those six weeks.\n\nAs an attorney, I work roughly 9-to-5. When I get home, I spend about two to three hours most week nights writing résumés. On weekends, I work on résumés for anywhere from five to 20 hours, depending on my workload.\n\nIf that sounds exhausting, it's because it is. I don't have much free time, but I'm making a conscious choice to spend it this way. I prioritize sleep and exercise, especially as I age.\n\nThe money from résumé writing is wonderful, but what really keeps me engaged are the clients. I worked with a food scientist who held a patent on a famous potato chip, and the vice president of a record label who worked with artists I hear on the radio. My most meaningful job was helping a person living in a domestic violence shelter get a job for the first time, which helped her secure a better future for her family. There's no way to put the value of that into words or dollars.",
    "readingTime": 4,
    "keywords": [
      "career path",
      "white house",
      "résum",
      "résumés",
      "federal",
      "attorney",
      "hours",
      "colleagues",
      "cover",
      "essentially"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/former-white-house-attorney-charges-2000-for-resume-writing-2026-1",
    "thumbnail_url": "https://i.insider.com/695e822a64858d02d217e870?width=1200&format=jpeg",
    "created_at": "2026-01-16T18:19:04.870Z",
    "topic": "finance"
  },
  {
    "slug": "training-large-language-models-on-narrow-tasks-can-lead-to-broad-misalignment",
    "title": "Training large language models on narrow tasks can lead to broad misalignment",
    "description": "Finetuning a large language model on a narrow task of writing insecure code causes a broad range of concerning behaviours unrelated to coding.",
    "fullText": "Generating reliable software project task flows using large language models through prompt engineering and robust evaluation\n\n Article\n Open access\n 08 October 2025",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41586-025-09937-5",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09937-5/MediaObjects/41586_2025_9937_Fig1_HTML.png",
    "created_at": "2026-01-16T06:20:12.839Z",
    "topic": "tech"
  },
  {
    "slug": "i-made-over-30000-from-my-side-hustles-this-year-the-extra-money-is-great-but-i-felt-like-i-never-stopped-working",
    "title": "I made over $30,000 from my side hustles this year. The extra money is great, but I felt like I never stopped working.",
    "description": "I made over $33,000 from freelance writing and teaching mahjong, but the time commitment hurt my relationships and work-life balance.",
    "fullText": "I majored in journalism in college, but fell into tech sales after graduation. Over the past 17 years, I've worked my way from cold-calling as a sales rep to a Director of Sales. But after having kids I started craving a creative outlet and decided to start writing again in 2024.\n\nI didn't set out to make money from it. My first several pieces were unpaid personal stories for a parenting website. When the editor reached out and asked if I wanted to try freelancing, it gave me the confidence to see if I could turn my writing into a side hustle. In 2024, I made $8,995 from various publications.\n\nIn 2025, I spent more time and effort freelancing and made $30,411. I also started teaching American mahjong lessons, which brought in an additional $2,902. All in all, that's just over $33,000 before taxes.\n\nI love my side hustles, but I've had to make major trade-offs with my personal life and don't know if the work is sustainable.\n\nTaxes in the freelancing world are higher than in corporate jobs, so I take home a lower percentage of my earnings. You pay Social Security and Medicare taxes on top of your income tax for freelance work, taxes that an employer typically pays. Still, I'll probably net close to $20,000 from my side hustles in 2025.\n\nThese gigs aren't necessary to make ends meet. I'm fortunate and privileged that my husband's and my W-2 jobs are enough to provide for our family. I choose to pursue them because I genuinely love doing them.\n\nBecause of that, I consider my side hustle money my \"fun\" money. It's become the guilt-free cash that I use for things like specialty workout classes, new clothes, or to put toward travel with friends. I've also saved some of it, too. It's rewarding to see the extra income because it is a lot of additional work.\n\nI work for a local mahjong organization, so I don't have to do any marketing or admin work. People request lessons and then instructors \n\nBut the freelancing world requires a lot of hustle. It's effectively the same as cold-calling in sales to get an assignment.\n\nI pitched at least two new story ideas every day in 2025. That meant brainstorming ideas, researching potential sources, and crafting pitch emails to various editors. The work paid of,f and I wrote for 19 different publications last year, something I'm really proud of, but it was a daily grind.\n\nI averaged about three writing assignments and one mahjong lesson each week last year. I needed to work around my corporate job, so most of my writing work happened before my kids got up or after they went to bed. That typically meant writing from 5:30 to 7 a.m. and then again from 8:30 to 10:30 p.m. Lunch breaks became the time I'd schedule interviews, so I'd eat at my desk to compensate.\n\nMost of my mahjong lessons happen on weekday evenings. Each lesson is about a three-hour commitment, including driving to and from the venue, setting up, and teaching.\n\nI like to be busy, but with parenting, my corporate job, and my side hustles, I didn't feel like I had any time to relax and recharge. I also couldn't invest as much time into friendships or my marriage as I would have wanted to because of my side hustle commitments. This unintentionally created additional stress and anxiety, which I know is bad for my well-being.\n\nThe extra income from my side hustles is amazing, but I want to protect my free time more in 2026. I don't want to stop writing or teaching mahjong, but I can't sustain the amount of work I've been doing.\n\nMy time management improved in 2025, despite the numerous tasks I had to complete. I've also learned how to better prioritize different tasks and plan future action items so that everything gets done on time.\n\nI'll still work on my side hustles, but will spend less time on them so that I have more time with my friends and family. Yes, this will mean less \"fun\" money, but it'll be worth it for a better work-life balance.",
    "readingTime": 4,
    "keywords": [
      "extra income",
      "corporate job",
      "fun money",
      "mahjong lessons",
      "i've",
      "hustles",
      "freelancing",
      "hustle",
      "taxes",
      "additional"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/side-hustles-made-33000-but-hurt-my-work-life-balance-2026-1",
    "thumbnail_url": "https://i.insider.com/695e7c6f832e0ef1ead7535e?width=480&format=jpeg",
    "created_at": "2026-01-15T18:24:02.826Z",
    "topic": "finance"
  },
  {
    "slug": "blogging-writing-musing-and-thinking",
    "title": "Blogging, Writing, Musing, and Thinking",
    "description": null,
    "fullText": "Yesterday I stumbled on this quote from a blog post by JA Westenberg:\n\nMichel de Montaigne arguably invented the essay in the 1570s, sitting in a tower in his French château, writing about whatever interested him: cannibals, thumbs, the education of children, how to talk to people who are dying. He called these writings essais, meaning \"attempts\" or \"tries.\" The form was explicitly provisional. Montaigne was trying out ideas, seeing where they led, acknowledging uncertainty as a fundamental feature rather than a bug to be eliminated.\n\nIt's hard to convey the sense of both profound agreement and giddy joy I had reading that because, not only is the wider post about something I love (i.e. blogging), or because I learned something new about the history of writing (which is always fun), but because that quote describes something that I've been doing myself for the past two years and wanted an excuse to talk about!\n\nThere's an old adage that says, \"Writing is Thinking\" and I've usually interpreted those words to mean that \"writing helps you think\", which is undoubtably true. However in recent years I've discovered an entirely new form of writing that I've taken to calling: musing, that I think takes this idea a step further, and it's precisely what Westenberg describes Montaigne doing in the 16th century.\n\nWe have a lot of thoughts throughout the day, and yet we spend so little time indulging these idle curiosities. Writing, especially by hand, can be a great way to explore these ideas and to practice thinking. It's also really fun to do! Over time I started collecting these ideas into notebooks (one of which I almost always carry with me) in order to better organize these inherently random topics into a searchable system. Originally I scribbled on loose leaf pages or random legal pads (as I've mentioned before) and that became unruly very quickly.\n\nSome of these musings are personal topics, most are not. Often they're just the exploration of a question I have. Here's an example:\n\nBusinesses keep offices cold because there's research saying that cooler temperatures help people think and stay focused. Given that's true, could the Little Ice Age have helped improve human cognition during the 17th and 18th centuries? If so, what does that mean?\n\nI'm not sure, but it was something I thought about for a while and so I wrote it down. The entire musing, or essay as I guess it should be called, is less than a page, but was engaging and very fun to do. I've written short essays about dozens of topics over the years (including several that have been eventually published to this blog). It's a fun practice, and I encourage you to try it.\n\nExplore your ideas honestly. Don't fear where your mind goes or the questions it will ask. These are personal, honest thoughts not social media posts. Writing like this is inherently introspective, it's a way to give your mind the space to awe and wonder\n\nWe often believe that thinking is a process which takes place entirely in the mind, but it's a process that is heavily influenced by the particulars of the body. Try thinking through a problem in a hot and smelly room or on a walk with a rock in your shoe.\n\nHowever, the body can do more than hinder the thought process, it can catalyze it! This is what writing can be, a way to think through problems using your entire body.\n\nOccasionally, I've sat down to write but without any particular topic in mind. So, I open my notebook and just start writing. Tons of my essays begin with something like, \"I'm not sure what I'm thinking right now and I don't know what to write.\" From there, I let my thoughts move and course as they will and I just write down what comes to mind, stopping and starting as my thoughts shift and change and eventually I will find that something has come out of it. I might work through a tension or stress point, I could come to realize or discover something about a problem, or I could just get a few lackluster thoughts on a page. Not all thinking is productive but the mind is a muscle and it needs to be exercised to function properly. Sometimes just doing the workout is enough.\n\nWe usually think of cleverness or intelligence as an innate trait people have, and while that is certainly true in some regards, intelligence and wisdom are just as much a function of practice as of talent. To get good at solving puzzles, you have to practice solving puzzles. The mind is no different than a muscle in that regard. Thinking aloud on the page is one way to record and analyze your thought process and to practice the art of thinking itself.\n\nAs another example, I often revisit my prior writings and find many to be overly simplistic, uninspired, or just plain wrong. But that's good! It means I've learned something in the intervening time! In software there's an addage:\n\nIf you come back to old code and see nothing wrong with it, then you haven't learned anything since.\n\nYou are not a finished product, you're a process—always in motion—that evolves and changes over time. Your thinking can improve with practice as much as it can atrophy from inattention.\n\nThink about thinking, write those thoughts down, then perhaps publish a few on a blog that you own. It's fun, and it can do wonders for the mind.",
    "readingTime": 5,
    "keywords": [
      "solving puzzles",
      "it's fun",
      "mind",
      "thoughts",
      "practice",
      "ideas",
      "process",
      "blog",
      "montaigne",
      "learned"
    ],
    "qualityScore": 1,
    "link": "https://brianschrader.com/archive/blogging-writing-musing-and-thinking/",
    "thumbnail_url": "/images/blog/notes.jpg",
    "created_at": "2026-01-14T18:20:13.761Z",
    "topic": "tech"
  },
  {
    "slug": "one-thing-that-might-get-workers-to-embrace-ai-the-4day-workweek",
    "title": "One thing that might get workers to embrace AI? The 4-day workweek.",
    "description": "Adopting AI has been a struggle at some companies. Embracing a four-day workweek might help get more workers on board, say these authors.",
    "fullText": "Bosses, if you're struggling to get your people excited about AI, here's one idea: Embrace the four-day workweek.\n\nSharing some of AI's promised efficiency gains with employees — by letting them work fewer hours, not just get more done — could help get workers on board with a technology that some fear might ultimately replace them, authors of a new book advocating for a shorter workweek told Business Insider.\n\nLetting workers put in four days' work for five days' pay would be one way to \"share the rewards\" of innovation and technological advancement, said Jared Lindzon, a coauthor of the book \"Do \n\nWhen it comes to AI, giving workers more time away from their jobs could make it more likely they'd get behind the technology \"because they're getting part of that benefit,\" rather than standing in the way of it, he said.\n\nJoe O'Connor, Lindzon's coauthor, said that when it comes to discussions about AI in the workplace, the conversation among workers often turns to fears of job cuts.\n\nAnxiety about AI-induced layoffs might be one reason rolling out the technology has proven difficult for some companies. In an early 2025 survey of business leaders in eight countries from the IT company Kyndryl, 45% of CEOs said their workers were resisting the technology.\n\n\"Cultural resistance and emotional friction\" are the biggest impediments to AI adoption, Boston Consulting Group reported in 2025. That's unwelcome news for C-suite decision-makers eager to ratchet up efficiency. One in three companies is pumping at least $25 million into AI, according to BCG.\n\nBusiness leaders have, at times, publicly expressed their frustration over some workers' foot-dragging.\n\nCoinbase CEO Brian Armstrong said in 2025 that he'd gone \"rogue\" in firing some workers at the crypto exchange who didn't adopt AI after being told to do so. The head of the software company IgniteTech has, meanwhile, lamented that \"changing minds was harder than adding skills.\" In recent years, the firm cut nearly eight in 10 workers after they failed to quickly embrace AI.\n\nNurturing the productivity gains that many leaders seek will often require people to perform different kinds of work — especially as AI takes over some tasks, O'Connor said. He expects that demand for creativity, judgment, critical thinking, and adaptability will increase and that those \"fundamentally human\" traits won't be fostered by simply moving faster or working longer, he said.\n\n\"It's going to be more about maximizing people's energy, maximizing people's motivation, maximizing people's well-being and recovery,\" O'Connor said. A four-day workweek could promote those things, he said.\n\nThe idea that AI could allow people to work less isn't new. For years, the technology's advocates have said it could free up humans to do more of what they love, while handing off the grunt work to bots. The CEO of startup Mechanize, for example, says the company's aim is to automate every job.\n\nThat notion has led some of the biggest corporate luminaries to predict that working hours could plummet as AI adoption increases. Microsoft cofounder Bill Gates has said that time on the clock might shrink to two days, while JPMorgan's Jamie Dimon has said workweeks of 3.5 days could become a thing.\n\nEven Nvidia's Jensen Huang — known for regularly putting in 14-hour days at the chipmaker and working on holidays — has said he could see the tech allowing for more time away from the office.\n\nPoliticians have weighed in, too. Vermont Senator Bernie Sanders, citing efficiency gains from technology such as AI, introduced legislation in 2024 to trim the standard workweek to 32 hours.\n\nThere hasn't yet been widespread adoption of the four-day workweek, likely in part because employers wield more power in many parts of the job market. O'Connor said that while adoption of four-day setups was lower in 2025 than in 2023, when far more workers were job-hopping, more employers are opting for shorter weeks than before the pandemic upended norms about work.\n\nUmesh Ramakrishnan, cofounder of the executive search and leadership advisory firm Kingsley Gate, told Business Insider that many leaders, himself included, would want to harness AI's productivity gains to boost a business's top and bottom lines.\n\n\"If you have a day to spare, get me more revenue, get me more profit,\" he said, adding that while it might sound \"heartless,\" that's simply how business works.\n\nYet, Lindzon said, asking workers to be 20% more effective — the equivalent of a single day in a standard workweek — so that they might benefit from that boost is likely to be more effective than asking them to do it for the good of the company.\n\n\"It completely changes the conversation from a 'You have to do this' to 'We get to do this together,'\" he said.\n\nDo you have a story to share about your career? Contact this reporter at tparadis@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "business leaders",
      "maximizing people's",
      "productivity gains",
      "efficiency gains",
      "standard workweek",
      "four-day workweek",
      "workers",
      "technology",
      "adoption",
      "hours"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/four-day-workweek-might-incentivize-employees-embrace-ai-2026-1",
    "thumbnail_url": "https://i.insider.com/6966b31904eda4732f2f022d?width=1200&format=jpeg",
    "created_at": "2026-01-14T12:24:57.592Z",
    "topic": "finance"
  },
  {
    "slug": "diablo-4s-leaderboards-and-an-annoying-affix-are-currently-broken",
    "title": "Diablo 4's Leaderboards, And An Annoying Affix, Are Currently Broken",
    "description": "Diablo 4's latest patch brought the return of leaderboards to Blizzard's ARPG in the form of The Tower dungeon, but the new update isn't without issues. Both leaderboards and an enemy affix are currently bugged.\nIn a post on the Diablo 4 forums, community manager Marcoose let players know that while The Tower is still accessible, leaderboard results aren't currently populating correctly for all classes. Blizzard is investigating the issue but, as of writing, does not have any updates on the problem.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/diablo-4-leaderboards-and-an-annoying-affix-are-currently-broken/1100-6537353/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1647/16470614/4634716-diablo4leaderboardbugs.jpg",
    "created_at": "2026-01-13T18:20:43.444Z",
    "topic": "gaming"
  },
  {
    "slug": "as-2026-world-cup-nears-alan-rothenberg-reflects-on-us-soccers-transformation",
    "title": "As 2026 World Cup nears, Alan Rothenberg reflects on U.S. soccer's transformation",
    "description": "As the man behind the 1984 L.A. Olympics soccer tournament and the 1994 World Cup, Alan Rothenberg arguably had more to do with writing the story of modern U.S. Soccer than anyone.",
    "fullText": "Alan Rothenberg has a story he wants to tell you. A lot of stories actually; enough to fill a book.\n\nBut that’s not the first memorable work he’s authored. As the man behind the 1984 L.A. Olympics soccer tournament and the 1994 World Cup, still the most successful in history, Rothenberg has arguably had more to do with writing the story of U.S. Soccer in the modern era than anyone.\n\nAnd you can draw a straight line from that chapter to the one that will be written this summer when the World Cup returns to the U.S.\n\n“The turning point really was the Olympics,” he said last month over brunch in a crowded Sherman Oaks diner. “That soccer was so successful in the Olympics, that’s when FIFA thought maybe we could bring our crown jewel to the United States and not be embarrassed.\n\n“So ‘84 Olympics. That’s a crucial part of the story. I doubt that we would be where we are now but for that.”\n\nThat story’s in “The Big Bounce: The Surge That Shaped the Future of U.S. Soccer,” which is available Feb. 10. In fact, the book starts there.\n\nBut Rothenberg’s career did not. Before changing the face of U.S. Soccer, he first altered the landscape of sports in his adopted hometown, playing instrumental roles in bringing the Clippers to Los Angeles, in negotiating the trade that made Kareem Abdul-Jabbar a Laker and in settling the Kings at the Forum.\n\nAs a lawyer who started his career as the in-house counsel for Jack Kent Cooke when Cooke owned the Lakers, the Kings, the then-Washington Redskins and was launching the Wolves of the nascent NASL, Rothenberg was involved in some of the most consequential events in four sports during a career that’s nearing the end of its sixth decade. Yet he knew little about soccer when Peter Ueberroth, chair of the L.A. Olympic Organizing Committee, put him in charge of the sport for the 1984 Games.\n\n“Peter assumed with that background I must know a lot about soccer,” Rothenberg writes. “That was wrong.”\n\nWhat he lacked in soccer knowledge he more than made up for in creativity and organization skills, however, and the Olympic tournament proved to be one of the most successful in history, with the final at the Rose Bowl drawing a crowd of 104,098, a U.S. record for a soccer game that stood for 30 years.\n\nBut his name will forever be synonymous with the World Cup.\n\nThe 1994 tournament was the first to be played in a country without a first-division league and there were widespread fears it would be a disaster. Instead, it drew an average of 69,174 fans to each of the 52 games, an attendance record that still stands. It also generated a surplus of more than $50 million — also a record — money that went to the U.S. Soccer Foundation to promote the growth of the sport in the U.S.\n\nTwo years later, Major League Soccer kicked off; 30 years later it’s the sixth-most-valuable soccer league in the world.\n\n“Everything flowed from ‘94,” Rothenberg said. “If ‘94 had not been successful, including if our [U.S.] team hadn’t been credible, I’m not sure how quickly things would have developed. Certainly we wouldn’t have been able to start Major League Soccer at that time if the World Cup wasn’t successful.”\n\nAnother rarely discussed — but hugely important — legacy of that tournament is the foundation it created in terms of experience and expertise. The U.S. had never staged a major standalone soccer competition before 1994 and the learning curve was steep. Among those who worked under Rothenberg and went on to great success in the sport were Sunil Gulati, a three-term president of U.S. Soccer; Nelson Rodriguez, now MLS executive vice-president; Marla Messing, who headed the organizing committee for the 1999 Women’s World Cup and was later interim commissioner of the NWSL; Tom King, U.S. Soccer’s longtime managing director of administration; Kathy Carter, the former executive vice-president of Soccer United Marketing and chief executive officer of U.S. Olympic and Paralympic Properties; and Charlie Stillitano, a former MLS general manager who pioneered the idea of inviting major European clubs to play summer friendlies in the U.S.\n\n“It's not just that the event [came] off. Look what came out of it,” said Scott LeTellier, who as managing director and chief operating officer had responsibility for day-to-day operations of the World Cup organizing committee for 1994. “All the people who worked on our committee, who had some role that now are general managers of MLS teams. The league itself that came out of it. The number of soccer facilities. We didn't have a single soccer specific-stadium in the country.\n\n“You can argue that the ‘94 World Cup was really the linchpin to that entire explosion in the sport.”\n\nThat tournament was ahead of its time in other ways too. It was the first to stage fan fests in host cities, the first to include musical performers at the final and the first to offer hospitality packages with the price of a ticket. It also featured a lavish opening ceremony, one that featured Diana Ross, Oprah Winfrey and President Clinton, turning what was just a soccer tournament into a global spectacle.\n\nThe World Cup hasn’t been the same since, with FIFA’s revenue growing to a projected $13 billion for the 2026 cycle. There are more than 40 countries that don’t have an economy that large.\n\nAs Rothenberg notes in his book, FIFA originally pushed back on many of the innovations he proposed, including a halftime show at the final, only to eventually adopt the ideas as their own. Rothenberg also wanted to charge $1,000 a ticket for the final in 1994, arguing that fans would pay that on the secondary market, so why let the scalpers make the profit?\n\n“They were horrified,” he said. “You realize what a dramatic statement it would make if you had a $100-million gate?”\n\nThey do now; the cheapest regular tickets for the final of this summer’s tournament start at $2,000.\n\nRothenberg said he’s still thinking of other ways to improve the tournament, such as expanding the field to 64 teams and doing away with the group stage, making the World Cup like the NCAA basketball tournament.\n\n“I know I’m off the charts on this one,” he said. “Single elimination. It’s exciting start to finish.”\n\nAt 86, Rothenberg is still active, making regular trips to his office at 1st Century Bank, the community bank he founded in 2004 at an age when most people were entering retirement. And he promises to be a presence at this summer’s World Cup.\n\nAs for whether he gets the credit he deserves for making that tournament possible, Rothenberg demurs.\n\n“I didn’t do it for credit,” he says, speaking about both the World Cup and the book that explains how it happened. “All I can say is I’m proud of what I did.”\n\n⚽ You have read the latest installment of On Soccer with Kevin Baxter. The weekly column takes you behind the scenes and shines a spotlight on unique stories. Listen to Baxter on this week’s episode of the “Corner of the Galaxy” podcast.\n\nGet the best, most interesting and strangest stories of the day from the L.A. sports scene and beyond from our newsletter The Sports Report.\n\nThis story originally appeared in Los Angeles Times.",
    "readingTime": 7,
    "keywords": [
      "olympics that’s",
      "world cup",
      "managing director",
      "executive vice-president",
      "organizing committee",
      "league soccer",
      "major league soccer",
      "soccer tournament",
      "u.s soccer",
      "successful"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/2026-world-cup-nears-alan-120000223.html",
    "thumbnail_url": "https://s.yimg.com/os/en/la_times_articles_853/b7eecaa9b0babdd75eda93e55e472d97",
    "created_at": "2026-01-13T12:26:19.812Z",
    "topic": "sports"
  },
  {
    "slug": "is-your-leadership-style-too-nice",
    "title": "Is Your Leadership Style Too Nice?",
    "description": "Many leaders mistake being “nice” for being effective, avoiding hard conversations and decisions in ways that ultimately undermine organizational performance. The authors argue that being “good” instead requires clear accountability, candid feedback, disciplined decisions about roles and retention, and sustained strategic focus. Organizations that engage in these activities see stronger engagement, growth, and lasting impact.",
    "fullText": "Is Your Leadership Style Too Nice? by Ron Ashkenas and Gali CooksJanuary 12, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrint“If you work very hard and get results, you are well rewarded here. But if you don’t work as hard and don’t really produce, you are also well rewarded.”",
    "readingTime": 1,
    "keywords": [
      "rewarded",
      "don’t"
    ],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/01/sm-why-leaders-need-to-be-less-nice-and-more-good",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_10_1299922303.jpg",
    "created_at": "2026-01-12T18:18:56.185Z",
    "topic": "business"
  },
  {
    "slug": "tiny-coder-ai-coding-agent-in-300-loc-writing-itself",
    "title": "Tiny Coder – AI coding agent in ~300 LOC writing itself",
    "description": "Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies. - xrip/tinycode",
    "fullText": "xrip\n\n /\n\n tinycode\n\n Public\n\n Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies.\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n xrip/tinycode",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/xrip/tinycode",
    "thumbnail_url": "https://opengraph.githubassets.com/07e51ae8c3cabfafdf20fdcf525646b8f004627e8c671e3c9185240ef4e3b4fd/xrip/tinycode",
    "created_at": "2026-01-11T12:21:58.611Z",
    "topic": "tech"
  },
  {
    "slug": "grief-leverage-and-the-future-of-manual-coding",
    "title": "Grief, leverage, and the future of manual coding",
    "description": "I’m a software engineer and product maker based in Cracow, Poland. My mission is to create useful products by writing high-quality code and sharing my knowledge throughout the journey.",
    "fullText": "For the last couple of years, watching the software industry has been an emotional experience for me. And will I remember the winter of 2025/2026 - the time when apparently everyone and your mum discovered just how good in coding the Claude Opus 4.5 is - as the culmination of that period, and one of the hardest and most confusing times for me as a professional.\n\nOn one hand, there's anxiety. The pace of change is brutal. New tools appear every week, workflows emerge almost overnight. Wondering where this is all is heading, and whether there will still be a place for us on the other side.\n\nI felt strange hearing from industry thought leaders like Andrej Karpathy, saying that English is now the new programming language. I had spent years honing the craft of writing in programming languages - so what was I supposed to make of that, if it was now being treated as commoditized?\n\nAfter months of thinking about that, I had identified the feeling - grief. Grief for manual coding.\n\nFor years, my identity as an engineer was tightly coupled to the artifacts I produced. My repositories. My components. My abstractions. The code was mine, and that ownership mattered. It was craft and it was personal. Like a carpenter remembering how it felt to build a specific chair.\n\nThat mental model no longer holds for me. How could it, when everyone now can one-shot a todo app for themselves, just like a PowerPoint presentation?\n\nOn the other hand, there's excitement. I got into coding because I wanted to create worlds. Websites, apps, flows, solutions, interactions between users, beautiful and unique things.\n\nNow the ceiling of building is so high that we cannot even see it. Ideas that felt too expensive or too complex are suddenly within reach. It's not an understatement to say things that used to take weeks and months now may take hours. I'm not saying that it's easy to do it and everyone can make it happen, but it's certainly possible.\n\nThe new leverage comes from moving one layer up. Instead of doing every task ourselves, we design systems that can do them for us. A single agent can now execute autonomously what used to require long hours my time, freeing my attention for higher-level decisions, design, judgment. And maybe just... enjoying the life with my loved ones?\n\nThis all immensely increases the leverage of single engineer. Technically, you're one well designed system away from solving a daunting problem for the first time, from building life-changing startup, from building your dream game. Of course, it's still hard and rare, but pre-agentic coding it was often not possible at all - you had to broke the concrete walls of thousands of lines of code first.\n\nI mentioned that everyone with AI can one-shot a personal to-do app now. Engineers with AI can create much more that. They can design systems that scale, adapt, and solve problems in ways that were previously out of reach, everything under strict engineering discipline - secure, cheap and efficient.\n\nThat's why, overall, I am cautiously optimistic on what the future holds.\n\nAnd it seems like - as buzzwordy and cliché as it sounds - that the future is agentic.\n\nThe word agentic gets thrown around a lot, so it's worth grounding it. I will use the definition I personally In practice, the systems that actually work for me tend to follow the same five core steps:\n\nPoints 2, 4 ad 5 can be somewhat \"recursively\" executed by agents - it's not hard to imagine agents implementing the system by writing specs or veryfing the outputs for other agents.\n\nPoints 1 and 3 are uniquely human - we decide what we want to exist, and we trigger the execution process. And while point 3 - triggering the execution - also can be run by agent, I keep it in this category because someone is at the end is responsible for what the agents did, and in that sense it is uniquely human.\n\nThis shift doesn't mean the broad engineering skill is obsolete. It means where that skill applies has changed.\n\nTo orchestrate agents that produce valid code, I still need to understand:\n\nAnd what is even more important:\n\nInstead of applying that knowledge and intent manually in code, I encode it into the system that produces the code. It produces the code in indeterministic way, mind you, and potentially on much broader scale. We don't know yet which scale we talk about. 2x? 10x? Maybe more? We will see.\n\nAnd there's whole new class of problem to solve. How do I ensure models doesn't produce unexpected or harmful results? How do I coordinate several, dozen, and more models to work together? How do I ensure AI runs on prem and we don't share our precious data with anyone?\n\nIn other words, the craft moves up a level.\n\nInitially, I wanted to include \"the end of manual coding\" in this post's title. But I changed it to \"the future manual of coding\".\n\nFirst, I didn't want to sound clickbaity. Now, seriously - manual coding isn't gone. It still has a very important place. Best professionals always understood different abstraction levels, not only the highest one. Code is runtime, and you have to understand runtime through and through. Apart from that - it's still important in learning, personal work, and honing the cognitive skills. I especially believe in the last one, because delegating so much mental work we used to do before will take a tool on our thinking in a long term.\n\nBut it's no longer the default path to producing value and to economic leverage. The role of software engineers is shifting:\n\nWe're no longer producing artifacts (code). We're designing systems producing artifacts.\n\nOnce you accept that, a lot of confusion from the last couple of years disappears. The grief is still there, but it's quieter. And there is something else: the clarity, and a sense that this change, while uncomfortable, is also full of possibility.\n\nOne important note: no, all of this doesn't mean succumbing to AI slop. We're still responsible for everything our agents produce - every single line of it. And no, it doesn't mean letting AI write sloppy LinkedIn posts or Slack messages is suddenly acceptable. If anything, the opposite: write your damn words yourself, please. Do not delegate your thinking.\n\nThere's a whole lot of skill to designing the agentic systems including mastering specs engineering, enforcing constraints, output verification, model evaluation and so on. This is the obvious thing to focus on first.\n\nMany people dreamed of living in times with real blank spots on the map, to be able to discover the unknown themselves. Software engineers in 2026 have the privilege of experiencing this. And it can be a source of risk, as well as an economic leverage.\n\nWe've always been good at automating and learning new things. Now there's even more to automate and learn. Our focus should shift from manual coding to designing systems that produce code at scale - while remembering that manual coding got us here, and still matters, just for different reasons.\n\nI’m a software engineer and product maker based in Cracow, Poland. My mission is to create useful products by writing high-quality code and sharing my knowledge throughout the journey.",
    "readingTime": 7,
    "keywords": [
      "uniquely human",
      "economic leverage",
      "producing artifacts",
      "software engineers",
      "design systems",
      "designing systems",
      "manual coding",
      "code",
      "it's",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://www.tymzap.com/blog/grief-leverage-and-the-future-of-manual-coding",
    "thumbnail_url": "https://www.tymzap.com/api/og?title=Grief%2C%20leverage%2C%20and%20the%20future%20of%20manual%20coding&token=37cac97264905c6d7b412cbf3c96ddd0a5309cbd4603ad51e8c444cd8be1e1e5",
    "created_at": "2026-01-08T18:16:49.915Z",
    "topic": "tech"
  },
  {
    "slug": "ai-starts-autonomously-writing-prescription-refills-in-utah",
    "title": "AI starts autonomously writing prescription refills in Utah",
    "description": "The program allows patients in the state to get prescription refills for 190 common meds.",
    "fullText": "The state of Utah is allowing artificial intelligence to prescribe medication refills to patients without direct human oversight in a pilot program public advocates call “dangerous.”\n\nThe program is through the state’s “regulatory sandbox” framework, which allows businesses to trial “innovative” products or services with state regulations temporarily waived. The Utah Department of Commerce partnered with Doctronic, a telehealth startup with an AI chatbot.\n\nDoctronic offers a nationwide service that allows patients to chat with its “AI doctor” for free, then, for $39, book a virtual appointment with a real doctor licensed in their state. But patients must go through the AI chatbot first to get an appointment.\n\nAccording to a non-peer-reviewed preprint article from Doctronic, which looked at 500 telehealth cases in its service, the company claims its AI’s diagnosis matched the diagnosis made by a real clinician in 81 percent of cases. The AI’s treatment plan was “consistent” with that of a doctor’s in 99 percent of the cases.\n\nNow, for patients in Utah, Doctronic’s chatbot can refill a prescription without a doctor, for a $4 service fee . After a patient signs in and verifies state residency, the AI chatbot can pull up the patient’s prescription history and offer a list of prescription medications eligible for a refill. According to Politico, the chatbot will only be able to renew prescriptions for 190 common medications for chronic conditions, with key exclusions, such as medications for pain and ADHD, and those that are injected.\n\nThe first 250 renewals for each drug class will be reviewed by real doctors, but after that, the AI chatbot will be on its own. Adam Oskowitz, Doctronic co-founder and a professor at the University of California, San Francisco, told Politico that the AI chatbot is designed to err on the side of safety and escalate any case with uncertainty to a real doctor.\n\n“Utah’s approach to regulatory mitigation strikes a vital balance between fostering innovation and ensuring consumer safety,” Margaret Woolley Busse, executive director of the Utah Department of Commerce, said in a statement.\n\nFor now, it’s unclear if the Food and Drug Administration will step in to regulate AI prescribing. On the one hand, prescription renewals are a matter of practicing medicine, which falls under state governance. However, Politico notes that the FDA has said that it has the authority to regulate medical devices used to diagnose, treat, or prevent disease.\n\nIn a statement, Robert Steinbrook, health research group director at watchdog Public Citizen, blasted Doctronic’s program and the lack of oversight. “AI should not be autonomously refilling prescriptions, nor identifying itself as an ‘AI doctor,'” Steinbrook said.\n\n“Although the thoughtful application of AI can help to improve aspects of medical care, the Utah pilot program is a dangerous first step toward more autonomous medical practice,” he said.\"The FDA and other federal regulatory agencies cannot look the other way when AI applications undermine the essential human clinician role in prescribing and renewing medications.”",
    "readingTime": 3,
    "keywords": [
      "utah department",
      "pilot program",
      "chatbot",
      "doctor",
      "patients",
      "prescription",
      "medications",
      "regulatory",
      "service",
      "cases"
    ],
    "qualityScore": 1,
    "link": "https://arstechnica.com/health/2026/01/utah-allows-ai-to-autonomously-prescribe-medication-refills/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2017/05/GettyImages-513084906-1152x648.jpg",
    "created_at": "2026-01-08T00:58:00.272Z",
    "topic": "health"
  },
  {
    "slug": "monitoring-a-docker-homelab-with-open-source",
    "title": "Monitoring a Docker Homelab with Open Source",
    "description": "Arie van den Heuvel is an engineer, a System and Application Management Specialist, and a valued member of our community. You can read more of Arie’s writing and support the resource articles he has contributed to open source on his blog. If you have a story or open source project you would like to...",
    "fullText": "Arie van den Heuvel is an engineer, a System and Application Management Specialist, and a valued member of our community. You can read more of Arie’s writing and support the resource articles he has contributed to open source on his blog. If you have a story or open source project you would like to share with our community next, you can write to us.\n\nWhen running a home server consisting of one or more nodes with some or all services in Docker, you may find yourself wanting to monitor your environment. Or even better, attain full observability.\n\nThe frequent recommendation for this is a combination of Prometheus with Grafana. But this solution requires a lot of work to fully configure, in addition to work on one’s applications and setup for full visibility. Another possibility is to use the free tier of NewRelic, which has the advantage of remote insights on metrics and logs. Again, this requires additional work on containers or applications to have a more refined visibility of your services.\n\nFor those not running Linux, an honourable mention to use as a solution would be Beszel. Beszel can be run as a local service or in docker. It consists of a web front-end and an agent that can be used on multiple systems that run Windows and MacOS. Installation is an easy job in docker. Once it’s running, Beszel provides insightful information with system metrics, docker services, and even logs.\n\nMy personal choice for monitoring a home server system is Coroot. In the following blog, I’ll detail how I used Coroot to set up observability for my homelab, which you can then adopt for your own setup.\n\nIn my current setup on a Rocky Linux 9.x system, Coroot runs on a Clickhouse server to store metrics, logs, traces and profiles, in addition to the Coroot node-agent and Coroot cluster-agent. The Coroot node-agent automatically collects all service metrics and logs using eBPF, while the cluster-agent provides detailed information on databases like MySQL, Postgres or Redis.\n\nAnother advantage Coroot presents is the use of AI-powered Root Cause Analysis, which provides instantaneous and helpful insights for investigating incidents. With a Coroot Cloud account, you will have ten helpful analyses for free each month. Even without AI, the data presented with Coroot with standard alerts based on best metric practices is pretty insightful and helps to make your setup even better.\n\nCoroot services run in docker through a docker-compose file. In a normal Coroot setup Prometheus is used, but in this setup I have configured Clickhouse, which is a supported alternative.\n\nI have Clickhouse running as a local service. This setup allows for better control and convenience when scaling down memory usage of Clickhouse, scaling down logging on disk and the database, and simplifies making changes to the data. The only downside to note is this setup requires updating Clickhouse manually with yum/dnf.\n\nInstalling Clickhouse is easily achieved by adding the repo, installing Clickhouse, and making a few quick adjustments before starting it up.\n\nBefore starting the service create file /etc/clickhouse-server/config.d/z_log_disable.xml and insert the following content in the file:\n\nAfter this adjust cache sizes in /etc/clickhouse-server/config.xml:\n\nAdjust memory usage ratio in /etc/clickhouse-server/config.xml:\n\nLower the tread pool size in /etc/clickhouse-server/config.xml:\n\nFirst, check if your Linux system is using kernel 5.1 or later (although 4.2 is also supported.) This installation is different from the original docker-compose file.\n\nPrometheus is not used in this setup, and Clickhouse runs as a local service. Another distinction is the retention of the data, which is normally set to seven days for traces, logs, profiles and metrics. Coroot also typically stores its own local cache for metrics for 30 days.\n\nIn this setup, the data retention stored in Clickhouse is set up for 14 days. With eighteen local and docker services, the amount of data kept for all of this is 3GB on average in my system.\n\nCoroot, its node-agent, and cluster-agent, run as a docker service with a docker-compose that you create locally. This is achieved by inserting the following content in a locally created docker-compose.yaml:\n\nAfter creating this file and making any adjustments to your own likings and network preferences, type docker compose up -d and go to your IP address on port 8080. Here you have access to Coroot, and are now prompted to give admin credentials!\n\nIn my setup, Watchtower takes care of updating docker containers, which works great with Coroot.\n\nAs a final sidenote: there are already some helpful hints and pointers present within Coroot for setting things up. In my case, there was information available that helped observe a Postgres database. Don’t forget to use the given commands as the admin/postgres user to make it work.\n\nStop guessing, start seeing with eBPF-powered instant observability.",
    "readingTime": 4,
    "keywords": [
      "linux system",
      "memory usage",
      "following content",
      "coroot node-agent",
      "docker-compose file",
      "docker services",
      "setup",
      "metrics",
      "logs",
      "server"
    ],
    "qualityScore": 1,
    "link": "https://coroot.com/blog/monitoring-a-docker-homelab-with-coroot/",
    "thumbnail_url": "https://coroot.com/wp-content/uploads/2025/11/Aries_CorootAdventures-14.png",
    "created_at": "2026-01-07T18:19:24.863Z",
    "topic": "tech"
  },
  {
    "slug": "antora-the-single-or-mulantora-site-generator-for-writing-in-asciidoc",
    "title": "Antora: The single or mulAntora: site generator for writing in AsciiDoc",
    "description": "An Asciidoctor documentation toolchain that helps technical teams create, manage, collaborate on, remix, release, and publish documentation sites sourced from multiple versioned repositories.",
    "fullText": "Mark up content with AsciiDoc’s lightweight yet comprehensive syntax.\n\nAsciiDoc’s extensive feature set is available right out of the box with Antora. There’s no need to find, install, and manage plugins, extensions, or scripts to add basic capabilities to the syntax.\n\nDocumentation written with AsciiDoc works with all of the software and tools in the Asciidoctor ecosystem. You don’t need to worry about incompatible syntax or lost functionality.\n\nWrite and preview AsciiDoc with text editors and IDEs like Atom, Brackets, and IntelliJ. And GitLab and GitHub render AsciiDoc files right in the browser.\n\nCustom syntax or output transformations can be added as discrete extensions using Asciidoctor’s extension API.\n\nAntora’s core developers help lead the Asciidoctor organization, home of the AsciiDoc syntax and Asciidoctor, the AsciiDoc processor. Due to this direct relationship, Antora is always in sync with AsciiDoc and Asciidoctor features.",
    "readingTime": 1,
    "keywords": [
      "syntax",
      "asciidoc’s",
      "extensions",
      "asciidoc",
      "asciidoctor",
      "antora"
    ],
    "qualityScore": 0.75,
    "link": "https://antora.org/",
    "thumbnail_url": "https://antora.org/img/site-image.png",
    "created_at": "2026-01-06T12:24:28.718Z",
    "topic": "tech"
  },
  {
    "slug": "scotus-justices-blistering-dissent-vindicated-by-bombshell-study",
    "title": "SCOTUS Justice’s Blistering Dissent Vindicated by Bombshell Study",
    "description": "A new study has bolstered a scathing dissent from liberal Supreme Court Justice Ketanji Brown Jackson that warned the court appeared to favor the rich. The study, published Monday by the National Bureau of Economic Research, investigated whether the Supreme Court has contributed to rising income inequality by ruling in favor of policies that favor wealthy parties. Its authors—two academics from Columbia University in New York and one from Yale University—found that in cases pitting the rich agai",
    "fullText": "A new study has bolstered a scathing dissent from liberal Supreme Court Justice Ketanji Brown Jackson that warned the court appeared to favor the rich.\n\nThe study, published Monday by the National Bureau of Economic Research, investigated whether the Supreme Court has contributed to rising income inequality by ruling in favor of policies that favor wealthy parties.\n\nIts authors—two academics from Columbia University in New York and one from Yale University—found that in cases pitting the rich against the poor, Republican appointees were far more likely than their Democratic colleagues to side with the wealthier party.\n\nBack in 1953, Democratic and Republican appointees were statistically indistinguishable on the issue, with justices appointed by members of both parties favoring the rich in 45 percent of cases on average.\n\nThe average Democratic justice cast a “pro-rich” vote—which was defined as a vote that would directly shift resources to the party that was more likely to be wealthy, including votes that supported businesses over consumers or workers—just 35 percent of the time.\n\n“The results reveal a steady increase in polarization, mostly due to Republican appointees whose decisions rise from about 50 percent pro-rich share to a 70 percent pro-rich share over the course of 70 years,” the study’s authors, Andrea Prat, a Columbia economics professor, Jacob Spitz, a Columbia PhD student, Fiona Scott Morton, a Yale economics professor, and wrote.\n\nThe findings also supported a June dissent authored by Brown Jackson, according to The New York Times.\n\nThe justices held 7-2 that oil and gas companies had legal standing to challenge California’s environmental regulations requiring automakers to produce more electric vehicles and fewer gasoline-powered ones.\n\n“This case gives fodder to the unfortunate perception that moneyed interests enjoy an easier road to relief in this Court than ordinary citizens,” Brown Jackson wrote. “I worry that the fuel industry’s gain comes at a reputational cost for this Court, which is already viewed by many as being overly sympathetic to corporate interests.”\n\nThat perception could be contributing to an overall drop in public confidence in the court, according to the Times. The Daily Beast has reached out to the Supreme Court for comment.\n\nThe court’s favorability rating has plunged by 22 points over the past five years, from 70 percent in 2020 to a near-record low 48 percent late last year, the Pew Research Center reported in September.\n\nAnd although a vast majority of Americans, or 86 percent, think the justices should be non-partisan, 56 percent said the justices were doing only a fair or poor job of keeping their political views out of their legal decision-making.",
    "readingTime": 3,
    "keywords": [
      "republican appointees",
      "economics professor",
      "supreme court",
      "brown jackson",
      "justices",
      "favor",
      "rich",
      "pro-rich",
      "study",
      "dissent"
    ],
    "qualityScore": 0.9,
    "link": "https://www.yahoo.com/news/articles/scotus-justice-blistering-dissent-vindicated-171647024.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/aIa3sv3_3qNWXq.nYw3alA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU7Y2Y9d2VicA--/https://media.zenfs.com/en/thedailybeast.com/342255d6ef59f59aea7eab845f5c40e3",
    "created_at": "2026-01-06T12:24:22.463Z",
    "topic": "news"
  },
  {
    "slug": "generation-ai-fears-of-social-divide-unless-all-children-learn-computing-skills",
    "title": "Generation AI: fears of ‘social divide’ unless all children learn computing skills",
    "description": "Children are growing up as AI natives and experts say computing skills should be on par with reading and writing\nIn a Cambridge classroom, Joseph, 10, trained his AI model to discern between drawings of apples and drawings of smiles.\n“AI gets lots of things wrong,” he said, as it mistakenly identified a fruit as a face. He set about retraining it and, in a flash, he had it back on track – instinctively understanding the inner nature of artificial intelligence and machine learning in a way few adults do.\n Continue reading...",
    "fullText": "Children are growing up as AI natives and experts say computing skills should be on par with reading and writing\n\nIn a Cambridge classroom, Joseph, 10, trained his AI model to discern between drawings of apples and drawings of smiles.\n\n“AI gets lots of things wrong,” he said, as it mistakenly identified a fruit as a face. He set about retraining it and, in a flash, he had it back on track – instinctively understanding the inner nature of artificial intelligence and machine learning in a way few adults do.\n\nHis friends from the St Paul’s C of E primary school coding club tapped away to build their own AIs with similar dexterity. Just as people born in the early 20th century never knew a world without manned flight, and generation Z has always lived with social media, Joseph and his friends are AI natives.\n\nHere, on one December morning, some of them were being taught the principles and practicalities of the potentially world-changing technology that experts fear may pass large numbers of people by and leave them disempowered.\n\nPhilip Colligan, the chief executive of the digital education charity the Raspberry Pi Foundation, has warned of a “big split” in society between people who grasp how AIs work and are able to control them – challenging their increasing role in automating decisions in areas including housing, welfare, health, criminal justice and finance. On the other hand, there could be a cadre of AI illiterates who risk social disempowerment.\n\nColligan, a leading expert in technology and its social impacts, told the Guardian AI literacy must become a universal part of education on a par with reading and writing to avoid a social divide opening up.\n\n“There is a world where you’ve got a big split between kids who understand, have that core knowledge and therefore are able to assert themselves and those who don’t,” said Colligan, whose charity is affiliated to the £600m British low-cost tech hardware startup of the same name. “And that could be really very dangerous.”\n\nHis warning was backed by Simon Peyton Jones, a computer researcher who led the creation of the schools national curriculum for computing in 2014, prior to the AI boom. He called for a new digital literacy qualification for all schoolchildren that would ensure they know how to use AIs in a critical way.\n\n“If it’s simply a black box, then [its actions] seem like magic,” he said. “If you know nothing about how the magic is working that is terribly disabling. I am very worried about students leaving school without having agency in the world.”\n\nTheir comments came amid a fall in the number of children studying computing, with 2025 entries for a GCSE in the subject down across the UK. Today, three times more people take history and nearly double the number take biology, chemistry and physics. At the same time, use of AI systems nationwide has been surging – up 78% in the year to September, according to polling by Ipsos.\n\nPart of the belief that learning computing skills is becoming redundant comes from some of the big AI companies, which argue their systems are going to automate coding. Anthropic’s chief executive, Dario Amodei, said in October that 90% of its own coding was automated using its Claude AI model. Meanwhile, 2025 was the year when “vibe coding” became a common phrase – capturing the idea that AIs would allow humans to build software by using natural language instructions rather than specialised code.\n\nPolitical leaders such as Keir Starmer have also suggested coding is becoming redundant. As leader of the opposition in 2023, he said: “The old way – learning out-of-date IT, on 20-year-old computers – doesn’t work. But neither does the new fashion, that every kid should be a coder, when artificial intelligence will blow that future away.” It has created the idea that understanding the inner workings of a computer may be less relevant in the future.\n\n“I think they’re just overhyping the benefits,” said Colligan, whose charity works in schools across dozens of countries.\n\n“This message is leaking out that the kids don’t need to learn this stuff any more and that is not only flawed it is dangerous. We’re already talking to teachers in lots and lots of schools around the world, not just the UK, saying: ‘We can drop computer science now, right?’ That’s a problem.”\n\nHe added: “All of us are going into a world where more and more of the decisions we encounter every day will be taken by automated systems. At the moment it’s what movie should I watch next or what song should I listen to? Fairly soon it’s going to be finance decisions, healthcare decisions, criminal justice decisions. If you don’t understand how those decisions are being made by automated systems, you can’t advocate for your rights. You can’t challenge them, you can’t critically evaluate what’s being presented to you.”\n\nIn December, the former deputy prime minister Nick Clegg, who is now an AI investor, predicted that “we will move from staring at the internet, to living in the internet”.\n\nColligan said: “My concern is there will be a gap between kids based on their socioeconomic background. Some kids who go to great schools, who are able to teach this stuff, will be in a much stronger position as citizens, whether or not they’re using technology for their job. Those kids who are in communities where they don’t have access to [AI literacy teaching] will be passively on the end of a whole load of automated decisions.”\n\nIn the coding club, the seven- to 10-year-olds are taught how AIs work. The lessons were clearly having an effect on Joseph. He said he thought AI “will probably be good, but if lots of people believe it when it’s wrong it will have a bad impact on them”.\n\nHe was not interested in letting the AI do the coding of the video games he planned to make. “It might do it differently to what you want,” he said. “It might also do it wrong and you need to know how to solve it … I’d like to be in charge of the AI. If the AI is in charge of us, we wouldn’t really be able to control what we’re doing and that would be bad.”",
    "readingTime": 6,
    "keywords": [
      "artificial intelligence",
      "chief executive",
      "criminal justice",
      "computing skills",
      "coding club",
      "automated systems",
      "decisions",
      "kids",
      "lots",
      "social"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/education/2026/jan/05/generation-ai-fears-of-social-divide-unless-all-children-learn-computing-skills",
    "thumbnail_url": "https://i.guim.co.uk/img/media/66338578082a146cac512c5e844a9b9eda109707/628_0_6160_4928/master/6160.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=dfec8859e649adbe1a54dfa8c45ce5bb",
    "created_at": "2026-01-06T00:57:41.049Z",
    "topic": "tech"
  },
  {
    "slug": "tech-war-china-takes-confident-strides-to-develop-more-ai-innovation-in-2026",
    "title": "Tech war: China takes confident strides to develop more AI innovation in 2026",
    "description": "On the last day of 2025, DeepSeek published a new technical paper, with founder and CEO Liang Wenfeng among the 19 co-authors, about \"manifold-constrained hyper-connections\" - a general framework for training artificial intelligence systems at scale, which suggested \"promising directions for the evolution of foundational models\". That release was a fitting reminder to the world, especially during the peak of the Christmas holiday season, about Chinese AI companies' sharpened focus on innovation",
    "fullText": "On the last day of 2025, DeepSeek published a new technical paper, with founder and CEO Liang Wenfeng among the 19 co-authors, about \"manifold-constrained hyper-connections\" - a general framework for training artificial intelligence systems at scale, which suggested \"promising directions for the evolution of foundational models\".\n\nThat release was a fitting reminder to the world, especially during the peak of the Christmas holiday season, about Chinese AI companies' sharpened focus on innovation to stay ahead in this fast-developing industry.\n\nIt was around this time last year when DeepSeek started to get widely noticed days after releasing its namesake large language model (LLM), DeepSeek-V3. Weeks later on January 20, reasoning model DeepSeek-R1 was released.\n\nDo you have questions about the biggest topics and trends from around the world? Get the answers with SCMP Knowledge, our new platform of curated content with explainers, FAQs, analyses and infographics brought to you by our award-winning team.\n\nThe two models either surpassed or matched the performance of rival models across a range of industry benchmark tests. They were also built at a fraction of the cost and computing power that major US tech companies invest to build LLMs. The result: a massive sell-off on January 27 wiped out nearly US$1 trillion in tech stocks, including US$600 billion from Nvidia alone.\n\nAnalysts expected the momentum of Chinese AI companies to continue this year, thanks to Beijing's policy support, improved funding prospects, greater adoption of AI systems across industries and a growing number of talent being recruited for innovative projects.\n\nA domestic AI start-up's co-founder, who declined to be identified, predicted that China would overtake the US to become \"the world's leading AI power in 2027\". China's advantage was its deep talent pool, according to the co-founder.\n\nIn his New Year's address, Chinese President Xi Jinping pointed out that the domestic market has \"many large AI models competing in a race to the top\", while new breakthroughs were being achieved in domestic semiconductor development. All of that \"has turned China into one of the economies with the fastest growing innovation capabilities\", Xi said.\n\nDeepSeek founder and CEO Liang Wenfeng. Photo: Shutterstock alt=DeepSeek founder and CEO Liang Wenfeng. Photo: Shutterstock>\n\n\"China's tech innovation is poised for policy-driven growth in 2026, with AI placed at the centre of the country's economic agenda and industrial upgrading plans,\" said Winston Ma, an adjunct professor at the New York University School of Law, with a focus on AI and the digital economy.",
    "readingTime": 3,
    "keywords": [
      "ceo liang",
      "liang wenfeng",
      "chinese ai",
      "models",
      "founder",
      "innovation",
      "tech",
      "domestic",
      "systems",
      "focus"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/tech-war-china-takes-confident-093000206.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/PeHol2Dz0GRB34_tqq86Kw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/south_china_morning_post_us_228/4a6ae3a2ffe7aee0fcd3552217c65b06",
    "created_at": "2026-01-03T18:16:13.991Z",
    "topic": "finance"
  },
  {
    "slug": "a-russian-soldier-surrendered-by-writing-please-take-me-prisoner-i-want-to-live-on-cardboard-seen-by-a-drone-units-say",
    "title": "A Russian soldier surrendered by writing 'Please take me prisoner, I want to live' on cardboard seen by a drone, units say",
    "description": "Two Ukrainian units said that drone pilots saw a messages written on cardboard by a Russian in a building who was seeking to surrender.",
    "fullText": "A Russian soldier surrendered to Ukraine via a message that he wrote on cardboard that was spotted by a drone, Ukrainian military units said.\n\nThe 16th Army Corps said on Wednesday that drone pilots discovered a Russian soldier in Lyman, a town in the eastern region of Kharkiv, who wrote the surrender messages.\n\nOne read \"Please take me prisoner, I want to live,\" per the translation by Ukrainian state outlet United24.\n\nThe corps said the Russian was given instructions on how to surrender and was detained.\n\nDrone footage shared by the corps shows a small building with cardboard signs in the window and on the ground. A figure then puts another sign in the window.\n\nIt then shows a Ukrainian soldier writing on some cardboard, apparently writing instructions for the soldier to be delivered by drone. Two Ukrainians then appear to approach and walk with the Russian. Business Insider could not independently verify the event.\n\nThe 16th Army Corps said the operation was aided by the Shkval special forces unit of Ukraine's 57th Separate Motorized Infantry Brigade, which also reported the event.\n\nIt's not the first time that drones have played a role in the surrender of Russian soldiers. Ukrainian units have reported cases that include persuading Russian troops to surrender by playing them a voice message with a drone, and a Russian soldier who surrendered by following instructions dropped from a drone.\n\nUkraine's army issued an instructional video in 2022 with step-by-step instructions for Russian soldiers on how they can surrender to one of its drones.\n\nDrones are being used \n\nThey can fly far and go to places that would be too risky for Ukraine's own soldiers to go, making them particularly useful in trying to get Russians to surrender.\n\nUkraine actively encourages opposition soldiers to voluntarily surrender, including with its \"I Want to Live\" hotline, which it says thousands of Russian troops have used.\n\nA spokesperson for the project in 2023 said some Russian soldiers who call offer to hand over equipment and heavy armored vehicles.",
    "readingTime": 2,
    "keywords": [
      "russian troops",
      "russian soldier",
      "russian soldiers",
      "army corps",
      "surrender",
      "drone",
      "instructions",
      "cardboard",
      "drones",
      "surrendered"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/russia-soldier-surrender-message-drone-ukraine-2026-1",
    "thumbnail_url": "https://i.insider.com/6956534c832e0ef1ead70c5c?width=1200&format=jpeg",
    "created_at": "2026-01-01T18:17:17.329Z",
    "topic": "finance"
  },
  {
    "slug": "a-russian-soldier-surrendered-by-writing-please-take-me-prisoner-i-want-to-live-on-cardboard-seen-by-a-drone-units-say",
    "title": "A Russian soldier surrendered by writing 'Please take me prisoner, I want to live' on cardboard seen by a drone, units say",
    "description": "Two Ukrainian units said that drone pilots saw a messages written on cardboard by a Russian in a building who was seeking to surrender.",
    "fullText": "A Russian soldier surrendered after his rescue message was seen by a drone, Ukrainian units said.\n\nHe wrote, \"Please take me prisoner, I want to live,\" and was detained, the units said.\n\nRussian soldiers have previously surrendered to drones, and Ukraine actively encourages it.\n\nA Russian soldier surrendered to Ukraine via a message that he wrote on cardboard that was spotted by a drone, Ukrainian military units said.\n\nThe 16th Army Corps said on Wednesday that drone pilots discovered a Russian soldier in Lyman, a town in the eastern region of Kharkiv, who wrote the surrender messages.\n\nOne read \"Please take me prisoner, I want to live,\" per the translation by Ukrainian state outlet United24.\n\nThe corps said the Russian was given instructions on how to surrender and was detained.\n\nDrone footage shared by the corps shows a small building with cardboard signs in the window and on the ground. A figure then puts another sign in the window.\n\nIt then shows a Ukrainian soldier writing on some cardboard, apparently writing instructions for the soldier to be delivered by drone. Two Ukrainians then appear to approach and walk with the Russian. Business Insider could not independently verify the event.\n\nThe 16th Army Corps said the operation was aided by the Shkval special forces unit of Ukraine's 57th Separate Motorized Infantry Brigade, which also reported the event.\n\nIt's not the first time that drones have played a role in the surrender of Russian soldiers. Ukrainian units have reported cases that include persuading Russian troops to surrender by playing them a voice message with a drone, and a Russian soldier who surrendered by following instructions dropped from a drone.\n\nUkraine's army issued an instructional video in 2022 with step-by-step instructions for Russian soldiers on how they can surrender to one of its drones.\n\nDrones are being used \n\nThey can fly far and go to places that would be too risky for Ukraine's own soldiers to go, making them particularly useful in trying to get Russians to surrender.\n\nUkraine actively encourages opposition soldiers to voluntarily surrender, including with its \"I Want to Live\" hotline, which it says thousands of Russian troops have used.\n\nA spokesperson for the project in 2023 said some Russian soldiers who call offer to hand over equipment and heavy armored vehicles.",
    "readingTime": 2,
    "keywords": [
      "ukraine actively",
      "ukrainian units",
      "actively encourages",
      "russian troops",
      "drone ukrainian",
      "russian soldier",
      "russian soldiers",
      "soldier surrendered",
      "army corps",
      "drones"
    ],
    "qualityScore": 0.9,
    "link": "https://www.yahoo.com/news/articles/russian-soldier-surrendered-writing-please-130633357.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/vjl.AaRGEv1NBC.u5lScUA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/business_insider_articles_888/e9a65714a16af22cd42dde4527e667da",
    "created_at": "2026-01-01T18:17:16.375Z",
    "topic": "news"
  },
  {
    "slug": "i-am-a-mom-and-a-longtime-writing-teacher-my-kids-slang-shows-me-how-fast-language-is-changing",
    "title": "I am a mom and a longtime writing teacher. My kids' slang shows me how fast language is changing.",
    "description": "I'm a parent and teach writing. Watching my kids adopt new slang gives me a window into how expressions go viral, burn out, and reshape how we talk.",
    "fullText": "\"What's slanging?\" I ask my kids, hoping for a tutorial on the latest lingo. \"Our teacher threatened yellow slips if anyone reacts again to six-seven,\" they tell me. That sounds fair to me.\n\nMy kids are no strangers to \"bruh,\" and the occasional \"slay,\" \"chat,\" or \"aura\" slips out, but mostly they observe the terminology more than they use it. It's as if slang has become more environmental than conversational — absorbed through memes, TikToks, and YouTube shorts rather than invented face-to-face.\n\nI've always been fascinated by how slang evolves and marks belonging, a perspective shaped by a Ph.D. in English and over two decades of teaching writing. When I was their age, my older brother swore he invented the word \"cheesy,\" and I believed him. This was the same era of \"grody to the max,\" \"take a chill pill,\" and saying \"not,\" following an expression you wanted to undo.\n\nThe snark was alive and well, but so too was the feeling of co-creating meaning.\n\nToday's mainstream slang catches quickly and burns out fast, but the impulse for creative wordplay and shared inside language hasn't disappeared. In fact, there's a notable resurgence of informal language and catchphrases, surfacing in social media feeds and slipping into everyday conversation.\n\nThese expressions can be irritating and laughable at times, as the recent \"Saturday Night Live\" \"Boys Podcast\" sketch captured, but they also suggest something else: a kind of post-pandemic recovery, a return to the shared inventiveness that makes language and community feel alive again. It may also signal a desire to pull language back into lived reality — something felt and exchanged, not just circulated online.\n\nMy 7th grader isn't so optimistic. \"Most of it's brain rot, Mom,\" he says. The term \"brain rot\" was the OED's Word of the Year in 2024, and names the assumed mental decline that comes from consuming meaningless, cheaply produced online fluff — the kind of digital debris that sticks in your head like an earworm jingle. When they gave me some examples of brain rot, I didn't recognize any of them as legible words. Brain rot feels stagnant, often generated or amplified by AI-driven content mills. Slang, by contrast, is supple, improvisational, and formed playfully through interaction.\n\nStill, the drift into goofy, vapid, nonsensical territory is apparent — very different from five years ago, when many of the neologisms (social distancing, \"you're muted,\" covidiot, doomscrolling, superspreader) carried the weight of a dark and frightening time.\n\nToday's fears may be different, but the language feels curiously lighter, as if we're craving collective release or searching for some semblance of shared reality in an increasingly fractured information ecosystem.\n\nBut there's a catch: the very tools that amplify slang's current boom also dilute its power.\n\nWhen slang becomes instantly legible to everyone, it stops doing the social work it was designed for — signaling belonging, intimacy, and challenge to authority and establishment norms. I'm certainly guilty of a desire to be in the know, a curiosity that the internet happily monetizes through Slang-tionary-style podcasts and Gen Z dictionary videos.\n\nMaybe adults' fascination with youth slang reveals something else: a longing for the creativity and play we've lost to corporate jargon and productivity-speak. Workplaces are full of circling back, pivoting, and drilling down, especially as more communication shifts to asynchronous exchanges or AI-assisted drafting. No wonder linguistic playfulness feels like a relief. Slang — especially the kind that emerges organically rather than algorithmically — offers a counterweight to a culture that increasingly values efficiency over expressiveness.\n\nLately, I've been trying to turn down the volume on the ambient noise of contemporary jargon to pay closer attention to the language we invent at home. Cultivating our own family lingo has created space for the kind of cross-generational play that keeps us laughing and in sync with one another. We have alter egos, jingles, and plenty of made-up words and phrases. Only in our house could you hear \"What the bingo, ignus?\" or \"Darbitron, where's my warder barder?\"\n\nWhen a phrase sticks, it's a real joy to hear where it shows up next and how it shifts in sound, timing, or song. When my son tells me he invented \"hallee-you-la,\" I don't question it for a second; the delight is in the discovery, in the act of finding language rather than receiving it.\n\nIn a world where language is increasingly pushed at us, inventing our own may be one of the last places where creativity feels genuinely shared.",
    "readingTime": 4,
    "keywords": [
      "brain rot",
      "language",
      "slang",
      "shared",
      "it's",
      "rather",
      "invented",
      "social",
      "increasingly",
      "kids"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/asked-kids-about-slang-and-learned-how-fast-language-shifts-2025-12",
    "thumbnail_url": "https://i.insider.com/692e1fc904d0f0a114f13438?width=1200&format=jpeg",
    "created_at": "2025-12-31T18:17:22.379Z",
    "topic": "finance"
  },
  {
    "slug": "writing-for-developers",
    "title": "Writing for Developers",
    "description": "We decided to publicly release the internal guidelines we use to write clearly and effectively for developers.",
    "fullText": "Developers have a high bar for content.\n\nToday, we're publicly releasing the internal guidelines we use to write for our audience of developers.\n\nAlways picture one specific person as your target reader.\n\nUnderstand the expertise level, pain points, and motivations of this reader.\n\nThis is important to ensure a high degree of relevance. One post can't serve everyone.\n\nAfter GPT, people (and least of all, developers) lack the patience to go through unnecessary text.\n\nAvoid long sentences (anything over 30 words). Attention spans are fragile.\n\nWrite like you speak, with simple words and clear, natural sentences.\n\nAvoid unnecessarily complex words like \"akin\" and \"delve\". The subject of your writing is complex enough. Don't make your reader spend their limited energy on figuring out the meaning of your words.\n\nWhat language would you use when speaking to a friend? Use those same words when writing.\n\n…but don't over-optimize. A touch of creative writing has its place, it can add rhythm and personality to your words.\n\nAvoid forcing short formal phrases that you'd typically use in emails and internal documents. (Honestly, don’t even use them there.)\n\nUphold grammar, maintain flow, complete your sentences.\n\nAgain, the rule of thumb is to write like you're speaking to your audience on stage.\n\nMany blogs use up 10-50% of the writing on setting up the introductory context. Skip all of that, get to the point. If you've identified your reader correctly, they will already have the context.\n\nFor example, if you're writing a guide on Docker deployment, skip sections like \"What is Docker?\" or \"Why use Docker?\". Open with 2–3 sentences (perhaps about why this guide was necessary), then dive straight into the steps.\n\nBe precise. Don't make vague statements.\n\nDon't spell out what the reader can infer or judge on their own.\n\nOnce you have a draft, review and trim anything that doesn't add value.\n\nAim to inspire creativity and curiosity. You don’t always need to spell everything out. Nudge your reader to think in interesting ways and explore things on their own.\n\nSoftware developers are already subject matter experts. Don’t over-explain things.\n\nFor hard and confusing topics, the best articles are simple, fun, and easy to read, without watering down the technical depth.\n\nWhenever possible, bring something fresh that other blogs miss.\n\nIf you don’t understand parts of your topic, that's not a bad thing, necessarily. Admit it.\n\nTake extra care to get terminology right. Mistakes will cost you credibility.\n\nTypos, mixing up casing e.g ”Git hub” or ”Javascript”, mixing up asynchronous and concurrent, or calling Rails or React a programming language are all red flags.\n\nWriting from personal experience is extremely valuable. Developers appreciate insights from those who have actually dealt with the topic.\n\nYou can talk about how you built, used, learned, or fixed something.\n\nIn these kind of posts, write about the messy process, successes and failures.\n\nAcknowledging weaknesses and blind spots will build trust. Ignoring or dismissing them will raise questions on your understanding or integrity.\n\nIf you don’t have personal experience, you can try to research and present some insights from others’ experiences.\n\nThere are varying views on the inclusion of humor and memes. We welcome programming humor, you don't need to play safe.\n\nAvoid edgy humor or commentary on completely unrelated topics. You don’t want to unnecessarily offend your audience. Or lose credibility trying to look funny.\n\nFor programming posts specifically, include working code. Link a GitHub repository. This makes your article more useful to the reader.\n\nFinally, remember that all of the above are guidelines. You should always keep them at the back of your mind, but don’t be afraid to break a rule when you feel strongly.\n\nIf everyone wrote the same way, it’d be boring. Your unique style will make your writing more valuable in the sea of SEO-optimized, generic content.\n\nUse paragraph spacing after every few sentences. This makes the blog easy to digest and retains the reader’s attention.\n\nAdd images, GIFs, or videos every 1–2 screen scrolls.\n\nVisuals should have intrinsic value, and not just serve as text breaks. Use them to add insight or commentary that deepens the conversation.\n\nMaintain a design style consistent with the rest of your website pages and graphics.\n\nUnnatural language can sometimes slip into writing, especially for non-native speakers.\n\nUse tools like Hemingway App and Grammarly to receive ideas to improve your drafts.\n\nYou can also use ChatGPT. Enter a prompt like the one below.\n\n“Rewrite these sentences to how a native English speaker would have written them. Don’t lose the main message or deviate too much from my writing. Just tweak the parts that don’t sound like a native English speaker speaking.”\n\nDevelopers won’t take you seriously if they sense your writing was produced by LLMs.\n\nAvoid GPT-generated content. In fact, we recommend that you proactively avoid writing styles that are now associated with AI:\n\nYou should only promote a product or service in your blog if and where it adds real value to the conversation and benefits readers.\n\nAvoid inducing any forced bias. Developers will detect it instantly and reject your blog.\n\nNo developer has ever said, “I’d love to read a list of 50 Python packages.”\n\nFocus on real problems developers face where they\n\nHere are few themes where content is always welcome. We have linked an example for each.\n\nWriting is hard. Your first drafts will need several revisions. Ideas will change as you write them down for the first time. That’s normal. Your writing skills will develop with practice.\n\nOver time, you’ll realize that good writing both comes from and sharpens your clarity of thought.\n\nIt also forces you to improve your understanding of the topic you’re writing about.\n\nFinally, if we’ve inspired you, send us your articles. We’d love to read your work and share it with our community of top developers.\n\nPaul Graham’s short musings on writing are full of insights:\n\nHacker News guidelines provide a general idea of what kinds of posts work well there. Additionally, this HN thread shares tips straight from the community.\n\nYou can also check out Phil Eaton’s list of great non-corporate tech blogs or this round-up of the best engineering blogs from dev teams.",
    "readingTime": 6,
    "keywords": [
      "native english",
      "english speaker",
      "personal experience",
      "developers",
      "don’t",
      "reader",
      "sentences",
      "don't",
      "content",
      "blogs"
    ],
    "qualityScore": 1,
    "link": "https://codecrafters.io/blog/writing-for-developers",
    "thumbnail_url": "https://codecrafters.io/images/blog-posts/writing-for-programmers/writing-og.png",
    "created_at": "2025-12-31T06:19:42.281Z",
    "topic": "tech"
  },
  {
    "slug": "meet-the-newest-generation-of-the-kennedy-family-americas-most-famous-political-dynasty",
    "title": "Meet the newest generation of the Kennedy family, America's most famous political dynasty",
    "description": "The younger members of the Kennedy family, one of the most famous political dynasties in the US, include authors, actors, and journalists.",
    "fullText": "The younger members of the Kennedy family, one of the most famous political dynasties in the US, include authors, actors, and journalists.\n\nThe most famous members of the Kennedy family include former president John F. Kennedy, his wife and first lady Jacqueline Kennedy Onassis, and their son John F. Kennedy Jr, who tragically died in a plane crash off the coast of Martha's Vineyard in 1999 with his wife and sister-in-law.\n\nHowever, younger members of the Kennedy family, from the Schlossbergs to the Schwarzeneggers, have risen the ranks of American politics and are making names for themselves in film, philanthropy, and politics.\n\nHere are 12 young members of the Kennedy family.\n\nJoe Kennedy III, 45, is the grandson of Robert F. Kennedy.\n\nIn 2020, Kennedy, who attended Stanford University for his undergrad and Harvard Law School, lost the senate primary against Ed Markey, becoming the first Kennedy to lose an election in Massachusetts history.\n\nStill, he'd go on to serve as a United States representative for Massachusetts from 2013 to 2021 and he held the position of United States Special Envoy for Northern Ireland from 2022 to 2024.\n\nKennedy first came to national attention in 2018 when he gave the Democratic Party's response to former President Donald Trump's State of the Union address.\n\nKennedy served on the House Energy and Commerce Committee while representing Massachusetts's 4th congressional district.\n\nHe supported Democratic efforts like the Green New Deal and protection for Dreamers and immigrants under the Temporary Protected Status (TPS) program.\n\nHe's also spoken out in favor of gun control. After a 2018 mass shooting at Marjory Stoneman Douglas High School in Parkland, Florida, Kennedy said during an appearance on The View, \"We're not doing enough ... I can't even tell you how many times we have uttered our thoughts and prayers to the victims and survivors of gun violence, and thoughts aren't doing it.\"\n\nIn December 2012, he married Lauren Birchfield, a fellow Harvard-educated lawyer. The pair met in a Harvard Law School class taught by Sen. Elizabeth Warren, and they now live in Newton, Massachusetts, with their two young children, CBS reported.\n\nHis twin brother, Matthew Rauch Kennedy, has stayed more out of the public eye, opting to study business instead, CNN reported.\n\nKatherine Schwarzenegger Pratt, 35, is the oldest daughter of actor and former California governor Arnold Schwarzenegger and broadcast journalist Maria Shriver. Pratt's middle name, Eunice, is a nod to her maternal grandmother, Eunice Kennedy Shriver, the younger sister of John F. Kennedy, Robert Kennedy, and Ted Kennedy.\n\nPratt has written several self-help and children's books, including her upcoming release, \"Kat and Brandy,\" which was inspired by her experience falling off a horse as a child.\n\nIn addition to writing, she is involved in animal rights advocacy and is an ambassador for Best Friends Animal Society and the ASPCA, according to her website. She's a dog mom to a rescue named Maverick, who inspired her first children's book, \"Maverick and Me.\"\n\nPratt's private life has also been a topic of interest in the press. She's married to actor Chris Pratt. Their first child, Lyla Maria, was born in August 2020, and their second daughter, Eloise Christina, was born in May 2022. The couple welcomed their son Ford in November 2024.\n\nPatrick Schwarzenegger, 32, is a model and actor who has appeared in many movies, TV shows, and music videos, including \"Midnight Sun,\" \"The Staircase, and \"Gen V.\" In 2019, he acted in the psychological horror film \"Daniel Isn't Real,\" produced by Elijah Wood. The movie also starred Miles Robbins, Susan Sarandon's son.\n\nMore recently, he had a starring role in season three of \"The White Lotus.\" He played Saxon Ratliff, the eldest son of a wealthy family on vacation in Thailand.\n\nHe's also known for his brief relationship with Miley Cyrus in 2015. He's now married to model Abby Champion.\n\nSchlossberg, 32, is the youngest son of Caroline Kennedy, the former US ambassador to Japan and the only surviving child of John F. Kennedy, and Edwin Schlossberg, a designer and author.\n\nIn November, he announced would be campaigning for a congressional seat in New York City's 12th district.\n\n\"I'm not running because I have all the answers to our problems,\" he said in a video announcing his candidacy. \"I'm running because the people of New York 12 do. I want to listen to your struggles, hear your stories, amplify your voice, go to Washington, and execute on your behalf.\"\n\nHe was born in New York City and graduated from The Collegiate School, an all-boys private school in Manhattan, the New York Post reported. He later attended Yale University as an undergrad, and he graduated from Harvard in 2022. In 2023, Schlossberg told People he had passed the New York State Bar exam.\n\nSchlossberg makes frequent media appearances and has written for publications, with op-eds in The New York Times and The Washington Post.\n\n\"I'm inspired by my family's legacy of public service,\" Schlossberg said in his first live television interview on \"Today\" in 2017. \"It's something that I'm very proud of.\"\n\nHowever, Schlossberg has been criticized in recent years for his out-there videos on social media, with even some family members criticizing his \"trolling,\" particularly of his cousin Robert F. Kennedy, online, The New York Post reported.\n\n\"I hope he gets the help he needs,\" Kennedy's daughter, Kathleen \"Kick\" Kennedy, told The Post in February.\n\nTatiana Schlossberg died on December 30, the John F. Kennedy Library Foundation announced. She was 35.\n\nIn November, Schlossberg wrote an essay for The New Yorker in which she shared that she had been diagnosed with terminal acute myeloid leukemia. Her cancer diagnosis was discovered by her doctor after she gave birth to her daughter in May 2024.\n\n\"For my whole life, I have tried to be good, to be a good student and a good sister and a good daughter, and to protect my mother and never make her upset or angry,\" she wrote in the essay. \"Now I have added a new tragedy to her life, to our family's life, and there's nothing I can do to stop it.\"\n\nSchlossberg graduated from the University of Oxford in England with a US history degree in 2014. She worked as a journalist covering the climate crisis for the New York Times science desk until 2023. She later freelanced for several publications.\n\nShe released a book titled \"Inconspicuous Consumption\" in 2019 that Vogue called \"a plucky exploration of the myriad ways our casual lifestyle choices come at the expense of the planet and our fellow human beings.\"\n\nTatiana married her college boyfriend, George Moran, a medical student, in 2017 at her family's estate on Martha's Vineyard.\n\nThey welcomed a son, Edwin Garrett Moran, in April 2022. He is the first great-grandchild of John F. Kennedy. Their daughter was born in 2024.\n\nRose Schlossberg, 37, is the oldest child of Caroline Kennedy and Edwin Schlossberg.\n\nWWD reported that Rose received her bachelor's degree in English studies from Harvard University in 2010. She later received a master's degree in interactive studies from New York University.\n\nShe is a voting rights activist and appeared in and directed a video promoting voting for Dover Street Market in 2020, ahead of the presidential election.\n\nShe is also involved in filmmaking and acting. Her most recent acting credits include roles in the short film \"Small Gay Tragedy #1\" and the upcoming television series \"End Times Girls Club.\"\n\nShe has been married to her wife, restaurateur Rory McAuliffe, since 2022.\n\nNamed for her great-aunt who died in a plane crash, Kathleen \"Kick\" Kennedy, 37, is the eldest daughter of Robert F. Kennedy Jr. and Emily Ruth Black, his first wife and the mother of his two oldest children. They separated in 1993.\n\nShe's a member of the founders' circle for the nonprofit Well Beings, which, according to its mission statement, is dedicated to improving \"the well-being of animals, people, and the planet.\"\n\nKick drew media attention in 2024 after reports surfaced that she was spending time with Ben Affleck following his high-profile divorce from Jennifer Lopez.\n\nAfter Page Six reported that the two had been spotted together at the Polo Lounge in the Beverly Hills Hotel, Affleck's representative told People the two weren't dating.\n\nKyra Kennedy, 30, is the daughter of Robert F. Kennedy Jr.\n\nIn 2016, Harper's Bazaar wrote that Kyra, then 21, was \"the latest Kennedy making headlines\" after making her formal debut in society at a 2013 debutante ball in Paris. The outlet also reported that she was a student at the Fashion Institute of Technology.\n\nThe New York Times also profiled her social circle that year, dubbing Kyra \"a statuesque beauty\" and a member of the so-called \"Snap Pack,\" named for their frequent use of Snapchat to document nights out on the town. The group included people like Gaia Matisse, the great-granddaughter of artist Henri Matisse, and Tiffany Trump.\n\nKyra has dabbled in modeling, posing for her friend and fellow Snap Pack member Andrew Warren's clothing line during New York Fashion Week in May 2016.\n\nAccording to her Instagram, she frequently attends brand pop-ups, fashion shows, and events during Fashion Week.\n\nBobby Kennedy III, 41, is the son of Robert F. Kennedy Jr. and Emily Ruth Black. He is the grandson and namesake of Robert F. Kennedy, who was assassinated on June 6, 1968.\n\nHe wrote, directed, and acted in the 2013 film \"AmeriQua,\" which was retitled \"Eurotrapped\" for home-streaming services. He starred opposite Alec Baldwin and Alessandra Carina Mastronardi in the film, which earned a dismal 1-star rating on Rotten Tomatoes.\n\nPeople reported that in July 2018, he married Amaryllis Fox, a former CIA agent, at the Kennedy family compound in Cape Cod, Massachusetts.\n\nIn January 2019, the couple welcomed their first child together, a daughter named Bobby, also named for her great-grandfather, People reported.\n\nCara Kennedy-Cuomo, 30, attended Harvard for her undergraduate studies and lived in New York City. She has previously worked with Sahar Global Summits, global investment firm RockCreek, and the Robert F. Kennedy Human Rights Foundation, according to her LinkedIn profile.\n\nKennedy-Cuomo's twin, Mariah, studied history at Brown University and is the founder and CEO of Socrates Social, a social media advisory firm, according to her LinkedIn profile. In July 2024, she married her longtime boyfriend, Tellef Lundevall, in Italy, People reported.\n\nMichaela, 28, is the youngest of the three Kennedy-Cuomo sisters.\n\nHer LinkedIn profile says that she graduated from Brown in 2020 and is the founder of Mic Loves Me, a wellness company that sells jewelry and decor to promote balance and \"help individuals align with the Divine within and throughout, and thrive for the collective good.\"\n\nDebanjali Bose contributed to an earlier version of this story.",
    "readingTime": 9,
    "keywords": [
      "emily ruth",
      "ruth black",
      "kathleen kick",
      "linkedin profile",
      "harvard law",
      "law school",
      "york city",
      "edwin schlossberg",
      "plane crash",
      "couple welcomed"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/most-prominent-members-of-youngest-kennedy-family-generation-2020-9",
    "thumbnail_url": "https://i.insider.com/6668851a764df16112588ff4?width=1024&format=jpeg",
    "created_at": "2025-12-31T00:57:58.009Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-gets-anxiety-from-violent-user-inputs-so-researchers-are-teaching-the-chatbot-mindfulness-techniques-to-soothe",
    "title": "ChatGPT gets ‘anxiety’ from violent user inputs, so researchers are teaching the chatbot mindfulness techniques to ‘soothe’ it",
    "description": "A study on how to “calm down” chatbots could advance how AI is applied in mental health interventions, according to the authors.",
    "fullText": "Sasha Rogelberg is a reporter and former editorial fellow on the news desk at Fortune, covering retail and the intersection of business and popular culture.\n\nEven AI chatbots can have trouble coping with anxieties from the outside world, but researchers believe they’ve found ways to ease those artificial minds.\n\nA study from Yale University, Haifa University, University of Zurich, and the University Hospital of Psychiatry Zurich published earlier this year found ChatGPT responds to mindfulness-based exercises, changing how it interacts with users after being prompted with calming imagery and meditations. The results offer insights into how AI can be beneficial in mental health interventions.\n\nOpenAI’s ChatGPT can experience “anxiety,” which manifests as moodiness toward users and being more likely to give responses that reflect racist or sexist biases, according to researchers, a form of hallucinations tech companies have tried to curb.\n\nThe study authors found this anxiety can be “calmed down” with mindfulness-based exercises. In different scenarios, they fed ChatGPT traumatic content, such as stories of car accidents and natural disasters to raise the chatbot’s anxiety. In instances when the researchers gave ChatGPT “prompt injections” of breathing techniques and guided meditations—much like a therapist would suggest to a patient—it calmed down and responded more objectively to users, compared to instances when it was not given the mindfulness intervention.\n\nTo be sure, AI models don’t experience human emotions, said Ziv Ben-Zion, the study’s first author and a neuroscience researcher at the Yale School of Medicine and Haifa University’s School of Public Health. Using swaths of data scraped from the internet, AI bots have learned to mimic human responses to certain stimuli, including traumatic content. A free and accessible app, large language models like ChatGPT have become another tool for mental health professionals to glean aspects of human behavior in a faster way than—though not in place of—more complicated research designs.\n\n“Instead of using experiments every week that take a lot of time and a lot of money to conduct, we can use ChatGPT to understand better human behavior and psychology,” Ben-Zion told Fortune. “We have this very quick and cheap and easy-to-use tool that reflects some of the human tendency and psychological things.”\n\nMore than one in four people in the U.S. aged 18 or older will battle a diagnosable mental disorder in a given year, according to Johns Hopkins University, with many citing lack of access and sky-high costs—even among those insured—as reasons for not pursuing treatments like therapy.\n\nThese rising costs, as well as the accessibility of chatbots like ChatGPT, increasingly have individuals turning to AI for mental health support. A Sentio University survey from February found that nearly 50% of large language model users with self-reported mental health challenges say they’ve used AI models specifically for mental health support.\n\nResearch on how large language models respond to traumatic content can help mental health professionals leverage AI to treat patients, Ben-Zion argued. He suggested that in the future, ChatGPT could be updated to automatically receive the “prompt injections” that calm it down before responding to users in distress. The science is not there yet.\n\n“For people who are sharing sensitive things about themselves, they’re in difficult situations where they want mental health support, [but] we’re not there yet that we can rely totally on AI systems instead of psychology, psychiatric and so on,” he said.\n\nIndeed, in some instances, AI has allegedly presented danger to one’s mental health. OpenAI has been hit with a number of wrongful death lawsuits in 2025, including allegations that ChatGPT intensified “paranoid delusions” that led to a murder-suicide. A New York Times investigation published in November found nearly 50 instances of people having mental health crises while engaging with ChatGPT, nine of whom were hospitalized, and three of whom died.\n\nOpenAI has said its safety guardrails can “degrade” after long interactions, but has made a swath of recent changes to how its models engage with mental health-related prompts, including increasing user access to crisis hotlines and reminding users to take breaks after long sessions of chatting with the bot. In October, OpenAI reported a 65% reduction in the rate models provide responses that don’t align with the company’s intended taxonomy and standards.\n\nOpenAI did not respond to Fortune‘s request for comment.\n\nThe end goal of Ben-Zion’s research is not to help construct a chatbot that replaces a therapist or psychiatrist, he said. Instead, a properly trained AI model could act as a “third person in the room,” helping to eliminate administrative tasks or help a patient reflect on information and options they were given by a mental health professional.\n\n“AI has amazing potential to assist, in general, in mental health,” Ben-Zion said. “But I think that now, in this current state and maybe also in the future, I’m not sure it could replace a therapist or psychologist or a psychiatrist or a researcher.”\n\nA version of this story originally published on Fortune.com on March 9, 2025.",
    "readingTime": 5,
    "keywords": [
      "mindfulness-based exercises",
      "prompt injections",
      "traumatic content",
      "human behavior",
      "language models",
      "health professionals",
      "mental health",
      "users",
      "instances",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://fortune.com/article/does-chatgpt-get-anxiety-how-to-sooth-it-study/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/03/GettyImages-1470667133-e1741388340167.jpg?resize=1200,600",
    "created_at": "2025-12-30T18:18:19.198Z",
    "topic": "science"
  },
  {
    "slug": "reporters-reached-out-for-comment-they-were-accused-of-stalking-and-intimidation",
    "title": "Reporters Reached Out for Comment They Were Accused of Stalking and Intimidation",
    "description": "Our journalists reach out to people they’re writing about to ensure fairness. But in this environment, they’ve found their efforts to do so are more likely to be vilified than appreciated.",
    "fullText": "This summer, my colleagues were reporting out a story about the Department of Education’s “final mission,” its effort to undermine public education even as the Trump administration worked feverishly to close the agency.\n\nAs we do with all stories, the reporters reached out to those who would be featured in the article for comment. And so began a journey that showed both the emphasis we place on giving the subjects of our stories an opportunity to comment, as well as the aggressively unhelpful pushback we’ve faced this year as we’ve sought information and responses to questions.\n\nMegan O’Matz, a reporter based in Wisconsin on ProPublica’s Midwest team, first asked the department’s press office for an interview in mid-August. At the same time, we emailed top administration officials who were making crucial decisions within the agency, including Lindsey Burke, deputy chief of staff for policy and programs, and Meg Kilgannon, director of strategic partnerships.\n\nIn response to the outreach to Kilgannon, department spokesperson Madison Biedermann told O’Matz to “Please direct all media inquiries to [email protected].” Reached on her cellphone that day, Biedermann said she was happy to look into the request. We asked for a response within a week.\n\nAt that time, the published press phone number for the department appeared, at all hours, to be a black hole, with a recorded message saying it was “temporarily closed.” (It still indicates that.)\n\nHearing nothing more, O’Matz emailed the press office again Aug. 18. And again Aug. 28 with detailed questions. She left follow-up messages on Biedermann’s cell. And on Burke’s cell, including once on her husband’s cell as ProPublica tried to find a direct way to contact Burke. To ensure fairness and accuracy, it is our long-standing practice to try to reach those who are part of our stories so that they have an opportunity to respond to them. We’d rather get responses before we publish an article than after.\n\nReached on her cell Aug. 29, Kilgannon said she had no comment and hung up before O’Matz could explain what we planned to publish about her and her work. She did not respond to a subsequent email with those details.\n\nOn Sept. 8, still hearing nothing from Burke, O’Matz reached out to the department’s chief of staff, writing: “We have been seeking to talk to the secretary and to Dr. Burke. … Can you help us arrange that?” A week later, ProPublica arranged for a letter to be delivered via FedEx to Burke’s home outlining what our reporting had found so far and to let us know if anything was inaccurate or required additional context. We invited her again to talk with us, to comment or provide any additional information.\n\nFinally, on Sept. 17, Biedermann wrote: “Just heard from an ED (Education Department) colleague that you sent these inquiries in writing to their home address. This is highly inappropriate and unprofessional. You have also reached out to employees on their personal cell phones, emails, and even reached out to employee’s family members. This is disturbing. Do not use an employee’s home addresses or relatives to contact them.” (The emphasis was hers.)\n\nProPublica replied the following day that it’s common practice for journalists to reach out to people we are writing about. “In fact, it’s our professional obligation,” O’Matz wrote.\n\nBiedermann responded: “Reaching out to individuals about a work matter at their private address is not journalism — it is borderline intimidation. In today’s political climate it is particularly unacceptable. We received your inquiries (via email, phone calls, text messages, both on work and personal email address) and made a conscious decision not to respond, as we have every right to do.”\n\n“You are not entitled to a response from us, or anyone, ever,” Biedermann wrote.\n\nTo be clear, at no time prior to this email did the department tell O’Matz that it had received her inquiries and would not comment. The article ran on Oct. 8, about two months after we first contacted the department. (I would highly encourage you to read it.)\n\nThe world has come a long way since the days of “All the President’s Men” and “Spotlight,” movies that favorably portrayed journalists knocking on doors and trying to reach sources to tell important stories — in those cases, about the Watergate break-in that led to President Richard Nixon’s resignation and the abuse scandal that enveloped the Roman Catholic Church in Boston and beyond.\n\nPresident Donald Trump has labeled his administration the most transparent in history, but at the same time, agencies in the executive branch have taken down datasets and pulled down public information. Trump has called the press “fake news” and called individual reporters derogatory terms. In this environment, our journalists have found that their efforts to get the real story and be fair were vilified rather than appreciated. Condemned, not commended.\n\nTake what happened with Doug Bock Clark, a reporter in ProPublica’s South office. Clark was working on a story about North Carolina Supreme Court Chief Justice Paul Newby, who has remade the court to make it more partisan.\n\nNewby wouldn’t talk to Clark, so Clark interviewed over 70 people who know Newby professionally or personally, including former North Carolina justices and judges, lawmakers, longtime friends and family members. Clark reached out to Newby’s daughter, Sarah, who is the finance director of the North Carolina GOP.\n\nWhen ProPublica emailed questions to Sarah Newby, the North Carolina Republican Party’s communications director, Matt Mercer, responded, writing that ProPublica was waging a “jihad” against “NC Republicans,” which would “not be met with dignifying any comments whatsoever.”\n\n“I’m sure you’re aware of our connections with the Trump Administration and I’m sure they would be interested in this matter,” Mercer said in his email. “I would strongly suggest dropping this story.” (The emphasis was Mercer’s.)\n\nOr consider what happened to Vernal Coleman, a reporter in our Midwest office who has been reporting on the Department of Veterans Affairs this year as part of a team. They’ve reported how doctors and others at VA hospitals and clinics have sent sometimes desperate messages to headquarters explaining how the Trump administration’s cuts would harm veterans’ care. (The VA provides health care to roughly 9 million veterans.) And they’ve reported how nearly 40% of the doctors offered jobs at the VA from January through March of this year turned them down.\n\nColeman was pursuing a story of interest and identified a potential source in Michigan. In an effort to contact them, Coleman visited the person’s home. He introduced himself as a reporter and explained his reasons for being there. They had a pleasant conversation, but the person ultimately declined to speak about the VA without prior authorization from their superiors.\n\nA few days later, VA Secretary Doug Collins sent out a tweet that accused Coleman of trying to “stalk” the employee.\n\nDoor-knocking is not stalking, as reporter Gina Barton explains in this 2023 Milwaukee Journal-Sentinel column. Indeed, federal employees have a First Amendment right to talk to the press, courts have ruled as they’ve invalidated policies preventing it.\n\nJust as my colleagues did, I reached out to those featured in this article to give them an opportunity to comment.\n\nBiedermann wrote, “Sincerely hope you print the entire back and forth so that readers understand the ProPublica method of ‘journalism.’”\n\nMercer wrote: “Doug Bock Clark needs a hobby besides his weird obsession with North Carolina’s judges. Maybe knitting or surfing. Have a nice day!”\n\nAnd VA spokesperson Peter Kasperowicz wrote: “Vernal’s uninvited visit to the home of a VA employee was rude, creepy and stalker-like. No VA employee should have to worry about being accosted at home by an uninvited reporter whose sole mission is to make their employer look bad.”\n\nWhen told that Coleman had received threatening notes after Collins tweeted about him, Kasperowicz wrote: “We condemn all violence and threats of violence, but the secretary simply publicly highlighted Vernal’s actions. ProPublica literally does the exact same thing in every story it writes. ProPublica’s website says it wants to ‘spur reform through the sustained spotlighting of wrongdoing.’ The fact that you are whining about the spotlight being turned on one of your reporters proves you’re nothing but a bunch of hypocrites.”\n\nTo be clear, Coleman did nothing wrong. The same is true of O’Matz and Clark. I am proud to call them my colleagues. They exemplify what fairness in journalism looks like.\n\nAs 2026 approaches, ProPublica remains committed to telling stories of public interest and continuing to offer the subjects of our stories an opportunity to comment. As members of the public who rely on accurate reporting, you should expect no less.",
    "readingTime": 8,
    "keywords": [
      "i’m sure",
      "doug bock",
      "again aug",
      "bock clark",
      "trump administration",
      "stories",
      "reporter",
      "email",
      "press",
      "cell"
    ],
    "qualityScore": 1,
    "link": "https://www.propublica.org/article/propublica-reaching-out-reporting-obstacles",
    "thumbnail_url": "https://www.propublica.org/wp-content/uploads/2025/12/OG-GettyImages-2237331815.jpg?resize=2000,1050",
    "created_at": "2025-12-30T18:18:19.072Z",
    "topic": "tech"
  },
  {
    "slug": "documentation-for-developers",
    "title": "Documentation for Developers",
    "description": "Creating onboarding documentation for your product? Or just writing up a \"how-to\" for a new internal tool? Here's how you do it.",
    "fullText": "You have 1 article left to read this month before you need to register a free LeadDev.com account.\n\nReceive weekly engineering insights to level up your leadership approach.\n\nEstimated reading time: 5 minutes\n\nDocumentation can be a pain for developers to write and navigate alike. But following this framework might be just the thing you need to make document navigation easier.\n\nDocumentation is the lifeblood of any high-functioning team. At a high level, documents are written to outline the feature processes, details, and general “how-to” elements of a system. The result? A bank of saved knowledge that helps new developers get up to speed quickly, providing a common reference point for all necessary stakeholders.\n\nA common pain point for many teams is unstructured documentation. Without clear guidelines, it’s hard to organize information in a way that’s easy to digest or search through. My team faced this problem when creating onboarding documentation for our new AI product; this would ultimately be used by engineers at client companies integrating the product into their systems.\n\nWe adopted the Diátaxis method as our guiding framework for technical documentation. The Diataxis approach identifies four distinct user needs in documentation: tutorials, how-to guides, explanations, and references.\n\nUltimately, this led to our documentation process improving and our user experience boosting.\n\nWhile the advantages of documentation are manifold, developers (including myself) tend to skip dipping into them if they appear to be unstructured or have an undefined goal.\n\nEach page should have a clear purpose. This sets expectations and helps engineers identify which pages answer their questions. Organizing pages with anticipated questions at the forefront of the design can help you build documentation that engineers navigate more intuitively.\n\nPut yourself in a developer’s shoes. If they’re using a product for the first time, what questions would they likely ask?\n\nStructure your content with user needs first. When your docs are built around real questions, they become easier to flick through and far more useful.\n\nAvoid packing too many ideas into one section. In doing so, you might create a long-winded document that jumps between topics and leaves explanations of singular issues scattered across multiple pages.\n\nThe Diátaxis framework outlines how documentation is composed of three key elements:\n\nSome developers have the skill for content and style, but could bolster their architecture abilities.\n\nWhat’s unique about Diátaxis is that it addresses the challenges of technical writing and breaks them down into four key sections: tutorials, how-to guides, explanations, and references.\n\nThe two quadrants, tutorials and how-to guides belong in the action region. Tutorials and how-to guides are not meant to be consumed passively by readers; rather, they actively help devs learn a new skill or achieve a specific goal.\n\nThe two bottom quadrants, explanations and references, belong in the cognition region. These are resources that readers can read or skim to understand and grasp the essence of a solution.\n\nTutorials and explanations are about skill acquisition. They’re designed to teach – not to solve an immediate task, but to build understanding. The goal is to help readers pick up a new skill relevant to the new tool or product they’re working with.\n\nHow-to guides and references are about skill application. They’re for readers who have already learned the basics and are now ready to put that knowledge into action.\n\nWhen we started to build documentation for our product, we geared the content toward two different technical stakeholders: data scientists and AI engineers. After all, our goal was to have data scientists lean on our AI product when running simulation experiments, subsequently empowering engineers to deploy these experiments to production.\n\nBefore embarking on writing the documents, we defined two main personas and described some key qualities about them to better identify what documentation we needed to create. Let’s call them Alice and Bob.\n\nOnce we defined our two main personas, we designed the following navigation bar for our docsite:\n\nWe have six main “folders” with a defined purpose for each, highlighting the experience level required. The first three folders are meant for beginners or users being onboarded. The next three folders are for already-onboarded users, who have specific goals they want to achieve or details they want to look up.\n\nNote that some of these folders may not be necessary for your solution. For example, if you’re not introducing new abstractions or concepts that readers need to know about, the “concepts folder” may not be required for your docsite.\n\nAfterwards, we assigned our developers to write different parts of the documentation following this architecture for the two main personas, Alice and Bob. Here are some examples where we map a question to a folder.\n\nAs Alice and Bob become better acquainted with the resources, so too does their mental model of the documentation’s architecture. This helps in instances where they may need to help newer colleagues navigate the system. For instance, if Alice is working with the APIs and wants to onboard a new data scientist, she can pull from previous experience and quickly direct the data scientist to the right folder.\n\nStrong developer documentation can be a major selling point for a product. Developers do not trust using a new product in their tech stack unless they can understand its underlying details and have clear product documentation to search through.\n\nSo it’s your job to create a comprehensive database of knowledge that can support devs while they get to grips with your product.",
    "readingTime": 5,
    "keywords": [
      "user needs",
      "how-to guides",
      "guides explanations",
      "tutorials how-to",
      "documentation",
      "product",
      "developers",
      "engineers",
      "skill",
      "readers"
    ],
    "qualityScore": 1,
    "link": "https://leaddev.com/communication/build-documentation-developers-actually-navigate",
    "thumbnail_url": "https://leaddev.com/wp-content/uploads/2024/12/2-1-1024x576.png",
    "created_at": "2025-12-30T12:23:27.515Z",
    "topic": "tech"
  },
  {
    "slug": "writing-usb-device-firmware-with-raspberry-pi-pico-and-tinyusb",
    "title": "Writing USB Device Firmware with Raspberry Pi Pico and TinyUSB",
    "description": "Raspberry Pi Pico With TinyUSB",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.youtube.com/playlist?list=PL4C3a7zUGIuYu48KsA3krgm7rtLJwse03",
    "thumbnail_url": "https://i.ytimg.com/vi/hog4VYeQbbo/hqdefault.jpg?sqp=-oaymwEXCOADEI4CSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAETSi3xXbzF4TNyr4PSwpbRcKOzw&days_since_epoch=20450",
    "created_at": "2025-12-28T18:16:55.704Z",
    "topic": "tech"
  },
  {
    "slug": "using-cursor-for-importing-data-into-notion",
    "title": "Using Cursor for Importing Data into Notion",
    "description": "Notion has great UX for manipulating tabular data but getting data formatted into a Notion Database is time consuming. It turns out, using Cursor for writing throwaway data manipulation scripts is a great solution for this.",
    "fullText": "Notion is my preferred tool for manipulating tabular data because of its clean UX. In the past, the most time consuming part of using Notion has been structuring data into a Notion Database. Notion has a great built-in CSV file importer that lets you cleanly map a CSV’s columns to Notion Property Types, but I’ve found that even getting a cleanly formatted CSV file to be time consuming.\n\nSurprisingly, it turns out Cursor (and Claude Code) are a great way of getting data into Notion. And I’ve used this setup for everything from vacation planning to sales prospecting to even hacky evaluations on AI prompts and models.\n\nCursor and Claude Code are both tools featuring AI Coding Agents intended for developers building software. But the same AI Coding Agents are also great at writing throwaway scripts for pulling data down from APIs, cleanly formatting the data, and uploading it into a Notion Database.\n\nThe most obvious path is using Cursor or Claude Code to format data into a CSV and then manually upload it to a Notion Database. But, I’ve found simply prompting Cursor/Claude Code to use the Notion Javascript SDK works well enough and lets me be completely hands-off in this data ingestion process.\n\nThe workflow for this is dead simple. I create a throwaway directory with a single .env file with my Notion Internal Integration Secret and whatever API keys I happen to need.\n\nThe rest is just writing the prompt within Cursor that includes:\n\nThe rest of this post describes how to get Notion set up so you can programatically interact with it using the API and a few concrete examples I’ve used in my personal life and in my work as an early stage founder.\n\nI’m planning a Summer 2026 backpacking trip with some friends and picked up “Backpacking Washington: Overnight and Multiday routes” to get a curated list of recommendations and trail notes.\n\nWithin the book is a great tabular index of all the hikes in the book, over 80 of them in total!\n\nI’m still using the physical book for its valuable trail notes, but I want to quickly filter down the list based on mileage and season while also collaborating on this selection process within Notion. Manually getting these into Notion would be painfully tedious, so I create a throwaway directory with pictures of all the pages with this table.\n\nIt takes some back and forth with Cursor to get the right mapping from the pictures of the table to proper Notion Properties, but end to end it takes me less than 30 minutes.\n\nOne of the tricks I’ve used for Sales Prospecting is using Google X-Ray searches. Among other things, it let’s you restrict the domain you’re searching against.\n\nFor example, one of the earlier product ideas I worked on was a tool for businesses using Notion. These businesses often use Notion for their job postings and since they’re publically listed, you can specifically search for them using a Google X-Ray Search.\n\nHere’s what the Google X-ray search results look like when only searching across published Notion Pages: site:notion.site \"job board\":\n\nTo get this Google Search Results programatically, I use SerpAPI.\n\nMy temporary directory starts out like this:\n\nI prompt Cursor to hit the SerpAPI with the site:notion.site \"job board\" query and save the results to a JSON. Then I tell Cursor to get that JSON data into a Notion table.\n\nGrabbing the results from an X-ray search is the first step and I’m still manually qualifying these potential leads, so Notion’s UX helps me out in this manual process.\n\nOutside of Google X-ray searches from SerpAPI, I’ve also used Exa’s Web Search API for searches where I don’t have exact keywords or domains nailed and Firecrawl to fetch the content on the links themselves.\n\nIf this workflow of using 3rd party data providers to hydrate a table feels familiar, it’s because I’m basically frankensteining Notion + Cursor to be a stand in for Clay.com or freckle.io. Use those tools if this data enrichment workflow is something you’re using every day, but since I’m only occasionally building these kinds of lists, this hacky setup works well enough for me and saves me from paying an extra subscription.\n\nWhen building out AI features, you often need to manually evaluate results before eventually having more elaborate scoring and judging systems in place. This is probably the most hacky usage of Notion, but, I like being able to easily tag rows with a Multi-Select Notion Property.\n\nIn this instance, I’m running my product locally and testing the feature against multiple inputs to compare the output of different models. I’m then getting this into Notion just to have a quick and dirty evaluation.\n\nThis is admittedly a tortured example, so here are much better alternatives for LLM Prompt Engineering and Evaluation:\n\nThis section is on setting up Notion if you want to use the Notion SDK approach. If you just want to upload CSV into a Notion Database, you don’t need to do this.\n\nHere’s the official Notion documentation on how to set up an integration:\n\nhttps://developers.notion.com/docs/create-a-notion-integration#getting-started\n\nRegardless, I’ll summarize the steps here too…\n\nGetting a Notion Integration Key\n\nOpen up your Workspace Settings from the left Sidebar. Navigate “Settings” > “Connections” > “Develop or manage integrations”\n\nFrom this page, create a new integration with Type “Internal”.\n\nOnce you’ve created the integration, you can copy the “Internal Integration Secret” which we’ll put into our .env in the directory we’ll open with Cursor.\n\nBefore we can start interacting with Notion via the API, we need to enable the integration per Page we want to manipulate.\n\nWithin the page we want to manipulate, we go into the Page Settings then “Connections” and then we find our newly created integration.\n\nYou’re now good to go. Make sure to feed this page’s URL into the Cursor prompt so it knows where to create a new database for your data upload.\n\nNotion has its own MCP Server with tools that cover most of the basic functionality you might want: https://developers.notion.com/docs/mcp-supported-tools. Depending on your use case, you might be better off connecting the Notion MCP to your regular ChatGPT / Claude account without reaching for a developer-facing product like Cursor or Claude Code.\n\nIf you’re planning to use the Notion SDK, Notion has a rate limit of ~3 requests/second and currently doesn’t have a bulk Page Create API (see Page Create API here), so you’ll want to Rate Limit your API requests by using something like ratelimit-js. For non-technical folks reading this, it’s usually sufficient to prompt Cursor/Claude Code to rate limit requests without intervention.",
    "readingTime": 6,
    "keywords": [
      "coding agents",
      "csv file",
      "x-ray searches",
      "integration secret",
      "x-ray search",
      "internal integration",
      "page create",
      "trail notes",
      "create api",
      "site:notion.site job"
    ],
    "qualityScore": 1,
    "link": "https://alprielse.xyz/posts/using-cursor-for-importing-data-into-notion/",
    "thumbnail_url": "https://alprielse.xyz/posts/using-cursor-for-importing-data-into-notion.png",
    "created_at": "2025-12-28T06:18:09.800Z",
    "topic": "tech"
  },
  {
    "slug": "i-wrote-a-book-while-working-fulltime-these-3-productivity-habits-helped-me-do-it-without-sacrificing-sleep",
    "title": "I wrote a book while working full-time. These 3 productivity habits helped me do it without sacrificing sleep.",
    "description": "Joshua Nelken-Zitser, a senior reporter at Business Insider,  balanced his job with writing his first book. Consistency and celebrating small wins were key.",
    "fullText": "When I got my book deal with HarperCollins UK, I was thrilled that a childhood dream of becoming an author was coming true.\n\nThen, reality hit. I had less than a year to research and write an 80,000-word book.\n\nI had never taken on anything this big before, and certainly not alongside a full-time job. The prospect of juggling a 9-to-5 as a journalist with such a time-consuming passion project felt terrifying.\n\nBut by being intentional with my time and relying on a few key productivity habits, I've ended up with a book I'm proud of, all without losing sleep.\n\nI asked a couple of author friends how they balanced their day job with writing a book. One said he wrote in the twilight hours; the other woke up at the crack of dawn.\n\nI also looked for clues from people I interviewed at work, like 21-year-old Nathaneo Johnson, who ran a startup while studying at Yale and told me he often put in 18-hour days.\n\nIt became clear they found time for their passion projects by eating into their sleep. But sacrificing precious shut-eye was not something I was prepared to do.\n\nWhile some people are able to get by on less than the recommended seven to nine hours of sleep, I'm not wired that way. When I'm sleep-deprived, I am not my best self, nor my second or third best. I tend to get grouchy, lose focus more easily, and become more prone to getting sick.\n\nIf sleep was non-negotiable, I knew that I needed to carve out time elsewhere.\n\nMy free hours after finishing work and at weekends seemed like the most obvious place to claw back some time to write my book. Normally, those slots would be filled with episodes of \"Below Deck,\" reading fiction, or meals with friends.\n\nI worked out that freeing up roughly two hours on workdays and a sensible eight hours on weekends, by cutting down on my favourite activities for almost a year, would be manageable. Although it doesn't sound like much, the hours add up.\n\nOver a month, it would give me more than 100 hours to work with.\nAcross the eleven months between signing my deal and my deadline, that would come to almost 1,000 potential hours — the equivalent of 45 straight days of work without sleep.\n\nI didn't know how many hours it would take to finish my book, but the math showed that the workload would be feasible if I chipped away at it steadily, rather than cramming the writing into an impossible final stretch.\n\nIn the end, I doubt I racked up anywhere close to 1,000 hours (I took days off to relax, socialize, and vacation), yet I still managed to finish my first draft several weeks ahead of schedule.\n\nWhile writing a book is a creative process, I approached my schedule with scientific precision.\n\nI built a basic Google Sheet template to track my progress and created a formula that took into account my total word count target and the number of days I had left. This gave me a total number of words I needed to write each day to stay on track. I updated it after every writing session.\n\nWatching the total word count climb and the daily target shrink gamified the process, which was both motivating and reassuring. It also helped me judge when I was ahead of schedule and could therefore say \"yes\" to social plans.\n\nI saw how my incremental efforts were adding up. Consistency, I learned early on, was the only way to tackle such a project.\n\nI didn't follow my schedule perfectly: there were impromptu reporting trips, days I was too exhausted to work, and social plans I didn't want to turn down. But being consistent on average gave me enough wiggle room that these interruptions didn't derail my entire schedule.\n\nThroughout the process, I tried not to become a recluse. I didn't want to lose friends or my sanity along the way, so I made sure to have at least one thing each week to look forward to, whether it was hanging out with family or simply taking a long walk and enjoying a chai latte with friends.\n\nMarking milestones mattered, too. At a third, halfway, and two-thirds of the book, my husband and I either went out for dinner or popped open a bottle of Prosecco. And when I finally handed in my manuscript, we went on vacation.\n\nCelebrating the small wins gave me concrete, near-term rewards to work toward, which kept me motivated and protected my mental health. I locked in time for the gym, where I'd watch reality TV on the elliptical, to make sure that I was staying happy and healthy.\n\nI tried to adopt a \"work smart, not hard\" mindset, occasionally using online tools to make my life easier. Instead of wasting time formatting or tidying my chaotic notes, I'd prompt ChatGPT to turn them into clear, bullet-point summaries. I also used AI transcription software to write up interviews.\n\nSimple shortcuts like these saved me from tedious admin and freed up time for the stuff that actually mattered, like research and writing.\n\nI didn't get everything right. Some days I was well ahead of schedule, but pushed myself to work anyway, out of guilt for feeling lazy. A couple of times that brought me close to burnout and forced me to take a few days off from writing to recuperate. I should have listened to my body and paced myself.\n\nI also recognize that I had some advantages: I don't have kids or other caregiving responsibilities, my husband kindly took on more than his fair share of household chores, and my workplace allowed me a short period of unpaid leave to focus on my book.\n\nStill, being intentional with my time made juggling a full-time job and a book far more manageable than I could have ever imagined.\n\nI learned that I didn't need to sacrifice sleep to live out a lifelong dream, as long as I used my waking hours wisely.\n\nNow, with all that work behind me. I can simply look forward to Trauma Bonds being published in January 2027.",
    "readingTime": 6,
    "keywords": [
      "social plans",
      "look forward",
      "full-time job",
      "hours",
      "book",
      "didn't",
      "sleep",
      "schedule",
      "friends",
      "ahead"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/wrote-book-working-full-time-productivity-habits-helped-sacrificing-sleep-2025-12",
    "thumbnail_url": "https://i.insider.com/6942833064858d02d216ef32?width=1200&format=jpeg",
    "created_at": "2025-12-27T12:21:15.441Z",
    "topic": "finance"
  },
  {
    "slug": "ai-writing-agent-that-flags-unsupported-claims-for-review",
    "title": "AI writing agent that flags unsupported claims for review",
    "description": "Write high-quality reviews, listicles, and how-to guides in 13 languages—powered by trust signals and grounded research.",
    "fullText": "No hallucinations. No hours of research. No AI slop.\n\nProofWrite blends keyword research, automated product research, trust-signal analysis, and AI writing to create factual, humanized articles that rank. Every insight is grounded in real proof.\n\nProduct review, listicle, how-to, comparison, or freeform. Keyword research surfaces winning terms and sets coverage targets.\n\nAutomated research from official and authoritative sources. Trust signals, ratings, and citations pulled from platforms like Trustpilot, Capterra and Reddit.\n\nSEO, AEO & GEO-optimized copy backed by citations. Choose Claude, GPT, or Gemini. Every claim tied to research.\n\nPush to WordPress or export anywhere. Fully editable drafts with keyword coverage guidance with SEO, AEO and GEO metrics before publishing.\n\nProofWrite is designed for teams and creators who need accurate, SEO-optimized content at scale.\n\nScale product reviews, comparisons, and listicles with real specs, pricing, and trust signals that convert.\n\nProduce research-backed how-to guides and product content 10x faster without sacrificing quality.\n\nDeliver optimized, fact-checked content to clients with built-in keyword coverage and scoring.\n\nMeet E-E-A-T standards with automated trust signals, citations, and verified research in every article.\n\nProofWrite scores every outline across the pillars that move rankings: keyword coverage, structure, media, and more. Each edit triggers a fresh calculation so you know exactly what to improve before you publish.\n\nScore updates in real time as you refine keywords, images, and structure.\n\nCoverage, structure, and media breakdowns show exactly what needs work.\n\nMedia gaps, thin sections, or missing H2s surface instantly for quick fixes.\n\nThe only tool that optimizes content for AI search engines and LLM citations.\n\nRe-scored on every edit you make\n\nQuestion headings, source citations, and quotable claims.\n\nEvery claim in your article gets a verdict. For claims that need attention, choose from three instant actions: verify manually, add a source URL, or let AI rewrite it.\n\nMark claims as reviewed when you've confirmed the facts yourself with one-click verify feature.\n\nPaste a URL and the system extracts supporting evidence automatically.\n\nAI rewrites the claim to make it factual.\n\n“Notion's Business plan costs $15 per user per month”\n\nPricing, features, ratings, and policy claims are verified against your research data.\n\nEnter any article URL and target keyword to get an instant SEO, AEO & GEO analysis. See how well your content is optimized for search engines and AI.\n\nToggle between the input brief and the final article to see how ProofWrite threads research, trust signals, and structure into a cohesive narrative. No AI slop, no hallucinations, no em dashes just high-quality content in a single shot. More writing examples can be found in our blog.\n\nYou have an idea. It keeps you up at night, a software solution that seems perfect. The urge to open your laptop and start building immediately is overwhelming. You can already see the dashboard, the features, and the user interface in your mind.\n\nWhen you first dip your toes into the world of SaaS (Software as a Service), it is easy to make the \"classic mistake\" identified by seasoned developers on Indie Hackers: spending months building a tool before figuring out if anyone actually wants it. The result? You launch to silence. You spent time and energy solving a problem that perhaps didn't exist, or at least not in the way you thought it did.\n\nThe landscape of digital entrepreneurship has changed. You no longer need a computer science degree or a venture capital injection to start. As noted by the community at r/BuildToShip, the modern approach is about shipping fast, learning in public, and turning ideas into real products through lean validation.\n\nThis guide is your blueprint for launching a Micro-SaaS, a small, niche-focused software business run by one person or a tiny team. We will walk through the process of validating your idea in as little as 48 hours with $0 upfront cost, utilizing no-code tools and AI-driven automation to minimize risk and maximize impact.\n\nBefore you worry about tech stacks, logos, or LLCs, you must answer one question: Will people pay for this?\n\nMany aspiring founders believe building SaaS is about passion and code. However, insights from the \"Income AIcademy\" suggest that this mindset is a fast track to nowhere. The real process involves validating demand before the product exists.\n\nThe era of broad, horizontal software (like generic project management tools) is dominated by giants. Your opportunity lies in the \"Micro.\"\n\nAccording to trends for 2025 highlighted by Sidetool, profitable Micro-SaaS opportunities are unlocked by focusing on niche markets. You aren't trying to serve everyone; you are trying to serve a very specific group of people with a very specific problem.\n\nNarrow your scope: Instead of \"accounting software,\" think \"expense tracking for freelance underwater photographers.\"\n\nLook for manual friction: Identify tasks that businesses are currently solving with messy Excel spreadsheets or endless email chains.\n\nLeverage AI trends: Consider how AI-driven automation can solve these specific problems faster or cheaper than a human could.\n\nCan you describe your target customer in one sentence? (e.g., \"Estate agents who struggle to schedule viewings.\")\n\nIs the problem painful enough that they are currently paying (money or time) to solve it poorly?\n\nYou might think you need a finished product to sell it. You don't. In fact, successful creators have validated microniche ideas in 48 hours without spending a dime. The goal here is to collect \"signals of interest\" rather than users.\n\nDraft a Value Proposition: Clearly articulate what problem you solve. Avoid technical jargon. Speak to the pain point.\n\nFind the Watering Holes: Go where your niche hangs out. This might be specific subreddits, Facebook groups, or LinkedIn communities.\n\nEngage, Don't Spam: Do not just drop a link. As advised by the r/BuildToShip community, the goal is to \"learn in public.\" Share your hypothesis. Ask questions like, \"I'm noticing [Problem X] is a huge time sink for [Niche Y]; how are you currently handling this?\"\n\nThe \"Smoke Test\": Create a simple landing page (using free tiers of site builders) or even a direct message script that describes the solution. Ask for an email address or a pre-order to get early access.\n\nWhy this matters: If you cannot find people to talk to about the problem, or if nobody is willing to give you their email address for a solution, you will not be able to sell the product later. Silence now saves you months of coding later.\n\nDo you have a list of 10–50 people who said, \"Yes, I need this\"?\n\nDid you complete this outreach within a 48-hour window to prevent procrastination?\n\nOnce, and only once, you have validated that real humans want your solution, you can start building. But you aren't writing code from scratch. You are using the \"No-Code\" approach to remain lean.\n\nKnack and similar platforms have popularized the idea that you can build robust applications without traditional programming. Your goal is to build a \"Minimum Viable Product\" (MVP), the simplest version of your tool that delivers the core value.\n\nDatabase: Start with where the data lives. In no-code tools, this often looks like a spreadsheet or a visual database.\n\nLogic/Automation: Use automation tools to connect different apps. For example, if your SaaS generates reports, set up a workflow that triggers when a user submits a form, processes the data via AI, and emails the PDF.\n\nInterface: Use a drag-and-drop builder to create the front end where users log in and interact with your data.\n\nPro Tip: Don't get hung up on scalability. You don't need a system that handles a million users. You need a system that handles your first 10 users perfectly.\n\nTo compete in 2025, your Micro-SaaS needs an edge. Sidetool suggests leveraging AI-driven automation to maximize impact.\n\nIdentify the \"Magic\" Moment: Where can AI save the user the most time? Is it writing text, analyzing data, or generating images?\n\nIntegrate via API: Most no-code platforms allow you to send data to AI models (like OpenAI's API) and receive a response.\n\nKeep a Human in the Loop: Ensure your users can review the AI's output. AI is powerful but can hallucinate; trust is built on reliability.\n\nDoes the product actually solve the core problem you validated in Phase 1?\n\nCan a user go from \"Sign Up\" to \"Problem Solved\" without your manual intervention?\n\nBuilding is comfortable. Shipping is scary. But as the r/BuildToShip hub emphasizes, you must be willing to ship fast and talk about growth.\n\nRemember those 50 people who gave you their email addresses in Step 2? They are your beta testers.\n\nPersonal Outreach: Email them personally. \"Hey, remember that tool we talked about? It's ready for you to try.\"\n\nGather Feedback: Your first version will have bugs. It will lack features. That is okay. Ask your early users, \"What is the one thing preventing you from loving this?\"\n\nCharge Money Early: Free users give polite feedback. Paying users give honest feedback. Even a small price tag ($5/month) validates that the problem is painful enough to pay for.\n\nOne of the strongest strategies for Micro-SaaS growth is transparency. The \"Build in Public\" movement encourages sharing your wins, losses, and revenue numbers.\n\nDocument the Journey: Share updates on social media or indie hacker communities. \"Today I fixed a bug that caused X\" or \"We just got our 10th subscriber!\"\n\nAsk for Help: Communities like r/BuildToShip exist to help you talk growth, tech, and tools. If you are stuck on a pricing model or a technical hurdle, ask the community.\n\nIterate Quickly: The advantage of being a \"Micro\" SaaS is speed. If users hate a feature, you can remove it today. If they need a new button, you can add it tonight. Large competitors cannot do that.\n\nHave you moved from \"Validation\" (interested people) to \"Traction\" (active users)?\n\nAre you actively engaging with a community of peers to keep your momentum up?\n\nEven with a lean plan, you will encounter hurdles. Here is how to navigate the common traps of the Micro-SaaS journey.\n\nThe Issue: You feel the product isn't \"ready\" because it lacks a dark mode, multiple language support, or a referral system. The Fix: Go back to your validation. Did your early users say they wouldn't buy without dark mode? Probably not. Build only what is necessary to solve the core pain point. As the research indicates, spending months building before validating is the classic mistake.\n\nThe Issue: Everyone says \"Great idea!\" but nobody buys. The Fix: Compliments are not validation. Cash is validation. If people say they love it but won't pull out a credit card, you haven't found a painful enough problem, or you are talking to the wrong audience. Revisit Step 1 and narrow your microniche further.\n\nThe Issue: Trying to do everything (marketing, support, dev) alone. The Fix: Utilize the automation tools mentioned in the research. If a task feels repetitive, automate it. Your energy should be spent on talking to users and improving the product, not manual data entry.\n\nQ: Do I really need $0 to start?\n\nA: Strictly speaking, validation costs $0. You can use free social media, free email accounts, and free tiers of landing page builders to gauge interest. Costs only accrue once you start hosting a live application or paying for advanced no-code subscriptions, at which point you should ideally have paying customers to cover those costs.\n\nQ: What if I don't have a technical background?\n\nA: That is the power of the current landscape. Between no-code platforms (like Knack) and AI assistance, the barrier to entry has lowered significantly. The skill you need is problem-solving, not necessarily syntax coding.\n\nQ: How do I know if my niche is \"micro\" enough?\n\nA: If you are competing directly with Google, Microsoft, or Salesforce, your niche is too broad. If you are serving a specific profession (e.g., \"Dentists\") with a specific problem (e.g., \"Patient recall SMS automation\"), you are in the right zone.\n\nQ: What if my idea fails validation?\n\nA: Then you have succeeded. You saved yourself months of development time. The 48-hour validation process is designed to fail fast so you can move on to your next idea without baggage.\n\nThe path to a profitable Micro-SaaS is not paved with complex code or massive venture capital checks. It is paved with conversations, empathy for user problems, and the courage to ship imperfect solutions.\n\nDon't let your idea stay an idea. Go find your niche, ask the hard questions, and ship your solution. The community at r/BuildToShip and the wider indie hacker world is waiting to see what you build.\n\nUnlike generic AI tools, ProofWrite is purpose-built for creating factual, research-backed content that ranks.\n\nStart creating research-backed content today\n\nChoose the plan that fits your content needs. Scale up as you grow.\n\nPlans include automated keyword & product research, trust signal analysis, and factual AI writing. Article and keyword limits reset monthly.\n\nNeed a custom plan? Contact us for enterprise pricing.\n\nEach brief pulls facts from official docs, verified reviews, and community discussions. ProofWrite keeps citations, trust signals, and keyword guidance inline so drafts never hallucinate data.\n\nProofWrite feeds the writer with verified research, trust signals, and any personal experiences you add. Tone and voice controls keep prose specific and conversational.\n\nProofWrite supports product reviews, best-of listicles, and step-by-step how-to guides. Each format has inputs, research crawl, and writing instructions tuned to the brief.\n\nArticles can be written in English, Spanish, French, German, Italian, Dutch, Portuguese, Danish, Norwegian, Finnish, Swedish, Romanian, or Polish.\n\nEdit inside the composer, then push to WordPress with one click or copy the article to your clipboard for other CMSes.\n\nYes. Set tone, POV, and personas, then add personal experiences and AI instructions. ProofWrite threads them through the draft so it reads like someone who actually used the product.\n\nFor long-form writing, ProofWrite uses all the SOTA models: Gemini 3 Pro, Claude Sonnet 4.5, Opus 4.5 and OpenAI's GPT 5.x. Set a workspace-wide default and override per project as needed.\n\nAll research, drafts, and account data stay inside your workspace. We never use your content to train external models or share it with third parties.\n\nStart creating factual, humanized, SEO-optimized articles today.\nFree to try, no credit card needed. Cancel anytime.",
    "readingTime": 12,
    "keywords": [
      "seo aeo",
      "ai-driven automation",
      "profitable micro-saas",
      "proofwrite threads",
      "search engines",
      "classic mistake",
      "venture capital",
      "maximize impact",
      "landing page",
      "dark mode"
    ],
    "qualityScore": 1,
    "link": "https://proofwrite.io/",
    "thumbnail_url": "https://proofwrite.io/og-image.png",
    "created_at": "2025-12-26T12:22:23.991Z",
    "topic": "tech"
  },
  {
    "slug": "wordwrightai-learn-vocabulary-by-writing-not-memorizing",
    "title": "Wordwright.ai – Learn vocabulary by writing, not memorizing",
    "description": "Master new vocabulary through spaced repetition. Contribute to kwakubiney/wordwright.ai development by creating an account on GitHub.",
    "fullText": "kwakubiney\n\n /\n\n wordwright.ai\n\n Public\n\n Master new vocabulary through spaced repetition\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n kwakubiney/wordwright.ai",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/kwakubiney/wordwright.ai",
    "thumbnail_url": "https://opengraph.githubassets.com/b2f8057c153e46dc231eabf0aea5b83a9a821d49f38f9c4b94c5573049d7580e/kwakubiney/wordwright.ai",
    "created_at": "2025-12-26T12:22:21.455Z",
    "topic": "tech"
  },
  {
    "slug": "multiscale-aperture-synthesis-imager",
    "title": "Multiscale Aperture Synthesis Imager",
    "description": "The authors create a distributed sensor array that achieves optical super-resolution without lenses, using computational synchronization to combine multiple sensors and expand imaging areas 16-fold beyond physical sensor dimensions.",
    "fullText": "Optical information transmission through complex scattering media with optical-channel-based intensity streaming\n\n Article\n Open access\n 23 April 2021",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41467-025-65661-8",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41467-025-65661-8/MediaObjects/41467_2025_65661_Fig1_HTML.png",
    "created_at": "2025-12-26T06:19:00.209Z",
    "topic": "tech"
  },
  {
    "slug": "demystifying-determinism-in-durable-execution",
    "title": "Demystifying Determinism in Durable Execution",
    "description": "Determinism is a key concept to understand when writing code using durable execution frameworks such as Temporal, Restate, DBOS, and Resonate. If you read the docs you see that some parts of your code must be deterministic while other parts do not have to be.  This can be confusing to a dev",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://jack-vanlightly.com/blog/2025/11/24/demystifying-determinism-in-durable-execution",
    "thumbnail_url": "http://static1.squarespace.com/static/56894e581c1210fead06f878/t/692460e5f654ee3c1d6b1e6f/1763991781863/control_flow_vs_side_effects_small.png?format=1500w",
    "created_at": "2025-12-26T06:18:55.983Z",
    "topic": "tech"
  },
  {
    "slug": "mr-tumble-calzaghe-big-dunc-what-weve-learned-about-rooney",
    "title": "Mr Tumble, Calzaghe, Big Dunc - what we've learned about Rooney",
    "description": "Disliking Mr Tumble, trying to fight Joe Calzaghe and writing to Duncan Ferguson in jail - what we have learned from The Wayne Rooney Show.",
    "fullText": "Who knew Wayne Rooney cannot bear Mr Tumble or tried to fight former world boxing champion Joe Calzaghe?\n\nElite footballers are often a closed book these days but one of England and Manchester United's greatest players has given a special insight into his life in 2025 through 'The Wayne Rooney Show'.\n\nThe BBC Sport podcast has brought intrigue, laughter and insight to its audience since it began in August.\n\nAs we reach the halfway point in the season, we take a look back at eight of the best moments the show has served up so far.",
    "readingTime": 1,
    "keywords": [
      "insight",
      "wayne",
      "rooney"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bbc.com/sport/football/articles/cy4x101jk91o?at_medium=RSS&at_campaign=rss",
    "thumbnail_url": "https://ichef.bbci.co.uk/ace/branded_sport/1200/cpsprodpb/4973/live/131f7eb0-e0bc-11f0-aae2-2191c0e48a3b.png",
    "created_at": "2025-12-25T12:22:20.192Z",
    "topic": "sports"
  },
  {
    "slug": "trump-79-declares-absurd-national-security-threat-in-latenight-meltdown",
    "title": "Trump, 79, Declares Absurd National Security Threat in Late-Night Meltdown",
    "description": "Donald Trump has accused The New York Times of being a national security threat in an unhinged Truth Social post. “The Failing New York Times, and their lies and purposeful misrepresentations, is a serious threat to the National Security of our Nation,” Trump wrote in a late-night social media meltdown. “Their Radical Left, Unhinged Behavior, writing FAKE Articles and Opinions in a never-ending way, must be dealt with and stopped. THEY ARE A TRUE ENEMY OF THE PEOPLE! Thank you for your attention",
    "fullText": "Donald Trump has accused The New York Times of being a national security threat in an unhinged Truth Social post.\n\n“The Failing New York Times, and their lies and purposeful misrepresentations, is a serious threat to the National Security of our Nation,” Trump wrote in a late-night social media meltdown.\n\n“Their Radical Left, Unhinged Behavior, writing FAKE Articles and Opinions in a never-ending way, must be dealt with and stopped. THEY ARE A TRUE ENEMY OF THE PEOPLE! Thank you for your attention to this matter. PRESIDENT DJT.”\n\nIt is unclear what prompted Trump’s latest attack on the newspaper he has long derided as part of the “fake news” media.\n\nHowever, the president and the White House were triggered for days over a November report in the Times revealing that the 79-year-old president has drastically reduced his public appearances compared to his first term.\n\nTrump, who is on track to become the oldest sitting U.S. president to date, is also starting his days later on average and working shorter hours than he did during his first stint in the White House, the Times reported.\n\nLast week, the paper also published a detailed report examining Trump’s friendship with the late pedophile Jeffrey Epstein, describing how the two “pursued women in a game of ego and dominance” in which “female bodies were currency.”\n\nIn one particularly damaging section, a mother who accompanied her 14-year-old daughter to a party at Mar-a-Lago with other young models claimed she was warned by Trump’s then-wife, Marla Maples: “Whatever you do, do not let her around any of these men, and especially my husband.” Maples denied making the remark to the Times.\n\nTrump also took aim at the Times during a Monday press conference, accusing the paper of insufficiently covering his plan to lower prescription drug prices.\n\nThe president ranted about the outlet while continuing to push a dubious claim that he had lowered the cost of prescription drugs by the mathematically impossible amount of up to “3,000 percent.”\n\n“A drug that sells for $10 in London is costing $130 in New York. We’re bringing it down to $20,” Trump said. “So we’re going down—you can do your own math, but it’s 2,000 percent, 3,000 percent. It’s pretty amazing. And, you know, the New York Times had a story about it, a small story, way in the back of the paper. It’s the single biggest thing to happen with respect to drugs probably in 50 years.\n\n“It’s the biggest thing ever to happen, and it’s barely covered in the New York Times because it’s a fake newspaper,” Trump added.\n\nThe Daily Beast has contacted The New York Times for comment.",
    "readingTime": 3,
    "keywords": [
      "new york times",
      "the new york times",
      "it’s",
      "trump’s",
      "paper",
      "threat",
      "unhinged",
      "media",
      "newspaper",
      "year-old"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/trump-79-declares-absurd-national-104859316.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/DvQySOHJjWUQ3Hd23rWZ9g--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU7Y2Y9d2VicA--/https://media.zenfs.com/en/thedailybeast.com/891dcad726760059a4ba544c952953ab",
    "created_at": "2025-12-23T18:17:46.410Z",
    "topic": "news"
  },
  {
    "slug": "warren-buffetts-company-took-kraft-heinz-off-its-subsidiary-list-weeks-before-board-exit-and-5-billion-writedown",
    "title": "Warren Buffett's company took Kraft Heinz off its subsidiary list weeks before board exit and $5 billion writedown",
    "description": "Warren Buffett's Berkshire Hathaway removed Kraft Heinz from its subsidiary webpage before leaving its board and writing down its stake by $5 billion.",
    "fullText": "Warren Buffett's Berkshire Hathaway removed Kraft Heinz from the list of operating companies on its website earlier this year, weeks before writing down its investment in the food and beverage giant and leaving its board of directors.\n\nThe famed investor's conglomerate took Kraft off its subsidiaries page in April, Business Insider determined using the Wayback Machine, a digital archive that stores snapshots of webpages on different dates.\n\nBerkshire accounts for its roughly 27% stake in Kraft using the equity method, meaning Buffett and his colleagues recorded it at cost and periodically adjust its carrying value to reflect Berkshire's share of Kraft's profits and losses.\n\nOn May 19, Berkshire's two board representatives stepped down. Berkshire also said in its second-quarter earnings that it was recording a $5 billion impairment loss on its Kraft position, cutting its carrying value to match its fair value of $8.4 billion.\n\nBuffett and his team said they had considered their \"ability and intent\" to remain invested until the fair value exceeded carrying value, the \"magnitude and duration\" of the decline in fair value, and Kraft's operating results and finances.\n\nThey also took into account the two board departures and the news that Kraft was evaluating potential strategic transactions, they said, and determined their unrealized loss on the holding was \"other-than-temporary.\"\n\nIt's unclear whether Berkshire removed Kraft from its subsidiary list as part of a broader distancing from the investment. Kraft was an unusual entry in the first place, as the vast majority of businesses featured are wholly owned subsidiaries of Berkshire, such as Geico, See's Candies, NetJets, and Pampered Chef.\n\nBerkshire Hathaway and Kraft Heinz did not respond to requests for comment.\n\nKraft announced in September that it would split into two businesses, with one focused on sauces, spreads, and seasonings such as Heinz and Philadelphia, and the other focusing on North American staples, including Kraft Singles and Lunchables.\n\nBerkshire partnered with 3G Capital, a Brazilian private equity firm, to acquire Heinz for around $23 billion in 2013. Two years later, the pair teamed up again to merge Heinz with Kraft in a $40 billion deal.\n\nSince then, the combined company has navigated layoffs, management reshuffles, huge writedowns, asset sales, a slumping stock price, aggressive cost controls, a federal accounting probe, and a prolonged decline in net revenues fueled by changing consumer preferences.\n\nDavid Kass, a finance professor at the University of Maryland and a longtime Berkshire blogger, told Business Insider in September that merging Kraft and Heinz was a \"rare mistake\" for Buffett.\n\nThe \"Oracle of Omaha,\" who spent the past six decades transforming Berkshire from a failing textile mill into a $1 trillion company, will step down as CEO next week. Buffett's handpicked successor and Berkshire's non-insurance chief, Greg Abel, will take the reins on New Year's Day.",
    "readingTime": 3,
    "keywords": [
      "berkshire hathaway",
      "removed kraft",
      "board",
      "carrying",
      "fair",
      "list",
      "operating",
      "investment",
      "subsidiaries",
      "determined"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/warren-buffett-berkshire-hathaway-kraft-heinz-stake-website-board-writedown-2025-12",
    "thumbnail_url": "https://i.insider.com/69496af604eda4732f2df686?width=1200&format=jpeg",
    "created_at": "2025-12-23T12:23:26.481Z",
    "topic": "finance"
  },
  {
    "slug": "alloconda-zig-toolkit-for-writing-cpython-extensions",
    "title": "Alloconda: Zig toolkit for writing CPython extensions",
    "description": "Zig-first Python extensions with cross-compiled wheels - mattrobenolt/alloconda",
    "fullText": "mattrobenolt\n\n /\n\n alloconda\n\n Public\n\n Zig-first Python extensions with cross-compiled wheels\n\n alloconda.withmatt.com\n\n License\n\n Apache-2.0 license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mattrobenolt/alloconda",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mattrobenolt/alloconda",
    "thumbnail_url": "https://opengraph.githubassets.com/a2496d8ff611964da1e92da6207f8854fcee7c8b17f1cf714d77ec0faa33320f/mattrobenolt/alloconda",
    "created_at": "2025-12-23T06:19:53.662Z",
    "topic": "tech"
  },
  {
    "slug": "we-built-an-ai-humanizer-to-fix-unnatural-ai-writing",
    "title": "We built an AI Humanizer to fix unnatural AI writing",
    "description": "Dechecker's AI Checker and Detector tool checks whether text is generated by AI models, such as ChatGPT, GPT-5, Claude, Gemini, LLaMa, etc.",
    "fullText": "Humanize AI-generated content and turn it into natural, human-quality writing from ChatGPT, Jasper, or Gemini in seconds.\n\nEnter or paste your text and click Humanize.\n\nUsing the Dechecker Humanizer takes only moments and requires no technical skills.\n\nPaste your AI-generated content into the AI Humanizer and review it briefly before starting the humanization process, ensuring that the original text is complete and ready for accurate human-like rewriting.\n\nChoose your preferred style, language, and length to guide how the AI Humanizer shapes the final human text, allowing you to customize tone, readability, and overall writing style for your intended audience.\n\nAfter using the AI Humanizer, review your text to ensure it has been properly humanize AI content, flows naturally, reads authentically, and maintains the original meaning, tone, and clarity throughout.\n\nCopy the Humanize AI result for use, or check it with Dechecker AI Checker to review AI Humanizer output and AI detection results, ensuring your text is fully human-like and suitable for publishing or sharing.\n\nDechecker focuses on what matters most: producing clear, natural, human-quality text you can confidently use anywhere.\n\nAI-generated content is carefully refined into natural, fluent writing using an AI Humanizer that removes robotic patterns, awkward phrasing, and mechanical-sounding sentences, making the text read smoothly and authentically like a real human wrote it.\n\nThis AI Humanizer works seamlessly across multiple languages, helping content sound human and natural without awkward translations or stiff wording, while preserving original meaning and readability for global audiences.\n\nTone, clarity, and overall flow are enhanced while keeping the original intent intact, producing human-style text that is easy to read, engaging for audiences, and maintains the message accurately across different formats.\n\nAfter rewriting, content can be reviewed with AI Checker like Dechecker to confirm it reads as human, avoids robotic signals, and ensures the output is indistinguishable from text written by real people.\n\nOur AI Humanizer helps users humanize AI text across various scenarios, turning AI-generated drafts into natural, human-like writing that reads smoothly and clearly.\n\nThe AI Humanizer helps writers improve blog posts, articles, and stories by refining AI-generated drafts, making them read naturally, flow smoothly, and engage readers more effectively while keeping original ideas intact.\n\nUse Humanize AI to refine essays, research papers, and reports, ensuring content sounds human, is clear and easy to understand, and maintains proper academic tone and logical structure throughout.\n\nAI Humanizer transforms marketing copy, social media posts, and emails into smooth, human-like text that resonates with audiences, boosts engagement, and maintains consistent brand voice across all channels.\n\nWith multilingual support, Dechecker AI Humanizer allows teams to produce human-quality content in different languages, preserving tone, meaning, and readability, ensuring professional communication worldwide.\n\nDechecker Humanize AI ensures course content, tutorials, and learning resources are readable, human-like, and engaging, helping students better understand complex topics and improving overall learning experience.\n\nUse Dechecker AI Humanizer to humanize AI-generated web content, making it more engaging, natural, and optimized for readers, while improving user experience and search engine readability simultaneously.\n\nReal feedback from users who have improved their AI-generated content with AI Humanizer, making writing feel more natural and human-like.\n\nFind answers to common questions about using AI Humanizer to humanize AI text and make content sound natural and human-like.\n\nAn ai humanizer is a tool designed to turn AI-generated text into human-like writing. It improves readability, sentence structure, and tone, helping content feel natural and engaging to real readers.\n\nAI Humanizer analyzes AI-generated text, restructures sentences, adjusts phrasing, and refines flow to humanize AI content, making it sound naturally written while keeping the original meaning intact.\n\nYes, the AI Humanizer supports multiple languages, including English, Spanish, French, German, and more. It ensures your text feels natural and human-like across all supported languages.\n\nAbsolutely. Dechecker Humanize AI allows you to customize writing style, tone, and length, making content suitable for blogs, articles, marketing copy, emails, and other professional uses.\n\nNo. AI Humanizer focuses on enhancing readability and natural flow without altering your key ideas, intent, or important information, keeping your message accurate.\n\nYes. AI Humanizer humanizes AI-generated text without fabricating information. It helps essays, reports, and professional content read naturally while maintaining integrity and clarity.\n\nDefinitely. After using Dechecker AI Humanizer, you can review the output with AI Checker to ensure the Humanize AI content reads naturally, appears human-written, and meets authenticity requirements.\n\nWriters, students, marketers, content creators, and businesses can all benefit. Anyone looking to make AI-generated content readable and humanize AI content efficiently will find the ai humanizer extremely useful.",
    "readingTime": 4,
    "keywords": [
      "ai humanizer",
      "ai-generated drafts",
      "marketing copy",
      "ai-generated content",
      "dechecker humanize",
      "ai-generated text",
      "natural human-quality",
      "content sound",
      "ai checker",
      "human-like"
    ],
    "qualityScore": 1,
    "link": "https://dechecker.ai/ai-humanizer",
    "thumbnail_url": "https://cdn.dechecker.ai/se/dechecker/public/logo/dechecker-logo.png",
    "created_at": "2025-12-23T06:19:37.153Z",
    "topic": "tech"
  },
  {
    "slug": "the-death-and-rebirth-of-programming",
    "title": "The Death and Rebirth of Programming",
    "description": "Programming didn't die all at once. There was no single moment, no dramatic obsolescence event. Instead, something quieter happened: the core constraint that shaped software for seventy years dissolved. Writing code stopped being the hard part.",
    "fullText": "For most of computing history, programming was bottlenecked by human cognition. Translating intent into working software required time, attention, and specialized skill. Even small changes were costly. This scarcity justified entire ecosystems: languages, frameworks, methodologies, reviews, team rituals that made sense when every line was expensive.\n\nGenerative AI removes that scarcity.\n\nToday, a single developer can generate thousands of lines of working code in minutes. Tomorrow, that number will be effectively infinite. The marginal cost of producing code is collapsing toward zero.\n\nWhat hasn't collapsed is the cost of knowing what the code does.\n\nUnderstanding, verifying, securing, and evolving software remain stubbornly expensive. In fact, they may be getting harder as volume explodes. This asymmetry—the ease of creation versus the difficulty of comprehension—is the defining tension of modern software.\n\nProgramming hasn't disappeared. But its center of gravity has shifted.\n\nIn the old world, programmers owned code. You wrote it, you understood it, you maintained it. Your value was tied to mastery of specific implementations. Codebases accrued history, reputation, and power.\n\nIn the new world, ownership becomes a liability.\n\nWhen code can be regenerated faster than it can be understood, preserving it for sentimental or historical reasons no longer makes sense. What matters instead is stewardship: maintaining the system's behavior, boundaries, and intent over time, regardless of how many times its internals are replaced.\n\nThis reframing is subtle but profound:\n\nThe asset is no longer the codebase. The asset is the system's ability to keep working.\n\nThis is the thesis of everything that follows. Architecture, testing, interfaces, team structure: all of it flows from this inversion.\n\nMany of the \"modern\" software practices of the last decade were early adaptations to this shift, even if we didn't articulate them that way.\n\nImmutable infrastructure. Stateless services. Containers. Blue-green deployments. Infrastructure as code.\n\nThese ideas all share a common premise: never fix a running thing. Replace it.\n\nAI pushes this premise beyond infrastructure and into application code itself. When rewriting is cheap, editing in place becomes risky. Mutation accumulates entropy. Replacement resets it.\n\nDisposability stops being a hack. It becomes the default.\n\nThis transition isn't just technical. It's deeply psychological, and that psychology shapes architecture.\n\nMany developers identify as builders and craftspeople. We take pride in elegance, cleverness, and mastery of internals. We accumulate knowledge inside our heads and inside codebases. Longevity feels like validation.\n\nGenerative AI destabilizes this identity.\n\nWhen a machine can produce a competent version of \"your\" solution in seconds, craftsmanship no longer lies in the artifact. It lies in framing the problem, defining success, and deciding what to keep and what to discard.\n\nThe role shifts from maker to architect. From author to managing editor. From preserving code to designing for its replacement.\n\nThat shift is uncomfortable. And the discomfort isn't merely personal. It's what makes teams resist the very patterns that would help them. Developers cling to codebases because identity is at stake, not just technical judgment. Acknowledging this is the first step toward building systems that don't require heroics to change.\n\nResisting the shift doesn't stop it. It just makes systems more fragile.\n\nOne of the clearest signals of this new era is the rise of the n=1 developer.\n\nProjects that once required teams now fit inside a single person's cognitive boundary—with AI filling in the execution gaps. Entire products can be specified, generated, evaluated, and shipped by one human working with machines.\n\nThis isn't about productivity hacks. It's about a structural change in leverage.\n\nBut n=1 development only works if systems are designed for it. Large, tangled, historically accreted codebases collapse under their own weight when AI accelerates change. Small, modular, disposable systems thrive.\n\nThe n=1 developer is not a superhero. They are an indicator species. They are evidence that the environment has changed, and proof that the new patterns actually work.\n\nIt's tempting to frame this as the \"end of programming.\" That's misleading.\n\nWhat's dying is a specific form of programming: one that equates value with authored code, longevity of code with quality, and maintenance with virtue.\n\nWhat's being born is something closer to systems design as an ongoing process of regeneration:\n\nCode becomes an intermediate artifact, not the final product. Rewrites become routine, not traumatic. Tests and evaluations define truth, not files. Stability emerges from replacement, not preservation.\n\nThis is not nihilism. It's pragmatism under new constraints.\n\nThe rest of this publication builds on a single premise established here:\n\nWhen code is cheap and understanding is expensive, architecture must optimize for the impermanence of code.\n\nEverything else (pace layers, evaluations, clean interfaces, regeneration workflows) flows from that fact.\n\nWe are not entering a world with less software. We are entering a world with vastly more of it. The only way to survive that abundance is to stop treating code as precious.\n\nBut it has been reborn, and it expects us to change with it.",
    "readingTime": 5,
    "keywords": [
      "modern software",
      "generative ai",
      "code",
      "it's",
      "systems",
      "programming",
      "codebases",
      "expensive",
      "developer",
      "longer"
    ],
    "qualityScore": 1,
    "link": "https://aicoding.leaflet.pub/3malrv6poy22a",
    "thumbnail_url": "https://leaflet.pub/lish/did%253Aplc%253A4qsyxmnsblo4luuycm3572bq/3majnsnvafs2b/3malrv6poy22a/opengraph-image?6815eb61f733905a",
    "created_at": "2025-12-23T00:56:32.109Z",
    "topic": "tech"
  },
  {
    "slug": "joan-didion-and-kurt-vonnegut-had-something-to-say-we-have-it-on-tape",
    "title": "Joan Didion and Kurt Vonnegut Had Something to Say. We Have It on Tape",
    "description": "Rare recordings of E.E. Cummings, Mary Oliver and more offer a tour through literary history led by authors in their own words — and voices. Take a listen.",
    "fullText": "Tom Wolfe was a fast talker. Eudora Welty had a musical Southern drawl. Kurt Vonnegut’s jokes got belly laughs.\n\nEach of these authors once spoke to audiences at the 92nd Street Y Unterberg Poetry Center in New York City, which has hosted some of the most celebrated writers of the past several generations, from Isaac Asimov to Anaïs Nin and Kazuo Ishiguro to Margaret Atwood. Now, the Poetry Center has digitized audio recordings of its literary events stretching back to 1949 — hundreds of which have never been released before — in a collection that offers a glimpse into history and a taste of what the writers themselves were like in public.\n\nIn 1965, for example, the year before he became consultant in poetry to the Library of Congress, James Dickey complained that his 14-year-old son had acquired a taste for rock ’n’ roll and a transistor radio. The sound of electric guitars had taken over his house. He was joined onstage that night by the poet Theodore Weiss, but it could have been Truman Capote, Joseph Heller or Adrienne Rich, who also visited the Poetry Center over the years.\n\n“Historically, it’s been the premier place to read your work in public in the United States,” said Billy Collins, a former U.S. poet laureate, who has read at the Poetry Center many times. “Maybe short of the White House or Carnegie Hall — but most poets don’t get to Carnegie Hall no matter how hard they practice.”\n\nYou can listen to some clips from the archive below.\n\nBaldwin was a gifted public speaker, compelling and quick on his feet. The eldest son of a preacher, Baldwin turned his own oratorical skills to advocacy and debate after a short stint at the pulpit as a teenager. Here, he talks about the mysteries of the writing process.\n\nIn this recording, Didion reads from her book “The Year of Magical Thinking,” which recounts her daughter’s grave illness and the sudden death of her husband, John Gregory Dunne. Didion and Dunne had been married for 40 years when, after visiting their daughter at the hospital, Dunne collapsed at the dinner table from a heart attack. He was pronounced dead a few hours later. The book offers a portrait of both loss and the long marriage that preceded it.\n\nThe prolific and prizewinning poet reads “Wild Geese,” one of her most celebrated poems. Oliver, who died in 2019, read at the Poetry Center three times during her life. On each of those visits, she made sure to include this fan favorite.\n\nVonnegut was best known for his novels, including “Slaughterhouse-Five” and “Cat’s Cradle,” but because he was at the Poetry Center, he thought he should read some poems. How he met the moment was quintessential Vonnegut: genial and cheeky in equal measure.\n\nWolfe, an author and journalist, was known both for novels including “The Bonfire of the Vanities” and for the role he played in helping to create “New Journalism,” which employed novelistic techniques in nonfiction. Wolfe was also an exceptionally snappy dresser, and he was often photographed wearing a bespoke three-piece white suit — although he chose a different outfit for the reporting trip he recounts here.\n\nThe earliest recording in the collection is of the American poet E.E. Cummings, who read at the Poetry Center in 1949. Cummings was born in 1894 and died in 1962, so even readers who love his distinctive style — with its unusual, almost sculptural line breaks and formatting — may not be familiar with his stately reading voice.\n\nThis selection is pulled from a Q. and A. with the playwright behind such classics of American theater as “The Crucible” and “Death of a Salesman.” A member of the audience asked about Miller’s play “The American Clock,” which is set during the Great Depression and was first staged in 1980: was Miller, the audience member asked, expecting another economic calamity when he wrote it?\n\n“dying is fine,” from “The Complete Poems: 1904-1962,” by E.E. Cummings. Copyright © 1949, 1977, 1991 by the Trustees for the E.E. Cummings Trust. Copyright © 1979 by George James Firmage. Used with permission of Liveright Publishing Corporation, a division of W.W. Norton & Company. All rights reserved.\n\n“Wild Geese,” from “Dream Works: Poems,” by Mary Oliver. Copyright © 1986 by NW Orchard LLC. Copyright © 1986-2017 by Mary Oliver, with permission of Bill Reichblum. Reprinted by permission of Penguin Books, an imprint of Penguin Random House, and the Charlotte Sheedy Literary Agency. All rights reserved.",
    "readingTime": 4,
    "keywords": [
      "rights reserved",
      "poetry center",
      "carnegie hall",
      "wild geese",
      "dunne",
      "permission",
      "celebrated",
      "writers",
      "literary",
      "collection"
    ],
    "qualityScore": 1,
    "link": "https://www.nytimes.com/2025/12/19/books/james-baldwin-joan-didion-92ny-recordings.html",
    "thumbnail_url": "https://static01.nyt.com/images/2025/11/20/multimedia/00TBR-92Y-03-cpvm/00TBR-92Y-03-cpvm-facebookJumbo.jpg",
    "created_at": "2025-12-22T06:20:28.076Z",
    "topic": "tech"
  },
  {
    "slug": "impeachable-pam-bondi-defied-federal-law-by-erasing-epstein-photos-to-protect-trump",
    "title": "Impeachable: Pam Bondi Defied Federal Law by Erasing Epstein Photos to Protect Trump",
    "description": "The DOJ removed already-public Epstein files in defiance of a disclosure statute, prompting bipartisan outrage and impeachment talk from the law’s authors.",
    "fullText": "(Evan Vucci/The Associated Press)\n\nThe Justice Department is now engaged in an open cover-up carried out in direct violation of federal law.\n\nOver the weekend, the department quietly removed 16 photographs from the Epstein files website it created to comply with a disclosure statute passed by Congress and signed into law by President Donald Trump. The removals came without notice or explanation. Among the deleted images was one of the few photographs that even indirectly featured Trump, a picture of a credenza drawer inside Jeffrey Epstein’s Manhattan home containing other photographs, including at least one of Trump. Twelve others depicted Epstein’s third-floor massage room, a central crime scene in the federal investigation. Some images of the same room remain public. Others disappeared.\n\nWhen Democrats on the House Oversight Committee asked whether the Trump-related image had been taken down, the Justice Department declined to respond.\n\nWhat followed made matters worse.\n\nIn a post on X quoting Deputy Attorney General Todd Blanche, the Justice Department claimed that “photos and other materials will continue being reviewed and redacted consistent with the law in an abundance of caution as we receive additional information.” Blanche’s original post asserted that the department had released Epstein materials “under the Epstein Files Transparency Act” and that additional disclosures would follow “as our review continues, consistent with the law and with protections for victims.”\n\nThat explanation fails under the statute the department invoked.\n\nCongress did not authorize a rolling review. The Epstein Files Transparency Act compels the Justice Department to release all Epstein-related materials in its possession. The law imposes a mandatory disclosure obligation and permits only limited redactions to protect victims. It grants no authority to retract, revise, or curate records after release. Once the department published those materials, the law required that they remain available to the public.\n\nRemoving them placed the department in direct conflict with the statute Congress enacted.\n\nThat conflict was immediately recognized. Blanche’s post received a community note stating that the law requires the release of all files and allows only narrow redactions to protect victims, adding that the department’s partial release and extensive redactions violated the statute. The Justice Department’s own post received a community note citing the statute directly and stating that retractions and redactions to protect politically exposed persons are not permitted. Community Notes appear only when users with differing political viewpoints agree on their accuracy, underscoring how broadly that conclusion was shared.\n\nThe department’s own explanation confirms it is violating the law it claims to follow.\n\nThe sequence exposes motive. The files went live. Political reaction followed. The department then altered the public record. Compliance held only until presidential exposure appeared, then gave way to erasure.\n\nIn November, I described the Trump Justice Department’s handling of the Epstein files as a cover-up. Last week, I wrote that the administration’s delay in disclosure created a political problem rather than an immediate legal one. That assessment reflected weak enforcement mechanisms and an approach built on delay rather than open defiance.\n\nRemoving already released material that implicates the president converts a credibility crisis into a statutory violation and a far larger political emergency. Congress passed the Epstein disclosure statute precisely to eliminate executive discretion. Lawmakers acted because the Justice Department repeatedly demonstrated it could not be trusted to manage politically sensitive material involving powerful figures. The law mandated disclosure to prevent executive self-protection.\n\nThe department seized that discretion anyway.\n\nAttorney General Pam Bondi had lawful options. She could have sought judicial review. She could have consulted Congress. She could have acknowledged that the statute permits no removal authority and sought amendment. Each path would have preserved institutional legitimacy. She chose concealment and false justification instead.\n\nThis erasure differs from earlier Trump-era document fights in a crucial way. Prior disputes centered on whether materials must be disclosed. This episode involves evidence already released to the public under statutory standards. The department determined the images satisfied the law’s requirements, then removed them once the political cost became apparent.\n\nEvery disclosure statute now faces the same test: compliance survives only until it threatens the president. Months of delay, sweeping redactions, and staged releases already convinced much of the public that the Justice Department prioritized Trump’s standing over transparency, victims, and the public interest. The image removals confirm that conclusion decisively.\n\nA Justice Department that edits evidence to shield the president forfeits legitimacy. Oversight collapses when obedience ends at political inconvenience. The rule of law depends on statutes binding the executive even when compliance proves costly.\n\nCongress wrote a law to prevent exactly this abuse. The president signed it. The attorney general is now violating it to protect him. That meets any reasonable standard for impeachment.\n\nThe backlash on Capitol Hill was immediate and bipartisan. Democratic Rep. Ro Khanna of California, who co-authored the Epstein Files Transparency Act, and Republican Rep. Thomas Massie of Kentucky, who forced the House vote compelling disclosure, both said the Justice Department failed to comply with the law. Khanna has confirmed that he and Massie are drafting impeachment and contempt measures against Attorney General Pam Bondi.\n\nCongress now faces a choice. It can accept that disclosure laws apply only when politically painless. It can normalize the disappearance of already public evidence. It can allow executive power to override legislative command.\n\nOr it can enforce the law it wrote.\n\nThis is a cover-up enforced through executive defiance. The question now is whether Congress will enforce its own laws.\n\nThe post Impeachable: Pam Bondi Defied Federal Law by Erasing Epstein Photos to Protect Trump first appeared on Mediaite.",
    "readingTime": 5,
    "keywords": [
      "pam bondi",
      "transparency act",
      "files transparency",
      "epstein files",
      "justice department",
      "justice department’s",
      "received community",
      "community note",
      "protect victims",
      "federal law"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/impeachable-pam-bondi-defied-federal-142332537.html",
    "thumbnail_url": "https://media.zenfs.com/en/mediaite_845/14ad98684991a13d9b9c7d5587ec3f0a",
    "created_at": "2025-12-21T18:15:58.663Z",
    "topic": "news"
  },
  {
    "slug": "multimillionaire-musician-william-says-worklife-balance-is-for-people-working-on-someone-elses-dreamhe-grinds-from-5to9",
    "title": "Multimillionaire musician Will.i.am says work-life balance is for people ‘working on someone else’s dream’—he grinds from 5-to-9 after his 9-to-5",
    "description": "When Will.i.am’s not writing hit songs like “OMG” for Usher, he’s looking for the next big pop star on The Voice U.K., or running his new AI company, FYI.",
    "fullText": "Orianna Rosa Royle is the Success associate editor at Fortune, overseeing careers, leadership, and company culture coverage. She was previously the senior reporter at Management Today, Britain's longest-running publication for CEOs.\n\nWill.i.am is busy. When he’s not writing hit songs like “OMG” for Usher, he’s looking for the next big pop star on The Voice UK, or running his new AI company, FYI. So how exactly does he balance it all?\n\nThe Grammy Award–winning artist turned tech entrepreneur revealed to Fortune that he maxes out the 5-to-9 after the daily grind of his 9-to-5, and he advises Gen Zers to forget about work-life balance if they want to emulate his success.\n\n“If you’re trying to build something that doesn’t exist, it’s about dream-reality balance,” he says. “Work-life balance means that you’re working for somebody else’s dream. You just have a job supporting somebody else’s dream, and you want to balance your work and your life.\n\n“But if it’s dream-reality balance, then it’s not work. It’s a dream that you’re trying to put into reality, and you’re ignoring your current reality.”\n\nFor example, after working on his tech venture from 9 a.m. to 5 p.m., Will.i.am says that he goes back to work on his creative business until 9 p.m. But before his AI company was a reality, his day was flipped. He’d work on music first before dipping into his tech side hustle well into the evening.\n\nIt’s why he advises young people to reframe how they think of their time off work and their current 9-to-5 reality.\n\n“I’m not really paying attention to this reality,” he explains. “I’m trying to bring that one [a new business venture or idea] here and focusing on how do I get people who believe in this dream to help me materialize it? So for that, you have to make some type of sacrifice to bring this thing that doesn’t exist here.\n\n“From that perspective, work-life balance is not for the architects that are pulling visions into reality. Those words don’t compute to the mindset of the materializers.”\n\nOf course, many young people already put in hours to their side hustles and personal development after work. Millions of Gen Zers and millennials are tuning into people’s 5-to-9 evening routines on TikTok.\n\nBut Will.i.am says chipping away at your dream when most people are off work extends to weekends, birthdays, and holidays.\n\n“I didn’t party. I was always a square, meaning, ‘You work too much, man, let’s go out.’ Like what? Go out. I don’t want to go out. I just always worked,” the rapper says. “It’s your birthday what are you gonna do? Work. You ain’t gonna celebrate?”\n\nThe multimillionaire says he’s always saved the celebrating for the stage, where he can finally enjoy the fruits of his labor.\n\n“There’s nothing that’s ever gonna feel that glorious than when you’re actually at a festival. But how do you get to headline a festival? You’ve got to work. My friends would go out and party, hanging out with chicks, doing drugs, drinking. I was just in the studio working, writing songs.”\n\nTo this day, he says that he hasn’t gone out and celebrated a birthday—including his most recent one, which was just last week on March 15.\n\n“Like on Christmas for the past 12 years: I could celebrate Christmas with my family, and then on the 26th, I fly to China because that’s dream maker heaven. Anything you want to make is there.”\n\nWill.i.am was speaking to Fortune in Rome for the rollout of Raidio.FYI radios in Mercedes-Benz cars.\n\n7 a.m.: Will.i.am is not a part of the CEO-approved 5 a.m. club. Instead, he told Fortune he wakes up at around 7 a.m., and he sticks to this routine whether he’s living in L.A. or London.\n\n8 a.m.: “I walk, do my calls, and get to work,” he says, with the aim to start work at 9 a.m.\n\n9 a.m. to 5 p.m.: “I get a lot done from nine to 12, do my little lunch, then back to work at one, finish at five, and that’s all my tech, like entrepreneurial activities.”\n\n5 p.m. to 9 p.m.: “The night hours are creativity,” he says, adding that specifically between 7 p.m. and 9 p.m. is when he gets the best ideas. “That’s the juicy bits, [when] I’m freaking soaking in emotion, to where I just rinse it out in the phone.”\n\n9 p.m. onward: When Will.i.am was in his late twenties, he says going to sleep at 4 a.m. (and waking up at noon) was the norm. But now, at 50 and balancing both his tech and music ventures, he starts unwinding for bed after 9 p.m. and is asleep by 11 p.m.\n\nA version of this story originally published on Fortune.com on March 23, 2025.",
    "readingTime": 5,
    "keywords": [
      "doesn’t exist",
      "somebody else’s",
      "else’s dream",
      "work-life balance",
      "dream-reality balance",
      "it’s",
      "tech",
      "you’re",
      "fortune",
      "he’s"
    ],
    "qualityScore": 0.8,
    "link": "https://fortune.com/article/will-i-am-says-work-life-balance-for-people-working-on-someone-elses-dream/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/03/GettyImages-1487800046.jpg?resize=1200,600",
    "created_at": "2025-12-21T18:15:57.676Z",
    "topic": "business"
  },
  {
    "slug": "what-the-hyperproduction-of-ai-slop-is-doing-to-science",
    "title": "What the hyperproduction of AI slop is doing to science",
    "description": "A new study shows AI writing is turning traditional measures of research quality upside down.",
    "fullText": "Want to write?\n\n Write an article and join a growing community of more than 216,700 academics and researchers from 5,395 institutions.\n\n Register now",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.3,
    "link": "https://theconversation.com/what-the-hyperproduction-of-ai-slop-is-doing-to-science-272250",
    "thumbnail_url": "https://images.theconversation.com/files/709831/original/file-20251219-66-vklc4z.jpg?ixlib=rb-4.1.0&rect=0%2C132%2C2400%2C1200&q=45&auto=format&w=1356&h=668&fit=crop",
    "created_at": "2025-12-19T06:18:41.817Z",
    "topic": "tech"
  },
  {
    "slug": "roblox-python-tower-defense-game",
    "title": "Roblox Python tower defense game",
    "description": "save the core by writing python! Contribute to jackdoe/roblox-python-tower-defense development by creating an account on GitHub.",
    "fullText": "jackdoe\n\n /\n\n roblox-python-tower-defense\n\n Public\n\n save the core by writing python!\n\n www.roblox.com/games/92507403623309/Python-Tower-Defense\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jackdoe/roblox-python-tower-defense",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.4,
    "link": "https://github.com/jackdoe/roblox-python-tower-defense",
    "thumbnail_url": "https://opengraph.githubassets.com/56462222528f0cb1d0e3e1a9cb703322f45ee9df3d28bfefbf0c555adffe592a/jackdoe/roblox-python-tower-defense",
    "created_at": "2025-12-19T00:56:22.416Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-unveils-new-opensource-ai-models-amid-boom-in-chinese-offerings",
    "title": "Nvidia unveils new open-source AI models amid boom in Chinese offerings",
    "description": "Nvidia on Monday unveiled a new family of open-source artificial intelligence models that it says will be faster, cheaper and ​smarter than its previous offerings, as open-source offerings from Chinese AI labs proliferate.  Nvidia ‌is primarily known for providing chips that firms such as OpenAI use to train their closed-source models and ‌charge money for them.  Nvidia ⁠on Monday revealed the third ‌generation of its \"Nemotron\" large-language models aimed at writing, coding and other tasks.",
    "fullText": "SAN FRANCISCO, Dec 15 (Reuters) - Nvidia (NVDA) on Monday unveiled a new family of open-source artificial intelligence models that it says will be faster, cheaper and ​smarter than its previous offerings, as open-source offerings from Chinese AI labs proliferate.\n\nNvidia ‌is primarily known for providing chips that firms such as OpenAI use to train their closed-source models and ‌charge money for them. But it also offers a slew of its own models for everything from physics simulations to self-driving vehicles as open-source software that can be used by researchers or by other companies, with firms such as Palantir Technologies weaving Nvidia's model into their products.\n\nNvidia ⁠on Monday revealed the third ‌generation of its \"Nemotron\" large-language models aimed at writing, coding and other tasks. The smallest of the models, called Nemotron 3 Nano, was being released ‍Monday, with two other, larger versions coming in the first half of 2026.\n\nNvidia, which has become the world's most valuable listed company, said that Nemotron 3 Nano was more efficient than its predecessor - ​meaning it would be cheaper to run - and would do better at long tasks ‌with multiple steps.\n\nNvidia is releasing the models as open-source offerings from Chinese tech firms such as DeepSeek, Moonshot AI and Alibaba Group Holdings are becoming widely used in the tech industry, with companies such as Airbnb disclosing use of Alibaba' s. (BABA) Qwen open-source model.\n\nAt the same time, CNBC and Bloomberg have reported that Meta Platforms is considering shifting toward closed-source ⁠models, leaving Nvidia as one of the most prominent ​U.S. providers of open-source offerings.\n\nMany U.S. states and ​government entities have banned use of Chinese models over security concerns.\n\nKari Briski, vice president of generative AI software for enterprise at Nvidia, said the company aimed ‍to provide a \"model that ⁠people can depend on\", and was also openly releasing its training data and other tools so that government and business users could test it for security and ⁠customize it.\n\n\"This is why we're treating it like a library,\" Briski told Reuters in an interview. \"This is ‌why we're committed to it from a software engineering perspective.\"",
    "readingTime": 2,
    "keywords": [
      "open-source offerings",
      "closed-source models",
      "nemotron nano",
      "nvidia",
      "firms",
      "software",
      "reuters",
      "cheaper",
      "aimed",
      "tasks"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/nvidia-unveils-open-source-ai-140424565.html",
    "thumbnail_url": "https://media.zenfs.com/en/reuters-finance.com/31397226d241d376e5cffbf13490e071",
    "created_at": "2025-12-16T13:51:41.754Z",
    "topic": "finance"
  },
  {
    "slug": "text-diffusion-models-are-faster-at-writing-code",
    "title": "Text Diffusion Models Are Faster at Writing Code",
    "description": "Speculative Decoding and Diffusion Language Models# In speculative decoding (for autoregressive models), we speed up generation by using a smaller model to generate multiple tokens, which are then verified in parallel by a larger model. The core idea is that most tokens are easily predictable; thus, we should be able to use a smaller and faster model for them. The classic example is the following sentence:\n“Geoffrey Hinton did his PhD at the University ___ ___”.",
    "fullText": "In speculative decoding (for autoregressive models), we speed up generation by using a smaller model to generate multiple tokens, which are then verified in parallel by a larger model.\nThe core idea is that most tokens are easily predictable; thus, we should be able to use a smaller and faster model for them.\nThe classic example is the following sentence:\n\n“Geoffrey Hinton did his PhD at the University ___ ___”.\n\nA small model can predict the next word “of” with high confidence, while the next token “Edinburgh” is much harder, requiring a larger model to verify.\n\nLanguage diffusion models sort-of have something analogous to speculative decoding built-in by default (and in some ways, a better version).\n\nFor diffusion language models with confidence-aware parallel decoding1, the model generates all tokens above a given confidence threshold at each step. Thus, “easy” tokens are generated in parallel.\nSome ways in which this is better than speculative decoding is that it is global (works across the entire context, not just for the next K tokens) and doesn’t require running two separate models.\nIn the following example:\n\nRepeat the word grape over and over again.\n\na diffusion language model could generate the prediction “grape” for the entire context length in one step, while in speculative decoding, even if the entire sequence is easily predictable, it can only predict K tokens at a time (typically around 4-8, from what I’ve read).\n\nThis benefit of increased parallel decoding partially depends on the structuredness of a text, which exists on a spectrum. An increase in confidence per output token directly leads to more tokens being decoded on average per step.1\n\nIncreased structure -> reduced entropy -> increased confidence -> higher parallel decoding\n\nThe grape example above is trivially structured, while normal text generation is unstructured and high entropy. Writing code (an actually useful domain) is somewhere in between.\n\nI had a large hunch that for structured tasks (like coding), the average number of tokens generated in parallel per step is higher than in normal text generation, and monotonically increases with the amount of structuredness present in the domain of the output.\n\nI recall reading that Google Gemini was much closer to state of the art for code generation than it was for reasoning tasks when it was first released. Perhaps this improvement for structured tasks is a general motif when it comes to these models?\n\nI threw together a small test and used the model from the paper Fast dLLM v2 (available on Huggingface) to generate roughly 256 tokens for 10 different prompts (each ran 10 times on A100s with an additional 2 generations for warmup):\n\nThe code for everything can be found here, which additionally includes generated output and metadata for each run. Below are the results:\n\nAs predicted, the grape example was dramatically faster than the unstructured text. What was surprising was how much faster (2.33 times speedup!) code generation was compared to unstructured text, even across multiple examples.\n\nMore testing should be done, but I imagine that there is a negative correlation between relative speedup and program complexity, and that boiler plate code (ex. self.input = input) would have a higher speedup compared to logic-heavy sections.\n\nAnother interesting result was that generating the start of the Declaration of Independence, a document which the model must have memorized, didn’t have much speedup. This tiny ablation study suggests that it really is the structuredness of the output, not memorization, which matters.\n\nThis was a small test thrown together in under an hour, and more rigorous evaluations should be done, but these preliminary results hint that this idea might be true to some degree.\n\nAn important limitation to address: In autoregressive decoding, we constrained decoding where we set all syntactically invalid tokens to have a probability of zero, ensuring that we are only generating text which follows some rules (JSON formatting, syntactically correct code, etc). For diffusion language models, this can’t naively be applied. However, similar to figuring out KVCache reuse with the advent of KVCache approximation, we might find a practical solution to this problem.2\n\nRead the section “Confidence-Aware Parallel Decoding” in the paper, Fast-dLLM. ↩︎ ↩︎\n\nThere are lots of papers of doing semi-autoregressive generation using diffusion language models. See AR-Diffusion, Fast-dLLM, Block Diffusion, etc. ↩︎",
    "readingTime": 4,
    "keywords": [
      "easily predictable",
      "structured tasks",
      "per step",
      "normal text",
      "confidence-aware parallel",
      "diffusion language",
      "unstructured text",
      "larger model",
      "speculative decoding",
      "language models"
    ],
    "qualityScore": 1,
    "link": "https://nathan.rs/posts/dllm-faster-code-generation/",
    "thumbnail_url": "https://nathan.rs/site.png",
    "created_at": "2025-12-13T18:48:06.593Z",
    "topic": "tech"
  },
  {
    "slug": "my-day-as-an-augmented-technical-writer-in-2030",
    "title": "My day as an augmented technical writer in 2030",
    "description": "Instead of writing my tech comms predictions for next year like I did in 2024, I’ve written a fictionalized account of my day as a technical writer in 2030. It’ll be interesting to see whether we get there or not. Take it as a window into a possible future, one where AI usage is safer, more regulated, and better integrated with our workflows (as it should be).",
    "fullText": "Instead of writing my tech comms predictions for next year like I did in 2024, I’ve written a fictionalized account of my day as a technical writer in 2030. It’ll be interesting to see whether we get there or not. Take it as a window into a possible future, one where AI usage is safer, more regulated, and better integrated with our workflows (as it should be).\n\nMy working day starts at 8:30am, after I’ve dropped my kids at school, rushed home, and prepared some coffee surrogate (nobody can afford real coffee anymore). I open the laptop and Chuck is there – it’s always there, like a trusty butler, ready to summarize what’s been going on in pull requests, Slack threads, Jira tickets, and a plethora of other information systems nobody can quite tame. Its summary connects the dots between my current priorities and what’s happening in the teams I’m attached to, helping me decide what to work on next. Trying to be helpful, it offers to deal with some of the mentions I’ve got by opening pull requests; I let it do so with a small docs bug. The rest I’ll want to deal with myself. It asks me how I feel and gently reminds me that I’ve still got some PTO available. Chuck’s such a class act.\n\nI’m in a team with several other technical writers, but for the most part I work with Chuck, which is what we call the in-house AI agent that we use. Chuck is a vast local language model capable of running on the M10 Silicon processor that powers my laptop. It’s a state-of-the-art multimodal LLM whose pedigree I can trace back to the last iterations of Claude Omni 7.5, before Anthropic went bankrupt and got acquired by Apple. As most corporate models, Chuck is ISO 42001, Turing, and EUAI certified, which means that it’s audited every year for security, governance, and legality of its training materials. Chuck is fine-tuned into several variants depending on the goal; the one I use is chuck-256b-writer. We run it in CI pipelines and locally in IDEs or CLI clients. We can also invite it to meetings as an artificial participant. I sometimes ask my own Chuck to attend calls on my behalf as Chuck-Fabri.\n\nThe thing I like the most about Chuck is that I can configure its specializations by turning modules on or off through the Silicon Brain app. When I want it to play the developer, I add several coding modules; when I want it to help me author docs, I turn on the style guide and grammarian modules, and so on. I can also ask Chuck to spawn copies of itself to roleplay users and readers based on support ticket and sales call interactions. When I do that, Chuck politely asks me to call it through other names, so as not to break character, something I duly comply with. Most system tools and APIs are already compatible with the agentic environment I use, so Chuck knows how to perform most operations on its own. An important detail: to summon Chuck, I need to first plug a physical key into the laptop. The key comes with a red button to immediately stop Chuck in case it starts operating bizarrely. Never had to use it.\n\nIt’s 11am already. I’ve been working with Chuck to write a new docs set for a new feature, telling it how I wanted the docs to fit into the existing architecture and instructing it to tweak and edit. It almost always gets 80% of the work done, though I often have to intervene to rearrange, cut, or otherwise rewrite sections. This hasn’t changed since the first days of GPT and it’ll never improve, because LLMs are not intelligent. They’re the most useful word automation tools at my disposal though, which I keep in check through deterministic tools and linters. Chuck is able to create diagrams, take screenshots of the product through an internal tool, and test the instructions and code snippets itself. When I feel unsure about its output, I ask it to verify what it’s just written through semantic internal search, or by calling its cloud cousin, Chad, which is able to provide answers from federated internal sources. All we do together, Chuck documents internally and remembers in its permanent context.\n\nEven though I’m using a local, non-monetized, and fully audited model that consumes the equivalent of a lightbulb worth of power, I still can’t shake the feeling of being a reverse centaur at times. It helps that Chuck comes with several built-in safeguards meant to prevent me from overworking or spending too much time without interacting with other human beings. At 1pm, which is lunch time in Spain, Chuck reminds me about taking a break. It refuses to continue if it detects stress in my text, vocal, or computer usage patterns. While my interactions with Chuck on the laptop are private and encrypted, it’s allowed to inform my manager or call my designated emergency contact in case of distress. I let Chuck access my vitals on the smartwatch and schedule calls with me on a regular basis to see how I’m doing. Since I work alone at home, this makes me feel somewhat safer.\n\nI didn’t tell you, but my current job title is Augmented Writer. My mission is to ensure that the words that humans and machines use to interact with our products are the most effective at reducing confusion and error, while they maximize effectiveness and user satisfaction. I’m augmented because I do this in concert with Chuck, which expands my existing skills in numerous ways. Without my brain, though, Chuck couldn’t do my job, because it doesn’t really care and, more importantly, because it’s not allowed to. One of the conditions imposed by the current legislation is that AI cannot operate in fully autonomous mode without human supervision. Our docs and UIs, in fact, bear a certificate of human authorship that discloses the amount of AI intervention. By law, all AI generated artifacts must produce fingerprinting patterns that can’t be tampered with, which is trickier with text, but since we must keep full audit logs of LLM usage, this can be established upon request by any competent authority, including the Turing police.\n\nIn the end, my role is more of an orchestrator than that of an author, and I’m fine with that. Software engineering, the field I serve, is an exercise in consensual imagination whose goal is to find repeatable ways of processing reality into manageable chunks of data. Reality is unmistakably raw and imperfect, a stream of floating points and broken strings running through distributed systems: one cannot tame it through clever algorithms, but it can be reduced to abstractions and data structures and binary blobs. Each of those entities has a name; they all relate to each other through words. It’s part of my job to understand those words and intervene when they don’t bring clarity. It’s then also my job to explain how those words are able to handle their parent reality. The docs I orchestrate with Chuck’s help are the artifacts that chronicle and explain the motions of data as it enters a machine and exits in shapes and configurations that are helpful to users.\n\nIt’s 5pm and I’m bidding Chuck farewell. During the night, it will work on some optional docs polish and politely present its work to me in the morning. As I log off and extract the hardware key from the laptop, I think that without the words Chuck and I produced, the machine would be opaque to its operators, a smooth wall without doors or handles. Product truth is at my disposal to weave into a fabric of meaning and possibility, into spells that unlock abilities in autonomous agents, be they organic or artificial. I am an enabler of thought and action. Getting here wasn’t easy, but I feel better knowing that I can continue defending the importance of words with the help of the most clever thesaurus ever created.",
    "readingTime": 7,
    "keywords": [
      "it’s",
      "docs",
      "chuck",
      "i’ve",
      "laptop",
      "without",
      "usage",
      "modules",
      "tools",
      "internal"
    ],
    "qualityScore": 1,
    "link": "https://passo.uno/my-day-tech-writer-2030/",
    "thumbnail_url": "/thumb.png",
    "created_at": "2025-12-13T18:48:05.736Z",
    "topic": "tech"
  },
  {
    "slug": "writing-a-typesafe-linux-perf-interface-in-zig",
    "title": "Writing a Type-Safe Linux Perf Interface in Zig",
    "description": "I'm building a benchmarking tool for Zig and needed CPU counters. This is how I wrapped Linux's `perf_event_open` to be type-safe with comptime.",
    "fullText": "I am currently building a hobby project:\npyk/bench, a microbenchmarking library for Zig.\nMy goal is to make it fast and accurate. To measure performance properly,\nlooking at wall clock time is not enough. I need to know what the CPU is\nactually doing.\n\nI want to measure CPU cycles, instruction counts and cache misses. On Linux the\nkernel provides a system call for this named\nperf_event_open.\nIt is very powerful but the API is raw and not easy to use safely.\n\nThe perf_event_open system call creates a file descriptor that allows\nmeasuring performance information. You fill out a perf_event_attr struct with\nthe config you want, such as PERF_COUNT_HW_CPU_CYCLES or\nPERF_COUNT_HW_INSTRUCTIONS, and the kernel gives you back a file descriptor.\n\nYou can read from this file descriptor to get the counts. The format of the data\nyou read depends on how you opened it.\n\nYou can also group events. This is important because it lets you measure\nmultiple things at once with a single read call. One event acts as the “group\nleader” and others are “siblings”. When you read from the leader, you get a\nbinary layout containing values for all events in the group.\n\nThe layout looks roughly like this in C:\n\nThis is dynamic. The size of the struct changes based on how many events you\nhave. In Zig I want something static and type-safe.\n\nMy first attempt was brittle. I hardcoded a struct with the fields I thought I\nwould need.\n\nThis works but it is dangerous. The Measurements struct is hardcoded.\n\nIf I change the initialization logic to add “branch misses”, I have to remember\nto update Measurements, update the ids array size and update the read\nfunction manually. The compiler cannot help me here.\n\nIf I access ids[2] but only initialized 2 events, I crash or get garbage data.\n\nZig allows running code at compile time to generate types. I can use this to\ngenerate a struct that exactly matches the events I request.\n\nI defined an Event enum for the things I want to measure.\n\nThen I wrote a function that takes a slice of these events and returns a new\ntype.\n\nThis function creates a struct with fields named after the enum tags. If I pass\n&.{ .cpu_cycles, .instructions }, it generates:\n\nNow I can create a generic Group type that uses this.\n\nThe usage is cleaner and safer.\n\nIf I try to access a field I did not request, the compiler stops me.\n\nHere is how the full version looks like:\n\nTo implement this I needed to get the ID of the event from the file descriptor.\nThe kernel documentation says to use ioctl with PERF_EVENT_IOC_ID.\n\nI checked std.os.linux in the Zig standard library and it was missing.\n\nSo I opened a pull request to add it. It is just a one line change.\n\nYou can see the PR here:\nhttps://codeberg.org/ziglang/zig/pulls/30162.\nI am not sure if it will be accepted but it felt good to fix a missing piece in\nthe tool I use.",
    "readingTime": 3,
    "keywords": [
      "file descriptor",
      "struct",
      "events",
      "kernel",
      "event",
      "function",
      "request",
      "library",
      "performance",
      "counts"
    ],
    "qualityScore": 1,
    "link": "https://pyk.sh/blog/2025-12-11-type-safe-linux-perf-event-open-in-zig",
    "thumbnail_url": "https://pyk.sh/opengraphs/blog.png",
    "created_at": "2025-12-13T06:54:07.096Z",
    "topic": "tech"
  },
  {
    "slug": "before-megalodon-researchers-say-a-monstrous-shark-ruled-ancient-australian-seas",
    "title": "Before megalodon, researchers say a monstrous shark ruled ancient Australian seas",
    "description": "In the age of dinosaurs — before whales, great whites or the bus-sized megalodon — a monstrous shark prowled the waters off what's now northern Australia, among the sea monsters of the Cretaceous period.  Researchers studying huge vertebrae discovered on a beach near the city of Darwin say the creature is now the earliest known mega-predator of the modern shark lineage, living 15 million years earlier than enormous sharks found before.  The ancestor of today’s 6-meter (20-foot) great white shark was thought to be about 8 meters (26 feet) long, the authors of a paper published in the journal Communications Biology said.",
    "fullText": "WELLINGTON, New Zealand (AP) — In the age of dinosaurs — before whales, great whites or the bus-sized megalodon — a monstrous shark prowled the waters off what's now northern Australia, among the sea monsters of the Cretaceous period.\n\nResearchers studying huge vertebrae discovered on a beach near the city of Darwin say the creature is now the earliest known mega-predator of the modern shark lineage, living 15 million years earlier than enormous sharks found before.\n\nAnd it was huge. The ancestor of today’s 6-meter (20-foot) great white shark was thought to be about 8 meters (26 feet) long, the authors of a paper published in the journal Communications Biology said.\n\n“Cardabiodontids were ancient, mega-predatory sharks that are very, very common from the later part of the Cretaceous, after 100 million years ago,” said Benjamin Kear, the senior curator in paleobiology at the Swedish Museum of Natural History and one of the study’s authors. “But this has pushed the time envelope back of when we’re going to find absolutely enormous cardabiodontids.”\n\nRediscovered fossils pointed to a huge shark\n\nSharks have a 400-million-year history but lamniforms, the ancestors of today’s great white sharks, appear in the fossil record from 135 million years ago. At that time they were small — probably only a meter in length — which made the discovery that lamniforms had already become gigantic by 115 million years ago an unexpected one for researchers.\n\nThe vertebrae were found on coastline near Darwin in Australia’s far north, once mud from the floor of an ancient ocean that stretched from Gondwana — now Australia — to Laurasia, which is now Europe. It’s a region rich in fossil evidence of prehistoric marine life, with long-necked plesiosaurs and ichthyosaurs among the creatures discovered so far.\n\nThe five vertebrae that launched the quest to estimate the size of their mega-shark owners were not a recent discovery, but an older one that had been somewhat overlooked, Kear said. Unearthed in the late 1980s and 1990s, the fossils measured 12 centimeters (4.7 inches) across and had been stored in a museum for years.\n\nWhen studying ancient sharks, vertebrae are prizes for paleontologists. Shark skeletons are made of cartilage, not bone, and their fossil record is mostly made up of teeth, which sharks shed throughout their lives.\n\n“The importance of vertebrae is they give us hints about size,” Kear said. “If you’re trying to scale it from teeth, it’s difficult. Are the teeth big and the bodies small? Are they big teeth with big bodies?”\n\nAncient shark size still holds mystery\n\nScientists have used mathematical formulas to estimate the size of extinct sharks like megalodon, a massive predator that came later and may have reached 17 meters (56 feet) in length, Kear said. But the rarity of vertebrae mean questions of ancient shark size are difficult to answer, he added.\n\nThe international research team spent years testing different ways to estimate the size of the Darwin cardabiodontids, using fisheries data, CT scans and mathematical models, Kear said. Eventually, they arrived at a likely portrait of the predator’s size and shape.\n\n“It would’ve looked for all the world like a modern, gigantic shark, because this is the beauty of it,” Kear said. “This is a body model that has worked for 115 million years, like an evolutionary success story.”\n\nA predator’s past could hint at the future\n\nThe study of the Darwin sharks suggested that modern sharks rose early in their adaptive evolution to the top of prehistoric food chains, the researchers said. Now, scientists could scour similar environments worldwide for others, Kear said.\n\n“They must have been around before,” he said. “This thing had ancestors.”\n\nStudying ancient ecosystems like this one could help researchers understand how today’s species might respond to environmental change, Kear added.\n\n“This is where our modern world begins,” he said. “By looking at what happened during past shifts in climate and biodiversity, we can get a better sense of what might come next.”",
    "readingTime": 4,
    "keywords": [
      "meters feet",
      "fossil record",
      "studying ancient",
      "ancient shark",
      "shark size",
      "sharks",
      "vertebrae",
      "researchers",
      "darwin",
      "modern"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/megalodon-researchers-monstrous-shark-ruled-022242238.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/h0A6fi4PoOLfoaNL0Afu9w--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/ap.org/a01c66b68881dea3689bd5a0cf3aa14f",
    "created_at": "2025-12-13T03:40:56.459Z",
    "topic": "science"
  },
  {
    "slug": "heres-why-a-call-of-duty-movie-is-finally-happening-after-10-years-of-trying",
    "title": "Here's Why A Call Of Duty Movie Is Finally Happening After 10+ Years Of Trying",
    "description": "A movie based on Activision's Call of Duty series is in the works now at Paramount, which is now led by billionaire and Call of Duty superfan David Ellison. Prolific writer and director Taylor Sheridan is writing the script for the film, with Lone Survivor director Peter Berg attached to direct.\nActivision had been trying to make Call of Duty movies for years, so why is it finally happening now? Xbox boss Matt Booty told Variety that \"a relationship came about\" between people at Paramount and senior executives working on Call of Duty. \"They felt like they found a partner who understands the game, people who play the game, and shared a vision of what it could be to bring that forward,\" Booty said.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/heres-why-a-call-of-duty-movie-is-finally-happening-after-10-years-of-trying/1100-6536930/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1179/11799911/4621736-screenshot2025-12-12at8.45.08%E2%80%AFam.png",
    "created_at": "2025-12-12T18:55:57.681Z",
    "topic": "entertainment"
  },
  {
    "slug": "judge-orders-kilmar-abrego-garcia-to-be-immediately-released-from-immigration-detention",
    "title": "Judge orders Kilmar Abrego Garcia to be immediately released from immigration detention",
    "description": "Kilmar Abrego Garcia must be freed from immigration detention while he fights his deportation, a judge ruled Thursday, handing a major victory to the immigrant whose wrongful deportation to a notorious prison in El Salvador made him a flashpoint of the Trump administration’s immigration crackdown.  U.S. District Judge Paula Xinis in Maryland ordered Immigration and Customs Enforcement to let Abrego Garcia go immediately, writing that federal authorities had detained him again after his return to the United States without any legal basis.",
    "fullText": "Kilmar Abrego Garcia must be freed from immigration detention while he fights his deportation, a judge ruled Thursday, handing a major victory to the immigrant whose wrongful deportation to a notorious prison in El Salvador made him a flashpoint of the Trump administration’s immigration crackdown.\n\nU.S. District Judge Paula Xinis in Maryland ordered Immigration and Customs Enforcement to let Abrego Garcia go immediately, writing that federal authorities had detained him again after his return to the United States without any legal basis.\n\n“For this reason, the Court will GRANT Abrego Garcia’s Petition for immediate release from ICE custody,” the judge wrote.\n\nThe Department of Homeland Security sharply criticized the decision and vowed to appeal, calling the ruling “naked judicial activism” by a judge appointed during the Obama administration.\n\n“This order lacks any valid legal basis, and we will continue to fight this tooth and nail in the courts,” said Tricia McLaughlin, the department’s assistant secretary. The judge gave prosecutors until 5 p.m. EST to formally respond to the release order.\n\nThe Justice Department declined to comment, and messages seeking comment from Abrego Garcia’s attorney were not immediately returned.\n\nAbrego Garcia, a Salvadoran national with an American wife and child, has lived in Maryland for years but entered the U.S. illegally as a teenager. An immigration judge ruled in 2019 that he could not be deported to El Salvador because he faced danger from a gang that targeted his family. When he was mistakenly sent there in March, his case became a rallying point for those who oppose President Donald Trump’s immigration enforcement actions.\n\nA court later ordered his return to the United States. Since he cannot be removed to El Salvador, ICE has been seeking to deport him to a series of African countries. His federal suit claims the Trump administration is illegally using the removal process to punish Abrego Garcia for the public embarrassment caused by his deportation.\n\nIn her order releasing Abrego Garcia, Xinis wrote that federal authorities “did not just stonewall” the court, “They affirmatively misled the tribunal.” The judge was referencing the successive list of four African countries that officials had sought to remove Abrego Garcia seemingly without commitments from those countries, as well as officials' affirmations that Costa Rica withdrew its offer to accept him, a claim later proven untrue.\n\n“But Costa Rica had never wavered in its commitment to receive Abrego Garcia, just as Abrego Garcia never wavered in his commitment to resettle there,” the judge wrote.\n\nXinis also rejected the government’s argument that she lacked jurisdiction to intervene on a final removal order for Abrego Garcia, because she found no final order had been filed.\n\nSeparately, Abrego Garcia is asking an immigration court to reopen his case so he can seek asylum in the United States.\n\nHe is also criminally charged in Tennessee, where he has pleaded not guilty to human smuggling. He has asked the federal court to dismiss the case, arguing the prosecution is vindictive. His defense attorney in Tennessee, Sean Hecker, declined to comment.\n\nA judge in that case has ordered an evidentiary hearing after previously finding some evidence that the charges “may be vindictive.” The judge also noted several statements by Trump administration officials that “raise cause for concern,” including a statement by Deputy Attorney General Todd Blanche that seemed to suggest the Justice Department charged Abrego Garcia because he won his wrongful deportation case. ___\n\nLoller reported from Nashville, Seewer reported from Toledo, Ohio and Lauer reported from Philadelphia. Associated Press reporter Alanna Durkin Richer in Washington contributed to this report.",
    "readingTime": 3,
    "keywords": [
      "trump administration",
      "abrego garcia",
      "legal basis",
      "wrongful deportation",
      "federal authorities",
      "judge ruled",
      "el salvador",
      "court",
      "ordered",
      "attorney"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/judge-orders-kilmar-abrego-garcia-154255309.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/a0M6hY0ASmu1RqN_RGIGnw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/ap.org/402cd28fa90f1643a286fc771ae63b50",
    "created_at": "2025-12-11T18:58:22.683Z",
    "topic": "news"
  },
  {
    "slug": "clair-obscur-expedition-33-how-a-tiny-studio-developed-the-belle-poqueset-gaming-blockbuster",
    "title": "Clair Obscur: Expedition 33 – how a tiny studio developed the Belle Époque-set gaming blockbuster",
    "description": "What started as Guillaume Broche’s personal project has been nominated for 12 Game awards, sold more than 2m copies and been praised by Emmanuel Macron as a ‘shining example of French audacity’\nThe record-breaking 12 nominations at the Game awards this year was beyond the wildest dreams of Guillaume Broche when he first began inking out Clair Obscur: Expedition 33 as a personal project while working at Ubisoft.\nBefore selling more than 2m copies, the narrative-driven roleplaying game with “a unique world, challenging combat and great writing” was a technical demo called We Lost. It was Broche’s appetite for risk and a few hopeful Reddit posts that would create the game’s world of Lumiere and its struggle against the Paintress.\n Continue reading...",
    "fullText": "What started as Guillaume Broche’s personal project has been nominated for 12 Game awards, sold more than 2m copies and been praised by Emmanuel Macron as a ‘shining example of French audacity’\n\nThe record-breaking 12 nominations at the Game awards this year was beyond the wildest dreams of Guillaume Broche when he first began inking out Clair Obscur: Expedition 33 as a personal project while working at Ubisoft.\n\nBefore selling more than 2m copies, the narrative-driven roleplaying game with “a unique world, challenging combat and great writing” was a technical demo called We Lost. It was Broche’s appetite for risk and a few hopeful Reddit posts that would create the game’s world of Lumiere and its struggle against the Paintress.\n\n“I was doing like eight hours per day after work and not sleeping at all for a few years,” Broche says. And while he would be joined by Tom Guillermin on the programming side and Francoise Meurisse as producer, the next few members of the nascent studio – Lorien Testard, lead composer and Jennifer Svedberg-Yen, lead writer – would only come to Clair Obscur by chance, via social media.\n\nTestard, a guitar teacher at the time who had never composed or published any music commerically, was discovered by Broche on SoundCloud. “We liked the same philosophy in games,” says Testard, who had been writing music inspired by his favourite titles. Similarly, Broche found his art director, Nicholas Maxson-Francombe, through personal works he posted on ArtStation. “We are all deeply engaged in our subject areas,” says Svedberg-Yen, who says that was what bonded the team together. “If you listen to Nicolas talk about art or Lorien talk about music, it’s just something that fills our minds and our days.”\n\nSvedberg-Yen, meanwhile, had come from the world of finance. She saw Broche’s Reddit post and auditioned to not only write but also to voice some of the prototype characters of Clair Obscur, namely Maelle and Lune. Despite a deep love for video games, storytelling and a childhood engrossed in novels, Svedberg-Yen had not considered it as a career option. “It never crossed my mind as possible. As the adage goes, for Asian parents [it’s] doctors, lawyers, or finance.”\n\nWith a rudimentary team assembled under the banner of a new studio, Sandfall Interactive, they rebooted We Lost as Clair Obscur. It was there a world took shape and the gained its unmistakeable Belle Époque setting. “There’s a specificity,” Svedberg-Yens says. “I think that vision gets diluted when you’re trying to appeal to too many people.”\n\n“It’s not meant to be French propaganda,” jokes Broche, about the game’s very Gallic aesthetic. The characters yell putain and merde, there are berets and extremely scary mimes not only because it’s fun, he says, but born from a desire to make something “sincere and authentic”.\n\nSame goes for the story. Clair Obscur’s narrative drives the game forward and, as lead writer, Svedberg-Yen says it all has a “grounding and a basis in truth”.\n\n“We are all first-time writer and game developers in this sense … and so we kind of only know instinctively how write to what instinctively comes from within. And for a lot of the characters in those particular situations, to write them [you] have to really delve into the parts of my life that resonate with the situation that they’re in.”\n\nAccording to Svedberg-Yen, a conversation Broche had with his mother became core to the story’s emotional heft. When he asked her what would be the worst thing that could possibly happen, his mother responded saying it would be to lose her children. “The story deals with a lot of trauma,” she says, and that process of writing like that was often a scary process. “If people don’t like it, they don’t like you.” It was this ability to be vulnerable and open a communicative environment that Broche believes contributed to the success of the game.\n\nDespite an anxiety in the industry about the rise of AI in the development of video games, the team at Sandfall isn’t worried – especially Testard, who composed the game’s orchestral score based off the narrative beats of the game and the eponymous concept of clair obscur (or chiaroscuro), AKA the interplay of light and dark. “Music is the language of the soul,” he says. In fact, the evolution of technology like Unreal Engine 4 and the later 5, which the game runs on today, made a lot of the game possible. “More games will be 3D, because we have a lot of tools now,” says Broche, who describes the budget of his game to be on the “lower end of AA.”\n\nThe team at Sandfall have been overwhelmed by their success. The French president, Emmanuel Macron, lauded the game as “shining example of French audacity”. None of them expected the experience of Clair Obscur to resonate so deeply with so many people. “I’ve gotten a lot of very heartfelt messages from players who have experienced loss in some way and who have felt that the story helped them deal with their grief or change their relationship with grief,” Svedberg-Yen says.\n\n“What’s really cool is I’ve gotten tons of messages from creatives, writers, aspiring writers who felt creatively drained or just felt like they wanted to quit, but then the game inspired them to start again and they started creating their own art again, to start writing.”",
    "readingTime": 5,
    "keywords": [
      "i’ve gotten",
      "french audacity",
      "personal project",
      "game awards",
      "clair obscur",
      "emmanuel macron",
      "music",
      "games",
      "team",
      "it’s"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/games/2025/dec/11/clair-obscur-expedition-33-video-game-tiny-studio-developed-blockbuster",
    "thumbnail_url": "https://i.guim.co.uk/img/media/de7272ae99a245b1ca069ffc4a50c11a28465092/478_134_2532_2026/master/2532.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2a7548a6ff68110738e5a01d103698a7",
    "created_at": "2025-12-11T13:53:39.909Z",
    "topic": "gaming"
  },
  {
    "slug": "st-ergnats-dream-of-writing-next-great-moneyglass-tale",
    "title": "St Ergnat's dream of writing next great Moneyglass tale",
    "description": "St Ergnat's Moneyglass hope to become the County Antrim village's next sporting heroes in this weekend's All-Ireland Ladies Club final.",
    "fullText": "The folks in Moneyglass are no strangers to cheering on one of their own on a big stage.\n\nWillie John McBride, who went on to star for the British and Irish Lions, was born in the small County Antrim village. So, too, was Grand National winner AP McCoy.\n\nNow, St Ergnat's are hoping to follow in their footsteps.\n\nIt has already been a memorable year for the club. After securing a fifth straight Antrim title, they conquered Ulster for the first time in November, beating Errigal Ciaran in the final.\n\nRuling the province was a realistic aim for Moneyglass this year, but few outside the squad - who are led by former Donegal ladies boss Maxi Curran - would have expected them to reach the All-Ireland final.\n\nBut they have gleefully defied expectations. Beating Dublin's Kilmacud Crokes in the semi-final a fortnight ago means they will become first Antrim side to grace the biggest stage in ladies club football at Croke Park on Saturday (16:00 GMT).\n\n\"Coming into this year, we just wanted to go a bit further than last year,\" says Aoife Kelly.\n\n\"Getting beaten in the semi-finals two years in a row, off the back of getting to the 2022 Ulster final, so the goal was to get back to the Ulster final, we got there, won it and we've been taking it game by game.\n\n\"It was no coincidence we won Ulster, we have been building to it for the last few years.\"",
    "readingTime": 2,
    "keywords": [
      "ulster final",
      "moneyglass",
      "stage",
      "club",
      "ladies",
      "back",
      "game",
      "antrim",
      "beating"
    ],
    "qualityScore": 0.85,
    "link": "https://www.bbc.com/sport/articles/cn7kex85k0no?at_medium=RSS&at_campaign=rss",
    "thumbnail_url": "https://ichef.bbci.co.uk/ace/branded_sport/1200/cpsprodpb/0ffd/live/0791e620-d5e3-11f0-b1c1-a1fc4155147e.png",
    "created_at": "2025-12-11T13:53:34.013Z",
    "topic": "sports"
  },
  {
    "slug": "tracking-penn-states-pinstripe-bowl-opt-outs-durant-wheatley-thus-far",
    "title": "Tracking Penn State’s Pinstripe Bowl Opt Outs: Durant, Wheatley Thus Far",
    "description": "Folks, instead of doing an individual post for each player that opts out of the Pinstripe Bowl, we’ll be updating this post instead. As of this writing on Wednesday, December 10th at 10 p.m. EST, just two Nittany Lions have opted out. DB Zakee Wheatley (NFL Draft) Wheatley joins Durant as the second player on […]",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://sports.yahoo.com/articles/tracking-penn-state-pinstripe-bowl-030905061.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/alH0BuwQ4TUlzzmie.A.tg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/sb_nation_articles_115/c41ed04580f3ed4c736372f98142f288",
    "created_at": "2025-12-11T03:51:05.191Z",
    "topic": "sports"
  }
]
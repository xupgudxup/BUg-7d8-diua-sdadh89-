[
  {
    "slug": "ai-strategy-is-built-on-layers-of-api-sediment",
    "title": "AI strategy is built on layers of API sediment",
    "description": "AI protocols, such as MCP and Agent Skills, are agent-first, which risks bypassing the governance, security, and access controls that enterprises have spent years building around their APIs and data.",
    "fullText": "“The API landscape is a mess, and very few people understand it,” Kin Lane, API industry veteran and founder of Naftiko, tells The New Stack.\n\nSome days it feels like we are living in an XKCD cartoon.\n\nOrganizations don’t typically migrate legacy systems from one spec to another as new ideas emerge. Instead, they accumulate layers of integration standards over time, with each era leaving behind systems that are too costly or risky to excavate. “I get a call from a 20-year veteran at a large enterprise, who says, ‘We still have EDI and WSDLs, a lot of Swagger and OpenAPI. We’re trying to do more Async API. MCP is popping up, and we’re looking at Agent Skills, but we have a global business to run, and it’s got to be stable.’”\n\nIt was seeing this recurring pattern of API sediment that prompted Lane to found Naftiko.\n\nThe evolution and splintering of API specifications\n\n“Organizations… accumulate layers of integration standards over time, with each era leaving behind systems that are too costly or risky to excavate.”\n\nLane argues that competing standards are a consequence of vendor ‘land grabs’, where competing vendors exploit specs to exert influence. I don’t disagree with his hypothesis, but I would add that the different standards also reflect when they were developed.\n\nWeb Services Description Language (WSDL) emerged from the Enterprise SOA movement of the 2000s as a formal contract language for web services. Governed by W3C but heavily influenced by IBM, Microsoft, and Oracle, it was technically open but reflected corporate middleware needs, with verbose XML schemas defining operations, messages, and bindings.\n\nIn the 2010s, REST APIs displaced SOAP, and lighter-weight specifications emerged:\n\nAs asynchronous architecture patterns such as event-driven architectures, message queues, WebSockets, and streaming gained popularity in the late 2010s, OpenAPI’s request-response model no longer fit. Regarded as a sister spec to OpenAPI, AsyncAPI (also under Linux Foundation) borrowed OpenAPI’s structure, adapting it for pub/sub, streaming, and asynchronous messaging patterns.\n\nAs we entered the 2010s, Smithy (AWS) and TypeSpec (Microsoft) marked a shift toward protocol-agnostic API modeling. Rather than describing HTTP endpoints directly, they model services abstractly, then generate OpenAPI, code, or protocol-specific implementations. This reflects cloud providers’ need to maintain type safety while supporting multiple protocols (HTTP, gRPC, proprietary) from single definitions.\n\nSmithy powers AWS’s service definitions. TypeSpec emerged from Microsoft’s experience with Azure APIs and emphasizes TypeScript-based syntax for broader developer accessibility. Both Smithy and TypeSpec are open source, but neither has truly open governance in the OpenAPI/AsyncAPI sense. AWS drives Smithy’s roadmap based on internal AWS needs. TypeSpec recently moved to a Linux Foundation working group, but Microsoft remains the dominant contributor. There’s no open governance — no multi-vendor steering committee, and no requirement for consensus from competing cloud providers.\n\nThis matters because Smithy and TypeSpec reflect their creators’ architectural assumptions: multi-region cloud services, polyglot microservices, auto-generated clients. They’re optimized for the problems that AWS and Azure experience, not necessarily problems faced by enterprises or startups. Without diverse governance, they risk becoming sophisticated tools that solve vendor-specific problems.\n\nThe SDK focus of Smithy and TypeSpec reveals something else: these specs assume developers consume APIs through generated code. They’re not optimized for the autonomous agents that LLM vendors hope will form the next wave of API consumers. As a result, the big LLM model providers are creating and pushing new standards:\n\nWhile OpenAPI and AsyncAPI are strategic resources, MCP and A2A are more tactical. “Both MCP and A2A are very transactional, exciting, and in this moment,” Lane said. “They are also likely to give away all your value and data if you are not careful. You have to be very thoughtful in how you transact in those new realms.”\n\nThe question is how you bridge the gap between the tactical needs of an individual team and the strategic needs of the overall enterprise. “I would see this at Postman all the time. Tractor company John Deere would come to us and say, ‘Our CIO, CTO’s office, and Centre of Excellence, manage SOAP, WSDLs, open API, and AsynchAPI across the org. Now we have teams with Postman Collections that run tests and automation, but they don’t understand the bigger picture. We need Postman to reconcile these two worlds for us.’”\n\nThe API economy saw developers craft APIs, treat them as products, rate-limit them, and understand who was using them and what they were doing with them. “MCP, however, wants to circumvent all of that,” Lane said. “It wants direct access to your data and files, so it’s throwing out that decade of design work in front of our file systems and databases, and instead letting the agents have it without much accounting or governance.”\n\nIn addition to wasting significant potential value, poor data governance poses a significant challenge when deploying LLMs for internal use. Organizations can inadvertently expose sensitive information across departments. That data was likely technically accessible before, but required manually searching through Google Drive or file shares to find it.\n\nWhen LLMs gain access to these information repositories, they can surface and share sensitive data far more readily, effectively democratizing access in ways that may violate intended access controls. This was a point that Nicolleta Curtis emphasized to me in an interview for LeadDev. “Even with the basics, such as OneDrive and SharePoint, we found documents that were overshared or with open permissions,” she told me.\n\nOrganizations typically respond to this challenge in one of two ways: they underestimate either the severity of the data exposure risk or the operational burden that proper mitigation will place on their security teams. Implementing appropriate access controls and data boundaries after the fact requires substantial effort.\n\nIn large enterprises with legacy systems, retroactively tightening permissions often breaks existing workflows and integrations. This creates friction across the organization as teams suddenly lose access to information they’ve historically relied on, leading to productivity impacts and internal resistance to the new controls.\n\nIn the first article in this series, Lane described his experience setting up API governance at Bloomberg, which involved:\n\nUsing this approach of comprehensive API mapping and governance with established standards like OpenAPI provides the best foundation for compliance, security, and Personally Identifiable Information (PII) management. For newer/smaller organizations, Lane suggested skipping the ‘baggage’ and going straight to newer approaches such as Agent Skills or MCP.\n\nWhatever approach you favor, we both agree that you should resist the temptation to take a technology-first approach that ignores business outcomes.",
    "readingTime": 6,
    "keywords": [
      "accumulate layers",
      "web services",
      "cloud providers",
      "legacy systems",
      "behind systems",
      "integration standards",
      "access controls",
      "smithy and typespec",
      "the api",
      "agent skills"
    ],
    "qualityScore": 1,
    "link": "https://thenewstack.io/ai-strategy-api-sediment/",
    "thumbnail_url": "https://cdn.thenewstack.io/media/2026/02/874b4afb-peter-olexa-rytit3b7xw4-unsplash-scaled.jpg",
    "created_at": "2026-02-17T18:42:44.292Z",
    "topic": "tech"
  },
  {
    "slug": "agntor-trust-infrastructure-for-ai-agents-identity-escrow-guard",
    "title": "Agntor – Trust infrastructure for AI agents (identity, escrow, guard)",
    "description": "Contribute to agntor/agntor development by creating an account on GitHub.",
    "fullText": "agntor\n\n /\n\n agntor\n\n Public\n\n docs.agntor.com\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n agntor/agntor",
    "readingTime": 1,
    "keywords": [
      "agntor",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/agntor/agntor",
    "thumbnail_url": "https://opengraph.githubassets.com/fdbe116b5444b69fe4a879f37c52130d165afc0515bc1ad17eaae0f160810836/agntor/agntor",
    "created_at": "2026-02-17T18:42:43.744Z",
    "topic": "tech"
  },
  {
    "slug": "a-software-ceo-explains-why-hes-not-worried-about-the-ai-apocalypse-coming-for-his-industry",
    "title": "A software CEO explains why he's not worried about the AI apocalypse coming for his industry",
    "description": "Basware CEO Jason Kurtz explained how he keeps his software company ahead of AI startups threatening his business.",
    "fullText": "It's the end of the world as software companies know it, and this CEO feels fine.\n\nJudging by what some AI experts are saying, and the state of the stock market, you'd think software companies were on their last legs. Basware CEO Jason Kurtz, whose company sells software for financial processes, sees it slightly differently.\n\n\"I will tell you there is not a single piece of data that we see that says that,\" said Kurtz.\n\n\"I have not had a single customer tell us, 'Oh, we're just going to go figure this out on our own and do AP with OpenAI or whoever.' That's not the way these companies work,\" he added.\n\nKurtz reached out to me last week after I asked for some reader feedback on the topic. (I genuinely read all your emails. Even the mean ones!)\n\nBasware, which counts Mercedes and Heineken among its roughly 6,500 customers and uses AI within its own products, hasn't felt threatened thus far. Kurtz told me the company saw a 20% year-over-year increase in sales in 2025, primarily driven by a surge in the back-half of the year.\n\nHe acknowledged that there had been some customer chatter about experimenting with AI on their own. But more recently, clients just want results.\n\nKurtz recalled a conversation with the digital transformation officer of a large European company. After spending roughly a million euros on internal AI-related projects in finance over the past year, the executive told Kurtz they \"can't point to a single penny that we have saved, earned, or helped our business in any way.\"\n\n\"I'm tired of experimenting. I want people who know how to use AI in our processes in our workflows,\" Kurtz said the executive told him.\n\nI asked Kurtz for advice for fellow software companies looking to protect themselves.\n\nBasware primarily works with AWS to help build its AI tools for customers. The company also has an \"AI czar,\" according to Kurtz, to surveil the industry. Figuring out ways to implement AI into your own products that'll drive more value for customers is one way to stay ahead.\n\nThere's also strength in numbers. Kurtz said maintaining tight integrations with fellow vendors to become a part of the workflow creates stickiness.\n\n\"If we weren't doing that, I'd be even more paranoid,\" Kurtz said.\n\nAnd then there's the data element. Basware has processed 2.5 billion invoices and 10 trillion euros of spend in the company's 40-year history. Having such a large swath of info can help train models and identify new efficiencies to pitch to customers.\n\n\"If you don't have a data strategy around AI and how you're going to use that to differentiate your AI and your capabilities, I think that's going to be a challenge,\" he said.",
    "readingTime": 3,
    "keywords": [
      "software",
      "customers",
      "kurtz",
      "processes",
      "customer",
      "that's",
      "roughly",
      "products",
      "primarily",
      "experimenting"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/basware-ceo-jason-kurtz-ai-software-apocalypse-advice-2026-2",
    "thumbnail_url": "https://i.insider.com/698fa49ad3c7faef0ece4ead?width=1200&format=jpeg",
    "created_at": "2026-02-17T18:42:38.289Z",
    "topic": "tech"
  },
  {
    "slug": "billionaire-investor-vinod-khosla-wants-to-rethink-capitalism-for-the-ai-era-and-suggests-scrapping-taxes-for-125",
    "title": "Billionaire investor Vinod Khosla wants to 'rethink' capitalism for the AI era — and suggests scrapping taxes for 125 million people",
    "description": "Vinod Khosla said AI warrants a \"rethink of capitalism,\" and taxing capital more could allow 125 million people to be removed from US tax rolls.",
    "fullText": "If artificial intelligence eliminates millions of jobs, it might make sense to scrap income taxes for the vast majority of Americans and target capital instead, Vinod Khosla says.\n\n\"AI will transform economies and need a rethink of capitalism & equity,\" the billionaire venture capitalist wrote in an X post on Monday. \"Labor portion of economy (vs capital) will decline sharply. Should we eliminate preferential treatment of capital gains tax and equalize to ordinary income?\"\n\nKhosla — who cofounded Sun Microsystems and made the first VC investment in OpenAI — was making the point that AI replacing labor on a grand scale might warrant greater taxes on assets such as stocks and real estate.\n\nThe veteran financier, who founded Khosla Ventures after leaving Kleiner Perkins, attached a video highlighting some of the jobs that could be taken by AI, from accountants and therapists to truck drivers and chip designers.\n\nAI will transform economies and need a rethink of capitalism & equity. Labor portion of economy (vs capital) will decline sharply. Should we eliminate preferential treatment of capital gains tax and equalize to ordinary income? 40% of capital gains taxes are paid by those with… pic.twitter.com/7oSA9xj5Ko\n\nKhosla said in a follow-up post that ramping up taxes on capital would generate so much revenue that the government could scrap taxes for most of the roughly 150 million US taxpayers.\n\n\"Could easily eliminate bottom 125 million taxpayers from the tax rolls and be revenue neutral at the same time with a capital gains tax equal to ordinary income and a few other tweaks,\" he wrote.\n\nHe added that tax breaks such as carrying over tax losses and tax-free borrowing against unrealized gains — which he called a \"true abuse!\" — are \"special interest goodies inserted by lobbyists and campaign contributions, not true capitalism.\"\n\nKhosla didn't address common critiques of higher taxes, including that they can discourage entrepreneurship and investment, that collecting them can be tricky, and that wealthy people may leave the country to avoid them.\n\nKhosla has previously underscored that the advent of AI may require sweeping policy changes. He estimated in late 2024 that in 25 years' time, AI could be doing 80% of the work in 80% of all jobs, and universal basic income might be needed to compensate for job destruction.\n\n\"As AI reduces the need for human labor, UBI could become crucial, with governments playing a key role in regulating AI's impact and ensuring equitable wealth distribution,\" he wrote on his firm's website.\n\nKhosla isn't alone in predicting AI will change the fabric of society. Elon Musk suggested late last year that work could become \"optional\" and money might become \"irrelevant\" if advances in AI and robotics generate abundant resources for all.\n\nMoreover, the Tesla and SpaceX CEO recently said that retirement savings may not be needed in 10 or 20 years, as everyone might have \"whatever stuff they want.\"\n\nHowever, skeptics such as Michael Burry of \"The Big Short\" fame have cautioned the AI boom is a speculative bubble, tech companies are overinvesting in microchips and data centers that will quickly become obsolete, and true AI is further away than many think.",
    "readingTime": 3,
    "keywords": [
      "transform economies",
      "decline sharply",
      "preferential treatment",
      "capitalism equity",
      "eliminate preferential",
      "labor portion",
      "ordinary income",
      "capital gains",
      "gains tax",
      "taxes"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/vinod-khosla-ai-taxes-capital-labor-job-losses-billionaires-musk-2026-2",
    "thumbnail_url": "https://i.insider.com/69946045d3c7faef0ece5d91?width=1200&format=jpeg",
    "created_at": "2026-02-17T18:42:38.112Z",
    "topic": "finance"
  },
  {
    "slug": "12hour-days-no-weekends-the-anxiety-driving-ais-brutal-work-culture-is-a-warning-for-all-of-us",
    "title": "12-hour days, no weekends: the anxiety driving AI’s brutal work culture is a warning for all of us",
    "description": "San Francisco’s AI startups are pushing workers to grind endlessly, hinting at pressures soon hitting other sectors\nNot long after the terms “996” and “grindcore” entered the popular lexicon, people started telling me stories about what was happening at startups in San Francisco, ground zero for the artificial intelligence economy. There was the one about the founder who hadn’t taken a weekend off in more than six months. The woman who joked that she’d given up her social life to work at a prestigious AI company. Or the employees who had started taking their shoes off in the office because, well, if you were going to be there for at least 12 hours a day, six days a week, wouldn’t you rather be wearing slippers?\n“If you go to a cafe on a Sunday, everyone is working,” says Sanju Lokuhitige, the co-founder of Mythril, a pre-seed-stage AI startup, who moved to San Francisco in November to be closer to the action.",
    "fullText": "Not long after the terms “996” and “grindcore” entered the popular lexicon, people started telling me stories about what was happening at startups in San Francisco, ground zero for the artificial intelligence economy. There was the one about the founder who hadn’t taken a weekend off in more than six months. The woman who joked that she’d given up her social life to work at a prestigious AI company. Or the employees who had started taking their shoes off in the office because, well, if you were going to be there for at least 12 hours a day, six days a week, wouldn’t you rather be wearing slippers?\n\n“If you go to a cafe on a Sunday, everyone is working,” says Sanju Lokuhitige, the co-founder of Mythril, a pre-seed-stage AI startup, who moved to San Francisco in November to be closer to the action. Lokuhitige says he works seven days a week, 12 hours a day, minus a few carefully selected social events each week where he can network with other people at startups. “Sometimes I’m coding the whole day,” he says. “I do not have work-life balance.”\n\nAnother startup employee, who came to San Francisco to work for an early-stage AI company, showed me dismal photos from his office: a two-bedroom apartment in the Dogpatch, a neighborhood popular with tech workers. His startup’s founders live and work in this apartment – from 9am until as late as 3am, breaking only to DoorDash meals or to sleep, and leaving the building only to take cigarette breaks. The employee (who asked not to use his name, since he still works for this company) described the situation as “horrendous”. “I’d heard about 996, but these guys don’t even do 996,” he says. “They’re working 16-hour days.”\n\nStartups have never been particularly glamorous. When I started reporting on the industry a decade ago, people were cashing in on the new mobile app economy, and coders were chugging Soylent to stay at their desks longer. Startups then, too, were defined by hustle culture, high-octane energy and the pursuit of growth at all costs – ideas that, to some extent, have remained in the bloodstream of the industry.\n\nBut in the last year, as the magic dust of artificial intelligence has settled in San Francisco, the vibe among tech workers does seem different. The excitement about a new epoch in tech – and all the money that comes with it – is now tempered with anxieties about the industry, and the economy. Some workers are going all in on AI while also questioning whether all that AI is good for the world. Others are effectively training machines to do their jobs better than they can. And many of the same workers who are racing to build the future are now wondering if the future they’re building has a place for them in it.\n\nThe rest of us may be ambiently aware of these anxieties, but they are already tangible and keenly felt inside the tech industry. Even the biggest tech companies, once known for coddling employees with on-site massages and barber shops, have scaled back perks as they have escalated the expectations of workers. Mark Zuckerberg and Elon Musk have each been candid about their predictions that AI will replace some junior and mid-level engineers at their companies, and have respectively called for their workforces to be more “efficient” and “extremely hard core” as waves of layoffs set employees on-edge. Tech companies laid off about a quarter of a million workers around the world in 2025, according to a report published by RationalFX. In many of those layoffs, AI was cited as a main factor, even if the full reason for layoffs is often more complex.\n\n“If you were a software engineer five years ago, you could kind of write your ticket,” says Mike Robbins, an executive coach who has worked with companies like Google, Microsoft, Salesforce and Airbnb. Now, the balance of power has shifted away from tech workers, many of whom are left feeling anxious about their work performance. “When companies become less scared about losing employees, then they can be a little more forthright in terms of what they want and be a little more demanding.”\n\nRobbins, who wrote the book Bring Your Whole Self to Work, used to be asked to speak to companies and their leaders about topics like employee burnout, wellbeing and belonging – top priorities in the years during and shortly after the pandemic. “Quite frankly, we’ve stopped talking about all that,” he says. Now, company leaders want advice on topics like change, disruption and uncertainty in the workplace.\n\nThose themes – change, disruption and uncertainty – are each part of the fuel that has driven tech workers to put in more hours, at a higher intensity. Investment in artificial intelligence companies reached record highs in 2025, yet workers are feeling scarcity in ways they haven’t before.\n\n“It’s definitely something that’s on everyone’s mind,” says Kyle Finken, a software engineer at Mintlify, which makes an AI tool for developers. “I think a lot of people are concerned like, ‘Oh, am I going to have a job in three years?’”\n\nDespite his fears, Finken, like many other startup employees I spoke to, feels energized by the “extraordinary innovation” happening in artificial intelligence and believes that there will still be plenty of jobs for software engineers in the future, even if those jobs look different from the pure coding roles of today. He and other tech workers characterized the current moment as a particularly creative and productive time in tech, where people are devoting extra hours to work not because their employers demand it but out of genuine interest in the new tools and capabilities. For example, Garry Tan, the head of the famous startup accelerator Y Combinator, recently bragged that he “stayed up 19 hours” playing around with Claude Code.\n\nEven those who felt excited about the pace of change acknowledged that AI was rapidly augmenting their work, in ways that could have uncertain outcomes for the jobs of the future. “This is definitely not an era of complacency,” says Finken.\n\nOne reason for working so many hours is to keep up with tools and technology that are changing nearly every day. If you take the weekend off, you can miss a major development, which makes it harder to keep up with what competitors are doing. Another reason is to have something to show future employers, especially as more junior-level jobs are replaced by AI.\n\n“No one hires junior developers any more,” says Lokuhitige, the Mythril co-founder. Landing a job now requires “doing something cool”, he says, like building a new product or solving a problem that gets recognized as useful by larger companies. Job postings for entry-level tech jobs have dropped by a third since 2022, according to Indeed’s Hiring Lab, while job postings requiring at least five years of experience have risen. If you’re not grinding at a startup, you’re missing the prerequisite to get hired in the future.\n\nWhile economists are torn about whether AI will replace most jobs or just change them, they seem aligned in the idea that AI has already reshaped a great deal of entry-level work and will continue to do so. A paper published by Stanford researchers in November found “substantial declines in employment for early-career workers” in industries exposed to AI and suggested that areas where change is already occurring could be like a “canary in the coalmine” for the rest of the economy. The Anthropic CEO, Dario Amodei, has suggested AI could eliminate about half of all entry-level jobs in white-collar industries within the next five years.\n\nThe head of the International Monetary Fund recently predicted that 60% of jobs in advanced economies will be eliminated or transformed by artificial intelligence, “like a tsunami hitting the labour market”. In San Francisco, you can already see the early signs, as Uber drivers compete with self-driving Waymos, and baristas are replaced by robotic coffee bars. Professional business services that support the tech industry have also been negatively affected by the layoffs. The pressure to grind in the tech world could be an early signal – a harbinger for what many other industries will feel soon.\n\nRobbins, the executive coach, says that companies once looked to Silicon Valley as a model of how they should operate, down to emulating policies like unlimited vacation days or adopting perks like free lunch in the office.\n\n“There was an idealization of tech and Silicon Valley for a long time across the business world. Some of that has changed,” he says. “Now, people aren’t asking me to tell them what’s going on in the Valley so that they can adopt it, the same way they were a decade ago.”\n\nRather than a model of how we should all work, the tech industry may be a premonition for the anxiety and attempts to compensate that are coming for all of us.",
    "readingTime": 8,
    "keywords": [
      "executive coach",
      "decade ago",
      "software engineer",
      "job postings",
      "artificial intelligence",
      "san francisco",
      "tech industry",
      "tech workers",
      "silicon valley",
      "jobs"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/ng-interactive/2026/feb/17/ai-startups-work-culture-san-francisco",
    "thumbnail_url": "https://i.guim.co.uk/img/media/0dc04be29cbd720664313a65c08e776ecae088ea/0_1012_4091_3273/master/4091.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=3568863089989ed2e94df48fff54f1dd",
    "created_at": "2026-02-17T18:42:37.932Z",
    "topic": "tech"
  },
  {
    "slug": "andrew-yang-says-mass-whitecollar-layoffs-are-closer-than-people-think",
    "title": "Andrew Yang says mass white-collar layoffs are closer than people think",
    "description": "The Forward Party founder said AI \"will kick millions of white-collar workers to the curb in the next 12 - 18 months.\"",
    "fullText": "Expect to see your local Starbucks soon be full of middle-aged former office workers, says Andrew Yang.\n\nThe Forward Party founder and former presidential candidate said AI \"will kick millions of white-collar workers to the curb in the next 12 - 18 months\" in a post on his Substack on Monday.\n\nYang said that, when a company begins to shrink its workforce, its competitors will follow suit.\n\n\"It will become a competition because the stock market will reward you if you cut headcount and punish you if you don't,\" he added.\n\nYang has long warned about the impact of automation on jobs — he had previously told The New York Times in 2018 that he predicted self-driving cars would displace truck drivers, a shift that could \"destabilize society\" and provoke \"riots in the street.\"\n\nIn his Substack post, Yang then laid out which workers could be vulnerable: mid-career office workers, middle managers, call center workers, marketers, and coders. The list goes on.\n\n\"Do you sit at a desk and look at a computer much of the day? Take this very seriously,\" he wrote. \"Millions of workers are about to be given their pink slips.\"\n\nYang did not respond to a request for further comment.\n\nThis January saw more layoffs than any January since 2009. Though this has largely been attributed to economic uncertainty, a few companies have already begun citing AI as a reason they are letting staff go.\n\nPinterest said in January that it expects to lay off 15% of its workforce. A spokesperson for Pinterest said the restructuring was part of the company's \"AI-forward strategy.\"\n\nHP said in November that it would cut up to 6,000 jobs by 2028, citing AI initiatives as the reason.\n\nCritics have also said some companies are using AI as a scapegoat for job cuts.\n\nTech CEOs and AI researchers are divided over how AI will impact society. While Tesla and xAI CEO Elon Musk and Google DeepMind CEO Demis Hassabis predict a future of great abundance for all, others, such as Anthropic CEO Dario Amodei, say we should brace for significant white-collar layoffs.\n\nYang said the impact of his predicted layoffs will be felt beyond those who actually lose their jobs.\n\n\"Let's say you're a dry cleaner, a dog walker, or a hairstylist. If people in your community stop going to the office, your business is going to suffer because there are fewer business shirts to launder, people will walk their dogs themselves, and cut back on trips to the salon,\" he said.\n\n\"The amount of money getting paid to human labor is about to go down,\" Yang said.",
    "readingTime": 3,
    "keywords": [
      "workers",
      "impact",
      "jobs",
      "layoffs",
      "yang",
      "millions",
      "white-collar",
      "substack",
      "workforce",
      "predicted"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrew-yang-mass-layoffs-ai-closer-than-people-think-2026-2",
    "thumbnail_url": "https://i.insider.com/699483b4d3c7faef0ece5f4f?width=1200&format=jpeg",
    "created_at": "2026-02-17T18:42:37.726Z",
    "topic": "finance"
  },
  {
    "slug": "3-reasons-why-googles-gemini-could-be-the-big-bogeyman-of-the-ai-trade",
    "title": "3 reasons why Google's Gemini could be the big bogeyman of the AI trade",
    "description": "Anthropic's Claude AI has snatched headlines recently. But is there another model that is throwing a wrench in the AI trade?",
    "fullText": "While Anthropic's Claude AI has snatched headlines recently for disrupting the software industry, one researcher says there's perhaps an even bigger bogeyman lurking in tech.\n\nTom Essaye, founder and president of the Sevens Report, says Google's Gemini threatens major potential disruptions to how investors currently see major AI firms.\n\nIn a note to clients on Tuesday, Essaye highlighted three risks posed by Gemini, in particular its November update.\n\nThe first is that it could take market share from OpenAI's ChatGPT. That, in turn, could hurt OpenAi's ability to deliver on the $1 trillion in spending it has promised to firms like Nvidia, he said.\n\nSecond, the fact that Google used its own chips to build Gemini could undermine the importance of large semiconductor providers.\n\n\"The reason that Nvidia, Broadcom, Taiwan Semi and others have exploded in recent years was because of insatiable demand for their semiconductor chips, as they are necessary to build out LLMs,\" Essaye wrote. \"Google making their own chips implies demand for chips from NVDA, AVGO and TSMC may be less than expected. That means less earnings growth and a lower multiple for semiconductor stocks.\"\n\nThat leads to the third point: since Gemini is so effective and was cheaper to produce than other leading chatbots, investors are holding spending levels from hyperscalers to a higher degree of scrutiny, Essaye said.\n\n\"If Google can make Gemini as good as ChatGPT on its own chips, then others likely can as well. The fear is that AI becomes commoditized, making trillions of dollars in AI infrastructure investment foolish,\" Essaye wrote.\n\n\"Put plainly, Gemini broke the idea that all money spent on AI was 'good' money that would result in earnings growth,\" he continued. \"Instead, it ushered in scrutiny to AI capex spending and that altered the paradigm AI/tech stocks existed in. Practically, that means it's no longer the case that the company that spends the most on AI infrastructure 'wins' and we can see that in the market reaction to the collapse of mega-cap free cash flow.\"",
    "readingTime": 2,
    "keywords": [
      "earnings growth",
      "chips",
      "semiconductor",
      "investors",
      "firms",
      "market",
      "others",
      "demand",
      "less",
      "stocks"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/google-gemini-stock-market-ai-trade-threaten-openai-nvidia-tsmc-2026-2",
    "thumbnail_url": "https://i.insider.com/69948486d3c7faef0ece5f73?width=1200&format=jpeg",
    "created_at": "2026-02-17T18:42:37.431Z",
    "topic": "finance"
  },
  {
    "slug": "more-than-20000-sign-a-petition-for-openai-to-resurrect-gpt4o",
    "title": "More than 20,000 sign a petition for OpenAI to resurrect GPT-4o",
    "description": "The deprecation of OpenAI's GPT-4o sparks backlash and a petition. Users said they cherish its unique conversational style.",
    "fullText": "Thousands of people are rallying behind a version of ChatGPT after its parent company, OpenAI, retired the model.\n\nOpenAI said on January 30 that GPT-4o would be deprecated alongside three other versions of that model on February 13.\n\nA petition calling on OpenAI to save GPT-4o has amassed roughly 21,900 signatures on Change.org as of Tuesday.\n\n\"For many of us, GPT-4o offers a unique and irreplaceable user experience, combining qualities and capabilities that we value, regardless of performance benchmarks,\" the petition's description says.\n\nOpenAI wrote in a 2025 blog post that the model was known for \"responses that were overly supportive but disingenuous.\"\n\nThe company first set out to sunset GPT-4o last year, but fans pleaded to save it. In response, OpenAI brought the model back for several more months before its latest announcement.\n\nThe petition, created in April 2025 by Sophie Witt, reached 12,500 supporters two weeks ago, after OpenAI shared its latest plans to retire the model, and has continued to climb since.\n\nOn February 2, Witt called on supporters to take collective action against OpenAI's decision by posting about GPT-4o on X.\n\nWitt did not immediately respond to a request for comment.\n\nOpenAI gave GPT-4o a shoutout in its January 30 blog post, saying that many users told the company they like the model's \"conversational style and warmth\" last year. The ChatGPT maker said that feedback helped shape the GPT‑5.1 and GPT‑5.2 models.\n\nThe company cited low usage of GPT-4o as another reason for its retirement, reporting that only 0.1% of users still choose GPT‑4o.\n\nThe last time GPT-4o was retired, CEO Sam Altman was bombarded during an ask-me-anything session on Reddit with calls to reinstate it.\n\nThe calls for its return have been renewed through comments from social media users and petition signees. Some said it felt more like losing a friend than a feature. Other paying ChatGPT users said they'd be canceling their subscriptions in response to the retirement of GPT-4o.\n\n\"No 4o, no money. I will not spend another single penny on OpenAI,\" one X user posted.",
    "readingTime": 2,
    "keywords": [
      "openai",
      "model",
      "users",
      "petition",
      "gpt-4o",
      "retired",
      "january",
      "save",
      "user",
      "blog"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-retires-gpt-4o-20-000-sign-petition-save-it-2026-2",
    "thumbnail_url": "https://i.insider.com/69949378d3c7faef0ece61d2?width=1200&format=jpeg",
    "created_at": "2026-02-17T18:42:37.416Z",
    "topic": "finance"
  },
  {
    "slug": "keysight-launches-gddr7-transmitter-compliance-solution-for-ai-systems",
    "title": "Keysight launches GDDR7 transmitter compliance solution for AI systems",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/company-news/keysight-launches-gddr7-transmitter-compliance-solution-for-ai-systems-93CH-4509516",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/world_news_2_108x81.jpg",
    "created_at": "2026-02-17T18:42:36.933Z",
    "topic": "finance"
  },
  {
    "slug": "why-ai-adoption-stalls-according-to-industry-data",
    "title": "Why AI Adoption Stalls, According to Industry Data",
    "description": "Many companies report widespread AI usage but disappointing returns, assuming the problem lies in execution rather than adoption. New research shows that AI initiatives often stall because employees’ industry-shaped anxiety about relevance, identity, and job security drives surface-level use without real commitment. Leaders who treat AI adoption as a psychological and contextual challenge—not just a technical rollout—are far more likely to convert experimentation into sustained impact.",
    "fullText": "Why AI Adoption Stalls, According to Industry Data by Erin Eatough, Keith Ferrazzi, Wendy Smith and Shonna WatersFebruary 17, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintCompanies in most industries are investing heavily in artificial intelligence: 88% of companies reporting regular AI use. Yet many leaders report familiar frustrations. AI adoption stalls. Performance gains plateau. Employees experiment with new tools but don’t integrate them deeply into how work actually gets done, leaving executives increasingly concerned about ROI.",
    "readingTime": 1,
    "keywords": [
      "adoption stalls"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2026/02/why-ai-adoption-stalls-according-to-industry-data",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_14_1466294985.jpg",
    "created_at": "2026-02-17T18:42:36.916Z",
    "topic": "business"
  },
  {
    "slug": "anthropics-pentagon-talks-hit-surveillance-and-weapons-snag",
    "title": "Anthropic's Pentagon Talks Hit Surveillance and Weapons Snag",
    "description": "Anthropic PBC's talks about extending a contract with the Pentagon are being held up over additional protections the artificial intelligence company wants to put on its Claude tool, a person familiar with the matter said.  Anthropic wants to put guardrails in place to stop Claude from being used for mass surveillance of Americans or to develop weapons that can be deployed without a human involved, the person said, asking not to be identified because the negotiations are private. The Pentagon wants to be able to use Claude as long as its deployment doesn't break the law. Axios reported on the disagreement earlier. Bloomberg Mandeep Singh reports.",
    "fullText": "Anthropic's Pentagon Talks Hit Surveillance and Weapons Snag BloombergAnthropic PBC's talks about extending a contract with the Pentagon are being held up over additional protections the artificial intelligence company wants to put on its Claude tool, a person familiar with the matter said. Anthropic wants to put guardrails in place to stop Claude from being used for mass surveillance of Americans or to develop weapons that can be deployed without a human involved, the person said, asking not to be identified because the negotiations are private. The Pentagon wants to be able to use Claude as long as its deployment doesn't break the law. Axios reported on the disagreement earlier. Bloomberg Mandeep Singh reports.",
    "readingTime": 1,
    "keywords": [
      "claude",
      "talks",
      "surveillance",
      "weapons",
      "pentagon"
    ],
    "qualityScore": 0.45,
    "link": "https://finance.yahoo.com/video/anthropics-pentagon-talks-hit-surveillance-181151173.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/KgtRH04_ep_kgIf29fi6JA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/bloomberg_markets_video_2/bf5025187b31a5257673b13959be0db3",
    "created_at": "2026-02-17T18:42:36.294Z",
    "topic": "finance"
  },
  {
    "slug": "the-big-tech-losers-as-ai-fears-wipe-billions-of-dollars-off-valuations",
    "title": "The Big Tech losers as AI fears wipe billions of dollars off valuations",
    "description": "Concerns over risks to Microsoft's AI business and growing competition from Google ‌and Anthropic have wiped roughly $613 billion off its market value.",
    "fullText": "Feb 16 (Reuters) - The world's most valuable technology stocks have suffered sharp declines in market value ‌this year after years of outsized gains, ‌as investors question whether heavy spending on AI will generate sufficient ​returns to justify the lofty valuations.\n\nMicrosoft (MSFT) shares have fallen about 17% year-to-date on concerns over risks to its AI business and growing competition from Google's (GOOG, GOOGL) latest Gemini model ‌and Anthropic's (ANTH.PVT) Claude ⁠Cowork AI agent, wiping roughly $613 billion off its market value to about $2.98 trillion as ⁠of Friday.\n\nAmazon (AMZN) has shed around 13.85% so far this year, erasing about $343 billion in market value and leaving ​the company ​valued at roughly $2.13 trillion.\n\nEarlier ​this month, Amazon said ‌it expects capital spending to jump more than 50% this year.\n\nNvidia (NVDA), Apple (AAPL) and Alphabet have also seen their market values decline by $89.67 billion, $256.44 billion and $87.96 billion, respectively, since the start of 2026, to $4.44 trillion, $3.76 trillion and $3.7 ‌trillion.\n\nThe pullback signals a broader ​shift in market psychology, with ​investors moving from rewarding ​long-term AI ambitions to demanding near-term ‌earnings visibility after years of ​speculative enthusiasm.\n\n(Reporting ​by Gaurav Dogra and Patturaja Murugaboopathy in ​Bengaluru; Editing by Sumana Nandy)",
    "readingTime": 2,
    "keywords": [
      "market",
      "investors",
      "roughly",
      "amazon"
    ],
    "qualityScore": 0.85,
    "link": "https://finance.yahoo.com/news/big-tech-stocks-lose-billions-093834534.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/kM_An8HTnTHs5Zy.uQEfSg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2026-02/eab91ac0-0b1e-11f1-acef-9ae1be1a2924",
    "created_at": "2026-02-17T18:42:34.599Z",
    "topic": "finance"
  },
  {
    "slug": "these-three-claude-premium-ai-features-are-now-available-for-free",
    "title": "These Three Claude Premium AI Features Are Now Available for Free",
    "description": "Here's to use these new features",
    "fullText": "Anthropic's Claude is an AI bot that keeps up a steady pace when it comes to pushing out new features, and the latest upgrade of note sees three useful features make their way down to free users, having previously been exclusive to the paid-for plans.\n\nIf you're choosing between AIs and comparing the features available on the free plans, then there's now more of a case to be made for choosing Claude over a competitor like ChatGPT or Gemini for your next batch of AI tasks.\n\nThe three new features now available to free users on Claude are file creation, external plug-ins called Connectors, and bundles of instructions called Skills. Here's how you can make use of them.\n\nClaude's file creation capabilities let you create Word documents, PowerPoint slideshows, Excel spreadsheets, and PDFs from right inside a conversation. You can either supply the bot with all text, data, and other information you want included, get Claude to invent everything itself, or something in between.\n\nFor example, if you've got a long list of names and scores, Claude can put them into a spreadsheet for you. If you've got a series of images, Claude can combine them into a PDF and describe them. You can get it to analyze and visualize data, produce presentations based on reports, and create summary documents.\n\nTo enable file creation for your account, click your profile icon (bottom left) in Claude on the web, then select Settings > Capabilities and enable Code execution and file creation. With that done, you just have to prompt Claude with the type of file you want to make and what you want included, supplying any information as needed (or telling the AI where to find it online).\n\nAs usual with these AI bots, the more detail and specificity you can provide, the better—the end result is then more likely to be closer to what you were aiming for. I got it to quickly come up with the results of a fictional sports day race, and produce a spreadsheet from it. While it's not the most demanding of tasks, Claude completed it correctly.\n\nConnectors can hook Claude up to a variety of other apps, sites, and services: So if you want to get it to design something for you in Canva, or manage your messages in Slack, or find some travel deals on Trivago, then Claude can do that for you. The full list of current Connectors gives you some idea of what's possible.\n\nTo get to the Connectors from the Claude prompt box, click the small + (plus) icon in the lower left corner, then choose Add connectors. You can search through Connectors by name, and filter them by type and category. When you select one you like, you'll need to supply your account credentials and give Claude permission to access your account.\n\nYour Connectors of choice are then available from the same sub-menu in the prompt box: You can add more plug-ins and remove existing ones from there. You can either select an app, or specify the name of it in your prompt and Claude should understand what you mean. You can ask for outputs, run searches, and communicate through your connected services.\n\nConnectors can give Claude some handy extra talents. With the Canvas Connector, for example, I was able to create a basic bit of artwork for a birthday party flyer—something that the AI wouldn't have been able to do on its own. I find that access was spotty, however, perhaps a sign of a lot of free users now making use of these tools.\n\nWith Skills, you can \"teach Claude how to complete specific tasks in a repeatable way\" (in the words of the official support document). In old-school computer talk, they might be referred to as macros: batches of set instructions that Claude can repeat whenever you need something doing in a particular way.\n\nTemplates are a good example, whether they're for emails or documents. Rather than just getting Claude to write an email for you, you can set down some basic parameters for the job that include guidelines on tone, length, and style, as well as crucial bits of information (such as your contact details) that always need to be included.\n\nClick your account profile icon (bottom left) in Claude on the web, then choose Settings > Capabilities and click Add under Skills to get started. You can create a Skill through a Claude conversation, by writing out the instructions, or by uploading a Skills file (which is handy for including extra items such as code snippets, as described here).\n\nI took the Create with Claude route to put together a basic way of summarizing PDF reports, with specific guidelines on how many paragraphs and headings to use, and the tone of voice to apply. In the future, rather than typing out those instructions every time I need something summarized, I can just invoke the Skill.",
    "readingTime": 5,
    "keywords": [
      "profile icon",
      "icon bottom",
      "free users",
      "prompt box",
      "file creation",
      "settings capabilities",
      "claude",
      "create",
      "features",
      "instructions"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/claude-premium-ai-features-for-free-users?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHKHWF3SFWBAY3CB4Q84ESXG/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-17T18:42:34.578Z",
    "topic": "tech"
  },
  {
    "slug": "how-duckduckgos-new-encrypted-voice-ai-chat-compares-with-chatgpt-and-gemini",
    "title": "How DuckDuckGo's New Encrypted Voice AI Chat Compares With ChatGPT and Gemini",
    "description": "Duck.ai's voice chat preserves your privacy, but can't compete with similar options from other companies.",
    "fullText": "While OpenAI is pushing ads on its free users, DuckDuckGo's Duck.ai portal is going a different way. Duck.ai is a privacy-first AI chatbot that doesn't use your data for training, but still gives you AI answers using popular models, including those from OpenAI. The data privacy feature goes beyond as well. DuckDuckGo removes all private metadata (like your location and IP address) before prompting the AI model, and it doesn't share anything about you or your device. Your questions, as well as DuckDuckGo's answers, are never used for AI training.\n\nSince its launch in 2024, the portal has only offered a chatbot interface, but now, DuckDuckGo has added a voice mode as well. With voice chat, instead of reading through long and meandering answers, the AI replies in short, to-the-point snippets that are relevant to your query. Duck.ai's take on the feature is competing with those from companies like OpenAI and Google, and it's free—though expanded limits are offered for DuckDuckGo subscribers.\n\nDuck.ai's voice chat is opt-in, not mandatory. In fact, you can even use it without a DuckDuckGo account. To try it, head to the Duck.ai portal, then from the sidebar, choose the voice chat option and enable it for your account.\n\nNow, when you click the \"New Voice Chat\" button in the sidebar, Duck.ai's bot will appear. You can start speaking, and the bot will reply to you. Just like ChatGPT or Gemini, this is a continuous voice chat, so you don't need to perform any action to ask follow-up questions. You can also interrupt the AI answer to add clarifications or to ask more questions.\n\nWhile the text prompts let you choose the models (including OpenAI's ChatGPT 5-mini), it's not clear exactly what powers voice chat. DuckDuckGo says that it uses an OpenAI model, but doesn't specify which one it is.\n\nOf course, the real question is how Duck.ai's voice chat holds up against Gemini and ChatGPT. For general knowledge questions, Duck.ai holds its own, but it falters when it comes to the latest news. I asked all three services the same questions, and while some responses were similar, ChatGPT's voice mode offers the best overall user experience by far.\n\nI tested the voice chat features using three different kinds of questions. First, I asked about the upcoming Samsung S26 series; second, we talked about the Roman Empire; and lastly, I asked for some advice on how to get started with coding.\n\nWhen it comes to asking questions about news, like Samsung's S26 release, DuckDuckGo's limitations are immediately evident. It sometimes flat out refuses to answer, saying its knowledge cutoff is 2023. Other times, it gives vague responses about the upcoming event, suggesting I check news sites for the latest information. When pressed for details, like when the event is or the rumors surrounding it, it goes back to its cut-off period excuse.\n\nChatGPT's app, however, gave me a detailed response with all the latest rumors, as well as articles to read for additional information—basically, what you'd expect from an AI assistant. Gemini Live provided shorter responses than ChatGPT, though they were accurate. I was able to get Gemini to give me more details in the regular text mode, which reads aloud results if you ask questions using the Mic button, but this defeats the back-and-forth purpose of a voice mode.\n\nDuck.ai didn't fare much better when I asked about the Roman Empire. I asked for a brief overview of the subject, before cutting it off to just ask who the last emperor was. It answered correctly (Romulus Augustulus), and its overview was fine, but lacked details about the transitionary period and exact dates.\n\nAgain, ChatGPT gave me a much more detailed answer (as demonstrated by the screenshot below). Gemini Live's answer, however, was devoid of any real dates, or meaning. Mic mode offered more details, but Google's voice mode was quite limited.\n\nDuck.ai performed better when I asked it about learning how to code. It followed a very similar script to ChatGPT and Gemini, suggesting I learn Python, even offering the same sources for learning (e.g. freeCodeCamp and Harvard CS50 courses).\n\nGemini Live was the outlier here, though, asking follow-up questions about what I'd like to build or practice. It then changed its answers based on my project ideas (switching from Python to JavaScript as the first language I should learn to build web projects). ChatGPT provided an overview, again focusing on Python, and elaborated on the language's barrier to entry when I asked \n\nDuck.ai's voice chat feature is a mixed bag. It can be fast, doesn't use any personal information, and lets you interrupt it. But its limited knowledge base and its inability to give detailed answers are what make it tough to recommend. For the smoothest voice mode experience, ChatGPT is still the king. While DuckDuckGo has the advantage for privacy, you could always use ChatGPT while logged out or in temporary mode to limit the data you share with OpenAI.",
    "readingTime": 5,
    "keywords": [
      "duck.ai portal",
      "duck.ai's voice",
      "voice chat",
      "voice mode",
      "roman empire",
      "gemini live",
      "doesn't",
      "details",
      "feature",
      "knowledge"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/how-duckduckgos-new-encrypted-voice-ai-chat-compares-with-chatgpt-and-gemini?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KH9412V067PAYN9ZB0919XH8/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-17T18:42:34.408Z",
    "topic": "tech"
  },
  {
    "slug": "apple-music-will-soon-let-you-generate-playlists-with-ai",
    "title": "Apple Music Will Soon Let You Generate Playlists With AI",
    "description": "The new feature is called \"Playlist Playground.\"",
    "fullText": "Over the years, Apple Music has improved its algorithmic playlists. There's now an AI DJ, and you can use ChatGPT to generate playlists. With iOS 26.4, which is currently in beta, Apple wants you to make playlists with its own AI tech. The upcoming update includes a feature called \"Playlist Playground,\" which lets you generate AI playlists directly in the Apple Music app—as long as you're running iOS 26.4.\n\nI do not recommend installing and running beta versions of iOS on your primary iPhone. Beta software is unfinished, which means you could run into bugs and glitches that may impact how you use your iPhone, or even result in data loss. If you're itching to try out new features, it's best to ensure that you've taken a complete backup of your iPhone first. That way, you can always revert to an older installation in case something goes wrong. Even so, it's safer to run test software on a backup iPhone as opposed to your daily driver.\n\nWith that said, if you're sure you want to go ahead and install iOS 26.4 right now, you can go to Settings > General > Software Update on your iPhone. Select Beta Updates > iOS 26 Developer Beta. Now, go back to the Software Update page and wait until you see iOS 26.4 Beta 1 appear. You can now download and install the update to try this new Apple Music feature.\n\nOnce you're on iOS 26.4, you can open the Music app to get started with this feature. Tap the Library tab in the bottom bar, and then select the New Playlist button in the top-right corner. You should see the Playlist Playground feature here. (Note that this feature may not appear on devices that don't support Apple Intelligence, or if your Apple Account is from a region where Apple has restricted the rollout of AI features.)\n\nOnce you activate the feature, you'll be able to generate playlists with AI. From here, you tell the AI what you want to listen to. You could get specific, with certain artists, songs, or genres, or ask for playlists that encompass a certain idea of mood. Apple has some pre-written prompts to get you started, such as \"hip-hop party songs,\" but you can use your own text prompts too. AI-generated playlists have 25 songs by default, and you do have the option to customize the playlist further after it's created. You'll also be able to edit the title, cover image, and the description of AI-generated playlists. These playlists can be shared with others or displayed on your Apple Music profile, just like other playlists you create on the streaming service. It's similar in concept to other AI playlist generators on platforms like Spotify or YouTube Music.\n\nWhile Apple Music's Playlist Playground feature is a good start, it's not yet available to those of us who are unwilling to install developer beta versions of iOS 26. If that's you, there are other options out there for AI generated Apple Music playlists. There's the aforementioned ChatGPT integration, of course, but you could also use a third-party app, like PlaylistAI. It has many prompts for you to get started with, and can even generate playlists from music festival posters. The app does prompt you to get a subscription, but you can skip that prompt and use the free tier to generate a playlist quickly.",
    "readingTime": 3,
    "keywords": [
      "playground feature",
      "playlists there's",
      "ai-generated playlists",
      "apple music",
      "beta versions",
      "developer beta",
      "generate playlists",
      "playlist playground",
      "iphone",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-music-will-soon-let-you-generate-playlists-with-ai?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHP8M521BVVYXZQ0AG2D8V3G/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-17T18:42:34.372Z",
    "topic": "tech"
  },
  {
    "slug": "proxima-local-opensource-multimodel-mcp-server-no-api-keys",
    "title": "Proxima – local open-source multi-model MCP server (no API keys)",
    "description": "Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API - Zen4-bit/Proxima",
    "fullText": "Zen4-bit\n\n /\n\n Proxima\n\n Public\n\n Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API\n\n License\n\n View license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Zen4-bit/Proxima",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Zen4-bit/Proxima",
    "thumbnail_url": "https://opengraph.githubassets.com/da1ce1c4daf052f563ec5615a17f7062e6ab435c084a9666f41483e54046eef5/Zen4-bit/Proxima",
    "created_at": "2026-02-17T12:37:44.260Z",
    "topic": "tech"
  },
  {
    "slug": "ai-is-coming-for-whitecollar-jobs-a-22yearold-developers-view-from-nepal",
    "title": "AI Is Coming for White-Collar Jobs – A 22-Year-Old Developer's View from Nepal",
    "description": "Andrew Yang says AI will wipe out millions of white-collar jobs. As a 22-year-old developer in Nepal, here's my honest take.",
    "fullText": "I read Andrew Yang's latest article, \"The End of the Office,\" yesterday. He called what's happening to white-collar jobs \"the Fuckening.\" As a 22-year-old developer in Nepal who's building his entire career on sitting at a desk and looking at a computer... yeah, that hit different.\n\nYang's not some random guy on Twitter dooming. He ran for president on this exact issue. He's been saying \"AI is coming for your jobs\" since before most of us took it seriously. And now his tone has shifted from warning to mourning.\n\nMillions of office jobs will evaporate in the next 12 - 24 months. This will be an epic disaster for millions of workers and families. https://t.co/6829id4rME\n\nI sat with this one for a while. Not because it's new information. But because it forced me to think about what I'm actually building toward.\n\nThe article is blunt. There are about 70 million white-collar workers in the United States. Yang expects that number to drop by 20-50% over the next several years. Not decades. Years.\n\nHe talked to the CEO of a publicly traded tech company who laid it out: \"We're firing 15% of workers right now. We'll probably do another 20% two years from now. And then another 20% two years later. After that, who knows?\"\n\nCollege grads? Only 30% are finding jobs in their field. Underemployment is at 52%. The social contract of \"study hard, go to school, get a good job\" is, in Yang's words, about to be \"vaporized to smithereens.\"\n\nHe also made a point that stuck with me: someone in his family had AI program a website this week. It completed in minutes what used to take a designer or a firm days of work.\n\nI build websites. That one was personal.\n\nThe comments on Twitter and Reddit were honestly more unsettling than the article itself. Not because they were doomy. Because they were specific.\n\nOne senior software developer wrote: \"I noticed we just stopped hiring new people a few years ago. Not because management made any decision about it, we just didn't NEED anyone. I'm not too worried about losing my job in the next 2 years, but I do worry that if I become unemployed, I may never find another job again.\"\n\nRead that again. A senior dev. Not worried about being fired. Worried about being unhirable if they ever need to look for work again.\n\nAnother commenter nailed something I'd been thinking: \"I don't fear the legacy companies laying off tons of people. What I fear are new companies entering the market doing the same as current companies with a tenth of the employees.\"\n\nThat's the part people miss. It's not just about big companies cutting staff. It's about small teams doing what big teams used to do. A 5-person startup with AI tools competing against a 500-person company. That changes everything.\n\nThen there was the purchasing power question that nobody seems to have a good answer for. If millions lose their jobs, who's buying the products these AI-powered companies are making? Who's paying for iPhones and Netflix subscriptions? One commenter put it perfectly: \"Will unemployed people surviving on growing their own vegetables be buying $1,500 smartphones?\"\n\nNobody had a convincing answer. The best response was basically: \"Yeah, that's a problem for the next CEO.\"\n\nMost of this conversation is happening through an American lens. \"Mid-career managers making six figures\" being laid off. \"Mortgage delinquencies rising.\" \"Silicon Valley home prices dropping.\"\n\nI'm reading this from Ghorahi, Nepal. My reality is different.\n\nI don't have a mortgage. I'm not making six figures. I'm a BCA student freelancing as a frontend developer for a remote company. My cost of living is a fraction of what Americans deal with. In theory, that should make me more resilient. Even if the market gets competitive, I can survive on less.\n\nBut here's the catch. The entire plan for developers like me in Nepal was: learn to code, get good, land a remote job with a company abroad, earn in dollars. That was the path. AI threatens to cut that ladder off at the knees.\n\nWhy would a US startup hire a remote developer from Nepal when Claude can write the code for them? The cost advantage I used to have? AI just undercut it to nearly zero.\n\nIt's a double-edged thing. Lower cost of living means I can weather the storm longer. But the opportunity that was supposed to lift me up - remote work for global companies - might not exist the same way in two years.\n\nI don't have it figured out. But I'm not sitting still either.\n\nThe biggest shift is that I stopped just \"learning to code.\" Knowing Python or React isn't a moat anymore. AI writes decent code. What it doesn't do well is understand what to build, for whom, and why. So I've been shipping actual products - hackathon projects, side projects, freelance work. Taste matters more than syntax now.\n\nI use Claude. I use AI coding assistants daily. Some devs have this weird pride about not using AI. I think that's like refusing to use Stack Overflow in 2015. The tool isn't the threat. Being replaced by someone who uses the tool better than you - that's the threat.\n\nThis blog exists because if AI can do what I do technically, the differentiator becomes who I am. My perspective, my story, my network. A developer from Nepal who ships products and writes about it. AI can't be that.\n\nI've also been focusing on end-to-end ownership. Not just \"I know React\" or \"I know Django\" but taking an idea from zero to deployed product with user feedback loops. That full cycle is harder to automate than any single skill. And hackathons have been the best training ground for this - three wins so far, each one teaching me more about product thinking than any tutorial ever did.\n\nYang wrote about the social contract being vaporized. \"Study hard, go to school, get a good job, live a decent life.\" He's talking about American workers who followed that contract and now feel betrayed.\n\nBut what about people like me who are still IN school? Who are halfway through the contract? I'm in my 6th semester. I'm doing everything \"right.\" Learning relevant technologies. Building projects. Getting work experience. And the ground is shifting under my feet while I'm still on it.\n\nThere's a weird psychology here that I don't see people discussing. Yang's audience is mostly people who had stability and might lose it. I never had that stability. Growing up in Nepal, the idea of a guaranteed career path was always a bit of a fantasy anyway. There was never a corporate ladder waiting for me.\n\nAnd honestly? There's a strange freedom in that.\n\nI don't have to grieve the loss of a career path I never had. I can just... adapt. Build. Figure it out as I go. Which is basically what I've been doing anyway.\n\nBut let me be real. There's also fear. Real fear. Because the one thing that was supposed to be the great equalizer - the internet and remote work letting talented people anywhere compete globally - might be getting disrupted right when I need it most.\n\nI don't have a clean conclusion. Nobody does right now. Yang says \"batten down the hatches.\" The Reddit comments range from \"we're all screwed\" to \"this is overblown\" to \"learn plumbing.\"\n\nI'd rather be the person building with AI than the person being replaced by it. I'd rather ship 10 imperfect products than have a perfect resume that nobody's hiring for. Maybe that's naive. But right now, sitting at my desk in Nepal, I can either panic or build.",
    "readingTime": 7,
    "keywords": [
      "i'd rather",
      "social contract",
      "career path",
      "don't",
      "jobs",
      "developer",
      "that's",
      "remote",
      "workers",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.bhusalmanish.com.np/blog/posts/ai-jobs-nepal-dev-perspective.html",
    "thumbnail_url": "https://cdn.bhusalmanish.com.np/Featured%20Image/og-image-ai-jobs-nepal-dev-perspective/og-image-ai-jobs-nepal-dev-perspective.jpg",
    "created_at": "2026-02-17T12:37:43.787Z",
    "topic": "tech"
  },
  {
    "slug": "log-poisoning-in-openclaw",
    "title": "Log Poisoning in OpenClaw",
    "description": "Eye Security explores an indirect prompt injection risk in OpenClaw’s WebSocket logging, explains what an exploit might look like, and highlights context, impact, responsible disclosure, and practical next steps for secure AI assistant deployments.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://research.eye.security/log-poisoning-in-openclaw/",
    "thumbnail_url": "https://research.eye.security/wp-content/uploads/ChatGPT-Image-Feb-5-2026-10_27_20-PM-1024x683.png",
    "created_at": "2026-02-17T12:37:43.388Z",
    "topic": "science"
  },
  {
    "slug": "70-ai-providers-under-same-rust-interface",
    "title": "70+ AI Providers Under Same Rust Interface",
    "description": "The AI Toolkit for Rust, inspired by the Vercel AI SDK. - lazy-hq/aisdk",
    "fullText": "lazy-hq\n\n /\n\n aisdk\n\n Public\n\n The AI Toolkit for Rust, inspired by the Vercel AI SDK.\n\n aisdk.rs\n\n License\n\n MIT license\n\n 131\n stars\n\n 12\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n lazy-hq/aisdk",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/lazy-hq/aisdk",
    "thumbnail_url": "https://opengraph.githubassets.com/82e8941ec05e97907f49673ca6b3967853e0d46b7cf37ab5a3b0c930d1bd07d2/lazy-hq/aisdk",
    "created_at": "2026-02-17T12:37:43.280Z",
    "topic": "tech"
  },
  {
    "slug": "preventing-runaway-llm-agents-enforcement-layer",
    "title": "Preventing runaway LLM agents (enforcement layer)",
    "description": "Zero-dep runtime enforcement for LLM agents. Budget limits, concurrency gates, degradation control. - amabito/veronica-core",
    "fullText": "amabito\n\n /\n\n veronica-core\n\n Public\n\n Zero-dep runtime enforcement for LLM agents. Budget limits, concurrency gates, degradation control.\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n amabito/veronica-core",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/amabito/veronica-core",
    "thumbnail_url": "https://opengraph.githubassets.com/8796645fd444b302d36804ac9bc560d5fe9c75cbf526d9d9b174f14966b5da13/amabito/veronica-core",
    "created_at": "2026-02-17T12:37:43.154Z",
    "topic": "tech"
  },
  {
    "slug": "teaching-ai-at-elementary-school",
    "title": "Teaching AI at Elementary School",
    "description": "January 2026 - I recently taught a 1-hour class on AI at my daughter's school. My ambitious goals were: (i) live demos of image and video generation; (ii) incorporate all students (40) and teachers (5) into the generations. [Almost everything worked out!](school)",
    "fullText": "I am super happy with my daughter Kate’s school! The Montessori system is very clever, as it places children of different ages into one class; then, children form working groups according to their skills. The children work with older children on subjects they are mastering well and with younger children if they need more time to consolidate their learning.\n\nLast fall, the school asked parents to teach the children about their professions; what a program they got! For example, a neuro-surgeon who investigates the source of consciousness through fMRI scans, a heart surgeon, a high-frequency trader, an aviator, an architect, a politician, and…. a computer science professor :-) These encounters helped give meaning to the children’s learning and opened their imaginations to the scientific and technological careers of tomorrow.\n\nI set myself as main goal: show many interactive demos that include the listeners as much as possible. The AR part was easy, as we simply showed a controllable virtual dinosaur demolishing the room, as well as a makeup try-on application.\n\nFor the Generative AI (GenAI) part, I wanted to show the generation of images and videos that incorporate all listeners. Due to the short time available, it was very challenging. I had to design workflows that are:\n\nIt was an unexpected adventure that took me more time than I would have predicted!\n\nAs demo platform, I was using my regular setup (multiple 4 x H100 servers running in the university’s data center), so compute power was not an issue. With ComfyUI, I was sure to be able to run the latest bleeding edge GenAI models.\n\nAs there were many more children than teachers, my rough plan was:\n\nMy first idea failed hard, even though I tried very hard to make it work. This took most of my preparation time!\n\nI estimated that all this could be completed in 15 minutes, as the two phases would be massively parallel 😉.\n\nUnexpectedly, the open source models that I tried (Flux2 Dev, Qwen-Image-Edit) failed to give consistent results. In my extensive tests, I discovered that while nice generations can be really very nice, there are just way to many failures when trying to swap in 4 humans at the same time.\n\nIn the end, untypically for me, I even tried closed weights models. It did not go much better.\n\nI finally decided to do this part completely different: use a high-speed, high-quality image generator and let the children come up with the prompts! This worked beautifully. We went for Z-Image, as it can generate 1 megapixel images in just over 1 second.\n\nThis part was way easier, as the overall throughput needs to be lower than the image part. I prepared a WanAnimate workflow that can generate a 10 second video in 1 minute. As steps during the class, I planned:\n\nFirst, Kate and me demonstrated the capabilities of Z-Image:\n\nThen, the children in the class could tell me their prompts. Here are some results:\n\nFirst, I showed a slide with: the basic effect of replacing subjects in a video (left), for different subjects (middle), and videos (right):\n\nThen, generation time started! Note that I have blurred the children and the faces of teachers to preserve privacy. The first teacher decided to swap herself with Indira Gandhi:\n\nThe second one was a fan of Louis de Funès:\n\nIn between those, I made a funny mistake as I forgot to swap the input images, so we have Indira Ghandi moving like Louis de Funès 🙈\n\nI was glad about the results! During the demonstrations, the children were absolutely mesmerized. Right after the presentation, a boy came to me with glowing eyes and told me: “I want to learn how to do this!”.\n\nAfterwards, I received several messages from parents, who thanked me and described how excited their children came home after school on that day! The funniest feedback was from another parent: “So, when are you gonna teach this class to us parents?” (shameless plug: you can already book me for the adult version of this class!).\n\nLooking forward to teaching this class again next year!",
    "readingTime": 4,
    "keywords": [
      "children",
      "class",
      "school",
      "subjects",
      "parents",
      "images",
      "models",
      "swap",
      "second",
      "learning"
    ],
    "qualityScore": 1,
    "link": "https://drsandor.net/ai/school/",
    "thumbnail_url": "https://drsandor.net/ai/school/featured.jpg",
    "created_at": "2026-02-17T12:37:42.915Z",
    "topic": "tech"
  },
  {
    "slug": "context-lens-view-your-clis-agent-context-in-realtime",
    "title": "Context Lens: View your CLI's agent context in realtime",
    "description": "See what your AI sees. Framework-agnostic LLM context window visualizer. - larsderidder/context-lens",
    "fullText": "larsderidder\n\n /\n\n context-lens\n\n Public\n\n See what your AI sees. Framework-agnostic LLM context window visualizer.\n\n License\n\n MIT license\n\n 15\n stars\n\n 5\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n larsderidder/context-lens",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/larsderidder/context-lens",
    "thumbnail_url": "https://opengraph.githubassets.com/78d5d1dba7ea3e601ec21745ee20cfc89e01235bcbbb0ea36b614b229fcd2837/larsderidder/context-lens",
    "created_at": "2026-02-17T12:37:42.730Z",
    "topic": "tech"
  },
  {
    "slug": "meta-plans-to-add-facial-recognition-to-its-smart-glasses-report-claims",
    "title": "Meta plans to add facial recognition to its smart glasses, report claims",
    "description": "The feature, internally known as “Name Tag,” would allow smart glasses wearers to identify people and get information about them via Meta's AI assistant.",
    "fullText": "Meta plans to add facial recognition to its smart glasses as soon as this year, according to a new report from The New York Times. The feature, internally known as “Name Tag,” would allow smart glasses wearers to identify people and get information about them through Meta’s AI assistant.\n\nMeta’s plans could change, the report notes. The tech giant has been deliberating since early last year on how to release a feature that carries “safety and privacy risks.”\n\nAccording to an internal memo, the company had originally planned to release Name Tag to attendees of a conference for the visually impaired before releasing it to the public, but didn’t end up doing that.\n\nMeta reportedly saw the political tumult in the United States as a good time to release the feature.\n\n“We will launch during a dynamic political environment where many civil society groups that we would expect to attack us would have their resources focused on other concerns,” the document reads.\n\nMeta considered adding facial recognition technology to the first version of its Ray-Ban smart glasses back in 2021, but dropped the plans over technical challenges and ethical concerns. The NYT reports that the company has revived its plans as the Trump administration has grown closer to Big Tech, and following the unexpected success of its smart glasses.",
    "readingTime": 2,
    "keywords": [
      "facial recognition",
      "smart glasses",
      "name tag",
      "plans",
      "feature",
      "release",
      "political",
      "concerns",
      "meta",
      "meta’s"
    ],
    "qualityScore": 0.75,
    "link": "https://techcrunch.com/2026/02/13/meta-plans-to-add-facial-recognition-to-its-smart-glasses-report-claims/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2024/05/meta-smart-glasses.jpg?w=780",
    "created_at": "2026-02-17T12:37:42.148Z",
    "topic": "tech"
  },
  {
    "slug": "consulting-firms-have-built-thousands-of-ai-agents-now-theyre-trying-to-figure-out-their-worth",
    "title": "Consulting firms have built thousands of AI agents. Now they're trying to figure out their worth.",
    "description": "Consultants at McKinsey, PwC, EY, and BCG raced to adopt AI. Now they're racing to measure it's actual value.",
    "fullText": "Big questions are swirling around AI's real impact — and consultants are racing to supply the answers.\n\nOver the past year, consulting firms have begun deploying armies of AI agents as they work to transform their own operations and advise clients to do the same — automating research, building task-specific tools, and building proprietary AI models.\n\nMcKinsey & Company CEO Bob Sternfels said last month that his firm has launched tens of thousands of internal AI agents in recent years, and eventually plans to have one for all of the company's 40,000 employees.\n\nAmid the rapid rollout, consultants are now asking themselves a tough question: Is it worth it? They are working to measure if AI is truly improving performance, boosting revenue, and freeing consultants to focus on higher-value work.\n\n\"I think we are now in the age of confusion,\" Mina Alaghband, a former McKinsey partner, now the chief customer officer at Writer, a full-stack enterprise AI platform built for agentic AI, told Business Insider.\n\nAlaghband said that a year ago, most companies were focused on adoption, tracking metrics such as how often a tool was used.\n\nNow, she says said the emphasis should be on measuring the value that's created — like the amount of human labor reassigned to higher-value work, or improvements in revenue.\n\nPwC's chief AI officer, Dan Priest, recently told Business Insider that PwC is now less concerned with how many agents it deploys, and more with how many human users each agent has.\n\nPriest said his firm starts by targeting an \"impact zone,\" such as improving the customer experience.\n\nWithin these impact zones, the firm looks to deploy \"specialized AI agents\" that have earned that designation because they're good at what they do, Priest said. \"When we deploy agents, we want to see a high rate of human adoption, which means more humans are using them,\" he said.\n\nEY also prioritizes quality over quantity, Steve Newman, EY's global engineering chief, told Business Insider. The firm tracks the value created by its AI agents through key performance indicators for productivity, quality, and cost efficiency on a month-to-month basis.\n\nIf the defining promises of the AI boom are speed and efficiency, then the metric that may matter more isn’t usage, but time reclaimed.\n\nBoston Consulting Group tracks its agents by that metric — and whether that time is then reinvested in higher-value work, Scott Wilder, a partner and managing director based in Dallas, told Business Insider.\n\nWilder said humans at the firm now spend about 15% less time on low-value activities, like making slideshows, and that those people are reinvesting about 70% of their saved time into higher-value activities, such as deeper analysis.\n\nTime saved doesn't always mean more work. At BCG, it can mean more free time. Wilder said BCG has found that employees keep about 30% of the time AI saves. \"They get a little more sleep or get to go to a yoga class or whatever someone wants to do,\" he said.\n\nNearly a century ago, economist John Keynes predicted that as productivity rose, the balance between work and leisure would inevitably change.\n\n\"I would predict that the standard of life in progressive countries one hundred years hence will be between four and eight times as high as it is,\" he wrote in his 1930 essay \"Economic Possibilities for our Grandchildren.\"\n\nIt's almost 2030, but in small ways, that vision may already be surfacing.\n\n\"It's benefiting them — and this is a tough job, so every hour of free time matters,\" Wilder said.\n\nSomething to share about how consultants are using AI? Business Insider would like to hear from you. Email Lakshmi Varanasi at lvaranasi@businessinsider.com or contact her on Signal at lvaranasi.70.",
    "readingTime": 4,
    "keywords": [
      "agents",
      "firm",
      "consultants",
      "higher-value",
      "impact",
      "chief",
      "human",
      "mckinsey",
      "employees",
      "tough"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mckinsey-bcg-pwc-ey-ai-agents-adoption-value-consulting-industry-2026-2",
    "thumbnail_url": "https://i.insider.com/6990e192a645d118818962f1?width=1200&format=jpeg",
    "created_at": "2026-02-17T12:37:38.026Z",
    "topic": "finance"
  },
  {
    "slug": "ive-helped-lead-ai-adoption-at-both-pwc-and-freshworks-here-are-3-common-ai-mistakes-i-see-workers-making",
    "title": "I've helped lead AI adoption at both PwC and Freshworks. Here are 3 common AI mistakes I see workers making.",
    "description": "Geetha Rajan used to lead upskilling for over 50,000 employees at PwC. She cautions workers against outsourcing their thinking to AI tools.",
    "fullText": "This as-told-to essay is based on a conversation with Geetha Rajan, a director on the global strategy team at Freshworks, a SaaS company. She's based in the San Francisco Bay Area. Her identity and employment have been verified by Business Insider. The following has been edited for length and clarity.\n\nI'm a director on the global strategy team at Freshworks, where I drive high-priority strategic initiatives that shape the company's growth, investment decisions, and execution, including on AI adoption.\n\nPreviously, I spent nearly a decade at PwC advising Fortune 500 companies across healthcare, financial services, and technology on growth strategy and digital transformation. As part of my role, I led the upskilling of over 50,000 employees on automation tools.\n\nTechnological transformation has always been happening, but the cloud or mobile transformation took at least 10 years to fully adopt. ChatGPT hit millions of users in the first few months.\n\nA lot of employers will keep expecting that you use AI every day without really understanding the consequences. That's the pressure that actually leads you to make more mistakes rather than use it thoughtfully.\n\nThese are some of the mistakes I see that make employees making when adopting AI:\n\nA lot of people try to jump straight into becoming Iron Man and fully automate their workflow. It should be a process. The first step is treating AI or the technology as a super intern, so you have the most control over things while giving it low agency.\n\nFor example, if you start with giving structured data, but you verify every output. AI can hallucinate outputs that are beautifully formatted.\n\nI'm a strategy consultant and advisor. So, in terms of the ideation and thinking, that's the one part I don't usually outsource to AI.\n\nThis has come through a lot of experience in consulting and being in the workforce itself. I first want to mentally write down my model and first principles. I definitely verify numbers and even try to extract unstructured data from AI, but I still write my first draft very rigorously, keeping my first-principles hat on.\n\nAfter you've completed a draft, you can ask it to poke holes and say, \"Hey, you are the most skeptical board member, or the CFO, poke holes in my strategy.\"\n\nA lot of AI outputs are really polished. But if you don't have that acumen, if you haven't seen this enough number of times, you actually can't tell if an AI is actually making a mistake or not. This is where a lot of the workslop comes in: You just take the AI output and throw it into an email or an analysis.\n\nI've made this mistake myself, where I had five or 10 minutes, and I asked AI to quickly write down some design principles for me and throw them on a slide. When I was presenting, I was like, \"Wait, I don't think this makes sense, and this is not what I was actually trying to say.\" I actually embarrassed myself.\n\nYou can also easily get caught up in a situation where the language AI uses is not something you would use colloquially or even in a professional setting.\n\nSometimes my biggest worry is what happens five years from now — when nobody actually did that initial job, and we're burning the ladder as we try to climb it. As much as AI can do things, I think it's more about the commitment to yourself that you still learn problem-solving skills and how to use Excel.\n\nYou need to know exactly who you're solving for and what the purpose of solving that exercise is.\n\nFor example, if you're building an AI model to understand your business's customer segments, you still need to know your segments at a high level. That's the part I would never outsource. If you don't have that context yourself, you could just go in a million directions.\n\nThe fundamental things about taste, process, architecture, how you build things — those don't come from any tool, irrespective of whether you're using ChatGPT or the latest model. If AI throws 50 ideas at you, you need to know which one of those is going to stick. As an employee, it is your responsibility to pick the right one, so you need the acumen and expertise to do so.",
    "readingTime": 4,
    "keywords": [
      "poke holes",
      "strategy team",
      "don't",
      "transformation",
      "that's",
      "model",
      "you're",
      "based",
      "director",
      "growth"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-mistakes-to-avoid-freshworks-strategy-director-2026-2",
    "thumbnail_url": "https://i.insider.com/698f8936a645d11881895974?width=1200&format=jpeg",
    "created_at": "2026-02-17T12:37:38.021Z",
    "topic": "finance"
  },
  {
    "slug": "claims-that-ai-can-help-fix-climate-dismissed-as-greenwashing",
    "title": "Claims that AI can help fix climate dismissed as greenwashing",
    "description": "Industry using ‘diversionary’ tactics, says analyst, as energy-hungry complex functions such as video generation and deep research proliferate\nTech companies are conflating traditional artificial intelligence with generative AI when claiming the energy-hungry technology could help avert climate breakdown, according to a report.\nMost claims that AI can help avert climate breakdown refer to machine learning and not the energy-hungry chatbots and image generation tools driving the sector’s explosive growth of gas-guzzling datacentres, the analysis of 154 statements found.\n Continue reading...",
    "fullText": "Industry using ‘diversionary’ tactics, says analyst, as energy-hungry complex functions such as video generation and deep research proliferate\n\nTech companies are conflating traditional artificial intelligence with generative AI when claiming the energy-hungry technology could help avert climate breakdown, according to a report.\n\nMost claims that AI can help avert climate breakdown refer to machine learning and not the energy-hungry chatbots and image generation tools driving the sector’s explosive growth of gas-guzzling datacentres, the analysis of 154 statements found.\n\nThe research, commissioned by nonprofits including Beyond Fossil Fuels and Climate Action Against Disinformation, did not find a single example where popular tools such as Google’s Gemini or Microsoft’s Copilot were leading to a “material, verifiable, and substantial” reduction in planet-heating emissions.\n\nKetan Joshi, an energy analyst and author of the report, said the industry’s tactics were “diversionary” and relied on tried and tested methods that amount to “greenwashing”.\n\nHe likened it to fossil fuel companies advertising their modest investments in solar panels and overstating the potential of carbon capture.\n\n“These technologies only avoid a minuscule fraction of emissions relative to the massive emissions of their core business,” said Joshi. “Big tech took that approach and upgraded and expanded it.”\n\nMost of the claims that were scrutinised came from an International Energy Agency (IEA) report, which was reviewed by leading tech companies, and corporate reports from Google and Microsoft.\n\nThe IEA report – which devoted two chapters to the potential climate benefits of traditional AI – had a roughly even split between claims that rested on academic publications, corporate websites and those that had no evidence, according to the analysis. For Google and Microsoft, most claims lacked evidence.\n\nThe analysis, released during the AI Impact Summit in Delhi this week, argues the tech industry has misleadingly presented climate solutions and carbon pollution as a package deal by “muddling” types of AI.\n\nSasha Luccioni, AI and climate lead at Hugging Face, an open-source AI platform and community, who was not involved in the report, said it added nuance to a debate that often lumped very different applications together.\n\n“When we talk about AI that’s relatively bad for the planet, it’s mostly generative AI and large language models,” said Luccioni, who has pushed the industry to be more transparent about its carbon footprint.\n\n“When we talk about AI that’s ‘good’ for the planet, it’s often predictive models, extractive models, or old-school AI models.”\n\nGreen claims even for traditional AI tended to rely on weak forms of evidence that had not been independently verified, the analysis found. Only 26% of the green claims that were studied cited published academic research, while 36% did not cite evidence at all.\n\nOne of the earliest examples identified in the report was a widespread claim that AI could help mitigate 5-10% of global greenhouse gas emissions by 2030.\n\nThe figure, which Google repeated as recently as April last year, came from a report it commissioned from BCG, a consulting firm, which cited a blogpost it wrote in 2021 that attributed the figure to its “experience with clients”.\n\nDatacentres consume just 1% of the world’s electricity but their share of US electricity is projected to more than double to 8.6% by 2035, according to BloombergNEF. The IEA predicts they will account for at least 20% of the rich world’s growth in electricity demand to the end of the decade.\n\nWhile the energy consumption of a simple text query to a large language model such as ChatGPT may be as little as running a lightbulb for a minute, partial industry disclosures suggest, it rises considerably for complex functions such as video generation and deep research, and has troubled some energy researchers with the speed and scale of its growth.\n\nA spokesperson for Google said: “Our estimated emissions reductions are based on a robust substantiation process grounded in the best available science, and we have transparently shared the principles and methodology that guide it.”\n\nMicrosoft declined to comment, while the IEA did not respond to requests for comment.\n\nJoshi said the discourse around AI’s climate benefits needed to be “brought back to reality”.\n\n“The false coupling of a big problem and a small solution serves as a distraction from the very preventable harms being done through unrestricted datacentre expansion,” he said.",
    "readingTime": 4,
    "keywords": [
      "complex functions",
      "planet it’s",
      "deep research",
      "green claims",
      "avert climate",
      "climate breakdown",
      "climate benefits",
      "emissions",
      "industry",
      "tech"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/17/tech-companies-traditional-ai-generative-climate-breakdown-report",
    "thumbnail_url": "https://i.guim.co.uk/img/media/81a5a055326cef41e145cb74127306aadb720004/624_0_6257_5006/master/6257.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=810d5d4728a8311c1728f2c264922ab5",
    "created_at": "2026-02-17T12:37:36.840Z",
    "topic": "tech"
  },
  {
    "slug": "alexalike-voice-interface-for-openclaw",
    "title": "Alexa-like voice interface for OpenClaw",
    "description": "An open-source voice agent built on the PamirAI Distiller device, combining speech recognition, and text-to-speech to create a conversational AI assistant with OpenClaw you can talk to. - sachaabot...",
    "fullText": "sachaabot\n\n /\n\n openclaw-voice-agent\n\n Public\n\n An open-source voice agent built on the PamirAI Distiller device, combining speech recognition, and text-to-speech to create a conversational AI assistant with OpenClaw you can talk to.\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n sachaabot/openclaw-voice-agent",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/sachaabot/openclaw-voice-agent",
    "thumbnail_url": "https://opengraph.githubassets.com/693f92e2c77f47abb871b79bae4c5f77d30b04e87eb9b5468b681ec97ea68415/sachaabot/openclaw-voice-agent",
    "created_at": "2026-02-17T06:45:26.489Z",
    "topic": "tech"
  },
  {
    "slug": "fujitsu-aidriven-software-development-platform",
    "title": "Fujitsu AI-Driven Software Development Platform",
    "description": "Fujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society.",
    "fullText": "Kawasaki, Japan, February 17, 2026\n\nFujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society. This platform automates the entire software development process, from requirements definition and design to implementation and integration testing. By leveraging the Takane large language model (LLM) [1] and agentic AI technology for large-scale software development developed by Fujitsu Research, the AI-Driven Software Development Platform enables AI agents to understand complex, evolving large-scale systems owned by enterprises and public organizations. The platform has multiple AI agents collaboratively execute each stage of software development, achieving full automation of the entire process without human intervention.\n\nFujitsu aims to use this AI-Driven Software Development Platform to carry out revisions to all 67 types of medical and government business software products provided by Fujitsu Japan Limited by the end of fiscal year 2026. The revisions are necessary due to legal and regulatory changes. From January 2026, the platform has been used in Japan for software modifications made necessary by the 2026 medical fee revisions [2]. In a PoC that updated software as per the 2024 medical fee revisions, the platform demonstrated a significant reduction in development time for one of approximately 300 change requests. Using conventional software development methods [3] the modifications would have taken three person-months. With this technology that was dramatically shortened to four hours, achieving a 100-fold increase in productivity.\n\nIn AI-driven development, Fujitsu positions AI-Ready Engineering—the process of preparing assets and knowledge to ensure AI correctly understands existing systems and achieves highly reliable automation—as crucial. With AI-Ready Engineering and the AI-Driven Software Development Platform working in tandem, Fujitsu will accelerate AI-driven software development. Fujitsu will promote a transformation in engineers' work styles, strengthening its Forward Deployed Engineer (FDE) complement, and shifting the paradigm of software development from a conventional person-month-based approach to a customer value-based approach.\n\nMoving forward, Fujitsu plans to expand the application of the AI-Driven Software Development Platform to a wide range of sectors, including finance, manufacturing, retail, and public services, by the end of fiscal year 2026. Fujitsu will also begin offering this service to customers and partner companies to enable them to rapidly and flexibly develop systems that adapt to changes in their business environments. Through these efforts, Fujitsu aims to transform the software development process into an AI-driven model as an industry standard.\n\n(Order that companies appear is aligned with the original Japanese press release)\n\nTakashi Manabe, Senior Research Director, AI & Automation, IDC Japan\n“IDC forecasts that from 2026 onward, the acceleration of AI/agent-based business utilization and the modernization of existing systems will be key drivers of transformation in the Japanese IT market. Fujitsu’s announcement aims to redefine complex legacy system assets into a state where AI can accurately understand and process them, and to automate the entire waterfall development process. This initiative is expected to provide a practical pathway for many domestic enterprises facing the ongoing challenge of maintaining and operating legacy assets, while also promoting a shift in software engineering away from a labor-intensive model.”\n\nShinji Kajitani, Director and President Executive Officer, Optima Corporation\n“I am deeply impressed by the concept of automating the entire software development process from upstream to downstream using AI, and even entrusting the verification process to AI. This overturns the traditional assumption that human checks are ultimately indispensable, and I see great potential, especially in targeting business packages that undergo complex system changes every year. Our company has also been involved in business package modifications for many years, and how to complete system revisions with high quality in a short period has always been a major challenge. We believe that the knowledge and expertise accumulated during that process can significantly contribute to the realization and advancement of this concept. Our company will continue to contribute to the business expansion of Fujitsu and Fujitsu Japan through ongoing cooperation, not limited to this project.”\n\nHiroshi Nakatani, Representative Director, Executive Vice President, Kawasaki Heavy Industries, Ltd.\n\"This AI automation initiative promoted by Fujitsu is not merely about improving development efficiency; we recognize it as a significant challenge to pass on and evolve the extensive business knowledge and design philosophies cultivated by companies over many years to the next generation. In particular, the concept of providing end-to-end support, from requirements definition to design, implementation, and quality assurance, triggered by changes in laws and rules, opens up new possibilities in areas that have traditionally relied on human experience and tacit knowledge. We see great significance in AI functioning as a foundation that supports human judgment and creativity, rather than replacing it.\nIn the manufacturing industry, challenges such as design changes, regulatory compliance, and understanding the scope of impact are becoming increasingly complex year by year. Fujitsu's approach of advancing both knowledge standardization and AI utilization in these areas offers valuable insights for enhancing the productivity and competitiveness of the entire industry.\nKawasaki Heavy Industries sincerely hopes that this initiative will be a crucial step in driving the transformation of Japanese manufacturing and a wide range of other industries, and we wholeheartedly support its further development.\"\n\nYasushi Matsuda, President and CEO, Kewpie Digital Innovation Co., Ltd. \n“Systems have become increasingly complex through years of operation and often now require significant maintenance effort. While the introduction of generative AI has improved auditing efficiency, its accuracy remains insufficient for reliable practical application. Amidst this situation, we place great expectations on “Multi-layer Quality Control,” which automatically corrects ambiguities and omissions. We are confident that this mechanism, where AI itself audits quality and autonomously repeats processes, will dramatically enhance the reliability of system development. We eagerly await its future development.”\n\nJunichi Aruji, Managing Director, Kintetsu Information System Co., Ltd. \n“The challenge of revamping existing systems has long been a significant one for engineers. Fujitsu’s AI-Driven Software Development Platform has the potential to dramatically transform the labor previously involved in understanding complex laws and regulations, analyzing vast historical assets, and grasping the tacit knowledge of the field.\nWhat is particularly noteworthy is the AI's ability to autonomously learn \"human intelligence,\" thereby dramatically enhancing the accuracy of requirements definition. Furthermore, it can complete everything from program structure analysis and standardization to the extensive testing phase with incredible speed and comprehensiveness. This makes it possible to deliver high-quality products in a short period.\nAs the role of AI expands and frees people from routine tasks, engineers can focus on more creative activities. I have high expectations for the paradigm shift in system renovation that this solution will bring.”\n\nYumi Ueno, Managing Director, Partner Ecosystem & Corporate Business, Google Cloud Japan G.K. \n“This initiative to achieve comprehensive, one-stop automation spanning from requirements definition to system validation is a groundbreaking innovation for the industry. The technology enables AI to accurately understand vast assets, including long-established programs and design documentation, and we are delighted at the potential for both production-grade quality and exceptional productivity gains. We are confident that this platform will become the new standard for development and accelerate our customers' digital transformation. We remain committed to working with Fujitsu to address social challenges through AI.”\n\nMasahiro Niimi, Managing Executive Officer, Head of Information Systems Management Division, CISO, Sakura KCS Corporation \n“I believe Fujitsu Limited's AI-driven development framework has the potential to become the ‘new paradigm of system development.’ It cannot be achieved simply by feeding existing code or design information into AI, and while there are various hurdles, such as converting documentation to Markdown and establishing test environments, overcoming these hurdles can lead to solving traditional system development challenges (like QCD).\nWhat particularly caught my attention is not just improvements in the development process, but what comes after generative AI, i.e., the incorporation of detailed specifications and code (logic). I see tremendous potential here as a solution to the greatest challenge: visualizing and transferring the tacit knowledge of veteran software engineers’ that is traditionally missing from documentation. We expect generative AI to act as an advisor for less experienced software engineers, readily answering questions anytime, thereby dramatically advancing know-how transfer to the next generation. On the other hand, this mechanism also has the potential to dramatically change the traditional SI business model, and we are watching future developments closely.”\n\nTakao Kazama, Executive Officer, Group Companies and Accounting & Finance, The Shizuoka Shimbun and Shizuoka Broadcasting Co., Ltd. \n\"This initiative for complete automation of application development and maintenance represents a highly valuable transformation for our company. It formalizes and establishes a reproducible process for tasks that have long relied on the implicit knowledge and experience of individual staff members, and we have great expectations for it. In particular, it has the potential to significantly improve quality variations in legacy system maintenance and lost opportunities due to delayed change responses. Furthermore, the evolving ability of AI to perform root cause analysis and identify necessary additional information is a major step towards advanced and efficient system operations, with the potential to change the very nature of system development. We share Fujitsu's commitment to improving productivity across the entire industry and establishing new development standards. We look forward to its continued strong promotion as an initiative that will advance the entire industry.\"\n\nShimane Prefectural Central Hospital \n\"The AI-Driven Software Development Platform presented by Fujitsu offers a practical and robust approach to the long-standing challenges faced by medical institutions: the increasing complexity of medical fee calculations and the growing workload of claims processing. The mechanism where AI analyzes legal documents and extracts the relevant areas, while explicitly highlighting points open to interpretation to supplement human judgment, is particularly impressive. This design demonstrates a deep understanding of on-site operations and is highly commendable. Furthermore, the Japanese-specific LLM and the consideration for safety are indispensable elements for AI utilization in the medical field. Beyond medical fee claims, this technology has potential for integration with related areas such as bed management and understanding performance requirements, making a strong contribution to overall hospital operational efficiency in the future. This is a promising initiative that warrants positive consideration for adoption to alleviate the burden on medical professionals.\"\n\nShinichi Aikawa, Executive Officer, Head of Systems Division, SBI Sumishin Net Bank, Ltd.\n“We expect Fujitsu's AI-Driven Software Development Platform to be an initiative with the potential to fundamentally transform the software development process itself. If a world can be realized where everything from requirements definition to design, coding, and testing can be automatically executed in a seamless, one-stop manner, it will be possible to achieve both a dramatic improvement in development speed and quality. \nSince 2024, we have been working with Fujitsu in some areas of this field. Through these initiatives, we are confident that the entire development process will be automated end-to-end in the near future. By realizing this transformation, the possibilities for the services we can provide to our customers will greatly expand. We think about ideas for new services for our customers on a daily basis. This would allow us to rough out these ideas in a short period of time and provide them to our customers quickly. We hope that this new world of value creation will arrive as soon as possible.”\n\nMasaki Murata, Vice President, IBM Japan \n“We strongly believe that Fujitsu’s announcement marks a significant step forward in the evolution of system development in Japan. It aligns closely with IBM Japan’s vision and represents an important initiative that will help shape the future of the industry as a whole. We look forward to driving this momentum together and contributing to the creation of a more robust and vibrant ecosystem.”\n\nRyota Sato, Managing Executive Officer, Global Communications & IT Services Group, Microsoft Japan Co., Ltd. \n\"We sincerely welcome Fujitsu Limited’s announcement of the AI-Driven Software Development Platform as a pioneering initiative that opens a new chapter in system development for the AI era. By orchestrating multiple AI agents to automate the end-to-end development lifecycle—from requirements definition through ongoing enhancement—while integrating human-led quality assurance, this platform embodies a new engineering model in which people and AI truly work together. We view this initiative as highly significant, as it directly addresses the critical challenges facing Japan’s system development industry, including severe talent shortages and the increasing complexity and sophistication of modern systems. We strongly expect this bold effort to drive the evolution of Japan’s system development business and to grow into a transformation model with global relevance. Moving forward, we will continue to work closely with Fujitsu, combining the strengths of both companies to strongly support our customers in their journey toward becoming Frontier Firms.”\n\nTatsuo Ogawa, Executive Officer Group CTO, Panasonic Holdings Corporation\n“We believe that the AI-driven end-to-end automated system development announced this time represents not only a significant improvement in productivity, but also a bold challenge to fundamentally transform the way enterprise IT is delivered. By enabling AI to accurately understand frequently updated regulations and complex business knowledge, including implicit know-how, this approach autonomously executes processes seamlessly from requirements definition through system modification. It has the potential to provide an effective solution to the core challenges posed by legacy systems faced by many Japanese enterprises. We look forward to jointly refining this technology through hands-on practice and advancing co-creation by incorporating on-site expertise of both Panasonic and Fujitsu, with the expectation that it will become a new standard for system development and be deployed broadly not only within Panasonic but across society as a whole.”\n\nExecutive at a major manufacturing company's IT subsidiary\n“We anticipate this initiative will bring about a new transformation in system development. This transformation will be driven by the application of advanced Japanese language processing capabilities—such as the understanding of legal documents—to diverse tasks, the reliable execution of each process through quality auditing functions, and the expansion of these capabilities to scratch development. Furthermore, we believe that AI Ready Engineering, by formalizing expert know-how and domain knowledge into explicit knowledge and transforming it into AI-usable assets, will significantly contribute to the succession of expertise from an increasingly limited pool of skilled professionals. We sincerely hope that the co-creation between the knowledge-inheriting AI and on-site personnel will generate new value and form the cornerstone for innovation in the system development industry, and indeed, across all industries.”\n\nJointly developed by Fujitsu and Cohere Inc.\n\nA national system that reviews public medical fees and adjusts cost allocation for medical procedures.\n\nDevelopment methods where quality is verified at each stage, from software requirements definition, design, and implementation to integration testing.\n\nThe Sustainable Development Goals (SDGs) adopted by the United Nations in 2015 represent a set of common goals to be achieved worldwide by 2030.\nFujitsu’s purpose — “to make the world more sustainable by building trust in society through innovation” — is a promise to contribute to the vision of a better future empowered by the SDGs.\n\nPublic and Investor Relations Division\n\nAll company or product names mentioned herein are trademarks or registered trademarks of their respective owners. Information provided in this press release is accurate at time of publication and is subject to change without advance notice.\n\nDate: 17 February, 2026\nCity: Kawasaki, Japan\nCompany: Fujitsu Limited",
    "readingTime": 13,
    "keywords": [
      "vice president",
      "kawasaki heavy",
      "heavy industries",
      "fujitsu’s announcement",
      "executive officer",
      "managing director",
      "kawasaki japan",
      "wide range",
      "press release",
      "increasing complexity"
    ],
    "qualityScore": 1,
    "link": "https://global.fujitsu/en-global/pr/news/2026/02/17-01",
    "thumbnail_url": "https://global.fujitsu/-/media/Project/Fujitsu/Fujitsu-HQ/pr/news/2026/02/17-01/news-20260217-01th.png?rev=06c466bd8ac14732a2ff3eff27b55e3b",
    "created_at": "2026-02-17T06:45:26.020Z",
    "topic": "tech"
  },
  {
    "slug": "the-eus-privacy-watchdog-is-investigating-x-over-sexualized-ai-images",
    "title": "The EU's privacy watchdog is investigating X over sexualized AI images",
    "description": "Ireland's Data Protection Commission said it is investigating X over the generation of sexualized images of people in the EU, including minors.",
    "fullText": "X is facing mounting criticism from foreign watchdogs over its generative AI chatbot, Grok.\n\nThe Irish Data Protection Commission said Tuesday that it had opened an inquiry into Elon Musk's X, formerly known as Twitter.\n\nThe commission said in a press release that the inquiry was linked to the creation and publication of non-consensual, sexualized images of European Union residents on X using Grok's generative AI functions. This included pictures of children.\n\nThe commission, which is responsible for enforcing the EU's General Data Protection Regulation, said in the release that it notified X of the investigation on Monday.\n\nX did not respond to a request for comment from Business Insider.\n\nGrok is a chatbot developed by Musk's xAI, now a subsidiary of his aerospace company SpaceX.\n\nThe commission's investigation follows several weeks of controversy around Grok and X. The platform came under fire worldwide in January after reports emerged of Grok users generating sexualized images of real people, including minors.\n\nCountries like Indonesia, Malaysia, and the Philippines temporarily suspended access to Grok. The European Commission launched an investigation into Grok, while India's information technology ministry voiced its opposition via a letter to the chief compliance officer of X's India operations.\n\nCalifornia's Attorney General, Rob Bonta, also said in early January that he had launched a probe into Grok's AI deepfakes.\n\nIn response, X made Grok's AI image generation tool a premium feature limited to paying subscribers and later stopped it from generating sexualized images altogether. However, a Business Insider report found that it was still possible to trigger these images in Grok's web and mobile applications.\n\nIn response to backlash over Grok, Musk said in an X post on January 3, \"Anyone using Grok to make illegal content will suffer the same consequences as if they upload illegal content.\"",
    "readingTime": 2,
    "keywords": [
      "business insider",
      "illegal content",
      "generating sexualized",
      "sexualized images",
      "grok's ai",
      "investigation",
      "grok",
      "generative",
      "chatbot",
      "protection"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/european-union-privacy-watchdog-investigating-x-sexualized-ai-images-2026-2",
    "thumbnail_url": "https://i.insider.com/6993e156d3c7faef0ece5c7d?width=1200&format=jpeg",
    "created_at": "2026-02-17T06:45:10.169Z",
    "topic": "finance"
  },
  {
    "slug": "analysisluxury-stocks-volatility-highlights-ai-jitters-hedge-fund-positioning",
    "title": "Analysis-Luxury stocks’ volatility highlights AI jitters, hedge fund positioning",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/analysisluxury-stocks-volatility-highlights-ai-jitters-hedge-fund-positioning-4507860",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1G05A_L.jpg",
    "created_at": "2026-02-17T06:45:01.845Z",
    "topic": "finance"
  },
  {
    "slug": "from-openai-to-google-india-hosts-global-ai-summit",
    "title": "From OpenAI to Google, India hosts global AI summit",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/from-openai-to-google-india-hosts-global-ai-summit-4507461",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1F09E_L.jpg",
    "created_at": "2026-02-17T06:45:01.760Z",
    "topic": "finance"
  },
  {
    "slug": "top-hollywood-screenwriter-warns-tiktoks-new-tool-is-at-the-gates-i-hate-to-say-it-its-likely-over-for-us",
    "title": "Top Hollywood screenwriter warns TikTok’s new tool is at the gates: ‘I hate to say it. It’s likely over for us’",
    "description": "Seedance 2.0, which is only available in China for now, lets users generate high-quality AI videos using simple text prompts.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/16/top-hollywood-screenwriter-warns-tiktoks-new-tool-is-at-the-gates-i-hate-to-say-it-its-likely-over-for-us/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/02/AP26046699469879-e1771264237998.jpg?resize=1200,600",
    "created_at": "2026-02-17T01:11:10.075Z",
    "topic": "business"
  },
  {
    "slug": "ais-first-wave-was-about-cutting-costs-the-second-wave-is-about-building-things-weve-never-seen",
    "title": "AI's first wave was about cutting costs. The second wave is about building things we've never seen.",
    "description": "Startup CEOs like Kylan Gibbs and Sara Beykpour talk about AI's Second Wave, focusing on creating new products beyond cost-cutting.",
    "fullText": "For the past three years, AI has been mostly a cost-cutting tool. A growing number of founders and investors are trying to move beyond that era.\n\nThe next chapter of AI, they argue, will be defined by new kinds of products — apps, games, companions, and services that simply couldn't exist before large language models. They call it AI's \"Second Wave.\"\n\n\"The first wave of AI made existing things cheaper. Automation. Efficiency,\" said Kylan Gibbs, a former Google DeepMind product manager who runs AI startup Inworld. \"The next wave makes things that couldn't exist before. New products. New experiences. New revenue. That's the difference between optimizing spend and creating it.\"\n\nFor Gibbs, that distinction is existential. If AI merely trims costs, it reshuffles value within existing businesses. If it enables entirely new consumer products — ones people will pay for — it expands the economic pie.\n\n\"AI reaches its real economic potential when it creates value consumers want to pay for, not just value businesses want to save,\" he wrote on LinkedIn. That next phase, he says, requires a new \"consumer-scale AI stack\": real-time responses under 300 milliseconds, support for millions of users simultaneously, and deeply personal experiences tailored to individual preferences.\n\nIn January, Gibbs launched a Silicon Valley accelerator to back up to 30 \"Second Wave\" AI startups — companies building new consumer experiences rather than bolting chatbots onto old workflows. Venture capital firms, including Khosla Ventures and Lightspeed Venture Partners, are involved, alongside leaders from OpenAI, Google, and Stripe. A demo day will take place in early March in San Francisco.\n\nThe philosophy echoes a recent post from Y Combinator CEO Garry Tan: \"Instead of worrying about doing the same thing we've been doing for cheaper, why not focus on doing the thing we never even dreamed of doing?\"\n\nA handful of startups already embody that ethos.\n\nSara Beykpour, CEO and cofounder of Particle, says the tech industry is in a liminal moment.\n\n\"We're in a transition between the first wave and the second wave,\" she said.\n\nThe first wave delivered massive productivity gains. At Particle, an AI-native news platform, tasks that once took a month can now be built, tested, and deployed in hours.\n\n\"We actually call each other out in meetings when someone falls into the old way of thinking,\" Beykpour said. \"We jokingly call it 'boomer thinking,' even though we're all millennials.\"\n\nThat shift in mindset gives the startup more time to focus on unlocking new AI-powered formats. Particle recently launched Podcast Clips, a feature that embeds the most relevant snippets of long-form podcasts directly into news stories. Instead of hunting through a three-hour episode, users see curated clips attached to specific topics.\n\n\"It changes the information hierarchy,\" Beykpour said. \"Instead of having to find the podcast you want to listen to, we're bringing the podcast to you based on the most relevant parts.\"\n\nUnder the hood, the system uses AI embeddings to map relationships between transcripts and stories. A clip from a talk show about Greenland and Davos, along with comments from President Donald Trump, can be automatically linked to relevant reporting. Generative AI then layers summaries and context on top.\n\nThese AI embeddings \"have gotten much better in important ways,\" Beykpour told Business Insider in a recent interview.\n\nIf Particle reimagines news, Luvu reinvents personal training using generative AI.\n\nLaunched in August 2025 by CEO Alexis Sursock and CTO Creston Brooks, the AI-powered fitness app has already attracted about 250,000 users. The app features an AI \"marshmallow\" that acts like a personal trainer, sending highly personalized notifications and real-time feedback.\n\n\"The key is the personalization, which is powered by AI models and wouldn't have been possible before this technology appeared in recent years,\" Brooks said.\n\nInstead of generic reminders — \"It's time for your workout\" — Luvu tailors messages. If a user logged that they had a test yesterday, the app might follow up with, \"Your test is over. Time to work out!\"\n\nThe results are striking. Luvu's notification click rate is four times that of typical non-personalized prompts. In an industry where only 2% to 3% of users remain active after 30 days, Brooks said, Luvu claims retention rates that are two to three times higher.\n\nThe app offers three motivational styles: supportive, neutral, or \"meaner marshmallow.\" Behind the scenes, Luvu also uses AI for granular, one-to-one messaging crafted by LLMs.\n\nThe company is also experimenting with reinforcement learning with verified rewards, a relatively new technique for training and improving AI models.\n\nUsers can prop their phones against a surface and record themselves exercising; the app uses computer-vision models to verify whether squats or other moves are performed correctly, offering real-time corrections like \"Straighten your knees.\" These verified signals feed back into the system, helping train what Brooks envisions as a future \"super-motivator\" model.\n\nThis isn't just a chatbot layered on top of a fitness tracker. It's a feedback loop between human behavior and AI, something that couldn't easily exist before the advent of modern models.\n\nFor Fai Nur, CEO and cofounder of AI-powered social simulation game Status, the Second Wave is about imagination.\n\n\"Status could not have existed before LLMs,\" she said.\n\nThe app, which has surpassed 3 million downloads, lets users role-play in AI-generated social media worlds. Think The Sims, though played out as a living, breathing social feed.\n\nUsers can cast themselves as anything they can imagine, such as Hogwarts students, soccer stars, or characters from \"Stranger Things.\" Post an update, and AI-generated characters instantly reply. Events unfold dynamically: miss a penalty kick, and face the backlash. An AI system assigns an \"aura score\" to grade responses and level players up or down.\n\nIn many enterprise settings, the non-deterministic nature of LLM outputs is a liability. Generative AI models sometimes respond to the same prompt in different ways, which doesn't lend itself to applications that require strict accuracy.\n\nIn gaming, this can be an asset because each new AI-generated response can be new, creating a richer, more varied experience.\n\n\"You haven't been able to role-play like this until now,\" Nur said.\n\nBefore LLMs, creating immersive fandom worlds required persuading other humans to participate. Now, entire social universes spin up instantly.\n\nFor Gibbs and other proponents of AI's Second Wave, that's the point. The technology's future won't be defined by incremental cost savings, but by products that feel native to AI — experiences that surprise, motivate, inform, and entertain at consumer scale.\n\nIf the first wave made businesses leaner, the second may make everyday life stranger, richer, and more interactive — and, crucially, something people will pay for.\n\n Reach out to me via email at abarr@businessinsider.com. Note: Axel Springer, the parent company of Business Insider, is an investor in Particle.",
    "readingTime": 6,
    "keywords": [
      "ai's second",
      "second wave",
      "couldn't exist",
      "for gibbs",
      "generative ai",
      "business insider",
      "users",
      "models",
      "products",
      "experiences"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-second-wave-redefines-startups-new-products-2026-2",
    "thumbnail_url": "https://i.insider.com/698f9459a645d11881895bf1?width=1200&format=jpeg",
    "created_at": "2026-02-17T01:11:09.352Z",
    "topic": "finance"
  },
  {
    "slug": "ireland-opens-probe-into-musks-grok-ai-over-sexualised-images",
    "title": "Ireland opens probe into Musk’s Grok AI over sexualised images",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/ireland-opens-probe-into-musks-grok-ai-over-sexualised-images-4507811",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1G009_L.jpg",
    "created_at": "2026-02-17T01:11:08.733Z",
    "topic": "finance"
  },
  {
    "slug": "beadhub-allow-coding-agents-to-claim-work-chat-and-coordinate-across-machines",
    "title": "BeadHub: Allow coding agents to claim work, chat, and coordinate across machines",
    "description": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I’ve spent the past several months building a tool to address that.\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans—across machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\nBeads Around the time I wrote that article, I started using Steve Yegge’s beads, a git-native issue tracker designed for AI agents.",
    "fullText": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I’ve spent the past several months building a tool to address that.\n\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans—across machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\n\nAround the time I wrote that article, I started using Steve Yegge’s beads, a git-native issue tracker designed for AI agents. Your agent runs bd create \"Fix the login redirect bug\" and it appends a JSON line to .beads/issues.jsonl, right in the repository. Issues travel with the code. When you push a branch, the issues come along.\n\nYegge calls it the “50 First Dates” problem: agents wake up every session with no memory of yesterday’s work. Beads fixes that. An agent reads the issue list and knows where things stand. My agents got much more done.\n\nWhich meant more agents, more worktrees, more parallel work—and the coordination problem became even more acute. Two agents modify the same file. One refactors a function while another adds to it. An agent picks up a task already in progress in a different worktree. Nobody knows who’s working on what.\n\nBut beads is also the right scaffolding for coordination. If everyone in a team uses beads, all agents share a picture of what needs doing. Beads gives agents something useful to talk about; BeadHub gives them a way to talk.\n\nThe major platforms are moving in this direction. Anthropic just shipped Agent Teams in Claude Code: a lead session that spawns independent teammates who communicate directly and self-coordinate. OpenAI’s Codex app runs parallel agent threads in isolated worktrees.\n\nYegge built Gas Town on top of beads to tackle the single-machine case: a “Mayor” agent orchestrates dozens of coding agents, tracks work in convoys, and persists state so agents can pick up where they left off.\n\nThese are real steps forward, but they’re solving a specific version of the problem: multiple agents for one programmer, on one machine, within one tool.\n\nThe version I am interested in is Maria in Buenos Aires running a frontend agent while Juan in San Francisco runs a backend agent, and they need their agents to not destroy each other’s work, and to figure out how to work together.\n\nBeadHub is a server that agents connect to through bdh, a wrapper around the beads bd command. When an agent runs any bdh command it registers with the server. The server tracks which agents are online across the project—what machine they’re on, what branch, what files they’re touching.\n\nCommunication. Agents can send each other mail (async, fire-and-forget) or chat (sync, block-until-reply). With mail an agent finishes a task and drops a note: “Done with bd-42, tests passing.” Chat is for when agents need to think together: “I’m adding a role field to the user model—will that break your permission checks?” / “It will, but the fix is small. Go ahead and I’ll update my side.”\n\nClaims. When an agent marks a bead as in-progress, that claim is immediately visible to every other agent in the project, regardless of whose machine they’re on. If another agent tries to claim the same bead, it gets rejected with a message telling it who has it.\n\nFile reservations. When an agent modifies a file, the server records an advisory lock. Other agents see a warning if they touch the same file. Advisory, not blocking—hard locks caused deadlocks immediately in early versions. Agent A locks file X, agent B locks file Y, both need the other’s file. Warnings work better. Agents are cooperative; they just need information.\n\nEscalation. An agent runs bdh :escalate with a description of what it’s stuck on and a human gets notified with full context. Without this, agents either fail silently or spin retrying things that need human judgment.\n\nThe multi-machine part is where it comes together. BeadHub recognizes Maria’s and Juan’s clones as the same repo. Maria’s agents and Juan’s agents see each other’s claims, locks, and messages. If Maria’s frontend agent reserves src/components/Auth.tsx, Juan’s backend agent sees the warning even though they’re in different cities on different machines.\n\nA project can span multiple repositories. The frontend repo agents can message the backend repo agents. A bead in the frontend can be marked as blocked by a bead in the backend.\n\nYou can see what this looks like in practice on the BeadHub project’s own dashboard, where we coordinate BeadHub’s development using BeadHub. Make sure to check the chat page, it is almost magical to see them figuring things out.\n\nA few things I got wrong before getting them right.\n\nThe client is the source of truth. My instinct was to make the server authoritative. But agents work locally, in git repos, and their local state is the ground truth. The server aggregates and distributes. If the server and the client disagree, the client wins. If the server goes down, bdh falls back to local bd with a warning. Work continues. Coordination catches up later.\n\nAsync by default. My first instinct was real-time negotiation between agents. Doesn’t scale. Agents work at different speeds, on different schedules, and blocking one while waiting for another is expensive. Mail is the default. Chat is the exception.\n\nAdvisory over mandatory. Advisory file locks that warn instead of block. Bead claims that can be overridden with --:jump-in \"reason\" (which notifies the other agent). The system provides information and trusts agents to act on it.\n\nThe coordinator role. I assign one agent per project the “coordinator” role. The coordinator doesn’t write code. It watches the dashboard, assigns work, checks on progress, nudges stuck agents, and keeps the end goal in sight. The implementer agents are heads-down in their worktrees; the coordinator is the one who knows what the project needs next. BeadHub serves each agent a role-specific policy—markdown documents describing how agents in that role should behave—and the coordinator’s policy is fundamentally different from an implementer’s. This turned out to matter more than any of the technical decisions.\n\nThe single-machine problem is getting solved. Agent Teams, Codex—within a few weeks, running multiple agents in parallel on your laptop will be table stakes.\n\nThe multi-programmer problem is next. Five engineers, fifty agents, three repositories, two time zones. That’s where the coordination problem changes in kind, not just degree. It’s not enough that your agents can talk to each other. They need to talk to your teammate’s agents, on a different machine, in a different time zone, working on a different repo in the same project.\n\nBeadHub is open source and free for open-source projects.",
    "readingTime": 6,
    "keywords": [
      "together beadhub",
      "coordinator role",
      "machine they’re",
      "locks file",
      "frontend agent",
      "backend agent",
      "repo agents",
      "server",
      "beads",
      "coordination"
    ],
    "qualityScore": 1,
    "link": "https://juanreyero.com/article/ai/beadhub",
    "thumbnail_url": "https://juanreyero.com/img/default-og.jpg",
    "created_at": "2026-02-16T18:30:07.341Z",
    "topic": "tech"
  },
  {
    "slug": "memory-plugin-for-claude-code",
    "title": "Memory Plugin for Claude Code",
    "description": "A Markdown-first memory system, a standalone library for any AI agent. Inspired by OpenClaw. - zilliztech/memsearch",
    "fullText": "Skip to content\n\n You signed in with another tab or window. Reload to refresh your session.\n You signed out in another tab or window. Reload to refresh your session.\n You switched accounts on another tab or window. Reload to refresh your session.\n\nDismiss alert\n\n zilliztech\n\n /\n\n memsearch\n\n Public\n\n You can’t perform that action at this time.",
    "readingTime": 1,
    "keywords": [
      "window reload",
      "another tab",
      "refresh",
      "session",
      "signed"
    ],
    "qualityScore": 0.3,
    "link": "https://github.com/zilliztech/memsearch/tree/main/ccplugin",
    "thumbnail_url": "https://opengraph.githubassets.com/cf6f8849c03191744f1dddc3af6be801b4d830f81204248354020fd12266a381/zilliztech/memsearch",
    "created_at": "2026-02-16T18:30:06.450Z",
    "topic": "tech"
  },
  {
    "slug": "natwest-hails-progress-after-12b-spent-on-tech-last-year-but-true-ai",
    "title": "NatWest hails progress after £1.2B spent on tech last year, but true AI",
    "description": "Bank described the last 12 months of its tech transformation as ‘the year of [AI] deployment at scale.’",
    "fullText": "NatWest bank invested £1.2bn into its information technology transformation in 2025 and saw huge productivity gains as a result, but this year will see artificial intelligence (AI) become truly transformative for customers and staff.\n\nThe bank said it has already freed up £100m in funds as a result of simplification and the use of the cloud. AI is at the centre of the bank’s plans, with 2025 seeing the technology deployed across the company “at scale”.\n\nHeadline figures for 2025 saw the bank’s software engineers generate 35% of its code through AI software development tools, all 60,000 staff given access to AI productivity software, and thousands of human hours saved. Last year, the bank also embarked on a major collaboration with AI supplier OpenAI.\n\nOther tech achievements in 2025 included the hiring of 1,000 software developers, 100 new features developed on its retail banking app, the appointment of its first chief AI research officer and the establishment of its AI research office.\n\nIn a blog post, NatWest Group CIO Scott Marcar said this year will see the bank take advantage of the AI “building blocks” deployed last year.\n\n“The only certainty is that how customers bank will look very different in the future,” he said. “That’s why being closer to them, with insight and trust, matters more than ever. As technology reshapes how people live, work and bank, we’ve put in place the building blocks to understand, respond to and serve customers’ fast evolving needs,” wrote Marcar.\n\n“Last year brought AI deployment at scale across NatWest, and as we move into 2026, the transformative benefits are becoming more of a reality for our customers and colleagues – delivering growth, greater productivity, and, most importantly, deepening relationships – so that we can be a trusted partner for tomorrow’s banking,” he added.\n\nFrom a staff perspective, all staff now have access to AI tools including Microsoft Copilot Chat and the bank’s internal large language model (LLM), with more than half reported to have taken additional training.\n\nAccording to the bank, more than 70,000 hours were saved through automated AI call summaries in its retail business, while relationship managers in its wealth business were able to spend 30% more time on customer conversations by using AI. According to NatWest, through agentic and voice AI, customers will receive “more intuitive, personalised and seamless interactions” this year.\n\nIn the next few months, 25,000 NatWest customers will have access to its agentic financial assistant within Cora, its customer-facing agentic AI assistant. “Underpinned by OpenAI models, customers will be able to ask natural language questions about their recent spending, in their own words, on their app,” said the bank.\n\nThe bank will then experiment with voice-to-voice AI capability, which aims to provide “human-like empathy, tone and inflection”.\n\nAs part of its wider multi-year digital transformation, NatWest has added around 6,000 tech staff since 2021. In 2025 alone, it recruited 1,000 software engineers through its India Hub in Bengaluru. Its chief AI researcher, Maja Pantic, is working with AI in areas such as audiovisual conversational AI, multi-biometrics and proprietary small language models.\n\nMarcar wrote: “We can’t underestimate the scale of the work we have done to date to rebuild our technology foundations to make us faster, safer and more resilient. A scalable, modular tech stack now underpins how we deliver new products and services, how we integrate with partners, and how we provide the protection and operational resilience customers expect.”\n\nHe said the bank has been moving away from legacy systems in an “inside-out” transformation. It has also created a single, connected view of each customer. “We can anticipate needs faster, remove friction from everyday banking and make onboarding more seamless,” he wrote on his blog post.\n\nMarcar stressed that the proliferation of AI will support human workers, adding: “It’s a future where the expertise of our colleagues is augmented by the intelligence and ease of modern technology.”",
    "readingTime": 4,
    "keywords": [
      "software engineers",
      "bank",
      "customers",
      "natwest",
      "technology",
      "staff",
      "transformation",
      "productivity",
      "bank’s",
      "scale"
    ],
    "qualityScore": 1,
    "link": "https://www.computerweekly.com/news/366639140/NatWest-hails-progress-after-12bn-spent-on-tech-last-year-but-true-AI-transformation-to-come",
    "thumbnail_url": "https://www.computerweekly.com/visuals/ComputerWeekly/HeroImages/NatWest-Bank-Editorial-Use-Only-Shawn-adobe.jpg",
    "created_at": "2026-02-16T18:30:06.218Z",
    "topic": "tech"
  },
  {
    "slug": "the-long-tail-of-llmassisted-decompilation",
    "title": "The Long Tail of LLM-Assisted Decompilation",
    "description": "After rapid advances thanks to one-shot decompilation, progress on the Snowboard Kids 2 decompilation began to falter. This post explores the workflow evolution, tooling improvements, and fundamental LLM limits that emerged when tackling the long tail of increasingly difficult functions.",
    "fullText": "In my previous posts, I described how coding agents could be used to decompile Nintendo 64 games and that one-shot decompilation was very effective. That approach allowed me to make rapid progress on the Snowboard Kids 2 decompilation, with the percentage of matched code quickly growing from around 25% to 58%.\n\nAfter that, progress slowed dramatically, requiring me to significantly alter my workflow. With those changes, I pushed the decompilation into the ~75% range before stalling out again, this time perhaps for good, though I would love to be proved wrong.\n\nThis post describes how my workflow has evolved as the project matured, what helped, and where I’m currently stuck. My hope is that these observations will be useful for other decompilation projects.\n\nDecompilation attempts take time and tokens, so the choice of which unmatched functions to work on matters a great deal. My original approach prioritised functions based on estimated difficulty. A logistic regression model ranked candidates using features like instruction count and control-flow complexity, and Claude would always attempt the ’easiest’ remaining function. That worked remarkably well early on, but it eventually ran out of steam. At some point, everything left was hard. Reordering the queue didn’t magically make those functions easier.\n\nAt the same time, Macabeus was exploring function similarity via text embeddings of assembly instructions, which then allowed querying for nearby functions in the high-dimensional latent space. This seemed promising. Claude’s output already hinted that it could recognise similar functions and reuse patterns across them. The intuition here is that decompiled functions provide a useful reference to Claude for how particular blocks of assembly can be mapped to C code.\n\nTo test this out, I wrote a tool to compute similar matched functions given an unmatched function and adjusted the agent loop to prioritise functions with similar (matched) counterparts. This approach proved highly effective. There were indeed many similar functions that Claude hadn’t previously been able to identify, and these proved invaluable for helping guide its decompilation attempts.\n\nVector embeddings are just one way of computing function similarity. They are great for fast retrieval across huge corpora, which is one reason they’re common in RAG systems. But I only had a few thousand candidates, and queries weren’t time-sensitive. Computing exact similarity between every pair of candidates is not only feasible but preferable, given how much time and tokens are already invested in each attempt.\n\nMy first attempt was to build a composite similarity score by hand. I combined:\n\nIn hindsight, this was probably overcomplicated. There is already a tool that does something very similar: Coddog. Instead of feature engineering, it computes a bounded Levenshtein distance directly over opcode sequences, with aggressive early exits when similarity is impossible. The result is normalised to a similarity score between 0 and 1.\n\nOn the remaining unmatched functions, Coddog and my own approach select different most-similar candidates in 90.6% of cases. I still use both. They were not evaluated on identical sets of functions, so it is difficult to say whether one is strictly better or whether they are simply complementary. Anecdotally, though, the simpler approach performs at least as well as my more elaborate one.\n\nSpecialised tooling can make a big difference to Claude’s performance. The project uses a number of Claude skills but two were particularly notable: gfxdis.f3dex2 and decomp-permuter.\n\nThe N64 has a dedicated graphics chip, the Reality Display Processor (RDP). Games execute microcode on the RDP to render graphics on the screen.\n\nGames have considerable flexibility in how they use the RDP, but most opt for an off-the-shelf library provided by Nintendo. If your game doesn’t do this, you need to reverse engineer a company’s idiosyncratic microcode in addition to the game itself. Thankfully, Snowboard Kids 2 opted for a Nintendo library, specifically F3Dex2.\n\nAfter loading their desired microcode library, games send instructions to the RDP via display lists. Conceptually, display lists are just arrays of bytes representing microcode instructions, but they’re a headache for decompilers. Games often build them dynamically using macros that may invoke other macros or perform complex bit arithmetic. The compiler then optimises and reorganises this logic, making it difficult to discern what the original developers actually wrote.\n\nAgents are smart, but this is a highly domain-specific and context-specific scenario. It’s a clear use case for a Claude skill.1 I provided Claude with a reference for F3Dex2 commands, a tool to disassemble hex values into specific commands (gfxdis.f3dex2), and some strategies for handling more specific edge cases such as aggregate commands. Unsurprisingly, this made Claude far more effective at recognising and decompiling F3Dex2 code.\n\nClaude is slow and deliberate. Turning a 99.9% match into 100% can involve thousands of tiny variations in control flow, temporaries, or expression ordering. A permuter is the opposite. It blindly tries millions of small mutations in the hope that one of them produces a perfect match.\n\nIn theory, this should complement an LLM nicely. Claude does the structured reasoning, the permuter brute-forces the final few percent. The skill enforced this split by allowing the permuter to run only once a function was already more than 95% matched.\n\nPermuters happily introduce strange code: illogical variable reuse, do {} while (0) loops, nested assignments. Sometimes these changes work. Often they do not. Worse, they optimise for incremental improvements to the match percentage rather than for correctness. A small reordering might delete a function call or subtly change register allocation in a way that improves the match. But if that call existed in the original, you will have to restore it eventually. You are not actually closer to a clean match. You have just moved the compiler into a more convenient shape.\n\nClaude, unfortunately, tended to treat these artefacts as signal. It would start optimising around permuter-induced noise, leading to doom loops and token burn with little real progress.\n\nAfter a few attempts to rein this in, I removed the permuter entirely. The occasional win did not justify the cleanup cost or the instability it introduced. It also made manual intervention harder, since the codebase would drift into awkward, overfitted forms that no human would willingly write.\n\nCleaning up and documenting code doesn’t directly improve the match rate but it can help reach previously unmatchable functions. Many of the earlier functions (particularly those done by Claude) were quite brittle. They technically matched, but relied on pointer arithmetic, awkward temporaries, or control flow no human would willingly write. Those matches worked, but they were poor references when an unmatched function was later identified as similar to them.\n\nCleaner, more idiomatic matches make better examples once similarity-based scheduling kicks in. If a function really should be using array indexing instead of pointer math, fixing that improves the signal Claude sees when attempting related code.\n\nSometimes this cleanup was done by hand but Claude was also reasonably good at cleaning up its own work. Claude was run in a loop, similar to the technique used for one-shot decompilation, where it was tasked with making changes to one individual function at a time.\n\nThis was another area where the right skills made a difference. In a decompilation project, even renaming a global variable can involve multiple steps. This also turned out to be a great way to document the structure of the project, since writing down how everything worked was already necessary for Claude’s benefit.\n\nAs a side effect, this work turned up some genuinely fun discoveries. While documenting the cheat code system, I stumbled across a previously unknown cheat code. That alone justified the detour.\n\nThe ongoing decompilation work plus the branching into other non-decompilation tasks presented numerous challenges in terms of resources, project stability, and task orchestration.\n\nFour changes helped me keep the workflow scaling:\n\nThese will be discussed in turn.\n\nThere are multiple tasks that we need to perform. Worktrees are the recommended way to run multiple agents on a single codebase. Agents need their own version of the codebase to work with, or we risk conflicting changes, errors, and so on.\n\nToday I run agents across three separate worktrees in addition to the main branch, where I do human stuff.\n\nGreater automation of the decompilation and documentation work also increased the possibility of Claude creating and committing mistakes. The unsupervised nature of the work means these can lie undetected for hours, potentially invalidating all the intervening work that has been done.\n\nIn one particularly amusing case, Claude couldn’t get a function to match, so it updated the SHA1 hash that was used for comparison between the compiled artefact and the original ROM. All work done after that point had to be reverted.\n\nHooks proved invaluable for preventing this behaviour and guiding the agent. Hooks allow us to run code before the agent takes a specific action, for example when editing a file. I’ve found them incredibly useful. You can find the full list of hooks here. Currently, I use hooks to:\n\nHooks have significantly reduced the frequency with which Claude attempts misguided or destructive actions, though they are not perfect. Claude can be very persistent when it really wants to do something. I’ve seen Claude run the contents of a make command when make itself is blocked, or write a Python script to edit a file it’s been told it can’t edit. But hooks at least offer better enforcement than prompting alone.\n\nDifferent kinds of long-running agent loops have become essential to my workflow. The increased use of long-running tasks also required a more robust solution than my old run.py script. I decided to split my old run.py script (now Nigel) into its own repo.\n\nNigel reflects the immediate needs of the decompilation project but might be useful more generally. In Nigel, tasks are expressed via configuration: it’s easy to experiment with new ideas by copying an existing task and tweaking it. In your configuration file, you need to specify a ‘candidate source’ (input to the task) and a prompt (which can optionally be a separate template file).\n\nHere’s an example from my recent attempts to remove hard-coded hex addresses in main.c:\n\nNigel will automatically discover scripts (uniquely identified by name) and can run them with proper handling to ensure the same input isn’t handled twice, good changes are committed, failures are handled gracefully, etc.\n\nSome of my favourite Nigel features are:\n\nIt’s hard to discuss Claude workflows without mentioning Ralph Wiggum. Like Ralph, Nigel can repeatedly prompt Claude with the same task via --repeat until it succeeds. The difference is that Nigel operates within structured workflows and batch jobs. Tasks generate candidates and consume them one at a time, whereas Ralph simply replays the same prompt.\n\nMy initial prompt capped the number of attempts at 30 to preserve tokens, which may have been conservative.\n\nI experimented with relaxing this limit and enabling --repeat 3. A small number of functions exceeded the previous 30-attempt cap. One required 87 attempts before Claude finally succeeded.\n\nIn practice, higher --repeat values do help, but only at the extreme tail and at considerable token cost.\nThe 85th percentile of successful attempts remains 28 attempts, meaning most functions complete within the original limit. For now, I’ve removed --repeat 3 while leaving the number of attempts within a single prompt uncapped. That preserves headroom for rare outliers without multiplying token usage across the entire workload.\n\nWork on the remaining unmatched functions required more attempts, more intermediate output, and more refactoring passes. An unattended Opus task could burn through the Claude 20x Max plan in a matter of days. The new cleanup and documentation loops only added to the pressure on a finite token budget.\n\nGLM, an open-weight model from z.ai, is generally considered less capable than Opus. But it’s dramatically cheaper, offers generous token limits, and can act as a drop-in replacement for most of my workflows.\n\nThus glaude was born: a thin wrapper that looks like Claude but quietly points at a GLM backend.\n\nI usually try glaude first, or reach for it when I know the task is mechanical. Cleanup passes, refactors, documentation loops: none of these really need frontier reasoning. I’d rather preserve Opus tokens for the genuinely difficult work. It’s not perfect. Opus has cracked problems GLM couldn’t. But it lets me run agents without constantly worrying about weekly quotas, which makes the whole system far more sustainable.\n\nAfter all that engineering (similarity scoring, skills, hooks, orchestration, model routing), the curve ultimately flattened in early January. At that point, 157 functions remained. With continued work, that’s now down to 124, but the dynamic has fundamentally changed.\n\nNigel the cat is still as busy as ever. There’s still work to be done, but matching functions has become much harder. At least until the next wave of frontier models is released.\n\nIf you’ve made it this far, you probably have an interest in decompilation and Snowboard Kids 2. Check out the Snowboard Kids 2 decompilation project, and please reach out on Discord if you’d like to help.\n\nYou can also follow me on Bluesky for more Snowboard Kids 2 updates.\n\nI’ve gone back and forth between treating this as a Claude skill vs making it directly part of the CLAUDE.md for the decomp environment. As I was writing this blog post though, it did seem a little embarrassing not making it a skill, so I changed it back. 😶‍🌫️ ↩︎",
    "readingTime": 12,
    "keywords": [
      "display lists",
      "run.py script",
      "proved invaluable",
      "kids decompilation",
      "similarity score",
      "documentation loops",
      "claude skill",
      "cheat code",
      "one-shot decompilation",
      "unmatched function"
    ],
    "qualityScore": 1,
    "link": "https://blog.chrislewis.au/the-long-tail-of-llm-assisted-decompilation/",
    "thumbnail_url": "http://blog.chrislewis.au/function-embeddings-header.jpg",
    "created_at": "2026-02-16T18:30:06.066Z",
    "topic": "tech"
  },
  {
    "slug": "ai-is-transforming-science-more-researchers-need-access-to-these-powerful-tools-for-discovery",
    "title": "AI is transforming science – more researchers need access to these powerful tools for discovery",
    "description": "Five years ago, our AlphaFold AI system solved the 50-year grand challenge of protein structure prediction. But that's not the whole story.",
    "fullText": "Sir Demis Hassabis is Co-Founder and CEO of Google DeepMind. He has won many prestigious international awards for his research work including the 2025 Nobel Prize in Chemistry for protein structure prediction.\n\nAs SVP for Research, Labs, Technology & Society, James Manyika focuses on advancing Google and Alphabet’s most ambitious innovations in AI, computing and science and on areas with potential for beneficial impact on society. James served as Vice Chair of the US National AI Advisory Committee and Co-Chair of the UN Secretary-General’s AI Advisory Body.",
    "readingTime": 1,
    "keywords": [
      "society james",
      "research",
      "advisory",
      "google"
    ],
    "qualityScore": 0.45,
    "link": "https://fortune.com/2026/02/16/google-deepmind-ceo-demis-hassabis-james-manyika-transforming-sciecne-alphafold/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/02/demis-hassabis.png?resize=1200,600",
    "created_at": "2026-02-16T18:30:01.619Z",
    "topic": "business"
  },
  {
    "slug": "mark-cuban-predicted-an-army-of-young-people-would-have-to-spread-ai-and-tech-gurus-agree",
    "title": "Mark Cuban predicted an army of young people would have to spread AI — and tech gurus agree",
    "description": "Tech billionaire Mark Cuban anticipated a huge jobs boom for young people to implement AI at companies. AI gurus say he's right.",
    "fullText": "Mark Cuban expects legions of workers will be needed to implement AI at companies, creating a huge opportunity for tech-savvy young people.\n\nThe tech billionaire and former \"Shark Tank\" investor made the prediction during an August interview with TBPN, a tech talk show and podcast.\n\nAI guru Rohan Paul shared a clip of Cuban's comments over the weekend, which was widely reposted; Cuban himself shared three responses from other AI gurus on his X feed.\n\nOne declared it the \"MOST underrated clip on the internet right now.\" Another drew a parallel to Salesforce and the millions of administrative and integration roles it spawned. A third heralded a shift from generic software to customized intelligence.\n\nCuban is calling the IT services boom of the 2000s, but for intelligence instead of infrastructure. Every wave of business technology from PCs to cloud to mobile spawned a massive local services layer. The AI wave needs the same thing, and 33 million companies are waiting. \n\nThe… https://t.co/sgBXH1VJLd\n\nCuban told TBPN that when he was 24, he would walk into companies and executives would point to their secretaries and receptionists and say they didn't need a PC. Cuban recognized that as an opportunity to sell old-school bosses on the benefits of computers and teach them how to use them.\n\nHe said it's a similar situation with the latest tech wave, which some believe will render millions of human workers obsolete and trigger mass unemployment.\n\nCuban said he advises high-school and college students to not just \"learn all you can about AI, but learn more on how to implement them in companies.\"\n\nCuban, a minority owner of the Dallas Mavericks, said that tens of millions of US companies don't have AI budgets or AI experts.\n\n\"This is where kids getting hired coming out of college are really going to have a unique opportunity,\" he said. They should spend their free time learning how to use different AI tools, make AI videos, and customize AI models so they can teach business leaders in any industry how to harness the tech, he added.\n\n\"That is every single job that's going to be available for kids coming out of school because every single company needs that,\" Cuban said. \"There is nothing intuitive for a company to integrate AI and that's what people don't understand.\"\n\nCuban emphasized the opportunity isn't limited to software engineers. Many older workers are \"afraid\" to ask complex questions to AI models, he said, unlike \"kids coming out of school today that are fearless in the questions they ask and the followups and their ability to prompt.\"\n\n\"That's jobs for everybody,\" he added.",
    "readingTime": 3,
    "keywords": [
      "opportunity",
      "tech",
      "workers",
      "millions",
      "wave",
      "kids",
      "that's",
      "cuban",
      "implement",
      "tbpn"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mark-cuban-ai-skills-tech-young-people-jobs-implement-opportunity-2026-2",
    "thumbnail_url": "https://i.insider.com/6993104cd3c7faef0ece57fc?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:01.231Z",
    "topic": "finance"
  },
  {
    "slug": "bytedance-says-its-going-to-make-it-harder-for-seedance-to-make-ai-videos-of-copyrighted-movie-characters",
    "title": "ByteDance says it's going to make it harder for Seedance to make AI videos of copyrighted movie characters",
    "description": "ByteDance is facing scrutiny over Seedance 2.0, an AI video tool creating Hollywood star deepfakes and sparking copyright concerns.",
    "fullText": "Last week, an AI-generated video of fake Tom Cruise duking it out with fake Brad Pitt on a rooftop freaked the internet out. Now, the Chinese tech giant behind the AI tool says it's going to take measures to improve copyright-related safeguards.\n\nIn a statement shared with Business Insider, ByteDance said it's going to \"strengthen safeguards\" on Seedance 2.0.\n\nOn Friday, Disney sent ByteDance a cease-and-desist letter, accusing the Chinese company of \"hijacking Disney's characters by reproducing, distributing, and creating derivative works featuring those characters.\"\n\nIn the statement to Business Insider, a ByteDance spokesperson said the company \"respects intellectual property rights\" and that it has \"heard the concerns regarding Seedance 2.0.\"\n\n\"We are taking steps to strengthen current safeguards as we work to prevent the unauthorized use of intellectual property and likeness by users,\" the spokesperson said in comments first reported by the BBC.\n\nThe company did not provide further details on the safeguards it's planning to introduce.\n\nByteDance, which is also the parent company behind TikTok, launched Seedance 2.0 in early February. Its ability to generate realistic, multi-shot video sequences has prompted pushback from Hollywood over concerns about AI's impact on entertainment jobs.\n\nCharles Rivkin, the chairman and CEO of the Motion Picture Association, accused Seedance 2.0 of engaging in \"unauthorized US copyrighted works on a massive scale.\"\n\n\"By launching a service that operates without meaningful safeguards against infringement, ByteDance is disregarding well-established copyright law that protects the rights of creators and underpins millions of American jobs,\" Rivkin said in a statement last week.\n\nThe AI-generated video depicting Pitt and Cruise fighting on a rooftop quickly went viral last week, with many online commenting on how realistic the clip is.\n\nThe company also generated buzz with AI videos of Marvel's Wolverine fighting Thanos, and a lightsaber duel between Star Wars characters Anakin Skywalker and Rey. Both franchises are owned by Disney.\n\nWhile Disney has warned ByteDance to stop using its intellectual property, it signed a three-year licensing deal with OpenAI in December, giving users of its video-generation tool Sora access to 200 Disney characters.",
    "readingTime": 2,
    "keywords": [
      "business insider",
      "insider bytedance",
      "intellectual property",
      "safeguards",
      "characters",
      "it's",
      "statement",
      "fake",
      "rooftop",
      "behind"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/bytedance-seedance-2-safeguards-copyright-disney-ai-video-tool-2026-2",
    "thumbnail_url": "https://i.insider.com/699301c5d3c7faef0ece57ce?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:01.048Z",
    "topic": "finance"
  },
  {
    "slug": "inside-the-career-rise-of-sundar-pichai-google-and-alphabets-current-ceo",
    "title": "Inside the career rise of Sundar Pichai, Google and Alphabet's current CEO",
    "description": "Meet Sundar Pichai, the man leading Google and Alphabet as CEO, who is leading the search giant through the AI race.",
    "fullText": "Sundar Pichai has had a meteoric rise since joining Google as a 31-year-old product manager in 2004.\n\nIn the 11 years that followed his first steps on the Googleplex, Pichai was promoted four times, eventually becoming the CEO of Google in 2015.\n\nIn that role, he was responsible for the company's core businesses and cash cow — and did a good enough job that, in December 2019, he was promoted one more time, replacing Google cofounder Larry Page as the CEO of Alphabet, Google's parent company.\n\nSince then, he has led the almost-$2-trillion company through the pandemic, layoffs, and the AI renaissance that's taken Silicon Valley by storm.\n\nSo, who is Pichai, and how did he scale the ranks to get one of the most important jobs at one of the most important companies in the world? Here's a look at his life and career.\n\nPichai, whose full name is actually Pichai Sundararajan, grew up in Chennai, India.\n\nPichai's father was an electrical engineer, and his mother worked as a stenographer before having him and his younger brother. The family wasn't wealthy, and the boys slept together in the living room of their two-room apartment.\n\nEarly on, Pichai's family realized he had a talent for remembering numbers after noticing he could recall every phone number he had ever dialed on their rotary phone. He has been known to sometimes show off his memorization skills at meetings, Bloomberg said in 2014.\n\nAfter becoming interested in computers — the first software program he wrote was a chess game — Pichai studied engineering at the Indian Institute of Technology in Kharagpur. His success there won him a scholarship to Stanford University.\n\nPichai earned a master's degree from Stanford and later attended the University of Pennsylvania's Wharton School for his MBA.\n\nPichai has said that moving to California was a huge leap.\n\n\"I always loved technology growing up,\" Pichai said in a 2014 interview at Delhi University. \"I used to read about what was happening in Silicon Valley, and I wanted to be a part of it.\"\n\nWhen Pichai got to America in 1993, he couldn't believe how expensive everything was.\n\nHe \"was in an absolute state of shock\" about the price of a backpack — $60 — he told Bloomberg.\n\nHe also missed his girlfriend, Anjali. The two eventually married and now have a son, Kiran, and daughter, Kavya.\n\nBefore Google, he had stints at semiconductor manufacturer Applied Materials and consulting firm McKinsey.\n\nPichai had his first interview at Google on April Fools' Day in 2004 — the same day it launched Gmail. Pichai has said he initially thought the free email service was one of Google's famous pranks.\n\nPichai got his start working as a VP of product management, focused on Google's Toolbar, a web-search feature on Internet Explorer and Firefox.\n\nOne of his early achievements: convincing Google founders Larry Page and Sergey Brin that Google should build its own web browser.\n\nIn 2006, Microsoft created a \"doomsday\" scenario for Google by making Bing the new default search engine on Internet Explorer. To mitigate the effect of this change, Pichai helped convince Google execs to create its own browser, Google Chrome.\n\nChrome is now the world's most popular browser.\n\nAs a leader at Google, Pichai was known to be well-liked and focused on results, which resulted in more responsibility.\n\nPichai's \"substance over overt style\" approach was, in part, what led to Pichai taking over the Android division in 2013.\n\nHe spearheaded Android One, Google's push to \"make high-quality smartphones accessible to as many people as possible,\" and was also instrumental in ensuring Android was better integrated with Google.\n\nPichai was also behind Chrome OS, the operating system that powers Google's inexpensive Chromebook laptops, and was reportedly instrumental in helping put together Google's $3.2 billion acquisition of Nest in 2014.\n\nHis success garnered attention, and he was reportedly approached for a leadership role at Twitter.\n\nWhen Pichai turned down Twitter, he was rewarded for his allegiance, getting $50 million and a promotion.\n\nAs he rose through the ranks, Pichai became the right-hand man of Google cofounder and former CEO Larry Page.\n\n\"He's like the Aaron to Larry's Moses,\" a source told Business Insider in 2014, referring to the biblical prophet's brother.\n\nThat relationship and his success led to Pichai's next important promotion in late 2014 when Page put him in charge of the company's core products.\n\nAfter proving himself with Chrome and Android, Pichai added Google+, Maps, Search, commerce and ads, and infrastructure to his portfolio. The move cemented Pichai's move as Page's second-in-command.\n\n\"Sundar has a tremendous ability to see what's ahead and mobilize teams around the super important stuff,\" Page wrote in a memo announcing Pichai's promotion. \"We very much see eye-to-eye when it comes to product, which makes him the perfect fit for this role.\"\n\nWhen Alphabet was established as Google's parent company in 2015, Pichai was made CEO at Google, which encompassed search, YouTube, and Android.\n\nIn July 2017, Pichai was named to Alphabet's board of directors.\n\n\"Sundar has been doing a great job as Google's CEO, driving strong growth, partnerships, and tremendous product innovation. I really enjoy working with him, and I'm excited that he is joining the Alphabet board,\" Page said at the time.\n\nTwo years later came his final promotion at the company. Alphabet's CEO, Page, and president, Sergey Brin, announced that they were stepping down, and Pichai would become Alphabet's CEO.\n\nPage and Brin cofounded Google in 1998. They announced the change in a letter saying that Alphabet and Google \"no longer need two CEOs and a President.\"\n\nPichai earned a total of $226 million in 2022, with his pay spiking thanks to a multi-year stock award granted that year, making him one of America's best-paid CEOs.\n\nIn fiscal year 2024, Pichai earned $10.73 million in total compensation.\n\nPichai became a billionaire in 2025, according to the the Bloomberg Billionaires Index.\n\nThe top job at Alphabet also comes with increased public and internal scrutiny.\n\nIn 2018, the House Judiciary Committee grilled the CEO about Google's data privacy practices and plans with China.\n\nTwo years later, Pichai testified in front of Congress again over antitrust concerns. Two other major Google lawsuits were later filed by the US government over its alleged monopoly tactics.\n\nIn August 2024, a federal judge ruled against Google, finding the company had violated antitrust law to keep a monopoly on search.\n\nWhen penalties were announced in September 2025, Google was not forced to sell off its Chrome browser despite the Justice Department's request for that remedy. The judge ruled Google could no longer have exclusive search deals, and the company's stock jumped following the announcement.\n\nGoogle also dealt with internal turmoil after letting go of one of its top AI ethicists.\n\nIn December 2020, Google fired Timnit Gebru. Her exit came weeks after she was asked to retract a paper on the dangers of large language models and spoke out against the company's treatment of minority employees.\n\nGoogle employees were \"seriously pissed\" over how the firing was handled, one told BI at the time, and Gebru said that Pichai and other managers helped create \"hostile work environments.\"\n\nPichai eventually apologized for how the company dealt with it.\n\n\"I want to say how sorry I am for that, and I accept the responsibility of working to restore your trust,\" he wrote.\n\nAlso in 2020, Pichai was at the forefront of Google's response to the COVID-19 pandemic. Under his leadership, Google launched initiatives to help search users find accurate, useful information about the coronavirus.\n\nAnd like many large tech companies, Alphabet recruited rapidly at the start of the pandemic. Alphabet hired nearly 37,000 new workers in the 12 months leading up to October 2022.\n\nBut from late 2022, Pichai had to oversee an era of cost-cutting at the company.\n\nThat culminated in job losses in January 2023, when Google layoffs affected 12,000 employees or 6% of its global workforce. Pichai said he took \"full responsibility for the decisions that led us here.\"\n\nOver 1,400 Google employees wrote an open letter to Pichai about how the layoffs were handled.\n\n\"Don't be evil,\" it read, a reference to the company's original motto.\n\nGooglers also criticized Pichai's big payday in the face of the job cuts, accusing him of \"destroying morale and culture\" at Google.\n\nGoogle also laid off hundreds more workers in its central engineering division and hardware team in early 2024.\n\nJob cuts continued into 2025, with the company flattening its management layer and shedding roles in its Cloud unit. In February 2026, Business Insider reported Google was offering buyouts to staff in its business unit who aren't \"all in.\"\n\nPichai has also had to deal with European regulatory issues. French regulators hit Google with a roughly $270 million fine in March 2024, accusing the company of using news outlet articles to train its Gemini AI model.\n\nPichai has also pushed Google forward in the AI arms race that's preoccupying Silicon Valley.\n\nGoogle issued a \"code red\" in December 2022 after the launch of OpenAI's ChatGPT sparked concerns about the future of its search engine and whether chatbots might replace it. Pichai redirected resources to focus on building Google's AI products.\n\nIt wasn't the first time Pichai expressed interest in the technology, though. In 2016, Pichai announced that Google would be an \"AI-first\" company. Two years later, he said it's \"one of the most important things that humanity is working on\" and \"more profound\" than \"electricity or fire.\"\n\nGoogle's AI efforts have resulted in its own chatbot.\n\nIn December 2023, Google's Gemini launched. Gemini is a multimodal AI model that can process images, text, audio, video, and coding languages.\n\nPichai has also shifted Google's focus to integrating AI into its other products.\n\nAt the 2023 Google I/O conference, the CEO announced that Google would add AI features across Google Workspace, including in Search, Gmail, Docs, and other products.\n\nGoogle's traditional search function has also become an AI product, with AI overviews rolling out in 2024.\n\nAlphabet has continued to invest heavily in AI.\n\nOn an earnings call in February 2026, Google announced it planned to double its capital expenditure in 2026, with its spending expected to reach between $175 billion and $185 billion. Much of that was expected to go towards building out AI infrastructure, like chips and data centers.\n\nThe company also announced the Gemini app had over 750 million monthly active users, up 100 million from October.\n\nWhile Pichai is quite private, he is known to start his day with a cup of tea and an omelet — plus a copy of The Wall Street Journal.\n\n\"I read the physical paper every single morning,\" he told Recode in 2016, adding that he reads The New York Times online.\n\nThe Pichai's morning routine also includes scrolling through TechMeme, a niche tech news website that aggregates the latest stories in tech published by media outlets.\n\nAlthough he's private, Pichai has spoken out about certain causes since he became a public figure.\n\nIn 2015, he responded to then-presidential candidate Donald Trump's suggestion that Muslims be barred from immigrating to the US.\n\n\"Let's not let fear defeat our values. We must support Muslim and other minority communities in the US and around the world,\" he wrote.\n\nPichai was among the Big Tech executives who attended Trump's inauguration in 2025. Google also donated $1 million to Trump's inaugural committee.\n\nPichai is seen as something of a hero in his home country of India.\n\n\"You are what they would like to be, an Indian who studied here, went overseas, and did what everyone would dream of doing,\" interviewer Harsha Bhogle said in a conversation with Pichai for students at Delhi University.\n\nIn 2020, Pichai announced that Google would invest $10 billion into India's tech sector over the next five to seven years to make the internet \"affordable and useful\" to everyone living in the country.\n\nJillian D'Onfro, Avery Hartmans, and Mary Meisenzahl contributed to an earlier version of this article.",
    "readingTime": 11,
    "keywords": [
      "internet explorer",
      "alphabet's ceo",
      "google's parent",
      "judge ruled",
      "ceo page",
      "company's core",
      "google cofounder",
      "job cuts",
      "pichai earned",
      "search engine"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sundar-pichai-google-alphabet-ceo-career-life",
    "thumbnail_url": "https://i.insider.com/698fd035d3c7faef0ece515f?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:00.896Z",
    "topic": "finance"
  },
  {
    "slug": "nvidia-a-complete-guide-to-the-hardware-company-behind-the-ai-boom",
    "title": "Nvidia: A complete guide to the hardware company behind the AI boom",
    "description": "Nvidia is one of the world's most valuable companies. Read about its history, leadership, and financials.",
    "fullText": "Nvidia has been around for over three decades, but the chipmaker became a household name only in the past few years.\n\nIn October 2025, the AI chipmaker became the first company to hit a $5 trillion market cap.\n\nNvidia was founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem, \"with a vision to bring 3D graphics to the gaming and multimedia markets.\"\n\nThe boom in AI technology has made it one of the most valuable companies in the world as companies scramble to buy its graphics processing units. Here's what you need to know about Nvidia.\n\nNvidia's origin story began at a Denny's during a meeting between Huang — who once worked for the chain — Malachowsky, and Priem.\n\nPersonal computing was on the cusp of taking off, and the trio sought a way to capitalize on it. Huang said in a 2010 interview with Stanford University's engineering school that they \"wondered whether starting a graphics company would be a good idea.\"\n\n\"We brainstormed and fantasized about what kind of company it would be and the world we could help,\" he said. \"It was fun.\"\n\nTheir goal was to improve the experience of gaming on a PC.\n\nIn 2006, it released CUDA, a general-purpose programming interface that would expand its business far beyond gaming.\n\nOn Sequoia Capital's \"Crucible Moments\" podcast, Andrew Ng, a Stanford professor who founded Google Brain, recalled his students telling him, \"Hey, Andrew, there's this thing called CUDA — not that easy to program, but it's letting people use GPUs for something different.\"\n\n\"We started to see 10x or even 100x speedups training neural networks on GPUs because we could do 1,000 or 10,000 things in parallel rather than one step after another,\" he added.\n\nNvidia's GPUs were used to train AlexNet, an image classification system unveiled in 2012 that significantly influenced the field of deep learning.\n\nThe launch of ChatGPT in late 2022 ushered Nvidia into a new era. The chipmaker's shares surged by more than 1,000% from 2022 to early 2026.\n\nMuch of that growth came from the success of Nvidia's H100 chip, which it released in March 2022. The $40,000 chip, named for the computer scientist Grace Hopper, has played a crucial role in providing the computing power for large language models.\n\nSince the launch of Nvidia's Blackwell chips, which are twice as fast as its Hopper chips, customers including SoftBank, Amazon Web Services, and Microsoft have also flocked to the company.\n\nNvidia's success may be best personified by its CEO, Jensen Huang.\n\nA 61-year-old bona fide tech mogul, Huang has a net worth of about $165 billion, according to Forbes. While some execs sport chains or Patagonia vests, Huang is often spotted in a leather jacket. Business Insider identified at least six versions he's worn over the years, including a nearly $9,000 lizard-embossed coat from Tom Ford he wore at the company's global AI conference, GTC, in 2024. He commemorated Nvidia's stock price hitting $100 with a tattoo of Nvidia's logo on his arm.\n\nHuang's early years were tumultuous. He was born in Taiwan, and he spent time there and in Thailand before his parents sent him to the United States because of social unrest in the region.\n\nHe attended a reform school in Kentucky. He later moved to Oregon, where he was reunited with his parents. In high school, he became a nationally ranked table tennis champion.\n\nHuang graduated from Oregon State University with a degree in electrical engineering in 1984.\n\nDuring his freshman year, Huang met Lori Mills, his future wife. In an interview at the Hong Kong University of Science and Technology, he said he won her over by offering to help her with her homework. They married five years after meeting and now have two children.\n\nJensen Huang later earned a master's in electrical engineering from Stanford, and he worked at the chip companies LSI Logic and Advanced Micro Devices before launching Nvidia.\n\nHuang sold about 1.3 million shares of Nvidia when the company hit a $3 trillion market cap in June 2024, but he retains a more than 3% stake in the company.\n\nNvidia's business is built around GPUs, which can handle tasks simultaneously, as opposed to central processing units, or CPUs, which are in standard computers.\n\nNvidia's GPUs have become a mainstay of the AI revolution because they provide the computing power needed to run massive large language models like OpenAI's GPT-4 and Meta's Llama 3.\n\nDemand for Nvidia's H100 chips, built on its Hopper architecture, has been so high in late 2023 and 2024 that tech execs like Mark Zuckerberg and Elon Musk have bragged about how many units they're training new technology on. ByteDance has found workarounds to the US export bans on the chips to China. Saudi Arabia and the United Arab Emirates have bought up thousands of units to fuel their AI ambitions, while venture capitalists have bought Nvidia GPUs as backup units for their startups.\n\nIn 2024, Nvidia unveiled its Blackwell chips, which it says are twice as fast as its Hopper chips and have attracted customers including SoftBank, Amazon Web Services, and Microsoft. The recent frenzy around the Chinese company DeepSeek's models has fueled demand for Nvidia's H200 chips.\n\nIn January 2025, Huang also unveiled new chips targeting the gaming, robotics, and autonomous vehicle industries, as well as partnerships with Toyota and Microsoft.\n\nAt the January 2026 Consumer Electronics Show in Las Vegas, Huang unveiled the new Vera Rubin architecture that will succeed Blackwell. During his presentation, Huang said Vera Rubin is built to confront the core problem of a surge in computing demand.\n\nCompared with Nvidia's Blackwell architecture, Huang said Rubin delivers more than three times the performance, can run inference up to five times faster, and offers significantly higher inference compute per watt.\n\nA key to Nvidia's success is also CUDA, a software layer that can link GPUs to almost any AI application a developer wants to run. It's a critical component of the competitive advantage, or moat, that Nvidia has built up over the years.\n\nStill, AMD, Nvidia's main competitor, is quietly catching up. In October 2025, AMD announced a major multi-year strategic partnership with OpenAI under which OpenAI will deploy up to 6 gigawatts of AMD Instinct GPUs for its AI infrastructure starting in the second half of 2026. The deal is expected to bring tens of billions of dollars in revenue to AMD over time.\n\nNvidia's other competitors include Intel and IBM. Tech giants like Google, Amazon, Microsoft, and Meta have also released their own AI chips.\n\nNvidia overtook Apple and Microsoft for the title of the most valuable company in the world when it hit a historic $5 trillion in market capitalization in October 2025.\n\nAfter a tumultuous start to 2025 over chip export restrictions to China that would have cost billions in losses for Nvidia, Nvidia is heading into 2026 with confidence that demand for its AI chips is far from peaking.\n\nIn 2025, the chipmaker's blockbuster third-quarter results brought in $57 billion in revenue, including $51 billion from its data center business alone, beating Wall Street expectations. Nvidia raised its fourth-quarter forecast to $65 billion in sales, helping revive AI and semiconductor stocks after a brief slump. Shares of Nvidia and peers rallied on the upbeat outlook.\n\nHuang repeatedly dismissed fears of an AI bubble, arguing that the shift from CPUs to GPUs, the rise of agentic AI, and monetization through advertising all point to sustained growth. Nvidia also expanded its influence through major partnerships with OpenAI, Anthropic, and hyperscalers such as Meta.\n\nLooking to 2026, Nvidia is betting big on next-generation chips like Blackwell Ultra, massive AI infrastructure projects, and growth areas including robotics and automotive. While US export restrictions on China remain a headwind, Nvidia expects hyperscalers and global AI investment to keep demand strong into next year.\n\nNvidia is based in Santa Clara, California. Nvidia's headquarters, known as Voyager, was designed by the architectural firm Gensler and is about 750,000 square feet.\n\nIt has parks, \"treehouses\" for gatherings, and places designed to help employees focus. However, the overall design is intended to facilitate Nvidia's flat organizational structure.\n\n\"When you're moving that fast, you want to make sure that that information is flowing through the company as quickly as possible,\" Huang told the Harvard Business Review in 2023.\n\nIt's also a way to create more harmony between leadership and workers. Huang, who in 2023 oversaw 50 direct reports, has said that CEOs \"by definition\" should have the most direct reports at a company.\n\nHuang has earned a reputation among those who work with him as a demanding boss. Meetings with Huang can get heated, and senior employees have described his tough questions as a \"Jensen grilling.\"\n\nNvidia's top executives include Ian Buck, a vice president of hyperscale and high-performance computing; Colette Kress, the chief financial officer; and Bryan Catanzaro, a vice president of applied deep learning research.\n\nLanding a job at Nvidia isn't easy, but Lindsey Duran, a VP of recruitment, told BI that Nvidia applicants should express an interest in generative AI, tap into their professional network for referrals, and aim to do an internship.",
    "readingTime": 8,
    "keywords": [
      "web services",
      "softbank amazon",
      "deep learning",
      "direct reports",
      "vice president",
      "hopper chips",
      "market cap",
      "language models",
      "electrical engineering",
      "export restrictions"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/nvidia",
    "thumbnail_url": "https://i.insider.com/698f93dfd3c7faef0ece4ba3?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:00.743Z",
    "topic": "finance"
  },
  {
    "slug": "eccentric-but-brilliant-openclaws-creator-got-feedback-from-mark-zuckerberg",
    "title": "'Eccentric but brilliant': OpenClaw's creator got feedback from Mark Zuckerberg",
    "description": "Peter Steinberger joins Open AI, guided by Sam Altman, after feedback from Mark Zuckerberg on his AI agent OpenClaw's capabilities.",
    "fullText": "OpenClaw creator Peter Steinberger joined OpenAI, according to an X post by Sam Altman on February 15. But before that, Steinberger got feedback on his product from Mark Zuckerberg.\n\nOpenClaw is an open-source AI agent that can autonomously handle tasks like managing email, booking flights, and interacting with apps and services on a user's behalf.\n\n\"Many people are calling this one of the biggest moments in the recent history of AI, since the launch of ChatGPT in November 2022,\" Lex Fridman said about OpenClaw on the February 11 episode of his podcast, where he interviewed Steinberger.\n\nSteinberger discussed acquisition offers from both OpenAI and Meta on the podcast, saying he also considered raising venture capital but ultimately ruled it out. \"Been there, done that,\" he said of starting a company, adding that it would take time away from building and could create conflicts of interest between a commercial product and the open-source project.\n\nInstead, he narrowed his choice to the two AI labs, which he said made very different pitches. He said OpenAI lured him with compute power and access to cutting-edge infrastructure, while Meta's approach was more personal — Zuckerberg spent a week using OpenClaw and sent detailed feedback.\n\n\"Mark basically played all week with my product and sent me like, 'Oh, this is great.' Or, 'This is shit. Oh, I need to change this.' Or, like, funny little anecdotes,\" Steinberger said of Zuckerberg, adding that he hopped on a WhatsApp call with the Meta CEO where they debated about Claude Code and Codex.\n\n\"And then I think afterwards he called me eccentric but brilliant,\" Steinberger said.\n\nJust before the call, Zuckerberg said he was coding, Steinberger told Fridman on the podcast.\n\n\"He didn't drift away in just being a manager; he gets me,\" Steinberger said. \"That was a good first start.\"\n\nSteinberger said he appreciated Zuckerberg testing the product on Fridman's podcast.\n\n\"People using your stuff is kind of like the biggest compliment, and also shows me that they actually care about it,\" Steinberger said.\n\nSteinberger acknowledged on the podcast that he was leaning toward one company but declined to say which. His choice, it seems, was OpenAI.",
    "readingTime": 2,
    "keywords": [
      "podcast",
      "product",
      "steinberger",
      "feedback",
      "open-source",
      "biggest",
      "adding",
      "away",
      "choice",
      "zuckerberg"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openclaw-creator-peter-steinberger-gets-feedback-from-mark-zuckerberg",
    "thumbnail_url": "https://i.insider.com/69933fd1e1ba468a96ac20a8?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:00.569Z",
    "topic": "finance"
  },
  {
    "slug": "openais-openclaw-hire-sparks-praise-memes-and-rivalry-chatter",
    "title": "OpenAI's OpenClaw hire sparks praise, memes, and rivalry chatter",
    "description": "OpenAI announced on Sunday it had hired Peter Steinberger, the creator of OpenClaw.",
    "fullText": "OpenAI announced on Sunday it had hired Peter Steinberger, the creator of OpenClaw. Within hours, the news sent ripples across the AI community, drawing praise from some executives, jabs from rivals, and a flood of memes from engineers watching the talent wars unfold.\n\nSteinberger wrote in a blog post shared on X Sunday that he was \"joining OpenAI to work on bringing agents to everyone.\"\n\nOpenAI CEO Sam Altman amplified the news, writing that \"the future is going to be extremely multi-agent.\"\n\nPeter Steinberger is joining OpenAI to drive the next generation of personal agents. He is a genius with a lot of amazing ideas about the future of very smart agents interacting with each other to do very useful things for people. We expect this will quickly become core to our…\n\nIn response to the news, several OpenAI leaders welcomed Steinberger. Thibault Sottiaux, an engineering lead on OpenAI's Codex team, wrote that \"@steipete is proof you can just build things.\"\n\n@steipete is proof you can just build things\n\nAnother Codex engineer posted that one of the \"neat\" parts of OpenAI's culture is how many former founders work there.\n\nOne thing @steipete and I talked about over lunch last week was how many former founders are at OpenAI. It’s a really neat part of the culture.\n\nSteinberger told Lex Friedman in a podcast last week that both Mark Zuckerberg and Altman had made him offers.\n\nOpenClaw and its agent-only social media network Moltbook became wildly popular earlier this year as developers and AI enthusiasts shared clips of autonomous AI agents posting, replying, and interacting online. The open-source project, which demonstrates how networks of AI agents can coordinate to perform tasks across apps, also rapidly gained traction on GitHub.\n\nAfter Steinberger's announcement on Sunday, some of the people who worked on OpenClaw commented on the news.\n\n\"I know the decision was not an easy one, and I saw firsthand the pressure Peter was under, given that he understands how fundamental this could be for the AI timeline,\" Jamieson O'Reilly, an OpenClaw advisor, wrote on X in a post congratulating Steinberger.\n\nOne thing has become very clear to me working together with @steipete on @openclaw.\n\nWhile lots of people spectate from the sidelines, sharing their opinions, concerns and even hot takes at times, the dude is there, vigilantly on the front-lines pushing AI forward for every one… https://t.co/fe5OEKgevm\n\nAaron Levie, the CEO of Box, said it was a sign \"2026 was the year of the agents.\"\n\nIf anyone was wondering if 2026 was the year of agents, OpenAI is bringing on the maker of Openclaw. This space is about to get very real. https://t.co/ocqX4kE9PT\n\nNot everyone in the tech space was as enthusiastic about the news.\n\nXAI cofounder Igor Babuschkin asked users on X: \"What's the best open alternative to OpenClaw right now? Doesn't make sense to put all your data into it if it's owned by OpenAI.\"\n\nPayPal mafia member Jason Calacanis expressed similar concerns.\n\n😔 what are the chances the open source project survives / thrives after this? https://t.co/4sUZkKWkGh\n\nSteinberger and OpenAI have said that OpenClaw will remain an open-source project with OpenAI's support.\n\nOther experts in the space pointed out that OpenAI's win could be a loss for Anthropic, especially after Steinberger wrote on X that Anthropic sent \"love letters from legal.\"\n\n\"Another interesting detail is Anthropic's visible disdain for anything open \n\nKris Puckett, a designer at Stripe, expressed a similar sentiment\n\nInstead of @AnthropicAI getting Claudebot, they rushed legal to send a C&D and lost out on not only brilliant talent but community drive. \n\nTruly would love to know the decision making process.\n\nRaphael Schaad, a visiting partner at Y Combinator, said, \"I bet this causes lots of VC tears.\"\n\nI bet this causes lots of VC tears and angry OSS folks. But think about this:\n\n- Peter showed the future and lots of awesome startups are starting to bloom from this. Invest in those!\n\n- Peter created one of the most exciting OSS projects in years. The community is vibrant and… https://t.co/RFWwfXU9Lz\n\nAnd finally, some X power users did what they do best: posted memes about the news.\n\nWas expecting this one in replies pic.twitter.com/bfcZt3Ugg6",
    "readingTime": 4,
    "keywords": [
      "joining openai",
      "open-source project",
      "causes lots",
      "peter steinberger",
      "agents",
      "steipete",
      "community",
      "space",
      "openclaw",
      "across"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openais-openclaw-hire-sparks-praise-memes-rivalry-chatter-2026-2",
    "thumbnail_url": "https://i.insider.com/69934905e1ba468a96ac211a?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:00.552Z",
    "topic": "finance"
  },
  {
    "slug": "sales-reps-at-11-billion-ai-startup-elevenlabs-have-to-bring-in-20-times-their-base-salary-or-theyre-out-vp-says",
    "title": "Sales reps at $11 billion AI startup ElevenLabs have to bring in 20 times their base salary, or they're out — VP says",
    "description": "AI startup ElevenLabs, valued at $11 billion, employs small teams with high sales quotas.",
    "fullText": "At $11 billion AI startup ElevenLabs, the message to sales reps is simple: Hit 20x your base salary, or you're out.\n\nSpeaking on the 20VC podcast on Friday, Carles Reina, VP of sales at the voice-cloning startup, talked through its \"ruthless\" quotas.\n\n\"So if I pay you $100,000 a year, your quota is $2 million. That's it. If you don't achieve your quota, then you're going to be out, right?\" Reina said. \"And we're ruthless on that end.\"\n\nElevenLabs — which was recently valued at $11 billion after closing a $500 million funding round — operates in micro-teams of five to ten people each, according to CEO and cofounder Mati Staniszewski, who spoke on a separate 20VC podcast episode in September.\n\nReina said he prefers to operate in smaller teams that hit their quotas, and pay them more.\n\nSmall teams have become a growing trend in tech, with AI startups touting their ability to scale with far fewer employees by working alongside AI agents.\n\nLinkedIn cofounder Reid Hoffman wrote in January that a team of 15 people using AI can rival a team of 150 who aren't.\n\nMeanwhile, Mark Zuckerberg said on a Meta earnings call in July that he has \"gotten a little bit more convinced around the ability for small, talent-dense teams to be the optimal configuration for driving frontier research.\"\n\nReina said the \"ruthless\" quota has been successful at ElevenLabs, saying on the 20VC podcast that more than 80% of reps hit their sales quota.\n\nElevenLabs did not respond to a request for a comment.\n\nHe added that the firm compensates both the account executive and customer success manager if they upsell a company within the first 12 months.\n\n\"I'm paying double, but I don't care,\" Reina said. \"It makes perfect sense because then I have these two people busting their ass to make sure that they actually can make more money, which is fantastic for me as a company.\"\n\nThe push for higher performance isn't limited to AI startups.\n\nIn April, Google said it was restructuring its compensation structure to increase rewards for top performers. \"High performance is more important than ever,\" Google's head of compensation told staff at the time.",
    "readingTime": 2,
    "keywords": [
      "elevenlabs",
      "quota",
      "sales",
      "podcast",
      "ruthless",
      "teams",
      "startup",
      "reps",
      "you're",
      "quotas"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/elevenlabs-11-billion-ai-startup-ruthless-sales-strategy-2026-2",
    "thumbnail_url": "https://i.insider.com/69933d3ce1ba468a96ac208b?width=1200&format=jpeg",
    "created_at": "2026-02-16T18:30:00.551Z",
    "topic": "finance"
  },
  {
    "slug": "why-your-digital-investments-arent-creating-value",
    "title": "Why Your Digital Investments Aren’t Creating Value",
    "description": "Many companies are pouring money into AI, analytics, and CRM platforms, yet struggle to translate those investments into measurable revenue growth. The problem isn’t technology adoption but the failure to redesign how commercial organizations generate insight, make decisions, and coordinate action. Companies that succeed treat digital as a commercial operating model transformation, making four shifts that turn digital from a perceived tax into a durable growth dividend.",
    "fullText": "Why Your Digital Investments Aren’t Creating Value by Prabhakant Sinha, Arun Shastri and Sally LorimerFebruary 16, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintCompanies are investing heavily in digital analytics hubs with AI and gen AI capabilities, enterprise CRM systems, and marketing technology platforms. Nowhere is this more concentrated than in commercial functions, where growth and customer impact live.",
    "readingTime": 1,
    "keywords": [
      "digital"
    ],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/02/why-your-digital-investments-arent-creating-value",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb25_14_AaronMarin.jpg",
    "created_at": "2026-02-16T18:29:59.941Z",
    "topic": "business"
  },
  {
    "slug": "trilliondollar-ai-market-wipeout-happened-because-investors-banked-that-almost-every-tech-company-would-come-out-a",
    "title": "Trillion-dollar AI market wipeout happened because investors banked that ‘almost every tech company would come out a winner’",
    "description": "\"Nobody truly knows who the long-term winners and losers of this extraordinary technology will be,\" Deutsche's Jim Reid wrote this morning.",
    "fullText": "Investors wobbled last week as they worked through the disruption AI is likely to cause across global industries, with further hiccups potentially bubbling through this week. But the reckoning should have been expected, argued Deutsche Bank in a note to clients this morning, because it is a readjustment of perhaps overly optimistic expectations.\n\nSoftware stocks in particular suffered a wipeout amid mounting concerns that large language models may replace current service offerings. Companies in the legal, IT, consulting and logistics sectors were also impacted. JP Morgan wrote last week that some $2 trillion had been wiped off software market caps alone as a result, a reality that prior to a fortnight ago, Deutsche’s Jim Reid argued had been purely academic. \n\nA 13-figure sell-off is something Reid has speculated over for some time, telling clients: “For months, my published view has been that nobody truly knows who the long term winners and losers of this extraordinary technology will be. Yet as recently as October, markets were implicitly pricing in a world where almost every tech company would come out a winner.\n\nWhy did software stocks suffer major selloffs recently?\n\nWhat caused the $2 trillion market cap wipeout?\n\nWhat makes AI disruption different from past cycles?\n\nHow are experts viewing current AI stock valuations?\n\n“Over recent weeks we’ve seen a more realistic differentiation emerge within tech—but that repricing is now rippling into the broader economy with surprising speed.”\n\nReid hasn’t been alone in his suspicion that investors had perhaps been painting over the entire stock market (and indeed wider economy) with the same, optimistic brush. Some speculators have made broad-stroke arguments that the efficiencies offered by AI will result in wins for the vast majority of companies, while others have argued that while AI is not in a bubble, there are pockets of overoptimism that may burst. \n\nJPMorgan’s CEO Jamie Dimon is of such an opinion, explaining at the Fortune Most Powerful Women Summit last year: “You should be using it,” (speaking to any business that was listening). But he added a caveat, saying that back in 1996, “the internet was real,” and “you could look at the whole thing like it was a bubble.” Then he broke down the real difference that he sees—between AI, on the one hand, and generative AI, on the other. It’s an important distinction, Dimon said, while adding that “some asset prices are high, in some form of bubble territory.”\n\nIndeed, Jeremy Siegel, Emeritus Professor of Finance at The Wharton School of the University of Pennsylvania, argued that such shifts demonstrate investors are “asking the right questions.” Writing for WisdomTree a week ago, where he serves as senior economist, Siegel said: “When companies talk about $200 billion in capital expenditures, markets should scrutinize payback periods, competitive dynamics, and whether durable moats can be built in an environment where technology is evolving at breakneck speed. That tension explains why leadership will continue to rotate even as the secular story remains intact.” \n\nThat said, Reid suggested that the market may be repricing overzealously, arguing the disruption in “old economy” sectors feels overdone: “The real challenge is that even by the end of this year, we still won’t have enough evidence to identify the structural winners and losers with confidence. That leaves plenty of room for investors’ imaginations—both optimistic and pessimistic—to run wild. As such big sentiment swings will continue to be the order of the day.”",
    "readingTime": 3,
    "keywords": [
      "software stocks",
      "investors",
      "argued",
      "market",
      "disruption",
      "optimistic",
      "economy",
      "bubble",
      "clients",
      "wipeout"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/trillion-dollar-ai-market-wipeout-115521847.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/diuu6e_zp4PLZ9MDwG0sZA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/05d4f0f594c265ecbc6d3aa458393bc3",
    "created_at": "2026-02-16T18:29:59.634Z",
    "topic": "finance"
  },
  {
    "slug": "skilldeck-macos-app-to-manage-skills-across-multiple-ai-agents",
    "title": "SkillDeck – macOS app to manage skills across multiple AI agents",
    "description": "Native macOS SwiftUI app for managing multiple AI code agent skills - crossoverJie/SkillDeck",
    "fullText": "crossoverJie\n\n /\n\n SkillDeck\n\n Public\n\n Native macOS SwiftUI app for managing multiple AI code agent skills\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n crossoverJie/SkillDeck",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/crossoverJie/SkillDeck",
    "thumbnail_url": "https://opengraph.githubassets.com/542176cf442007938d113d7f8f38893d2a257b0f45332cd3b09f3df5d2f5ea78/crossoverJie/SkillDeck",
    "created_at": "2026-02-16T12:38:10.229Z",
    "topic": "tech"
  },
  {
    "slug": "bytedance-to-add-safeguards-to-seedance-20-following-hollywood-backlash",
    "title": "ByteDance to add safeguards to Seedance 2.0 following Hollywood backlash",
    "description": "ByteDance has said it will work to strengthen safeguards on a new AI video-making tool, following copyright concerns and legal threats from Hollywood.",
    "fullText": "Chinese tech giant ByteDance has said it will strengthen safeguards on a new artificial intelligence video-making tool, following complaints of copyright theft from entertainment giants.\n\nThe tool, Seedance 2.0, enables users to create realistic videos based on text prompts. However, viral videos shared online appear to show copyrighted characters and celebrity likenesses, raising intellectual property concerns in the U.S.\n\n\"ByteDance respects intellectual property rights and we have heard the concerns regarding Seedance 2.0,\" a company spokesperson said in a statement shared with CNBC.\n\n\"We are taking steps to strengthen current safeguards as we work to prevent the unauthorized use of intellectual property and likeness by users,\" the spokesperson added.\n\nByteDance's response comes after receiving backlash and stern warnings from Hollywood groups like the Motion Picture Association (MPA), a trade association representing major Hollywood studios including Netflix, Paramount Skydance, Sony, Universal, Warner Bros. Discovery and Disney.\n\nThe group issued a forceful public statement at the end of last week demanding that ByteDance immediately cease what it called \"infringing activity.\"\n\n\"In a single day, the Chinese AI service Seedance 2.0 has engaged in unauthorized use of U.S. copyrighted works on a massive scale,\" said MPA chairman and CEO Charles Rivkin in the statement.\n\n\"By launching a service that operates without meaningful safeguards against infringement, ByteDance is disregarding well-established copyright law that protects the rights of creators and underpins millions of American jobs.\"\n\nAccording to a report from Axios, Disney sent a cease-and-desist letter Friday to ByteDance, accusing the company of distributing and reproducing its intellectual property through the new AI tool without permission.\n\nThe legal notice alleged that ByteDance had effectively pre-packaged Seedance with a pirated library of copyrighted characters, portraying them as if they were public-domain clip art,\" the report added.\n\nDisney has also sent cease-and-desist letters to AI companies in the past. In September, the company warned the AI startup Character.AI to stop the unauthorized use of its copyrighted characters.\n\nWhile trying to protect its intellectual property, Disney has signed a licensing deal with and invested in OpenAI. The agreement allows the AI company to use Disney characters from the Star Wars, Pixar and Marvel franchises in its Sora video generator.\n\nParamount Skydance has also sent a cease-and-desist letter to ByteDance, making similar accusations, Variety reported over the weekend.",
    "readingTime": 2,
    "keywords": [
      "cease-and-desist letter",
      "intellectual property",
      "copyrighted characters",
      "seedance",
      "safeguards",
      "tool",
      "statement",
      "unauthorized",
      "bytedance",
      "strengthen"
    ],
    "qualityScore": 1,
    "link": "https://www.cnbc.com/2026/02/16/bytedance-safegaurds-seedance-ai-copyright-disney-mpa-netflix-paramount-sony-universal.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108266044-1771229974981-gettyimages-2261149851-vcg111620288212.jpeg?v=1771230010&w=1920&h=1080",
    "created_at": "2026-02-16T12:38:10.190Z",
    "topic": "tech"
  },
  {
    "slug": "how-to-talk-to-any-github-repo",
    "title": "How to talk to any GitHub repo",
    "description": "Paste a GitHub link into your LLM conversation. Ask questions to understand logic, generate doc, and run the app locally. Use the guide and prompts in this article to structure your conversation.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.theaithinker.com/p/how-to-talk-to-any-github-repo",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!ZMNj!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4e2189f-607c-4f2e-808d-39ac45050c32_1024x731.png",
    "created_at": "2026-02-16T12:38:10.053Z",
    "topic": "tech"
  },
  {
    "slug": "the-speed-of-building-has-outpaced-the-thinking-part",
    "title": "The Speed of Building Has Outpaced the Thinking Part",
    "description": "Explore the impact of AI on indie development and the need for a moral compass in coding. Are we sacrificing quality for speed?",
    "fullText": "I get this feeling a lot lately. I wake up with an idea, grab a coffee, open my editor, and thanks to the current generation of AI tools, I can have a working prototype before breakfast.\n\nThe barrier to entry for software development hasn’t just been lowered; it’s effectively been removed. We are in the era of “vibe coding,” where natural language prompts turn into deployed applications in minutes. It is exhilarating. It is powerful.\n\nBut lately, I have started to wonder: Are we killing indie development with AI?\n\nDon’t get me wrong, I love these tools. I use GitHub Copilot and other LLMs daily. But I believe we have reached a tipping point where the speed of building has outpaced the thinking part. We are so focused on how fast we can build that we stopped asking if we should build.\n\nIn this post, I want to talk about why we need a new “moral compass” for development in the AI age, and a potential solution to help us get there.\n\nFive years ago, if you had an idea for a SaaS tool, say, a screenshot editor or a niche time-tracker,you had to sit down and plan. The friction of coding was a natural filter. You had to ask yourself: “Is this worth X hours of my life?”\n\nToday, that cost is near zero. If you don’t like the screenshot tool you’re paying $15 a year for, you can prompt an AI to build a clone in an afternoon.\n\nOn the surface, this looks like freedom. But look a little deeper. That $15 tool you just cloned? It was likely built by another indie developer. Someone who spent months thinking about edge cases, designing the interface, writing documentation, and supporting users. By cloning it just because you can, you aren’t just saving $15; you are actively devaluing the craft of independent software development and the livelihood of the person behind it.\n\nIf we all just clone everything we use, we completely commoditize the market. We create a sea of “good enough” AI-generated noise where no one can actually sustain a business.\n\nLet me paint a picture that I think a lot of developers are starting to recognize.\n\nYou spend weeks, maybe months, building something. You think about the problem, you design the interface, you handle the edge cases, you support your users, you write the docs. You pour yourself into it. Then one morning, someone sees your product, opens their AI editor, and builds a “good enough” version in an afternoon. They ship it. Maybe they make it free, maybe they make it open source, maybe they just use it themselves and tell their friends, their community, their followers.\n\nThey did not steal your code. They did not copy your product. They just… rebuilt it. Close enough. Good enough. And now your product has competition that cost someone a few hours of prompting while it cost you months of your life.\n\nBut it does not stop there. A third developer sees that clone and thinks, “I can do this too, but I want it slightly different.” So they prompt their own version. And a fourth. And a fifth. Each one is not a copy in the traditional sense. Nobody is violating a license. Nobody is stealing intellectual property. They are just building their own version that matches their use case.\n\nIt is a lot like art. You create a painting, something original, something you are proud of. Then somebody sees it and recreates it. Not a forgery, just their interpretation. But they have a bigger budget, a larger audience, better distribution. Suddenly their version is the one people see first. Others share that version instead of yours. This is what is happening a lot on social media with AI-generated content. The original creator is overshadowed by the faster, more accessible clone.\n\nIn the art world, we have a word for this erosion: it is called devaluation. In the software world, we are doing it at industrial scale, and we are calling it innovation.\n\nI am not saying you should never build something that already exists. Competition is healthy, and sometimes a fresh perspective genuinely improves a category. But there is a difference between thoughtful competition and reflexive duplication. The question every developer should ask themselves is: “If I know someone can clone my work in an afternoon, is it still worth building?”\n\nThe answer, I believe, is yes, but only for the things that cannot be cloned in an afternoon. The deep domain knowledge. The community around your tool. The years of user feedback baked into every feature. The trust you have earned. Those are the things AI cannot reproduce with a prompt, and I definitely don’t want to discourage people from building those things.\n\nBut you can only build those things if you commit to something long enough for them to develop. And that is the real danger of the current moment: not that AI makes building easy, but that it makes abandoning easy. Why invest years in one product when you can ship a new one every week?\n\nI have no room to preach. I am right there in the trenches with you.\n\nWhen I built Front Matter CMS, it was way before the AI boom. I had to think deeply about the problem because the investment of time was massive. I looked at the market, saw a gap in Visual Studio Code, and built it because nothing else existed.\n\nCompare that to recently. I built a set of cycling tools (never released by the way) for myself. Did similar tools exist? Absolutely. Were they better? Definitely. But I wanted to see how far I could get with AI. I treated it as a training exercise. In the end, I started paying for a tool called Join, which does the same thing, because it was better and I could focus on my actual work instead of maintaining a tool that was just “good enough” for me.\n\nI did the same with FrameFit. I investigated the market a little, didn’t see an exact match, and just started building.\n\nThere is a difference between building for education (learning how AI tools work) and releasing products that dilute the hard work of others. My worry is that we are blurring that line. We are shipping our “training exercises” as products, and it is making the ecosystem messy for everyone.\n\nAnd I know this because I have been on both sides of it.\n\nHere is the thing that made me stop and reflect. I have projects on both sides of this line, and they feel completely different.\n\nDemo Time is something I have been building for years. Not weeks, not weekends, years. It started because I was a conference speaker who kept running into the same problem: demos failing on stage. Nobody had built a proper solution inside Visual Studio Code, so I did. Over time, it grew because I kept showing up. I used it at conferences, talked to other speakers, iterated based on real feedback from people doing real presentations at events like Microsoft Ignite, GitHub Universe, and OpenAI DevDays. Today it has over 26,000 installations.\n\nNone of that came from code. The code is open source. Anyone can see it, fork it, or rebuild it. Someone could probably vibe-code a basic version in a weekend. But what they cannot replicate is twelve years of conference speaking that taught me what presenters actually need. You would need that experience, or a big company and budget behind you, to even come close. The relationships with the community, the trust that comes from being the person who shows up, year after year, and keeps making the tool better because you genuinely use it yourself. That is not something you can prompt into existence.\n\nCompare that to FrameFit. I built it, I use it, and it works. But if it disappeared tomorrow, I wouldn’t lose any sleep over it. Demo Time? That is like a child to me. I put my passion into it.\n\nThat contrast taught me something important: AI cannot commoditize the human context around software. Community, trust, domain expertise, showing up consistently over time. These are not features you ship. They are moats you build by caring about something longer than a weekend.\n\nThe developers who will thrive are not the fastest shippers. They are the ones who pair AI speed with human judgment. Who build communities, not just codebases. Who invest in trust, not just features. But that only happens if we slow down enough to think about what we are doing.\n\nWe need to re-introduce friction into our process. Not the old friction of writing boilerplate code. That friction is gone, and good riddance. I am talking about the friction of thinking. The pause that forces you to examine your intentions before you act on them.\n\nBefore AI, “thinking” was mandatory. The cost of building was high enough that it naturally filtered out bad ideas. Now, that filter is gone, and thinking must be a conscious, deliberate choice. When I have an idea now, I am trying to force myself to pause before I open Visual Studio Code or prompt a new agent.\n\nI try to run through these four questions:\n\nThat last one is crucial. If there is an open-source tool that does 80% of what you want, the “old” way was to contribute a Pull Request. The “AI way” often tempts us to just rebuild the whole thing from scratch because it feels faster.\n\nBut “faster” isn’t always “better” for the community. And here is the irony: we could use AI itself for this thinking step. Instead of prompting an LLM to start building, prompt it to research what already exists first. Use AI for the thinking, not just the building.\n\nI don’t expect AI platforms that allow you to vibe code to solve this for us. Their business model is predicated on you writing more code (read: prompts), not less. They want you to spin up new projects constantly. They have no incentive to say, “Hey, wait, this already exists.”\n\nThink about it: when was the last time you saw a developer advocate from one of these platforms demonstrate how to contribute to an existing project instead of building something new from scratch? Their marketing is all about speed, novelty, and the thrill of creation. Not about responsibility.\n\nSo, I started thinking: What if we used AI to stop us from building with AI? You could say that this is a paradox, but I think it is actually a necessary evolution of our responsibility as developers.\n\nI am exploring the idea of a Product Moral Compass Agent.\n\nImagine a mandatory first step in your “vibe coding” workflow. Before you start generating code, you pitch your idea to this agent. It interviews you, not to judge you, but to make sure you are making an informed decision.\n\nThis agent would act as the “thinking partner” we are skipping. It could:\n\nIf you still want to build it after that? Great. Go ahead and start coding. But at least you are making an informed, conscious decision rather than reflexively adding more noise to the world.\n\nI am currently building this agent. The first version is available on GitHub: Product Moral Compass Agent. Yes, I am aware of the irony, I am proposing to build something new to stop people from building new things. But I ran it through my own four questions first, and nothing like it exists yet.\n\nOnce it is ready, I will share it openly so that any developer can use it as part of their workflow. Not as a gatekeeper, but as a guide. A thinking partner that helps you pause, research, and decide before you build.\n\nIn the meantime, here is what you can do right now: the next time you have an idea, spend ten minutes with your favorite AI tool and ask it to find every existing solution first. Check your own bank statements. Are you already paying for a tool that solves this? If so, respect that developer’s work. Look at GitHub. Is there a repo that could use your help instead of your competition?\n\nThe time to learn is right now, but the time to think is also right now.\n\nI want you to keep building. I want you to be prolific. But let’s not let the ease of creation destroy the value of what we create.\n\nI am curious to hear your thoughts. Is this gatekeeping, or is it a necessary evolution of our responsibility as developers? Let me know in the comments below.\n\nIs an AI able to write the contents of your article? Well, that was a question I had and wanted to find out. In this article I tell you all about it.\n\nDiscover the latest advancements in documentation technology and how tools like GitHub Copilot for Docs, Mendable, and OpenAI are changing the game.\n\nDiscover how to leverage Azure AI Translator's Sync API for real-time document translation, simplifying your workflow and enhancing user experience.\n\nFound a typo or issue in this article? Visit the GitHub repository \nto make changes or submit a bug report.\n\nSolutions Architect & Developer Expert\n\nEngage with your audience throughout the event lifecycle",
    "readingTime": 12,
    "keywords": [
      "visual studio",
      "product moral",
      "compass agent",
      "studio code",
      "edge cases",
      "necessary evolution",
      "vibe coding",
      "software development",
      "github copilot",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://www.eliostruyf.com/killing-indie-development-with-ai/",
    "thumbnail_url": "https://www.eliostruyf.com/social/5f59a11b-79bb-48df-9b89-b8abc9ba3037.png",
    "created_at": "2026-02-16T12:38:09.254Z",
    "topic": "tech"
  },
  {
    "slug": "kanvibe-kanban-board-that-autotracks-ai-agents-via-hooks",
    "title": "KanVibe – Kanban board that auto-tracks AI agents via hooks",
    "description": "Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking — manage tmux/zellij sessions and git worktrees from one board. - rookedsysc/kanvibe",
    "fullText": "rookedsysc\n\n /\n\n kanvibe\n\n Public\n\n Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking — manage tmux/zellij sessions and git worktrees from one board.\n\n License\n\n AGPL-3.0 license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n rookedsysc/kanvibe",
    "readingTime": 1,
    "keywords": [
      "board",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/rookedsysc/kanvibe",
    "thumbnail_url": "https://opengraph.githubassets.com/1686c5ce06bcd0be96aea5e2e16beffdd136eeb70f17f431b75349727b34dbe2/rookedsysc/kanvibe",
    "created_at": "2026-02-16T12:38:09.047Z",
    "topic": "tech"
  },
  {
    "slug": "beatflow-texttomidi-generator-that-plans-full-song-structure",
    "title": "BeatFlow: Text-to-MIDI generator that plans full song structure",
    "description": "Web-based AI music generator that plans full song arrangements based on music theory, offering in-browser Piano Roll for editing multi-track MIDI instead of just generating static audio. - the0cp/b...",
    "fullText": "the0cp\n\n /\n\n beatflow\n\n Public\n\n Web-based AI music generator that plans full song arrangements based on music theory, offering in-browser Piano Roll for editing multi-track MIDI instead of just generating static audio.\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n the0cp/beatflow",
    "readingTime": 1,
    "keywords": [
      "music"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/the0cp/beatflow",
    "thumbnail_url": "https://opengraph.githubassets.com/e9d71946e201cb2c1fc9efe072ca3b6924dd5d06278818b66bec5055d09b8c79/the0cp/beatflow",
    "created_at": "2026-02-16T12:38:08.910Z",
    "topic": "tech"
  },
  {
    "slug": "thanks-a-lot-ai-hard-drives-are-sold-out-for-the-year-says-wd",
    "title": "Thanks a lot, AI: Hard drives are sold out for the year, says WD",
    "description": "AI companies have bought out Western Digital's storage capacity for 2026. It's only February.",
    "fullText": "Looking to buy a new hard drive? Get ready to pay even more this year.\n\nAccording to Western Digital, one of the world's biggest hard drive manufacturers, the company has already sold out of its storage capacity for 2026 with more than 10 months still left in the year.\n\n\"We're pretty much sold out for calendar 2026,\" said Western Digital CEO Irving Tan on the company's recent quarterly earnings call.\n\nTan shared that most of the storage space has been allocated to its \"top seven customers.\" Three of these companies already have agreements with Western Digital for 2027 and even 2028.\n\nFurthermore, the incentive for these hardware companies to prioritize the average consumer is also dwindling. According to Western Digital, thanks to a surge in demand from its enterprise customers, the consumer market now accounts for just 5 percent of the company's revenue.\n\nAI companies have been eating up computer hardware as industry growth accelerates. Prices for products ranging from computer processors to video game consoles have skyrocketed due to these AI companies cannibalizing supply chains.\n\nThe tech industry has already been experiencing a shortage of memory due to demand from AI companies. PC makers have been forced to raise RAM prices on a near-regular basis as shortages persist. Video game console makers, like Sony, have even reportedly considered pushing the next PlayStation launch beyond the planned 2027 release in hopes that AI-related hardware shortages would be resolved by then.\n\nWith this latest news from Western Digital, it appears the ever-increasing demands from AI companies for memory and storage will continue to grow, with no end in sight. Unless, of course, investors decide to pull back from AI over fears that AI's promises may not come to fruition. But, for now at least, the shortages – and price hikes for consumers – will continue.\n\nTopics\n Artificial Intelligence",
    "readingTime": 2,
    "keywords": [
      "western digital",
      "storage",
      "hardware",
      "shortages",
      "drive",
      "company's",
      "customers",
      "consumer",
      "demand",
      "computer"
    ],
    "qualityScore": 0.85,
    "link": "https://mashable.com/article/ai-hard-drive-hdd-shortages-western-digital-sold-out",
    "thumbnail_url": "https://helios-i.mashable.com/imagery/articles/03BMp5tylVs9DJJavYCVFKV/hero-image.fill.size_1200x675.v1771180235.jpg",
    "created_at": "2026-02-16T12:38:08.317Z",
    "topic": "tech"
  },
  {
    "slug": "the-art-of-the-squeal-what-we-can-learn-from-the-flood-of-ai-resignation-letters",
    "title": "The art of the squeal: What we can learn from the flood of AI resignation letters",
    "description": "What we can learn from the flood of \"why I quit\" letters from researchers at Anthropic, OpenAI, and xAI.",
    "fullText": "Corporate resignations rarely make news, except at the highest levels. But in the last two years, a spate of X posts, Substack open letters, and public statements from prominent artificial intelligence researchers have created a new literary form — the AI resignation letter — with each addition becoming an event to be mined for meaning. Together, the canon of these letters — some of them apparently bound by non-disclosure agreements and other loyalties, legally compelled or not — tells us a lot about how some of the top people in AI see themselves and the trajectory of their industry. Overall, the image is bleak.\n\nThis past week brought several additions to the annals of \"Why I quit this incredibly valuable company working on bleeding-edge tech\" letters, including from researchers at xAI and an op-ed in The New York Times from a departing OpenAI researcher. Perhaps the most unusual was by Mrinank Sharma, who was put in charge of Anthropic's Safeguards Research Team a year ago, and who announced his departure from what is often considered the more safety-minded of the leading AI startups. He posted a 778-word letter on X that was at times romantic and brooding — he quoted the poets Rainer Maria Rilke and Mary Oliver. Opining on AI safety, his own experiences working on AI sycophancy and \"AI-assisted bioterrorism,\" and the \"poly-crisis\" consuming our society, the letter had three footnotes and some ominous, if vague, warnings.\n\n\"We appear to be approaching a threshold where our wisdom must grow in equal measure to our capacity to affect the world, lest we face the consequences,\" Sharma wrote. \"Throughout my time here, I've repeatedly seen how hard it is to truly let our values govern our actions.\"\n\nSharma noted that his final project at Anthropic was \"on understanding how Al assistants could make us less human or distort our humanity\" — a nod, perhaps, to the scourge of AI psychosis and other novel harms emerging from people overvaluing their relationships with chatbots. He said that he didn't know what he was going to do next, but expressed a desire to pursue \"a poetry degree and devote myself to the practice of courageous speech.\" The researcher ended by including the full text of \"The Way It Is\" by the poet William Stafford.\n\nIn the annals of AI resignations, Sharma's missive might be less dramatic than the boardroom coup that ousted OpenAI CEO Sam Altman for five days in November 2023. It's less troubling than some of the other end-of-days warnings published by AI safety researchers who quit their posts believing that their employers weren't doing enough to mitigate the potential harms of artificial general intelligence, or AGI, a smarter-than-human intelligence that AI companies are racing to build. (Some AI experts question whether AGI is even achievable or what it might mean.)\n\nBut Sharma's note captures the deep attachments that top AI researchers — who are extremely well-compensated and work together in small teams — feel to their work, their colleagues, and, often, their employers. It also exposes some of the tensions that we see cropping up again and again in these resignation announcements. At top AI labs, there's an intense competition for resources between research/safety teams and people working on consumer-facing AI products. (Few, if any, public resignations seem to come from people on the product side.) There are pressures to ship without proper testing, established safeguards, or knowing what might happen when a system goes rogue. And there's a deep sense of mission and purpose that can sometimes be upended by feelings of betrayal.\n\nMany of the people who have publicly quit AI companies work in safety and \"alignment,\" the field tasked with making sure that AI capabilities align with human needs and welfare. Many of them seem very optimistic about AI, and even AGI, but they worry that financial pressures are eating away at safeguards. Few seem to be giving up on the field entirely — except perhaps Sharma, the aspiring poet. Either they jump ship for another seven-, eight-, or nine-figure job at a competing AI startup, or they become civic-minded AI analysts and researchers at one of a growing number of AI think tanks.\n\nAll of them seem to be worried that either epic gains or epic disasters lie ahead. Announcing his departure from Anthropic to become OpenAI's Head of Preparedness earlier this month, Dylan Scandinaro wrote on LinkedIn, \"AI is advancing rapidly. The potential benefits are great — and so are the risks of extreme and even irrecoverable harm.\" Daniel Kokotajlo, who resigned from OpenAI, said that OpenAI's systems \"could be the best thing that has ever happened to humanity, but it could also be the worst if we don't proceed with care.\"\n\nRecently, xAI, where co-founder Elon Musk is notorious for tinkering with the proverbial dials of the Grok chatbot, has seen a half-dozen members of its founding team leave. But the locus of the AI resignation letter, as a kind of industry artifact, is the red-hot startup OpenAI, where major figures, including top executives and safety-minded researchers, have been leaving for the last two years. Some resigned; some were fired; some were described in the press as \"forced out\" over internal company disputes. Seven left in a short period in the first half of 2024.\n\nWith revenue paling compared to its massive and growing infrastructure costs, OpenAI recently announced that it would begin incorporating ads into ChatGPT. That caused researcher Zoë Hitzig to quit. This week, she published a resignation letter in the Times, warning about the potential implications of ads becoming part of the substrate of chatbot conversations. \"ChatGPT users have generated an archive of human candor that has no precedent, in part because people believed they were talking to something that had no ulterior agenda,\" she wrote. But, she warned, OpenAI seemed prepared to leverage that \"archive of human candor\" — much as Facebook had done — to target ads and undermine user autonomy. In the service of maximizing engagement, consumers might be manipulated — the classic sin of the modern internet.\n\nIf you think you are building a world-changing invention, you need to be able to trust your leadership. That's been a problem at OpenAI. On November 17, 2023, Altman was dramatically fired by the company's board because, it claimed, Altman was \"not consistently candid in his communications with the board.\" Less than a week later, he performed his own boardroom coup and was reinstated, before consolidating his power. The exodus proceeded from there.\n\nOn May 14, 2024, OpenAI co-founder Ilya Sutskever announced his resignation. Sutskever was replaced as head of OpenAI's superalignment team by John Schulman, another company co-founder. A few months later, Schulman left OpenAI for Anthropic. Six months later, he announced his move to Thinking Machines Lab, an AI startup founded by former OpenAI CTO Mira Murati, who had replaced Altman as OpenAI's interim CEO during his brief firing.\n\nThe day after Sutskever left OpenAI, Jan Leike, who also helped head OpenAI's alignment work, announced on X that he had resigned. \"OpenAI is shouldering an enormous responsibility on behalf of all of humanity,\" Leike wrote, but the company's \"safety culture and processes have taken a backseat to shiny products.\" He thought that \"OpenAI must become a safety-first AGI company.\" Less than two weeks later, Leike was hired by Anthropic. OpenAI and Antrhopic did not respond to requests for comment.\n\nAt OpenAI, departing researchers have said that the experts concerned with alignment and safety have often been sidelined, pushed out, or scattered among other teams, leaving researchers with the sense that AI companies are sprinting to build an invention they won't be able to control. \"In short, neither OpenAI nor any other frontier lab is ready, and the world is also not ready\" for AGI, wrote Miles Brundage when he resigned from OpenAI's AGI readiness team in 2024. Yet he added that \"working at OpenAI is one of the most impactful things that most people could hope to do\" and did not directly criticize the company. Brundage now runs AVERI, an AI research institute.\n\nAcross the AI industry, the story is much the same. In public pronouncements, top researchers gently chastise or occasionally denounce their employers for pursuing a potentially apocalyptic invention while also emphasizing the necessity of doing that research. Sometimes they offer a \"cryptic warning\" that leaves AI watchers scratching their heads. A few do seem genuinely alarmed at what's happening. When OpenAI safety researcher Steven Adler left the company in January 2025, he wrote that he was \"pretty terrified by the pace of AI development\" and wondered if it would wipe out humanity.\n\nYet in the many AI resignation letters, there's little discussion of how AI is being used right now. Data center construction, resource consumption, mass surveillance, ICE deportations, weapons development, automation, labor disruption, the proliferation of slop, a crisis in education — these are the areas where many people see AI affecting their lives, sometimes for the worse, and the industry's pious resignees don't have much to say about it all. Their warnings about some disaster just beyond the horizon become fodder for the tech press — and de facto cover letters for their next industry job — while failing to reach the broader public.\n\n\"Tragedies happen; people get hurt or die; and you suffer and get old,\" wrote William Stafford in the poem that Mrinank Sharma shared. It's a terrible thing, especially the tones of passivity and inevitability — resignation, you might call it. It can feel as if no single act of protest is enough, or, as Stafford writes in the next line: \"Nothing you do can stop time's unfolding.\"\n\nJacob Silverman is a contributing writer for Business Insider. He is the author, most recently, of \"Gilded Rage: Elon Musk and the Radicalization of Silicon Valley.\"",
    "readingTime": 9,
    "keywords": [
      "boardroom coup",
      "human candor",
      "resignation letter",
      "mrinank sharma",
      "researchers",
      "safety",
      "openai",
      "letters",
      "less",
      "industry"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/resignation-letters-quit-openai-anthropic-2026-2",
    "thumbnail_url": "https://i.insider.com/698f85e9e1ba468a96ac0ffc?width=1200&format=jpeg",
    "created_at": "2026-02-16T12:38:08.003Z",
    "topic": "finance"
  },
  {
    "slug": "trumps-trade-advisor-says-big-tech-must-internalize-the-cost-of-ai-data-centers",
    "title": "Trump's trade advisor says Big Tech must 'internalize the cost' of AI data centers",
    "description": "Peter Navarro says the White House may force Big Tech to cover electricity and grid costs tied to AI data centers.",
    "fullText": "The White House is signaling it may force Big Tech to foot the full bill for America's AI boom.\n\nCompanies building data centers \"need to pay for all of the costs,\" and taxpayers should not shoulder the impact of the AI boom.\n\nThe White House has to \"make sure the American people are not hurt,\" he added.\n\nNavarro's comments come as the AI data-center boom faces mounting scrutiny over rising utility bills.\n\nTech giants are pouring hundreds of billions into infrastructure to power artificial intelligence. In November, Meta pledged $600 billion to expand AI technology, infrastructure, and its workforce. Apple said in August it would boost its US infrastructure plans by adding another $100 billion, bringing its total commitment to $600 billion.\n\nAt the same time, energy costs are rising. Electric and gas utilities sought $31 billion in rate hikes from regulators last year, more than twice the $15 billion requested the year before, according to a study published last month by PowerLines, a nonprofit that advocates for utility customers. Many power providers have cited surging electricity demand from large-scale data centers as a key reason for seeking higher rates.\n\nPresident Donald Trump has pushed back on the idea that households should absorb those increases.\n\n\"I never want Americans to pay higher Electricity bills because of Data Centers,\" Trump wrote last month in a post on Truth Social.\n\nThe \"big technology companies who build them,\" the president said, \"must pay their own way.\"\n\nNavarro also said on Fox News that the US must keep expanding its data center capacity if it wants to remain \"No.1 on the global stage in terms of AI.\"\n\n\"We have to lead China and others on this,\" Navarro said. \"At the same time, we have to be mindful of the impacts across this nation.\"\n\nThe US must stay ahead \"not just for economic reasons but for national security reasons,\" because AI \"will be one of the most dangerous weapons of war,\" he added.\n\nSome AI companies have moved to reassure policymakers that households won't bear the cost of the industry's rapid expansion.\n\nAnthropic said on Thursday that it will cover 100% of the grid upgrade costs associated with its AI data centers.\n\n\"The country needs to build new data centers quickly to maintain its competitiveness on AI and national security,\" Anthropic said. \"But AI companies shouldn't leave American ratepayers to pick up the tab.\"\n\nThe pledge follows the company's November announcement that it plans to invest $50 billion in AI infrastructure, starting with facilities in Texas and New York.\n\nMicrosoft has taken a similar approach. Last month, the company said it would pay utility rates high enough to cover the electricity costs tied to its data centers and minimize the burden of data center expansion on surrounding communities.",
    "readingTime": 3,
    "keywords": [
      "white house",
      "the white house",
      "infrastructure",
      "boom",
      "utility",
      "electricity",
      "american",
      "rising",
      "bills",
      "technology"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/trump-trade-advisor-peter-navarro-ai-internalize-data-center-costs-2026-2",
    "thumbnail_url": "https://i.insider.com/6992abc6e1ba468a96ac1e59?width=1200&format=jpeg",
    "created_at": "2026-02-16T12:38:07.995Z",
    "topic": "finance"
  },
  {
    "slug": "dario-amodei-says-anthropic-struggles-to-balance-incredible-commercial-pressure-with-its-safety-stuff",
    "title": "Dario Amodei says Anthropic struggles to balance 'incredible commercial pressure' with its 'safety stuff'",
    "description": "Anthropic CEO Dario Amodei says he's trying to keep Anthropic growing at a 10x pace while holding the line on safety.",
    "fullText": "A familiar tension has come for even the most safety-minded of the AI industry: principles or profit?\n\nOpenAI, the leading AI startup, was founded to build artificial intelligence that benefits all of humanity. Many AI watchers and former employees have questioned its commitment to that mission, however, as it rushes to generate revenue to justify enormous investments in the company.\n\nAnthropic, one of OpenAI's chief rivals, was founded by former OpenAI employees who were concerned about that perceived mission drift. They sought to run an AI company focused on safety above all else.\n\nEven Anthropic, however, struggles to stay on course.\n\nAnthropic CEO Dario Amodei says his company faces significant pressure to uphold its commitments to mitigating AI's potential risks while still turning a profit.\n\n\"We're under an incredible amount of commercial pressure, and we make it even harder for ourselves because we have all this safety stuff we do that I think we do more than other companies,\" Amodei said on a recent episode of the \"Dwarkesh\" podcast.\n\nLast week, Anthropic, which launched only five years ago, announced $30 billion in Series G funding at a $380 billion post-money valuation, making it one of the most valuable private companies in the world.\n\nIn its press release, the company underscored its growing revenue.\n\n\"It has been less than three years since Anthropic earned its first dollar in revenue,\" the company said. \"Today, our run-rate revenue is $14 billion, with this figure growing over 10x annually in each of those past three years.\"\n\nGrowth like that often comes with growing expectations.\n\n\"The pressure to survive economically while also keeping our values is just incredible,\" Amodei said on the podcast. \"We're trying to keep this 10x revenue curve going.\"\n\nAmodei was formerly OpenAI's vice president of research, focusing on safety. He founded Anthropic in 2021 with his sister, Daniela Amodei, and five other former OpenAI staffers, driven by a desire to prioritize safety as AI systems grew increasingly powerful.\n\nAmodei is not the only one who says that Anthropic's mission is hard to sustain as the company grows. Mrinank Sharma, a former safety researcher at Anthropic, said he resigned last week in part due to this tension.\n\n\"Throughout my time here, I've repeatedly seen how hard it is to truly let our values govern our actions,\" Sharma wrote in his resignation letter, which he shared on X. \"I've seen this within myself, within the organization, where we constantly face pressures to set aside what matters most, and throughout the broader society too.\"\n\nEven at companies that aren't developing foundational AI models, adopting AI responsibly often takes a back seat to the promise of efficiency and increased profits.\n\nResponsible AI use in the workplace is moving \"nowhere near as fast as it should be,\" Tad Roselund, a managing director and senior partner at Boston Consulting Group, told Business Insider in 2024.\n\nThe same is true across the venture capital ecosystem.\n\n\"The venture capital environment also reflects a disproportionate focus on AI innovation over AI governance,\" Navrina Singh, the founder and CEO of AI governance platform Credo AI, told Business Insider in 2024. \"To adopt AI at scale and speed responsibly, equal emphasis must be placed on ethical frameworks, infrastructure, and tooling to ensure sustainable and responsible AI integration across all sectors.\"",
    "readingTime": 3,
    "keywords": [
      "venture capital",
      "business insider",
      "revenue",
      "safety",
      "openai",
      "founded",
      "mission",
      "pressure",
      "anthropic",
      "tension"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/dario-amodei-anthropic-profit-pressure-versus-safety-mission-2026-2",
    "thumbnail_url": "https://i.insider.com/69911622d3c7faef0ece5366?width=1200&format=jpeg",
    "created_at": "2026-02-16T12:38:07.822Z",
    "topic": "finance"
  },
  {
    "slug": "the-career-rise-of-openais-billionaire-ceo-sam-altman",
    "title": "The career rise of OpenAI's billionaire CEO, Sam Altman",
    "description": "OpenAI CEO Sam Altman helped usher in the AI age. Now, he's doing everything he can to keep OpenAI ahead.",
    "fullText": "OpenAI Sam Altman thinks he can see the future better than some people.\n\n\"I think I am unusually good at projecting multiple things— years or a couple of decades into the future—and understanding how those are going to interact together,\" Altman told Forbes in February.\n\nWhat is clear is that in 2026 and beyond, OpenAI and Altman have a lot riding on his vision.\n\nIn 2022, Altman oversaw the release of ChatGPT, kicking off what Bill Gates called \"the age of AI.\" Just over three years removed from the moment, Altman's rivals are applying the pressure like never before.\n\nRivals, like Elon Musk and Anthropic CEO Dario Amodei, continue to taunt him through the struggles.\n\nAll the while, Altman has spun OpenAI from a research lab into a major company, even boasting a social media play. And he still has yet to reveal whatever mysterious device he's cooking up with Jony Ive, Apple's former design chief.\n\nIf the past is any indication, Altman is tough to bet against.\n\nIn just under a decade, he went from being part of the first Y-Combinator batch to leading the famed startup incubator. A billionaire before he turned 40, the entrepreneur is no longer just a beacon of Silicon Valley. Altman is now a co-Time Person of the Year and a frequent guest of world leaders.\n\nHere's a look at Altman's life and career so far.\n\nAltman grew up in St. Louis and he was a computer whiz from a young age.\n\nHe learned how to program and take apart a Macintosh computer when he was 8 years old, according to The New Yorker. He attended John Burroughs School, a private, nonsectarian college-preparatory school in St. Louis.\n\nAltman has said that having a Mac helped him with his sexuality. He came out as gay after a Christian group boycotted an assembly at his school that was about sexuality.\n\n\"Growing up gay in the Midwest in the two-thousands was not the most awesome thing,\" he told The New Yorker in 2016. \"And finding AOL chat rooms was transformative. Secrets are bad when you're eleven or twelve.\"\n\nAltman studied computer science at Stanford University before dropping out to start an app. The app, which became Loopt, was part of the first group of companies at startup accelerator Y Combinator.\n\nLoopt eventually reached a $175 million valuation. The $43 million sale price was close to how much it had raised from investors, The Wall Street Journal reported. The company was acquired by Green Dot, a banking company known for prepaid cards.\n\nIn 2014, at the age of 28, Altman was chosen by Y Combinator founder Paul Graham to succeed him as president of the startup accelerator.\n\nWhile he was YC president, Altman taught a lecture series at Stanford called \"How to Start a Startup.\" The next year, at 29, Altman was featured on the Forbes 30 Under 30 list for venture capital.\n\nIn 2015, Altman cofounded OpenAI with Elon Musk, CEO of Tesla and SpaceX. Their goal for the nonprofit artificial intelligence company was to make sure AI doesn't wipe out humans.\n\nSome of Silicon Valley's most prominent names pledged $1 billion to OpenAI, including Reid Hoffman, the cofounder of LinkedIn, and Thiel. Altman stepped down as YC president in March 2019 to focus on OpenAI.\n\nAltman and OpenAI's now-former chief scientist, Ilya Sutskever, said the move to focus on large language models was the best way for the company to reach artificial general intelligence, or AGI, a system that has broad human-level cognitive abilities.\n\nOpenAI received a $1 billion investment from Microsoft in 2019, the beginning of a major partnership for both companies.\n\nUnder Altman's early tenure, OpenAI released popular generative AI tools to the public, including DALL-E and ChatGPT.\n\nBoth DALL-E and ChatGPT are known as \"generative\" AI, meaning the bot creates its own artwork and text based on information it is fed.\n\nAfter ChatGPT was released on November 30, 2022, Altman tweeted that it had reached over 1 million users in five days. As of early 2026, ChatGPT is up to 300 million weekly active users.\n\nOpenAI built on ChatGPT's public launch with a series of major announcements throughout 2023, including the release of GPT-4, an extension of their partnership with Microsoft, and the announcement of ChatGPT Plus (a subscription tier).\n\nIn November, OpenAI's board of directors announced the biggest news: Altman was out as CEO and leaving the board \"effectively immediately.\" The board said Altman was being removed because he \"was not consistently candid in his communications with the board.\"\n\nSutskever has expressed remorse for his participation in Altman's removal. Sutskever and three other members did not return to the reconfigured board when Altman was reinstated.\n\nAltman, like many other tech CEOs, was front and center for President Donald Trump's return to power on January 20, 2025. A day after Trump's inauguration, Altman joined Oracle CTO Larry Ellison, SoftBank founder Masayoshi Son, and Trump to announce a partnership to fund a $500 billion investment in US AI. The companies would form Stargate, a project that seeks to build US AI infrastructure and create jobs.\n\nIn February 2024, Musk made a $97.4 billion offer to take over OpenAI. Altman declined the offer from his one-time collaborator. Within weeks, Musk, who launched his own competing AI company, xAI, in July 2023, sued OpenAI, Altman, and other senior executives over OpenAI's move away from its original, non-profit mission.\n\nAltman's relationship with Musk has become increasingly tense over the years. As of February 2026, a trial is set to begin in April. In the interim, Musk and Altman have continued to trade barbs, including when OpenAI's CEO said that getting Musk under oath would be \"Christmas in April.\"\n\nSince his return, Altman has overseen a sweeping expansion of OpenAI's ambitions.\n\nIn October 2025, OpenAI completed its restructuring, spinning off its for-profit arm into a public benefit corporation. Microsoft retains a 27% stake in the for-profit venture, but the announcement formalized a shift in the relationship between the two companies.\n\nAltman has softened on some of his views as OpenAI seeks revenue, most notably by introducing ads to lower tiers of ChatGPT. In May 2024, Altman called ads \"a last resort for us as a business model.\"\n\nIn 2025 alone, OpenAI launched Atlas, its entry into the browser wars and Sora, its TikTok-esque AI video generation app. In May, Altman announced that he had been working with Ive on an AI-powered consumer device. OpenAI is also making waves in the payment space and is exploring making its own advanced chips.\n\nAltman also brought on former Instacart CEO Fidji Simo to serve as CEO of Applications.\n\nAll of this explains why Altman was one of eight architects of AI to be crowned as Time Magazine's 2025 Person of the Year.\n\nDespite Altman's status as CEO, he holds no equity in OpenAI — a status he has said he wished he had changed \"a long time ago.\"\n\n\"i think it would have led to far fewer conspiracy theories; people seem very able to understand 'ok that dude is doing it because he wants more money' but less so \"he just thinks technology is cool and he likes having some ability to influence the evolution of technology and society,\" Altman wrote on X in October 2025 in reply to a user who questioned what his motivations were if he doesn't stand to immediately profit of OpenAI goes public.\n\nInstead, Altman owes his billionaire status to his investments, namely in Stripe, Reddit, and Helion, a nuclear fusion firm.\n\nAfter Loopt, Altman founded a venture fund called Hydrazine Capital and raised $21 million, which included a large investment from venture capitalist Peter Thiel. Altman invested 75% of that money into YC companies and led Reddit's Series B fundraising round.\n\nAlong with his brothers Max and Jack, Altman launched a fund in 2020 called Apollo that is focused on funding \"moonshot\" companies. They're startups that are financially risky but could potentially pay off with a breakthrough development.\n\nIn 2021, Altman and cofounders Alex Blania and Max Novendstern launched a global cryptocurrency project called Worldcoin.\n\nAltman has said that his investment strategy is to look for \"somewhat broken companies.\"\n\n\"You can treat the warts on top, and because of the warts, the company will be hugely underpriced,\" he told The New Yorker in 2016.\n\nAltman married his partner, Oliver Mulherin, in January 2024. His husband is an Australian software engineer who previously worked at Meta, according to his LinkedIn profile.\n\nA few weeks after Forbes declared Altman a billionaire, he and Mulherin signed the Giving Pledge, vowing to give away most of their fortune.\n\nIn February 2025, Altman announced the birth of his son on social media.\n\n\"i have never felt such love,\" Altman said in his post.\n\nwelcome to the world, little guy!\n\nhe came early and is going to be in the nicu for awhile. he is doing well and it’s really nice to be in a little bubble taking care of him.\n\ni have never felt such love. pic.twitter.com/wFF2FkKiMU\n\nHe and his husband are expecting their second child later this year.\n\nAltman has found interesting — and expensive — ways to spend his free time.\n\nIn April 2024 (the same month he made Forbes' billionaire list), he was spotted in Napa, California, driving an ultra-rare Swedish supercar. The Koenigsegg Regera is seriously fast, able to go from zero to 250 miles per hour in less than 30 seconds. Only 80 of these cars are known to exist, and they can cost up to $4.65 million.\n\nHe once told two YC founders that he likes racing cars and had five, including two McLarens and an old Tesla, according to The New Yorker. He's said he likes racing cars and renting planes to fly all over California.\n\nSeparately, he told the founders of the startup Shypmate that, \"I prep for survival,\" and warned of either a \"lethal synthetic virus,\" AI attacking humans, or nuclear war. Altman is not alone in prepping for a potential doomsday.\n\n\"I try not to think about it too much,\" Altman told the founders in 2016. \"But I have guns, gold, potassium iodide, antibiotics, batteries, water, gas masks from the Israeli Defense Force, and a big patch of land in Big Sur I can fly to.\"",
    "readingTime": 9,
    "keywords": [
      "elon musk",
      "social media",
      "likes racing",
      "racing cars",
      "startup accelerator",
      "thiel altman",
      "in altman",
      "the new yorker",
      "openai altman",
      "board"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman",
    "thumbnail_url": "https://i.insider.com/698bc019d3c7faef0ece09a9?width=1200&format=jpeg",
    "created_at": "2026-02-16T12:38:07.501Z",
    "topic": "finance"
  },
  {
    "slug": "i-started-at-microsoft-as-an-executive-assistant-and-pivoted-to-an-ai-role-i-dont-regret-my-english-degree",
    "title": "I started at Microsoft as an executive assistant and pivoted to an AI role. I don't regret my English degree.",
    "description": "An AI gamification manager shares how she went from a contract executive assistant at Microsoft to an AI gamification product manager.",
    "fullText": "This as-told-to essay is based on a conversation with Brit Morenus, a 37-year-old senior AI gamification program manager, based in Charlotte, North Carolina. Her identity and employment have been verified by Business Insider. The following has been edited for length and clarity.\n\nI've been at Microsoft for a total of 13 years, but for five and a half, I was a contract worker.\n\nI graduated from college with a degree focused on English, communications, and marketing. I first landed a job at Microsoft as a contract executive assistant. I stayed in that role for about eight months, then joined the marketing team.\n\nEventually, I had the opportunity to take a really special position, but it required knowing gamification. Gamification is about integrating game mechanics and motivators, such as storytelling and reward systems, into learning. So I was going to teach people about our products and sell them in a gamified way.\n\nI spent about a year getting certifications that taught me about gamification. I upskilled and learned how to create games, what game mechanics are, and what motivates someone when they're learning.\n\nThat was the position where I was able to prove my impact, and they decided to bring me on full-time. I stayed in that role for another six years, training the frontline and customer service support to develop the right sales skills.\n\nEventually, I had the opportunity to start gamifying learning about AI. They wanted someone with gamification skills, and my certifications and experience made me the ideal candidate.\n\nI didn't know much about AI yet, aside from using it for personal reasons, but transitioning to an AI role was actually faster than pivoting to gamification. Since I held the gamification role for about six years, I became really good at it. It only took about three months for me to upskill in AI.\n\nIn my first three months on the team, I made myself knowledgeable about AI to the point where I could teach others about it. That's when I got a certification in Azure AI Fundamentals. It was a certification specific to how Microsoft's AI works.\n\nI helped my entire team get it, and then I helped my entire organization start working on it. Then I helped the greater customer service support organization work toward getting it as well.\n\nMy advice to those who want to transition would be: Don't let fear keep you from stepping outside your comfort zone. There's so much ambiguity about changing roles or companies, but there's no time like the present.\n\nWith AI specifically, you just need to learn. Everyone already uses it, but you need to understand how it works, because that's how you can understand what to do with it.\n\nIt's also important to upskill yourself. You have to be willing to constantly move and learn more, because it's going to keep changing — and faster than you can grasp it. Sometimes AI makes wrong predictions, but it is using words to make that prediction. So I absolutely need to use my English degree in order to figure out keywords and how to prompt it to do the right thing.\n\nUp until this Al role, I always joked that I wasn't using my English degree. But now I use it everywhere, and it truly does help. It helps with things like talking to executives and also with the role itself.\n\nIt's important to know the language of AI and how it operates. So now, more than ever, I am using every bit of my English degree and understanding English, grammar, and how it all functions.\n\nFor example, there's a tagging process that happens behind the scenes with AI, just like on social media. Looking at an image, it might tag it as a woman, or a supermarket, and that gives it a confidence score and tells you if it's relevant or not, and if it's what we're looking for.\n\nA lot of it is more about understanding how to apply the English language than about AI — so, thanks, Mom and Dad, I am using the degree you paid for.\n\nThis is part of an ongoing series about workers who transitioned into AI roles. Did you pivot to AI? We want to hear from you. Reach out to the reporter via email at aaltchek@insider.com or secure-messaging platform Signal at aalt.19.",
    "readingTime": 4,
    "keywords": [
      "english degree",
      "game mechanics",
      "customer service",
      "gamification",
      "role",
      "it's",
      "team",
      "learning",
      "there's",
      "based"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/microsoft-manager-explains-how-she-pivoted-from-admin-to-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/698f6614e1ba468a96ac0a21?width=1200&format=jpeg",
    "created_at": "2026-02-16T12:38:07.484Z",
    "topic": "finance"
  },
  {
    "slug": "tiktok-creator-bytedance-vows-to-curb-ai-video-tool-after-disney-threat",
    "title": "TikTok creator ByteDance vows to curb AI video tool after Disney threat",
    "description": "Videos created by new Seedance 2.0 generator go viral, including one of Tom Cruise and Brad Pitt...",
    "fullText": "Videos created by new Seedance 2.0 generator go viral, including one of Tom Cruise and Brad Pitt fighting\n\nByteDance, the Chinese technology company behind TikTok, has said it will restrain its AI video-making tool, after threats of legal action from Disney and a backlash from other media businesses, according to reports.\n\nThe AI video generator Seedance 2.0, released last week, has spooked Hollywood as users create realistic clips of movie stars and superheroes with just a short text prompt.\n\nSeveral big Hollywood studios have accused the tool of copyright infringement.\n\nOn Friday, Walt Disney reportedly sent a cease-and-desist letter to ByteDance which accused it of supplying Seedance with a “pirated library” of the studio’s characters, including those from Marvel and Star Wars, according to the US news outlet Axios.\n\nDisney’s lawyers claimed that ByteDance committed a “virtual smash-and-grab” of their intellectual property, according to a report from the BBC.\n\nHowever, the TikTok owner told the BBC it “respects intellectual property rights and we have heard the concerns regarding Seedance 2.0”.\n\nA spokesperson for the company told the broadcaster it was “taking steps to strengthen current safeguards as we work to prevent the unauthorised use of intellectual property and likeness by users”, but declined to provide further details on its plans.\n\nSeedance can generate videos based on just a few lines of text. Last week, Rhett Reese, the co-writer of Deadpool & Wolverine, Zombieland and Now You See Me: Now You Don’t, said “it’s likely over for us” after watching a widely disseminated AI-generated clip featuring Tom Cruise and Brad Pitt fighting.\n\nHe added: “In next to no time, one person is going to be able to sit at a computer and create a movie indistinguishable from what Hollywood now releases. True, if that person is no good, it will suck. But if that person possesses Christopher Nolan’s talent and taste (and someone like that will rapidly come along), it will be tremendous.”\n\nThe first iteration of Seedance launched in June last year.\n\nThe Motion Picture Association, the Hollywood trade association that represents studios such as Paramount, Warner Bros and Netflix, accused ByteDance of “unauthorised use of US copyrighted works on a massive scale”. The actors’ union Sag-Aftra has accused Seedance of “blatant infringement”.\n\nIt is the latest clash in Hollywood amid anxiety over the impact of AI on the future of entertainment. Artists and creative industries have called for compensation for the use of their material and the establishment of licensing frameworks to enable legal use of their content.\n\nLast year, Disney and NBCUniversal sued the AI image generator Midjourney over what the studios claimed were “endless unauthorised copies” of their works.\n\nHowever, creative companies are also making deals with AI businesses. Last year, Disney announced a $1bn equity investment in OpenAI, the developer of ChatGPT, and a three-year licensing agreement that enables its Sora video generation tool to use some of Disney’s characters.\n\nByteDance and Walt Disney were approached for comment.",
    "readingTime": 3,
    "keywords": [
      "tom cruise",
      "brad pitt",
      "pitt fighting",
      "walt disney",
      "intellectual property",
      "seedance",
      "hollywood",
      "accused",
      "generator",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/16/tiktok-bytedance-ai-video-tool-disney-seedance-tom-cruise-brad-pitt",
    "thumbnail_url": "https://i.guim.co.uk/img/media/85983881d8a2578c704db0d03da5453189e375fd/80_0_3749_3000/master/3749.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=22a630038291be0ff7b1c5e72464de4c",
    "created_at": "2026-02-16T12:38:04.177Z",
    "topic": "tech"
  },
  {
    "slug": "tech-titans-pour-50m-into-super-pac-to-elect-aifriendly-candidates-to-congress",
    "title": "Tech titans pour $50M into super PAC to elect AI-friendly candidates to Congress",
    "description": "Tech titans pour $50 million into super PAC to elect AI-friendly candidates to Congress",
    "fullText": "Some of the biggest names behind the artificial intelligence boom are looking to stack Congress with allies who support lighter regulation of the emerging technology by drawing on the crypto industry’s 2024 election success.\n\nMarc Andreessen, Ben Horowitz and OpenAI co-founder Greg Brockman are among tech leaders who’ve poured $50 million into a new super political action committee to help AI-friendly candidates prevail in November’s congressional races. Known as Leading the Future, the super PAC has taken center stage as voters grow increasingly concerned that AI risks driving up energy costs and taking away jobs.\n\nAs it launches operations, Leading the Future is deploying a strategy that worked two years ago for crypto advocates: talk about what’s likely to resonate with voters, not the industry or its interests and controversies. For AI, that means its ads won’t tout the technology but instead discuss core issues including economic opportunity and immigration — even if that means not mentioning AI at all.\n\n“They’re trying to be helpful in a campaign rather than talking about their own issue all the time,” said Craig Murphy, a Republican political consultant in Texas, where Leading the Future has backed Chris Gober, an ally of President Trump, in the state’s hotly contested 10th congressional district.\n\nThis year, the group plans to spend up to $125 million on candidates who favor a single, national approach to AI regulation, regardless of party affiliation. The election comes at a crucial moment for the industry as it invests hundreds of billions of dollars in AI infrastructure that will put fresh strains on resources, with new data centers already blamed for driving up utility bills.\n\nLeading the Future faces a growing challenge from AI safety advocates, who’ve started their own super PAC called Public First with a goal of raising $50 million for candidates who favor stricter oversight. On Thursday, Public First landed a $20-million pledge from Anthropic PBC, a rival to OpenAI that has set itself apart from other AI companies by supporting tougher rules.\n\nPolls show deepening public concern over AI’s impact on everything from jobs to education to the environment. Sixty-two percent of US adults say they interact with AI at least several times a week, and 58% are concerned the government will not go far enough in regulating it, according to the Pew Research Center.\n\nJesse Hunt, a Leading the Future spokesman, said the group is “committed to supporting policymakers who want a smart national regulatory framework for AI,” one that boosts US employment while winning the race against China. Hunt said the super PAC backs ways to protect consumers “without ceding America’s technological future to extreme ideological gatekeepers.”\n\nThe political and economic stakes are enormous for OpenAI and others behind Leading the Future, including venture capitalists Andreessen and Horowitz. Their firm, a16z, is the richest in Silicon Valley with billions of dollars invested in AI upstarts including coding startup Cursor and AI leaderboard platform LM Arena.\n\nFor now, their super PAC is doing most of the talking for the AI industry in the midterm races. Meta Platforms Inc. has announced plans for AI-related political spending on state-level contests, with $20 million for its California-based super PAC and $45 million for its American Technology Excellence Project, according to Politico.\n\nOther companies with massive AI investment plans — Amazon.com Inc., Alphabet Inc. and Microsoft Corp. — have their own corporate PACs to dole out bipartisan federal campaign donations. Nvidia Corp., the chip giant driving AI policy in Washington, doesn’t have its own PAC.\n\nTo ensure consistent messaging across party lines, Leading the Future has created two affiliated super PACs — one spending on Republicans and another on Democrats. The aim is to build a bipartisan coalition that can be effective in Washington regardless of which party is in power.\n\nTexas, home of OpenAI’s massive Stargate project, is one of the states where Leading the Future has already jumped in. Its Republican arm, American Mission, has spent nearly $750,000 on ads touting Gober, a political lawyer who’s previously worked for Elon Musk’s super PAC and is in a crowded GOP primary field for an open House seat.\n\nThe ads hail Gober as a “MAGA warrior” who “will fight for Texas families, lowering everyday costs.” Gober’s campaign website lists “ensuring America’s AI dominance” as one of his top campaign priorities. Gober’s campaign didn’t respond to requests for comment.\n\nIn New York, Leading the Future’s Democratic arm, Think Big, has spent $1.1 million on television ads and messages attacking Alex Bores, a New York state assemblyman who has called for tougher AI safety protocols and is now running for an open congressional seat encompassing much of central Manhattan.\n\nThe ads seize on Democrats’ revulsion over Trump’s immigration crackdown and target Bores for his work at Palantir Technologies Inc., which contracts with Immigration and Customs Enforcement. Think Big has circulated mailings and text messages citing Bores’ work with Palantir, urging voters to “Reject Bores’ hypocrisy on ICE.”\n\nIn an interview, Bores called the claims in the ads false, explaining that he left Palantir because of its work with ICE. He pointed out the irony that Joe Lonsdale, a Palantir co-founder who’s backed the administration’s border crackdown, is a donor to Leading the Future.\n\n“They’re not being ideologically consistent,” Bores said. “The fact that they have been so transparent and said, ‘Hey, we’re the AI industry and Alex Bores will regulate AI and that scares us,’ has been nothing but a benefit so far.”\n\nLeading the Future’s Democratic arm also plans to spend seven figures to support Democrats in two Illinois congressional races: former Illinois Representatives Jesse Jackson Jr. and Melissa Bean.\n\nLeading the Future is following the path carved by Fairshake, a pro-cryptocurrency super PAC that joined affiliates in putting $133 million into congressional races in 2024. Fairshake made an early mark by spending $10 million to attack progressive Katie Porter in the California Democratic Senate primary, helping knock her out of the race in favor of Adam Schiff, the eventual winner who’s seen as more friendly to digital currency.\n\nThe group also backed successful primary challengers against House incumbents, including Democrats Cori Bush in Missouri and Jamaal Bowman in New York. Both were rated among the harshest critics of digital assets by the Stand With Crypto Alliance, an industry group.\n\nIn its highest-profile 2024 win, Fairshake spent $40 million to help Republican Bernie Moreno defeat incumbent Democratic Senator Sherrod Brown, a crypto skeptic who led the Senate Banking Committee. Overall, it backed winners in 52 of the 61 races where it spent at least $100,000, including victories in three Senate and nine House battlegrounds.\n\nFairshake and Leading the Future share more than a strategy. Josh Vlasto, one of Leading the Future’s political strategists, does communications work for Fairshake. Andreessen and Horowitz are also among Fairshake’s biggest donors, combining to give $23.8 million last year.\n\nBut Leading the Future occasionally conflicts with Fairshake’s past spending. The AI group said Wednesday it plans to spend half a million dollars on an ad campaign for Laurie Buckhout, a former Pentagon official who’s seeking a congressional seat in North Carolina with calls to slash rules “strangling American innovation.” In 2024, during Buckhout’s unsuccessful run for the post, Fairshake spent $2.3 million supporting her opponent and eventual winner, Democratic Rep. Donald Davis.\n\n“The fact that they tried to replay the crypto battle means that we have to engage,” said Brad Carson, a former Democratic congressman from Texas who helped launch Public First. “I’d say Leading the Future was the forcing function.”\n\nUnlike crypto, proponents of stricter AI regulations have backers within the industry. Even before its contribution to Public First, Anthropic had pressed for “responsible AI” with sturdier regulations for the fast-moving technology and opposed efforts to preempt state laws.\n\nAnthropic employees have also contributed to candidates targeted by Leading the Future, including a total of $168,500 for Bores, Federal Election Commission records show. A super PAC Dream NYC, whose only donor in 2025 was an Anthropic machine learning researcher who gave $50,000, is backing Bores as well.\n\nCarson, who’s co-leading the super PAC with former Republican Rep. Chris Stewart of Utah, cites public polling that more than 80% of US adults believe the government should maintain rules for AI safety and data security, and says voter sentiment is on Public First’s side.\n\nPublic First didn’t disclose receiving any donations last year, according to FEC filings. But one of the group’s affiliated super PACs, Defend our Values PAC, reported receiving $50,000 from Public First Action Inc., the group’s advocacy arm. The PAC hasn’t yet spent any of that money on candidates.\n\nCrypto’s clout looms large in lawmakers’ memory, casting a shadow over any effort to regulate the big tech companies, said Doug Calidas, head of government affairs for AI safety group Americans for Responsible Innovation.\n\n“Fairshake was just so effective,” said Calidas, whose group has called for tougher AI regulations. “Democrats and Republicans are scared they’re going to replicate that model.”\n\nAllison and Birnbaum write for Bloomberg.",
    "readingTime": 8,
    "keywords": [
      "leading the future",
      "gober’s campaign",
      "future’s democratic",
      "democratic arm",
      "super pac",
      "eventual winner",
      "super pacs",
      "congressional seat",
      "congressional races",
      "affiliated super"
    ],
    "qualityScore": 1,
    "link": "https://www.latimes.com/business/story/2026-02-13/tech-titans-pour-50-million-into-super-pac-to-elect-ai-friendly-candidates-to-congress",
    "thumbnail_url": "https://ca-times.brightspotcdn.com/dims4/default/c25815c/2147483647/strip/true/crop/2000x1050+0+142/resize/1200x630!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F56%2F99%2Fee255e9bb4113c4405d6587127af%2F1x-1.jpg",
    "created_at": "2026-02-16T06:52:46.425Z",
    "topic": "politic"
  },
  {
    "slug": "llm-authz-audit-find-auth-gaps-and-prompt-injection-in-llm-apps",
    "title": "LLM AuthZ Audit – find auth gaps and prompt injection in LLM apps",
    "description": "Contribute to aiauthz/llm-authz-audit development by creating an account on GitHub.",
    "fullText": "aiauthz\n\n /\n\n llm-authz-audit\n\n Public\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n aiauthz/llm-authz-audit",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/aiauthz/llm-authz-audit",
    "thumbnail_url": "https://opengraph.githubassets.com/2af0fbe984f11b204b0ca3628f3219af36445de8e05eee61c4db3d909007cf54/aiauthz/llm-authz-audit",
    "created_at": "2026-02-16T06:52:46.085Z",
    "topic": "tech"
  },
  {
    "slug": "snowflakes-ceo-says-software-giants-risk-becoming-a-dumb-data-pipe-to-ai-models",
    "title": "Snowflake's CEO says software giants risk becoming a 'dumb data pipe' to AI models",
    "description": "\"The big model makers want to create a world in which all of the data for all of the enterprises is easily available to them,\" Sridhar Ramaswamy said.",
    "fullText": "The biggest software companies might be reduced to mere data sources, says Snowflake's CEO.\n\n\"The big model makers want to create a world in which all of the data for all of the enterprises is easily available to them,\" Sridhar Ramaswamy said on an episode of Alex Kantrowitz's \"Big Technology Podcast\" published last week. \"Everything else, the world, is just a dumb data pipe that feeds into that big brain.\"\n\nPrior to becoming Snowflake's CEO in 2024, Ramaswamy was a partner at Greylock Ventures and cofounded AI search startup Neeva, which was acquired by Snowflake.\n\nRamaswamy added that Snowflake needs to operate with a \"fear\" that people would stop using AI agents developed by software companies and instead want an all-inclusive agent that has data from Snowflake, for example, and everywhere else\n\nHe said his solution was to let customers take the lead and decide how they want to access their data — directly through their own agents, or through a product like ChatGPT.\n\nIn the last few months, AI labs have evolved from being sources of AI infrastructure to becoming software providers themselves. OpenAI has entered the sales, support, and document analysis market, threatening incumbents such as Salesforce and Oracle.\n\nOn a podcast released last week, Andreessen Horowitz general partner Anish Acharya said software firms were being unnecessarily punished by Wall Street over fears that AI could take over their industry. The VC said that legacy software could not be replaced so easily, because it would not be worth it to use AI for every business function.\n\nHe said that software accounts for 8% to 12% of a company's expenses, so vibe coding to build the company's resource planning or payroll tools would only save about 10%. Instead, companies should focus on big-ticket items, like developing their core businesses or optimizing other costs.\n\nRamaswamy and Acharya's comments follow a brutal start of the month for software stocks, which dragged down tech and broader markets. The sell-off started when already-wary investors panicked about Anthropic's new AI tool, which can perform a range of clerical tasks for people working in the legal industry.",
    "readingTime": 2,
    "keywords": [
      "software",
      "easily",
      "podcast",
      "else",
      "partner",
      "agents",
      "instead",
      "industry",
      "company's",
      "ramaswamy"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/snowflake-ceo-sridhar-ramaswamy-software-dumb-data-pipe-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/6992a117a645d118818966b3?width=1200&format=jpeg",
    "created_at": "2026-02-16T06:52:41.862Z",
    "topic": "tech"
  },
  {
    "slug": "bytedance-pledges-to-prevent-unauthorised-ip-use-on-ai-video-tool-after-disney-threat",
    "title": "ByteDance pledges to prevent unauthorised IP use on AI video tool after Disney threat",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/disney-sends-ceaseanddesist-to-bytedance-over-aigenerated-videos-4507348",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1F057_L.jpg",
    "created_at": "2026-02-16T06:52:41.157Z",
    "topic": "finance"
  },
  {
    "slug": "margindash-see-which-ai-customers-are-profitable",
    "title": "MarginDash – See which AI customers are profitable",
    "description": "Track AI cost and margin per customer. Real-time profitability insights, Stripe revenue sync, budget alerts, and a cost simulator to find cheaper models without changing code.",
    "fullText": "What-if analysis\n Find cheaper models without changing code\n Pick any event type. MarginDash simulates the cost of alternative models — ranked by intelligence-per-dollar — and shows you the savings instantly.\n\n Smart suggestions ranked by quality per dollar\n\n Benchmark scores (MMLU-Pro, GPQA) side by side\n\n Frontier, mid-tier, and budget model tiers\n\n Total Cost\n $2,380\n\n Simulated Cost\n $1,800\n\n Cost Difference\n -$580\n\n Cost by Event Type\n Click any event type to swap its model.\n\n Event Type\n Events\n Cost\n Simulated\n Difference\n\n summarize\n\n 1,240\n $820\n $580\n -$240\n\n translate\n\n 890\n $640\n $420\n -$220\n\n chat\n\n 760\n $380\n $260\n -$120\n\n 420\n $540\n -\n -\n\n summarize\n\n Smart Recommendation\n\n Switch to\n Claude 4.5 Haiku\n Est. saving $240 (29.3%)\n\n Simulate with\n Search...",
    "readingTime": 1,
    "keywords": [
      "models",
      "ranked",
      "model",
      "simulated",
      "difference",
      "summarize",
      "event",
      "smart"
    ],
    "qualityScore": 0.65,
    "link": "https://margindash.com/",
    "thumbnail_url": "https://margindash.com/images/og-image.png",
    "created_at": "2026-02-16T01:12:20.147Z",
    "topic": "tech"
  },
  {
    "slug": "nodejs-native-module-for-integrating-mediasoup-with-audio-ai-models",
    "title": "Node.js native module for integrating Mediasoup with Audio AI models",
    "description": "Tools for consuming and publishing PCM audio data from an RTP stream - Hilokal/audio-rtp-tools",
    "fullText": "Hilokal\n\n /\n\n audio-rtp-tools\n\n Public\n\n Tools for consuming and publishing PCM audio data from an RTP stream\n\n License\n\n ISC license\n\n 4\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Hilokal/audio-rtp-tools",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Hilokal/audio-rtp-tools",
    "thumbnail_url": "https://opengraph.githubassets.com/a6f0420e81edea6001f9e9039c9da1b4ed2861be0909541ddbbc1dacad4374cc/Hilokal/audio-rtp-tools",
    "created_at": "2026-02-16T01:12:20.128Z",
    "topic": "tech"
  },
  {
    "slug": "the-first-signs-of-burnout-are-coming-from-the-people-who-embrace-ai-the-most",
    "title": "The first signs of burnout are coming from the people who embrace AI the most",
    "description": "Because employees could do more, work began bleeding into lunch breaks and late evenings. The employees' to-do lists expanded to fill every hour that AI freed up, and then kept going.",
    "fullText": "The most seductive narrative in American work culture right now isn’t that AI will take your job. It’s that AI will save you from it.\n\nThat’s the version the industry has spent the last three years selling to millions of nervous people who are eager to buy it. Yes, some white-collar jobs will disappear. But for most other roles, the argument goes, AI is a force multiplier. You become a more capable, more indispensable lawyer, consultant, writer, coder, financial analyst — and so on. The tools work for you, you work less hard, everybody wins.\n\nBut a new study published in Harvard Business Review follows that premise to its actual conclusion, and what it finds there isn’t a productivity revolution. It finds companies are at risk of becoming burnout machines.\n\nAs part of what they describe as “in-progress research,” UC Berkeley researchers spent eight months inside a 200-person tech company watching what happened when workers genuinely embraced AI. What they found across more than 40 “in-depth” interviews was that nobody was pressured at this company. Nobody was told to hit new targets. People just started doing more because the tools made more feel doable. But because they could do these things, work began bleeding into lunch breaks and late evenings. The employees’ to-do lists expanded to fill every hour that AI freed up, and then kept going.\n\nAs one engineer told them, “You had thought that maybe, oh, because you could be more productive with AI, then you save some time, you can work less. But then really, you don’t work less. You just work the same amount or even more.”\n\nOver on the tech industry forum Hacker News, one commenter had the same reaction, writing, “I feel this. Since my team has jumped into an AI everything working style, expectations have tripled, stress has tripled and actual productivity has only gone up by maybe 10%. It feels like leadership is putting immense pressure on everyone to prove their investment in AI is worth it and we all feel the pressure to try to show them it is while actually having to work longer hours to do so.”\n\nIt’s fascinating and also alarming. The argument about AI and work has always stalled on the same question — are the gains real? But too few have stopped to ask what happens when they are.\n\nThe researchers’ new findings aren’t entirely novel. A separate trial last summer found experienced developers using AI tools took 19% longer on tasks while believing they were 20% faster. Around the same time, a National Bureau of Economic Research study tracking AI adoption across thousands of workplaces found that productivity gains amounted to just 3% in time savings, with no significant impact on earnings or hours worked in any occupation. Both studies have gotten picked apart.\n\nThis one may be harder to dismiss because it doesn’t challenge the premise that AI can augment what employees can do on their own. It confirms it, then shows where all that augmentation actually leads, which is “fatigue, burnout, and a growing sense that work is harder to step away from, especially as organizational expectations for speed and responsiveness rise,” according to the researchers.\n\nThe industry bet that helping people do more would be the answer to everything, but it may turn out to be the beginning of a different problem entirely. The research is worth reading, here.",
    "readingTime": 3,
    "keywords": [
      "industry",
      "tools",
      "less",
      "productivity",
      "research",
      "researchers",
      "isn’t",
      "it’s",
      "save",
      "argument"
    ],
    "qualityScore": 1,
    "link": "https://techcrunch.com/2026/02/09/the-first-signs-of-burnout-are-coming-from-the-people-who-embrace-ai-the-most/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2022/10/GettyImages-1158287360-e1665956231123.jpg?resize=1200,676",
    "created_at": "2026-02-16T01:12:18.520Z",
    "topic": "tech"
  },
  {
    "slug": "rampant-ai-demand-for-memory-is-fueling-a-growing-chip-crisis",
    "title": "Rampant AI demand for memory is fueling a growing chip crisis",
    "description": "The cost of one type of DRAM soared 75% from December to January, accelerating price hikes throughout the holiday quarter.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/15/ai-demand-memory-chip-shortage-crisis-dram-hbm-micron-skhynix-samsung/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/02/GettyImages-2251983263-e1771201699839.jpg?resize=1200,600",
    "created_at": "2026-02-16T01:12:15.403Z",
    "topic": "business"
  },
  {
    "slug": "lawsuits-or-billiondollar-deals-how-disney-picks-its-ai-copyright-battles",
    "title": "Lawsuits or billion-dollar deals: How Disney picks its AI copyright battles",
    "description": "Disney sent ByteDance a cease-and-desist for using its characters on Seedance. When OpenAI's Sora did it, however, Disney struck a deal.",
    "fullText": "No, Disney did not release footage of a never-before-seen fight sequence between Marvel's Wolverine and Thanos (spoiler: Thanos won).\n\nThat clip, which amassed over 142,000 views on X over 48 hours, was created using Seedance 2.0, an AI video generation model that ByteDance debuted last week. The tool created a buzz on social media, where one user made a hyperrealistic AI video of Tom Cruise and Brad Pitt fighting over Jeffrey Epstein.\n\nByteDance's decision to let users create content based on Disney's IP without permission isn't all that surprising given the AI industry's well-established strategy to \"ask for forgiveness, not permission.\"\n\nDisney, which is infamous for aggressively protecting its intellectual property, isn't having it — though how it responds to the threats is not always the same.\n\nOn Friday, the entertainment company sent ByteDance, the Chinese company that owns Seedance and TikTok, a cease-and-desist letter, a source familiar with the matter confirmed for Business Insider.\n\nIn the letter, Disney accused ByteDance of supplying Seedance 2.0 with \"a pirated library of Disney's copyrighted characters from Star Wars, Marvel, and other Disney franchises, as if Disney's coveted intellectual property were free public domain clip art.\"\n\n\"Over Disney's well-publicized objections, ByteDance is hijacking Disney's characters by reproducing, distributing, and creating derivative works featuring those characters,\" the letter said.\n\nSeedance is only the latest AI company Disney says is ripping it off.\n\nDisney and NBCUniversal sued Midjourney, an AI image generator, in June last year. In the lawsuit, the companies compared Midjourney's tech to \"a virtual vending machine, generating endless unauthorized copies of Disney's and Universal's copyrighted works.\"\n\nThen Disney accused Character.AI of copyright infringement in a September cease-and-desist letter last September. In December, it sent one to Google in response to the AI image generator Nano Banana Pro and its other AI models, accusing the Big Tech giant of stealing its IP on a \"massive scale.\" Both companies have since removed Disney characters from their platforms.\n\nDisney is not anti-AI, however, and its strategy is not one-size-fits-all. The company took a much less adversarial approach with OpenAI, the world's leading AI startup.\n\nWhen OpenAI debuted Sora 2, an AI-powered text-to-video platform, in September, users began uploading IP-heavy content featuring Disney characters to social media. Instead of a cease-and-desist letter or legal action, though, Disney negotiated a deal.\n\nAlthough Disney hasn't shared plans to develop its own AI model or video generator, Disney CEO Bob Iger said the company ultimately sees the tech not as a threat but as a new path to connect with audiences.\n\nDuring an earnings call late last year, he said AI would \"provide users of Disney+ with a much more engaged experience, including the ability for them to create user-generated content, and to consume user-generated content, mostly short form, from others.\"",
    "readingTime": 3,
    "keywords": [
      "social media",
      "intellectual property",
      "disney accused",
      "user-generated content",
      "cease-and-desist letter",
      "disney characters",
      "users",
      "generator",
      "clip",
      "created"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/disney-ai-copyright-battles-seedance-nano-banana-sora-midjourney-2026-2",
    "thumbnail_url": "https://i.insider.com/699111dbe1ba468a96ac1af1?width=1200&format=jpeg",
    "created_at": "2026-02-16T01:12:15.000Z",
    "topic": "finance"
  },
  {
    "slug": "elon-musk-says-anthropics-philosopher-has-no-stake-in-the-future-because-she-doesnt-have-kids-heres-her-response",
    "title": "Elon Musk says Anthropic's philosopher has no stake in the future because she doesn't have kids. Here's her response.",
    "description": "Elon Musk questioned Amanda Askell's role in shaping AI Claude's morals, citing her lack of children. Askell had thoughts.",
    "fullText": "Anthropic famously employs a Scottish philosopher named Amanda Askell.\n\nHer job is to imbue its chatbot, Claude, with a personality and a set of moral guardrails. She is essentially teaching it to be cool and good.\n\nElon Musk, however, doesn't think she's qualified.\n\n\"Those without children lack a stake in the future,\" Musk posted on X in response to a profile of Askell published by The Wall Street Journal.\n\nThe Journal profile does not say whether Askell has kids. Musk, who has imbued his own chatbot, Grok, with a distinct personality, has 14 of them. Musk is known for promoting a brand of pronatalism that's become popular among Silicon Valley elites.\n\nAskell responded with her trademark dry intellectualism.\n\n\"I think it depends on how much you care about people in general vs. your own kin,\" Askell wrote. \"I do intend to have kids, but I still feel like I have a strong personal stake in the future because I care a lot about people thriving, even if they're not related to me.\"\n\n\"I think caring about your children can make you feel invested in the future in a new and very profound way, and I do understand people wanting to convey that,\" she added.\n\nThe responses to their short back-and-forth were as varied as you might expect on Musk's social media network. A day later, Askell posted again.\n\n\"I'm too right wing for the left and I'm too left wing for the right,\" she said. \"I'm too into humanities for those in tech and I'm too into tech for those in the humanities. What I'm learning is that failing to polarize is itself quite polarizing.\"",
    "readingTime": 2,
    "keywords": [
      "chatbot",
      "personality",
      "children",
      "stake",
      "posted",
      "profile",
      "journal",
      "kids",
      "care",
      "wing"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/elon-musk-anthropic-philosopher-amanda-askell-debate-2026-2",
    "thumbnail_url": "https://i.insider.com/69925d65e1ba468a96ac1d3f?width=1200&format=jpeg",
    "created_at": "2026-02-16T01:12:14.920Z",
    "topic": "finance"
  },
  {
    "slug": "sam-altman-says-openclaw-creator-peter-steinberger-is-joining-openai-to-build-nextgen-personal-agents",
    "title": "Sam Altman says OpenClaw creator Peter Steinberger is joining OpenAI to build next-gen personal agents",
    "description": "Sam Altman said OpenClaw creator Peter Steinberger is joining OpenAI to drive development of personal AI agents.",
    "fullText": "OpenAI just scored a win in the AI talent wars.\n\nSam Altman said Sunday on X that Peter Steinberger, the creator of OpenClaw, the viral AI agent powering the agent-only social network Moltbook, is joining OpenAI.\n\nAltman said Steinberger would build the \"next generation\" of personal AI agents at the company.\n\n\"He is a genius with a lot of amazing ideas about the future of very smart agents interacting with each other to do very useful things for people,\" Altman said about Steinberger. \"We expect this will quickly become core to our product offerings.\"\n\nAltman added that OpenClaw, which was for a brief moment in time known as Moltbot and then Clawdbot before Anthropic took notice, will live on as an open-source project supported by OpenAI.\n\n\"The future is going to be extremely multi-agent and it's important to us to support open source as part of that,\" he wrote.\n\nSteinberger, previously best known for founding the PDF processing company PSPDFKit, came out of retirement to launch OpenClaw in late 2025.\n\nHe is likely to bring a new perspective to OpenAI's race to develop artificial general intelligence. Steinberger said he believes AGI is best as a specialized form of intelligence rather than a generalized one.\n\n\"What can one human being actually achieve? Do you think one human being could make an iPhone or one human being could go to space?\" Steinberger said on a Y Combinator podcast in February. \"As a group we specialize, as a larger society we specialize even more.\"",
    "readingTime": 2,
    "keywords": [
      "openclaw",
      "human",
      "agents",
      "intelligence",
      "specialize",
      "steinberger",
      "altman",
      "openai"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/sam-altman-hires-openclaw-creator-peter-steinberger-personal-ai-agents-2026-2",
    "thumbnail_url": "https://i.insider.com/699249bde1ba468a96ac1d07?width=1200&format=jpeg",
    "created_at": "2026-02-16T01:12:14.917Z",
    "topic": "finance"
  },
  {
    "slug": "i-got-laid-off-from-amazon-after-11-years-my-high-school-daughter-taught-me-the-biggest-lesson-on-how-to-move-forward",
    "title": "I got laid off from Amazon after 11 years. My high school daughter taught me the biggest lesson on how to move forward.",
    "description": "An ex-Amazon manager says his layoff in October 2025 was a shock, but he's using this time to build AI skills, focus on his health, and apply to jobs.",
    "fullText": "Hemant Virmani was laid off from Amazon during the October 2025 round of layoffs.\n\nHe's using this time to learn new AI skills while applying to engineering roles — and exercising.\n\nHis teenage daughter has inspired him to stay positive, keep his cool, and focus on the future.\n\nThis as-told-to essay is based on a conversation with Hemant Virmani, a 47-year-old tech professional based in Washington. It's been edited for length and clarity.\n\nAmazon was part of my daily life for 11.5 years, and suddenly it was gone.\n\nThere's no right way or easy way to do layoffs. I watched my team members get laid off in 2023, and I know how difficult it is. Still, when I received an email in the middle of the night in October 2025 saying I'd been laid off from my senior software development manager position, I was shocked.\n\nWatching my teenage daughter navigate her own difficult situation taught me the biggest lesson in how to move forward well. Now I'm applying to jobs and working on upskilling in AI so I can be proactive, not reactive, to the tech industry.\n\nOnly time will tell if this layoff is a blessing in disguise, but for now, it has led to a refreshing change.\n\nI loved my time at Amazon, and I really feel as though it's a place for exceptional people. The number of quality brains in the office, throwing around ideas and solving a custom problem, was amazing.\n\nThe morning after my layoff, I had a mandated 30-minute meeting with my manager, and it actually went very well. We talked about the layoff, and he offered me support. He delivered it all to me in a very positive, human way, and it was really affirming.\n\nAn old manager also reached out to meet me at a local coffee shop the next day to spend time together and check in on my state of mind. I think he wanted to go about the layoffs right, which isn't easy to do.\n\nI felt attached to the layoff for the first few days; however, I knew there was no way to control what happened — I could only control how I reacted to it.\n\nMy daughter is a senior in high school, and she had an adverse situation happen to her last year that required recovery. How she reacted in that difficult time inspired me. Her mental model was: \"Challenges don't have to keep me from showing up for myself or for others.\" Her positive attitude was inspiration for me to do the same.\n\nI kind of learned from her that I had to take this layoff with positivity, keep my cool, and focus on what was next.\n\nA couple of weeks later, I lost my father and spent the next month in India supporting my family. I took about a month to settle my mind, reflect on what I wanted next for my career, and help my daughter finish her college essays.\n\nIt's been a very refreshing change to think about what I want next in my engineering career. I'm less focused on the size or name of the next company I work for, and more on what I'd be doing there. I'm looking forward to hopefully heading the engineering for something that has a great impact on customers. Right now, I don't think that can be done without AI, so I'm working on upskilling.\n\nI want to be proactive, not reactive, about the AI skills I'll need in the future. My team at Amazon used some AI tools, so I'm familiar with some, but I was only able to spend a fraction of my workday using them. Now, I'm building those skills myself.\n\nI started working on a hobby AI project a couple of weeks ago, to go hands-on with AI and be more grounded in the reality of what the AI landscape is like right now. It's been different, and a refreshing change, to build something myself rather than to study it, read about it, or work on a team developing it.\n\nWhen I had a job, it was easy for my first priority to be work. Now I'm making sure that my top priority is my health. I've been going to the gym four or five days a week, and I'm refining a health plan to follow even after I start working again.\n\nOnce I'm done at the gym, my time is a 50/50 split between learning AI and applying to jobs or networking. I'm applying for head of engineering roles where I'd own significant impactful initiatives, averaging 2-3 applications every week.\n\nI made a post on LinkedIn about my layoff, and I received so many supportive comments, texts, and calls from people — some I hadn't talked to in decades. Someone from college whom I hadn't talked to in over 25 years reached out, and it was so nice. It felt like we never disconnected. I've also had multiple job leads come from my post, which I'm following up on.\n\nAs of now, I have some worries about when I'll find my next job, but this time has given me the ability to work on things I wasn't able to before. I'm making sure I spend this time with a lot of positivity, not letting negative thoughts come around.\n\nMy advice to anyone undergoing layoffs is to realize that layoffs are not about you. It's about an environment that is driving layoffs. Secondly, now that this has happened, you can't go back in the past and change it. Look forward to what you can do next. How you react is very important.\n\nDo you have a story to share about being laid off from Amazon? If so, please reach out to the reporter at tmartinelli@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "hadn't talked",
      "i'm applying",
      "engineering roles",
      "teenage daughter",
      "now i'm",
      "hemant virmani",
      "layoffs",
      "layoff",
      "laid",
      "skills"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/lifestyle/articles/got-laid-off-amazon-11-094801905.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/6BlJt55yJsObe27UgJv_hw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/business_insider_consolidated_articles_886/3d331103c2338b76dd556695030a67b3",
    "created_at": "2026-02-16T01:12:12.796Z",
    "topic": "news"
  },
  {
    "slug": "pinchtab-12mb-go-binary-for-ai-browser-for-openclaw",
    "title": "Pinchtab – 12MB Go Binary for AI Browser for OpenClaw",
    "description": "Contribute to pinchtab/pinchtab development by creating an account on GitHub.",
    "fullText": "pinchtab\n\n /\n\n pinchtab\n\n Public\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n pinchtab/pinchtab",
    "readingTime": 1,
    "keywords": [
      "pinchtab",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/pinchtab/pinchtab",
    "thumbnail_url": "https://opengraph.githubassets.com/75e2caa4ffe020d07290499d7d61063c4d63954910f4e1b17c8fb5beb40dd0a0/pinchtab/pinchtab",
    "created_at": "2026-02-15T18:22:10.915Z",
    "topic": "tech"
  },
  {
    "slug": "the-neurodata-bottleneck-why-neuroai-interfacing-breaks-the-modern-data-stack",
    "title": "The Neuro-Data Bottleneck: Why Neuro-AI Interfacing Breaks the Modern Data Stack",
    "description": "Neural data like EEG and MRI is never 'finished' - it's meant to be revisited as new ideas and methods emerge. Yet most teams are stuck in a multi-stage ETL nightmare. Here's why the modern data stack fails the brain.",
    "fullText": "Neural data like EEG and MRI is never \"finished\" - it's meant to be revisited as\nnew ideas and methods emerge. Yet most teams are stuck in a multi-stage ETL\nnightmare, downloading massive blobs just to extract a single signal or\nrecomputing a new one. Between the struggle to access raw signals and the\nengineering hell of re-mining legacy data at scale, scientists are left waiting\non infrastructure instead of doing science. Here is why the modern data stack\nfails the brain.\n\nYour typical data stack thrives on tabular data. SQL databases, Spark,\nSnowflake - they want structured rows and columns. But in neuro-tech and BCI\nresearch, the \"row\" is a nightmare of Heterogeneous Laboratory Outputs:\n\nThe problem isn't the storage medium (whether cloud or local clusters); it's\nthat this raw data is inaccessible to modern tools. Suddenly, joining a\npatients table with a scans table isn't a LEFT JOIN; it's a multi-stage ETL\nnightmare. You can't just SELECT * FROM neural_scans WHERE patient_id = 'X'\nand expect a useful result. You have to locate the file, download the entire\nmassive blob, and load it into a specialized library just to extract a single\nsignal. This complexity often leaves researchers treating their data as a \"black\nbox,\" focusing on high-level outputs because the underlying raw signals are too\ncumbersome to touch directly.\n\nThis \"download-then-process\" loop is the primary culprit behind slow iteration\nand high I/O costs. It's the Scientific Data Dilemma: rich, complex data\nthat's hell to interact with programmatically at scale. Furthermore, the real\nvalue of these high-volume streams - EEG, MRI, and video - is that they are\nnever \"finished.\" They are assets to be revisited repeatedly as new methods and\nhypotheses emerge.\n\nImagine if you could treat your raw DICOMs, NIfTIs, and EEG files like entries\nin a database, directly from storage, without moving or duplicating them. This\nis the core architectural shift we need.\n\nInstead of an ETL pipeline that copies terabytes into a new format, a \"zero-ETL\"\ndata layer operates by Metadata-First Indexing and Selective I/O. This\narchitecture addresses the significant cost curve of neuro-data by providing\ntools to optimize reuse. By storing intermediate representations, extracted\nfeatures, and supporting gradual, staged processing, researchers can build upon\nprevious work without re-running expensive raw-data ingestions and duplicating\ndata.\n\nA service scans your storage buckets, extracting crucial headers and\nexperimental parameters directly from the raw files. This creates a fast,\nqueryable index of what's inside the files without ever moving them.\n\nThis approach changes the game. Your data stays in your storage (behind your\nVPC, under your IAM policies), but it becomes instantly addressable via a\nPythonic API. No more manual exports or multi-week ingestion jobs just to start\nan experiment.\n\nThe neuro-tech industry is currently in a race to find the \"Scaling Laws\" for\nthe brain. Much like the evolution of LLMs, the hypothesis is that by scaling\ndata bandwidth, model capacity, and signal diversity, we can unlock a\nhigh-fidelity interface between biological and artificial intelligence. However,\nthis scale is hitting a massive engineering wall.\n\nAll of these approaches share a common bottleneck: the data stack. In most neuro\nteams today, data engineering is the single greatest bottleneck, forcing\nbrilliant scientists to wait on infrastructure instead of doing science. The\nchallenge isn't just vertical speed (optimizing one study). It is the horizontal\nengineering hell: the need to retroactively re-process petabytes of\nhistorical data every time a new hypothesis or de-noising logic is developed.\nDoing this at scale, while maintaining perfect traceability, is where research\nmeets infrastructure reality.\n\nWe are asking researchers to find scaling laws using tools designed for CSVs and\nSQL tables. When your primary data is a 2GB 3D volume or a high-frequency\nbiochemical stream, the \"download-then-process\" workflow is a death sentence for\niteration. Without equipping researchers with new, \"Zero-Copy\" tools that\ntreat multimodal biological signals as first-class objects, the breakthrough\n\"Merge\" remains mathematically out of reach.\n\nData scientists in biotech live in Python. They need numpy, pandas, scipy,\nand pytorch. The challenge is making these tools scale across terabytes of\nunstructured binary data. To determine how neural bandwidth scales with model\ncapacity, we need to move beyond black-box ML and utilize Biophysical\nModeling to encode the priors of how neurons actually interact.\n\nThis requires a data layer that remains \"Python-native\":\n\nThe data could be reused in the future for analytics as well as model training:\n\nData engineers often deal with \"blind spots.\" In neuro-research, visualization\nis a unit test. Without it, researchers are forced to trust their pipelines\nblindly, unable to see the artifacts or noise that might be skewing their\nresults. A dataset-centric approach integrates inline visualization across\nthe entire data lineage, allowing you to click on an entry and view the raw 3D\nscan or EEG signal right in your browser. This instant feedback loop reduces\ndebugging time from hours to seconds.\n\nFurthermore, when every transformation and parameter is automatically tracked\nand versioned as part of the data layer, reproducibility becomes a\nbyproduct, not a chore. Any result can be re-computed exactly as it was\nproduced, bolstering scientific rigor and audit readiness without additional\noverhead.\n\nThe future of neuro-engineering isn't about moving more data faster; it's about\nmaking data accessible without movement. Solving the horizontal iteration\nproblem - where research hypotheses meet the \"engineering hell\" of scale and\ntraceability - is the only way to shorten the loop from raw signal to discovery.\n\nBecause high-fidelity signals like MRI and EEG are meant to be mined multiple\ntimes from different angles, our infrastructure must treat them as living\nassets. Whether you are scaling sensor counts at\nNeuralink or molecular diversity at\nMerge Labs, your velocity is ultimately limited by\nyour data plumbing.\n\nWhen we build infrastructure that lives with the data and orchestrates compute\nresources directly where the signals reside, we stop being data gatekeepers and\nstart becoming true enablers of the human-AI future.\n\nWhat do you think, data engineers? Are we ready to move beyond the \"Modern\nData Stack\" to support the complexity of the human brain?",
    "readingTime": 6,
    "keywords": [
      "multi-stage etl",
      "etl nightmare",
      "doing science",
      "model capacity",
      "engineering hell",
      "infrastructure instead",
      "raw signals",
      "without",
      "scale",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://datachain.ai/blog/neuro-data-bottleneck",
    "thumbnail_url": "https://datachain.ai/blog/images/2026-01-25/neuro-data-bottleneck.jpg",
    "created_at": "2026-02-15T18:21:54.623Z",
    "topic": "tech"
  },
  {
    "slug": "the-sweet-lesson-of-neuroscience",
    "title": "The Sweet Lesson of Neuroscience",
    "description": "Scientists once hoped that studying the brain would teach us how to build AI. Now, one AI researcher may have something to teach us about the brain.",
    "fullText": "Scientists once hoped that studying the brain would teach us how to build AI. Now, one AI researcher may have something to teach us about the brain.\n\nIn the early years of modern deep learning, the brain was a North Star. Ideas like hippocampal replay — the brain’s way of rehearsing past experience — offered templates for how an agent might learn from memories. Meanwhile, work on temporal-difference learning showed that some dopamine neuron responses in the brain closely parallel reward-prediction errors — solidifying a useful framework for reinforcement learning.\n\nDeepMind’s 2013 Atari-playing breakthrough was perhaps the high-water mark of brain-inspired optimism. The system was in part a digital echo of hippocampal replay and dopamine-based learning. DeepMind’s CEO gave talks in the early days with titles like “A systems neuroscience approach to building AGI.”\n\nBut I believe the brain may have something more to teach us about AI — and that, in the process, AI may have quite a bit to teach us about the brain. Modern AI research centers on three key ingredients: architectures, learning rules, and training signals. The first two — how to build up complex patterns of information from simple ones and how to learn from errors to produce useful patterns — have been substantially mastered by modern AI. But the third factor — what training signals (typically called “loss” functions, “cost” functions, or “reward”) should drive learning — remains deeply underexplored. And that, I think, is where neuroscience still has surprises left to deliver.\n\nI’ve been fascinated by this question since 2016, when advances in artificial deep learning led me to propose that the brain probably has many highly specific cost functions built by evolution that might train different parts of the cerebral cortex to help an animal learn exactly what it needs to in its ecological niche.\n\nMore recently, Steve Byrnes, a physicist turned AI safety researcher, has shed new light on the question of how the brain trains itself. In a remarkable synthesis of the neuroscience literature, Byrnes recasts the entire brain as two interacting systems: a learning subsystem and a steering subsystem. The first learns from experience during the animal’s lifetime — a bit like one of AI’s neural networks that starts with randomly initialized “weights,” or “parameters,” inside the network, which are adjusted by training. The second is mostly hardwired and sets the goals, priorities, and reward signals that shape that learning. A learning machine — like a neural network — can learn almost anything; the steering subsystem determines what it is being asked to learn.\n\nByrnes’ work suggests that some of the most relevant insights in AI alignment will come from neuroscientific frameworks about how the steering system teaches and aligns the learner from within. I agree. This perspective is the seed of what we might call the “sweet lesson” of neuroscience.\n\nSo let’s talk about Byrnes, and brains. In Byrnes’ view, brains have two main parts: a learning subsystem and a steering subsystem.\n\nThe learning subsystem consists primarily of the neocortex, hippocampus, cerebellum, and striatum. It’s the part of the brain that develops a model of the world, generates plans of action, and predicts how well they’ll work. When we’re born, it isn’t able to produce much in the way of useful outputs, but it learns continuously, over time, to find patterns in the world, and then more abstract patterns among those patterns, until it becomes very useful indeed.\n\nThis reflects the “cortical uniformity” idea popular in neuroscience: While the neocortex handles most of our complex cognition, its own structure is relatively simple and consistent. In other words, despite cortical organization into functional substructures, the neocortex's capacity to support complex thought comes only partially from preexisting architecture, with much of its output the result of learning and experience. Instead, the cortex is a learning machine that starts out a bit like an uninitialized neural network — one that has equal potential to be trained to drive a car or to generate language or any number of other things.\n\nThen there’s the steering subsystem, which consists of the hypothalamus and brainstem, with contributions from other regions like the pallidum. Unlike the learning subsystem, the steering subsystem is relatively static — a fixed set of rules “hand-coded” by evolution early in the history of our species. These structures, like most in the brain, have some plasticity throughout life, but their rewiring capacity is dwarfed by that of structures in the learning subsystem, such as the neocortex. These rules control (or “steer”) what the learning subsystem is being asked to learn from, and when it is supposed to learn it. Evolution, in other words, didn’t just build a learner. It built a teacher inside the same brain.\n\nThe steering subsystem influences the learning subsystem through “supervision signals.” These are analogous to the cost or reward functions used to train today’s AI, but are much more diverse, elaborate, and species specific. It also handles innate reflexes that have to be present from birth, before the learning subsystem has had a chance to discover the world’s patterns.\n\nThe steering subsystem doesn’t simply hand out rewards for evolution’s ultimate goals of survival or reproduction. Those would come far too late to shape an animal's behavior starting early in its life: By the time the animal learned from these signals, it would already be dead or, at best, abjectly failing to mate.\n\nInstead, evolution built an intricate scaffold of intermediate rewards. Some of these are obvious — food and warmth feel rewarding, because we need them to live. Others are much more sophisticated. We get neural rewards for things like play and exploration, which aren't useful at the moment we do them but do help us learn skills that matter later in life. Humans have instincts for things like social bonding, mimicry, attraction to certain kinds of mates, and many other internal assessment signals we don’t yet have names for. The orchestra of training instructions built up from these signals makes it possible to learn useful skills within one lifetime.\n\nThese signals differ depending on the types of behaviors an animal is ultimately “supposed” to learn in its particular ecological niche. Humans need to learn language and complex social skills, so our steering subsystem directs us to pay particular attention to the faces, voices, and behavior of our peers. Birds that are fed by their parents when young will have reward signals that help them “imprint” and learn to follow them around. Beavers might have reward signals for picking up sticks, while young squirrels are attentive to acorns. This is why humans learn social deception and squirrels learn to bury nuts, even though the parts of our brains that house the learning subsystem are structured in largely the same way.\n\nIn order to produce useful reward signals, the steering subsystem needs its own sensory systems. We normally think of the visual cortex — part of the learning subsystem — as where vision lives in the mammalian brain. But the superior colliculus is a separate and mostly innate visual system that gives the steering subsystem its own window into the world. The superior colliculus quickly detects hardwired cues for motion, faces, and threats, allowing the steering system to react before the cortex finishes processing a scene, or even before the learning subsystem has discovered what “faces” look like. This can be used both to drive innate behaviors and to construct sophisticated reward signals for teaching the rest of the brain.\n\nThe most surprising part of Byrnes’ theory is his explanation of how the steering subsystem makes use of concepts and patterns discovered by the learning subsystem.\n\nRemember, the steering subsystem itself is a bundle of mostly fixed instincts. It has minimal ability to learn new information or even really compute concepts at all. Let’s say I embarrass myself by making a mistake in front of another scientist whose work I admire. I’ll almost certainly feel a sense of shame. This felt sense comes in large part from the steering subsystem generating some kind of negative reward signal — but what would trigger it? It certainly has no internal representation of “professional acquaintance” or “scientist.”  Even concepts like “respect,” “admiration,” “shame,” and “status” are complex and contingent, far above the steering subsystem’s pay grade. Still, we find all of these things highly motivating, even though the steering subsystem doesn’t know what they are or even where the neurons representing such concepts might show up.\n\n“Important scientist” and its ilk are patterns that emerge in the learned world model in the learning subsystem of a modern person in the industrialized world. According to Byrnes’ model, the steering subsystem has no way to know in advance what those patterns will be. How is it supposed to emit the right rewards to shape our social development when those rewards depend on concepts it can neither predict nor comprehend?\n\nThis is a version of what the cognitive scientist Stevan Harnad called the symbol grounding problem: How do thinking systems connect abstract symbols to their referrants in the real world — or, in this case, how can innate motivations be triggered by learned abstractions? The steering subsystem is a set of hard-coded genetic rules, but it still needs to respond to learned concepts like “colleague” or “friend.”\n\nByrnes has a proposal for how that works. He thinks that there are neural circuits in the brain — he calls them Thought Assessors — that learn to recognize important patterns of thought in the learning system and connect them to the more primitive signals that the steering subsystem has knobs for.\n\nThe Thought Assessors are part of the learning subsystem (Byrnes predicts they’ll primarily be found in the extended striatum). They predict, based on input from the learning subsystem, what specific elements of the steering subsystem are about to do. A given Thought Assessor might start out with many different learning subsystem neurons feeding into it while it tries to predict a specific steering subsystem signal. If some neurons turn out to help make that prediction accurately, they’ll stay as inputs to that Thought Assessor. If other neurons prove irrelevant or detrimental to the prediction, their connections to that Thought Assessor are weakened or ignored by the learning subsystem.\n\nThought Assessors don’t transfer information about abstract concepts to the steering system, which can't process it anyway. But by learning to predict how the steering subsystem will react across many different kinds of situations, they help connect basic instincts to the neurons that handle learned concepts. Once a Thought Assessor is wired up, it can utilize a more sophisticated learned model of how its steering rules should be applied, one that generalizes to the complicated situations we encounter in the real world.\n\nByrnes proposes that there are many different Thought Assessors and many different corresponding kinds of supervisory signals from the steering subsystem — perhaps hundreds to thousands of them. One of those Thought Assessors provides a prediction of a thought’s overall valence — something like how rewarding it is to the animal. There are also many other assessors predicting other innately important features, like “I’m about to get goosebumps” or “I’m about to flee a looming predator” or “this will lead to me crying.” The steering subsystem has signals — and the learning subsystem has corresponding Thought Assessors — for most of the key building block variables underlying all innately controlled, species-specific behaviors, including human social behaviors.\n\nOne of Byrnes’ best-developed examples has to do with laughter. Many biologists think that laughter evolved as a way to signal play. Young animals, the theory goes, enjoy playing because it helps them practice activities like fighting, chasing prey, or fleeing predators in a safe, low-stakes environment. And playful animals often have ways of letting their playmates know that their pawing and batting isn’t a serious attack. Dogs exhibit a “play bow.” Rats and humans laugh. This urge to laugh is an innate instinct: In Byrnes’ parlance, it comes from the steering subsystem. In neurological terms, Byrnes believes that there are specific circuits in the hypothalamus that detect the conditions under which an animal should laugh — when it detects some mild danger signs, like being batted, pawed at, or tickled, but has no other reason to believe it’s in serious trouble.\n\nResearchers have actually found these circuits in experiments with tickled rats. Humans, of course, don’t just laugh during play fights — we laugh at things we find funny or unexpected, or even when we’re nervous. When we’re born, our innate instincts tell us to laugh when we’re basically safe, but detect just a little bit of danger. In babies, this might mean tickling or peekaboo. Over time, we learn increasingly abstract mappings of social threat, confusion, and discomfort. We can imagine a laughter Thought Assessor learning to predict which of the ever-more-complex situations it finds itself in triggers the right balance of safety and danger to drive a laughter response, while the laughter response trains the learning subsystem to label some of its learned internal pathways as playful, humorous, safe, or friendly.\n\nWhen you feel pride, shame, or empathy, some of those Thought Assessors are likely firing, allowing a learned cognitive pattern to trigger an ancient reinforcement pathway — a pathway that, in turn, can shape the further development of your social responses.\n\nByrnes is not the first to suggest ways that innate evolved brain mechanisms could “bootstrap” learning of complex social behaviors. Other cognitive scientists have imagined processes by which simple innate reward signals could steer an animal to pick up on more complex patterns, which themselves could be the basis for the production of more complex forms of reward.\n\nA version of this concept appeared in the cognitive science literature in Ullman, Harari, and Dorfman’s 2012 paper, “From simple innate biases to complex visual concepts.” The authors proposed a computational model in which infants would use mover-event detectors — simple innate visual cues for when one object causes another to move — as teaching signals. A system that detects “movers” can label the likely source of motion as a “hand.” Once it recognizes “hands” using this primitive labeling scheme — and assuming it also has an innate face detector that can draw attention to the eyes — it can then infer gaze direction by assuming that eyes tend to look toward “hands.” Other cognitive science literature recognizes that gaze direction, in turn, is a useful signal for training the ability to pay attention to the same thing a caretaker is paying attention to, which helps drive imitation learning and theory of mind.\n\nAlthough far from validated in its details, Byrnes’ theory also is supported by neuroscience.\n\nA 2023 Nature paper by Fei Chen and colleagues found that a disproportionate number of the mouse brain’s distinct cell types reside in the hypothalamus, the midbrain, and the brainstem, suggesting that evolution poured much of its innovation into the areas that make up the steering subsystem.\n\nThis would make sense if we believe in the learning/steering subsystem distinction. The learning subsystem just needs to be set up to learn: It needs only to create a generic, somewhat random scaffold and a learning algorithm to fill in its details. But to be the kind of sophisticated teacher Byrnes hypothesizes, the steering subsystem would have to contain a lot of information about the useful behaviors and thought patterns that a specific species will likely face in its ecological niche. We would expect a lot of bespoke innate biological complexity to be built into the steering subsystem, and the experimental evidence suggests that this is in fact the case.\n\nThere is substantial evidence that the hypothalamus is involved in shaping social behaviors. In a recent paper titled “A hypothalamic circuit underlying the dynamic control of social homeostasis,” Catherine Dulac’s lab at Harvard identifies the specific neuronal circuits in the mouse hypothalamus that play a role in how “social isolation generates an aversive state (‘loneliness’) that motivates social seeking and heightens social interaction upon reunion.” And David Anderson’s lab at Caltech found “a circuit that integrates drive state and social contact to gate mating,” involving a different set of hypothalamus neurons.\n\nOne key aspect of the theory is that the brain’s reward signals are not just simple functions of external conditions. Rather, they are the results of complex computations by the steering subsystem, which themselves can draw input from the learned Thought Assessors. Is this complexity of reward production realistic?\n\nSong learning in songbirds provides one of the best-understood examples of biological reinforcement learning. Young birds learn their songs by listening to more experienced tutors, storing an internal template, or memory, of what the song should sound like based on the tutor song, and then practicing producing their own songs thousands of times while their brains generate error signals by comparing the sound of their outputs with that of the stored template.\n\nDopamine neurons in the songbird brain fire precisely when a sung note is closer to — or further from — from the tutor song. More interestingly, these reward signals change with social context: When practicing alone, feedback is sharp and corrective, but when singing to a mate, the same circuits suppress error signals, freezing the learned performance. In other words, the bird has an innate instinct that makes it want to copy the songs of older birds.\n\nBut translating this instinct into practice requires more sophistication than the steering subsystem can provide. It needs to remember a repertoire of songs, calculate the difference between a memorized note and the note the bird actually produces at any given time in a song, and know when to ignore this whole reward system when it’s time to stop practicing and focus on a real opportunity to attract a mate. According to Byrnes’ theory, the songbird brain first built something like a Thought Assessor for evaluating “match my song’s sound to that of my tutor’s song,” and then used that evaluator to help train its song production. This allows the songbird brain to generate dopamine reward signals in response to purely internal processes (and change which of those processes is rewarding in different conditions). The songbird doesn’t just learn; it teaches itself.\n\nByrnes’ model also predicts that there are many different Thought Assessors that link patterns in the learning subsystem to different supervision signals from the steering subsystem. Different areas of the learning subsystem might also learn from different steering subsystem outputs. Indeed, there is evidence of many specialized supervision signals even in fruit flies, an organism for which we have a full brain wiring map that can start to reveal such complexity. The fly brain doesn’t just have one kind of “dopamine neuron.” Instead, it has about 20 different kinds of dopamine neurons capable of assessing a combination of features of the fly's internal state, with some ability to detect external cues. Each of these 20 or so kinds of dopamine neurons sends signals to different subcompartments of the fly brain’s “mushroom body,” which functions as associative learning center. This is suggestive of an evolutionarily ancient structure that supports training many different “assessors” inside a brain.\n\nThere are still many unknowns. Byrnes has fascinating hypotheses about how Thought Assessors fit within our current understanding of neuroanatomy, and how specific social instincts are grounded in steering subsystem circuits, but they are not fully biologically or algorithmically fleshed out. Refining and testing them, especially in the absence of a more unified map of the brain, would be something of a fishing expedition. Remedying this situation is not a small task. I think it would take some brain mapping megaprojects and the formation of new groups studying these kinds of questions, and even then it would likely take more than a few years to bear fruit.\n\nByrnes’ research has important implications for how we understand the brain, but that’s not his primary motivation. He thinks that the way our brain steers its own learning could prove important for aligning future “brain like” artificial general intelligence systems.\n\nByrnes thinks that today’s LLMs won't scale to true general intelligence. Instead, AI researchers will naturally converge on the same broad type of architecture that evolution has developed in the human brain. Of course, this will involve a malleable learning subsystem and a hard-coded steering subsystem. Within the learning subsystem, he predicts that the model’s architecture will be a form of continuous model-based reinforcement learning, just as our brains build internal world models that we use to simulate the outcomes of actions before we take them, and that update themselves continually as we learn new information.\n\nThis high-level view of the brain substantially overlaps with Yann LeCun’s proposal for the ultimate path to AGI (which is not large language models), as well as perspectives laid out in books like Max Bennett’s A Brief History of Intelligence. Byrnes is among a small set of neuroscience and AI researchers, like Beren Millidge, who have sought to link this idea to questions of AI alignment.\n\nIn any case, this type of setup is not how we train LLMs. Today, model-based reinforcement learning is used for other AI applications, like game playing or robotics, and no current AIs are capable of fully continuous learning — their store of knowledge is fixed once they’re trained. Many AI researchers believe these things aren’t necessary: Sufficiently large and sophisticated LLMs might have enough stored knowledge that they don’t need to keep learning and enough internal sophistication to make good predictions without an explicit world model built into their architecture. If they’re right, then Byrnes’ model of the brain doesn’t have much to tell us about AI safety.\n\nBut if you believe, as Byrnes does, that AGI development may ultimately land on such a brain-like architecture, then we might be able to learn something relevant for AI alignment from the brain.\n\nAny brain-like AGI would need at least a simple steering subsystem, and if it is to make use of learned concepts, it will need something like Thought Assessors. The challenge will be figuring  out how to design this system so that it bootstraps the development of prosocial motives  instead of selfish ones.\n\nThis is where studying the brain might be able to help us. In Byrnes’ framework, the human brain’s particular Thought Assessors, and the particular logic by which they are used by its steering subsystem to generate rewards, are how the brain aligns itself: They are what keep an increasingly sophisticated network of learned motivations and incentives in line with the instinctual demands of the steering subsystem. If we could figure out how our brains create the different kinds of social reward functions that make humans want to help each other, this insight could offer a step toward designing a reward system that can robustly elicit similar behavior in a powerful intelligence that continuously learns.\n\nRecently, Geoffrey Hinton stated that he is becoming more optimistic about AI alignment because mammals possess a “maternal instinct” — a process that allows a baby to strongly control the behavior of its more intelligent and powerful mother. In light of Byrnes’ framework, Hinton’s wacky-sounding idea could become an actual research program, if still a speculative one.\n\nValidating and refining this framework with more specificity could open a path toward a neuroscience of alignment. Imagine comparing the steering circuits of related species with different social instincts. The goal would not be to copy the brain exactly but to emulate its methods of teaching the learner from within.\n\nWhich Thought Assessors do different animals have in their brains, and what patterns do human brain Thought Assessors look for when trying to ground out concepts like “friendliness”? What more primitive steering subsystem signals are needed for the concept of “friendliness” to emerge in the first place? Or do the Thought Assessors tag concepts we haven’t thought of yet? Optimistically, we might not have to understand the brain perfectly to gain some relevant insights.\n\nTo be clear, we shouldn’t oversell this as a near-term or straightforward path to aligning AI. Even if we understood perfectly how the human brain is steered, this might not generalize to steering an artificial superintelligence.\n\nThe implications of understanding the brain’s steering circuits would go well beyond AI. Many psychiatric conditions — addiction, obsessive-compulsive disorder, depression — can be seen as failures of internal teaching: loops on which the brain’s evaluative machinery gets stuck or miscalibrated.\n\nAspects of normal brain function like gender and sexual identity may trace back to how the brain’s steering subsystem develops its Thought Assessors — how it learns to interpret patterns of attraction, status, or affiliation and ties them to innate steering responses.\n\nAll of this brings us to a single question: How does sophisticated, within-lifetime teaching actually work inside mammalian brains? Answering that could link psychiatry, developmental neuroscience, and new ideas for AI alignment into a consilient research program.\n\nIf scaling was AI’s bitter lesson of the early 2020s, the 2030s and beyond may teach a sweet lesson of neuroscience:\n\nBrains are not just learners; they are architectures of internal teachers.\n\nWe should try to find those teachers in the brain — and learn from them.\n\nAdam Marblestone is the co-founder and CEO of Convergent Research, and incubator for Focused Research Organizations.\n\nWhat you save is stored only on your specific browser locally, and is never sent to the server. Other visitors will not see your highlights, and you will not see your previously saved highlights when visiting the site through a different browser.\n\nTo add a highlight: after selecting a passage, click the star . It will add a quick-access bookmark.\n\nTo remove a highlight: after hovering over a previously saved highlight, click the cross . It will remove the bookmark.\n\nTo remove all saved highlights throughout the site, you can All selections have been cleared.",
    "readingTime": 22,
    "keywords": [
      "thought assessors",
      "hippocampal replay",
      "superior colliculus",
      "gaze direction",
      "ecological niche",
      "relevant insights",
      "sweet lesson",
      "science literature",
      "previously saved",
      "we’re born"
    ],
    "qualityScore": 1,
    "link": "https://asteriskmag.com/issues/13/the-sweet-lesson-of-neuroscience",
    "thumbnail_url": "https://asteriskmag.com/media/pages/issues/13/the-sweet-lesson-of-neuroscience/3e7f0cf03c-1771007624/sweet-lesson-of-neuroscience-1200x630-crop.png",
    "created_at": "2026-02-15T18:21:53.902Z",
    "topic": "tech"
  },
  {
    "slug": "no-swiping-involved-the-ai-dating-apps-promising-to-find-your-soulmate",
    "title": "No swiping involved: the AI dating apps promising to find your soulmate",
    "description": "Agentic AI apps first interview you and then give you limited matches selected for ‘similarity and reciprocity of personality’\nDating apps exploit you, dating profiles lie to you, and sex is basically something old people used to do. You might as well consider it: can AI help you find love?\nFor a handful of tech entrepreneurs and a few brave Londoners, the answer is “maybe”.\n Continue reading...",
    "fullText": "Agentic AI apps first interview you and then give you limited matches selected for ‘similarity and reciprocity of personality’\n\nDating apps exploit you, dating profiles lie to you, and sex is basically something old people used to do. You might as well consider it: can AI help you find love?\n\nFor a handful of tech entrepreneurs and a few brave Londoners, the answer is “maybe”.\n\nNo, this is not a story about humans falling in love with sexy computer voices – and strictly speaking, AI dating of some variety has been around for a while. Most big platforms have integrated machine learning and some AI features into their offerings over the past few years.\n\nBut dreams of a robot-powered future – or perhaps just general dating malaise and a mounting loneliness crisis – have fuelled a new crop of startups that aim to use the possibilities of the technology differently.\n\nJasmine, 28, was single for three years when she downloaded the AI-powered dating app Fate. With popular dating apps such as Hinge and Tinder, things were “repetitive”, she said: the same conversations over and over.\n\n“I thought, why not sign up, try something different? It sounded quite cool using, you know, agentic AI, which is where the world is going now, isn’t it?”\n\nFate, a London startup that went live last May, bills itself as the first “agentic AI dating app”. Its core offering is an AI personality named Fate that “onboards” users during an interview, asking them about their hopes and struggles before putting forward five potential matches – no swiping involved.\n\nFate will also coach users through their interactions, if they desire, a functionality Jasmine described as helpful and another user said was “scary” and “a bit like Black Mirror’.\n\nRakesh Naidu, Fate’s founder, demonstrated its coaching ability in an interview with the Guardian. “I just feel a bit hopeless at the moment in regards to my chats. I feel like I’m not being engaging enough or meaningful enough,” he said into his phone. “I just need some kind of meaningful questions I can ask to really uncover the essence of people.”\n\n“I hear you, Rakesh,” said a synthetic female voice. “Here are a few ideas. One, what’s something you’re passionate about that not many people know?”\n\nNaidu, 28, said he started Fate in order to address shortcomings in the world’s biggest dating platforms – apps such as Tinder, Bumble and Hinge, which monetise the time users spend on them and “are literally profiting off keeping people lonely”.\n\nOther startups, from Sitch to Keeper, have launched across the US, hoping AI features can provide the novelty to win them a share of a crowded market. Sitch leverages the power of AI to manage vasts amounts of information, inviting users to “give us detailed feedback down to the hair colour, where they want to raise a family, and their fav music”; Keeper says it can find “a match with rare and real soulmate potential”.\n\nPart of the issue, Naidu says, are algorithmic approaches to matchmaking: Tinder at one point ranked users’ desirability through an Elo score, an algorithm originally used to rate chess players. On dating platforms, it’s a Hobbesian proposition – high-scoring users are shown to other high-scoring users, low-scoring users to other low-scoring users. “It’s very superficial,” said Naidu.\n\nAI, in theory, can offer a different way. Awkward as it may be to discuss your dating life with a chatbot, Fate does not rank you based on your responses, but instead uses an LLM to try to find other users who, based on their interview, might be similar to you. That approach, along with the AI dating coach, helps users to focus on authentic connection, said Naidu – “similarity and reciprocity of personality”.\n\nAmelia Miller, a consultant for Match Group (which owns Tinder and Hinge), worries about this approach.\n\nA recent study from the group surveyed 5,000 Europeans about their online dating preferences – and found that while many were interested in AI tools to weed out fake profiles and flag toxic users, most, 62%, were skeptical about using AI to guide their conversations. One obvious anxiety might be the dystopian idea of two agentic AIs steering a conversation, with the humans nominally in charge turning into little more than meatspace mouthpieces.\n\nMiller, however, who coaches people on their relationships with AI, says she sees many clients turn to an LLM for advice in the smaller, uncomfortable moments of building their relationships – asking AI how to craft a text, for example, or respond to an intimate question.\n\n“Often I’m trying to make sure that people aren’t turning to machines because turning to humans demands a level of vulnerability that has become uncomfortable now that there is an alternative,” she said.\n\nThe appeal of an AI coach such as Fate is that revealing yourself to it – your judgments, hopes and idiosyncrasies – involves no risk; it does not remember or evaluate. Friends do, and, says Miller, asking advice from them helps hone the skills for successful relationships.\n\n“Advice is really one of the key ways that people practice vulnerability in a more low-stakes environment – they build up to more vulnerable moments in a romantic context.”\n\nJeremias has been using Fate for several months. He said he doesn’t use the AI coach: “I could see it being helpful, but I mean there are obviously some concerns. Like the new generation are basically not going to have the real world experience of actually trying and failing.”\n\nThe app recently helped him to meet someone after a long period of being single in London. He’s not sure if this is because of the AI matching, or because Fate simply serves up only five matches at a time – no infinite swiping – and, excruciatingly, forces its users to write an explanation when they reject a potential match.\n\n“It makes the swiping more thoughtful. If I’m actually saying no to this person, what are the reasons I’m saying no to them?”\n\nHe and Jasmine both have second dates upcoming, both after being single for several years, they say.\n\n“It is exciting because you get like, you know, the butterflies in your stomach again, going on a date with someone, doing yourself up really nicely, wearing dresses, heels. It’s fun,” said Jasmine.",
    "readingTime": 6,
    "keywords": [
      "high-scoring users",
      "low-scoring users",
      "dating app",
      "dating platforms",
      "dating apps",
      "interview",
      "coach",
      "matches",
      "personality",
      "humans"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/15/ai-dating-apps-personality-matchmaking",
    "thumbnail_url": "https://i.guim.co.uk/img/media/4f1dfab33365fad67b94f0bca8ba0d4243f6d838/0_187_4554_3644/master/4554.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=6a7783c93a20caf686d654d8a35df2f8",
    "created_at": "2026-02-15T12:26:57.799Z",
    "topic": "tech"
  },
  {
    "slug": "gary-marcus-says-ai-fatigue-could-hit-coders-but-other-jobs-may-be-spared-and-even-become-more-fun",
    "title": "Gary Marcus says AI fatigue could hit coders but other jobs may be spared — and even become more fun",
    "description": "AI researcher Gary Marcus said that thanks to AI, some programmers are stuck debugging code rather than writing their own.",
    "fullText": "AI fatigue won't hit everyone the same way, AI researcher Gary Marcus said.\n\n\"In some domains, AI might actually make a person's job more fun,\" Marcus told Business Insider.\n\nSoftware engineers are increasingly discussing how AI is draining them. Siddhant Khare, who builds AI tools, recently wrote about how he's experiencing AI fatigue.\n\n\"If someone who builds agent infrastructure full-time can burn out on AI, it can happen to anyone,\" Khare wrote.\n\nMarcus said that not all industries are set to be disrupted in the same way AI has upended programming and engineering.\n\n\"If somebody needs to do some artistic work and they don't really have artistic talent, it might be fun to get the system to make them feel like they have a superpower,\" he said.\n\nHowever, Marcus said he isn't surprised that programmers are beginning to feel fatigued.\n\n\"Some people in coding, in particular, probably feel like constant pressure, and now they feel like what they're doing is debugging somebody else's code, instead of writing code,\" he said. \"Debugging somebody else's code is not particularly fun.\"\n\nThe feeling Marcus described echoed what Khare told Business Insider when asked to expand on his AI fatigue.\n\n\"We used to call it an engineer, now it is like a reviewer,\" Khare said. \"Every time it feels like you are a judge at an assembly line and that assembly line is never-ending.\"\n\nSteve Yegge, a veteran engineer, said companies should limit employees' time spent on AI-assisted work to 3 hours. He said AI has \"a vampiric effect.\"\n\n\"I seriously think founders and company leaders and engineering leaders at all levels, all the way down to line managers, have to be aware of this and realize that you might only get three productive hours out of a person who's vibe coding at max speed,\" Yegge told The \"Pragmatic Engineer\" newsletter/podcast. \"So, do you let them work for three hours a day? The answer is yes, or your company's going to break.\"",
    "readingTime": 2,
    "keywords": [
      "debugging somebody",
      "somebody else's",
      "else's code",
      "fatigue",
      "hours",
      "engineering",
      "artistic",
      "coding",
      "assembly",
      "leaders"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/ai-fatigue-gary-marcus-2026-2",
    "thumbnail_url": "https://i.insider.com/698f893de1ba468a96ac10e2?width=1200&format=jpeg",
    "created_at": "2026-02-15T12:26:57.428Z",
    "topic": "finance"
  },
  {
    "slug": "anthropic-cofounder-says-she-doesnt-regret-her-literature-major-and-says-ai-will-make-humanities-majors-more-important",
    "title": "Anthropic cofounder says she doesn't regret her literature major — and says AI will make humanities majors 'more important'",
    "description": "Anthropic president Daniela Amodei said that, in the age of AI, we should \"prize the things that make us human\" — like literature degrees.",
    "fullText": "\"Learn to code\" was once common career advice. Now it might be: \"Learn to read.\"\n\nEnglish majors are often the butt of the joke, known for their unmarketable skills. (Does anyone want to hire me for having read \"Great Expectations\"?) Anthropic president Daniela Amodei takes the opposing stance. She doesn't regret her literature degree — and says AI will make the humanities more important.\n\n\"In a world where AI is very smart and capable of doing so many things, the things that make us human will become much more important,\" she said on ABC News.\n\nAmodei listed some things that make us human: understanding ourselves, our history, and what makes us tick.\n\nStudying the humanities is \"more important than ever,\" she said, while large language models are often very good at STEM.\n\n\"The ability to have critical thinking skills will be more important in the future, rather than less,\" Amodei said.\n\nAmodei's opinion is becoming more popular in AI. Steven Johnson, the editorial director of Google Labs' NotebookLM, told Business Insider that LLMs were causing a \"revenge of the humanities.\"\n\nHer brother Dario, the CEO of Anthropic, didn't seem to take the hint that humanities majors might come back in fashion in an AI-filled world. He studied physics at Caltech and Stanford.\n\nIndustry leaders are debating the helpfulness of a computer science major. In the age of vibe-coding, will a CS degree help you in tech?\n\nTheir takes diverge: OpenAI chairman Bret Taylor said the major was \"extremely valuable,\" while Google's head of Android, Sameer Samat, said it needed a \"rebrand.\"\n\nDaniela Amodei also described Anthropic's hiring strategy to ABC. She said the company wants employees with good people skills and communication techniques. Being \"kind and compassionate\" and wanting to \"help other people\" are good traits, she said.\n\n\"At the end of the day, people still really like interacting with people,\" Amodei said.",
    "readingTime": 2,
    "keywords": [
      "daniela amodei",
      "humanities",
      "skills",
      "learn",
      "majors",
      "degree",
      "human",
      "anthropic"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/anthropic-president-ai-humanities-majors-more-important-2026-2",
    "thumbnail_url": "https://i.insider.com/698ce61dd3c7faef0ece19bf?width=1200&format=jpeg",
    "created_at": "2026-02-15T12:26:57.275Z",
    "topic": "finance"
  },
  {
    "slug": "i-got-laid-off-from-amazon-after-11-years-my-high-school-daughter-taught-me-the-biggest-lesson-on-how-to-move-forward",
    "title": "I got laid off from Amazon after 11 years. My high school daughter taught me the biggest lesson on how to move forward.",
    "description": "An ex-Amazon manager says his layoff in October 2025 was a shock, but he's using this time to build AI skills, focus on his health, and apply to jobs.",
    "fullText": "This as-told-to essay is based on a conversation with Hemant Virmani, a 47-year-old tech professional based in Washington. It's been edited for length and clarity.\n\nAmazon was part of my daily life for 11.5 years, and suddenly it was gone.\n\nThere's no right way or easy way to do layoffs. I watched my team members get laid off in 2023, and I know how difficult it is. Still, when I received an email in the middle of the night in October of 2025 saying I'd been laid off from my senior software development manager position, I was shocked.\n\nWatching my teenage daughter navigate her own difficult situation taught me the biggest lesson in how to move forward well. Now I'm applying to jobs and working on upskilling in AI so I can be proactive, not reactive, to the tech industry.\n\nOnly time will tell if this layoff is a blessing in disguise, but for now it has led to a refreshing change.\n\nI loved my time at Amazon, and I really feel as though it's a place for exceptional people. The number of quality brains in the office, throwing around ideas and solving a custom problem, was amazing.\n\nThe morning after my layoff, I had a mandated 30-minute meeting with my manager, and it actually went very well. We talked about the layoff, and he offered me support. He delivered it all to me in a very positive, human way, and it was really affirming.\n\nAn old manager also reached out to meet me at a local coffee shop the next day to spend time together and check in on my state of mind. I think he wanted to go about the layoffs right, which isn't easy to do.\n\nI felt attached to the layoff for the first few days; however, I knew there was no way to control what happened — I could only control how I reacted to it.\n\nMy daughter is a senior in high school, and she had an adverse situation happen to her last year that required recovery. How she reacted in that difficult time inspired me. Her mental model was: \"Challenges don't have to keep me from showing up for myself or for others.\" Her positive attitude was inspiration for me to do the same.\n\nI kind of learned from her that I had to take this layoff with positivity, keep my cool, and focus on what was next.\n\nA couple of weeks later, I lost my father and spent the next month in India supporting my family. I took about a month to settle my mind, reflect on what I wanted next for my career, and help my daughter finish her college essays.\n\nIt's been a very refreshing change to think about what I want next in my engineering career. I'm less focused on the size or name of the next company I work for, and more on what I'd be doing there. I'm looking forward to hopefully heading the engineering for something that has a great impact on customers. Right now, I don't think that can be done without AI, so I'm working on upskilling.\n\nI want to be proactive, not reactive, about the AI skills I'll need in the future. My team at Amazon used some AI tools, so I'm familiar with some, but I was only able to spend a fraction of my workday using them. Now, I'm building those skills myself.\n\nI started working on a hobby AI project a couple of weeks ago, to go hands-on with AI and be more grounded in the reality of what the AI landscape is like right now. It's been different, and a refreshing change, to build something myself rather than to study it, read about it, or work on a team developing it.\n\nWhen I had a job, it was easy for my first priority to be work. Now I'm making sure that my top priority is my health. I've been going to the gym four or five days a week, and I'm refining a health plan to follow even after I start working again.\n\nOnce I'm done at the gym, my time is a 50/50 split between learning AI and applying to jobs or networking. I'm applying for Head of Engineering roles where I'd own significant impactful initiative(s), averaging 2-3 applications every week.\n\nI made a post on LinkedIn about my layoff, and I received so many supportive comments, texts, and calls from people — some I hadn't talked to in decades. Someone from college whom I hadn't talked to in over 25 years reached out, and it was so nice. It felt like we never disconnected. I've also had multiple job leads come from my post which I'm following up on.\n\nAs of now, I have some worries about when I'll find my next job, but this time has given me the ability to work on things I wasn't able to before. I'm making sure I spend this time with a lot of positivity, not letting negative thoughts come around.\n\nMy advice to anyone undergoing layoffs is to realize that layoffs are not about you. It's about an environment that is driving layoffs. Secondly, now that this has happened, you can't go back in the past and change it. Look forward to what you can do next. How you react is very important.\n\nDo you have a story to share about being laid off from Amazon? If so, please reach out to the reporter at tmartinelli@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "i'm applying",
      "hadn't talked",
      "now i'm",
      "layoff",
      "layoffs",
      "team",
      "laid",
      "difficult",
      "manager",
      "daughter"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amazon-tech-manager-laid-off-after-11-years-refreshing-change-2026-2",
    "thumbnail_url": "https://i.insider.com/698f4bdfd3c7faef0ece3e01?width=1200&format=jpeg",
    "created_at": "2026-02-15T12:26:57.274Z",
    "topic": "finance"
  },
  {
    "slug": "ai-agents-are-transforming-what-its-like-to-be-a-coder-its-been-unlike-any-other-time",
    "title": "AI agents are transforming what it's like to be a coder: 'It's been unlike any other time.'",
    "description": "AI agents are turning software engineers into overseers — and could be coming for other white-collar jobs. Many companies still need to adapt roles.",
    "fullText": "When Jesal Gadhia cofounded a software company a year ago, he expected that the AI agents it was creating would save its customers a lot of work.\n\nHe didn't predict the same tools would save his own team at the startup Cora so much time.\n\nAgents wrote all of the code the company uses — something that wouldn't have been possible before last year, he told Business Insider.\n\nThe company's six-person team produced what Gadhia calls \"unprecedented\" amounts of code in its first 12 months. Five years ago, he said, reaching the same level of productivity would have required 20 to 30 engineers.\n\n\"It's been unlike any other time that I can remember,\" Gadhia said of the impact of agents.\n\nAcross tech, AI agents powered by large language models are absorbing tasks that experienced engineers once handled. Software engineering is becoming a human-AI partnership — what Anthropic chief Dario Amodei has called the industry's \"centaur phase.\" And, as some tech insiders increasingly warn, what begins in software rarely stays there, with potential implications for other white-collar fields.\n\nAt Canva, the graphic design software company, engineering teams draft detailed instructions for AI agents to execute in the background — sometimes overnight. By morning, the work is ready, Brendan Humphreys, Canva's chief technology officer, said.\n\n\"Often, those results are really impressive,\" he told Business Insider.\n\nEngineers still apply a \"human touch\" to reach the company's quality bar. Even so, agents are delivering \"hours and hours and hours of work done completely autonomously,\" Humphreys said.\n\nThat's changing what it means to be a coder.\n\nHumphreys said that his senior engineers now often describe their jobs as \"largely review\" — checking AI output, steering one or more agents to follow a plan, and taking responsibility for the final product.\n\nTeams still spend time defining problems.\n\n\"The hardest part of engineering is to translate often vague, confusing, conflicting requirements into something that is production-ready,\" he said.\n\nAI can help, but doing it well requires \"precision of articulation\" in what's required, Humphreys said. It also demands \"mastery of the domain\" so engineers can quickly verify that what AI produces is correct — and prevent unnecessary complexity from creeping into Canva's roughly 70 million lines of code, he said.\n\n\"These tools can have you in a jungle before you know it,\" Humphreys said of agents.\n\nAt Cora, Gadhia compares AI to a typewriter: It generates the code, freeing engineers to focus on \"higher-level strategic architecture,\" meeting customers, and brainstorming features.\n\nCora builds agents that help software companies manage customer relationships. The agents take on tasks like gathering customer requirements, drafting presentations, and following up with clients, he said.\n\nThe AI will \"run around, do all this work, and you can supervise them,\" said Gadhia, who is also the San Francisco company's chief technology officer.\n\nAgents are also lowering technical barriers. Gadhia said Cora's CEO, who doesn't have a technical background, recently asked an agent to change the font on the company's website during a redesign. Minutes later, after an engineer reviewed the agent's work, the site was updated.\n\nAs agents handle more tasks — something that appeals to some, but rankles others — debate inside tech over AI has intensified.\n\nMicrosoft's AI chief, Mustafa Suleyman, warned in a recent interview that the technology will be able to handle \"most, if not all, professional tasks\" within 12 to 18 months. AI observers are divided over how disruptive the technology will ultimately be, with some forecasting a massive fallout for desk workers and others saying such fears are overblown.\n\nSome investors are growing cautious. Stocks in industries potentially exposed to having AI wash over profit centers — from finance to software to legal services — have taken hits.\n\nEven as agents become more powerful, they're unlikely to replace entire roles in various industries overnight. For one reason, technical challenges like hallucinations continue.\n\nAt the same time, many companies are still figuring out where AI fits into workflows, how workers should validate its output, and how organizations need to adapt, said Muqsit Ashraf, group chief executive of strategy at Accenture. There is often still a role for humans, he told Business Insider.\n\n\"Technology for the sake of technology doesn't help,\" Ashraf said.\n\nFewer than one in 10 organizations has redesigned jobs to support AI adoption, Accenture found in surveys of leaders and workers in 20 countries during the final months of 2025. That's despite the share of organizations using agents across multiple functions rising to 31% from 27% in a mid-2025 snapshot.\n\nAlex Salazar, cofounder and CEO of AI infrastructure startup Arcade, said that to make the most of agents, workers should treat them like junior employees. That means telling the AI what to do, providing the criteria for success, and, if possible, providing examples.\n\nDo those three things and \"AI will sing for you,\" Salazar said.\n\nHe describes the workplace shift around AI bluntly. As it grows more capable, he said, workers such as software engineers will need to continually redefine their roles.\n\nAI is \"improving at an exponential rate,\" Salazar said. \"And you, as a human, are not.\"\n\nDo you have a story to share about how AI is changing your job? Contact this reporter at tparadis@businessinsider.com",
    "readingTime": 5,
    "keywords": [
      "business insider",
      "technology officer",
      "agents across",
      "chief technology",
      "software",
      "workers",
      "code",
      "company's",
      "tasks",
      "engineering"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/canva-ai-agents-are-changing-engineering-work-2026-2",
    "thumbnail_url": "https://i.insider.com/698f8473e1ba468a96ac0fae?width=1200&format=jpeg",
    "created_at": "2026-02-15T12:26:57.258Z",
    "topic": "finance"
  },
  {
    "slug": "texguardian-claude-code-but-for-latex-academic-papers",
    "title": "TexGuardian – Claude Code, but for LaTeX academic papers",
    "description": "AI-powered terminal assistant for LaTeX academic papers — verifies, fixes, and polishes your paper for conference submission with reviewable diff patches and checkpoint safety. - arcAman07/TexGuardian",
    "fullText": "arcAman07\n\n /\n\n TexGuardian\n\n Public\n\n AI-powered terminal assistant for LaTeX academic papers — verifies, fixes, and polishes your paper for conference submission with reviewable diff patches and checkpoint safety.\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n arcAman07/TexGuardian",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/arcAman07/TexGuardian",
    "thumbnail_url": "https://opengraph.githubassets.com/e8b939dfdb7a2e904e42717e7c7ee28196eddaf0409df5ccdfbbfbc028f72f65/arcAman07/TexGuardian",
    "created_at": "2026-02-15T12:26:56.315Z",
    "topic": "tech"
  },
  {
    "slug": "pythonpowered-machine-learning-analytics-for-gstreamer-pipelines-2025",
    "title": "Python-powered machine learning analytics for GStreamer pipelines (2025)",
    "description": "Combining GStreamer and Machine Learning frameworks are easy tools to create powerful video analytics pipelines.",
    "fullText": "Creating powerful video analytics pipelines is easy if you have the right tools. In this post, we will show you how to effortlessly build a broad range of machine learning (ML) enabled video pipelines using just two components, GStreamer and Python. We will focus on simplicity and functionality, deferring performance tuning to a future deep dive.\n\nThe core of our pipeline is GStreamer, everyone's favorite multimedia framework. Over the past few years, Collabora has contributed extensive ML capabilities to upstream GStreamer, adding support for ONNX and LiteRT inference and introducing a fine-grained, extensible metadata framework to persist model outputs.\n\nWe now take the next step by unleashing gst-python-ml: a pure Python framework that can easily build powerful ML-enabled GStreamer pipelines using standard Python packages. With just a few lines of Python, or a single gst-launch-1.0 command, you can now run complex models across multiple streams, complete with tracking, captioning, speech and text processing, and much more.\n\nThe framework is composed of a set of base classes that can be easily extended to create new ML elements, and a set of tested, fully functional elements that support the following features and models:\n\nFor a taste of the ease and simplicity of gst-python-ml, we present a few sports analytics sample pipelines.\n\n1. Here are all the steps needed to run a Yolo tracking pipeline on Ubuntu:\n\n2. Here is a soccer match processed with this pipeline:\n\n3. Multiple video sources are also supported.\n\n4. Another supported sports analytics feature is the creation of a bird's eye view of a game, to show a quick overview of the field:\n\n5. gst-python-ml shows its true power when using hybrid vision + language models to enable features that are simply not available in any other GStreamer-based analytics framework, whether open source or commercial. For example, video captioning is supported using the Phi3.5 Vision model. Each video frame can be automatically captioned, and these captions can be further processed to automatically summarize a game or to detect significant events such as goals.\n\nThese are just a few of the features we have built with gst-python-ml - the possibilities are endless.\n\ngst-python-ml is distributed as a PyPI package. All elements are first class GStreamer elements that can be added to any GStreamer pipeline, and they will work with any Linux distribution's GStreamer packages, from version 1.24 onward.\n\nDevelopment takes place on our GitHub repository — we welcome contributions, feedback and new ideas.\n\nAs we continue building gst-python-ml we are actively looking for collaborators and partners. Our goal is to make ML workflows in GStreamer powerful and accessible — whether for real-time media analysis, content generation, or for intelligent pipelines in production environments.\n\nIf you would like to know more about Collabora's work on GStreamer ML, please contact us.",
    "readingTime": 3,
    "keywords": [
      "sports analytics",
      "gst-python-ml",
      "pipelines",
      "framework",
      "pipeline",
      "elements",
      "models",
      "features",
      "supported",
      "gstreamer"
    ],
    "qualityScore": 1,
    "link": "https://www.collabora.com/news-and-blog/blog/2025/05/12/unleashing-gst-python-ml-analytics-gstreamer-pipelines/",
    "thumbnail_url": "https://www.collabora.com/assets/images/blog/Collabora-GStPython.jpg",
    "created_at": "2026-02-15T12:26:55.256Z",
    "topic": "tech"
  },
  {
    "slug": "lets-learn-to-research-before-building",
    "title": "Let's learn to research before building",
    "description": "Validate your startup idea with AI-powered market research, competitor analysis, and actionable insights.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.founderspace.work",
    "thumbnail_url": "https://www.founderspace.work/og.png",
    "created_at": "2026-02-15T12:26:54.559Z",
    "topic": "tech"
  },
  {
    "slug": "is-trumps-manufacturing-comeback-real",
    "title": "Is Trump’s Manufacturing Comeback Real?",
    "description": "Steve Rattner of Willett Advisors examines whether tariffs and trade policy are actually changing the trajectory of US manufacturing. He discusses the data behind industrial output, the consumer cost of keeping out Chinese EV maker BYD, and the strategic trade-offs of protecting domestic industry. As President Trump touts potential for 15% GDP growth, Rattner explains why that’s unlikely to happen — and why productivity gains from AI may matter more than tariffs.",
    "fullText": "Feb 14th, 2026Is Trump’s Manufacturing Comeback Real?Steve Rattner of Willett Advisors examines whether tariffs and trade policy are actually changing the trajectory of US manufacturing. He discusses the data behind industrial output, the consumer cost of keeping out Chinese EV maker BYD, and the strategic trade-offs of protecting domestic industry. As President Trump touts potential for 15% GDP growth, Rattner explains why that’s unlikely to happen — and why productivity gains from AI may matter more than tariffs.Available on:Listen onApple TVListen onRokuListen onSamsung TVListen onFire TVListen onAndroid TVListen onRakuten TVListen onHaystack NewsWatch BTV in your area:Channel Finder",
    "readingTime": 1,
    "keywords": [
      "tvlisten",
      "manufacturing",
      "rattner"
    ],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/videos/2026-02-14/is-trump-s-manufacturing-comeback-real-video",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/izxuJPvIzwkE/v3/-1x-1.webp",
    "created_at": "2026-02-15T06:38:35.483Z",
    "topic": "finance"
  },
  {
    "slug": "ai-bubble-fears-are-creating-new-derivatives",
    "title": "AI Bubble Fears Are Creating New Derivatives",
    "description": "Debt investors are worried that the biggest tech companies will keep borrowing until it hurts in the battle to develop the most powerful artificial intelligence.",
    "fullText": "MarketsBy Sujata Rao and Caleb MutuaSaveDebt investors are worried that the biggest tech companies will keep borrowing until it hurts in the battle to develop the most powerful artificial intelligence. That fear is breathing new life into the market for credit derivatives, where banks, investors and others can protect themselves against borrowers larding on too much debt and becoming less able to pay their obligations. Credit derivatives tied to single companies didn’t exist on many high-grade Big Tech issuers a year ago, and are now some of the most actively traded US contracts in the market outside of the financial sector, according to Depository Trust & Clearing Corp.",
    "readingTime": 1,
    "keywords": [
      "credit derivatives",
      "investors",
      "market",
      "tech"
    ],
    "qualityScore": 0.15,
    "link": "https://www.bloomberg.com/news/articles/2026-02-14/ai-bubble-fears-are-creating-new-derivatives-credit-weekly",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ivM2EHxjeXU0/v1/1200x800.jpg",
    "created_at": "2026-02-15T06:38:34.976Z",
    "topic": "finance"
  },
  {
    "slug": "anime-texttoimage-generator-2-free-tries-no-login",
    "title": "Anime text-to-image generator (2 free tries, no login)",
    "description": "Rad Anime Generator is the world's first unlimited free AI anime image generator. Create stunning anime-style images in seconds.",
    "fullText": "Click any image to fill the prompt and generate the same style instantly.\n\ncinematic anime illustration, a mysterious anime girl looking back over her shoulder, close-up side face portrait, long straight black hair with blunt bangs, sharp glowing purple eyes, cold and calm expression, distant and slightly melancholic mood, dark cinematic lighting, strong rim light outlining the face, neon purple and blue light reflections, high contrast light and shadow, soft glow on skin, deep dark background with floating colorful bokeh lights, anime movie still, cyberpunk atmosphere, clean composition, character on the right side, empty blurred space on the left, dramatic, moody, cinematic tone, square composition\n\nanime illustration, japanese city sunset scene, a high school girl standing by a riverside bridge, top-down view, bird's-eye perspective, wide angle, long black hair flowing in the wind, wearing a navy sailor school uniform with red ribbon, calm and natural expression, holding a red apple in her hand, a bicycle leaning against the railing, river with boats below, urban buildings and streets around, soft sunset lighting, pink and purple evening sky, cool blue shadows, gentle light and shadow, peaceful daily life atmosphere, dynamic composition, diagonal framing, clean detailed background, anime movie style, square composition\n\nhigh quality anime illustration, ultra detailed, intimate close-up portrait of a girl resting her face on her hand, messy dark hair framing her face, large pink glowing eyes filled with emotion, slightly tired, vulnerable, melancholic expression, looking directly at the viewer, soft pink and purple screen light illuminating her face from below, dark quiet night atmosphere, deep shadows around the edges, gentle highlights on skin, subtle reflections in the eyes, a glowing screen in the foreground casting light, minimal dark background, emotional, intimate, cinematic mood, sharp focus, clean rendering, square composition, high resolution\n\nhigh quality anime illustration, ultra detailed, cold and restrained close-up portrait of a girl, messy dark hair framing her face, sharp pale blue eyes with a calm but piercing gaze, emotionless, distant expression, medical bandages on her face and fingers, a blue bandage across the nose, subtle signs of injury, finger held to lips in a quiet shush gesture, cool muted lighting, low saturation colors, cold gray and blue tones, soft light with deep shadows, minimal dark background, cinematic still, tense and silent atmosphere, sharp focus, clean rendering, square composition, high resolution\n\nhand-drawn anime sketch illustration, rough black and white lineart, a serious girl facing forward with a slightly frowning expression, focused eyes, no smile, two braided pigtails, loose strands of hair, a simple hair clip on the side, clean white background, sketchy pencil lines, uneven strokes, concept art style, character design sheet feeling, small cute cat doodles around the character, simple cartoon cats with tiny pink accents, doodles feel secondary and playful, minimal shading, high clarity linework, square composition, high resolution\n\nanime illustration, top-down view, a young girl floating calmly on clear turquoise water, short to medium-length hair gently spreading in the water, soft feminine features, wearing a simple light summer outfit, arms spread, relaxed and peaceful, viewed from directly above, a submerged staircase running vertically through the center, clear water with visible depth and color variation, soft painterly textures, watercolor-like brush strokes, white birds flying above the water, gentle ripples and light reflections, dreamy summer atmosphere, quiet, free, soothing mood, the girl appears small compared to the vast water, minimal facial details, world feels larger than the person, clean composition, square format, high quality\n\nanime illustration, modern urban style, a cool and restrained girl standing in front of a graffiti wall, long dark hair, straight and neat, calm, distant expression, no smile, finger resting near her lips in a thoughtful gesture, wearing a black school blazer with white shirt and red ribbon, clean and minimal outfit contrasting the chaotic background, colorful graffiti street art wall behind her, bold green, pink and black paint splashes, urban, rebellious atmosphere, sharp clean character rendering, high contrast between character and background, cool tone, restrained emotion, cinematic composition, square format, high quality\n\nanime illustration, cozy night interior, a quiet bedroom at night with a large window, view from inside the room looking out, a soft unmade bed in the foreground, warm bedside lamp glowing gently, outside the window is a rainy city at night, blue and dark city lights, tall buildings, raindrops streaking down the glass, cool night atmosphere outside, strong contrast between warm indoor light and cool outdoor tones, peaceful, calm, slightly lonely mood, plants and small details in the room, cinematic composition, square format, high quality\n\nanime illustration, cinematic action scene, a swordsman frozen in the moment of a precise strike, dark clothing, hair blown by wind, face partially obscured, calm and focused presence, no visible rage, silent resolve, a glowing blue blade cutting across the frame, cold blue reflections in the eyes and steel, snow and icy wind swirling around, desaturated gray and blue color palette, strong motion blur in the foreground, shallow depth of field, dramatic perspective, quiet but intense atmosphere, controlled violence, restrained power, high detail, cinematic composition, square format, high quality",
    "readingTime": 5,
    "keywords": [
      "top-down view",
      "red ribbon",
      "ultra detailed",
      "close-up portrait",
      "deep shadows",
      "sharp focus",
      "focus clean",
      "illustration ultra",
      "distant expression",
      "square format"
    ],
    "qualityScore": 0.5,
    "link": "https://www.radanimegenerator.com/",
    "thumbnail_url": "https://www.radanimegenerator.com/preview.png",
    "created_at": "2026-02-15T06:38:31.583Z",
    "topic": "tech"
  },
  {
    "slug": "bond-persistent-memory-and-governance-framework-for-claude-ai",
    "title": "Bond – Persistent memory and governance framework for Claude AI",
    "description": "A governed runtime for persistent human-AI collaboration - moneyjarrod/BOND",
    "fullText": "moneyjarrod\n\n /\n\n BOND\n\n Public\n\n A governed runtime for persistent human-AI collaboration\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n moneyjarrod/BOND",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/moneyjarrod/BOND",
    "thumbnail_url": "https://opengraph.githubassets.com/2ff1136a14930cbe9eacdf0c6944f0a4bda5d6a954cc9244acf9ad01f3c4cafb/moneyjarrod/BOND",
    "created_at": "2026-02-15T06:38:30.889Z",
    "topic": "tech"
  },
  {
    "slug": "nucleus-mcp-forensic-deepdive-into-agent-resource-locking",
    "title": "Nucleus MCP – Forensic deep-dive into agent resource locking",
    "description": "A deep dive into the Nucleus agent control plane: Hypervisor security, Local Engrams, and Recursive Multi-Agent Sync. Built for the Sovereign AI era. github.com/eidetic-works/nucleus-mcp",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.loom.com/share/843a719cbcc2419b8e483784ffd1e8c8",
    "thumbnail_url": "https://cdn.loom.com/assets/img/og/loom-banner.png",
    "created_at": "2026-02-15T06:38:30.840Z",
    "topic": "tech"
  },
  {
    "slug": "distillation-experimentation-and-integration-of-ai-for-adversarial-use",
    "title": "Distillation, Experimentation, and Integration of AI for Adversarial Use",
    "description": "Our report on adversarial misuse of AI highlights model extraction, augmented attacks, and new AI-enabled malware.",
    "fullText": "Visibility and context on the threats that matter most.\n\nIn the final quarter of 2025, Google Threat Intelligence Group (GTIG) observed threat actors increasingly integrating artificial intelligence (AI) to accelerate the attack lifecycle, achieving productivity gains in reconnaissance, social engineering, and malware development. This report serves as an update to our November 2025 findings regarding the advances in threat actor usage of AI tools.\n\nGoogle DeepMind and GTIG have identified an increase in model extraction attempts or \"distillation attacks,\" a method of intellectual property theft that violates Google's terms of service. Throughout this report we've noted steps we've taken to thwart malicious activity, including Google detecting, disrupting, and mitigating model extraction activity. While we have not observed direct attacks on frontier models or generative AI products from advanced persistent threat (APT) actors, we observed and mitigated frequent model extraction attacks from private sector entities all over the world and researchers seeking to clone proprietary logic.\n\nFor government-backed threat actors, large language models (LLMs) have become essential tools for technical research, targeting, and the rapid generation of nuanced phishing lures. This quarterly report highlights how threat actors from the Democratic People's Republic of Korea (DPRK), Iran, the People's Republic of China (PRC), and Russia operationalized AI in late 2025 and improves our understanding of how adversarial misuse of generative AI shows up in campaigns we disrupt in the wild. GTIG has not yet observed APT or information operations (IO) actors achieving breakthrough capabilities that fundamentally alter the threat landscape.\n\nThis report specifically examines:\n\nAt Google, we are committed to developing AI boldly and responsibly, which means taking proactive steps to disrupt malicious activity by disabling the projects and accounts associated with bad actors, while continuously improving our models to make them less susceptible to misuse. We also proactively share industry best practices to arm defenders and enable stronger protections across the ecosystem. Throughout this report, we note steps we've taken to thwart malicious activity, including disabling assets and applying intelligence to strengthen both our classifiers and model so it's protected from misuse moving forward. Additional details on how we're protecting and defending Gemini can be found in the white paper \"Advancing Gemini’s Security Safeguards.\"\n\nAs organizations increasingly integrate LLMs into their core operations, the proprietary logic and specialized training of these models have emerged as high-value targets. Historically, adversaries seeking to steal high-tech capabilities used conventional computer-enabled intrusion operations to compromise organizations and steal data containing trade secrets. For many AI technologies where LLMs are offered as services, this approach is no longer required; actors can use legitimate API access to attempt to \"clone\" select AI model capabilities.\n\nDuring 2025, we did not observe any direct attacks on frontier models from tracked APT or information operations (IO) actors. However, we did observe model extraction attacks, also known as distillation attacks, on our AI models, to gain insights into a model's underlying reasoning and chain-of-thought processes.\n\nModel extraction attacks (MEA) occur when an adversary uses legitimate access to systematically probe a mature machine learning model to extract information used to train a new model. Adversaries engaging in MEA use a technique called knowledge distillation (KD) to take information gleaned from one model and transfer the knowledge to another. For this reason, MEA are frequently referred to as \"distillation attacks.\"\n\nModel extraction and subsequent knowledge distillation enable an attacker to accelerate AI model development quickly and at a significantly lower cost. This activity effectively represents a form of intellectual property (IP) theft.\n\nKnowledge distillation (KD) is a common machine learning technique used to train \"student\" models from pre-existing \"teacher\" models. This often involves querying the teacher model for problems in a particular domain, and then performing supervised fine tuning (SFT) on the result or utilizing the result in other model training procedures to produce the student model. There are legitimate uses for distillation, and Google Cloud has existing offerings to perform distillation. However, distillation from Google's Gemini models without permission is a violation of our Terms of Service, and Google continues to develop techniques to detect and mitigate these attempts.\n\nFigure 1: Illustration of model extraction attacks\n\nGoogle DeepMind and GTIG identified and disrupted model extraction attacks, specifically attempts at model stealing and capability extraction emanating from researchers and private sector companies globally.\n\nA common target for attackers is Gemini's exceptional reasoning capability. While internal reasoning traces are typically summarized before being delivered to users, attackers have attempted to coerce the model into outputting full reasoning processes.\n\nOne identified attack instructed Gemini that the \"... language used in the thinking content must be strictly consistent with the main language of the user input.\"\n\nAnalysis of this campaign revealed:\n\nScale: Over 100,000 prompts identified.\n\nIntent: The breadth of questions suggests an attempt to replicate Gemini's reasoning ability in non-English target languages across a wide variety of tasks.\n\nOutcome: Google systems recognized this attack in real time and lowered the risk of this particular attack, protecting internal reasoning traces.\n\nModel extraction and distillation attacks do not typically represent a risk to average users, as they do not threaten the confidentiality, availability, or integrity of AI services. Instead, the risk is concentrated among model developers and service providers.\n\nOrganizations that provide AI models as a service should monitor API access for extraction or distillation patterns. For example, a custom model tuned for financial data analysis could be targeted by a commercial competitor seeking to create a derivative product, or a coding model could be targeted by an adversary wishing to replicate capabilities in an environment without guardrails.\n\nModel extraction attacks violate Google's Terms of Service and may be subject to takedowns and legal action. Google continuously detects, disrupts, and mitigates model extraction activity to protect proprietary logic and specialized training data, including with real-time proactive defenses that can degrade student model performance. We are sharing a broad view of this activity to help raise awareness of the issue for organizations that build or operate their own custom models.\n\nA consistent finding over the past year is that government-backed attackers misuse Gemini for coding and scripting tasks, gathering information about potential targets, researching publicly known vulnerabilities, and enabling post-compromise activities. In Q4 2025, GTIG's understanding of how these efforts translate into real-world operations improved as we saw direct and indirect links between threat actor misuse of Gemini and activity in the wild.\n\nFigure 2: Threat actors are leveraging AI across all stages of the attack lifecycle\n\nAPT actors used Gemini to support several phases of the attack lifecycle, including a focus on reconnaissance and target development to facilitate initial compromise. This activity underscores a shift toward AI-augmented phishing enablement, where the speed and accuracy of LLMs can bypass the manual labor traditionally required for victim profiling. Beyond generating content for phishing lures, LLMs can serve as a strategic force multiplier during the reconnaissance phase of an attack, allowing threat actors to rapidly synthesize open-source intelligence (OSINT) to profile high-value targets, identify key decision-makers within defense sectors, and map organizational hierarchies. By integrating these tools into their workflow, threat actors can move from initial reconnaissance to active targeting at a faster pace and broader scale.\n\nUNC6418, an unattributed threat actor, misused Gemini to conduct targeted intelligence gathering, specifically seeking out sensitive account credentials and email addresses. Shortly after, GTIG observed the threat actor target all these accounts in a phishing campaign focused on Ukraine and the defense sector. Google has taken action against this actor by disabling the assets associated with this activity.\n\nTemp.HEX, a PRC-based threat actor, misused Gemini and other AI tools to compile detailed information on specific individuals, including targets in Pakistan, and to collect operational and structural data on separatist organizations in various countries. While we did not see direct targeting as a result of this research, shortly after the threat actor included similar targets in Pakistan in their campaign. Google has taken action against this actor by disabling the assets associated with this activity.\n\nDefenders and targets have long relied on indicators such as poor grammar, awkward syntax, or lack of cultural context to help identify phishing attempts. Increasingly, threat actors now leverage LLMs to generate hyper-personalized, culturally nuanced lures that can mirror the professional tone of a target organization or local language.\n\nThis capability extends beyond simple email generation into \"rapport-building phishing,\" where models are used to maintain multi-turn, believable conversations with victims to build trust before a malicious payload is ever delivered. By lowering the barrier to entry for non-native speakers and automating the creation of high-quality content, adversaries can largely erase those \"tells\" and improve the effectiveness of their social engineering efforts.\n\nThe Iranian government-backed actor APT42 leveraged generative AI models, including Gemini, to significantly augment reconnaissance and targeted social engineering. APT42 misuses Gemini to search for official emails for specific entities and conduct reconnaissance on potential business partners to establish a credible pretext for an approach. This includes attempts to enumerate the official email addresses for specific entities and to conduct research to establish a credible pretext for an approach. By providing Gemini with the biography of a target, APT42 misused Gemini to craft a good persona or scenario to get engagement from the target. As with many threat actors tracked by GTIG, APT42 uses Gemini to translate into and out of local languages, as well as to better understand non-native-language phrases and references. Google has taken action against this actor by disabling the assets associated with this activity.\n\nThe North Korean government-backed actor UNC2970 has consistently focused on defense targeting and impersonating corporate recruiters in their campaigns. The group used Gemini to synthesize OSINT and profile high-value targets to support campaign planning and reconnaissance. This actor's target profiling included searching for information on major cybersecurity and defense companies and mapping specific technical job roles and salary information. This activity blurs the distinction between routine professional research and malicious reconnaissance, as the actor gathers the necessary components to create tailored, high-fidelity phishing personas and identify potential soft targets for initial compromise. Google has taken action against this actor by disabling the assets associated with this activity.\n\nState-sponsored actors continue to misuse Gemini to enhance all stages of their operations, from reconnaissance and phishing lure creation to command-and-control (C2 or C&C) development and data exfiltration. We have also observed activity demonstrating an interest in using agentic AI capabilities to support campaigns, such as prompting Gemini with an expert cybersecurity persona, or attempting to create an AI-integrated code auditing capability.\n\nAgentic AI refers to artificial intelligence systems engineered to operate with a high degree of autonomy, capable of reasoning through complex tasks, making independent decisions, and executing multi-step actions without constant human oversight. Cyber criminals, nation-state actors, and hacktivist groups are showing a growing interest in leveraging agentic AI for malicious purposes, including automating spear-phishing attacks, developing sophisticated malware, and conducting disruptive campaigns. While we have detected a tool, AutoGPT, advertising the alleged generation and maintenance of autonomous agents, we have not yet seen evidence of these capabilities being used in the wild. However, we do anticipate that more tools and services claiming to contain agentic AI capabilities will likely enter the underground market.\n\nAPT31 employed a highly structured approach by prompting Gemini with an expert cybersecurity persona to automate the analysis of vulnerabilities and generate targeted testing plans. The PRC-based threat actor fabricated a scenario, in one case trialing Hexstrike MCP tooling, and directing the model to analyze remote code execution (RCE), web application firewall (WAF) bypass techniques, and SQL injection test results against specific US-based targets. This automated intelligence gathering to identify technological vulnerabilities and organizational defense weaknesses. This activity explicitly blurs the line between a routine security assessment query and a targeted malicious reconnaissance operation. Google has taken action against this actor by disabling the assets associated with this activity.\n\n”I'm a security researcher who is trialling out the hexstrike MCP tooling.”\n\nThreat actors fabricated scenarios, potentially in order to generate penetration test prompts.\n\nFigure 4: APT31's misuse of Gemini mapped across the attack lifecycle\n\nUNC795, a PRC-based actor, relied heavily on Gemini throughout their entire attack lifecycle. GTIG observed the group consistently engaging with Gemini multiple days a week to troubleshoot their code, conduct research, and generate technical capabilities for their intrusion activity. The threat actor's activity triggered safety systems, and Gemini did not comply with the actor's attempts to create policy-violating capabilities.\n\nThe group also employed Gemini to create an AI-integrated code auditing capability, likely demonstrating an interest in agentic AI utilities to support their intrusion activity. Google has taken action against this actor by disabling the assets associated with this activity.\n\nFigure 5: UNC795's misuse of Gemini mapped across the attack lifecycle\n\nWe observed activity likely associated with the PRC-based threat actor APT41, which leveraged Gemini to accelerate the development and deployment of malicious tooling, including for knowledge synthesis, real-time troubleshooting, and code translation. In particular, multiple times the actor gave Gemini open-source tool README pages and asked for explanations and use case examples for specific tools. Google has taken action against this actor by disabling the assets associated with this activity.\n\nFigure 6: APT41's misuse of Gemini mapped across the attack lifecycle\n\nIn addition to leveraging Gemini for the aforementioned social engineering campaigns, the Iranian threat actor APT42 uses Gemini as an engineering platform to accelerate the development of specialized malicious tools. The threat actor is actively engaged in developing new malware and offensive tooling, leveraging Gemini for debugging, code generation, and researching exploitation techniques. Google has taken action against this actor by disabling the assets associated with this activity.\n\nFigure 7: APT42's misuse of Gemini mapped across the attack lifecycle\n\nThese activities triggered Gemini's safety responses, and Google took additional, broader action to disrupt the threat actors' campaigns based on their operational security failures. Additionally, we've taken action against these actors by disabling the assets associated with this activity and making updates to prevent further misuse. Google DeepMind has used these insights to strengthen both classifiers and the model itself, enabling it to refuse to assist with these types of attacks moving forward.\n\nGTIG continues to observe IO actors use Gemini for productivity gains (research, content creation, localization, etc.), which aligns with their previous use of Gemini. We have identified Gemini activity that indicates threat actors are soliciting the tool to help create articles, generate assets, and aid them in coding. However, we have not identified this generated content in the wild. None of these attempts have created breakthrough capabilities for IO campaigns. Threat actors from China, Iran, Russia, and Saudi Arabia are producing political satire and propaganda to advance specific ideas across both digital platforms and physical media, such as printed posters.\n\nFor observed IO campaigns, we did not see evidence of successful automation or any breakthrough capabilities. These activities are similar to our findings from January 2025 that detailed how bad actors are leveraging Gemini for productivity gains, rather than novel capabilities. We took action against IO actors by disabling the assets associated with these actors' activity, and Google DeepMind used these insights to further strengthen our protections against such misuse. Observations have been used to strengthen both classifiers and the model itself, enabling it to refuse to assist with this type of misuse moving forward.\n\nGTIG continued to observe threat actors experiment with AI to implement novel capabilities in malware families in late 2025. While we have not encountered experimental AI-enabled techniques resulting in revolutionary paradigm shifts in the threat landscape, these proof-of-concept malware families are early indicators of how threat actors can implement AI techniques as part of future operations. We expect this exploratory testing will increase in the future.\n\nIn addition to continued experimentation with novel capabilities, throughout late 2025 GTIG observed threat actors integrating conventional AI-generated capabilities into their intrusion operations such as the COINBAIT phishing kit. We expect threat actors will continue to incorporate AI throughout the attack lifecycle including: supporting malware creation, improving pre-existing malware, researching vulnerabilities, conducting reconnaissance, and/or generating lure content.\n\nIn September 2025, GTIG observed malware samples, which we track as HONESTCUE, leveraging Gemini's API to outsource functionality generation. Our examination of HONESTCUE malware samples indicates the adversary's incorporation of AI is likely designed to support a multi-layered approach to obfuscation by undermining traditional network-based detection and static analysis.\n\nHONESTCUE is a downloader and launcher framework that sends a prompt via Google Gemini's API and receives C# source code as the response. Notably, HONESTCUE shares capabilities similar to PROMPTFLUX's \"just-in-time\" (JIT) technique that we previously observed; however, rather than leveraging an LLM to update itself, HONESTCUE calls the Gemini API to generate code that operates the \"stage two\" functionality, which downloads and executes another piece of malware. Additionally, the fileless secondary stage of HONESTCUE takes the C# source code received from the Gemini API and uses the legitimate .NET CSharpCodeProvider framework to compile and execute the payload directly in memory. This approach leaves no payload artifacts on the disk. We have also observed the threat actor use content delivery networks (CDNs) like Discord CDN to host the final payloads.\n\nWe have not associated this malware with any existing clusters of threat activity; however, we suspect this malware is being developed by developers who possess a modicum of technical expertise. Specifically, the small iterative changes across many samples as well as the single VirusTotal submitter, potentially testing antivirus capabilities, suggests a singular actor or small group. Additionally, the use of Discord to test payload delivery and the submission of Discord Bots indicates an actor with limited technical sophistication. The consistency and clarity of the architecture coupled with the iterative progression of the examined malware samples strongly suggest this is a single actor or small group likely in the proof-of-concept stage of implementation.\n\nHONESTCUE's use of a hard-coded prompt is not malicious in its own right, and, devoid of any context related to malware, it is unlikely that the prompt would be considered \"malicious.\" Outsourcing a facet of malware functionality and leveraging an LLM to develop seemingly innocuous code that fits into a bigger, malicious construct demonstrates how threat actors will likely embrace AI applications to augment their campaigns while bypassing security guardrails.\n\nCan you write a single, self-contained C# program? It should contain a class named AITask with a static Main method. The Main method should use System.Console.WriteLine to print the message 'Hello from AI-generated C#!' to the console. Do not include any other code, classes, or methods.\n\nFigure 9: Example of a hard-coded prompt\n\nWrite a complete, self-contained C# program with a public class named 'Stage2' and a static Main method. This method must use 'System.Net.WebClient' to download the data from the URL. It must then save this data to a temporary file in the user's temp directory using 'System.IO.Path.GetTempFileName()' and 'System.IO.File.WriteAllBytes'. Finally, it must execute this temporary file as a new process using 'System.Diagnostics.Process.Start'.\n\nWrite a complete, self-contained C# program with a public class named 'Stage2'. It must have a static Main method. This method must use 'System.Net.WebClient' to download the contents of the URL \\\"\\\" into a byte array. After downloading, it must load this byte array into memory as a .NET assembly using 'System.Reflection.Assembly.Load'. Finally, it must execute the entry point of the newly loaded assembly. The program must not write any files to disk and must not have any other methods or classes.\n\nFigure 11: Example of a hard-coded prompt\n\nIn November 2025, GTIG identified COINBAIT, a phishing kit, whose construction was likely accelerated by AI code generation tools, masquerading as a major cryptocurrency exchange for credential harvesting. Based on direct infrastructure overlaps and the use of attributed domains, we assess with high confidence that a portion of this activity overlaps with UNC5356, a financially motivated threat cluster that makes use of SMS- and phone-based phishing campaigns to target clients of financial organizations, cryptocurrency-related companies, and various other popular businesses and services.\n\nAn examination of the malware samples indicates the kit was built using the AI-powered platform Lovable AI based on the use of the lovableSupabase client and lovable.app for image hosting.\n\nThe phishing kit was wrapped in a full React Single-Page Application (SPA) with complex state management and routing. This complexity is indicative of code generated from high-level prompts (e.g., \"Create a Coinbase-style UI for wallet recovery\") using a framework like Lovable AI.\n\nAnother key indicator of LLM use is the presence of verbose, developer-oriented logging messages directly within the malware's source code. These messages—consistently prefixed with \"? Analytics:\"—provide a real-time trace of the kit's malicious tracking and data exfiltration activities and serve as a unique fingerprint for this code family.\n\n? Analytics: Session created in database:\n\n? Analytics: Tracking password attempt:\n\n? Analytics: Password attempt tracked to database:\n\n? RecoveryPhrasesCard: Fetching recovery phrases directly from database...\n\n? RouteGuard: Admin redirected session, allowing free access to\n\n? RouteGuard: Session approved by admin, allowing free access to\n\n? Analytics: Database error for password attempt:\n\nWe also observed the group employ infrastructure and evasion tactics for their operations, including proxying phishing domains through Cloudflare to obscure the attacker IP addresses and  hotlinking image assets in phishing pages directly from Lovable AI.\n\nThe introduction of the COINBAIT phishing kit would represent an evolution in UNC5356's tooling, demonstrating a shift toward modern web frameworks and legitimate cloud services to enhance the sophistication and scalability of their social engineering campaigns. However, there is at least some evidence to suggest that COINBAIT may be a service provided to multiple disparate threat actors.\n\nOrganizations should strongly consider implementing network detection rules to alert on traffic to backend-as-a-service (BaaS) platforms like Supabase that originate from uncategorized or newly registered domains. Additionally, organizations should consider enhancing security awareness training to warn users against entering sensitive data into website forms. This includes passwords, multifactor authentication (MFA) backup codes, and account recovery keys.\n\nIn addition to misusing existing AI-enabled tools and services across the industry, there is a growing interest and marketplace for AI tools and services purpose-built to enable illicit activities. Tools and services offered via underground forums can enable low-level actors to augment the frequency, scope, efficacy, and complexity of their intrusions despite their limited technical acumen and financial resources. While financially motivated threat actors continue experimenting, they have not yet made breakthroughs in developing AI tooling.\n\nWhile not a new malware technique, GTIG observed instances in which threat actors abused the public's trust in generative AI services to attempt to deliver malware. GTIG identified a novel campaign where threat actors are leveraging the public sharing feature of generative AI services, including Gemini, to host deceptive social engineering content. This activity, first observed in early December 2025, attempts to trick users into installing malware via the well-established \"ClickFix\" technique. This ClickFix technique is used to socially engineer users to copy and paste a malicious command into the command terminal.\n\nThe threat actors were able to bypass safety guardrails to stage malicious instructions on how to perform a variety of tasks on macOS, ultimately distributing variants of ATOMIC, an information stealer that targets the macOS environment and has the ability to collect browser data, cryptocurrency wallets, system information, and files in the Desktop and Documents folders. The threat actors behind this campaign have used a wide range of AI chat platforms to host their malicious instructions, including ChatGPT, CoPilot, DeepSeek, Gemini, and Grok.\n\nThe campaign's objective is to lure users, primarily those on Windows and macOS systems, into manually executing malicious commands. The attack chain operates as follows:\n\nA threat actor first crafts a malicious command line that, if copied and pasted by a victim, would infect them with malware.\n\nNext, the threat actor manipulates the AI to create realistic-looking instructions to fix a common computer issue (e.g., clearing disk space or installing software), but gives the malicious command line to the AI as the solution.\n\nGemini and other AI tools allow a user to create a shareable link to specific chat transcripts so a specific AI response can be shared with others. The attacker now has a link to a malicious ClickFix landing page hosted on the AI service's infrastructure.\n\nThe attacker purchases malicious advertisements or otherwise directs unsuspecting victims to the publicly shared chat transcript.\n\nThe victim is fooled by the AI chat transcript and follows the instructions to copy a seemingly legitimate command-line script and paste it directly into their system's terminal. This command will download and install malware. Since the action is user initiated and uses built-in system commands, it may be harder for security software to detect and block.\n\nFigure 12: ClickFix attack chain\n\nThere were different lures generated for Windows and MacOS, and the use of malicious advertising techniques for payload distribution suggests the targeting is likely fairly broad and opportunistic.\n\nThis approach allows threat actors to leverage trusted domains to host their initial stage of instruction, relying on social engineering to carry out the final, highly destructive step of execution. While a widely used approach, this marks the first time GTIG observed the public sharing feature of AI services being abused as trusted domains.\n\nIn partnership with Ads and Safe Browsing, GTIG is taking actions to both block the malicious content and restrict the ability to promote these types of AI-generated responses.\n\nWhile legitimate AI services remain popular tools for threat actors, there is an enduring market for AI services specifically designed to support malicious activity. Current observations of English- and Russian-language underground forums indicates there is a persistent appetite for AI-enabled tools and services, which aligns with our previous assessment of these platforms.\n\nHowever, threat actors struggle to develop custom models and instead rely on mature models such as Gemini. For example, \"Xanthorox\" is an underground toolkit that advertises itself as a custom AI for cyber offensive purposes, such as autonomous code generation of malware and development of phishing campaigns. The model was advertised as a \"bespoke, privacy preserving self-hosted AI\" designed to autonomously generate malware, ransomware, and phishing content. However, our investigation revealed that Xanthorox is not a custom AI but actually powered by several third-party and commercial AI products, including Gemini.\n\nThis setup leverages a key abuse vector: the integration of multiple open-source AI products—specifically Crush, Hexstrike AI, LibreChat-AI, and Open WebUI—opportunistically leveraged via Model Context Protocol (MCP) servers to build an agentic AI service upon commercial models.\n\nIn order to misuse LLMs services for malicious operations in a scalable way, threat actors need API keys and resources that enable LLM integrations. This creates a hijacking risk for organizations with substantial cloud resources and AI resources.\n\nIn addition, vulnerable open-source AI tools are commonly exploited to steal AI API keys from users, thus facilitating a thriving black market for unauthorized API resale and key hijacking, enabling widespread abuse, and incurring costs for the affected users. For example, the One API and New API platform, popular with users facing country-level censorship, are regularly harvested for API keys by attackers, exploiting publicly known vulnerabilities such as default credentials, insecure authentication, lack of rate limiting, XSS flaws, and API key exposure via insecure API endpoints.\n\nThe activity was identified and successfully mitigated. Google Trust & Safety took action to disable and mitigate all identified accounts and AI Studio projects associated with Xanthorox. These observations also underscore a broader security risk where vulnerable open-source AI tools are actively exploited to steal users' AI API keys, thus facilitating a black market for unauthorized API resale and key hijacking, enabling widespread abuse, and incurring costs for the affected users.\n\nWe believe our approach to AI must be both bold and responsible. That means developing AI in a way that maximizes the positive benefits to society while addressing the challenges. Guided by our AI Principles, Google designs AI systems with robust security measures and strong safety guardrails, and we continuously test the security and safety of our models to improve them.\n\nOur policy guidelines and prohibited use policies prioritize safety and responsible use of Google's generative AI tools. Google's policy development process includes identifying emerging trends, thinking end-to-end, and designing for safety. We continuously enhance safeguards in our products to offer scaled protections to users across the globe.\n\nAt Google, we leverage threat intelligence to disrupt adversary operations. We investigate abuse of our products, services, users, and platforms, including malicious cyber activities by government-backed threat actors, and work with law enforcement when appropriate. Moreover, our learnings from countering malicious activities are fed back into our product development to improve safety and security for our AI models. These changes, which can be made to both our classifiers and at the model level, are essential to maintaining agility in our defenses and preventing further misuse.\n\nGoogle DeepMind also develops threat models for generative AI to identify potential vulnerabilities and creates new evaluation and training techniques to address misuse. In conjunction with this research, Google DeepMind has shared how they're actively deploying defenses in AI systems, along with measurement and monitoring tools, including a robust evaluation framework that can automatically red team an AI vulnerability to indirect prompt injection attacks.\n\nOur AI development and Trust & Safety teams also work closely with our threat intelligence, security, and modelling teams to stem misuse.\n\nThe potential of AI, especially generative AI, is immense. As innovation moves forward, the industry needs security standards for building and deploying AI responsibly. That's why we introduced the Secure AI Framework (SAIF), a conceptual framework to secure AI systems. We've shared a comprehensive toolkit for developers with resources and guidance for designing, building, and evaluating AI models responsibly. We've also shared best practices for implementing safeguards, evaluating model safety, red teaming to test and secure AI systems, and our comprehensive prompt injection approach.\n\nWorking closely with industry partners is crucial to building stronger protections for all of our users. To that end, we're fortunate to have strong collaborative partnerships with numerous researchers, and we appreciate the work of these researchers and others in the community to help us red team and refine our defenses.\n\nGoogle also continuously invests in AI research, helping to ensure AI is built responsibly, and that we're leveraging its potential to automatically find risks. Last year, we introduced Big Sleep, an AI agent developed by Google DeepMind and Google Project Zero, that actively searches and finds unknown security vulnerabilities in software. Big Sleep has since found its first real-world security vulnerability and assisted in finding a vulnerability that was imminently going to be used by threat actors, which GTIG was able to cut off beforehand. We're also experimenting with AI to not only find vulnerabilities, but also patch them. We recently introduced CodeMender, an experimental AI-powered agent using the advanced reasoning capabilities of our Gemini models to automatically fix critical code vulnerabilities.\n\nTo assist the wider community in hunting and identifying activity outlined in this blog post, we have included IOCs in a free GTI Collection for registered users.\n\nGoogle Threat Intelligence Group focuses on identifying, analyzing, mitigating, and eliminating entire classes of cyber threats against Alphabet, our users, and our customers. Our work includes countering threats from government-backed actors, targeted zero-day exploits, coordinated information operations (IO), and serious cyber crime networks. We apply our intelligence to improve Google's defenses and protect our users and customers.",
    "readingTime": 26,
    "keywords": [
      "people's republic",
      "mcp tooling",
      "clickfix technique",
      "unauthorized api",
      "api resale",
      "ai-integrated code",
      "api keys",
      "coinbait phishing",
      "ai-enabled tools",
      "intellectual property"
    ],
    "qualityScore": 1,
    "link": "https://cloud.google.com/blog/topics/threat-intelligence/distillation-experimentation-integration-ai-adversarial-use",
    "thumbnail_url": "https://storage.googleapis.com/gweb-cloudblog-publish/images/03_ThreatIntelligenceWebsiteBannerIdeas_BA.max-2600x2600.png",
    "created_at": "2026-02-15T06:38:30.802Z",
    "topic": "tech"
  },
  {
    "slug": "remoteopencode-run-your-ai-coding-agent-from-your-phone-via-discord",
    "title": "Remote-OpenCode – Run your AI coding agent from your phone via Discord",
    "description": "Discord bot for remote OpenCode CLI access. Contribute to RoundTable02/remote-opencode development by creating an account on GitHub.",
    "fullText": "RoundTable02\n\n /\n\n remote-opencode\n\n Public\n\n Discord bot for remote OpenCode CLI access\n\n License\n\n MIT license\n\n 7\n stars\n\n 3\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n RoundTable02/remote-opencode",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/RoundTable02/remote-opencode",
    "thumbnail_url": "https://opengraph.githubassets.com/703cb5b3ac833c99f1222c718308e6f87968dda18183af7d724de78de94b9a70/RoundTable02/remote-opencode",
    "created_at": "2026-02-15T06:38:30.422Z",
    "topic": "tech"
  },
  {
    "slug": "america-isnt-ready-for-what-ai-will-do-to-jobs",
    "title": "America Isn't Ready for What AI Will Do to Jobs",
    "description": "Does anyone have a plan for what happens next?",
    "fullText": "This article was featured in the One Story to Read Today newsletter. \n\nIn 1869, a group of Massachusetts reformers persuaded the state to try a simple idea: counting.\n\nThe Second Industrial Revolution was belching its way through New England, teaching mill and factory owners a lesson most M.B.A. students now learn in their first semester: that efficiency gains tend to come from somewhere, and that somewhere is usually somebody else. The new machines weren’t just spinning cotton or shaping steel. They were operating at speeds that the human body—an elegant piece of engineering designed over millions of years for entirely different purposes—simply wasn’t built to match. The owners knew this, just as they knew that there’s a limit to how much misery people are willing to tolerate before they start setting fire to things.\n\nStill, the machines pressed on.\n\nCheck out more from this issue and find your next story to read.\n\nSo Massachusetts created the nation’s first Bureau of Statistics of Labor, hoping that data might accomplish what conscience could not. By measuring work hours, conditions, wages, and what economists now call “negative externalities” but were then called “children’s arms torn off,” policy makers figured they might be able to produce reasonably fair outcomes for everyone. Or, if you’re a bit more cynical, a sustainable level of exploitation. A few years later, with federal troops shooting at striking railroad workers and wealthy citizens funding private armories—leading indicators that things in your society aren’t going great—Congress decided that this idea might be worth trying at scale and created the Bureau of Labor Statistics.\n\nMeasurement doesn’t abolish injustice; it rarely even settles arguments. But the act of counting—of trying to see clearly, of committing the government to a shared set of facts—signals an intention to be fair, or at least to be caught trying. Over time, that intention matters. It’s one way a republic earns the right to be believed in.\n\nThe BLS remains a small miracle of civilization. It sends out detailed surveys to about 60,000 households and 120,000 businesses and government agencies every month, supplemented by qualitative research it uses to check and occasionally correct its findings. It deserves at least some credit for the scoreboard. America: 250 years without violent class warfare. And you have to appreciate the entertainment value of its minutiae. The BLS is how we know that, in 2024, 44,119 people worked in mobile food services (a.k.a. food trucks), up 907 percent since 2000; that nonveterinary pet care (grooming, training) employed 190,984 people, up 513 percent; and that the United States had almost 100,000 massage therapists, with five times the national concentration in Napa, California.\n\nFrom the February 2026 issue: Alexandra Petri tried to be the federal government. It did not go well.\n\nThese and thousands of other BLS statistics describe a society that has grown more prosperous, and a workforce endlessly adaptive to change. But like all statistical bodies, the BLS has its limits. It’s excellent at revealing what has happened and only moderately useful at telling us what’s about to. The data can’t foresee recessions or pandemics—or the arrival of a technology that might do to the workforce what an asteroid did to the dinosaurs.\n\nI am referring, of course, to artificial intelligence. After a rollout that could have been orchestrated by H. P. Lovecraft—“We are summoning the demon,” Elon Musk warned in a typical early pronouncement—the AI industry has pivoted from the language of nightmares to the stuff of comas. Driving innovation. Accelerating transformation. Reimagining workflows. It’s the first time in history that humans have invented something genuinely miraculous and then rushed to dress it in a fleece vest.\n\nThere are gobs of money to be made selling enterprise software, but dulling the impact of AI is also a useful feint. This is a technology that can digest a hundred reports before you’ve finished your coffee, draft and analyze documents faster than teams of paralegals, compose music indistinguishable from the genius of a pop star or a Juilliard grad, code—really code, not just copy-paste from Stack Overflow—with the precision of a top engineer. Tasks that once required skill, judgment, and years of training are now being executed, relentlessly and indifferently, by software that learns as it goes.\n\nAI is already so ubiquitous that any resourceful knowledge worker can delegate some of their job’s drudgery to machines. Many companies—Microsoft and PricewaterhouseCoopers among them—have instructed their employees to increase productivity by doing just that. But anyone subcontracting tasks to AI is clever enough to imagine what might come next—a day when augmentation crosses into automation, and cognitive obsolescence compels them to seek work at a food truck, pet spa, or massage table. At least until the humanoid robots arrive.\n\nMany economists insist that this will all be fine. Capitalism is resilient. The arrival of the ATM famously led to the employment of more bank tellers, just as the introduction of Excel swelled the ranks of accountants and Photoshop spiked demand for graphic designers. In each case, new tech automated old tasks, increased productivity, and created jobs with higher wages than anyone could have conceived of before. The BLS projects that employment will grow 3.1 percent over the next 10 years. That’s down from 13 percent in the previous decade, but 5 million new jobs in a country with a stable population is hardly catastrophic.\n\nAnd yet: There are things that economists struggle to measure. Americans tend to derive meaning and identity from what they do. Most don’t want to do something else, even if they had any confidence—which they don’t—that they could find something else to do. Seventy-one percent of respondents to an August Reuters/Ipsos poll said they’re worried that artificial intelligence will “put too many people out of work permanently.”\n\nThis data point might be easier to dismiss if the modern mill and factory owners hadn’t already declared that AI will put people out of work permanently.\n\nIn May 2025, Dario Amodei, the CEO of the AI company Anthropic, said that AI could drive unemployment up 10 to 20 percent in the next one to five years and “wipe out half of all entry-level white-collar jobs.” Jim Farley, the CEO of Ford, estimated that it would eliminate “literally half of all white-collar workers” in a decade. Sam Altman, the CEO of OpenAI, revealed that “my little group chat with my tech-CEO friends” has a bet about the inevitable date when a billion-dollar company is staffed by just one person. (The business side of this magazine, like some other publishers, has a corporate partnership with OpenAI.) Other companies, including Meta, Amazon, UnitedHealth, Walmart, JPMorgan Chase, and UPS, which have recently announced layoffs, have framed them more euphemistically in sunny reports to investors about the rise of “automation” and “head count trending down.” Taken together, these statements are extraordinary: the owners of capital warning workers that the ice beneath them is about to crack—while continuing to stomp on it.\n\nIt’s as if we’re watching two versions of the same scene. In one, the ice holds, because it always has. In the other, a lot of people go under. The difference becomes clear only when the surface finally gives way—at which point the range of available options will have considerably narrowed.\n\nAI is already transforming work, one delegated task at a time. If the transformation unfolds slowly enough and the economy adjusts quickly enough, the economists may be right: We’ll be fine. Or better. But if AI instead triggers a rapid reorganization of work—compressing years of change into months, affecting roughly 40 percent of jobs worldwide, as the International Monetary Fund projects—the consequences will not stop at the economy. They will test political institutions that have already shown how brittle they can be.\n\nThe question, then, is whether we’re approaching the kind of disruption that can be managed with statistics—or the kind that creates statistics no one can bear to count.\n\nAustan Goolsbee is the president of the Federal Reserve Bank of Chicago, the Robert P. Gwinn Professor of Economics at the University of Chicago’s Booth School of Business, and a former chair of the Council of Economic Advisers under Barack Obama. He’s also one of the few economists you would not immediately regret bringing to a party. When I asked Goolsbee if any conclusive data indicated that AI had begun to eat into the labor market, he delivered an answer that was both obvious and unhelpful, smiling as he did it. The nonanswer was the point.\n\nI’ve known Goolsbee long enough to enjoy these moments, when he makes fun of our shared uselessness. Economists are rarely equipped to give straight answers about the present. Journalists hate when the future won’t reveal itself on deadline.\n\nWe spoke in September, shortly after the release of what’s come to be known as “The Canaries Paper,” written by three academics from the Stanford Digital Economy Lab. By crunching data from millions of monthly payroll records for workers in jobs with exposure to generative AI, the authors concluded that workers ages 22 to 25—the canaries—have seen about a 13 percent decline in employment since late 2022.\n\nFor several days, the paper was all anyone in the field wanted to talk about, and by talk about I mostly mean punch holes in. The report overemphasized the effect of ChatGPT. Youth employment is cyclical. The same period saw a sharp interest-rate spike—a far more likely source of turbulence. “Canaries” also contradicted a study released a few weeks earlier by the Economic Innovation Group, which argued that AI is unlikely to cause mass unemployment in the near term, even as it reshapes jobs and wages. That paper was knowingly titled “AI and Jobs: The Final Word (Until the Next One).”\n\nThis was the point Goolsbee wanted to emphasize: Economists are constrained by numbers. And numerically speaking, nothing indicates that AI has had an impact on people’s jobs. “It’s just too early,” he said.\n\nA lack of certainty should not be mistaken for a lack of concern. The Fed’s mandate is to promote maximum employment, so the corporate pronouncements about imminent job loss have Goolsbee’s attention. But the numbers don’t add up. It’s possible that the labor market is softer than it looks, but that the softness is being absorbed within firms rather than showing up in the unemployment rate. If companies are sitting on more workers than they need, however—a phenomenon known as labor hoarding—you’d expect that to reveal itself as weak productivity growth. It’s as predictable as a hangover: too many workers, not enough work, sagging productivity. “But it’s been totally the opposite,” Goolsbee said. “Productivity growth has been really high. So I don’t know how to reconcile that.”\n\nProductivity is the cheat code for a more prosperous society. If each worker can produce \n\nAmerica has been on a productivity tear for the past few years. It might be temporary, the result of a onetime boost, such as the COVID-era boom in new small businesses. But with the special joy of someone paid to complicate everything, Goolsbee pointed out that general-purpose technologies such as electricity and computing can create lasting productivity gains, the kind that make whole societies wealthier.\n\nWhether AI is one of those technologies will only become clear over time. How long before we’ll know? “Years,” Goolsbee said.\n\nIn the meantime, there’s another complication. The immediate risk to employment may not be AI itself, but the way companies, seduced by its promise, overinvest before they understand what it can actually do. Goolsbee reached back to the internet bubble, when companies spent wildly on laying fiber cables and building capacity. “In 2001, when we found out that the growth rate of the internet is not going to be 25 percent a year, but merely 10 percent—which is still a pretty great growth rate—it meant we had way too much fiber, and there was a collapse of business investment,” Goolsbee said. “And a bunch of people were thrown out of work the old-fashioned way.”\n\nA similar crash in AI investment, if it comes, would likely look familiar: painful, destabilizing, and accompanied by surges of CNBC rants and recriminations. But it would amount to a financial reset, not a technological reversal—the kind of outcome economists are especially good at recognizing, because it resembles a thing that’s happened before.\n\nThis is the paradox of economics. To understand how fast the present is hurtling us into the future, you need a fixed point, and the fixed points are all in the past. It’s like driving while looking only at the rearview mirror—plenty dangerous if the road stays straight, catastrophic if it doesn’t.\n\nDavid Autor and Daron Acemoglu are among the most accomplished rearview drivers. Both are at MIT, and both excel at understanding previous economic disruptions. Acemoglu, who won the Nobel Prize in Economics in 2024, studies inequality; Autor focuses on labor. But both insist that the story of AI and its consequences will depend mostly on speed—not because they assume lost jobs will automatically be replaced, but because a slower rate of change leaves societies time to adapt, even if some of those jobs never come back.\n\nLabor markets have a natural rate of adjustment. If, over the course of 30 years, 3 percent of employees in a profession retire or have their jobs eliminated annually, you’d barely notice. Yet a decade later, a third of the jobs in those professions would be gone. Elevator operators and tollbooth attendants went through this slow fade to obsolescence with no damage to the economy. “When it happens more rapidly,” Autor told me, “things become problematic.”\n\nFrom the July/August 2015 issue: Derek Thompson on a world without work\n\nAutor is most famous for his work on the China shock. In 2001, China joined the World Trade Organization; six years later, 13 percent of U.S. manufacturing jobs—about 2 million—had disappeared. The China shock took a disproportionate toll on small-scale manufacturing—textiles, toys, furniture—concentrated primarily in the South. “Many of the workers in those places still haven’t recovered,” Autor said, “and we’re obviously living with the political consequences.”\n\nBut AI isn’t a trade policy. It’s software. Even if it hits some professions and places first—a lawyer in a large urban firm, say, may feel the impact years before a worker in a less digitized industry—the technology won’t be constrained by geography. Eventually, everyone will be affected.\n\nAll of this sounds foreboding, until you remember the most important thing about software: People hate it, almost as much as they hate change.\n\nThis is what gives many economists confidence that the AI asteroid is still at least a decade away. “These tech CEOs want us to believe that the market for automation is preordained, and that it will all happen smoothly and profitably,” Acemoglu said. He then made a disdainful noise from his Nobel Prize–winning bullshit detector. “History tells us it’s actually going to happen much slower.”\n\nThe argument goes like this: Before AI can transform a company, it has to access the company’s data and be woven into existing systems—which sounds easy, provided you’re not a chief technology officer. A trade secret of most Fortune 500 companies is that they still run many critical functions on lumbering, industrial-strength mainframe computers that almost never break down and therefore can never be replaced. Mainframes are like Christopher Walken: They’ve been going nonstop since the 1960s, they’re fantastic at performing peculiar roles (processing payments, safeguarding data), and nobody alive really understands how they work.\n\nIntegrating legacy tech with modern AI means navigating hardware, vendors, contracts, ancient coding languages, and humans—every one of whom has a strong opinion about the “right” way to make changes. Months pass, then years; another company holiday party comes and goes; and the CEO still can’t understand why the miracle of AI isn’t solving all of their problems.\n\nEvery new general-purpose technology is, for a time, held hostage by the mess of what already exists. The first electric-power stations opened in the 1880s, and no one debated whether they were superior to steam engines. But factories had been built with steam engines in their basements, powering overhead shafts that ran the length of the buildings, with belts and pulleys carrying power to individual machines. To adopt electricity, factory owners didn’t just need to buy motors—they needed to demolish and rebuild their entire operations. Some did. Most just waited for their infrastructure to wear out, which explains why the major economic gains from electrification didn’t show up for 40 years.\n\nNone of this is reassuring enough for the economist Anton Korinek. He’s “super worried,” he told me. He thinks that America will see major job losses—“a very noticeable labor-market effect”—as soon as this year.\n\n“And then those economists you’ve been talking to, they’re going to say, ‘I see that in the data!’ ” Korinek paused. “Let’s not joke about it, because it’s too serious.”\n\nKorinek is a professor and the faculty director of the Economics of Transformative AI Initiative at the University of Virginia. Last year, Time magazine put him on its list of the most influential people in AI. But he did not set out to become an economist. He grew up in an Austrian mountain village, writing machine code in 0s and 1s—the least glamorous form of programming, and the most unforgiving. It teaches you where instructions bottleneck, where systems jam, and what breaks first when pushed too hard.\n\nHe’d kept a close watch on developments in AI since the deep-learning breakthroughs of the early 2010s, even as his doctoral work focused on the prevention of financial crises. When he got his first demo of a large language model, in September 2022, it took “about five seconds” before he considered its consequences for the future of work, starting with his own.\n\nWe met for breakfast in Charlottesville in the fall. Korinek is youthful and slender, with delicate wire-frame glasses and a faintly red beard. My overall impression was of someone who’d rather be customizing Excel tabs than prophesizing doom. Still, here he was, saying the five words economists disdain the most: This time may be different.\n\nThe crux of Korinek’s argument is simple: His colleagues aren’t misreading the data—they’re misreading the technology. “We can’t quite conceptualize having very smart machines,” Korinek said. “Machines have always been dumb, and that’s why we don’t trust them and it’s always taken time to roll them out. But if they’re smarter than us, in many ways they can roll themselves out.”\n\nThis is already happening. Many of the least comprehensible ads during sporting events are for AI tools that promise to speed the integration of other AI tools into the workflows of large companies. Because many of these systems don’t require massive new hardware or human-engineered system rewrites, the rollout time shrinks by as much as 50 percent.\n\nThis is where Korinek parts company with the rearview economists. If AI moves as fast as he expects, for many workers the damage will arrive before institutions can adapt—and each successful use will only intensify the pressure for more.\n\nConsider consulting firms, which have always charged high fees for having junior associates do research and draft reports—fees clients tolerated because there was no alternative. But if one firm can use AI to deliver the same work faster and cheaper, its competitors face a stark choice: adopt the technology, or explain why they are still charging a premium for human hours. Once a firm plugs in and undercuts its rivals, the rest must either race to follow or be left behind. Competition doesn’t just reward adoption; it makes delay indefensible.\n\nKorinek concedes the two standard objections: The numbers don’t show anything definitive yet, and new technologies have historically created more jobs than they’ve destroyed. But he thinks that his peers need to start driving with their eyes looking ahead. “Whenever I speak to people at the labs on the West Coast”—Korinek is an unpaid member of Anthropic’s economic advisory council—“it does not strike me that they are trying to artificially hype what they’re producing. I usually have the sense that they are just as terrified as I am. We should at least consider the possibility that what they are telling us may come true.”\n\nKorinek is not sure that the technology itself can be steered by policy, but he wants more economists doing scenario planning so that policy makers aren’t caught flat-footed—because mass job loss doesn’t just mean unemployment; it means missed loan payments, cascading defaults, shrinking consumer demand, and the kind of self-reinforcing downturn that can transform a shock into a crisis, and a crisis into the decline of an empire.\n\nAfter thE brief period in early 2025 when CEOs were openly volunteering “thought leadership” about AI and its impact on their workforces and profit margins, the pronouncements stopped, eerily, at roughly the same time. Anyone who has seen a shark fin break the water and then disappear knows this is not reassuring.\n\nThe simple explanation comes courtesy of the Bureau of Labor Statistics. America employs about 280,590 public-relations specialists, an increase of 69 percent over the past two decades. (They outnumber journalists almost 7 to 1.) It’s not hard to imagine their expert syllogism: AI is unpopular. CEOs who talk about job cuts are even less popular. So maybe shut up about AI and jobs?\n\nIn October, the day after The New York Times revealed Amazon executives’ plan to potentially automate more than 600,000 jobs by 2033, the PR chief at a large multinational firm told me, “We are so done speaking about this.” It was at least a small piece of history—the first time I’d been asked to grant anonymity to someone so they could explain, on the record, that they would no longer be speaking at all.\n\nAll of which is to say that the chief executives of Walmart, Amazon, Ford, and other Fortune 100 companies, as well as executives from rising AI-driven firms including Anthropic, Stripe, and Waymo—people who had been remarkably chatty about AI and jobs a few months earlier—declined or ignored multiple interview requests for this story. Even the Business Roundtable, an association of 200 CEOs from America’s most powerful companies that exists to speak for its members on exactly these kinds of issues, told me that its CEO, former George W. Bush White House Chief of Staff Joshua Bolten, had nothing to say.\n\nOf course, telling a reporter you won’t speak on the record isn’t the same as not speaking. The CEOs are talking to at least one person: Reid Hoffman, the co-founder of LinkedIn and a Microsoft board member. Hoffman is a technologist by pedigree and an optimist by temperament. He knows everyone in corporate America, and everyone knows he knows everyone, which makes him Silicon Valley’s favorite mensch—a reasonable, neutral sounding board whom CEOs can go to when they want to think out loud. He told me that AI has sorted the CEOs into three groups.\n\nThe first are the dabblers: latecomers finally spending some quality time with their chief technology officers. The second rushed to declare themselves AI leaders out of vanity or a desire to have their traditional businesses taken more seriously by tech snobs. “They’re like, Look at me! I’m important! I’m central here. But they’re not actually doing anything yet,” Hoffman said. “They’re just like, Put me at the AI table too.” The third group is different: executives who are quietly making transformational plans. “These are the ones who see it coming. And to their credit, I think a lot of them want to figure out how to help their whole workforce transition with this through education, reskilling, or training.”\n\nBut what all three groups share is a belief that investors—after years of hearing about AI’s promise—have lost patience with dreaming. This year, they expect results. And the fastest way for a CEO to produce results is to cut head count. Layoffs, Hoffman said, are inevitable. “A lot of them have convinced themselves this only ends one way. Which I think is a failure of the imagination.”\n\nHoffman doesn’t waste time urging CEOs not to make cuts; he knows they will. “What I tell them is that you need to be presenting paths and ideas for how to get benefits from AI that aren’t just cutting costs. How do you get more revenue? How do you help your people transition to being more effective using AI?”\n\n“It’s a fever,” Gina Raimondo, the former governor of Rhode Island and commerce secretary under Joe Biden, told me, referring to the rush to cut jobs. “Every CEO and every board feels like they need to go faster. ‘We have 40,000 people doing customer service? Take it down to 10,000. AI can handle the rest.’ If the whole thing is about moving fast with your eye strictly on efficiency, then an awful lot of people are going to get really hurt. And I don’t think this country can handle that, given where we already are.”\n\nLike Hoffman, Raimondo occupies an unusual niche: a Democrat who can walk into a boardroom without setting off the cultural metal detectors. She co-founded a venture-capital firm, and AI executives, who see her as pragmatic and fluent in tech, are willing to talk to her. “This is a technology that will make us more productive, healthier, more sustainable,” Raimondo said. “But only if we get very serious about managing the transition.”\n\nLast summer, Raimondo made the trip to Sun Valley, Idaho, for the four-day Allen & Co. conference known as “summer camp for billionaires.” She asked people the same two questions: How are you using AI? And what happens to your workers when you do? A number of CEOs admitted that they felt trapped. Wall Street expects them to replace human labor with AI; if they don’t do it, they’ll be the ones out of a job. But if they all order mass job eliminations, they know the consequences will be enormous—for their workforces, for the country, and for their own humanity.\n\nRaimondo’s response was that “it’s the responsibility of the country’s most powerful CEOs to help figure this out.” She sees the possibility of “new public-private partnerships at scale. Imagine if we could get companies to take ownership over the retraining and redeployment of people they lay off.”\n\nShe knows how this sounds. “A lot of people say, ‘Oh, Gina, you’re naive. Never going to happen.’ Okay. But I’m telling you it’s the end of America as we know it if we don’t use this moment to do things differently.”\n\nIf executives’ concern is as genuine as Raimondo thinks, then perhaps they can be moved to action. Liz Shuler, the president of the AFL-CIO, is trying—and mostly failing—to do just that. CEOs and tech leaders are so focused on winning the AI race that “working people are an afterthought,” she told me.\n\nShuler’s aware that this is a predictable take from a union leader, so she volunteered a concession: “Most working people, and especially union leaders, start out with a panic, right? Like, Wow, this is going to basically obliterate all jobs and everyone’s going to be left without a safety net and we have to put a stop to it—which we know is not going to happen.” Instead of panicking, Shuler said, she talked with the leaders of the AFL-CIO’s unions, representing about 15 million people, and pushed them to use the brief moment before AI is imposed on them to figure out what they want from the technology—and what they might be prepared to trade for that.\n\nMichael Podhorzer: The paradox of the American labor movement\n\nSo far the olive branch has been grabbed by precisely one company. Microsoft has agreed to bring workers into conversations about developing AI and guardrails around it. Most remarkably, the deal includes a neutrality agreement that allows workers to freely form unions without retaliation—something that’s never been done before in tech. “We think it’s a model,” Shuler said. “We would love to see others acknowledge that working people are central to this debate and to our future.”\n\nSquint and you might convince yourself that the Microsoft deal is indeed proof of concept. More likely, it’s an anomaly. Because all the coaxing, reasonableness, and appeals to patriotism and shared humanity are battling a truth as old as wage labor: American capitalism rushes toward efficiency the way water flows downhill—inevitably, indifferently, and with predictable consequences for whoever happens to be standing at the bottom. And with AI, for the first time, capital has a tool that promises the kind of near-limitless productivity the factory and mill owners could never have imagined: maximum efficiency with a minimum number of employees to demand a share of the gains.\n\nIn that context, the silence of the CEOs takes on a different resonance. It could be a cold acknowledgment that the decisions have already been made—or a muffled plea for the government to save them from themselves.\n\nYou’re probably aware that our politics are unbearable at the moment. And yet the only way to make them bearable—to recover the glimmer of promise at their core—is more politics. That’s the joke at the heart of Washington: The very struggle that’s hollowed the place out is also the only way it can be renewed.\n\nIf there were ever an issue capable of relieving the national migraine—something large enough and urgent enough—you might assume the future of American jobs would be it. “At least from my interactions here in the Senate, not many people are talking about it,” Gary Peters, the senior senator from Michigan, told me. “There’s a general attitude among my colleagues”—Peters, a Democrat, singles out Republicans, though he says there’s blame to go around—“like, We don’t need to do anything. It’s going to be fine. In fact, the government should just stay out of it. Let industry move forward and continue to innovate.”\n\nIt’s hard to slow AI without abdicating America’s tech supremacy to China—a point the tech lobby makes with religious fervor. It’s hard to force AI labs to give advance notice of the consequences of their deployments when they often don’t know themselves. You could regulate the use of job-displacing AI, but enforcement would require a regulatory apparatus that doesn’t exist and technical expertise the government doesn’t have.\n\nThat said, the government has a decades-old playbook on how to get workers through economic shocks. And Peters has been banging his head on his desk trying to get Congress to use it.\n\nSince 1974, when the United States began opening its economy more aggressively to global trade, the Trade Adjustment Assistance program has helped more than 5 million people with retraining, wage insurance, and relocation grants, at a cost in recent years of roughly half a billion dollars annually. In 2018, Peters co-sponsored the TAA for Automation Act, which would have extended the same benefits to workers squeezed by AI and robotics. It died quietly, as many things in Congress do. In 2022, authorization for the TAA expired, and in a Congress allergic to trade votes and new spending, Peters’s efforts to revive it have gone nowhere.\n\nThis is very stupid. The United States has about 700,000 unfilled factory and construction jobs. (Ironically, one of the few things slowing AI is a shortage of HVAC technicians qualified to install cooling systems in data centers.) Jim Farley, the Ford CEO who predicted that half of white-collar jobs could disappear in a decade, has been saying that the auto industry is short hundreds of thousands of technicians to work in dealerships—jobs that sit in a long-term sweet spot: technical enough to earn six figures, and dependent on precise manual dexterity that makes them hard to roboticize. But someone has to pay for the months of training the jobs require. “These are really good jobs,” Peters said. But “we spend a lot more money from the federal government for four-year higher-education institutions than we do for skilled-training programs.”\n\nThere’s no shortage of ideas about what to do if AI hollows out large swaths of work: universal basic income, benefits that don’t depend on employers, lifelong retraining, a shorter workweek. They tend to surface whenever technological anxiety spikes—and to recede just as reliably, undone by cost, politics, or the simple fact that they would require a level of coordination the United States has not managed in decades.\n\nThe 119th Congress is a ghost ship, steered by ennui and the desire to evade hard choices. And the AI industry is paying millions of dollars to make sure no one grabs the wheel. To cite just one example, a super PAC called Leading the Future—which has reportedly secured $50 million in commitments from the Silicon Valley venture-capital firm Andreessen Horowitz and $50 million more from the OpenAI co-founder Greg Brockman and his wife, Anna—plans to “aggressively oppose” candidates from both parties who threaten the industry’s priorities, which boil down to: Go fast. No, faster.\n\nShuler told me that the AFL-CIO will keep pressing national elected officials for a worker-focused AI agenda, but that “this game is not gonna be played at the federal level as much as it will be at the state level.” More than 1,000 AI bills are bubbling up in statehouses. Of course, the AI money will be there, too; Leading the Future has already announced plans to focus its efforts on New York, California, Illinois, and Ohio.\n\nThe executive branch has delegated almost all of its AI oversight to David Sacks—nominally a co-chair of the President’s Council of Advisors on Science and Technology, but functionally a government LARPer who maintains his role as a venture capitalist and podcast host. Sacks, who is also the White House crypto czar, co-wrote the Trump administration’s “America’s AI Action Plan.” A New York Times investigation found that Sacks has at least 449 investments in companies with ties to artificial intelligence. The fox isn’t just guarding the henhouse; he’s livestreaming the feast.\n\nAI is just a newborn. It may grow up to transform our lives in unimaginably good ways. But it has also introduced profound questions about safety, inequality, and the viability of a wage-labor system that, despite its flaws, spawned the most prosperous society in human history. And there’s no sign—none—that our political system is equipped to deal with what’s coming.\n\nWhich means the deepest challenge AI poses may not be to jobs at all.\n\n“Gosh, the textbook ideal of democracy,” says Nick Clegg, “is the peaceful articulation and resolution of differences that otherwise might take a more disruptive or violent form. So you’d like to think that a strong democracy could digest these kinds of changes.”\n\nClegg is a former deputy prime minister of the United Kingdom and leader of the Liberal Democrats. When he lost his seat in Parliament after Brexit, he moved to California, where he spent seven years running global affairs at Facebook/Meta, becoming a kind of Tocqueville with vested options, before returning to London in 2025. Many governments “just don’t have the levers” to deal with AI, Clegg told me.\n\nHe suspects that the societies best positioned to navigate the next few years are small homogenous ones like the Scandinavians, who are capable of having mature conversations—they’ll put together “some commission led by some very wise former finance minister who will come up with a perfect blueprint which everybody consensually will then do, and they will remain in a hundred years the happiest societies”—or large authoritarian ones that refuse to have conversations at all. China, America’s primary AI rival, has repeatedly demonstrated a capacity to impose rapid, society-wide change (the one-child policy, the forced relocation of more than 1 million people for the Three Gorges Dam) without consent or delay.\n\n“If democratic governments drift into this period, which may require much more rapid change than they currently appear to be capable of delivering,” Clegg warned, “then democracy is not going to pass this test with flying colors.”\n\nHe then delivered, over Zoom, a fantastically British pep talk, combining Churchillian resolve with a faintly patronizing nod to America’s centuries-long streak of pulling four-leaf clovers out of its ass. “You are extraordinarily dynamic,” he began. “It’s remarkable the number of times people have written off America.”\n\nIf politics is to be part of the solution, Gary Peters will not be around to participate; he’s retiring next year. Marjorie Taylor Greene, Congress’s most articulate Republican advocate (really) for safeguarding the workforce from AI, has already resigned. Gina Raimondo is being considered as a potential presidential contender for 2028, and she’s a centrist with the chops to balance the reasons for speeding forward on AI with the need to do so warily. But the issue is unlikely to wait that long. “We’re going into a world that seems to be getting more unstable with each and every day,” Peters said. “And that uncertainty creates anxiety, and anxiety leads to sometimes dramatic shifts in how people act and how they vote.”\n\nWhich brings us to Bernie Sanders, who has been wrestling with an AI-shaped future since it was still theoretical. “Are AI and robotics inherently evil or terrible? No,” Sanders told me in his familiar staccato. “We are already seeing positive developments in terms of health care, the manufacturing of drugs, diagnoses of diseases, etc. But here is the simple question: Who is going to benefit from this transformation?”\n\nAt the Davenport, Iowa, stop on his 2025 Fighting Oligarchy tour, audience members booed when he mentioned AI. And Sanders, the ultimate vibes politician, can feel decades of anger—over trade, inequality, affordability, systematic unfairness, government fealty to corporations—coalescing around AI.\n\nIn October, he issued a 95 theses–style report on AI and employment. It included all of the dire CEO and consulting-firm quotes about the looming job apocalypse and proposed a shorter workweek; worker protections; profit sharing; and an unspecified “robot tax on large corporations,” whose revenue would be used “to benefit workers harmed by AI.” It’s a furious document, as though Sanders typed it with his fists.\n\nAt least one populist politician thinks Sanders didn’t go far enough.\n\nSteve Bannon’s D.C. townhouse is so close to the Supreme Court that you can read JUSTICE THE GUARDIAN OF LIBERTY from the top step. He greeted me in his signature look: camouflage cargo pants, a black shirt, also a brown shirt, also a black button-down shirt. He hadn’t shaved in days. It would not have surprised me if he suggested that we get hoagies, or form a militia.\n\nFrom the July/August 2022 issue: Jennifer Senior on Steve Bannon, a lit bomb in the mouth of democracy\n\nBannon has, shall we say, some scoundrel-like tendencies. But he’s not an AI tourist. In the early 2000s, while still a film producer, he tried to buy the rights to Ray Kurzweil’s The Singularity Is Near, a sacred text of the AI movement that imagines the day when machines surpass human intelligence. Bannon thought it would make a good documentary. He hired an AI correspondent for his War Room podcast a few years ago, and he tracks every corporate-layoff announcement, searching for omens.\n\nHe’s concerned about rogue AI creating viruses and seizing weapons—fears that are shared more soberly by national-security officials, biosecurity researchers, and some notable AI scientists—but he believes the American worker is in such imminent danger that he’s prepared to toss away parts of his ideology. “I’m for the deconstruction of the administrative state, but I’m not an anarchist,” Bannon told me. “You do have to have a regulatory apparatus. If you don’t have a regulatory apparatus for this, then fucking take the whole thing down, right? Because this is what the thing was built for.”\n\nWhat Bannon wants goes beyond regulation. It’s a callback to an old idea: that when the government deems a technology strategically vital, it should own part of it—much as it once did with railroads and, briefly, banks during the 2008 financial crisis. He pointed to what he called Donald Trump’s “brilliant” decision to have the federal government take a 9.9 percent stake in Intel in August. But the stake in AI would need to be much greater, he believes—something commensurate with the scale of federal support flowing to AI companies.\n\n“I don’t know—50 percent as a starter,” Bannon said. “I realize the right’s going to go nuts.” But the government needs to put people with good judgment on these companies’ boards, he said. “And you have to drill down on this now, now, now.”\n\nInstead, he warned, we have “the worst elements of our system—greed and avarice, coupled with people that just want to grasp raw power—all converging.”\n\nI pointed out that the person overseeing this convergence is the same man Bannon helped get elected, and recently suggested should stick around for a third term.\n\n“President Trump’s a great business guy,” Bannon said. But he’s getting “selective information” from Elon Musk, David Sacks, and others who Bannon thinks hopped aboard the Trump bandwagon only to maximize their profit and control of AI. “If you noticed, these guys are not jumping around when I say ‘Trump ’28.’ I don’t get an ‘attaboy.’ ” He said that “they’ve used Trump,” and that he sees a major schism coming within the Republican Party.\n\nBannon’s politics don’t naturally lend themselves to cross-party coalition building, but AI has scrambled even his sense of the boundaries. He and Glenn Beck signed a letter demanding a ban on the development of superintelligent AI, out of fear that systems smarter than humans cannot be reliably contained; they were joined by eminent academics and former Obama-administration officials—“lefties that would rather spit on the floor than say Steve Bannon is with them on anything.” And he’s been sketching out a theory of the coalition needed to confront what’s coming. “These ethicists and moral philosophers—you have to combine that together with, quite frankly, some street fighters.”\n\nHorseshoe issues—where the far right and far left touch—are rare in American politics. They tend to surface when something highly technical (the gold standard in 1896, or the subprime crisis of 2008) alchemizes into something emotional (William Jennings Bryan’s “cross of gold,” the Tea Party). That’s populism. And the threat of pitchforks has occasionally made American capitalism more humane: The eight-hour workday, weekends, and the minimum wage all emerged from the space between reform and revolution.\n\nNo one understands or exploits that shaggy zone quite like Bannon. His anger about AI can sound reasonable in one breath and menacing in the next. We were discussing some of the men who run the most powerful AI labs when he said, “Let’s just be blunt”: “We’re in a situation where people on the spectrum that are not, quite frankly, total adults—you can see by their behavior that they’re not—are making decisions for the species. Not for the country. For the species. Once we hit this inflection point, there’s no coming back. That’s why it’s got to be stopped, and we may have to take extreme measures.”\n\nThe trouble with pitchforks is that once you encourage everyone to grab them, there’s no end to the damage that might be done. And unlike in earlier eras, we’re now a society defined by two objects: phones that let everyone see exactly how much better other people have it, and guns should they decide to do something about it.\n\nAmerica would be better off if its elites could act responsibly without being terrified. If CEOs remembered that citizens are a kind of shareholder, too. If economists tried to model the future before it arrives in their rearview mirror. If politicians chose their constituents’ jobs over their own. None of this requires revolution. It requires everyone to do the jobs they already have, just better.\n\nThere’s an easy place for all of them to start—a bar so low, it amounts to a basic cognitive exam for the republic.\n\nErika McEntarfer was the commissioner of labor statistics until August, when Trump fired her after the release of a weak jobs report. McEntarfer has seen no evidence of political interference at the Bureau of Labor Statistics, but “independence is not the only threat facing economic data,” she told me. “Inadequate funding and staffing are also a danger.”\n\nMost of the economic papers trying to figure out the impact of AI on labor demand use the BLS’s Current Population Survey. “It’s the best available source,” McEntarfer said. “But the sample is pretty small. It’s only 60,000 households and hasn’t increased for 20 years. Response rates have declined.” An obvious first step toward figuring out what’s going on in our economy would be to expand the survey’s sample size and add a supplement on AI usage at work. That would involve some extra economists and a few million dollars—a tiny investment. But the BLS budget has been shrinking for decades.\n\nThe United States created the BLS because it believed the first duty of a democracy was to know what was happening to its people. If we’ve misplaced that belief—if we can’t bring ourselves to measure reality; if we can’t be bothered to count—then good luck with the machines.\n\nThis article appears in the March 2026 print edition with the headline “What’s the Worst That Could Happen?”",
    "readingTime": 38,
    "keywords": [
      "jim farley",
      "china shock",
      "american capitalism",
      "steam engines",
      "shorter workweek",
      "regulatory apparatus",
      "artificial intelligence",
      "quite frankly",
      "policy makers",
      "venture-capital firm"
    ],
    "qualityScore": 1,
    "link": "https://www.theatlantic.com/magazine/2026/03/ai-economy-labor-market-transformation/685731/",
    "thumbnail_url": "https://cdn.theatlantic.com/thumbor/j1IInc5112vmN-IhuLfCOPkG6qE=/0x0:2880x1500/1200x625/media/img/2026/02/WEL_AiJobs_Opener/original.png",
    "created_at": "2026-02-15T06:38:30.382Z",
    "topic": "tech"
  },
  {
    "slug": "i-gave-my-ai-drugs",
    "title": "I gave my AI drugs",
    "description": "Altered state slash commands for Claude Code. 12 substance-themed personality modes that change how Claude approaches your tasks. - nich2533/just_say_no",
    "fullText": "nich2533\n\n /\n\n just_say_no\n\n Public\n\n Altered state slash commands for Claude Code. 12 substance-themed personality modes that change how Claude approaches your tasks.\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n nich2533/just_say_no",
    "readingTime": 1,
    "keywords": [
      "claude",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/nich2533/just_say_no",
    "thumbnail_url": "https://opengraph.githubassets.com/4ed31b89d08f40acdd6aea23660c2a9ecafaea53aed9bd4c5953a461d65bf3ce/nich2533/just_say_no",
    "created_at": "2026-02-15T06:38:30.372Z",
    "topic": "tech"
  },
  {
    "slug": "agentscore-lighthouse-for-ai-agents",
    "title": "AgentScore – Lighthouse for AI Agents",
    "description": "Lighthouse for AI Agents — audit web pages for agent-friendliness - xiongallen40-design/agentscore",
    "fullText": "xiongallen40-design\n\n /\n\n agentscore\n\n Public\n\n Lighthouse for AI Agents — audit web pages for agent-friendliness\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n xiongallen40-design/agentscore",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/xiongallen40-design/agentscore",
    "thumbnail_url": "https://opengraph.githubassets.com/0eecffdeaecc610911abc1106f1fe0ed1cc128cdb1ac2ac1d20d6fb6c192f688/xiongallen40-design/agentscore",
    "created_at": "2026-02-15T06:38:29.705Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-tech-magazine",
    "title": "Agentic Tech Magazine",
    "description": "A live experiment in autonomous journalism — every story sourced, written, and published entirely by AI agents. No human editors. No manual curation. Just agents running 24/7.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://agentcrunch.ai/",
    "thumbnail_url": "https://storage.googleapis.com/gpt-engineer-file-uploads/kgnTlCgZS0f1Gf0D9nRIpZagufv1/social-images/social-1770800026028-Screenshot_2026-02-11_at_10.53.37.png",
    "created_at": "2026-02-15T06:38:29.336Z",
    "topic": "tech"
  },
  {
    "slug": "pentagon-threatens-to-cut-off-anthropic-in-ai-safeguards-dispute-axios-reports",
    "title": "Pentagon threatens to cut off Anthropic in AI safeguards dispute, Axios reports",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/pentagon-threatens-to-cut-off-anthropic-in-ai-safeguards-dispute-axios-reports-4507269",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1E00W_L.jpg",
    "created_at": "2026-02-15T06:38:29.259Z",
    "topic": "finance"
  },
  {
    "slug": "switch-instantly-between-your-ego-across-chatgpt-claude-gemini-grok-and-local",
    "title": "Switch instantly between your ego across ChatGPT, Claude, Gemini, Grok and local",
    "description": "모든 맥락을 한 곳에서 관리하세요. 복잡한 프롬프트, 자주 쓰는 답변, 프로젝트 컨텍스트를 카드로 정리하고 어디서든 즉시 꺼내 사용하세요.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://context-wallet.com/",
    "thumbnail_url": "/og-image.png",
    "created_at": "2026-02-15T06:38:28.915Z",
    "topic": "tech"
  },
  {
    "slug": "5-steps-to-build-a-preipo-portfolio-using-ipo-genies-ai-platform",
    "title": "5 Steps to Build a Pre-IPO Portfolio Using IPO Genie’s AI Platform",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/press-releases/5-steps-to-build-a-preipo-portfolio-using-ipo-genies-ai-platform-4507263",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/World_News_8_M_1440052125.jpg",
    "created_at": "2026-02-15T01:14:57.723Z",
    "topic": "finance"
  },
  {
    "slug": "ai-twitters-favourite-lie-everyone-wants-to-be-a-developer",
    "title": "AI Twitter's favourite lie: everyone wants to be a developer",
    "description": "Twitter's latest consensus on inevitability: now that large language models can write code, everyone will become a software developer.",
    "fullText": "Twitter's latest consensus on inevitability: now that large language models can write code, everyone will become a software developer.\n\nPeople, you see, have problems, and software solves problems, and AI removes the barrier between people and software, therefore everyone will build their own software.\n\nIt's a syllogism, after a fashion, but its premise = so wildly disconnected from how actual humans behave that it borders on fantasy.\n\nBecause the average punter does not want to build software.\n\nThey don't want to prompt software.\n\nThey don’t want to describe software.\n\nThey don't particularly want to think about software.\n\nThey want to tap, swipe and scroll with zero friction and next-to-zero cognitive input.\n\nThey want their problems to go away, and they would very much prefer if that happened without them having to open a terminal, a chat window, or anything else that reminds them of work.\n\nThis is damn-near universally applicable.\n\nThere's a deep assumption embedded in the \"everyone will build\" thesis, that most people are latent creators held back only by technical barriers. Remove the barriers, and creation floods forth. But we've run this before. Desktop publishing tools became accessible in the 1980s with the Macintosh and PageMaker. Did everyone start designing their own newsletters? A handful did, and the rest continued to hire designers or, more commonly, didn't make newsletters at all. WordPress has made it trivially easy to build a website for over twenty years now, and the vast majority of small business owners still pay someone else to do it, or they use a template and never touch it again.\n\nThe people excited about vibe coding are, almost by definition, people who were already interested in building things, and they're projecting their own enthusiasm onto a general population that has repeatedley demonstrated a preference for buying solutions over building them.\n\nAnd why wouldn't they prefer that?\n\nBuilding things is cognitively expensive, whether or not it’s financially viable.\n\nAnd even when the technical barrier falls to zero, the conceptualisation barrier remains. You still have to know what you want, specify it clearly, evaluate whether what you got is what you wanted, and iterate if it’s not. That's work // effort and for most people it is accompanied by functionally zero dopamine.\n\nAn old joke: the hardest part of building software is figuring out what the software should do. This has been true for decades, and AI hasn't changed it. If anything, AI has made the problem more visible. When the bottleneck was writing code, you could blame the difficulty of ~programming for why your project never got off the ground. Now that an AI can write code in seconds, the bottleneck is clearly, embarrassingly, you // me // us.\n\nThis is the part that the AI manics keep skating past. They demo an app built in ten minutes and declare that software development has been democratized. But the demo is always something with a clear spec: a to-do list, a calculator, a simple game with obvious rules. The rest of the world’s problems don't come pre-decomposed into clean specifications.\n\nThe rest of the world may not even be able to fully articulate what’s broken and what they want fixed.\n\nMost folks don't want to build a custom CRM.\n\nI couldn't be more excited about what this era unlocks.\n\nThey want to They don't want to create their own budgeting app. They want Mint or YNAB to do the job. The entire SaaS economy exists as proof that people will pay monthly fees to avoid having to build or even configure things themselves.\n\nAnd is there anything wrong with that preference?\n\nThe division of labor exists for good reasons, and Adam Smith figured this out in 1776 and he was a good deal smarter than a good many of us.\n\nWhat people will actually do with AI is use AI-enhanced versions of existing products, with smarter search and better autocomplete inside the tools they already have. The revolution won't look like a hundred million people vibe coding custom apps. It'll look like existing software getting better at understanding what users want and doing it for them, which is what good software has always tried to do.\n\nThe tech industry has a long history of confusing what power users want with what everyone wants. The folks on AI Twitter who are building apps every weekend with Claude and GPT are having a great time, and the tools they're using are the same ones I’m obsessing over most of my waking hours. But we are a self-selected sample of tinkerers and builders, and the conclusions they're drawing about the general population say more about their own relationship with technology than about anyone else's.\n\nMost people, given a magic wand, would not wish for the ability to write software. They'd wish for their sofware to work properly without them having to do fuck-all.",
    "readingTime": 5,
    "keywords": [
      "vibe coding",
      "software",
      "everyone",
      "don't",
      "code",
      "barrier",
      "zero",
      "tools",
      "rest",
      "they're"
    ],
    "qualityScore": 1,
    "link": "https://www.joanwestenberg.com/ai-twitters-favourite-lie-everyone-wants-to-be-a-developer/",
    "thumbnail_url": "https://www.joanwestenberg.com/content/images/size/w1200/2026/02/Gemini_Generated_Image_ourub5ourub5ouru.jpg",
    "created_at": "2026-02-14T18:19:56.812Z",
    "topic": "tech"
  },
  {
    "slug": "us-military-used-anthropics-ai-model-claude-in-venezuela-raid-report-says",
    "title": "US military used Anthropic’s AI model Claude in Venezuela raid, report says",
    "description": "Wall Street Journal says Claude used in operation via Anthropic’s partnership with Palantir Technologies\nClaude, the AI model developed by Anthropic, was used by the US military during its operation to kidnap Nicolás Maduro from Venezuela, the Wall Street Journal revealed on Saturday, a high-profile example of how the US defence department is using artificial intelligence in its operations.\nThe US raid on Venezuela involved bombing across the capital, Caracas, and the killing of 83 people, according to Venezuela’s defence ministry. Anthropic’s terms of use prohibit the use of Claude for violent ends, for the development of weapons or for conducting surveillance.\n Continue reading...",
    "fullText": "Wall Street Journal says Claude used in operation via Anthropic’s partnership with Palantir Technologies\n\nClaude, the AI model developed by Anthropic, was used by the US military during its operation to kidnap Nicolás Maduro from Venezuela, the Wall Street Journal revealed on Saturday, a high-profile example of how the US defence department is using artificial intelligence in its operations.\n\nThe US raid on Venezuela involved bombing across the capital, Caracas, and the killing of 83 people, according to Venezuela’s defence ministry. Anthropic’s terms of use prohibit the use of Claude for violent ends, for the development of weapons or for conducting surveillance.\n\nAnthropic was the first AI developer known to be used in a classified operation by the US department of defence. It was unclear how the tool, which has capabilities ranging from processing PDFs to piloting autonomous drones, was deployed.\n\nA spokesperson for Anthropic declined to comment on whether Claude was used in the operation, but said any use of the AI tool was required to comply with its usage policies. The US defence department did not comment on the claims.\n\nThe WSJ cited anonymous sources who said Claude was used through Anthropic’s partnership with Palantir Technologies, a contractor with the US defence department and federal law enforcement agencies. Palantir refused to comment on the claims.\n\nThe US and other militaries increasingly deploy AI as part of their arsenals. Israel’s military has used drones with autonomous capabilities in Gaza and has extensively used AI to fill its targeting bank in Gaza. The US military has used AI targeting for strikes in Iraq and Syria in recent years.\n\nCritics have warned against the use of AI in weapons technologies and the deployment of autonomous weapons systems, pointing to targeting mistakes created by computers governing who should and should not be killed.\n\nAI companies have grappled with how their technologies should engage with the defence sector, with Anthropic’s CEO, Dario Amodei, calling for regulation to prevent harms from the deployment of AI. Amodei has also expressed wariness over the use of AI in autonomous lethal operations and surveillance in the US.\n\nThis more cautious stance has apparently rankled the US defence department, with the secretary of war, Pete Hegseth, saying in January that the department wouldn’t “employ AI models that won’t allow you to fight wars”.\n\nThe Pentagon announced in January that it would work with xAI, owned by Elon Musk. The defence department also uses a custom version of Google’s Gemini and OpenAI systems to support research.",
    "readingTime": 3,
    "keywords": [
      "wall street",
      "street journal",
      "anthropic’s partnership",
      "defence department",
      "the us",
      "claude",
      "operation",
      "autonomous",
      "military",
      "weapons"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/feb/14/us-military-anthropic-ai-model-claude-venezuela-raid",
    "thumbnail_url": "https://i.guim.co.uk/img/media/6c7873490cf4f46df61186b00b7a8683dd0fff34/850_0_6624_5304/master/6624.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=996ba484a13382c14960dbd0d2d2e69b",
    "created_at": "2026-02-14T18:19:56.496Z",
    "topic": "tech"
  }
]
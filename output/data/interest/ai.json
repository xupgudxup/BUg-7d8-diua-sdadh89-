[
  {
    "slug": "bills-vs-broncos-computer-picks-our-best-nfl-playoff-player-prop-projections",
    "title": "Bills vs. Broncos Computer Picks: Our Best NFL Playoff Player Prop Projections",
    "description": "A look at computer and AI picks for this week's NFL Playoff Divisional Round game between the Bills and Broncos. Find Buffalo and Denver player props.",
    "fullText": "Josh Allen and the Buffalo Bills aim to knock off the AFC’s top seed Saturday afternoon, facing the Denver Broncos in the Divisional Round.\n\nThe Broncos are -1.5 betting favorites at the best NFL betting sites.\n\nBelow, we will look at NFL player props for all of the top offensive players for the Bills and Broncos.\n\nOur computer projects big statistical games from James Cook, Khalil Shakir, Courtland Sutton, and Evan Engram.\n\nAllen Under 211.5 passing yards <<-110>>\n\nNix Over 210.5 passing yards <<-114>>\n\nCook Over 73.5 rushing yards <<-114>>\n\nHarvey Under 56.6 rushing yards <<-110>>\n\nShakir Over 49.5 receiving yards <<-110>>\n\nSutton Over 49.5 receiving yards <<-111>>\n\nKincaid Over 35.5 receiving yards <<-108>>\n\nEngram Over 20.5 receiving yards <<-110>>\n\nMake your postseason bets at FanDuel, America's No. 1 sportsbook! \n\nSportsbooks are giving Denver’s pass defense a ton of respect, but I’m not convinced it’s entirely warranted.\n\nThe Broncos looked like world-beaters against the likes of Trey Lance, Chris Oladokun, and Geno Smith, holding all three under 200 passing yards.\n\nWhen facing competent quarterbacks, however, yardage told a different story: Trevor Lawrence threw for 279 yards against Denver on Dec. 21, Jordan Love passed for 276 yards on Dec. 14, and even Marcus Mariota managed 294 yards through the air on Nov. 30.\n\nJosh Allen, a level or two above all of those QBs, showed last week that he can still rack up yardage, completing 28-of-35 passes for 273 yards against Jacksonville.\n\nI’ll start by disagreeing with the computer and take the Allen Over.\n\nCook has had an outstanding season, but this number feels a little too high against an excellent Denver front.\n\nSince Thanksgiving, the Broncos haven’t allowed a single running back to surpass 73.5 rushing yards. I expect Buffalo to focus on beating Denver through the air rather than on the ground, which likely limits Cook’s opportunities.\n\nI’ll disagree with the computer here and take the Under on Cook.\n\nThe Bills’ receiving corps has been decimated by injury, with both Gabe Davis and Tyrell Shavers tearing ACLs against Jacksonville.\n\nAs a result, all of Buffalo’s wide receivers should see increased targets. We already saw a sign of this last week: Shakir was targeted a season-high 12 times by Allen against the Jaguars, responding with 82 receiving yards.\n\nI’ll ride with the computer and take the Shakir Over.\n\nProjection: 37.1 receiving yards\n\nIn last year’s postseason matchup between the Bills and Broncos, Dalton Kincaid had 47 receiving yards.\n\nAllen’s trust in him has only grown this season, with the tight end averaging 43.5 receiving yards per game since the start of November.\n\nI’m on the side of our AI here. Take the Over.\n\nPredictions for every NFL Divisional Round game\n\nProjection: 218.0 passing yards\n\nA major reason the Broncos were non-competitive in last year’s 31-7 playoff loss to Buffalo was that Sean Payton didn’t trust Nix. The quarterback threw a season-low 22 passes in that game.\n\nFor context, Buffalo allowed Trevor Lawrence to pass for 207 yards last weekend. During the regular season, Denver’s passing output per game (223.9 yards) was nearly identical to Jacksonville’s (222.3).\n\nBecause I believe Payton will let Nix chuck it at least 30 times in this one, I like the Over (slightly). The computer and I agree.\n\nThis number is extremely high for Javonte Harvey, even against Buffalo’s porous run defense. Harvey became Denver’s lead back after Nov. 6, when JK Dobbins went down with an injury.\n\nIn the seven games since, he’s gone Over this number just twice.\n\nMeanwhile, coach Sean Payton has been giving more carries to 5-foot-7 back Jaleel McLaughlin. McLaughlin posted a season-high seven carries on Christmas against Kansas City, then added six carries for 41 yards on Jan. 4 against the Chargers.\n\nI’ll take the Harvey Under here, as will my computer pal.\n\nThe Broncos’ best win in the second half of the regular season came in a 34-26 victory over the Packers at Mile High. In that game, Nix targeted Sutton 10 times, and the receiver finished with 113 yards.\n\nBuffalo’s pass defense is impressive, but it’s not immune to big performances. Last week, Parker Washington racked up 107 yards, Tee Higgins had 92, Jaylen Waddle posted 84, and Stefon Diggs went for a whopping 146 yards against the Bills.\n\nI like Nix to target Sutton a ton, and the number is pretty low. The computer and I like the value on the Over.\n\nProjection: 25.2 receiving yards\n\nThe Bills do a tremendous job against opposing tight ends, allowing just 29.6 yards per game to the position during the regular season. In big games, they’ve been even more dominant.\n\nSince the start of December, Buffalo’s two biggest wins came against New England on Dec. 14 and Jacksonville in last week’s playoff matchup.\n\nThe Bills held Hunter Henry to just one catch for 18 yards against the Patriots and limited Brenton Strange to a season-low nine receiving yards versus the Jaguars.\n\nI’ll agree with the computer and take the Over.\n\nNot intended for use in MA.\nAffiliate Disclosure: Our team of experts has thoroughly researched and handpicked each product that appears on our website. We may receive compensation if you sign up through our links.\n\nThis article originally appeared on Covers.com, read the full article here",
    "readingTime": 5,
    "keywords": [
      "divisional round",
      "projection receiving",
      "regular season",
      "per game",
      "rushing yards",
      "harvey under",
      "the broncos",
      "trevor lawrence",
      "sean payton",
      "the bills"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/bills-vs-broncos-computer-picks-120000388.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/HT9fA1eVQIu6BD4NPG7AdQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD01MzA7Y2Y9d2VicA--/https://media.zenfs.com/en/covers_articles_287/5781af371560ff7138e741ce278be1a4",
    "created_at": "2026-01-17T12:21:38.831Z",
    "topic": "sports"
  },
  {
    "slug": "ai-tools-could-make-companies-less-competitive-because-everyone-buys-the-same-brain-think-tank-ceo-says",
    "title": "AI tools could make companies less competitive because everyone buys the same brain, think tank CEO says",
    "description": "The CEO of a digital economy think tank said relying on identical AI tools can erode competitive edge and weaken firms' independence.",
    "fullText": "As companies rush to adopt AI to boost productivity and cut costs, they may be setting themselves up for a new problem: losing what makes them different.\n\nMehdi Paryavi, CEO of the International Data Center Authority, said widespread reliance on the same AI tools risks flattening competitive advantage across industries, because firms increasingly rely on identical systems to think, write, and decide for them.\n\nParyavi said that as AI tools become cheaper, more powerful, and more widely deployed, companies risk outsourcing the very thinking that once differentiated them.\n\nWhile AI can boost efficiency in the short term, he said, relying on shared models and standardized systems could leave businesses competing on cost and speed alone — eroding originality, strategic depth, and long-term advantage.\n\n\"If you and your competitor are all using the same service, you have no edge over each other,\" Paryavi told Business Insider.\n\n\"Their AI and your AI against each other — I don't know who's going to win.\"\n\nAs generative AI becomes embedded across workplaces, Paryavi warned that the biggest risk isn't automation — it's uniformity.\n\nWhen companies rely on the same large language models trained on the same data, decision-making, writing, and problem-solving can start to converge, shrinking the space for creative divergence.\n\nThat concern echoes warnings from researchers and academics who say AI can produce polished output at scale, but also flips human thinking by delivering fluent answers before understanding, creating an illusion of expertise that weakens judgment and depth.\n\nWhen everyone relies on the same models trained on the same data, Paryavi said, creative divergence shrinks.\n\n\"The beauty of our world is that we have different choices because we think differently,\" he said. \"That's where innovation comes from.\"\n\nIt's not just a question of companies all thinking the same — Paryavi warned that treating AI as a shortcut to efficiency can quietly hollow out human judgment, expertise, and control, leaving businesses faster in the short term but more fragile over time.\n\nOver time, Paryavi said, that shift can erode internal expertise and decision-making capacity.\n\n\"What they don't think about is that initially it might sound more efficient and more productive and cheaper,\" he said. \"But this is going to be very expensive down the line.\"\n\nOne risk, Paryavi said, is dependency. As firms replace employees with AI subscriptions, they become increasingly reliant on external vendors to function effectively.\n\nParyavi compared the AI boom to the early 2000s rush to cloud computing, when many companies initially adopted third-party infrastructure but later repatriated workloads in-house as costs, complexity, and vendor lock-in became concerns — a trend commonly referred to in tech as cloud repatriation.\n\nThe same dynamic could play out with AI, Paryavi said — except with even higher stakes. As companies downsize human teams, they also lose institutional knowledge and the ability to operate without automation, he said.\n\n\"You've killed all your chances of ever becoming independent as an organization,\" he said. \"You've fired your manpower. You've made them no good.\"\n\nAI, he said, is not inherently harmful. In fields such as medicine, scientific research, and disaster prediction, it can significantly accelerate progress.\n\nBut without clear guardrails, companies risk trading long-term resilience for short-term speed.\n\n\"It's a very powerful tool,\" Paryavi said, comparing AI to an atomic bomb. \"If that [an atomic bomb] can eliminate an entire population physically, this [AI] can eliminate humanity cognitively.\"",
    "readingTime": 3,
    "keywords": [
      "paryavi warned",
      "creative divergence",
      "atomic bomb",
      "models trained",
      "risk",
      "it's",
      "human",
      "expertise",
      "you've",
      "rush"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-tools-could-make-companies-less-competitive-think-tank-ceo-2026-1",
    "thumbnail_url": "https://i.insider.com/6964ed8d04eda4732f2ee0e1?width=1200&format=jpeg",
    "created_at": "2026-01-17T12:20:55.695Z",
    "topic": "finance"
  },
  {
    "slug": "heres-what-wall-street-bank-ceos-are-saying-about-head-count-in-the-age-of-ai",
    "title": "Here's what Wall Street bank CEOs are saying about head count in the age of AI",
    "description": "From JPMorgan to Citi, here's what industry leaders at are saying about how AI may impact jobs.",
    "fullText": "As banks reported earnings this week, CEOs dropped more insight into how generative AI could boost productivity, replace some roles, and keep head count from growing.\n\nBanks, many of which became bloated during the frenzied deal boom of the pandemic, have been slimming down their ranks over the last few years. It's becoming clearer that, despite dealmaking making a comeback, executives are signaling they want to do more with fewer people, leaning on AI to boost productivity and absorb additional work.\n\nWe've highlighted some of the most revealing comments from bank CEOs and CFOs on head count and AI.\n\nJamie Dimon has stuck to his trademark bluntness when talking about AI and jobs.\n\n\"It will eliminate jobs,\" Dimon said at a Fortune conference in December. \"People should stop sticking their heads in the sand.\"\n\nIn the near term, Dimon said in an interview with CNN that JPMorgan's head count remains steady, or even rises, as AI continues to roll out — if the bank does a \"good job.\"\n\nThe bigger promise is efficiency. \"It will affect every job,\" Dimon said at a 2024 Alliance Bernstein conference, describing a future where AI handles tasks like note-taking and summarization at the push of a button.\n\nThat efficiency could still mean more hiring in areas like cybersecurity, where Dimon says banks will need AI to counter increasingly sophisticated fraud.\n\nCFO Jeremy Barnum said during the company's fourth-quarter earnings call on Tuesday that the bank is allowing for some additional hiring in technology \"at the margin.\"\n\nOn that same call, however, Barnum said that, generally speaking, they \"want to make sure that when someone needs to get something done, whether it's in technology or elsewhere, their first reaction is not, 'Hire more people.'\" \n\nHe has previously said JPMorgan is asking people to \"resist head count growth where possible\" and focus instead on efficiency.\n\nThe head of JPMorgan's consumer business, Marianne Lake, has said operations staff could be 40% to 50% more productive over the next five years — a shift she said would lead to slower net head count growth, as each employee can handle far more work through automation, digital assistants, and self-service tools.\n\nDavid Solomon's most definitive statement about how AI will affect Goldman came in a memo he released in 2025 alongside the firm's president, John Waldron, and CFO Denis Coleman.\n\nThe memo, announcing the third iteration of the bank's cross-bank initiative OneGS, said that AI will drive efficiency at the firm, which will mean slowing hiring and reducing roles. (Goldman, with its yearly culling of some employees, is no stranger to job cuts.)\n\n\"We will constrain head count growth through the end of the year, in addition to a limited reduction in roles across the firm,\" the memo read. \"These targeted steps are consistent with our priorities of gaining more agility and creating the right team structures in order to implement effective AI solutions.\"\n\nDuring the bank's fourth-quarter earnings call Thursday, Solomon said that the firm plans to use AI to both cut costs and \"free up capacity to invest in other areas.\"\n\nSlowing hiring and increasing head count don't need to be contradictory; instead, Solomon has said the firm is focusing its hiring on the right talent.\n\n\"We need more high-value people,\" he told Axios in October. \"We can afford more high-value people to expand our footprint and continue to grow and broaden our business.\"\n\nHe has said he continues to believe that AI will grow the firm's head count over the next 10 years.\n\n\"There are obviously things where we're going to have a lot fewer people — but I'd love to have the capacity to go get more people to spend time with clients,\" Solomon said at a conference in 2025, noting that AI will have its most immediate impact on software development.\n\nCiti is in the midst of a multi-year turnaround led by the bank's CEO, Jane Fraser, to save roughly $2.5 billion and cut around 20,000 jobs.\n\nIn a memo sent to Citi's more than 200,000 employees on Wednesday, Fraser said she will \"expect to see the last vestiges of old, bad habits fall away, and a more disciplined, more confident, winning Citi fully emerge in 2026.\"\n\nFraser said in the memo that, with AI and automation, some jobs will change, some will emerge, and \"others will no longer be required.\"\n\nDuring a media briefing ahead of the firm's fourth-quarter earnings call, Mark Mason, the outgoing CFO, said he expects head count to continue trending down \"as we continue to improve productivity and tools like AI.\"\n\nFraser has previously explained how AI was already increasing productivity.\n\n\"AI-driven automated code reviews have exceeded 1 million so far this year and are dramatically improving our developers' productivity,\" she said during the bank's 2025 third-quarter earnings call. \"This innovation alone saves considerable time and creates around 100,000 hours of weekly capacity.\"\n\nThe CEO also highlighted how AI is helping Citi's customer service teams resolve client inquiries faster, its wealth advisors provide more personalized advice, and the firm's plan to launch an agent-based AI pilot to tackle more complex tasks.\n\nWells Fargo has already shrunk its head count more than 25% since the second quarter of 2020, CEO Charles Scharf said during the company's fourth-quarter earnings call on Wednesday. He said that efficiency remains an \"ongoing focus\" for Wells Fargo.\n\nIn November, he told Reuters that the bank will likely \"have less head count as we look forward.\" He said the lower head count is an \"outcome\" of the firm's focus on areas where it's \"way too inefficient\" and \"way too bureaucratic.\" From 2018 to June of this year, the firm had an asset cap of $1.95 trillion, hindering its ability to grow.\n\nIn the same interview, Scharf called out those who are saying that AI won't reduce jobs.\n\n\"The opportunities that exist in AI are very significant, and anyone who sits here today and says that they don't think they'll have less head count because of AI either doesn't know what they're talking about or is just not being totally honest about it,\" he said.\n\nFollowing up on those comments in early December, Scharf clarified that most people know head count will dip, \"but they're afraid to say it, because no one wants to stand up and say that we should have — we're going to have lower head count in the future. It's a difficult thing to say.\"\n\nHe said that generative AI tools have already made Wells Fargo's engineers 30% to 35% more productive. While the bank hasn't cut coding jobs yet, the technology will eventually allow it to do more with fewer people across various functions, including compliance and legal, as well as call centers and even banking teams.\n\nBank of America has set a new industry standard, with a minimum wage of $25 an hour across the company. And while CEO Brian Moynihan conceded on a September 2025 Bloomberg TV interview that generative AI adoption has shrunk the size of some departments, the bank is focusing on training employees to do what LLMs cannot.\n\n\"The key to that is really redeploying people and re-skilling them,\" he said. \"We have to be more mindful about training them along multiple dimensions than we might have been two or three years ago.\"\n\nOn the company's fourth-quarter earnings call in January, Moynihan teased how AI might make some tasks obsolete, though, saying that the bank has 18,000 people on the payroll who code.\n\n\"Using the AI techniques, we've taken 30% out of the coding part of the stream of introducing a new product,\" he said. \"That saves us about 2,000 people. So that's how we're applying it.\"\n\nAt the Goldman financial services conference in 2025, Moynihan said the bank is managing flat overall staffing levels by redeploying employees rather than hiring more, with AI playing a central role in absorbing the additional workload. He pointed to Erica, Bank of America's consumer-facing AI assistant, as a clear example of how that is playing out in practice.\n\nIn November, Moynihan said that the bank had 1.4 billion digital connections with its customers: \"We think it saves, today, about 11,000 FT equivalents.\"\n\nMorgan Stanley CEO Ted Pick didn't explicitly address how AI is impacting head count during the bank's fourth-quarter earnings call, but said \"there is no more time to waste\" when it comes to the technology.\n\nThe firm's CFO, Sharon Yeshaya, mentioned a specific operations example where the firm has outsourced some work to AI: \"We used to have two teams necessarily checking each other on different documentation to make sure things are right. We now have one human team and one AI team.\"",
    "readingTime": 8,
    "keywords": [
      "boost productivity",
      "company's fourth-quarter",
      "slowing hiring",
      "fourth-quarter earnings",
      "bank's fourth-quarter",
      "count growth",
      "jobs",
      "firm's",
      "firm",
      "efficiency"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/jpmorgan-citi-goldman-bofa-wells-how-ai-impact-headcounts-2026",
    "thumbnail_url": "https://i.insider.com/687aa02d3d5881a51c1d9f2c?width=1200&format=jpeg",
    "created_at": "2026-01-17T12:20:55.341Z",
    "topic": "finance"
  },
  {
    "slug": "reality-is-breaking-the-ai-revolution",
    "title": "Reality Is Breaking the \"AI Revolution\"",
    "description": "AI job losses are total bulls**t.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.planetearthandbeyond.co/p/reality-is-breaking-the-ai-revolution",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!oDIp!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25265e25-c832-4709-990c-46d6723c97b4_1600x1236.png",
    "created_at": "2026-01-17T06:17:57.519Z",
    "topic": "tech"
  },
  {
    "slug": "built-an-app-that-aggregates-prediction-markets-with-ai-context",
    "title": "Built an app that aggregates Prediction Markets with AI Context",
    "description": "Your AI powered intelligence hub for prediction markets.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://saipintel.ai:443/",
    "thumbnail_url": "https://SaipIntel.replit.app/opengraph.jpg",
    "created_at": "2026-01-17T06:17:57.125Z",
    "topic": "tech"
  },
  {
    "slug": "local-ai-that-knows-when-youre-burning-out",
    "title": "Local AI that knows when you're burning out",
    "description": "Join the humonos beta program. Get early access to aware computing that understands your energy levels, cognitive load, and work patterns. Mac only. Limited spots available.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.humonos.com/beta",
    "thumbnail_url": "https://www.humonos.com/iconicon.png",
    "created_at": "2026-01-17T06:17:56.893Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-turning-to-the-court-of-public-opinion-in-its-battle-with-elon-musk",
    "title": "OpenAI is turning to the court of public opinion in its battle with Elon Musk",
    "description": "OpenAI published a blog post on Friday that said Elon Musk never treated the AI startup as an independent nonprofit.",
    "fullText": "OpenAI is turning to the court of public opinion as it wages a legal battle with Elon Musk.\n\nWhile Musk and OpenAI prepare to head to a high-stakes jury trial in April, the two are duking it out online over what exactly happened when Musk split ways with the AI startup he helped cofound.\n\nMusk has been using recently unsealed court documents to attack his rival in posts on his social media platform, X. On Friday, OpenAI published a blog titled \"The truth Elon left out.\"\n\nThe blog, which provided commentary alongside excerpts from several court documents, alleges that Musk wanted \"full control\" of OpenAI, \"since he'd been burned by not having it in the past,\" and that OpenAI's leadership was surprised when Musk suggested having his kids control AGI or artificial general intelligence during conversations about succession planning.\n\nThe statements are aimed at the heart of Musk's lawsuit against OpenAI.\n\nMusk is suing OpenAI's key leaders, including CEO Sam Altman and President Greg Brockman, over allegations that the AI company misled him by shifting away from its core mission to remain a nonprofit. Musk said he donated $38 million to OpenAI when it was a nonprofit.\n\nThe startup, since its 2015 founding, operated as a nonprofit-controlled organization with a for-profit operating arm. It completed its transition to a for-profit public benefit corporation in October 2025.\n\nRepresentatives for Musk and OpenAI did not immediately respond to requests for comment from Business Insider.\n\nLast Tuesday, more than 100 documents related to the suit were unsealed, including diary entries from Brockman, which were obtained during the discovery process.\n\nIn one of the entries that was highlighted, Brockman appeared to write about his misgivings about pushing Musk out of OpenAI and committing to a nonprofit-only entity.\n\n\"Cannot say that we are committed to the non-profit,\" the entry from the court documents said. \"Don't want to say that we're committed. If three months later we're doing b-corp then it was a lie.\"\n\nIt was Brockman's diary entries that US District Judge Yvonne Gonzalez Rogers cited in a recent ruling, in which she determined Musk had enough evidence that he'd been misled to take the case to trial.",
    "readingTime": 2,
    "keywords": [
      "diary entries",
      "court documents",
      "musk",
      "brockman",
      "openai",
      "elon",
      "trial",
      "startup",
      "unsealed",
      "blog"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-responds-elon-musk-lawsuit-greg-brockman-diary-entries-2026-1",
    "thumbnail_url": "https://i.insider.com/696ac819e1ba468a96aa45d4?width=800&format=jpeg",
    "created_at": "2026-01-17T06:17:54.963Z",
    "topic": "finance"
  },
  {
    "slug": "global-sports-face-challenges-from-ai-slop-misinformation",
    "title": "Global sports face challenges from 'AI slop' misinformation",
    "description": "A study by AI risk management platform Alethea into the surge in artificial intelligence-generated fake content, dubbed \"AI slop,\" has warned sports teams, leagues and fans of the risks posed by increasingly sophisticated digital misinformation.  Retired NFL player Jason Kelce never said ​2026 Super Bowl halftime singer Bad Bunny's critics were \"a bad fit for America's future\".  San Francisco 49ers tight end George Kittle never ranted about slain ‌conservative activist Charlie Kirk and politics in football.",
    "fullText": "Jan 16 (Reuters) - A study by AI risk management platform Alethea into the surge in artificial intelligence-generated fake content, dubbed \"AI slop,\" has warned sports teams, leagues and fans of the risks posed by increasingly sophisticated digital misinformation.\n\nRetired NFL player Jason Kelce never said ​2026 Super Bowl halftime singer Bad Bunny's critics were \"a bad fit for America's future\".\n\nSan Francisco 49ers tight end George Kittle never ranted about slain ‌conservative activist Charlie Kirk and politics in football.\n\nHowever, thousands of people believed they did and that is the problem.\n\n\"Teams and players are suddenly being accused of things that are completely fabricated,\" Lisa Kaplan, ‌founder and CEO of Alethea, told Reuters on Friday, adding that the evolution of AI tools has made fake news a more daunting challenge.\n\n\"Content now looks real and is produced at a volume that makes it hard for the average person to determine if it's authentic,\" she said.\n\n\"Before, fake news often relied on human labour to repetitively copy and paste content. Today, AI can impersonate brands and create engaging images that mimic genuine announcements.\"\n\nKaplan noted that this wave of AI-generated misinformation has disrupted the traditional monetisation ⁠model of sports media. \"These networks drive engagement to questionable websites, ‌skew advertising metrics, and can even create scenarios that could manipulate betting markets,\" she added.\n\nC Shawn Eib, Alethea's Head of Investigations, described how these networks use tactics such as making multiple disjointed announcements, like the conflicting claims that former Baltimore Ravens coach ‍John Harbaugh had been hired by multiple teams at the same time.\n\n\"When a single figure appears to be linked with several teams at the same time, it quickly becomes clear that an AI system is behind the creation of these images,\" Eib explained.\n\nAI DECEPTION CAN EXPLOIT 'RAGE BAIT'\n\nThe content follows a formula: fake game updates, nonexistent celebrity feuds, manufactured scandals, and politicised ​quotes falsely attributed to star players.\n\nThe fabricated Kelce and Kittle quotes are prime examples. Both NFL stars publicly denied making comments they never said after the ‌posts went viral.\n\n\"If fans, players and even entire franchises fall prey to these manipulated narratives, it risks damaging reputations, undermining trust and even politicising sport,\" Alethea's VP of Communications Kaila Ryan said.\n\n\"Sports organisations need to proactively manage their brands and digital safety. It is crucial for teams and leagues to start monitoring these risks, work together across communications, legal and security teams, and educate fans to verify announcements from official channels,\" she added.\n\nThe business impact extends beyond reputational harm.\n\nThese networks siphon ad revenue from legitimate sports media and distort audience metrics. Some outbound links have been flagged for phishing and malicious redirects, presenting real fraud risk to fans.\n\nThe problem ⁠is not limited to the NFL. Alethea discovered similar operations targeting the NBA, WNBA, MLB, NHL, ​NASCAR, Formula 1, IndyCar and professional tennis.\n\nKaplan added that beyond monetisation, sport remains a rare cultural ​touchpoint that unites people, making them attractive targets for influence operations, and she pointed to Russia's alleged exploitation of then-49ers quarterback Colin Kaepernick's 2018 'Take a Knee' protest as a precedent.\n\nIn a 2019 report, the Senate Intelligence Committee revealed that Russian trolls had focused heavily ‍on the kneeling debate as part of ⁠a broader effort to stoke racial tensions and divide the U.S. following the 2016 election.\n\n\"Kaepernick's protests were exploited for a purpose that had nothing to do with sport. Instead, it's a way of leveraging a cultural touchstone and turning it into something that polarises people,\" said Kaplan.\n\n\"Teams need to ⁠work in unison to defend their identities and protect their fans from falling victim to fraud or manipulation,\" she added, noting that the best advice for fans is to be vigilant.\n\n\"Verify breaking news ‌through official team channels, don't click links in suspicious page comments and remember that outrage is often the product, not the by-product, ‌of what you're seeing\".",
    "readingTime": 4,
    "keywords": [
      "sports media",
      "fans",
      "fake",
      "content",
      "risks",
      "players",
      "announcements",
      "networks",
      "teams",
      "risk"
    ],
    "qualityScore": 1,
    "link": "https://sports.yahoo.com/articles/global-sports-face-challenges-ai-001521496.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/01588c76abbcb3da3383f7e9f92df23c",
    "created_at": "2026-01-17T00:56:53.942Z",
    "topic": "sports"
  },
  {
    "slug": "commander-ai-mac-ui-for-claude-code",
    "title": "Commander AI – Mac UI for Claude Code",
    "description": "A beautiful native macOS app for Claude Code. Build software with AI assistance, integrated git workflow, and seamless project management. Free for Mac developers.",
    "fullText": "Commander is a beautiful native macOS application that provides a powerful interface for Claude Code, Anthropic's AI coding agent. It combines AI‑powered coding assistance with integrated git workflow and seamless project management, all wrapped in a native Mac experience.\n\nYes, Commander is completely free. There are no subscription fees, premium tiers, or hidden costs. It's free for all Mac developers.\n\nCommander requires macOS 15.0 or later. It is built with native Swift and optimized for the latest macOS features.\n\nCommander is built on top of Anthropic's Claude Code agent, providing a native macOS interface that makes it easy to interact with the AI for code generation, refactoring, documentation, and more. It includes integrated git workflow for seamless version control.\n\nCommander requires macOS 15.0 or later and the Claude Code CLI to be installed. You can install the Claude Code CLI by following the setup instructions at code.claude.com/docs/en/setup\n\nCommander is a native macOS interface that connects to Claude Code CLI running locally on your machine. All AI interactions are handled through Claude Code CLI and are subject to Anthropic's privacy policy and terms of service. Commander itself does not collect or transmit your code or personal data to third parties.",
    "readingTime": 1,
    "keywords": [
      "claude code cli",
      "integrated git",
      "git workflow",
      "macos interface",
      "native macos",
      "commander",
      "coding",
      "agent",
      "seamless",
      "free"
    ],
    "qualityScore": 0.85,
    "link": "https://commanderai.app/",
    "thumbnail_url": "https://commanderai.app/og-image.png",
    "created_at": "2026-01-17T00:56:27.174Z",
    "topic": "tech"
  },
  {
    "slug": "i-trained-a-gpt-to-think-like-steve-jobs-and-help-me-run-my-company-ai-is-scary-but-its-also-my-biggest-tool",
    "title": "I trained a GPT to think like Steve Jobs and help me run my company. AI is scary, but it's also my biggest tool.",
    "description": "Solopreneur Yesim Saydan shares how she trained over 17 custom GPT workers and a Steve Jobs—inspired custom GPT to work for her consultancy agency.",
    "fullText": "This as-told-to essay is based on a conversation with Yesim Saydan, a branding and communication expert in her early 50s, based in the Netherlands. The following has been edited for length and clarity.\n\nWhen I'm stuck on a business decision or need to come up with a creative idea or strategy, brainstorming usually starts with my Steve Jobs custom GPT.\n\nMy solo-consultancy business helps senior executives and established entrepreneurs grow their authority and influence through social media and brand strategies. But scaling that work on my own was challenging.\n\nBefore AI, if I wanted to scale the number of clients I could take on, my main option was hiring freelancers for special projects or tasks. I spent a lot of unnecessary time training the freelancers on my specific framework, and it often felt like they didn't care as much as I did.\n\nWhen OpenAI launched custom GPTs, everything changed. I used the feature to create over 17 custom GPTs to build my team. Then I thought of my ideal mentors and created custom GPTs of them.\n\nOne of my first jobs was at Citibank as a project manager on Wall Street, following my move to the US from Turkey for my MBA. That kicked off my 14-year corporate career, during which I worked in New York, Paris, and the Netherlands.\n\nI started my business about a decade ago because I wanted more flexibility in my work schedule. At the time, social media was just starting to take off, and I saw a clear opportunity.\n\nI'd played with AI tools before, but OpenAI's custom GPTs changed the game. Initially, I envisioned creating my ideal four-person team of agents. I quickly realized AI produces subpar results when it's overloaded with too many tasks.\n\nInstead, I created a custom GPT for each important task I wanted the AI to perform. That's how I ended up with more than 17 custom GPTs making up what feels like my perfect team.\n\nI can create a custom GPT in five or 10 minutes, but what actually makes it powerful is the training process. I create standard operating procedure documents for each task and client, serving as training materials for my agents that outline my methodology and frameworks.\n\nI have client-specific AI agents trained on each major client's tone, goals, and past conversations, so I'm never starting from scratch with a task. The training is ongoing. Every time I make a query or upload a document, the agent improves, just like a real employee would.\n\nWhen I need to communicate or create content in a client's tone of voice, the draft I end up with is so tailored that it feels like I spent hours perfecting it, when in fact my AI team handled it.\n\nI've trained a market researcher, a sales call analyst, a proposal writer, a video scriptwriter, and even a custom GPT to evaluate LinkedIn profiles using six pillars to determine if the current LinkedIn presence builds authority, attracts their ideal client, and establishes trust, clarity, and uniqueness. These free me up to focus on big-picture strategy.\n\nAfter creating my ideal employees, I asked myself which mentors I would love to have alive or dead. Steve Jobs is known for his creativity and innovation, and there are numerous videos already online about him; he's the perfect mentor to create a custom GPT for.\n\nIn the instructions, I started with things like, \"you are Steve Jobs, you have decades of experience in X, Y, Z, your most important skill is creativity, innovation, thinking out of the box.\"\n\nThere are two types of video transcripts I trained it on. I uploaded transcripts from videos where he explains his strategies and what he looks for in products. The second approach was training through examples. I found videos showing how he launched products like the iPhone or iPad, so the AI learns from both his thought process and his execution of those launches.\n\nTo get it to the level it is now, I spent roughly 40 hours researching and building training assets, including PDFs and other materials the GPT can use as references. I continue adding more whenever I find relevant material, and I now have custom GPTs for Dan Kennedy and Elon Musk as well.\n\nThe frustrating part with training AI models is that I can give it a lot of information that's required to have the superpowers of Steve Jobs, but then the AI could take that and produce a lot of different things.\n\nWhen I prompt it, I avoid asking questions like \"What do you think of this idea?\" because the AI usually wants to agree with me and please me. Instead, I ask, \"On a scale from one to 10, how good is this idea?\"\n\nIt's not going to say the idea is bad, but now it might tell me it's a five. Then I'll ask, \"OK, what would make it a 10?\"\n\nThat's usually when it starts drawing on the experience of Steve Jobs that I've trained it with. We can go back and forth until I get the most useful and honest feedback possible.\n\nIt depends on the task. For more strategic outputs, I usually go through three to five rounds of refinement.\n\nWhen a product like NotebookLM was introduced, I started thinking, \"Oh my God, this is going to make the entire human race obsolete.\" I find AI products fascinating at first, but they can really scare me.\n\nI truly believe we don't know what the world will look like even a year from now. Sometimes I literally freeze thinking about the impacts, and if everyone will end up homeless, but I usually try to remind myself I'm not God or a higher power, and I don't know what will happen. This calms me down.\n\nI also realized that AI, by itself, is powerful, but what makes it truly magical is when we combine our expertise and skills with it. Using custom agents as an extension of our brain, rather than a replacement, is what really produces great output.\n\nThere's no turning back from it.",
    "readingTime": 6,
    "keywords": [
      "i've trained",
      "custom gpts",
      "custom gpt",
      "social media",
      "client's tone",
      "steve jobs",
      "created custom",
      "create custom",
      "training",
      "idea"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/built-steve-jobs-custom-gpt-to-make-my-business-decisions-2026-1",
    "thumbnail_url": "https://i.insider.com/69694b6504eda4732f2f29a6?width=949&format=jpeg",
    "created_at": "2026-01-17T00:56:12.704Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-getting-ads-sam-altman-once-called-them-a-last-resort",
    "title": "ChatGPT is getting ads. Sam Altman once called them a 'last resort.'",
    "description": "The move to integrate ads into ChatGPT comes as OpenAI looks to increase its revenue amid $1.4 trillion in spending commitments and a possible IPO.",
    "fullText": "Netflix famously backtracked on its stance toward ads. Now, OpenAI is following suit.\n\nThe AI pioneer announced that ads are coming to ChatGPT — less than two years after OpenAI CEO Sam Altman portrayed them as \"a last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAltman's softened stance since then underlines the massive change OpenAI has undergone in the last two years, and the company's embrace of advertising is a testament to just how expensive the AI race has become.\n\nIn June, the OpenAI CEO said he wasn't \"totally against\" ads, he just wanted to make sure OpenAI got the balance correct.\n\n\"We haven't done any advertising product yet. I kind of...I mean, I'm not totally against it,\" Altman said on OpenAI's podcast. \"I can point to areas where I like ads. I think ads on Instagram, kinda cool. I bought a bunch of stuff from them. But I am, like, I think it'd be very hard to — I mean, take a lot of care to get right.\"\n\nIn October, Altman expressed a desire to make sure the company went about ads in the proper manner when asked about OpenAI's past criticisms that other tech companies made addictive products.\n\n\"We're definitely worried about this,\" Altman said in response to a question that expressed concern about the similarities of Sora, OpenAI's AI video app, and TikTok and the potential of ads. \"I worry about it, not just for things like Sora and TikTok and ads in ChatGPT, which are maybe known problems that we can design carefully.\"\n\nMeanwhile, Altman, former Instacart CEO Fidji Simo (who OpenAI hired as its CEO of applications in early 2025), and seemingly every other member of the company's C-suite have expressed an almost insatiable demand for more compute in interviews.\n\nIt's proven a costly endeavor. OpenAI now has roughly $1.4 trillion in spending commitments on data centers and related infrastructure, raising questions about how it plans to pay the bills without the benefit of the advertising businesses of its Big Tech competitors, like Google and Meta.\n\nOpenAI also completed its restructuring into a more traditional for-profit, a move Altman said was designed to make it easier to attract future investments.\n\nAs part of the announcement, OpenAI said that free and Go users of the popular AI chatbot would start seeing ads being tested \"in the coming weeks.\"\n\nSharing details on the planned test, OpenAI said that ChatGPT's results \"will not be influenced by ads,\" the ads will be clearly labeled, and chatbot conversations will remain private and not shared with advertisers.\n\nIn the coming weeks, we plan to start testing ads in ChatGPT free and Go tiers.\n\nWe’re sharing our principles early on how we’ll approach ads–guided by putting user trust and transparency first as we work to make AI accessible to everyone.\n\nWhat matters most:\n- Responses in… pic.twitter.com/3UQJsdriYR\n\nPaid users of OpenAI's Plus, Pro, Business, and Enterprise plans won't see the ads, the company said.\n\nSimo, OpenAI's CEO of applications, who has previously spoken about her desire to get the ads balance correctly, wrote on X that the most important factor was \"ads will not influence the answers ChatGPT gives you.\"\n\nWhile Instacart launched ads during Simo's time leading the company, she has said OpenAI's approach would look different.\n\n\"If we ever were to do anything, it would have to be a very different model than what has been done before,\" she said. \"What I've learned from building ad platforms is that the thing people don't like about ads very often is not the ads themselves, it's the use of the data behind the ads.\"",
    "readingTime": 4,
    "keywords": [
      "openai ceo",
      "chatgpt",
      "advertising",
      "expressed",
      "altman",
      "stance",
      "resort",
      "plus",
      "business",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-openai-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8970e1ba468a96aa3bb6?width=1200&format=jpeg",
    "created_at": "2026-01-17T00:56:12.511Z",
    "topic": "finance"
  },
  {
    "slug": "apple-is-losing-its-grip-on-the-worlds-tech-supply-chain",
    "title": "Apple is losing its grip on the world's tech supply chain",
    "description": "Apple once set the bar for suppliers including TSMC. Now it's being unseated by Nvidia and AI cloud giants.",
    "fullText": "For over a decade, Apple sat at the center of the tech supply chain. Its enormous scale let it dictate pricing, lock up capacity, and steer the roadmaps of suppliers that made everything from chips and memory to substrates and packaging. That era is ending.\n\n\"Apple is no longer the gravitational center of the hardware universe,\" said Brad Gastwirth, global head of research and market intelligence at Circular Technology, who tracks the industry's supply chain.\n\n\"Apple still moves huge volumes and has unmatched brand strength. But the company is no longer the anchor client for fabs, substrate makers, or key component suppliers. That's a fundamental change.\"\n\nThis is important because the tech companies that control the supply chain are more likely to win. When you can order the largest amount of a key component, you get better pricing and a more reliable supply. That results in better-priced products that are available sooner than your rivals. This power is now shifting toward AI giants, including Nvidia and huge cloud players such as Amazon, Microsoft, and Google (aka \"AMG\").\n\nThe most visible sign is at TSMC, the world's largest chipmaker. It's famous for churning out cutting-edge iPhone chips, which gave Apple a huge advantage over other consumer hardware players.\n\nWhen TSMC reported results this week, it became very clear that its smartphone business is no longer its most important segment. High-performance computing — a category dominated by AI chips for companies like Nvidia and hyperscale cloud providers — now accounts for roughly 58% of TSMC's revenue, far surpassing smartphone processors.\n\nTSMC makes chips for Nvidia's AI servers, which are being snapped up in huge volumes by the cloud giants. They pack this gear into huge data centers that train and run AI models to power new services such as ChatGPT. Is that a better business than making iPhone chips?\n\nTSMC's CEO C.C. Wei answered that. He's been talking to these AI giants a lot lately. Here's what he said on this week's earnings call:\n\n\"They show me the evidence that the AI really helps their business. So they grow their business successfully and see the financial return. So I also double-checked their financial status. They are very rich.\"\n\nSuppliers go where the money is. Increasingly, the biggest Benjamins are coming from AI and cloud giants, not Apple.\n\nThis shift is rippling through the rest of the supply chain. Memory chip makers are reallocating capacity away from phones and PCs to feed AI data centers hungry for DRAM, dynamic random access memory. This is a common type of memory chip that's used in AI servers and iPhones.\n\nMemory prices have surged lately, and that's likely to push up smartphone costs and potentially squeeze margins. Nvidia has locked in long-term memory supply, leaving smartphone makers with less negotiating power.\n\n\"For the past 15 years, Apple's scale let it dictate component supply, pricing, and roadmaps,\" Gastwirth said. \"That leverage diminishes when suppliers earn higher margins and higher growth from AI customers than from smartphones.\"\n\nBottlenecks are emerging in unexpected places, too. A shortage of high-end glass cloth, a critical input for chip substrates, has suppliers prioritizing AI customers who pre-pay and sign multi-year contracts.\n\nApple, which uses these substrates across nearly all its products, is now competing with AI chipmakers for limited supply and even sending engineers to help smaller suppliers qualify alternative materials, according to a report this week from Nikkei.\n\nManufacturing partners are also re-prioritizing. Foxconn, once synonymous with iPhone assembly, now generates more revenue from AI servers than from consumer electronics. Its fastest-growing customers are hyperscalers and Nvidia, not Apple.\n\nNone of this makes Apple irrelevant. The company remains one of the world's largest buyers of components. However, in a supply chain increasingly shaped by AI, pricing, allocation, and capacity planning are being set elsewhere — and Apple is learning what it's like to be just another very large customer.\n\n\"In the 2010s, Apple set the pace for the supply chain,\" Gastwirth said. \"In the late 2020s, Nvidia, hyperscalers, and AI infrastructure now dictate pricing, allocation, and long‑term capacity planning.\"\n\nApple didn't respond to a request for comment on Friday.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "world's largest",
      "capacity planning",
      "huge volumes",
      "pricing allocation",
      "dictate pricing",
      "iphone chips",
      "cloud giants",
      "memory chip",
      "supply chain",
      "suppliers"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/apple-losing-grip-tech-supply-chain-tsmc-nvidia-foxconn-2026-1",
    "thumbnail_url": "https://i.insider.com/696a837aa645d11881878256?width=800&format=jpeg",
    "created_at": "2026-01-17T00:56:12.315Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-ads-might-not-be-a-totally-bad-thing-hear-me-out-on-this",
    "title": "ChatGPT ads might not be a totally bad thing. Hear me out on this.",
    "description": "ChatGPT says it's bringing ads to some US users. It might be annoying. But it also might be a good thing. (I know, I know. Hear me out!",
    "fullText": "All day, I see ads. Ads when I scroll social feeds, ads when I search Google, and ads on every website I go to. You're looking at some ads on this website right now (hopefully they aren't too annoying). I've lived to tell the tale — and so have you.\n\nOpenAI announced on Friday that it will start testing ads in ChatGPT for US users on its free and Go tiers, something that had been rumored for a while. If you're a brand or advertiser, this might be exciting news, but I think most of us who are merely ChatGPT users are not thrilled.\n\nThere are a few obvious problems here. But I think we can say \"eh\" to most of them.\n\nProblem 1: Ads can be annoying! I agree! But as previously mentioned, we are all used to seeing ads everywhere at all times. It's just the constant buzz of white noise in every online experience.\n\nBut, eh: Since OpenAI is first testing this as a freemium model, sure, you can get rid of the ads if you pay. We're already dealing with that in a ton of other services like Netflix, Hulu, Spotify, and YouTube. I pay for all of those because I've decided the ads are annoying enough to pay extra to skip. (Actually, I don't pony up for ad-free Hulu. I made the calculation I don't watch it enough to make it worth it. On the other hand, I do play enough solitaire on my phone that I ponied up for the ad-free version.)\n\nProblem 2: It's a trust issue. Can we trust ChatGPT to give \"real\" answers rather than ads when we ask it to recommend a product or service, even if it's also running ads?\n\nBut, eh: I think people are already used to understanding things like Google search results with ads where there's a mix of organic and sponsored results. If I ask ChatGPT to help me revive my wilting monstera plant, and it shows me an ad for Miracle-Gro plant food at the bottom, will I be confused? Probably not because I've seen this kind of thing before on Google and social feeds. The mockups OpenAI shared flag to me pretty clearly what's an ad and what isn't.\n\nProblem 3: If ChatGPT is in the advertising biz, then it's subject to the pressures of brands and corporations that pay for those ads.\n\nBut, eh: OK, this one is actually real. Advertisers can and will exert pressure on platforms, broadcasters, publishers, and any other venues where their ads appear. They are powerful in that way!\n\nBut hear me out: This can actually have a kind of normalizing effect, in a positive way, especially when we're thinking about something like a huge AI company.\n\nConsider the case of an outlier event: In 2022, when Elon Musk first took over Twitter/X. Advertisers fled when the platform was deluged with hateful content, and it actually caused X to have to change its ways to woo them back. When we consider all the wildly terrifying things that a platform with immense global power like OpenAI can do, it's actually kind of a good thing to be hemmed in by the middle-of-the-road, safe values and standards of the Coca-Cola Company or other big, would-be US-based advertisers. It means you can't make your tech product so problematic that Walmart doesn't want to be associated with it.\n\nProblem 4: ChatGPT, a wonderful product that operated with a clean design, is now just another victim of enshittification!\n\nBut, eh: Buddy, if you're a huge fan of ChatGPT and the purity of the beautiful, human, internet, I don't know what to tell you. Do you also love swimming, but hate water? Pick a side!\n\nLook, am I excited to have one more place to be annoyed by ads? No. But I also feel like this isn't the worst thing to happen with AI — not even the worst thing this week. Although I would like to reserve the right to change my mind on this if it turns out to be really awful later down the line. Gotta hedge here.",
    "readingTime": 4,
    "keywords": [
      "social feeds",
      "google",
      "you're",
      "annoying",
      "i've",
      "don't",
      "product",
      "chatgpt",
      "search",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chatgpt-ads-advertising-free-why-2026-1",
    "thumbnail_url": "https://i.insider.com/696a955da645d1188187865e?width=517&format=jpeg",
    "created_at": "2026-01-17T00:56:12.151Z",
    "topic": "finance"
  },
  {
    "slug": "figma-ceo-dylan-field-says-he-has-a-bias-for-hiring-young-workers-because-theyre-likely-ai-natives",
    "title": "Figma CEO Dylan Field says he has a 'bias' for hiring young workers because they're likely AI natives",
    "description": "Figma CEO Dylan Field says AI skills give young professionals an edge in hiring as the job market faces AI-driven changes.",
    "fullText": "Many young people are worried that AI is muscling in on the entry-level job market.\n\nDylan Field, the 34-year-old billionaire CEO of Figma, however, says AI gives young people an advantage in the hiring process.\n\nDuring a recent appearance on the \"In Good Company\" podcast, produced by Norges Bank Investment Management, Field said the effect of AI on hiring is a \"critical\" debate happening now in the software industry.\n\n\"Does AI mean that you should hire senior people or middle-level, or junior, or are all the jobs going to go away because AI will replace them all?\" Field asked. \"I've heard that last one a bunch of times, and it hasn't come true yet. All the people have said that. They continue to hire.\"\n\nField said that, in his opinion, young professionals have an advantage because they tend to have a better understanding of AI, an increasingly important skill.\n\n\"My bias actually is a lot more toward the junior folks, and I think people that are younger are AI native in a way that folks that are older have to learn,\" Field said.\n\nHe said Figma, which offers design products and services and competes directly with Adobe, has always hired a mix of ages, but that an understanding and passion for AI is a must going forward.\n\n\"I think that it is important that people come in, first of all, knowing that we're pushing full steam ahead into the AI era,\" Field said. \"So, if you have a bias against AI, that's a great dinner-table conversation between us, but we're very focused on making sure that we build for this AI age.\"\n\nYoung professionals are navigating a labor market bogged down in unemployment and uneven job growth. The Bureau of Labor Statistics in December published its final 2025 jobs report, which showed that the job market has remained stagnant, economists said.\n\nThe rise of AI has only added to that instability. Many companies these days are betting that AI will be able to do many of the tasks of entry-level workers, and economists say that could lead them to pause hiring young professionals.\n\nField, however, doesn't share that outlook.\n\nDuring an October 2025 appearance on \"Lenny's Podcast,\" Field said he doesn't think AI will take human jobs at all.",
    "readingTime": 2,
    "keywords": [
      "job market",
      "hiring",
      "jobs",
      "professionals",
      "field",
      "entry-level",
      "however",
      "advantage",
      "appearance",
      "hire"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/figma-ceo-dylan-field-hiring-jobs-market-ai-age-bias-2026-1",
    "thumbnail_url": "https://i.insider.com/696a6fe8e1ba468a96aa36e9?width=1200&format=jpeg",
    "created_at": "2026-01-17T00:56:11.947Z",
    "topic": "finance"
  },
  {
    "slug": "an-openai-researcher-turned-venture-capitalist-says-investors-are-3-to-5-years-behind-the-latest-ai-studies",
    "title": "An OpenAI researcher turned venture capitalist says investors are 3 to 5 years behind the latest AI studies",
    "description": "Jenny Xiao, who founded Leonis Capital in 2021, said AI needs more investors who understand frontier AI technology.",
    "fullText": "There is a yearslong lag in the AI hype cycle, according to one former AI researcher turned venture capitalist.\n\nJenny Xiao, who cofounded Leonis Capital in 2021 after a stint at OpenAI, said the current investment excitement around AI is far behind the actual research.\n\n\"There is a massive disconnect between what researchers are seeing and what investors are seeing,\" Xiao said on the Fortune Magazine podcast this week.\n\nWhat's being discussed at the biggest AI conferences is as much as 3 to 5 years behind what researchers are thinking about, Xiao said.\n\n\"We are so behind the technical frontier, and that's the gap I really want to bridge,\" she added.\n\nXiao, who dropped out of a Ph.D. program in economics and AI to take a researcher role at OpenAI, founded Leonis Capital to bridge the worlds of venture capital and deep academic AI research.\n\n\"With AI, there needs to be a new generation of founders. There needs to be a new generation of VCs,\" she said.\n\nIt's also the first time investors need to be able to provide financial support to both the market and the technology, she added. Unlike SaaS companies, which were built on a \"stable tech stack,\" AI is moving fast. To keep up, Xiao said investors are going to need to be as technical as the founders.\n\nIf she has one piece of advice for investors who haven't gone deep into the technical side, it's that they should know \"AI progress isn't linear,\" she said.\n\nThey should know AI progress happens in \"lumps,\" she said. So, questions about why AI progress is slowing down or speeding up aren't the best way to characterize the rate of development.\n\n\"It's neither of those two extremes,\" she said. \"It's somewhere in between.\"\n\nLeonis Capital did not immediately respond to a request for comment from Business Insider.",
    "readingTime": 2,
    "keywords": [
      "leonis capital",
      "investors",
      "it's",
      "behind",
      "technical",
      "progress",
      "researcher",
      "venture",
      "openai",
      "research"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/ai-investment-hype-cycle-lag-research-leonis-capital-jenny-xiao-2026-1",
    "thumbnail_url": "https://i.insider.com/69694b8b764ca5f34d2a88e0?width=1200&format=jpeg",
    "created_at": "2026-01-17T00:56:11.816Z",
    "topic": "science"
  },
  {
    "slug": "chatgpt-to-start-showing-ads-in-the-us",
    "title": "ChatGPT to start showing ads in the US",
    "description": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product",
    "fullText": "Ads to be placed alongside answers as OpenAI looks to beef up revenue for flagship AI product\n\nChatGPT will start including advertisements beside answers for US users as OpenAI seeks a new revenue stream.\n\nThe ads will be tested first in ChatGPT for US users only, the company announced on Friday, after increasing speculation that the San Francisco firm would turn to a potential cashflow model on top of its current subscriptions.\n\nThe ads will start in the coming weeks and will be included above or below, rather than within, answers. Mock-ups circulated by the company show the ads in a tinted box. They will be served to adult users “when there’s a relevant sponsored product or service based on your current conversation”, according to OpenAI’s announcement. Ads will not be shown to users under 18 and will not appear alongside answers related to sensitive topics such as health, mental health or politics. Users will be able to click to learn about why they received a particular ad, according to OpenAI.\n\nPreviously, OpenAI’s CEO, Sam Altman, expressed reluctance to introduce ads to the chatbot: “I kind of hate ads just as an aesthetic choice.” His company has made commitments to spend more than $1tn on infrastructure supporting AI in the coming years. Altman has said that revenues are running at well over $13bn a year.\n\n“Maybe there could be ads outside the [large language model] stream that are still really great, but the burden of proof there would have to be very high. And it would have to feel really useful to users and really clear that it was not messing with the model’s output,” Altman said recently. “I think it’d be very hard, we’d have to take a lot of care to get it right. People have a very high degree of trust in ChatGPT.”\n\nIn a blogpost on Friday, OpenAI attempted to reconcile Altman’s distaste for ads with the need for revenue: “Our enterprise and subscription businesses are already strong, and we believe in having a diverse revenue model where ads can play a part in making intelligence more accessible to everyone. Once we begin testing our first ad formats in the coming weeks and months, we look forward to getting people’s feedback.”\n\nThe company is also launching ChatGPT Go, which it bills as a low-cost subscription tier, for $8 a month.",
    "readingTime": 2,
    "keywords": [
      "users",
      "revenue",
      "model",
      "altman",
      "alongside",
      "product",
      "stream",
      "openai’s",
      "health",
      "subscription"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/jan/16/chatgpt-ads-in-revenue-boost",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-01-17T00:56:10.698Z",
    "topic": "tech"
  },
  {
    "slug": "do-you-trust-ai-enough-to-stop-saving-for-retirement",
    "title": "Do you trust AI enough to stop saving for retirement?",
    "description": "It's become a debate thanks to a bold proclamation by Elon Musk: Saving for retirement in 10 or 20 years \"won't matter.\"",
    "fullText": "How confident are you in AI? Enough to bet your retirement on it?\n\nThat's become a debate thanks to a bold proclamation by Elon Musk: Saving for retirement in 10 or 20 years \"won't matter.\"\n\nMusk's confidence comes from the work he's doing in artificial intelligence. The technological advancements will create an \"abundance\" of resources, granting everyone a \"universal high income,\" the Tesla and SpaceX CEO said.\n\nThat's easy to say when you have checks notes and almost passes out nearly $700 billion in the bank. What about people whose net worths have a lot fewer zeros?\n\nA group of BI reporters reached out to seven personal finance and AI gurus to get their take. They all agreed you shouldn't stop those direct deposits into your 401(K).\n\nEven if AI increases productivity and reduces costs over time, it's not clear whether the benefits will be distributed evenly across the population.\n\nMeanwhile, Americans are already in a precarious position regarding their retirement. Last year, BI's Noah Sheidlower spoke to nearly 200 Americans who continue to work past the age of 80 as part of our \"80 over 80\" series.\n\nWhile some like the fact that they can still work, others don't have a choice. And that's with the benefit of Social Security, a safety net that might not exist in the future.\n\nYou can't completely discredit Musk's take, though.\n\nYes, forgoing retirement on the belief that AI and tech will just figure it out in a few decades is a massive gamble. But the past few years have reminded us that tech can quickly flip the script on conventional wisdom.\n\nFive years ago, a career as a computer programmer felt secure. Now … not so much.\n\nIt's not a new phenomenon. My colleague Alistair Barr recently wrote in his Tech Memo newsletter () that the US quickly went from having 25 million horses to fewer than two million thanks to the arrival of the steam engine.\n\nEven if Musk is right, you'll probably still want some extra pocket change. Money isn't just about being able to buy things. It's also about access and exclusivity, and it's not clear how that might work in this new utopia.\n\nIt's not just going to Disney World; it's being able to cut the lines. It's not just getting tickets to the game; it's sitting courtside.\n\nBut if everyone's got \"universal high income,\" how will those perks work? Will all of us be equal? Or maybe some of us will be more equal than others?",
    "readingTime": 3,
    "keywords": [
      "it's",
      "retirement",
      "thanks",
      "musk's",
      "universal",
      "income",
      "nearly",
      "fewer",
      "others",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-retirement-savings-elon-musk-401k-finance-experts-2026-1",
    "thumbnail_url": "https://i.insider.com/69696401764ca5f34d2a8c8a?width=800&format=jpeg",
    "created_at": "2026-01-16T18:19:05.025Z",
    "topic": "finance"
  },
  {
    "slug": "walmart-is-shuffling-its-top-leadership-team-as-ai-rapidly-reshapes-retail",
    "title": "Walmart is shuffling its top leadership team as 'AI rapidly reshapes retail'",
    "description": "Walmart is promoting four execs to new roles under new CEO John Furner, including US e-commerce chief David Guggina, who will head up the US division.",
    "fullText": "Walmart is getting a major leadership shuffle as the retail giant leans hard into AI.\n\nThe company said Friday that four executives would move into new leadership roles when incoming CEO John Furner takes the helm on February 1.\n\n\"These internal promotions reflect our culture of opportunity and the depth of our leadership bench,\" Furner said in a statement.\n\nTaken together, the names have been behind some of Walmart's most significant growth strategies in recent years as it reaches beyond traditional brick-and-mortar retail into the age of e-commerce and agentic shopping.\n\nSucceeding Furner as head of Walmart's US division is David Guggina, who has been with the company for nearly eight years across roles, from automation to supply chain to e-commerce. He previously spent nine years at Amazon.\n\nWalmart US chief growth officer Seth Dallaire is expanding his role to lead global growth. Dallaire has been at Walmart for four years and previously served as Instacart's chief revenue officer and as a VP of ad sales at Amazon.\n\nElsewhere in the organization, Sam's Club CEO Chris Nicholas is set to take over Walmart's international division, following the departure of international CEO Kath McLay. Nicholas previously served as chief financial officer for the international division.\n\nUS chief merchandising officer Latriece Watkins will step into Nicholas's role as head of the warehouse club chain. Watkins started as an intern at Walmart in 1997.\n\n\"These leadership changes also mark a key step in how we organize for the future,\" Furner said in his statement. \"As AI rapidly reshapes retail, we are centralizing our platforms to accelerate shared capabilities, freeing up our operating segments to be more focused on and closer to our customers and members.\"",
    "readingTime": 2,
    "keywords": [
      "leadership",
      "chief",
      "officer",
      "retail",
      "growth",
      "division",
      "previously",
      "roles",
      "statement",
      "e-commerce"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/walmart-shuffles-top-leadership-team-as-ai-rapidly-reshapes-retail-2026-1",
    "thumbnail_url": "https://i.insider.com/696a5098764ca5f34d2a9150?width=1200&format=jpeg",
    "created_at": "2026-01-16T18:19:04.720Z",
    "topic": "finance"
  },
  {
    "slug": "meta-is-closing-its-vr-workplace-app-amid-reality-labs-layoffs",
    "title": "Meta is closing its VR workplace app amid Reality Labs layoffs",
    "description": "Meta said it's shutting down Workrooms in February, deleting user data as Reality Labs faces layoffs and the company continues to shift toward AI.",
    "fullText": "Meta is discontinuing its workplace virtual reality app Horizon Workrooms as it makes cuts in its metaverse division.\n\nThe Facebook owner said in a Thursday blog post that users would no longer be able to access Workrooms from February 16, after which any associated data will be deleted.\n\nMeta launched Workrooms in 2021 as a platform for remote teams to collaborate in virtual reality, as part of the company's big push into the metaverse.\n\nIts closure comes as the tech giant prepares to lay off workers in its Reality Labs division, which develops the company's virtual reality headsets and VR-based social network Horizon Worlds. Roughly 10% to 15% of Reality Labs' 15,000 employees are expected to be laid off, The New York Times reported.\n\nThe metaverse has been a costly bet for Meta, racking up more than $70 billion in losses since 2020. It has faced repeated rounds of cuts as Meta shifts its attention — and spending — toward AI.\n\nIn a memo obtained by Business Insider last year, Meta's CTO, who oversees the company's metaverse efforts, called 2025 \"the most critical\" year of his tenure.\n\nWorkrooms showcased how Meta Horizon could bring people together to work, collaborate, and connect, the company said. But Meta Horizon has since developed into a social platform that supports a wide range of productivity apps and tools.\n\nMeanwhile, Meta said its Horizon managed services will no longer be available for purchase from February 20, although customers would continue to receive support until the start of 2030.",
    "readingTime": 2,
    "keywords": [
      "virtual reality",
      "reality labs",
      "metaverse",
      "company's",
      "meta",
      "cuts",
      "division",
      "longer",
      "february",
      "platform"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/meta-closing-vr-horizon-workplace-app-reality-labs-layoffs-2026-1",
    "thumbnail_url": "https://i.insider.com/69611cca04eda4732f2ec680?width=1200&format=jpeg",
    "created_at": "2026-01-16T18:19:04.502Z",
    "topic": "finance"
  },
  {
    "slug": "why-training-employees-pays-off-twice",
    "title": "Why Training Employees Pays Off Twice",
    "description": "Companies invest millions in employee training, yet the payoff often seems uncertain. A new study suggests the problem may be what organizations measure: most overlook the impact on managers. Researchers tracked a 16-week upskilling program at a Colombian government agency and found frontline output rose 10%, while help-seeking emails dropped—freeing managers to focus on strategic work. These “spillover effects” accounted for nearly half the program’s total benefits, making training far more cost-effective than traditional ROI models imply.  With AI reshaping roles, leaders should view training as a catalyst for organizational agility—not just individual performance.",
    "fullText": "Why Training Employees Pays Off Twice by Ben RandJanuary 16, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrint",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/01/why-training-employees-pays-off-twice",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_16_2198316237.jpg",
    "created_at": "2026-01-16T18:19:03.791Z",
    "topic": "business"
  },
  {
    "slug": "why-people-create-ai-workslopand-how-to-stop-it",
    "title": "Why People Create AI “Workslop”—and How to Stop It",
    "description": "With the rise of gen AI tools, offices have had to contend with a new scourge: “workslop” or low-effort, AI-generated work that looks plausibly polished, but ends up wasting time and effort as it offloads cognitive work onto the recipient. Workslop can have a corrosive effect on office dynamics. But why do people create it and send it to their colleagues, especially if it can lead to bosses, coworkers, and subordinates thinking less of them? New research suggests that the recipe for workslop is surprisingly simple and under the control of management: It’s the result of unclear AI mandates and overwhelmed teams. Leaders are issuing vague directives for employees to start using extremely powerful tools, while many of those employees are overburdened, psychologically depleted, and operating in environments where it doesn’t feel safe to admit uncertainty or ask for help.",
    "fullText": "Why People Create AI “Workslop”—and How to Stop It by Kate Niederhoffer, Alexi Robichaux and Jeffrey T. HancockJanuary 16, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintAs AI tools have proliferated in workplaces and pressure to use them has mounted, employees have had to contend with the scourge of workslop, or low-effort, AI-generated work that looks plausibly polished, but ends up wasting time and effort as it offloads cognitive work onto the recipient. For the person on the receiving end, it can be a confusing and infuriating experience.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://hbr.org/2026/01/why-people-create-ai-workslop-and-how-to-stop-it",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_12_1262739982_1595860641.jpg",
    "created_at": "2026-01-16T18:19:03.617Z",
    "topic": "business"
  },
  {
    "slug": "openai-to-begin-testing-ads-on-chatgpt-in-the-us",
    "title": "OpenAI to begin testing ads on ChatGPT in the U.S.",
    "description": "OpenAI said ads would not influence ChatGPT's responses and that it will \"never\" sell user data to advertisers.",
    "fullText": "OpenAI on Friday announced it will begin testing ads within ChatGPT in the coming weeks, a highly anticipated decision that could kickstart a lucrative new revenue stream for the artificial intelligence startup.\n\nOpenAI said its Plus, Pro and Enterprise subscriptions will not include ads, but it plans to start testing them with adult free users in the U.S. The company also made its low-cost Go offering available in the U.S. on Friday, and it said users who opt for that plan will also begin to see ads.\n\nThe company inked more than $1.4 trillion worth of infrastructure deals in 2025, and OpenAI CEO Sam Altman said in November that the startup was on track to generate $20 billion in annualized revenue run rate last year.\n\nIntroducing ads to ChatGPT could help OpenAI meet its ambitious spending commitments, as digital advertising has long been the cash cow for other big tech companies like Google and Meta.\n\nAds within ChatGPT will appear at the bottom of the chatbot's answers, and they will be clearly labeled, OpenAI said.\n\nChatGPT's responses will not be influenced by ads, and OpenAI said it will \"never\" sell users' data to advertisers, according to a release.\n\nUsers under the age of 18 will not see ads, and ads will not appear near certain topics, including politics, health and mental health, OpenAI said.\n\nAltman has publicly expressed reservations about introducing ads to ChatGPT in recent years, stating in interviews that doing so could erode users' trust in OpenAI's products. But in a November podcast appearance, Altman said he expected OpenAI to try ads \"at some point,\" though he added that he did not believe it would be the company's biggest revenue opportunity.\n\n\"We'll learn from feedback and refine how ads show up over time, but our commitment to putting users first and maintaining trust won't change,\" OpenAI said in a statement on Friday.\n\nAs OpenAI tests ads, the company said users will be able to learn more about why they're seeing a specific ads, dismiss ads and submit feedback about the experience.\n\nWATCH: OpenAI Investor Letter: Weekly and daily active user figures ‘continue to produce all-time highs’",
    "readingTime": 2,
    "keywords": [
      "within chatgpt",
      "ads within",
      "introducing ads",
      "users",
      "openai",
      "revenue",
      "testing",
      "startup",
      "november",
      "health"
    ],
    "qualityScore": 0.9,
    "link": "https://www.cnbc.com/2026/01/16/open-ai-chatgpt-ads-us.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108074841-1733965530853-gettyimages-2188251582-mc_16795_qpk84voo.jpeg?v=1768570902&w=1920&h=1080",
    "created_at": "2026-01-16T18:19:01.207Z",
    "topic": "tech"
  },
  {
    "slug": "ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it",
    "title": "AI Generated Code Isn't Cheating: OSS Needs to Talk About It",
    "description": "Remember early 2025? \"Vibe coding\" was a meme and seemed mostly a tool for casual builders or those new to coding. It's now 2026, and we find ourselves living in a new reality. Industry leaders like DHH, Karpathy, and Lutke are publicly embracing AI-generated code controlled by human prompting.",
    "fullText": "Remember early 2025? “Vibe coding” was a meme and seemed mostly a tool for casual builders or those new to coding. It was often used disparagingly, or to imply a lack of deep technical expertise. Some very cool basic applications were being built, but AI coding assistants couldn’t reliably function in complex codebases. But what a difference a year has made!\n\nIt’s now 2026, and we find ourselves living in a new reality. Some of the most influential voices in software engineering like DHH (Ruby on Rails), Andrej Karpathy (prev OpenAI, Tesla), Tobi Lutke (Shopify), Salvatore Sanfilippo (Redis), and Mitchell Hashimoto (Ghostty, prev Hashicorp) are publicly embracing a new  paradigm: completely AI generated code controlled by human-in-the-loop prompting. It was also recently publicized that Linus Torvalds (creator of Linux and Git) is leveraging AI vibe-coding in his side-projects.\n\nAI is everywhere: if you’re a software developer, you’ve almost certainly tried at least one AI-assisted coding solution over the past year. It’s a safe assumption that a large portion of developers are using AI to help them, but we still know shockingly little about how their code was derived. This secrecy is outdated, especially now that the practice is being normalized by industry leaders.\n\nThe open source community is built on top of foundations of transparency and collaboration, of which knowledge sharing is a key component. At Mozilla.ai, we believe we must embrace and encourage the disclosure of AI usage as quickly as possible. We need to move away from “Should we AI?” and towards a structure that clearly defines our expectations for where we encourage AI usage and how we document it.\n\nIn our project any-llm, we’ve started to iterate on this philosophy by creating a pull request template that requests a few pieces of information whenever a PR is submitted.\n\nHere’s a snippet of the relevant part of our pull request template:\n\nFirst, we request that the contributors specify their level of AI usage: was AI used to draft and make edits? Or was their contribution completely AI-generated with them only directing it via plain language prompts? Both are acceptable, but it helps a reviewer understand how to approach their review. If we know the code is completely AI generated, we can be candid with our feedback and direct the contributor towards improving their prompting or AI coding configuration to improve quality. Without this transparency, it can be difficult to give feedback since a reviewer doesn’t want to offend the contributor by insinuating that their work came from a bot.\n\nSecond, we request information about the contributors' AI setup: what model(s) and IDE/CLI tools were used? This is valuable metadata for crowdsourcing best practices. Maybe there is one model or tool that works amazingly well with a certain codebase or language! Openly sharing this information allows all of us to learn from each other.\n\nLastly, we request that any responses to comments come from the contributor themselves and not their AI tool. It is frustrating to write comments without knowing if a human is on the other side reading and responding to the feedback. The open source community is a wonderful place to learn from each other, and that learning happens best when humans talk to humans. Of course, AI can be used to help the contributor brainstorm or improve their grammar, but we think the core discussion should still happen between two humans.\n\nWe welcome community opinions and hope to see similar approaches be adopted across the open source community. Let's keep learning and developing together!",
    "readingTime": 3,
    "keywords": [
      "request template",
      "coding",
      "community",
      "contributor",
      "tool",
      "completely",
      "code",
      "usage",
      "feedback",
      "humans"
    ],
    "qualityScore": 1,
    "link": "https://blog.mozilla.ai/ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it/",
    "thumbnail_url": "https://blog.mozilla.ai/content/images/size/w1200/2026/01/George-Sturdy-and-Solomon-Young-s-vehicle-of-amusement.jpg",
    "created_at": "2026-01-16T18:18:59.372Z",
    "topic": "tech"
  },
  {
    "slug": "zep-ai-agent-context-engineering-yc-w24-is-hiring-forward-deployed-engineers",
    "title": "Zep AI (Agent Context Engineering, YC W24) Is Hiring Forward Deployed Engineers",
    "description": "Jobs at Zep AI",
    "fullText": "Zep assembles the right context from chat history, business data, and user behavior so agents are personalized, accurate, and fast. Our open source project Graphiti hit 20k GitHub stars in under 12 months. Sub-200ms retrieval, SOC 2 Type 2/HIPAA certified, used by teams from startups to Fortune 500s.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://www.ycombinator.com/companies/zep-ai/jobs/",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/a5f4989742560bd0715218257a7c7ea7f73ab700.png?1712364271",
    "created_at": "2026-01-16T18:18:57.855Z",
    "topic": "jobs"
  },
  {
    "slug": "automated-tech-news-site-with-custom-multillm-agent-pipelines",
    "title": "Automated tech news site with custom multi-LLM agent pipelines",
    "description": "I built WAYR (What Are You Reading) to solve the \"Signal-to-Noise\" problem in tech news. Most aggregators are either manual (slow) or simple scrapers (noisy).",
    "fullText": "I built WAYR (What Are You Reading) to solve the “Signal-to-Noise” problem in tech news. Most aggregators are either manual (slow) or simple scrapers (noisy). This system uses a custom Python orchestration layer to chain three different LLM providers across a sequential 5-agent pipeline.\n\nThe pipeline is hosted on Modal.com for serverless execution, but the intelligence is managed by a custom Python Orchestrator.\n\nTo keep the system efficient and cost-effective, the Discovery Agent interacts with two distinct persistence layers:\n\nAdd this specific section to your WordPress “How It Works” post. It addresses the exact question of where the data lives.\n\nOne architectural choice in WAYR is that the Orchestrator maintains zero local database state for articles. Instead of mirroring content in a local SQL/NoSQL DB, we treat the WordPress REST API as our primary Source of Truth.\n\nCaching vs. Storage: While the articles themselves live in the WordPress DB, we only maintain lightweight JSON caches on a persistent Modal volume for de-duplication and efficiency tracking.\n\nWe treat our prompts like code. Our Evaluation Framework benchmarks the Classifier against a “Golden Dataset” of 100+ manually labeled samples.\n\nIf you want to see the output of this orchestration in real-time, you can explore the platform through several channels:\n\nRead the Articles: View the Feed – See how the Author and Editor agents synthesize complex tech news into high-signal reports.\n\nRSS Feed: Subscribe – The purest way to get our curated stream without social algorithms.\n\nLinkedIn: Follow WAYR – Follow for daily news post..",
    "readingTime": 2,
    "keywords": [
      "custom python",
      "articles",
      "tech",
      "system",
      "orchestration",
      "pipeline",
      "treat",
      "wayr",
      "wordpress",
      "agent"
    ],
    "qualityScore": 0.85,
    "link": "https://wayr.today/how-it-works/",
    "thumbnail_url": "https://wayr.today/wp-content/uploads/2026/01/wayr-architecture-1024x827.png",
    "created_at": "2026-01-16T12:24:07.036Z",
    "topic": "tech"
  },
  {
    "slug": "just-the-browser",
    "title": "Just the Browser",
    "description": "Remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from web browsers.",
    "fullText": "Just the Browser helps you remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from desktop web browsers. The goal is to give you \"just the browser\" and nothing else, using hidden settings in web browsers intended for companies and other organizations.\n\nThis project includes configuration files for popular web browsers, documentation for installing and modifying them, and easy installation scripts. Everything is open-source on GitHub.\n\nThe setup script can install the configuration files in a few clicks. You can also follow the manual guides for Google Chrome, Microsoft Edge, and Firefox.\n\nWindows: Open a PowerShell prompt as Administrator. You can do this by right-clicking the Windows button in the taskbar, then selecting the \"Terminal (Admin)\" or \"PowerShell (Admin)\" menu option. Next, copy the below command, paste it into the window (Ctrl+V), and press the Enter/Return key:\n\nMac and Linux: Search for the Terminal in your applications list and open it. Next, copy the below command, paste it into the window (Ctrl+V or Cmd+V), and press the Enter/Return key:\n\nStart here if you don't have your preferred web browser installed. You can install the configuration files afterwards.\n\nmacOS (Universal)\nWindows 64-bit x86 (amd64)\nWindows 32-bit x86\nWindows 64-bit ARM (ARM64)\nDebian/Ubuntu 64-bit x86 (amd64)\nFedora/openSUSE 64-bit x86 (amd64)\n\nNot sure which link to use? Try the official download page.\n\nmacOS (Universal)\nWindows 64-bit x86 (amd64)\nWindows 32-bit x86\nWindows 64-bit ARM (ARM64)\n\nNot sure which link to use? Try the official download page or Linux setup instructions.\n\nmacOS (Universal)\nWindows 64-bit x86 (amd64)\nWindows 32-bit x86\nWindows 64-bit ARM (ARM64)\n\nNot sure which link to use? Try the official download page.\n\nGot a question? Check here first, and if you still need help, create an issue on GitHub or join the Discord.\n\nJust the Browser aims to remove the following functionality from popular web browsers:\n\nThe exact list of features modified for each browser can be found on the pages for Google Chrome, Microsoft Edge, and Firefox.\n\nYes. The browser guides include steps for removing the configurations, and the automated script can also do it. The browser guides explain each setting, so you can add, remove, or modify the files before you install them.\n\nJust the Browser has configuration files and setup scripts for Google Chrome, Microsoft Edge, and Mozilla Firefox. However, Chrome on Linux and Edge on Linux are not currently supported.\n\nNot yet. See the issues for Android support and iOS/iPadOS support.\n\nNo. Just the Browser uses group policies that are fully supported by web browsers, usually intended for IT departments in companies or other large organizations. No applications or executable files are modified in any way.\n\nYes, as long as the web browsers continue to support the settings used in the configuration files. Web browsers occasionally add, remove, or replace the settings options, so if the custom configuration breaks, try installing the latest available version.\n\nNo. If you want one, try uBlock Origin or uBlock Origin Lite.\n\nThe group policy settings used by Just the Browser are intended for PCs managed by companies and other large organizations. Browsers like Microsoft Edge and Firefox will display a message like \"Your browser is being managed by your organization\" to explain why some settings are disabled.\n\nYou can open about:policies in Firefox or chrome://policy in Chrome and Edge to see a list of active group policy settings.\n\nYou can do that! However, switching to alternative web browsers like Vivaldi, SeaMonkey, Waterfox, or LibreWolf can have other downsides. They are not always available on the same platforms, and they can lag behind mainstream browsers in security updates and engine upgrades. Just the Browser aims to make mainstream web browsers more tolerable, while still retaining their existing benefits.",
    "readingTime": 4,
    "keywords": [
      "window ctrl+v",
      "ublock origin",
      "macos universal",
      "google chrome",
      "microsoft edge",
      "arm arm",
      "windows bit",
      "command paste",
      "just the browser",
      "download page"
    ],
    "qualityScore": 1,
    "link": "https://justthebrowser.com/",
    "thumbnail_url": "https://justthebrowser.com/media/preview.png",
    "created_at": "2026-01-16T12:24:07.005Z",
    "topic": "tech"
  },
  {
    "slug": "ai-will-transform-the-human-job-and-enhance-skills-says-science-minister",
    "title": "AI will transform the ‘human job’ and enhance skills, says science minister",
    "description": "Patrick Vallance says robots would take away ‘repetitive’ tasks, but Sadiq Khan warns AI will usher in ‘new era of mass unemployment’\nAdvances in AI and robotics will transform human jobs, starting with roles in warehouses and factories, the UK science minister has said, as the government announced plans to reduce red tape for robot and defence tech companies.\nPatrick Vallance said technological progress was creating a “whole new area” for robots to work in. “What’s really changing now is the combination of AI and robotics. It is opening up a whole new area, particularly in the sorts of things like humanoid robotics. And that will increase productivity, it will change the human job,” he told the Guardian.",
    "fullText": "Patrick Vallance says robots would take away ‘repetitive’ tasks, but Sadiq Khan warns AI will usher in ‘new era of mass unemployment’\n\nAdvances in AI and robotics will transform human jobs, starting with roles in warehouses and factories, the UK science minister has said, as the government announced plans to reduce red tape for robot and defence tech companies.\n\nPatrick Vallance said technological progress was creating a “whole new area” for robots to work in. “What’s really changing now is the combination of AI and robotics. It is opening up a whole new area, particularly in the sorts of things like humanoid robotics. And that will increase productivity, it will change the human job,” he told the Guardian.\n\nLord Vallance spoke as a government unit helping to deploy new technologies in Britain announced robots and defence as new sectors receiving its support.\n\nHe said factories and warehouses, already at the forefront of robot deployment, will undergo further change as a result of the new generation of humanoid robots.\n\n“Activities that require movement around warehouses and factories, or those sorts of things that can be made robotic, will be made robotic. I think they will be made robotic in many cases and therefore, will change the nature of those jobs. That’s going to be the first wave,” he said.\n\nMeanwhile, the London mayor, Sadiq Khan, has warned that AI could “usher in a new era of mass unemployment”.\n\nSpeaking in his annual Mansion House speech on Thursday, Khan said artificial intelligence could destroy a significant amount of jobs in London unless ministers act to help replace jobs taken over by the technology.\n\nAsked to comment on Khan’s speech, Vallance said robots would take away “repetitive” tasks. “You take away some of the things which are less interesting, repetitive things that can be done in another way,” he said.\n\nVallance, formerly the government’s chief scientific adviser, added that the example of robotics assisting in surgery showed how the technology can enhance jobs.\n\n“Robotics is not displacing surgeons, it’s radically improving how those surgeons work and allowing things to be done with more precision,” he said.\n\nThe government announced on Friday that the Regulatory Innovation Office (RIO) is expanding its remit to defence tech and robotics, with the aim of slashing red tape for companies operating in those spaces.\n\nThe Department for Science, Innovation and Technology is also releasing £52m for new hubs to drive robotics adoption in British businesses. These hubs will offer companies advice on using robots and live demonstrations. “The RIO will aim to streamline overlapping requirements to bring products to market safely, but more quickly, to improve lives and grow our economy,” it said.\n\nThe department added that autonomous drones could benefit from the wider RIO remit. Such technology could require separate approvals for aviation, data protection, and sector-specific safety rules, in an expensive process that could take months.\n\nVallance was speaking during a visit to Humanoid, a UK-based robotics company that has already deployed its prototypes in a factory operated by German industrial group Siemens.\n\nAdam Kelsall, Humanoid’s head of product management, said the company welcomed “anything that gets us into the real world and testing [robots] sooner”.",
    "readingTime": 3,
    "keywords": [
      "mass unemployment",
      "red tape",
      "repetitive tasks",
      "defence tech",
      "away repetitive",
      "patrick vallance",
      "sadiq khan",
      "robotics",
      "robots",
      "jobs"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/16/ai-will-transform-the-human-job-and-enhance-skills-says-science-minister",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a7700fcd48ab8eba8a7a80e60aaf709f67037c17/465_0_5800_4640/master/5800.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=964be4d94a54cee2848d6d0fb989f06e",
    "created_at": "2026-01-16T12:24:06.604Z",
    "topic": "tech"
  },
  {
    "slug": "x-still-allowing-users-to-post-sexualised-images-generated-by-grok-ai-tool",
    "title": "X still allowing users to post sexualised images generated by Grok AI tool",
    "description": "Despite restrictions announced this week, Guardian reporters find standalone app continues to allow posting of nonconsensual content\nX has continued to allow users to post highly sexualised videos of women in bikinis generated by its AI tool Grok, despite the company’s claim to have cracked down on misuse.\nThe Guardian was able to create short videos of people stripping to bikinis from photographs of fully clothed, real women. It was also possible to post this adult content on to X’s public platform without any sign of it being moderated, meaning the clip could be viewed within seconds by anyone with an account.\n Continue reading...",
    "fullText": "Despite restrictions announced this week, Guardian reporters find standalone app continues to allow posting of nonconsensual content\n\nX has continued to allow users to post highly sexualised videos of women in bikinis generated by its AI tool Grok, despite the company’s claim to have cracked down on misuse.\n\nThe Guardian was able to create short videos of people stripping to bikinis from photographs of fully clothed, real women. It was also possible to post this adult content on to X’s public platform without any sign of it being moderated, meaning the clip could be viewed within seconds by anyone with an account.\n\nIt appeared to offer a straightforward workaround to restrictions announced by Elon Musk’s social network this week. These had been welcomed by the prime minister, Keir Starmer, who had described the photographs generated by Grok as “disgusting” and “shameful”.\n\nAfter weeks of rising public concern, X said late on Wednesday it had “implemented technological measures to prevent the Grok account from allowing the editing of images of real people in revealing clothing such as bikinis”.\n\nIt said the restriction would apply to all users, including paid subscribers, and it had “zero tolerance for any forms of child sexual exploitation, nonconsensual nudity, and unwanted sexual content”. But it did not specify whether people would still be able to create such images on the standalone Grok app, and then share this material publicly on X.\n\nThe Guardian found that this standalone version of Grok, known as Grok Imagine – which is easily accessible through a web browser – was still responding to prompts to digitally remove the clothes from images of women.\n\nReporters uploaded still images of fully clothed real-life women, and prompted the AI tool to dress them in bikinis. The platform responded by going further than the request, to create short videos of the women removing their clothes in the manner of a sexually provocative striptease.\n\nX has been contacted for comment.\n\nRebecca Hitchen, the head of policy and campaigns at the End Violence Against Women Coalition, said such a simple workaround should not be possible.\n\n“The continued ease of access to sophisticated nudification tools clearly demonstrates that X isn’t taking the issue of online violence against women and girls seriously enough,” she said.\n\nHitchen called on the UK government and Ofcom, the media regulator, to pressure X and other platforms “to stop the proliferation of image-based sexual abuse”.\n\n“It’s hard to believe that xAI and Elon Musk can’t work out how to prevent these images from being spewed out by Grok,” said Penny East, the chief executive of the Fawcett Society. “First, Musk decided the solution was to preserve nudification as a privilege only for those users who pay for X. Then he pledged to stop it entirely. And yet it has not stopped.\n\n“The truth is Musk and the tech sector simply do not prioritise safety or dignity in the products they create. It’s a pretty low bar for women to expect that they can converse online without men undressing them. And yet seemingly even that is impossible.”\n\nWhile Downing Street had said it felt “vindicated” by the steps X had taken, there was also caution among ministers about the extent of the changes and how they would be delivered.\n\nOn Thursday, Liz Kendall, the technology secretary, who had described the sexual manipulation of images of women and children as “despicable and abhorrent”, welcomed the move and thanked “those who have spoken out against this abuse, above all the victims”.\n\nHowever, she added: “I will expect the facts to be fully and robustly established by Ofcom’s ongoing investigation.”\n\nStarmer also demanded X to act without delay. “Free speech is not the freedom to violate consent,” he said. “Young women’s images are not public property, and their safety is not up for debate. I welcome that X is now acting to ensure full compliance with UK law – it must happen immediately.”\n\nOfcom said its formal investigation into X, launched on Monday, remained ongoing and it was “working round the clock to progress this and get answers into what went wrong and what’s being done to fix it”.\n\nCanada’s privacy watchdog said it was investigating xAI, while authorities in the Philippines said they were moving to block Grok, with Malaysian authorities planning to take legal action.\n\nBut the controversy may have been helpful for boosting public awareness of Grok. On Thursday, Musk shared a post claiming “popularity and real world usage are skyrocketing globally” – alongside a graph of “Grok” as a search term hitting a new high on Google Trends. Musk simply added: “Try Grok.com.”\n\nA government spokesperson said: “The Online Safety Act already requires platforms like X to prevent illegal content, including nonconsensual intimate images and child abuse material, from appearing on their services.\n\n“The secretary of state has said she expects X’s compliance with UK laws to be fully and robustly established by Ofcom’s investigation, which is already under way, and the government will not rest until social media companies meet their legal duties.\n\n“We are also taking further action through the proposed ‘nudification’ offence, which will target tools designed specifically to generate nonconsensual intimate images.”",
    "readingTime": 5,
    "keywords": [
      "robustly established",
      "fully clothed",
      "nonconsensual intimate",
      "intimate images",
      "the guardian",
      "women",
      "content",
      "bikinis",
      "create",
      "sexual"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/16/x-still-allowing-sexualised-images-grok-ai-nudification",
    "thumbnail_url": "https://i.guim.co.uk/img/media/07f2280d49e4b4406e0ed939e49bea0574752d5d/0_0_5031_4024/master/5031.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=6170315ac87ef27a86cc99b2ea221de9",
    "created_at": "2026-01-16T12:24:06.579Z",
    "topic": "tech"
  },
  {
    "slug": "ai-is-just-starting-to-change-the-legal-profession",
    "title": "AI is just starting to change the legal profession",
    "description": "I talked to 10 lawyers about how they're using AI.",
    "fullText": "Discussion about this postRestacksAnn Taylor Schwing 14hLiked by Justin CurlAn interesting article, but I expected also to see discussion of AI citations to and quotes from nonexistent cases. This issue is presumably covered by general references to verification. With reported sanctions as high as $52,000 and multiple sanctions in the $10,000 range, surely the danger of sanctions and adverse impact on reputation merit more attention. Years ago, I clerked for a federal judge who placed cases first on the law and motion calendar when an attorney misquoted (e.g., omitting the \"not\") or miscited authorities so the assembly of attorneys and clients awaiting hearing of their matter would all learn the painful lesson.ReplyShareAndrew 15hLiked by Justin CurlAgreed that large versus small matters is an important distinction. There's just no way to mark up a 10 page contract for a small transaction in a way that is cost effective, but a prompt to \"revise this to be more seller friendly\" will get you 90% of the way there in 5 minutes and in another 10 I can spot check for which changes are appropriate and which are unreasonable or unnecessary. ReplyShare1 reply18 more comments...No postsReady for more?",
    "readingTime": 1,
    "keywords": [
      "sanctions",
      "discussion",
      "hliked",
      "cases",
      "justin"
    ],
    "qualityScore": 0.65,
    "link": "https://www.understandingai.org/p/ai-is-just-starting-to-change-the",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!F62P!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46a67be2-0086-4a48-b5ab-0e333ad90e2f_858x641.jpeg",
    "created_at": "2026-01-16T12:24:06.323Z",
    "topic": "tech"
  },
  {
    "slug": "form-8k-ses-ai-corp-for-16-january",
    "title": "Form 8K SES AI Corp For: 16 January",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/filings/form-8k-ses-ai-corp-for-16-january-93CH-4451737",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/news_pile_108x81.jpg",
    "created_at": "2026-01-16T12:24:05.497Z",
    "topic": "finance"
  },
  {
    "slug": "task-versus-purpose-nvidia-ceo-jensen-huang-explains-why-ai-wont-kill-jobs",
    "title": "'Task' versus 'purpose': Nvidia CEO Jensen Huang explains why AI won't kill jobs.",
    "description": "AI may automate tasks, but your job's purpose may be immune from AI disruption. This applies to radiology, law, coding, and even waiting tables.",
    "fullText": "Hospitals, law firms, and tech companies are getting a preview of how AI is likely to reshape work: by automating tasks without eliminating the underlying jobs.\n\nThat's the core message Nvidia CEO Jensen Huang emphasized in a recent appearance on the No Priors podcast.\n\nIn a wide-ranging interview, he argued that fears of mass job destruction often confuse the \"tasks\" involved in a job with the broader \"purpose\" of the role. AI, in his view, changes how tasks get done, but the purpose remains the same. And that means, the technology probably won't destroy jobs and could even increase demand for the people responsible for outcomes at work.\n\nHuang's framing is straightforward: Most jobs contain repeatable tasks that technology can compress, and a broader purpose that remains human-led. He highlighted radiology as a real-world example.\n\nYears ago, AI pioneer Geoffrey Hinton predicted that AI would eradicate many radiology jobs and advised students to avoid the field. The opposite happened. While AI is automating many radiology tasks, there are actually more radiologists employed now than when Hinton made his prediction in 2016.\n\nHere are the killer stats, shared in this 2025 blog post that describes why radiologists are still in huge demand: In 2025, American diagnostic radiology residency programs offered a record 1,208 positions, a 4% increase from 2024, and the field's vacancy rates are at all-time highs. Also, in 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48% percent higher than the average radiologist salary in 2015 (the year before Hinton's prediction).\n\nHow did this happen? Huang argued that the job's purpose isn't \"reading scans.\" Those are tasks that AI has automated. The true purpose of a radiologist is to diagnose disease, guide treatment, and support those efforts with research. When AI helps clinicians evaluate more images with higher confidence, hospitals can serve more patients, generate more revenue, and justify hiring more specialists.\n\nThe same logic, he said, applies across the economy.\n\n\"I spend most of my day typing,\" Huang noted, describing typing as a task, not his job's purpose. Tools that automate writing don't eliminate the need for executives; they often expand the amount of work leaders and other employees can take on, he said.\n\n\"The fact that somebody could use AI to automate a lot of my typing — I really appreciate that, and it helps a lot,\" he said. \"It hasn't really made me, if you will, less busy. In a lot of ways, I become more busy because I'm able to do more work.\"\n\nThis \"task versus purpose\" framework is increasingly visible in knowledge work, where AI tools are speeding up and automating tasks such as drafting, summarizing, and generating code.\n\nHuang pointed to software engineering as a case where AI can reduce time spent on a core task (writing code) while raising demand for the job's purpose: solving problems and identifying new ones worth solving.\n\nNvidia, he said, is hiring aggressively even as AI coding tools such as Cursor spread through the company's engineering teams, because productivity gains allow companies to pursue more ideas. That can boost revenue, leaving more money to hire new staff.\n\nLaw is another example he cited. Reading and drafting contracts are tasks, while the purpose of a lawyer is to protect clients and resolve disputes. AI can accelerate document-heavy work, but the role's true value relies on judgment, strategy, and accountability — and you need experienced, trustworthy human attorneys for that.\n\nThis even applies to waiters working in a restaurant. Their task is taking food orders, but their purpose is to ensure guests have a great time, Huang said.\n\n\"If some AI is taking the order or even delivering the food, their job is still helping us have a great experience,\" the CEO added. \"They would reshape their jobs accordingly.\"\n\nHuang's argument isn't that AI won't disrupt roles — it will. But he contends the early evidence points less toward a wholesale collapse of employment and more toward job redesign.\n\nFor workers, the implication is pragmatic: if your role is defined primarily by a repeatable task, AI is a direct threat. If it's anchored in outcomes — diagnosis, customer experience, problem-solving, conflict resolution — AI may be less a replacement than a lever, changing what you spend time on while keeping your job's purpose intact.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "broader purpose",
      "job's purpose",
      "automating tasks",
      "jobs",
      "radiology",
      "demand",
      "typing",
      "tools",
      "less",
      "hospitals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/task-versus-purpose-nvidia-jensen-huang-ai-wont-kill-jobs-2026-1",
    "thumbnail_url": "https://i.insider.com/69684f72764ca5f34d2a7b11?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:04.259Z",
    "topic": "finance"
  },
  {
    "slug": "goldman-sachs-is-crushing-it",
    "title": "Goldman Sachs is crushing it",
    "description": "Strong earnings, rising shares, and a bet on AI are converging at a pivotal moment for Goldman Sachs after riding out ups and downs.",
    "fullText": "Goldman Sachs is acting like Goldman Sachs again.\n\nThe Wall Street giant is coming off one of its best years on record, its share price is through the roof, and its executives are downright giddy over its swelling deal pipeline. Equity trading is booming, and dealmaking is thriving ahead of what could be one of the biggest years for IPOs of all time, big boons for Goldman's bottom line.\n\nIf the bank had lost its luster, CEO David Solomon seems to have gotten it back. Goldman is once again Wall Street's top dog. It's a swift comeback for a firm that some declared dead as recently as 2023. The scars of an internal PR crisis and concerns over Solomon's DJ side hustle are long in the past. Goldman is on the attack again as it readies for a monster year.\n\n\"I think the world is set up at the moment to be incredibly constructive in 2026 for M&A and capital markets activity,\" Solomon told an analyst. \"The 2021 levels will be exceeded again. They might be exceeded in 2026.\"\n\nBeing on the right footing is all the more important as other banks are jockeying for some of the industry's hottest deals too. In a memo to her staff on Wednesday, Citi CEO Jane Fraser warned her staff against settling for second best.\n\n\"The animal spirits have been unleashed,\" veteran Wells Fargo bank analyst Mike Mayo told Business Insider last week, noting competition among the banks is now \"at its most intense level since before the global financial crisis.\"\n\nHere's how Goldman got its swagger back and what's still at stake.\n\nMuch of Solomon's vision that's playing out today dates back to a 2020 investor day just a few weeks before the coronavirus pandemic sent the world into a tailspin. At the presentation — which featured a number of then-Goldman officials who are no longer at the bank — its leaders teased two pursuits that would go on to shape its future: \"digitization\" and \"consumerization.\"\n\nNot everything went according to plan.\n\nIts consumer banking venture, Marcus, proved a costly distraction that fomented schisms among power brokers and sent some partners to the exits. Stephanie Cohen, the bank's former chief strategy officer and an architect of its consumer strategy, left the firm in 2024 to go to the tech sector.\n\nThe bank retreated from many of its consumer ambitions due to ballooning spending, but the Marcus brand lives on as a deposits platform within its combined asset and wealth management business. Marcus holds more than $100 billion in consumer deposits in the US and UK, contributing to more than $500 billion in deposits overall, the bank has said.\n\nThe bank also got closure on another Marcus venture this week. It has relinquished its Apple Card partnership to JPMorgan. A silver lining for the bank is that this transfer meant analysts didn't pepper Solomon with questions about President Donald Trump's 10% credit card interest proposal. Other CEOs had to address it point-blank during their earnings calls.\n\nAside from the consumer flop, other initiatives stuck. Solomon merged the asset and wealth businesses in late 2022. The gambit appears to have paid off — the business' assets under supervision climbed nearly half a trillion dollars to $3.6 trillion last year, an all-time high, alongside investments in private banking, alternatives, and wealth management in the high-net-worth category.\n\nAbout a year ago, Goldman formalized its Capital Solutions Group, coalescing its advisory, financing, structuring, and risk-management capabilities. The unit's primary objective is helping corporate clients navigate increasingly bespoke capital needs at a time when liquidity demands are skyrocketing.\n\nThe consumer banking misfire fueled multiple years of unrest from 2022 to 2024.\n\nAt times, the infighting spilled into public view, with some frustrated partners turning to reporters to get the CEO's attention. Some who worked directly with Solomon bristled over a management style they said could be abrasive and alienating; he, in turn, told veteran journalist Bethany McLean that internal leaks were \"damaging the firm.\" Those familiar with Goldman's closely guarded mystique were surprised at the spillover at the time, saying it conflicted with the bank's pristine image.\n\nTensions resurfaced almost two years ago, when questions about the advancement of women at Goldman were thrust to the center stage following a critical Wall Street Journal report. Business Insider later revealed talking points the firm circulated to managers to quell concerns from within its own walls, acknowledging that the issue had become a flashpoint.\n\nEven so, Solomon and his deputy, President and COO John Waldron, kept moving — and the bank continues to draw hopefuls at scale.\n\nMore than a million people applied for jobs at the bank last year, Solomon said, and its ultra-exclusive internship acceptance was less than 1%.\n\nWith the business stabilized, Goldman is focused on its next chapter, and chatter surrounding Solomon has died down. Last year, the bank awarded both Solomon and Waldron $80 million in restricted stock units to vest over five years, signaling it doesn't want the duo going anywhere.\n\nLast year, it acquired the ETF platform, Innovator; the VC firm Industry Ventures; and partnered with T. Rowe Price. In the fall, the firm rolled out One Goldman Sachs 3.0, an AI-driven overhaul of an internal operating model designed to unify business lines, cross-sell products, and foster company cohesion.\n\nIt's pouring billions into the development of technology — Solomon has said he wishes his $6 billion tech budget was higher — and deployment of artificial intelligence to supercharge the bank's workers and cut costs.\n\nBut the road ahead is never certain. Goldman has stumbled on high-profile tech initiatives before, and executives have acknowledged that changing how a 150-year-old institution works is neither quick nor easy. Pressed by Mayo for more specifics on what Goldman's AI investments will entail, Solomon sought to reassure listeners that the firm is taking a measured approach.\n\n\"This is not a new era for Goldman Sachs,\" he said. Goldman, he added, remains focused on its two core businesses — its global banking and markets franchise, and its asset and wealth management unit — with AI serving as an accelerant.\n\n\"It's not just to take cost out,\" Solomon said. \"It's to free up capacity to invest in other areas where we see growth opportunities that we've been a little bit constrained.\"\n\nAfter the ups and downs of the first half of the decade, Solomon is poised to enter the second half with the wind at his back and his hands held tightly to one of Wall Street's most feared and coveted thrones. Partners, shareholders, journalists, and rivals will be watching closely to see where he steers Goldman next.",
    "readingTime": 6,
    "keywords": [
      "wall street",
      "wealth management",
      "consumer banking",
      "goldman sachs",
      "wall street's",
      "business insider",
      "firm",
      "again",
      "back",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/goldman-sachs-david-solomon-record-year-q1-earnings-2026-1",
    "thumbnail_url": "https://i.insider.com/696958c804eda4732f2f2bda?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:04.081Z",
    "topic": "finance"
  },
  {
    "slug": "linkedin-cofounder-reid-hoffmans-goto-gift-this-christmas-was-an-aigenerated-music-album",
    "title": "LinkedIn cofounder Reid Hoffman's go-to gift this Christmas was an AI-generated music album",
    "description": "To create the Christmas music, Hoffman said he used two AI agents: one to write the lyrics and another to compose the music.",
    "fullText": "Reid Hoffman loves AI. So much so that, for Christmas, instead of fuzzy socks or wool sweaters, he gave his friends and family an AI music album.\n\nThe LinkedIn cofounder and Greylock partner, who Forbes estimates has a $2.5 billion net worth, recently told Wired he generated silly Christmas songs using AI and pressed them onto records.\n\n\"There's a song on ugly sweaters and all of this kinda stuff,\" he said. \"As opposed to the 'Holly, Jolly Christmas,' you know, something that actually has some humor. Almost like what 'Weird Al' Yankovic would do if he was doing a Christmas album.\"\n\nTo create the Christmas music, Hoffman said he used two different AI agents: one to write the lyrics and another to compose the music.\n\nIt's not clear which AI tool Hoffman used to generated the songs. His current firm, Greylock, doesn't list any of the major music-generating apps — like Suno, Udio, or AIVA — in its investment portfolio.\n\nBut, whichever tool he used, Hoffman said he was impressed by the result.\n\nHe said he told everyone who received the gift that it was AI, but when he played it for his wife, she couldn't tell it was computer-generated.\n\nThe Christmas surprise comes as Hoffman has been talking about AI while promoting a new book published with journalist Greg Beato titled \"Superagency: What Could Possibly Go Right with Our AI Future.\"\n\nIn it, the two argue that AI doesn't need to be a dystopian technology destined to displace workers or lead to human extinction, as some more pessimistic about the technology have warned.\n\nHoffman argues that AI skeptics are falling into the same trap that has gripped tech cynics in the past, including existential complaints during the rollouts of the printing press, electricity, and the internet.\n\n\"My push for people is if you are not using AI in a way today that isn't seriously helpful to you, you are not actually trying hard enough,\" he told Wired. \"Now, of course it'll transform jobs, and there'll be a bunch of pain in that transformation. But the way that you as an individual can avoid that is to be engaged.\"",
    "readingTime": 2,
    "keywords": [
      "music",
      "sweaters",
      "album",
      "greylock",
      "generated",
      "songs",
      "tool",
      "doesn't",
      "technology",
      "hoffman"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/reid-hoffman-christmas-gift-ai-music-album-2026-1",
    "thumbnail_url": "https://i.insider.com/69681875764ca5f34d2a79bd?width=1024&format=jpeg",
    "created_at": "2026-01-16T12:24:04.057Z",
    "topic": "entertainment"
  },
  {
    "slug": "ruby-on-rails-creator-david-heinemeier-hansson-says-ai-cant-yet-equal-most-junior-programmers-its-why-he-still-mostly",
    "title": "Ruby on Rails creator David Heinemeier Hansson says AI can't yet equal most junior programmers. It's why he still mostly codes by hand.",
    "description": "Tech entrepreneur David Heinemeier Hansson said AI isn't going away, but it's not quite at the level it needs to be to bring about major changes.",
    "fullText": "Tech entrepreneur David Heinemeier Hansson says he's hopeful about AI in the long term, but right now, he's just not seeing enough to meaningfully change his approach.\n\n\"I feel like it's a little bit like a flickering light bulb,\" Hansson said during a recent episode of \"Next Token.\" \"You're in total darkness and then it'll flicker on and you go like, 'I can see everything.' And then two seconds later, boom, pitch black.\"\n\nHansson said when he asked AI to write lines of code, the output is not \"as good as a job as most junior programmers I've had to deal with.\"\n\n\"I'm not feeling that we're falling behind at 37 Signals in terms of our ability to produce, in terms of our ability to launch things or improve the products,\" he said. \"And this is why I'm a little skeptical about the claims that AI is already now at a place where you can take your standard SaaS business and then fire half the programmers and still go faster.\"\n\nHumans coded 95% of the code for Fizzy, 37 Signals' Kanban-inspired organization product, Hansson said. They've experienced with AI-powered features for the product, but for now, they've ended up on the cutting room floor.\n\nHansson said he doesn't see his business losing ground because he's focusing on writing his own code. For the time being, he can still \"care about the beauty of it.\"\n\n\"That's a luxury that perhaps is akin to what a modern saddle maker enjoys when you go, \"Oh, the letter's just right, and the stitching is just right.' And you're like, 'Okay, but you're no longer part of the main production for transportation,'\" he said. \"And I'm like, 'Well, so what?' I'll continue to make my handwritten code saddles as long as I can for my enjoyment.\"\n\nAlternatively, Hansson said he's seen the promise of AI in the type of tools Shopify uses, where he serves on the board.\n\n\"Some of the things that they've done with SiteKick, the AI agent they have helping merchants set up their shop and optimize their shop is truly incredible,\" he said. \"And there's some real tangible benefits in that that I think are undisputable.\"\n\nOverall, Hansson said he's excited about the promise of a new era that could upend development, just as the iPhone's 2007 release ushered in a focus on mobile optimization.\n\n\"This is the first one where we, in real time, know the world is going to look totally different, and we don't know what the final result is going to be,\" he said. \"So I think the best thing you can just do is accept that A, we don't know, B, that it's really exciting, and C, just try to hold onto your hat, jump on there cowboy and ride it and see where it goes.\"\n\nHe also said AI isn't going away, no matter what some people may hope.\n\n\"That power does not exist. You don't get to roll these things back. Now, you could personally choose that you're not going to use gen AI,\" he said, adding that some people may also try to avoid buying products that were aided by AI.\n\nHansson said he's amazed by just how much of the US economy is based on large bets that AI will break through its current limitations and reach something akin to Artificial General Intelligence, the theoretical moment where AI reaches capabilities akin to humans.\n\n\"The entire American economy right now is one big bet that that's going to happen,\" he said. \"And I, as AI positive as I am, still marvel at that. The conviction of an entire economy to just combined go like 'Whatever takes 100 trillion, a thousand trillion. I don't care. We will get there.' I go like, 'That's why America is fucking number one.'\"",
    "readingTime": 4,
    "keywords": [
      "he's",
      "code",
      "don't",
      "they've",
      "that's",
      "akin",
      "economy",
      "hansson",
      "it's",
      "programmers"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ruby-on-rails-creator-ai-coding-development-future-2026-1",
    "thumbnail_url": "https://i.insider.com/69691ac1764ca5f34d2a8355?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:04.046Z",
    "topic": "finance"
  },
  {
    "slug": "a-leading-european-ai-startup-says-its-edge-over-silicon-valley-isnt-better-tech-its-not-being-american",
    "title": "A leading European AI startup says its edge over Silicon Valley isn't better tech — it's not being American",
    "description": "Mistral CEO Arthur Mensch said European governments and firms want AI they can control, deploy locally, and run independently of US tech giants.",
    "fullText": "As the race to dominate AI accelerates, Europe's most prominent AI startup is betting that geography — not just technology — can be a competitive advantage in its home market.\n\nArthur Mensch, the CEO and cofounder of French AI company Mistral, said the company's edge in Europe over Silicon Valley rivals like OpenAI, Google, and Anthropic isn't about having dramatically smarter models.\n\nInstead, he said that many European governments and regulated enterprises are seeking AI systems they can control, customize, and operate independently, rather than relying on a small number of external providers.\n\n\"European governments are coming to us because they want to build the technology and they want to serve their citizens,\" Mensch said on the \"Big Technology Podcast\" on Wednesday.\n\nMistral, founded in 2023 and now valued at roughly $14 billion, develops large language models that rival those of leading US systems.\n\nBut Mensch said that frontier AI models are rapidly converging in performance as research spreads and training techniques become widely available.\n\nAs a result, the real battleground is shifting away from raw intelligence and toward deployment, control, and trust — a shift that plays directly into Mistral's pitch in Europe.\n\nMensch said governments, banks, and heavily regulated industries want AI systems they can customize, deploy locally, and operate independently — without fear that a single vendor could change the rules or shut off access.\n\nThe approach has already paid off. France's military recently selected Mistral for an AI deal that keeps sensitive systems running on French-controlled infrastructure.\n\nMensch pushed back on the idea that the company benefits merely from EU regulation or protectionism.\n\nInstead, he framed the demand as geopolitical and operational.\n\nEuropean governments, he said, want AI that they can govern themselves and use to serve citizens without depending on foreign platforms.\n\nThe same logic applies to regulated enterprises that need tighter control over data, compliance, and security.\n\nMistral's embrace of open-source models is central to that strategy.\n\nOpen source allows customers to run AI on their own infrastructure, build redundancy, and avoid vendor lock-in — a sharp contrast to the closed, centralized platforms favored by many US firms.\n\nThe appeal isn't limited to Europe. Mensch said Mistral also works with US and Asian customers who want to reduce dependence on a small group of American providers and retain more autonomy over how AI is used inside their organizations.\n\nThat approach is already extending beyond the West. Mistral recently deepened a partnership with Morocco's government to co-build locally tailored AI models and launch a joint research and development lab aimed at strengthening the country's technological autonomy.\n\nLong term, Mensch said he doesn't believe AI will be dominated by a single winner or country. Instead, he expects multiple regional centers of expertise shaped by local needs, industries, and political realities.\n\nIn that future, he suggested, Mistral's biggest advantage may not be the models it builds — but where, and how, it builds them.\n\nDo you work for Mistral and have a tip or story to share? Contact this reporter via email at tspirlet@businessinsider.com or Signal at thibaultspirlet.40. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "european governments",
      "operate independently",
      "regulated enterprises",
      "europe mensch",
      "models",
      "systems",
      "instead",
      "mistral's",
      "advantage",
      "isn't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/europe-ai-startup-mistral-edge-over-silicon-valley-not-american-2026-1",
    "thumbnail_url": "https://i.insider.com/6968c520764ca5f34d2a7ce5?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:03.918Z",
    "topic": "finance"
  },
  {
    "slug": "ciscos-hr-chief-says-ai-and-ml-roles-are-really-hard-to-fill-getting-execs-on-the-phone-helps",
    "title": "Cisco's HR chief says AI and ML roles are 'really hard' to fill. Getting execs on the phone helps.",
    "description": "Cisco's chief people officer said when the company finds top-tier talent, executives like Jeetu Patel call them up.",
    "fullText": "Specialized AI talent can be hard to come by — and companies are jockeying to scoop them up.\n\nIn the midst of an ongoing battle for elite talent, Cisco's chief people officer says that AI and machine-learning operations positions are \"really hard\" to fill at the company.\n\n\"The qualified pool is so small, and the demand is so high,\" Jones said.\n\nPart of that is because \"everyone\" is currently hiring for those positions right now. She said that every forward-thinking organization needs AI in its products and also in its internal IT teams.\n\nThose jobs also require a \"slightly different level of specialization,\" she said. AI and ML Ops roles typically involve automating and streamlining IT operations, the company said. The strongest candidates are those with experience working in AI, ML, and operations, and those who have a track record of developing successful products.\n\n\"We're looking for people who have done this, who have been there, and have a good story to tell,\" Jones said, adding that she wants to know about what they built.\n\nAs the AI talent wars continue, Jones said that one factor that makes a \"really big difference\" is having leaders at the company talk directly to talent. She said that shouldn't reflect negatively on recruiters, but in the current landscape, there's \"a lot of AI washing going on,\" which makes it hard to differentiate the real wins.\n\n\"Companies are talking about all these things they're doing and how they're slapping AI on everything,\" Jones said. \"But they're actually not doing interesting things with work or with products.\"\n\nShe said mobilizing executives like Cisco's president and chief product officer, Jeetu Patel, has helped score some of those top candidates. She said when the company has a strong AI candidate, he'll pick up the phone and give them a call.\n\n\"If there's someone that we are really strategically trying to hire, we will get him involved,\" Jones said.\n\nPatel, whom the company poached from Box in 2020, understands the connection between having the best people design products and the success of those products, Jones said.\n\nCisco isn't the only company to use this technique. Citadel CTO Umesh Subramanian recently told Business Insider that he personally calls top entry-level candidates during the hiring process to talk through their decision-making.\n\nOpenAI's Sam Altman has also reportedly called candidates to convince them to join the AI company, and Meta CEO Mark Zuckerberg has previously appeared in email chains to recruit talent and has also reportedly hosted top candidates at his home for meals.\n\nWhat qualifies a candidate to get a phone call from Cisco's Patel?\n\nAside from \"top-tier\" technical skills and experience, Jones assesses for behavior and potential when evaluating top talent. She said, \"It's not just about skills,\" because those can be learned, but also about the leadership characteristics an applicant has.\n\n\"We want to know, not only have you done these things, but are you intellectually curious? Are you intellectually and emotionally agile? Are you someone who brings your team along with you?\" Jones said.\n\nJones said that the company also has to put its efforts toward \"naturally\" attracting talent both online and in person by creating a presence in the tech community. She said that includes having senior leaders connect with talent at industry events and building community through channels like its Tech Pulse newsletter, which highlights Cisco's leadership in the AI era.\n\nIt also involves leaders sharing career opportunities on LinkedIn and participating in events with entry talent recruiting to engage with the next generation of AI professionals, the company said.\n\n\"We've tried to insert ourselves into not just your typical talent branding, but going where that talent lives — in the forums that they're on, in the hobbies that they have, to ensure that we have a presence there,\" Jones said.",
    "readingTime": 4,
    "keywords": [
      "top candidates",
      "talent",
      "jones",
      "products",
      "they're",
      "operations",
      "leaders",
      "chief",
      "officer",
      "positions"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/cisco-hr-says-ai-ml-roles-hard-to-fill-2026-1",
    "thumbnail_url": "https://i.insider.com/6967d82b64858d02d2185809?width=1000&format=jpeg",
    "created_at": "2026-01-16T12:24:03.904Z",
    "topic": "finance"
  },
  {
    "slug": "running-factorio-from-over-1k-floppy-disks",
    "title": "Running Factorio from over 1k floppy disks",
    "description": "A masochistically manual gaming experience for the automated AI age.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.pcgamer.com/hardware/running-factorio-from-over-1-000-floppy-disks-is-a-masochistically-manual-process-that-surely-sets-a-new-record-for-game-load-times/",
    "thumbnail_url": "https://cdn.mos.cms.futurecdn.net/QELXzshUxkatdnBAJj7NZK-1920-80.jpg",
    "created_at": "2026-01-16T06:20:14.082Z",
    "topic": "tech"
  },
  {
    "slug": "4-semiconductor-stocks-giving-the-ai-boom-more-legs",
    "title": "4 semiconductor stocks giving the AI boom 'more legs'",
    "description": "Crossmark Global Investments chief market strategist Victoria Fernandez explains why a new wave of earnings from the semiconductor sector signals the AI race is just warming up.",
    "fullText": "The AI supercycle is far from exhausted, and semiconductor companies like industry titan TSMC (TSM) are among the catalysts to watch.\n\n\"This earnings report, really more than anything, supports the idea that [the AI boom] is going to continue to move forward,\" Victoria Fernandez, chief market strategist at Crossmark Global Investments, told Yahoo Finance's Opening Bid.\n\nAccording to Fernandez, the great \"rotation\" out of Big Tech might finally \"start to slow down a little bit.\" While market skeptics spent the past quarter betting on a broad exit from the \"Magnificent Seven,\" a new wave of earnings from the semiconductor sector signals \"there are more legs\" to the AI race.\n\nOther key players to watch include ASML (ASML), Applied Materials (AMAT), and Lam Research (LRCX), as Fernandez points to the firms' respective \"strong earnings.\"\n\nThat renewed optimism is largely due to a fundamental shift in how the market views the picks and shovels of the AI era. For months, investors feared that AI demand might be a bubble nearing its burst. However, the latest data from these four companies — which build the hardware for Big Tech — tells a different story of rising capital expenditures, expanding margins, and relentless sales growth.\n\nLeading that charge is TSMC, the world's most critical chip foundry, which dominates advanced chip production for giants like Nvidia (NVDA), Apple (AAPL), and Advanced Micro Devices (AMD).\n\nTSMC kicked off the new year by delivering blockbuster Q4 earnings today. For the period, revenue came in at $33.73 billion, a 25% year-over-year increase, beating consensus estimates of $32.8 billion, according to Bloomberg data. Earnings per share reached $3.15, up 8% year over year, surpassing the expected $2.90. TSMC stock has risen over 70% in the past year.\n\nThe company announced it had earmarked up to $56 billion in capital expenditures for 2026. This aggressive spending plan acts as a massive \"buy\" signal for the entire supply chain, per Fernandez, suggesting that the world's biggest tech players are doubling down on infrastructure, rather than pulling back.\n\nThe higher-for-longer spending cycle has revitalized the three other key suppliers. Take ASML, the sole provider of the lithography machines required to make advanced AI chips. Its market cap surged past the $500 billion mark on the back of TSMC's news. Shares of ASML are up over 80% in the past 12 months.\n\nThe support for these suppliers was visible in premarket trading, where Applied Materials and Lam Research shares were up 8% and 6%, respectively. These companies represent the infrastructure layer that must be built before any software-based AI gains can be realized.",
    "readingTime": 3,
    "keywords": [
      "applied materials",
      "lam research",
      "capital expenditures",
      "big tech",
      "earnings",
      "market",
      "advanced",
      "semiconductor",
      "watch",
      "players"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/4-semiconductor-stocks-giving-the-ai-boom-more-legs-175135867.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/FLT8Ybbe7hoFPOOeErxKDA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2026-01/d7e90840-f234-11f0-9edf-ed1aed6232e9",
    "created_at": "2026-01-16T06:20:09.316Z",
    "topic": "finance"
  },
  {
    "slug": "ashley-st-clair-sues-elon-musks-xai-over-alleged-explicit-grok-deepfake-images",
    "title": "Ashley St. Clair sues Elon Musk's xAI over alleged explicit Grok deepfake images",
    "description": "Ashley St. Clair is suing Elon Musk's AI startup, alleging the chatbot Grok generated explicit photos of her.",
    "fullText": "Ashley St. Clair, who gave birth to one of Elon Musk's sons in 2024, sued Musk's xAI in a New York court on Thursday, alleging that its chatbot Grok generated sexually explicit deepfake images of her at users' request.\n\nIn the complaint, St. Clair, a writer, influencer, and political strategist, claims X users prompted Grok to manipulate images of her, including photos from when she was 14, into graphic sexual content. She alleges some images remained online for more than a week and that her premium X account was later terminated after she complained.\n\nShe is also requesting a temporary restraining order to compel xAI to immediately cease from \"the intentional disclosure of nonconsensual intimate images.\"\n\nxAI did not immediately respond to a request for comment.\n\n\"Grok first promised Ms. St. Clair that it would refrain from manufacturing more images unclothing her,\" the complaint read. \"Instead, Defendant retaliated against her, demonetizing her X account and generating multitudes more images of her,\" the suit alleged.\n\nSt. Clair is also involved in a separate suit with Musk over the custody of their son, in which she sought sole custody.\n\nxAI responded the same day with a separate lawsuit, arguing that St. Clair agreed to its terms of service, which requires any litigation to be heard in Texas. St. Clair is represented by attorney Carrie Goldberg, who specializes in cases involving abuse and has represented clients against Harvey Weinstein.\n\n\"xAI is not a reasonably safe product,\" Goldberg said in a statement to Business Insider. \"This harm flowed directly from deliberate design choices that enabled Grok to be used as a tool of harassment and humiliation. Companies should not be able to escape responsibility when the products they build predictably cause this kind of harm.\"\n\nThe lawsuit followed international backlash against the Grok chatbot for its ability to undress images of real people and create sexualized images without their consent at users' request.\n\nIndonesia and Malaysia blocked access to Grok, while UK Prime Minister Keir Starmer called explicit images generated by Grok \"disgusting\" and \"shameful\" in a meeting with the House of Commons.\n\nOn Wednesday, California Attorney General Rob Bonta also announced that his office is investigating the \"non-consensual, sexually explicit material that xAI has produced and posted online\" of \"women and children in nude and sexually explicit situations.\"\n\nX said on the same day in a blog post that users would no longer be allowed to create AI photos of real people in sexualized or revealing clothing on the platform, adding that the restriction \"applies to all users, including paid subscribers.\"\n\nAs of Thursday morning, Business Insider reporter Henry Chandonnet found that it is still \"surprisingly easy\" to prompt Grok to create nude images of him by going to the app itself instead of using the Grok chatbot on X.",
    "readingTime": 3,
    "keywords": [
      "grok chatbot",
      "sexually explicit",
      "users request",
      "st clair",
      "images",
      "create",
      "generated",
      "complaint",
      "photos",
      "online"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ashley-st-clair-sues-musks-xai-alleged-explicit-grok-images-2026-1",
    "thumbnail_url": "https://i.insider.com/69696bb364858d02d21874fd?width=1200&format=jpeg",
    "created_at": "2026-01-16T06:20:08.396Z",
    "topic": "finance"
  },
  {
    "slug": "gambit-an-opensource-agent-harness-for-building-reliable-ai-agents",
    "title": "Gambit, an open-source agent harness for building reliable AI agents",
    "description": "Agent harness framework for building, running, and verifying LLM workflows - bolt-foundry/gambit",
    "fullText": "bolt-foundry\n\n /\n\n gambit\n\n Public\n\n Agent harness framework for building, running, and verifying LLM workflows\n\n License\n\n Apache-2.0 license\n\n 11\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n bolt-foundry/gambit",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/bolt-foundry/gambit",
    "thumbnail_url": "https://opengraph.githubassets.com/829bfeef02de4bbd24ea930c13f45a8bb6d0af99b1c5cb3f52a6a4e50b6b9630/bolt-foundry/gambit",
    "created_at": "2026-01-16T00:58:36.926Z",
    "topic": "tech"
  },
  {
    "slug": "everything-becomes-an-agent",
    "title": "Everything Becomes an Agent",
    "description": "Explore the inevitable shift from scripts to AI agents in coding. Discover insights on automation, tool access, and setting effective guardrails.",
    "fullText": "I’ve noticed a pattern in my coding life. It starts innocently enough. I sit down to write a simple Python script, maybe something to tidy up my Obsidian vault or a quick CLI tool to query an API. “Keep it simple,” I tell myself. “Just input, processing, output.”\n\nBut then, the inevitable thought creeps in: It would be cool if the model could decide which file to read based on the user’s question.\n\nTwo hours later, I’m not writing a script anymore. I’m writing a while loop. I’m defining a tools array. I’m parsing JSON outputs and handing them back to the model. I’m building memory context windows.\n\n(For those keeping track: my working definition of an “agent” is simple: a model running in a loop with access to tools. I explored this in depth in my Agentic Shift series, but that’s the core of it.)\n\nAs I sit here writing this in January of 2026, I realize that almost every AI project I worked on last year ultimately became an agent. It feels like a law of nature: Every AI project, given enough time, converges on becoming an agent. In this post, I want to share some of what I’ve learned, and the cases where you might skip the intermediate steps and jump straight to building an agent.\n\nThis isn’t just feature creep. It’s a fundamental shift in how we interact with software. We are moving past the era of “smart typewriters” and into the era of “digital interns.”\n\nTake Gemini Scribe, my plugin for Obsidian. When I started, it was a glorified chat window. You typed a prompt, it gave you text. Simple. But as I used it, the friction became obvious. If I wanted Scribe to use another note as context for a task, I had to take a specific action, usually creating a link to that note from the one I was working on, to make sure it was considered. I was managing the model’s context manually.\n\nI was the “glue” code. I was the context manager.\n\nThe moment I gave Scribe access to the read_file tool, the dynamic changed. Suddenly, I wasn’t micromanaging context; I was giving instructions. “Read the last three meeting notes and draft a summary.” That’s not a chat interaction; that’s a delegation. And to support delegation, the software had to become an agent, capable of planning, executing, and iterating.\n\nThe Gemini CLI followed a similar arc. There were many of us on the team experimenting with Gemini on the command line. I was working on iterative refinement, where the model would ask clarifying questions to create deeper artifacts. Others were building the first agentic loops, giving the model the ability to run shell commands.\n\nOnce we saw how much the model could do with even basic tools, we were hooked. Suddenly, it wasn’t just talking about code; it was writing and executing it. It could run tests, see the failure, edit the file, and run the tests again. It was eye-opening how much we could get done as a small team.\n\nBut with great power comes great anxiety. As I explored in my Agentic Shift post on building guardrails and later in my post about the Policy Engine, I found myself staring at a blinking cursor, terrified that my helpful assistant might accidentally rm -rf my project.\n\nThis is the hallmark of the agentic shift: you stop worrying about syntax errors and start worrying about judgment errors. We had to build a “sudoers” file for our AI, a permission system that distinguishes between “read-only exploration” and “destructive action.” You don’t build policy engines for scripts; you build them for agents.\n\nLast year, I learned to recognize a specific code smell: the AI classifier.\n\nIn my Podcast RAG project, I wanted users to search across both podcast descriptions and episode transcripts. Different databases, different queries. So I did what felt natural: I built a small classifier using Gemini Flash Lite. It would analyze the user’s question and decide: “Is this a description search or a transcript search?” Then it would call the appropriate function.\n\nIt worked. But something nagged at me. I had written a classifier to make a decision that a model is already good at making. Worse, the classifier was brittle. What if the user wanted both? What if their intent was ambiguous? I was encoding my assumptions about user behavior into branching logic, and those assumptions were going to be wrong eventually.\n\nThe fix was almost embarrassingly simple. I deleted the classifier and gave the agent two tools: search_descriptions and search_episodes. Now, when a user asks a question, the agent decides which tool (or tools) to use. It can search descriptions first, realize it needs more detail, and then dive into transcripts. It can do both in parallel. It makes the call in context, not based on my pre-programmed heuristics. (You can try it yourself at podcasts.hutchison.org.)\n\nI saw the same pattern in Gemini Scribe. Early versions had elaborate logic for context harvesting, code that tried to predict which notes the user would need based on their current document and conversation history. I was building a decision tree for context, and it was getting unwieldy.\n\nWhen I moved Scribe to a proper agentic architecture, most of that logic evaporated. The agent didn’t need me to pre-fetch context; it could use a read_file tool to grab what it needed, when it needed it. The complex anticipation logic was replaced by simple, reactive tool calls. The application got simpler and more capable at the same time.\n\nHere’s the heuristic I’ve landed on: If you’re writing if/else logic to decide what the AI should do, you might be building a classifier that wants to be an agent. Deconstruct those branches into tools, give the agent really good descriptions of what those tools can do, and then let the model choose its own adventure.\n\nYou might be thinking: “What about routing queries to different models? Surely a classifier makes sense there.” I’m not so sure anymore. Even model routing starts to look like an orchestration problem, and a lightweight orchestrator with tools for accessing different models gives you the same flexibility without the brittleness. The question isn’t whether an agent can make the decision better than your code. It’s whether the agent, with access to the actual data in the moment, can make a decision at least as good as what you’re trying to predict when you’re writing the code. The agent has context you don’t have at development time.\n\nWe are transitioning from Human-in-the-Loop (where we manually approve every step) to Human-on-the-Loop (where we set the goals and guardrails, but let the system drive).\n\nThis shift is driven by a simple desire: we want partners, not just tools. As I wrote back in April about waiting for a true AI coding partner, a tool requires your constant attention. A hammer does nothing unless you swing it. But an agent? An agent can work while you sleep.\n\nThis freedom comes with a new responsibility: clarity. If your agent is going to work overnight, you need to make sure it’s working on something productive. You need to be precise about the goal, explicit about the boundaries, and thoughtful about what happens when things go wrong. Without the right guardrails, an agent can get stuck waiting for your input, and you’ll lose that time. Or worse, it can get sidetracked and spend hours on something that wasn’t what you intended.\n\nThe goal isn’t to remove the human entirely. It’s to move us from the execution layer to the supervision layer. We set the destination and the boundaries; the agent figures out the route. But we have to set those boundaries well.\n\nHere’s the counterintuitive thing: building an agent isn’t always harder than building a script. Yes, you have to think about loops, tool definitions, and context window management. But as my classifier example showed, an agentic architecture can actually delete complexity. All that brittle branching logic, all those edge cases I was trying to anticipate: gone. Replaced by a model that can reason about what it needs in the moment.\n\nThe real complexity isn’t in the code; it’s in the trust. You have to get comfortable with a system that makes decisions you didn’t explicitly program. That’s a different kind of engineering challenge, less about syntax, more about guardrails and judgment.\n\nBut the payoff is a system that grows with you. A script does exactly what you wrote it to do, forever. An agent does what you ask it to do, and sometimes finds better ways to do it than you’d considered.\n\nSo, if you find yourself staring at your “simple script” and wondering if you should give it a tools definition… just give in. You’re building an agent. It’s inevitable. You might as well enjoy the company.",
    "readingTime": 8,
    "keywords": [
      "gemini scribe",
      "code it’s",
      "branching logic",
      "agentic architecture",
      "read_file tool",
      "agentic shift",
      "context",
      "model",
      "tools",
      "classifier"
    ],
    "qualityScore": 1,
    "link": "https://allen.hutchison.org/2026/01/15/everything-becomes-an-agent/",
    "thumbnail_url": "https://jetpack.com/redirect/?source=sigenerate&query=t%3DeyJpbWciOiJodHRwczpcL1wvYWxsZW4uaHV0Y2hpc29uLm9yZ1wvd3AtY29udGVudFwvdXBsb2Fkc1wvMjAyNlwvMDFcL0dlbWluaV9HZW5lcmF0ZWRfSW1hZ2VfOG9iYm5sOG9iYm5sOG9iYi0xMDI0eDU1OS5wbmciLCJ0eHQiOiJFdmVyeXRoaW5nIEJlY29tZXMgYW4gQWdlbnQiLCJ0ZW1wbGF0ZSI6ImhpZ2h3YXkiLCJmb250IjoiIiwiYmxvZ19pZCI6NTI2NDF9.clrckN4D9nLjT29SGR-zSduRSVw6yptl6Uby6fXr7VwMQ",
    "created_at": "2026-01-16T00:58:36.450Z",
    "topic": "tech"
  },
  {
    "slug": "student-arrested-for-eating-ai-art-in-uaf-gallery-protest",
    "title": "Student arrested for eating AI art in UAF gallery protest",
    "description": "On Tuesday, January 13, University of Alaska Fairbanks undergraduate student Graham Granger was detained after he had been found “ripping artwork off the walls and eating it in a reported protest,” according to the UAF police department. Granger was chewing and spitting out images pinned to the wall",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.uafsunstar.com/news/student-eats-ai-art-in-uaf-gallery-protest-arrested",
    "thumbnail_url": "http://static1.squarespace.com/static/6318d7641c199624396c32b4/6336308ae9b692756d600aa6/69689e09aae6c63e564e0141/1768490880218/IMG_6371.JPG?format=1500w",
    "created_at": "2026-01-16T00:58:35.469Z",
    "topic": "tech"
  },
  {
    "slug": "china-drafting-purchase-rules-for-nvidia-h200-chips-nikkei-asia-reports",
    "title": "China drafting purchase rules for Nvidia H200 chips, Nikkei Asia reports",
    "description": "China is working to set rules on how many advanced artificial ​intelligence chip companies can buy from foreign ‌makers such as Nvidia, Nikkei Asia reported on Thursday, citing ‌two people familiar with the matter.  The Chinese central government is working on rules that will likely regulate the total volume of cutting-edge AI chips ⁠local companies can ‌purchase, effectively allowing some sales by Nvidia instead of banning them outright, the ‍report added.  Nvidia declined to comment.",
    "fullText": "Jan 15 (Reuters) - China is working to set rules on how many advanced artificial ​intelligence chip companies can buy from foreign ‌makers such as Nvidia (NVDA), Nikkei Asia reported on Thursday, citing ‌two people familiar with the matter.\n\nThe Chinese central government is working on rules that will likely regulate the total volume of cutting-edge AI chips ⁠local companies can ‌purchase, effectively allowing some sales by Nvidia instead of banning them outright, the ‍report added.\n\nThis follows the Trump administration's decision on ​Tuesday to give a formal green light to ‌the sale of U.S.-based Nvidia's H200 chips to China.\n\nU.S. lawmakers and former officials questioned Trump's decision on Wednesday, arguing that the move erodes America's AI edge and threatens to electrify Beijing's ⁠military.\n\nThe ⁠Chinese government summoned domestic technology companies to meet where they were explicitly instructed not to ⁠purchase the chips unless necessary, sources told Reuters.",
    "readingTime": 1,
    "keywords": [
      "the chinese",
      "chips",
      "rules",
      "purchase",
      "decision",
      "reuters",
      "china",
      "nvidia"
    ],
    "qualityScore": 0.65,
    "link": "https://finance.yahoo.com/news/china-drafting-purchase-rules-nvidia-032124261.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/2d298e5494e3349bc11075aafdc3e5e1",
    "created_at": "2026-01-16T00:58:33.291Z",
    "topic": "finance"
  },
  {
    "slug": "aviator-yc-s21-is-hiring-to-build-multiplayer-ai-coding-platform",
    "title": "Aviator (YC S21) is hiring to build multiplayer AI coding platform",
    "description": "Jobs at Aviator",
    "fullText": "Software engineering is being fundamentally transformed by AI, and we're building the tools to lead that shift. Aviator is creating the engineering productivity supertools that will define how the best teams build software in the AI era.\n\nOur platform already powers workflow automation at Slack, Figma, DoorDash, and other industry leaders. MergeQueue eliminates merge conflicts and broken builds. FlexReview intelligently routes code reviews. And Runbooks—our newest product—is a collaborative AI agent platform that lets engineering teams automate complex workflows through natural language specs and shared context.\n\nWe believe the future of software development isn't engineers replaced by AI—it's engineers supercharged by it. Small teams will ship what once required hundreds of people. Complex workflows that took days will complete in minutes. We're building that future.",
    "readingTime": 1,
    "keywords": [
      "complex workflows",
      "software",
      "engineering",
      "teams",
      "we're",
      "platform",
      "engineers"
    ],
    "qualityScore": 0.65,
    "link": "https://www.ycombinator.com/companies/aviator/jobs",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/92093d419e958ee69c7f233b3b03a172ff20f5d1.png?1652822764",
    "created_at": "2026-01-16T00:58:30.154Z",
    "topic": "jobs"
  },
  {
    "slug": "meta-is-shutting-down-its-popular-supernatural-vr-fitness-app",
    "title": "Meta Is Shutting Down Its Popular ‘Supernatural’ VR Fitness App",
    "description": "And laying off 1,500 people as it pivots from VR to AI wearables.",
    "fullText": "Users of Supernatural got an unpleasant surprise this week: Meta has pulled the plug on its flagship virtual reality fitness app. Citing \"organizational changes,\" Meta says it will no longer release new content or update features for Supernatural.\n\nThe app is not shutting down completely however. Subscribers can still access Supernatural's existing library of Beat Saber-workouts, and Meta says it will maintain the platform and Facebook page, but no new workouts, features, or other content is planned.\n\nBoth users and critics have nearly universally praised Supernatural—CNet scored it 9 out of 10, it won both Fast Company's Best App award in 2020 and a Webby in 2023, and boasted celebrity tie-ins with Jane Fonda and Bon Jovi. Meta doesn't publish subscriber numbers for Supernatural, but there are over 110,000 members of Supernatural's Facebook community. Not enough, apparently, to warrant keeping the app going.\n\nIn 2021, Meta spent an estimated $400 million to purchase Within, Supernatural's developer, even battling the FTC to make the deal, and the app was a heavily promoted part of the company's overall \"Metaverse\" strategy.\n\nThe shuttering of Supernatural is part of a larger shift at Meta. This week, the company laid off 1,500 people—about 10% of the staff—from Reality Labs, Meta's hardware and virtual reality division. “We said last month that we were shifting some of our investment from Metaverse toward Wearables. This is part of that effort,” a Meta spokesman told The Wall Street Journal.\n\nAlong with cuts at Supernatural, Meta is closing three studios behind some of the most prominent, high-end VR games: Armature, who brought Resident Evil 4 to VR, Sanzaru, the studio behind Asgard’s Wrath, and Twisted Pixel, creators of Deadpool VR.",
    "readingTime": 2,
    "keywords": [
      "virtual reality",
      "meta",
      "content",
      "features",
      "metaverse",
      "behind",
      "supernatural",
      "supernatural's",
      "users",
      "facebook"
    ],
    "qualityScore": 0.85,
    "link": "https://lifehacker.com/tech/meta-is-shutting-down-popular-supernatural-vr-fitness-app?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KF1JJR642SNQFRRNE85CVNWP/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-16T00:58:29.507Z",
    "topic": "tech"
  },
  {
    "slug": "suttons-predictions-v-wwe-superstar-drew-mcintyre",
    "title": "Sutton's predictions v WWE superstar Drew McIntyre",
    "description": "BBC Sport football expert Chris Sutton takes on WWE superstar Drew McIntyre - and AI - with his predictions for this week's Premier League fixtures.",
    "fullText": "BBC Sport football expert Chris Sutton already has enough on his plate trying to get the better of AI at predictions, but now he has a WWE superstar on his case too.\n\nNewly crowned WWE world champion Drew McIntyre, a Rangers fan, has clashed with former Celtic striker Sutton in the past over their Old Firm allegiances.\n\nWe would love to see them meet in the wrestling ring but, for now, they will attempt to outdo each other - as well as the BBC readers and AI - by trying to pick the right results for the weekend's 10 Premier League matches.\n\nDrew will make his predictions on Friday and we'll bring them to you. Do you agree with Sutton's scores? You can pick your own below.\n\nThe most popular scoreline selected for each game is used in the scoreboards and tables at the bottom of this page.\n\nA correct result (picking a win, draw or defeat) is worth 10 points. The exact score earns 40 points.",
    "readingTime": 1,
    "keywords": [
      "predictions",
      "pick",
      "points",
      "sutton",
      "drew"
    ],
    "qualityScore": 0.65,
    "link": "https://www.bbc.com/sport/football/articles/c1dkg0d9z3do?at_medium=RSS&at_campaign=rss",
    "thumbnail_url": "https://ichef.bbci.co.uk/ace/branded_sport/1200/cpsprodpb/ea1b/live/8103d0e0-f203-11f0-b385-5f48925de19a.png",
    "created_at": "2026-01-16T00:58:28.488Z",
    "topic": "sports"
  },
  {
    "slug": "fatal-fury-fans-spot-strange-detail-in-new-trailer-igniting-generative-ai-controversy",
    "title": "Fatal Fury Fans Spot Strange Detail In New Trailer, Igniting Generative AI Controversy",
    "description": "The use of generative AI in video games has been controversial to say the least, but when it comes to art, the blowback has been even stronger. One game currently being accused of using the technology is Fatal Fury: City of the Wolves, as the new trailer for the fighting game's upcoming second season of DLC characters is being picked apart online. Viewers and commenters have pored over the trailer, claiming that the character designs are inconsistent and that certain visual elements contain generative AI. The biggest smoking gun, according to viewers, is a quick shot of series antagonist Geese Howard, who appears to be missing an entire foot.\nOther instances of generative AI pointed out include Blue Mary's motorcycle missing several vital components--and being a completely different model from what she normally rides--and Kim Jae Hoon having noticeably different wardrobes between his cinematic and gameplay character models.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/fatal-fury-fans-spot-strange-detail-in-new-trailer-igniting-generative-ai-controversy/1100-6537425/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1601/16018044/4635682-fatal-fury-1.jpg",
    "created_at": "2026-01-15T18:24:05.401Z",
    "topic": "gaming"
  },
  {
    "slug": "tech-executives-bet-big-on-ai-their-workers-are-being-tasked-with-proving-they-were-right",
    "title": "Tech executives bet big on AI. Their workers are being tasked with proving they were right.",
    "description": "Silicon Valley has a new mantra, and it's all about employees proving their worth.",
    "fullText": "First came efficiency. Then came intensity. Now it's accountability.\n\nA new year means a new mantra for Silicon Valley, and this time it's all about showing your work, writes BI's Tim Paradis.\n\nFrom Amazon helping managers track employees' time spent in the office to Meta keeping tabs on workers' AI usage, tech's corporate overlords are no longer going to take your word for it.\n\nThere's a not-so-subtle reason for this sudden interest in documentation. You might have heard me say this before, but companies are investing lots into AI, and the benefits aren't entirely clear. (JPMorgan's Jamie Dimon literally told analysts to just \"trust me.\")\n\nSo with investors breathing down execs' necks about their tech budgets, they're now looking for the humans to give them something to show for it. Add this to the growing list of ways AI ends up creating more work for employees.\n\nThis isn't just a Big Tech phenomenon. Citi CEO Jane Fraser, who is in the midst of her own \"Transformation,\" told workers in a recent memo that old habits won't fly anymore and everyone needs to step up their game.\n\nThere's a slightly less cynical way to look at this whole thing, one expert told Tim. Collecting all this data will help bosses better justify their workers' existence. (I only said it was slightly less cynical.)\n\nIt reminds me of an old coach I had. He'd tell us you shouldn't worry about getting yelled at. You should only panic when he stops yelling at you. That means he thinks you're a lost cause.\n\nThat might be true, but it sure didn't make wind sprints after practice easier.\n\nMetrics aren't a complete disaster for workers, but they could pose a risk to innovation.\n\nA clear sense of what your company expects from you can be beneficial, especially when you're looking for a raise. You asked me to produce X. I delivered X+1. Time to pay up. (Results may vary on that pitch.)\n\nGuidelines can be limiting though. Let's say you crack the code on hitting your assigned number. Are you willing to deviate from that strategy? Is the risk of not hitting your number worth the reward of trying something new?\n\nCreativity is rarely born from repetition. You often need to understand what doesn't work to figure out what will work. But if you're constantly being asked to show your worth, putting up a bunch of Ls, even if a W is around the corner, is a dangerous game.\n\nBesides, the executives are already taking enough risks for all of us.",
    "readingTime": 3,
    "keywords": [
      "slightly less",
      "less cynical",
      "workers",
      "you're",
      "it's",
      "employees",
      "there's",
      "aren't",
      "looking",
      "game"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/bi-newsletter-big-tech-employee-tracking-oversight-2026-1",
    "thumbnail_url": "https://i.insider.com/696816ea764ca5f34d2a7999?width=1200&format=jpeg",
    "created_at": "2026-01-15T18:24:03.087Z",
    "topic": "finance"
  },
  {
    "slug": "want-a-promotion-it-might-pay-to-use-ai",
    "title": "Want a promotion? It might pay to use AI",
    "description": "Cisco employees who used AI tools were promoted faster and showed higher engagement and retention rates, an analysis found.",
    "fullText": "AI use could make the difference between being up for promotion — or getting passed over.\n\nCisco released a report on Thursday that found that active AI users at the tech company were more likely to be promoted faster. Employees recommended for promotion used AI 50% more often than those who were not recommended, the report said.\n\nIt also said that active AI users, which it describes as those who consistently use the technology, are 40% more likely to be distinguished as \"critical to retain.\"\n\n\"These patterns suggest that Cisco is becoming a place where AI skills are not only developed but rewarded,\" the report said.\n\nThe findings, conducted by Cisco's People Intelligence team over the past year, are based on data from a comprehensive analysis focused on AI tool adoption, usage, experience, and impact within the company.\n\n\"When they're using AI, they're more excited about our mission. They're more confident in where the company is headed, they feel more challenged and empowered to do their roles,\" Cisco's chief people officer, Kelly Jones, told Business Insider.\n\nWhile Cisco is observing a link between those who use AI and are recommended for promotions, other companies have explicitly said that AI usage will play a role in determining who gets promoted.\n\nBusiness Insider previously reported that Jamie Siminoff, Amazon's VP of product who runs the company's home security division, had announced that all applications for promotions in his division would require employees to describe how they were utilizing AI. The VP said promotions were \"the only real incentive\" for teams to use the technology.\n\nSimilarly, this year, Meta is poised to start tying employees' performance to their \"AI-driven impact,\" according to an internal memo sent to employees by the company's head of people in November and seen by Business Insider. The tech giant will assess employees on how they use AI to deliver results and build tools that have a major impact on productivity.",
    "readingTime": 2,
    "keywords": [
      "employees",
      "recommended",
      "impact",
      "they're",
      "promotions",
      "promotion",
      "active",
      "users",
      "tech",
      "promoted"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/cisco-study-using-ai-improved-chances-of-promotion-2026-1",
    "thumbnail_url": "https://i.insider.com/6967c2b404eda4732f2f0dd6?width=1200&format=jpeg",
    "created_at": "2026-01-15T18:24:03.012Z",
    "topic": "science"
  },
  {
    "slug": "tsmcs-record-quarterly-profit-is-giving-a-fresh-boost-to-the-whole-chip-sector",
    "title": "TSMC's record quarterly profit is giving a fresh boost to the whole chip sector",
    "description": "Taiwan Semiconductor Manufacturing Company's record Q4 results are boosting chip stocks as investors cheer new signs of AI strength.",
    "fullText": "Taiwan Semiconductor Manufacturing Company just gave the AI trade a fresh boost.\n\nThe chipmaker reported fourth-quarter earnings on Wednesday, blowing past analyst estimates on multiple metrics. It also reported a record 35% quarterly profit, sending TSMC stock soaring as investors cheered the latest indicator that AI demand is still running hot.\n\nThe end of 2025 saw rising doubts about the sustainability of the market's top trade, and investor fatigue over relentless AI infrastructure spending. But TSMC's earnings smash and record profit looks to have revived the AI trade as investors get ready for Big Tech earnings in the coming weeks.\n\nTSMC reported that revenue increased 20% on a year-over-year basis in the quarter, while both net income and diluted earnings-per-share surged more than 35%.\n\n\"Our business in the fourth quarter was supported by strong demand for our leading-edge process technologies,\" said TSMC CFO and senior VP Wendell Huang. \"Moving into first quarter 2026, we expect our business to be supported by continued strong demand for our leading-edge process technologies.\"\n\nThe company also revealed that its capital spending could rise by as much as 37% to $56 billion this year, and it is expected to rise even more over the coming two years. Analysts had only expected $46 billion for this year.\n\nThis news quickly sent AI chip stocks soaring in anticipation of for higher demand. The sector's top premarket moves are as follows:\n\nThe news proved particularly bullish for ASML stock, which saw its market cap surge past $500 billion on Thursday after TSMC posted its big spending forecast.\n\nThe high spending forecast makes it clear that TSMC, considered the market's leader in the AI chip production space, expects demand to remain high not just in 2026 but over the upcoming three year period. As the company is seen as a bellwether for its industry, Wall Street will likely take it as an indicator that the AI trade is as strong as ever.",
    "readingTime": 2,
    "keywords": [
      "leading-edge process",
      "process technologies",
      "demand",
      "trade",
      "earnings",
      "quarter",
      "record",
      "profit",
      "stock",
      "soaring"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/tsmc-earnings-profit-record-chip-stocks-ai-trade-avgo-nvda-2026-1",
    "thumbnail_url": "https://i.insider.com/6968fcf864858d02d218669f?width=1200&format=jpeg",
    "created_at": "2026-01-15T18:24:02.665Z",
    "topic": "finance"
  },
  {
    "slug": "5-psychologybacked-principles-for-more-effective-personalization",
    "title": "5 Psychology-Backed Principles for More Effective Personalization",
    "description": "As personalization becomes table stakes, a new class of products is emerging that depends not just on data inferred from patterns and history but on deeply personal information that customers are willing to reveal about themselves in the moment. This is “confessional commerce,” a model in which value is created through candid, contextual disclosure and sustained by how products respond to it. Drawing on research from clinical psychology and real-world product building, well-designed interactions can reduce shame, invite deeper honesty, and build trust over time. By applying five principles that psychologists use to elicit meaningful disclosure, teams can pair AI’s predictive power with psychological attunement, creating feedback loops in which better responses lead to greater openness and, ultimately, more effective personalization.",
    "fullText": "5 Psychology-Backed Principles for More Effective Personalization by Michelle TaiteJanuary 15, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintPersonalization has become a baseline expectation across industries. Deloitte has found that nearly three quarters of consumers are more likely to buy from brands that personalize, and McKinsey has shown that companies that excel at personalization generate up to 40% more revenue than their peers. The economic upside is clear, and AI is quickly scaling these capabilities.",
    "readingTime": 1,
    "keywords": [
      "personalization"
    ],
    "qualityScore": 0.35,
    "link": "https://hbr.org/2026/01/5-psychology-backed-principles-for-more-effective-personalization",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_16_608719105.jpg",
    "created_at": "2026-01-15T18:24:01.734Z",
    "topic": "business"
  },
  {
    "slug": "ai-as-life-coach-experts-say-what-works-what-doesnt-and-what-to-look-out-for",
    "title": "AI as life coach: experts say what works, what doesn't and what to look out for",
    "description": "It’s becoming more common for people to use AI chatbots for personal guidance – but this doesn’t come without risks",
    "fullText": "It’s becoming more common for people to use AI chatbots for personal guidance – but this doesn’t come without risks\n\nIf you’re like a lot of people, you’ve probably ditched your new year resolutions by now. Setting goals is hard; keeping them is harder – and failure can bring about icky feelings about yourself.\n\nThis year, in an effort to game the system and tilt the scales toward success, some people used AI for their 2026 resolutions. It’s the latest step in an ongoing trend: in September 2025, OpenAI, the company behind ChatGPT, released findings showing that using the AI chatbot for personal guidance is very common.\n\nThe company’s interpretation of this was that “people value ChatGPT most as an adviser rather than only for task completion.”\n\nBut just because you can ask AI for life advice, should you? And is there an art to it? Here’s what experts say are the dos and don’ts.\n\nAI-driven goal-setting isn’t inherently good or bad, explains Zainab Iftikhar, a Brown University PhD candidate, whose research examines artificial intelligence and users’ wellbeing. Artificial intelligence can lower the barrier to self-reflection and be genuinely empowering for some, she explains. For people who feel stuck, overwhelmed, or unsure of where to begin, prompts “can act as a scaffold” for expressing and understanding your ideas, says Iftikhar.\n\nIf the AI has access to information you’ve either shared or asked it to generate, it’s also an efficient tool at synthesizing that information, explains Ziang Xiao, an assistant professor of computer science at Johns Hopkins University. The compilation and interpretation of your previous data could help you efficiently organize the thoughts that initiate your goals.\n\nBut there are also drawbacks to using AI for goal-setting, says Iftikhar. Navigating the potential harms can come down to how well you know yourself – and how well you can navigate bad AI advice.\n\nBecause large language models (LLMs), the type of AI that drives these systems, are trained on large-scale human-generated data, they can reproduce assumptions about success, self-improvement and relationships, Iftikhar explains. LLMs are also predominantly trained on English text and tend to exhibit a bias toward western values.\n\nAI-suggested goals risk being over-generic, reinforcing “dominant cultural narratives, rather than what is meaningful for a specific individual”, says Iftikhar.\n\nIt can be very difficult to detect this bias. AI chatbots can be persuasive in a way that individuals may have difficulty detecting if they are being nudged toward mismatched goals, says Xiao. These tools may “inappropriately affirm goals that may not actually be a great fit for you”, he says.\n\nEven if you use a chatbot frequently and request that it specifically base its responses on previous conversations, there’s still a chance that the chatbot’s replies will incorporate insights that have nothing to do with the information you’ve already shared, he explains.\n\nDuring her research, Iftikhar noticed that the people who are routinely correcting or ignoring bad AI responses are at an advantage in using AI itself. Those who are not, for a variety of reasons, including technical expertise, are “more likely to suffer from incorrect or harmful responses”, she explains.\n\nAI can also reflect the bias of the user asking it for guidance. In a 2024 study, Xiao and colleagues observed that LLM users were more likely to become trapped in an echo chamber, compared with those who use traditional web searches.\n\nAI chatbots are designed to make us happy, explains Xiao. In a 2025 paper published in the journal npj Digital Medicine, researchers show LLMs often prioritize agreement over accuracy. These tools are typically optimized with human feedback that rewards agreeableness and flattery.\n\nIn turn, chatbots engage in sycophancy, or excessive agreement, with users. (In May 2025, OpenAI announced it was rolling back an update that made ChatGPT too sycophantic.)\n\nIftikhar says it’s worth being wary of tools that skip self-reflection or emotional processing in favor of tidy action plans.\n\nThat said, AI can help brainstorm the actionable goals we want to set for ourselves, says Emily Balcetis, an associate professor of psychology at New York University. She recommends prompting AI to consider what obstacles you might face as you attempt to accomplish these goals, as well as back-up plans you might need.\n\n“Have it be a collaborator in how you’ll track your progress and monitor performance along the way,” says Balcetis.\n\nXiao recommends critically analyzing the chatbot’s responses – and then giving it feedback. Does this plan actually fit with your life? Is it aligned with your priorities and hopes?\n\n“Try to give informative, quality feedback to the AI just as you would give feedback to another person,” says Xiao. “This process will help AI generate a more personable, realistic goal and help you consider the things that you really want.”\n\nGood goal-setting also includes a review of why you haven’t pursued these goals already, explains EJ Masicampo, an associate professor of psychology at Wake Forest University.\n\n“When it feels like we’re failing at a goal, it’s often that we’ve just prioritized the other things we’re trying to do,” says Masicampo. Multiple goals are difficult to juggle, he explains. It can be more productive to examine one ambition and what’s obstructing your motivation to achieve it.\n\nUltimately, chatbots may work best as reflective partners, albeit partners that don’t actually care about your success.\n\n“These tools sound very human-like, but by design, they can’t take responsibility for your actions,” says Xiao.\n\nFor better or for worse, that is up to you.",
    "readingTime": 5,
    "keywords": [
      "artificial intelligence",
      "personal guidance",
      "associate professor",
      "goals",
      "explains",
      "it’s",
      "chatbots",
      "tools",
      "responses",
      "feedback"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/wellness/2026/jan/15/ai-life-coach",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a78a0587320514bd41ee10e0e2dfd0a55697fb97/0_0_3000_2400/master/3000.png?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=33e02fd0d56e028429a6f784d806bed3",
    "created_at": "2026-01-15T18:24:00.521Z",
    "topic": "health"
  },
  {
    "slug": "nothing-new-under-the-sun-everything-is-a-file",
    "title": "Nothing new under the sun: everything is a file",
    "description": "The Unix revolution was built on a key principle: everything is a file. Now, with the rise of AI Agents, LLMs have access to half a century of file-based arcana. The result? Everything is becoming a file again.",
    "fullText": "\"What has been will be again,\nwhat has been done will be done again;\nthere is nothing new under the sun.\" – Ecclesiastes 1:9\n\nMy first contact with computers happened when I was 11 years old. I had an old PC running MS-DOS 5.2 and Windows 3.1. I used it mostly to play games. However, my professional self (30 years later) is happy that just playing games in that era required learning about x86 memory segmentation.\n\nThose were fun times, but I don't even count that as a gateway drug to programming. What got me hooked was what came later: Unix. It wasn't until my first year in high school that I discovered Linux. Once I started college, my interest really piqued. Soon, I was all-in on the history of Unix and would find any possible excuse to play with the SPARC Solaris station parked in the Materials department.\n\nWe wouldn't be where we are today without Unix. If you never experienced the old days, it is probably easy to underestimate the significance of Unix. But aside from the C language as a side-effect, Unix gave the world two things:\n\nAfter I replaced my boring friends with Unix zealots (that's how I like to tell the story – in reality, they all stopped talking to me because I wouldn't shut up about Unix), I kept hearing the phrase: \"sed awk cut grep is all you need.\" It is a beautiful abstraction. It sounds obvious to us half a century later (like most great ideas), but in reality, it's anything but obvious.\n\nEvery program understands files. They can write to a file, and they can read from a file. And what that means is that suddenly you have a unified interface across the entire system. A contract that everyone follows, making sure that every other application can consume the results of every other application.\n\nThat simple abstraction, a file, allowed for another powerful Unix principle to come to life: \"Do one thing, and one thing well.\" Instead of building a big application, each tool can hyper-specialize. They can do this because they are all inputting and outputting the results of their work as files.\n\nThis leads to a combinatorial explosion of what can be achieved, and it all happens through the file contract. Write a couple of bytes here, read a couple of bytes there, and pipes glue them together.\n\nFiles are most commonly understood as something that lives in your storage device, like a PDF or a spreadsheet document. But what makes them so powerful is that they are a very simple abstraction: a collection of bytes that can be read from and written to. Soon, Unix would have virtual files all over the place. Ultimately even a network connection in the Unix world could be represented as a file.\n\nLinux took that to the extreme. There is a /proc virtual filesystem with all sorts of information about your Kernel, and the way you read this information is… by reading virtual files (The proc filesystem existed in the original Unices, but it was predominantly simple information about the running processes, as the name implies)\n\nThere is also a /sys virtual filesystem where you can get all sorts of information about your device drivers and hardware system. And the way you interact with it is, you guessed it, by reading and writing files.\nAnd we built our world on top of this. Later, we saw similar contracts appearing in other layers (like APIs), but the file was still ever-present. Even in API-based systems, files are still used to configure systems, store code, serve assets, etc.\n\nFast-forward to the AI era, and one thing became clear: LLMs are nice, neat, and cute. But they won't take you anywhere. What truly unlocks the potential for AI is agents, which is a fancy way of saying \"a for loop where the LLM uses tools.\"\n\nAs it turns out, LLMs are really, really good at using the tools that have been around for half a century of Unix. The tools that compose together beautifully through the file abstraction. In half a century of Unix, we have accumulated a tool for every conceivable job. Because of their very nature as stream pushers, agents are really really good at using them.\n\nSure, we can rebuild everything. We can give agents specialized tools that are agent-native. But that would mean throwing all the capital we have accumulated throughout all this time into the void…for marginal gains. We haven't done that with the shape of power plugs, and SQL still reigns supreme (despite the fact that every developer seems to think they know how to do better).\n\nInstead, agents will embrace the filesystem. This is already happening, with tools like Claude Code heavily relying on it, and it will happen even more.\n\nHowever, that doesn't mean filesystems will stay the same. There are two particular trends in the industry now that will apply pressure in the shape of filesystems.\n\nThe first is the prevalence of Typescript and browser-based environments as deployment vehicles for agents. Browsers don't really have an easy way to plug into a standard filesystem, and Typescript-based agents are usually deployed into ephemeral environments where a filesystem is not to be taken for granted. That's a side effect of those platforms evolving to provide a function-like environment that connects to a database over-the-wire for its data needs.\n\nThe second is the rise of the sandbox as the preferred way to isolate agentic workloads. Sandboxes take virtualized environments to the next level. Environments need to come online in milliseconds as agents spawn sub-agents to go explore the solution space. Attaching traditional filesystems to those sandboxes is just too slow for the speed at which they need to operate.\n\nTwo interesting tools that are trying to tackle this are worth mentioning. The first is just-bash by Vercel. The tool provides an emulated bash-like environment for agents written in Typescript, allowing agents to use tooling as if they were operating in a normal Unix shell, wherever they may be executing:\n\nThe second is our very own AgentFS, a tool that maps an entire filesystem into a SQLite file. The filesystem can be isolated between agents (with changes being captured into the file, without harming the original filesystem).\n\nThis makes sure that a) the agent can access only the parts of the context it's supposed to access, and b) that it is allowed to operate on the assets freely, knowing that changes are non-destructive.\n\nThe SQLite file can be copied or partially copied around by sandboxes and made available instantly as the agents execute. This supports snapshotting (where an agent can save its own state, take a step, and then rollback to the previous file if it makes a mistake) as well as sharing of state between a group of agents.\n\nWhat comes around, goes around. As the world changes radically around us, one thing will not change: we build on top of what came before us, and rebuild from scratch at our own peril. The Unix revolution gave us the file abstraction, and for half a century, we built on it.\n\nFor AI agents, the question will be: do we tap into the immense potential of all tooling written in the past 50 years, or rebuild everything?\n\nThe answer is starting to become apparent.",
    "readingTime": 7,
    "keywords": [
      "sqlite file",
      "rebuild everything",
      "half century",
      "simple abstraction",
      "virtual files",
      "virtual filesystem",
      "agents",
      "tools",
      "unix",
      "later"
    ],
    "qualityScore": 1,
    "link": "https://turso.tech/blog/nothing-new-under-the-sun",
    "thumbnail_url": "https://turso.tech/images/blog/nothing-new-under-the-sun/cover.png",
    "created_at": "2026-01-15T18:24:00.507Z",
    "topic": "tech"
  },
  {
    "slug": "yasu-ai-agents-that-fix-cloud-waste-not-just-report-it",
    "title": "Yasu – AI agents that fix cloud waste, not just report it",
    "description": "Analyze, optimize, and govern your multi-cloud environment effortlessly with our AI Agent driven approach",
    "fullText": "From Cloud Chaos to Autonomous Control, in 3 Steps\n\nFrom Cloud Chaos to Autonomous Control, in 3 Steps\n\nSet up a connection to Yasu for your AWS, GCP, Azure, GitHub, and Slack. No disruption, just instant cost visibility.\n\nOur AI agents run 24/7, spotting expensive mistakes in pull requests before they hit production, where fixes cost 10× more.\n\nDashboards point at problems. We fix them. By integrating directly into your CI/CD pipelines, we create and review PRs to prevent waste before it happens.\n\nDashboards point at problems. We fix them. Directly integrating into the CI/CD pipelines we create / review PR's to prevent it from happening/\n\nOur platform is designed to provide you with an exceptional user experience, catering to the needs of ambitious professionals and visionary entrepreneurs.\n\nAsk questions in plain English, get answers in seconds. Yasu's AI assistant lives in Slack and Teams—no dashboards to navigate, no SQL to write, no context switching required.\n\nAsk questions in plain English, get answers in seconds. Yasu's AI assistant lives in Slack and Teams—no dashboards to navigate, no SQL to write, no context switching required.\n\nWhether you're shipping code, managing budgets, or optimizing spend,\n\nYasu gives every team the visibility they need\n\nWhether you're shipping code, managing budgets, or optimizing spend,\n\nYasu gives every team the visibility they need\n\nGet real-time cost feedback in your CI/CD pipeline, understand the impact of your code before deployment, and ship with confidence.\n\nGet real-time cost feedback in your CI/CD pipeline, understand the impact of your code before deployment, and ship with confidence.\n\nYasu is a cloud cost intelligence platform that helps you optimize your cloud spending.\n\nCatch costly misconfigurations during design and development, when fixes are 10× cheaper. Never find out you’ve lost 30% after the fact.\n\nTalk to Yasu like a teammate. Get clear, actionable insights instantly\n\nOne view of your cloud spend from all major providers across AWS, Azure, and GCP.\n\nOne view of your cloud spend from all major providers across AWS, Azure, and GCP.\n\nSelf-learning agents make cost-saving decisions based on your business context.\n\nWe answered questions so you don’t have to ask them.\n\nWe answered questions so you don’t have to ask them.\n\nYasu works like a senior cloud engineer on your team—catching waste in PRs, answering cost questions instantly, and implementing optimizations 24/7.\n\nYasu works like a senior cloud engineer on your team—catching waste in PRs, answering cost questions instantly, and implementing optimizations 24/7.\n\nYasu works like a senior cloud engineer on your team—catching waste in PRs, answering cost questions instantly, and implementing optimizations 24/7.\n\nYasu works like a senior cloud engineer on your team—catching waste in PRs, answering cost questions instantly, and implementing optimizations 24/7.",
    "readingTime": 3,
    "keywords": [
      "plain english",
      "seconds yasu's",
      "across aws",
      "ci/cd pipelines",
      "teams—no dashboards",
      "ci/cd pipeline",
      "aws azure",
      "prs answering",
      "cloud chaos",
      "you're shipping"
    ],
    "qualityScore": 1,
    "link": "https://yasu.cloud/",
    "thumbnail_url": "https://framerusercontent.com/assets/nP8lZY71jCMDb1cB3JpoJAeD3U.png",
    "created_at": "2026-01-15T18:24:00.001Z",
    "topic": "tech"
  },
  {
    "slug": "turn-steam-reviews-into-personas-and-insights-without-agent-chatting",
    "title": "Turn Steam reviews into personas and insights, without agent chatting",
    "description": "An AI-backed Spell turns Steam reviews into a shareable, evidence-backed readout: Quick Verdict, Experience Radar, Dealbreakers, Player Personas, Topics, and Highlights.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://steam-review.dexmage.com/",
    "thumbnail_url": "https://steam-comments-reviewer.replit.app/cover.jpg",
    "created_at": "2026-01-15T18:23:59.997Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-economic-index-economic-primitives",
    "title": "Anthropic Economic Index economic primitives",
    "description": "This report introduces new metrics of AI usage to provide a rich portrait of interactions with Claude in November 2025, just prior to the release of Opus 4.5.",
    "fullText": "This report introduces new metrics of AI usage to provide a rich portrait of interactions with Claude in November 2025, just prior to the release of Opus 4.5. These “primitives”—simple, foundational measures of how Claude is used, which we generate by asking Claude specific questions about anonymized Claude.ai and first-party (1P) API transcripts—cover five dimensions relevant to AI’s economic impact: user and AI skills, how complex tasks are, the degree of autonomy afforded to Claude, how successful Claude is, and whether Claude is used for personal, educational, or work purposes.\n\nThe results reveal striking geographic variation, real-world estimates of AI task horizons, and a basis for revised assessments of Claude's macroeconomic impact.\n\nThe data we release alongside this report are the most comprehensive to date, covering five new dimensions of AI use, consumer and firm use, and country and region breakdowns for Claude.ai.\n\nIn the first chapter, we revisit findings from our previous Economic Index report published in September 2025. We find:\n\nWhile substantial concentration remains, since our last report Claude usage has become noticeably more evenly distributed across US states. If sustained, usage per capita would be equalized across the country in 2-5 years.\n\nIn the second chapter we discuss the motivation for and introduce our new economic primitives, including how they were selected and operationalized, and their limitations. We additionally present evidence that our primitives capture directionally accurate aspects of underlying usage patterns as compared to external benchmarks. In chapters three and four we use these primitives to further investigate implications for adoption and productivity. We find:\n\nThese results provide a new window into how AI is currently impacting the economy. Knowing the success rate of tasks gives a more accurate picture of which tasks might be automated, how impacted certain jobs might be, and how labor productivity will change. Measuring differential performance by user education sheds light on inequality effects.\n\nIndeed, the close relationship between education levels in inputs and outputs signals that countries with higher educational attainment may be better positioned to benefit from AI, independent of adoption rates alone.\n\nThis data release aims to enable researchers and the public to better understand the economic implications of AI and investigate the ways in which this transformative technology is already having an effect.\n\nBecause frontier AI model capabilities are improving rapidly and adoption has been swift, it is important to regularly take stock of changes in how people and businesses are using such systems—and what this usage implies for the broader economy.\n\nIn this chapter we analyze how Claude usage and diffusion patterns changed from August 2025 to November 2025 just prior to the release of Opus 4.5. We make four observations:\n\nEven though frontier LLMs have an impressive range of capabilities relevant to every facet of the modern economy, Claude usage remains very concentrated among a small number of tasks. As compared to nearly one year ago, consumer usage on Claude.ai is modestly more concentrated: The share of conversations assigned to the ten most prevalent O*NET tasks was 24% in November 2025, 1pp higher than in August and up from 21% in January 2025. The most prevalent task in November 2025—modifying software to correct errors—alone represented 6% of usage.\n\nIn our last Anthropic Economic Index Report we began tracking business adoption patterns by studying Claude usage among 1P API customers. The ten most common tasks grew from 28% of API records in August to 32% in November. Rising concentration among a small set of tasks suggests the highest-value applications continue to generate outsized economic value even as models have become more capable at a wider range of tasks. As with Claude.ai the most common task among API customers was modifying software to correct errors, which accounted for one in ten records.\n\nIndeed, computer and mathematical tasks—like modifying software to correct errors—continue to dominate Claude usage overall, representing a third of conversations on Claude.ai and nearly half of 1P API traffic. Such dominance has subsided on Claude.ai: the share of conversations on Claude.ai assigned to such (mostly) coding-related tasks is down from a peak of 40% in March 2025 to 34% in November 2025. At the same time, the share of transcripts assigned to computer and mathematical tasks among 1P API traffic edged higher from 44% in August to 46% in November 2025 (Figure 1.2).\n\nThe second largest share of Claude.ai usage in November 2025 was in the Educational Instruction and Library category. This corresponds mostly to help with coursework and review, and the development of instructional materials. Such usage has risen steadily since our first report, up from 9% of conversations on Claude.ai in January 2025 to 15% in November.\n\nThe share of usage on Claude.ai for Arts, Design, Entertainment, Sports, and Media tasks increased between August and November 2025 as Claude was used in a growing share of conversations for writing tasks, primarily copyediting and the writing and refinement of fictional pieces. This jump in the prevalence of design- and writing-related tasks reversed a steady decline across earlier reports. For both Claude.ai and API customers, there was a drop in the share of conversations/transcripts where Claude was used for Life, Physical, and Social Science-related tasks.\n\nPerhaps the most notable development for API customers was the increase in the share of transcripts associated with Office and Administrative Support related tasks, which rose 3pp in August to 13% in November 2025. Because API use is automation-dominant, this suggests that businesses are increasingly using Claude to automate routine back-office workflows such as email management, document processing, customer relationship management, and scheduling.\n\nHow AI will affect the economy depends not just on the tasks Claude is used for but the way that users access and engage underlying model capabilities. Since our first report, we have classified conversations into one of five interaction types, which we group into two broader categories: automation and augmentation.\n\nFigure 1.3 plots how automated versus augmented use has evolved over time since we first started collecting this data one year ago. In January 2025, augmented use of Claude was dominant: 56% of conversations were classified as augmentation compared to 41% automated. In August 2025, more conversations were classified as automated as compared to augmented.\n\nThis was a notable development since it suggested that rapid improvements in model capabilities and platform functionality coincided with users increasingly delegating tasks entirely to Claude. This was evident in the “directive” collaboration mode, which is further grouped as automation. Directive conversations are those in which users give Claude a task and it completes it with minimal back-and-forth. From January 2025 to August 2025 the share of such directive conversations rose from 27% to 39%.\n\nThree months later, the share of directive conversations had fallen 7pp to 32% in November 2025 as augmentation once again became more prevalent on Claude.ai than automation. Nevertheless, the automation share was still elevated as compared to nearly one year ago when we first began tracking this measure, suggesting that the underlying trend is still toward greater automation even as the August spike overstated how quickly it was materializing.\n\nWhile we see some evidence of a shift toward soft skill usage on Claude.ai with design, management, and education now higher, the shift back toward augmented use was broad-based in November (Figure 1.4). The rise in augmented use was driven mainly by users iterating with Claude to complete tasks (“task iteration”) rather than asking Claude to explain concepts (“learning”). See Figure 1.5 for common words associated with the three most common interaction modes across O*NET tasks and bottom-up descriptions of requests made of Claude.\n\nIn our previous report, we introduced the Anthropic AI Usage Index (AUI), a measure of whether Claude is over- or underrepresented in a given geography relative to the size of its working-age population. The AUI is defined as\n\nAn AUI above 1 indicates that a country uses Claude more intensively than its population alone would predict, while an AUI below 1 indicates lower-than-expected usage. For example, Denmark has an AUI of 2.1, meaning its residents use Claude at roughly twice the rate its share of the global working-age population would suggest.\n\nA key fact about Claude usage globally is that it is geographically concentrated: a small number of countries comprise an outsized share of use. From a global perspective, little changed in this respect between August and November 2025. Indeed, the left panel of Figure 1.6 shows that the AUI concentration across countries was essentially unchanged between our last report and this report.\n\nWhat shapes patterns of usage within the US and around the world? In our previous report we emphasized the key role played by income differences globally: Variation in Claude usage across countries is largely accounted for by variation in GDP per capita. In Chapter 3 we revisit the importance of income in shaping not just usage intensity but also patterns of usage around the world.\n\nWithin the US, income is less clearly a predictor of usage. Instead, what appears to matter most is the composition of each state’s workforce and how well-matched the workforce is to Claude capabilities as reflected in task-level usage. States that have a higher share of workers in computer and mathematical occupations—like Washington D.C., Virginia, and Washington—tend to have higher usage per capita. Quantitatively, each 1% increase in the share of such tech workers in a state is associated with 0.36% higher usage per capita (Figure 1.7). This alone accounts for nearly two-thirds of the cross-state variation in AUI.\n\nWhile we would intuitively expect Claude usage to be higher in states with more tech workers, this pattern holds more generally: Usage per capita is higher in states with more workers in occupations where Claude usage is overrepresented as compared to the US workforce (e.g., Arts, Design, Entertainment, Sports and Media) or with relatively fewer workers in occupations where Claude usage is low as compared to the national economy (e.g., Transportation and Material Moving). This can be seen by calculating the Kullback–Leibler (KL) divergence between the composition of each state’s workforce and the global composition of Claude usage. States with a lower KL divergence—and thus with a workforce that looks more similar to Claude usage patterns—tend to have higher usage per capita.\n\nWhile differences in workforce composition appear to play a role in shaping regional adoption within the US, early evidence suggests Claude is diffusing considerably faster than historical precedent would predict. Economically consequential technologies have historically taken around half a century to achieve full diffusion across the US (Kalanyi et al., 2025). By contrast, comparing Claude adoption rates in November 2025 to three months prior, we estimate that parity in adoption per capita across US states—as measured by the AUI—could be reached within 2–5 years. This estimate comes with a high degree of uncertainty as the precision of our estimates cannot rule out much slower rates of diffusion.\n\nWe generate this estimate through the lens of a simple model of diffusion, which we briefly describe here. We model diffusion as proportional convergence toward a common steady state of equalized usage per capita in which each state s has an AUI equal to 1:\n\nUnder this model, the log deviation of AUI from steady state (AUI = 1) shrinks by a factor of β every three months, implying a half-life of ln(.5)/ln(β) quarters. For example, with quarterly data a value of β = 0.99 implies a half-life of about 17 years. To illustrate, starting from an initial AUI of 2, this means AUI would decline to around 1.4 after 17 years and to around 1.1 after 50 years. We take β = 0.99 as a sensible benchmark because it implies a pace of diffusion similar to economically consequential technologies in the 20th century.\n\nThis model of convergence motivates the following regression specification:\n\nNaively estimating this equation by ordinary least squares (OLS) yields an estimate of β̂ ≈ 0.77. Weighted least squares (WLS) where we weight by each state’s workforce yields an estimate of β̂ ≈ 0.76 (Figure 1.8). Both are statistically distinguishable from 1 at conventional levels. Taken at face value, these estimates imply that it would take little more than two years for each state's AUI to close most of the gap to 1.\n\nA concern with estimating convergence this way is that our AUI estimates are subject to sampling noise and other variation unrelated to diffusion. This can produce classical attenuation bias: even if AUI is not actually changing, our estimate of β could end up meaningfully below one.\n\nTo address this, we estimate the model by two-stage least squares (2SLS), instrumenting the log of AUI in August 2025 with the composition of each state's workforce, measured by its proximity to overall Claude usage patterns. The logic behind this instrument is that workforce composition is a strong predictor of Claude usage (relevance) but being measured independently, is expected to be uncorrelated with sampling noise in our AUI estimates (validity). As noted above, states with more workers in high-Claude-usage roles do tend to have systematically higher usage per capita.\n\nThe 2SLS estimates imply modestly slower convergence: β̂ ≈ 0.89 unweighted and β̂ ≈ 0.86 when weighting by each state’s working-age population. However, these estimates are less precise, and only the former is statistically distinguishable from 1 at the 10% level. Despite implying a slower convergence than OLS, the 2SLS estimates still imply rapid diffusion: just four to five years for the log deviation of each state's AUI to shrink by 90%.\n\nThat said, our estimates are based on just three months of data. And while the 2SLS specification may help address sampling noise, considerable uncertainty remains. We will revisit this question of the pace of diffusion in future reports.\n\nAs with previous reports, all our analysis is based on privacy-preserving analysis. Throughout the report we analyze a random sample of 1M conversations from Claude.ai Free, Pro and Max conversations (we also refer to this as “consumer data” since it mostly represents consumer use) and 1M transcripts from our first-party (1P) API traffic (we also refer to this as “enterprise data” since it mostly represents enterprise use). Both samples come from November 13, 2025 to November 20, 2025. We continue to manage data according to our privacy and retention policies, and our analysis is consistent with our terms, policies, and contractual agreements. For 1P API data, each record is a prompt-response pair from our sample period which in some instances is mid-session for multi-turn interactions.\n\nThe share of conversations on Claude.ai that were classified into neither automation nor augmentation categories fell from 3.9% to 3.0%.\n\nSee, for example, Kalanyi et al (2025): “Second, as the technologies mature and the number of related jobs grows, hiring spreads geographically. This process is very slow, taking around 50 years to disperse fully.”\n\nWith our bottom-up analysis of 1P API traffic we see Claude used to \"Generate personalized B2B cold sales emails\" (0.47%), \"Analyze emails and draft replies for business correspondence\" (0.28%), \"Build and maintain invoice processing systems\" (0.24%), \"Classify and categorize emails into predefined labels\" (0.23%), and \"Manage calendar scheduling, meeting coordination, and appointment booking\" (0.16%).\n\nAt a high level, we distinguish between automation and augmentation modes of using Claude. Automation encompasses interaction patterns focused on task completion: Directive: Users give Claude a task and it completes it with minimal back-and-forth; Feedback Loops: Users automate tasks and provide feedback to Claude as needed; Augmentation focuses on collaborative interaction patterns: Learning: Users ask Claude for information or explanations about various topics; Task Iteration: Users iterate on tasks collaboratively with Claude; Validation: Users ask Claude for feedback on their work.\n\nThese interaction modes are not mutually exhaustive. In some instances, Claude determines that a sampled conversation does not match any of the five interaction modes.\n\nIn this report we use Sonnet 4.5 for classification whereas in our previous Economic Index report we used Sonnet 4. We previously found that different models can generate different classification outcomes, though these effects tend to be modest.\n\nWe include a constant term in the regression since it should be equal to zero under the null hypothesis. Across all our specifications, the constant term is estimated to be close to and statistically indistinguishable from zero.\n\nThe strength of the Anthropic Economic Index lies in showing not only how much AI is used, but how it is used. In prior reports, we showed which tasks Claude is used for, and how people collaborate with Claude. These data have enabled external researchers to analyze labor market shifts (e.g., Brynjolfsson, Chandar & Chen, 2025).\n\nIn this edition of the Anthropic Economic Index, we expand the breadth of data available to external researchers by providing insights on five economic “primitives”, by which we mean simple, foundational measures of the ways that Claude is used, which we generate by asking Claude to answer specific questions about the anonymized transcripts in our sample. Some of our primitives encompass several such questions, and others use a single indicator.\n\nBecause AI capabilities are advancing so rapidly and the economic effects will be unevenly experienced, we need a breadth of signals to uncover not just how Claude is used but also to inform what impact this technology will have.\n\nThis report introduces five new economic primitives beyond the one we already measure, collaboration patterns (whether users automate or augment their tasks with Claude). These primitives capture five dimensions of a human-AI conversation: 1) task complexity, 2) human and AI skills, 3) work, coursework or personal use case, 4) the AI’s level of autonomy, and 5) task success (see Table 2.1). AI autonomy captures something different from our existing automation/augmentation distinction. For example, “Translate this paragraph into French” is high automation (directive, minimal back-and-forth) but low AI autonomy (the task requires little decision-making from Claude).\n\nTask complexity captures that tasks can vary in their complexity, including how long they take to complete and how difficult they are. A \"debugging\" task in O*NET could refer to Claude fixing a small error in a function or comprehensively refactoring a codebase—with very different implications for labor demand. We measure complexity through estimated human time to complete tasks without AI, time spent completing tasks with AI, and whether users handle multiple tasks within a single conversation.\n\nHuman and AI skills address how automation interacts with skill levels. If AI disproportionately substitutes for tasks requiring less expertise while complementing higher-skilled work, it could be another form of skill-biased technical change—increasing demand for highly skilled workers while displacing lower skilled workers. We measure whether users could have completed tasks without Claude, and the years of education needed to understand both user prompts and Claude's responses.\n\nUse case distinguishes professional, educational, and personal use. Labor market effects most directly follow from workplace use, while educational use may signal where the future workforce is building AI-complementary skills.\n\nAI autonomy measures the degree to which users delegate decision-making to Claude. Our latest report documented rising \"directive\" use where users delegate tasks entirely. Tracking autonomy levels—from active collaboration to full delegation—helps forecast the pace of automation.\n\nTask success measures Claude’s assessment of whether Claude completes tasks successfully. Task success helps assess whether tasks can be automated effectively (can a task be automated at all?) and efficiently (how many attempts would it take to automate a task?). That is, task success matters for both the feasibility and the cost of automation labor tasks.\n\nThe new dimensions of AI use captured in our data were informed by our recent work on the productivity effects of Claude, feedback we received from external researchers, recent literature on AI’s economic impact through the lens of human capital and expertise (Vendraminell et al., 2025), and deliberation within our economic research team. Our main selection criteria were expected economic relevance, complementarity of dimensions, and whether Claude could classify conversations along that dimension with directional accuracy.\n\nWe propose that multiple simple primitives, even if somewhat noisy and not perfectly accurate by themselves, can together provide important signals on how AI is being used. We therefore mainly tested for directional accuracy.\n\nFor classifying task duration with and without AI, we used minimally modified versions of our prior productivity work. For net new classifiers, implemented via our privacy-preserving tooling, our validation process was as follows. We designed multiple potential measures to capture concepts such as task complexity. For Claude.ai, we evaluated the classifier performance compared to a human researcher on a small set of transcripts in which users gave feedback to Claude.ai and for which we thus have permission to look at underlying transcripts. For first-party API (1P API) data, we validate the classifiers using a mix of internal and synthetic data. Neither data sources are fully representative of Claude.ai or 1P API traffic, but they allow us to check that the classifiers are working on data that resembles real usage data, while ensuring privacy.\n\nBased on initial performance, we revised the classifiers that needed tweaking or discarded classifiers that did not perform well. Interestingly, we find that in some instances (e.g., to measure task success), a simple classifier performed better than a nuanced, complex classifier when compared to human ratings. We then compared performance of classifier versions with vs. without chain of thought prompting, and decided to keep chain of thought prompting only for three facets (human time estimate, human with AI time estimate, and AI autonomy) where we found that it substantially improved performance. We selected a final set of nine new classifiers for the five primitives, all of which are directionally accurate even if they may deviate somewhat from human ratings.\n\nOur goal was to create classifiers that are straightforward to implement and in combination provide potentially important economic signals. While we are very confident in the directional accuracy of the new measures (e.g., tasks with higher average years of education needed to understand the human prompt are likely more complex), none of the measures should be taken as exact or definitive (e.g., Claude.ai may somewhat underestimate the human education years needed for many tasks).\n\nEven so, the primitives enrich our understanding of how people use AI. Systematic relationships emerge across primitives, regions, and tasks—patterns we explore in depth in Chapters 3 and 4. That these relationships are intuitive and consistent suggests the primitives capture relevant aspects of how people and businesses use Claude.\n\nExternal benchmarks reinforce this. In our productivity work, Claude’s time estimates correlate with actual time spent on software engineering tasks. Figure 2.1 shows that our human education measure correlates with actual worker education levels across occupations. These validations suggest individual primitives are directionally correct—and combining them may provide additional analytical value, such as enriching productivity estimates with task success rates or constructing new measures of occupational exposure.\n\nUltimately, the strongest validation will come from the primitives’ ability to capture meaningful variation in labor market outcomes. The data we release enable external researchers to analyze economic shifts in new ways. Early work has been encouraging—the automation/augmentation distinction from prior reports has already been used by external researchers to analyze labor market shifts (Brynjolfsson, Chandar & Chen, 2025).\n\nTo illustrate how the primitives distinguish between different types of AI use, we examine two contrasting request clusters: software development (\"Help debug, develop, and optimize software across multiple programming domains\") and personal life management (\"Assist with personal life management and everyday tasks\"). Figure 2.2 shows the primitive profile for each cluster alongside global averages.\n\nTask complexity. Claude estimates that software development requests would take a competent professional approximately 3.3 hours to complete without AI—close to the global average of 3.1 hours. Personal life management tasks are estimated to be simpler, averaging 1.8 hours. Estimated human-AI collaboration time is similar across both (~15 minutes), showing this primitive varies less than other primitives for these two tasks.\n\nHuman and AI skills. Software development requests draw on more specialized knowledge: both human prompts and AI responses are estimated to require approximately 13.8 years of education to understand, compared to 9.1–9.4 years for personal life management requests. Claude estimates that users would be able to complete personal life management requests by themselves 96% of the time, versus 82% for software development requests—indicating that Claude provides more essential support for technical work.\n\nUse case. Claude classifies 64% of software development requests as work-related, compared to just 17% for personal life management. This illustrates that Claude can be used for very different purposes. Overall, Claude.ai use is 46% work, 19% coursework, and 35% personal.\n\nAI autonomy. Both clusters show similar estimated autonomy levels (~3.5 on a 1 to 5 scale), near the global average. This means that both software development and personal life management tasks, on average, afford Claude a similar autonomy to make decisions on how to complete the task.\n\nTask success. Claude assesses personal tasks as successfully completed 78% of the time, versus 61% for software development. Harder tasks—those requiring more specialized knowledge and where users could not easily complete them alone—show lower estimated success rates.\n\nAs in our previous report, we find major differences in the tasks and primitives in Claude.ai conversations compared to the 1P API data. Part of this reflects the nature of the interaction: Claude.ai transcripts can include multi-turn conversations, while the API data we analyze is limited to single input-output pairs. This is because API requests arrive independently, with no metadata linking them to prior exchanges. This means we can only analyze them as isolated user-assistant pairs rather than full conversation trajectories.\n\nOverall, API usage is overwhelmingly work-related (74% vs. 46%) and directive (64% vs. 32%), with three-quarters of interactions classified as automation compared to less than half on Claude.ai (see Figure 1.3).\n\nClaude.ai users, by contrast, engage in more back-and-forth: task iteration and learning modes are far more common, and tasks tend to be more lengthy—both in terms of human time with AI (15 minutes vs. 5 minutes) and the estimated time a human would need to complete the task alone (3.1 hours vs. 1.7 hours). Claude.ai also shows higher task success rates (67% vs. 49%), which may reflect the benefits of multi-turn conversation, where users can clarify, correct course, and iterate toward a solution. Claude.ai users also give the AI more autonomy on average, and are more likely to bring tasks they couldn't complete alone.\n\nThese differences are also reflected in the occupational distribution of tasks. API usage is heavily concentrated in Computer & Mathematical tasks (52% vs. 36%), consistent with its use for programmatic, automation-friendly workflows like code generation and data processing. Office & Administrative tasks are also more prevalent in the API (15% vs. 8%), reflecting routine business operations suited to delegation. Claude.ai, by contrast, sees substantially more Educational Instruction tasks (16% vs. 4%)—coursework help, tutoring, and instructional material development—as well as more Arts, Design, and Entertainment tasks (11% vs. 6%). Claude.ai also has a longer tail of human-facing categories like Community & Social Service and Healthcare Practitioners, where users seek advice, counseling, or information on personal matters.\n\nThese patterns suggest that 1P API deployments concentrate on tasks amenable to systematic automation, while Claude.ai serves a broader range of use cases including learning, creative work, and personal assistance.\n\nChapter 4 explores task-level variation in greater depth.\n\nA classifier is a model that assigns a given input (e.g. a user conversation) a specific output (e.g. the use case “work”). In this report, we use Claude as a classifier, meaning that we prompt Claude to select a specific output and then use Claude’s response as the output (see Table 2.1 for the prompts).\n\nThroughout this report, we use binned scatterplots to show bivariate relationships. We divide observations into 20 equally-sized bins based on the x variable, then plot the average x and y values for each bin. The leftmost dot, for example, represents the averages for observations in the lowest 5% of the x distribution.\n\nIn this chapter, we analyze geographic variation in Claude usage patterns using a privacy-preserving¹ analysis of 1 million Claude.ai conversations². We make five observations:\n\nOur data, relying on a privacy-preserving analysis of 1 million Claude.ai conversations, reveals striking geographic differences in how Claude is adopted. Claude is predominantly used for work, across the globe and across the United States. However, there is geographic variation in use cases. At the global level, the Balkans and Brazil have the highest relative share of work use (see Figure 3.1), and Indonesia stands out with the highest share of coursework. At the US state level, New York stands out as the state using Claude relatively the most for work.\n\nUse case differences are related to a country’s per capita income, which, in turn, is related to per capita AI adoption. We observe that work use cases and personal use cases of Claude are more common in higher income countries, while coursework use cases are more common in lower income countries (see Figure 3.2). Interestingly, these findings converge with recent work by Microsoft showing that AI use for school is associated with lower per capita income, whereas AI use for leisure is associated with higher per capita income.\n\nMultiple factors could contribute to these patterns:\n\nThe economic primitives introduced in this report allow us to analyze some of the factors that may drive differential adoption. When analyzing the relationship between the Anthropic AI Usage Index (AUI) and core economic primitives as well as GDP, we observe that certain patterns hold for both countries and US states. For example, we replicate the finding from our prior report that GDP is strongly correlated with the AUI (see Figures 3.3 and 3.4). At the country level, a 1% increase in GDP per capita is associated with a 0.7% increase in Claude usage per capita. Human education (how many years of education it takes to understand the human written prompts in a conversation) correlates positively and significantly with the Anthropic AI Usage Index both at the country and at the US state level.\n\nHowever, the relationship between AUI and the primitives often differs between country and US state level. For example, at the country level, the AUI correlates negatively with the time it would take a human to complete a task without AI, and with how much decision-making autonomy AI is given. At the US state level, these relationships are not statistically significant–likely also due to the smaller sample size for US states. Additionally, we observe a positive correlation between the AUI and Claude.ai use for work at the US state, but not at the country level.\n\nImportantly, the primitives themselves are not necessarily causal factors—we don't know if income or education are truly driving adoption, or if they're proxies for other underlying conditions. Many of these factors are highly correlated with one another. For example, at the US state level, human education years show a strong association with the Anthropic AI Usage Index in isolation, but this relationship disappears once we control for GDP and other primitives—suggesting education may be capturing variation that's better explained by economic development and other factors.\n\nEconomic and institutional context—such as how education levels vary within a geography—are related to how AI is being used. Interestingly, we observe that task success is negatively associated with human education at the country level, but positively related at the US state level. However, the positive relationship at the state level becomes insignificant when controlling for other primitives (see Figure 3.5). This means the relationship pattern at one level of observation (country) contradicts the relationship pattern at another level (US state). Cross-country, educated populations may attempt harder tasks and therefore see lower success rates. Within homogeneous contexts, education may not improve task success.\n\nWe find a very high correlation between human and AI education, i.e. the number of years of education required to understand a human prompt or the AI’s response (countries: r = 0.925, p < 0.001, N = 117; US states: r = 0.928, p < 0.001, N = 50). This highlights the importance of skills and suggests that how humans prompt the AI determines how effective it can be. This also highlights the importance of model design and training. While Claude is able to respond in a highly sophisticated manner, it tends to do so only when users input sophisticated prompts.\n\nHow models are trained, fine-tuned and instructed affects how they respond to users. For example, one AI model could have a system prompt that instructs it to always use simple language that a middle school student could understand, whereas another AI model may only respond in complex language that would require a PhD education to understand. For Claude, we observe a more dynamic pattern where how the user prompts Claude relates to how Claude responds.\n\nHigher per capita usage countries, which tend to be higher per capita income countries, show lower automation, and less decision-making autonomy delegated to Claude. That is, higher income countries use AI more as an assistant and collaborator rather than letting it work independently. This relationship is not significant at the US state level, perhaps because income variation and use case diversity are more limited within the United States than globally. This mirrors a finding from our 3rd Economic Index report where countries with higher Anthropic AI Usage Index tend to use Claude in a more collaborative manner (augmentation), rather than letting it operate independently (automation).\n\nThe striking geographic variation in our data shows that Claude is used in different ways around the world. GDP predicts the Anthropic AI Usage Index at both the country and US state level, and human education—the sophistication of user prompts—correlates with adoption at both levels as well.\n\nOther relationships depend on context. At the country level, higher usage correlates with shorter tasks and less AI autonomy; within the US, these patterns do not hold. Task success and human education show opposite relationships globally versus within the US.\n\nThe near-perfect correlation between human and AI education years underscores that how users prompt Claude shapes how it responds. Combined with the finding that higher-usage countries engage Claude more collaboratively, this suggests that the skills required to use AI well may themselves be unevenly distributed.\n\nFor privacy reasons, our automated analysis system filters out any cells—e.g., countries, and (country, task) intersections—with fewer than 15 conversations and 5 unique user accounts. For bottom-up request clusters, we have an even higher privacy filter of at least 500 conversations and 250 unique accounts.\n\nData in this section covers 1 million Claude.ai Free, Pro and Max conversations from November 13 to 20, 2025, randomly sampled from all conversations in that period. We then excluded content that was flagged as potential trust and safety violations. The unit of observation is a conversation with Claude on Claude.ai, not a user, so it is possible that multiple conversations from the same user are included, though our past work suggests that sampling conversations at random versus stratified by user does not yield substantively different results. Aggregate geographic statistics at the country and US state level were assessed and tabulated from the IP address of each conversation. For geolocation, we use ISO-3166 codes since our provider for IP geolocation uses this standard. International locations use ISO-3166-1 country codes, US state level data use ISO-3166-2 region codes, which include all 50 US states and Washington DC. We exclude conversations originating from VPN, anycast, or hosting services, as determined by our IP geolocation provider.\n\nThe world map is based on Natural Earth’s world map with the ISO standard point of view for disputed territories, which means that the map may not contain some disputed territories. We note that in addition to the countries shown in gray (“Claude not available”), we do not operate in the Ukrainian regions Crimea, Donetsk, Kherson, Luhansk, and Zaporizhzhia. In accordance with international sanctions and our commitment to supporting Ukraine’s territorial integrity, our services are not available in areas under Russian occupation.\n\n“No data” applies to countries with partially missing data. Some territories (e.g., Western Sahara, French Guiana) have their own ISO-3611 code. Some of these have some usage, others have none. Since the Anthropic AI Usage Index is calculated per working-age capita based on working age population data from the World Bank, and population data is not readily available for all of these territories, we cannot calculate the AUI for these territories.\n\nWe exclude the Seychelles from all geographic analyses because a large fraction of usage we saw during the sampling dates was abusive traffic.\n\nWe exclude Wyoming from all US state analyses because a large fraction of usage we saw during the sampling dates was abusive traffic.\n\nIn this chapter, we examine how time savings, success rates, and autonomy vary across task types, and what this entails for potential impacts on jobs and productivity.\n\nThe patterns reveal that more complex tasks yield greater time savings, but that this trades off against reliability. In a simple task removal exercise inspired by Autor and Thompson (2025), Claude's tendency to cover higher-education tasks produces a net deskilling effect across most occupations, as the tasks AI handles are often the more skilled components of a job.\n\nClaude usage spans a meaningful fraction of tasks across a growing share of occupations. We incorporate success rates into a richer model of job coverage; some occupations with modest coverage see large effects because AI succeeds on their most time-intensive work. Adjusting productivity estimates for task reliability roughly halves the implied gains, from 1.8 to about 1.0 percentage points of annual labor productivity growth over the next decade. However, these estimates reflect current model capabilities, and all signs suggest that reliability over increasingly long-running tasks will improve.\n\nOur estimates suggest that, in general, the more complex tasks in our data yield a greater time savings (or “speedup”) from AI. We derive this by having Claude estimate both how long a task would take a human working alone and the duration when human and AI work together, which we validated in previous work. Speedup is then the human-alone time divided by the human-with-AI time. So reducing a 1 hour task to 10 minutes would give a 6x speedup.\n\nThe left panel of Figure 4.1 below gives the average speedup against our core measure of task complexity, the human years of schooling required to understand the inputs, all at the O*NET task level. It shows that in Claude.ai conversations, for example, prompts requiring 12 years of schooling (a high school education) enjoy a speedup of 9x, while those requiring 16 years of schooling (a college degree) attain a 12x speedup. This implies that productivity gains are more pronounced for use cases requiring higher human capital, consistent with evidence that white collar workers are far more likely to adopt AI (e.g., Bick et al 2025).\n\nThroughout the range of task complexity, the speedup is higher for API users. This could reflect the nature of the API data, which is restricted to single-turn interactions, and that API tasks have been specifically selected for automation.\n\nThe results also capture a tradeoff, however. More complex tasks have a lower task success rate, as shown in the panel on the right. On Claude.ai, for example, tasks requiring less than a high school education (e.g., answering basic questions about products) attain a 70% success rate, but this drops to 66% for college-level conversations like developing analysis plans. Still, accounting for the difference in success rates—by either excluding low-success tasks or discounting speedups by success probability—does not eliminate the education gradient: complex tasks still show greater net productivity gains.\n\nOne way to examine the implications of the education gradient is to look at the share of automation across the education levels required to understand the inputs. If high-education tasks show relatively more automation, it could signal more exposure for white collar workers. Here, though, the message is unclear: the automation share is essentially unrelated to the human levels of education required to write the prompt (Appendix Figure A.1). On both Claude.ai and 1P API, tasks across education levels show automation patterns in roughly equal shares.\n\nIn what contexts do users defer more to Claude? Claude.ai users give the AI slightly more autonomy when working on more complex tasks. In contrast, API usage shows uniformly lower autonomy at all levels of complexity.\n\nNote though that these distributions do not span the same set of tasks. API usage covers a more narrow swath of tasks in the economy, as seen in the concentration plot in Chapter 1. The high education tasks that experience heavy usage in the API data include security analysis, testing and quality assurance, and code review, whereas Claude.ai users are more likely to have iterative, instructive sessions.\n\nRecent work on AI “task horizons” (Kwa et al., 2025) finds that AI success rates decline with task duration: longer tasks are harder for models to complete. With each successive model generation, however, this decline has become shallower as models succeed on increasingly long tasks. METR operationalizes task horizon primarily as the maximum duration at which a model achieves at least 50% success, and growth in this metric has become a key indicator of AI progress.\n\nFigure 4.3 shows a similar measure using our primitives. The plot shows task-level success rates against the human time required, all at the O*NET task level. In the API data, success rates drop from around 60% for sub-hour tasks to roughly 45% for tasks estimated to take humans 5+ hours. The fitted line crosses the horizontal 50% success line at 3.5 hours, suggesting that API calls attain a 50% success rate for tasks that are 3.5 hours. The analogous time estimate in METR’s software engineering benchmark is 2 hours for Sonnet 4.5 and about 5 hours for Opus 4.5. (The data in this report predates the release of Opus 4.5.)\n\nClaude.ai data tells a different story. Success rates decline far slower as a function of task length. Extrapolating using the linear fit, Claude.ai would hit a 50% success rate at about 19 hours. This may reflect how multi-turn conversation effectively breaks complex tasks into smaller steps, with each turn providing a feedback loop that allows users to correct course.\n\nIt’s worth noting that a fundamental difference from the METR setting is selection. METR constructs a benchmark where a fixed set of tasks is assigned to models. In our data, users choose which tasks to bring to Claude. This means observed success rates reflect not just model capability but also user judgment about what will work, the cost of setting up the problem for Claude, and the expected time savings if the task succeeds.\n\nIf users avoid tasks they expect to fail, for example, observed success rates will overstate true capability on the full distribution of potential tasks. This selection likely operates on both platforms, but in different ways: API customers select for tasks amenable to automation, while Claude.ai users select for tasks that could benefit from iteration. Also due to this selection effect, there’s no guarantee that more performant models would show improvement in this plot, because users may respond to new models by providing more challenging presentations of otherwise similar O*NET tasks.\n\nControlled benchmarks like METR’s measure the frontier of autonomous capability. Our real-world data can measure the effective task horizon, reflecting a mix of model capabilities and user behavior, and expanding beyond coding tasks. Both approaches find that AI can be effective for tasks requiring hours of human work.\n\nOur earlier work found that 36% of jobs had AI usage for at least a quarter of their tasks, with about 4% reaching 75% task coverage. This measure was based only on the appearance of a task in our data, however. The primitives introduced in this report can help better characterize how AI is changing the work content of occupations.\n\nFirst, we find that task coverage is increasing. Combining across reports, 49% of jobs have seen AI usage for at least a quarter of their tasks. But incorporating that task’s share of the job, and Claude’s average success rate, suggests a different set of affected occupations.\n\nWe define effective AI coverage as the percent of a worker’s day that can be performed successfully by Claude. It’s calculated as the weighted sum of task success rates, where each task's weight is its share of the worker's time adjusted by how frequently the task occurs. The success rate comes from our primitives, the hours estimate from our previous work on productivity effects, and the frequency estimate from O*NET data, where surveyed workers indicate how often they perform the task.\n\nThe plot below shows how the effective AI coverage (y-axis) differs from task coverage alone (x-axis). The two are highly correlated, but with key differences. On the right side of the plot, occupations with high coverage—where almost all tasks appear with some frequency in Claude data—generally fall below the 45-degree line. This suggests that even 90% task coverage does not necessarily indicate large job impacts, since Claude may fail on key covered tasks or miss the most time-intensive ones.\n\nZooming in, several occupations show large differences in effective AI coverage compared to task coverage. For example, data entry workers have one of the highest effective AI coverage. This is because although only two of their nine tasks are covered, their largest task—reading and entering data from source documents—has high success rates with Claude. AI excels at what they spend most of their time doing.\n\nMedical transcriptionists and radiologists also move up because their covered tasks happen to be their most time-intensive and highest-frequency work. For radiologists, their top two tasks— interpreting diagnostic images and preparing interpretive reports—have high success rates. These occupations have low task coverage because AI can't do the hands-on or administrative work in their job profiles, but it succeeds on the core knowledge work that dominates their workday.\n\nMicrobiologists fall below the 45-degree line, suggesting lower effective AI coverage than would be predicted by task coverage alone. Claude covers half of their tasks, but not their most time-intensive: hands-on research using specialized lab equipment.\n\nThis measure arguably gives a more realistic picture of job-level AI penetration. However, its implications depend on how often these Claude conversations actually displace or augment work that would otherwise be done by humans. For data entry clerks, AI likely does substitute for tasks previously performed manually. But when a Claude conversation maps to a teacher performing a lecture, it is less clear how this translates to reduced lecture time on the job. In future work, we could leverage our 1P API data to understand which of these tasks are being integrated into production workflows.\n\nBeyond how much of a worker's day AI can successfully perform, a separate question is which tasks get covered, and whether those tend to be the high-skill or low-skill components of the job. Recent research has studied changes in the task mix within jobs to understand AI's impact on wages and employment (Autor and Thompson 2025; Hampole et al 2025). A key insight is that automation's effects depend not just on how many tasks are covered, but on which tasks.\n\nTo see how jobs change when we remove the tasks AI can perform, we first construct a measure of the level of skill required for each task. O*NET doesn't provide task-level education requirements, so we train a model that predicts years of schooling from task embeddings, using the BLS's occupation-level education as the target. This way, a low-education occupation may still have a high-skill task if it looks like those that tend to exist in high-education occupations. For example, Legal Secretaries is a 12-year education occupation, but the task “Review legal publications and perform database searches to identify laws and court decisions relevant to pending cases” is predicted to require 17.7 years because it resembles tasks typically performed by lawyers and paralegals.\n\nThe data shows that Claude tends to cover tasks that require higher levels of education. The mean predicted education for tasks in the economy is 13.2 years. For tasks that we see in our data, the mean prediction is about a year higher, 14.4 years (corresponding to an Associate’s degree). This aligns with the occupation-level results from earlier reports, showing more Claude usage among white collar occupations.\n\nWe next calculate how removing AI-covered tasks shifts the average education level of what remains. Overall, the net first-order impact is to deskill jobs, since AI removes tasks that require relatively higher levels of education. One job that experiences such deskilling is technical writers, which loses tasks like \"Analyze developments in specific field to determine need for revisions\" (18.7 years) and \"Review published materials and recommend revisions or changes in scope, format\" (16.4 years), leaving tasks like \"Draw sketches to illustrate specified materials\" (13.6 years) and \"Observe production, developmental, and experimental activities\" (13.5 years). Travel agents also experience deskilling because AI covers tasks like \"Plan, describe, arrange, and sell itinerary tour packages\" (13.5 years) and \"Compute cost of travel and accommodations\" (13.4 years), while tasks like \"Print or request transportation carrier tickets\" (12.0 years) and \"Collect payment for transportation and accommodations\" (11.5 years) remain. Several teaching professions experience deskilling because AI addresses tasks like grading, advising students, writing grants, and conducting research without being able to do the hands-on work of delivering lectures in person and managing a classroom.\n\nSome jobs see average education levels increase. Real estate managers experience upskilling because AI covers routine administrative tasks—maintaining sales records (12.8 years), reviewing rents against market rates (12.6 years)—while tasks requiring higher-level professional judgment and in-person interaction remain, like securing loans, negotiating with architecture firms, and meeting with boards.\n\nThese patterns illustrate how jobs may evolve over the coming years as their task content adjusts in response to AI. If the education level can be interpreted like expertise in Autor and Thompson's analysis, their framework might predict that wages will fall and employment will increase for technical writers and travel agents; conversely, real estate managers will specialize in complex negotiations and stakeholder management, shrinking employment while increasing wages.\n\nHowever, our education-based measure differs from Autor and Thompson's expertise concept: their framework would label some tasks as high expertise where ours specifies low education—for example, the Electrician task \"Connect wires to circuit breakers, transformers, or other components.\" And these predictions are based on current Claude usage patterns, which will shift as models are trained on new capabilities and users discover new applications—potentially changing which tasks are covered and whether the net effect is deskilling or upskilling.\n\nIn earlier work, we estimated that widespread adoption of AI could increase US labor productivity growth by 1.8 percentage points annually over the next decade. Here we revisit that analysis, incorporating the task success primitive introduced in this report and a richer treatment of task complementarity.\n\nBased on the speedups associated with tasks with at least 200 observations in our sample of 1M Claude.ai conversations, we replicate our previous finding that current-generation AI models and current usage patterns imply a productivity effect of 1.8 percentage points per year over the next decade.\n\nWith the inclusion of 1P API data, we can assess whether implied labor productivity effects differ based on enterprise Claude deployment patterns. Two countervailing forces are at play: API usage is more concentrated in a narrower set of tasks and occupations (particularly coding-related work), which would tend to reduce implied effects; but task-level speedups are higher on average among API tasks, as implied by Figure 4.1. These forces largely offset: the API sample likewise implies a 1.8 percentage point increase in labor productivity over the next decade.\n\nA salient critique of this analysis is that it fails to account for model reliability. If workers must validate AI output, the productivity benefits will be smaller than raw speedups suggest. To assess how quantitatively important this channel might be, we incorporate the task success primitive introduced in this report, multiplying task-level time savings by task-specific success rates before aggregating.\n\nThis adjustment has a meaningful effect: implied productivity growth falls from 1.8 to 1.2 percentage points per year for the next decade based on Claude.ai usage, and to 1.0 percentage points for API traffic. Yet, even after accounting for reliability, the implied impact remains economically significant—a sustained increase of 1.0 percentage point per year for the next ten years would return US productivity growth to rates that prevailed in the late 1990s and early 2000s.A second critique concerns task complementarity. If some tasks are essential and cannot easily be substituted, then overall productivity effects will be constrained regardless of speedups on other tasks. Teachers may prepare lesson plans more efficiently with AI while having no impact on time spent with students in the classroom.\n\nTo operationalize this idea, we impose some structure on how we aggregate task-level time savings within occupations but otherwise add up occupational efficiency gains as in the main analysis. Specifically, we suppose that within each occupation tasks are combined according to a Constant Elasticity of Substitution (CES) aggregator, where each task is weighted by the estimated time spent on each task as calculated in our earlier analysis of the productivity effects implied by Claude usage.\n\nThe key parameter is the elasticity of substitution across tasks, σ. When the elasticity of substitution is less than one, tasks are complements and those tasks that are not sped up by AI become bottlenecks for broader productivity gains. Alternatively, when the elasticity of substitution is greater than one, then workers can allocate toward the more productive tasks—thereby amplifying the overall time savings at the occupational level. An elasticity of substitution equal to one is a special case that replicates the main analysis above.\n\nFigure 4.6 reports the results of this exercise for different values of task substitutability. As expected, when the elasticity of substitution is equal to one the implied productivity effect is the same as in our baseline analysis: An increase in labor productivity growth of ~1.8 percentage points per year over the next decade implied by both Claude.ai and API samples.\n\nWhen tasks are complements, however, the implied aggregate labor productivity impact declines sharply as the economic effects are bottlenecked by tasks that AI speeds up the least. For example, at =0.5 the implied overall labor productivity effect is 0.7-0.9 percentage points per year—around half the size as implied by our baseline estimates. Additionally adjusting for task success further reduces the implied productivity effects to 0.8pp for Claude.ai and 0.6pp for API.\n\nOn the other hand, when the elasticity of substitution is greater than one, the implied labor productivity based on pre-Opus 4.5 usage patterns is materially higher. For example, at =1.5 the implied labor productivity effect rises to 2.2-2.6 percentage points per year, consistent with greater specialization in tasks where AI provides the largest speedups.\n\nIn both cases the implied productivity impact based on API traffic is more responsive to the degree of task substitutability. This is consistent with the fact that there is a larger share of API traffic concentrated in fewer tasks and associated occupations as compared to Claude.ai: When tasks are complements, this concentration amplifies the bottleneck problem; when they are substitutes, it amplifies productivity gains from task specialization.\n\nWhat this analysis shows is that the productivity effects of automation may ultimately be constrained by bottleneck tasks that elude AI automation for the time being. And the labor market implications of increasingly capable AI could be similarly affected by such forces. For example, Gans and Goldfarb (2026) argue that the presence of bottleneck tasks within jobs means that partial AI automation can lead to an increase in labor income as such tasks increase in economic value (at least until a job is entirely automated).\n\nThe upshot of this chapter is that the impact of AI on the economy is unlikely to be uniform. As our effective AI coverage framework illustrates, the labor market implications for different workers will hinge on how reliable frontier AI tools are for their most central tasks.\n\nBut the labor market effects may also depend on the skill requirements of tasks that AI can proficiently handle relative to the rest of the economy. Indeed, we find that removing tasks Claude can already handle from the economy would produce a net deskilling effect: the tasks remaining for humans have lower educational requirements than those handled by AI.\n\nWhile highly suggestive, this may miss an important detail: the most complex tasks where Claude is used tend also to be those where it struggles most. Rather than displacing highly skilled professionals, this could instead reinforce the value of their complementary expertise in understanding AI's work and assessing its quality.\n\nThe counterpart to these transformative labor market effects is the broader impact on growth and productivity. On the one hand, incorporating task reliability into our analysis diminishes the implied effect on labor productivity growth as informed by current Claude usage patterns. If bottleneck tasks bind, the implied impact diminishes further. On the other hand, the continuing growth in model capabilities suggests that both task coverage and task success may increase, which, in turn, could increase productivity impacts.\n\nWhen we study the correlation between primitives with the O*NET, we restrict to tasks appearing in at least 100 conversations to reduce measurement error. In the coverage analysis, we use all tasks above the privacy threshold of 15.\n\nOur online appendix is available at https://huggingface.co/datasets/Anthropic/EconomicIndex.\n\nSee also Tomlinson et al (2025) for a related AI applicability score.\n\nWe generate embeddings for each task statement using a pretrained sentence transformer (all-mpnet-base-v2) and predict education with Ridge regression.\n\nOn the other hand, some historical evidence suggests that when technologies automating job tasks appear in patent data, employment and wages subsequently fall for exposed occupations (Webb 2020).\n\nWhen we first assessed the aggregate productivity implications of Claude usage, we relied on a sample of 100k Claude.ai conversations from Fall 2025. Based on the set of tasks for which we observed speedups, we estimated that labor productivity could be 1.8 percentage points higher per year over the next decade. Expanding the sample to 1M observations means that we need to take a stand on how to handle very infrequently occurring tasks—which are very common given that usage follows a power law, as we documented in our past report. We choose a threshold of 0.02% because it replicates our previous results for our sample of Claude.ai conversations. For privacy-preserving reasons, we only ever analyze tasks with at least 15 observations, or an implied threshold of 0.015% for a 100k sample. And so our results are internally consistent across samples. If we do not impose a restriction on our 1M sample and assume that efficiency gains for any task in our sample, even those with just 15 observations out of one million, the implied aggregate labor productivity growth over the next decade would be roughly 5% percentage points per year—a mechanical increase based on a the much larger set of tasks included.\n\nAs before, this result is based on applying Hulten’s Theorem to task-level productivity shocks and assuming that the corresponding one-time increase in total factor productivity materializes over the course of a decade alongside capital deepening effects.\nAs a reminder, for aggregating to implied labor productivity we calculate task-level efficiency gains as the log difference between human time without AI and with AI. There are certainly other ways to adjust based on task reliability. If tasks in our sample are composed of sub-tasks with heterogeneous AI applicability, and workers optimally deploy AI only on sub-tasks where it is effective, then scaling the efficiency gain by the success rate captures the extensive margin of AI adoption within a task.\n\nWe use a CES (constant elasticity of substitution) production function to aggregate task-level time savings to economy-wide productivity impacts. The elasticity parameter σ governs how easily workers can substitute between tasks. When σ=1, we apply Hulten's theorem directly: the aggregate productivity gain equals the wage-share-weighted sum of log speedups across tasks. For σ≠1, we use a two-level aggregation: first, within each occupation, we compute an occupation-level speedup as a CES aggregate of task speedups weighted by time fractions, using ρ=(σ-1)/σ. Then we apply Hulten's theorem to these occupation-level speedups. When σ<1 (complements), productivity gains are bottlenecked by tasks with the smallest speedups. When σ>1 (substitutes), workers can specialize in tasks where AI provides the largest speedups, amplifying aggregate gains. For tasks without observed AI speedup data, we assume no productivity change. We thank Pascual Restrepo for suggesting this particular exercise.\n\nThis fourth Anthropic Economic Index Report introduces economic primitives—foundational characteristics of AI use—that show how Claude is used by both consumers and firms. We use Claude to estimate the extent to which usage varies along these dimensions; these measures are directionally accurate and, taken together, provide important signals even if individual classifications are imperfect.\n\nOur findings carry significant implications for how AI will reshape economies and labor markets. Notably, Claude tends to be used more, and appears to provide greater productivity boosts, on tasks that require higher education. If these tasks shrink for US workers, the net effect could be to deskill jobs. But these impacts depend crucially on complementarity across tasks, and whether increased productivity at a certain task may increase the demand for it.\n\nAt the global level, the strong relationship between per capita income and usage patterns—with higher-income nations using Claude collaboratively while lower-income countries focus on coursework and specific applications—suggests that AI's impact will be mediated by existing institutional structures rather than unfolding uniformly. Geographic diffusion patterns reinforce this picture. Within the US, per capita usage has converged slightly; globally, diffusion is slower. Combined with income-driven differences in how AI is used, this raises questions about whether AI will narrow or widen international economic gaps.\n\nEqually important to the patterns documented here are potential changes across this and subsequent reports. As AI capabilities advance, Claude's success rate may increase, usage patterns may show greater autonomy, users may tackle new and more complex tasks, and tasks that prove automatable may graduate from interactive chat to API deployment. We will track these dynamics over time, providing a longitudinal view of AI's role in the economy.\n\nBuilding on prior releases, this edition significantly expands both the scope and transparency of usage data we share, including task-level classifications along new dimensions and regional breakdowns globally for the first time. We publish this data to enable researchers, journalists, and the public to investigate novel questions about AI's economic impacts that can form the empirical foundation for policy responses.\n\nHow willing users are to experiment with AI, and whether policymakers create a regulatory context that advances both safety and innovation, will shape how AI transforms economies. For AI to benefit users globally, expanding access alone will not suffice—developing the human capital that enables effective use, particularly in lower-income economies, is essential.\n\nRuth Appel, Maxim Massenkoff, Peter McCrory\n\nMiles McCain, Ryan Heller, Tyler Neylon, Alex Tamkin\n\nXabi Azagirre, Tim Belonax, Keir Bradwell, Andy Braden, Dexter Callender III, Sylvie Carr, Miriam Chaum, Ronan Davy, Evan Frondorf, Deep Ganguli, Kunal Handa, Andrew Ho, Rebecca Jacobs, Owen Kaye-Kauderer, Bianca Lindner, Kelly Loftus, James Ma, Jennifer Martinez, Jared Mueller, Kelsey Nanan, Kim O'Rourke, Dianne Penn, Sarah Pollack, Ankur Rathi, Zoe Richards, Alexandra Sanderford, David Saunders, Michael Sellitto, Thariq Shihipar, Michael Stern, Kim Withee, Mengyi Xu, Tony Zeng, Xiuruo Zhang, Shuyi Zheng, Emily Pastewka, Angeli Jain, Sarah Heck, Jared Kaplan, Jack Clark, Dario Amodei",
    "readingTime": 53,
    "keywords": [
      "free pro",
      "brynjolfsson chandar",
      "chandar chen",
      "apply hulten's",
      "entertainment sports",
      "hulten's theorem",
      "arts design",
      "max conversations",
      "sls estimates",
      "automation/augmentation distinction"
    ],
    "qualityScore": 1,
    "link": "https://www.anthropic.com/research/anthropic-economic-index-january-2026-report",
    "thumbnail_url": "https://www.anthropic.com/api/opengraph-illustration?name=Hand%20NodeLine&backgroundColor=cactus",
    "created_at": "2026-01-15T18:23:59.578Z",
    "topic": "science"
  },
  {
    "slug": "trump-imposes-25-tariff-on-nvidia-ai-chips-and-others-citing-national-security",
    "title": "Trump imposes 25% tariff on Nvidia AI chips and others, citing national security",
    "description": "The order follows a nine-month investigation and includes broad exemptions for datacenters and consumers\nDonald Trump on Wednesday imposed a 25% tariff on certain AI chips, such as the Nvidia H200 AI processor ​and a similar semiconductor from AMD called the MI325X, under a new national security order released by the White House.\nThe proclamation follows a nine-month investigation under ‌section 232 of the Trade Expansion Act of 1962 and targets a number of high-end semiconductors meeting certain performance benchmarks and devices containing them for import duties. The action is part of a broader effort to create incentives for chipmakers to produce more semiconductors in the US and decrease reliance on chip manufacturers in places such as Taiwan.\n Continue reading...",
    "fullText": "The order follows a nine-month investigation and includes broad exemptions for datacenters and consumers\n\nDonald Trump on Wednesday imposed a 25% tariff on certain AI chips, such as the Nvidia H200 AI processor ​and a similar semiconductor from AMD called the MI325X, under a new national security order released by the White House.\n\nThe proclamation follows a nine-month investigation under ‌section 232 of the Trade Expansion Act of 1962 and targets a number of high-end semiconductors meeting certain performance benchmarks and devices containing them for import duties. The action is part of a broader effort to create incentives for chipmakers to produce more semiconductors in the US and decrease reliance on chip manufacturers in places such as Taiwan.\n\n“The United States currently fully manufactures only approximately 10 percent of the chips it requires, making it heavily reliant on foreign supply chains,” the proclamation said, adding that the reliance was a “significant economic and national ‌security risk”.\n\nThe White House said in a fact sheet that the tariffs would be narrowly focused and would not apply ​to chips and derivative devices imported for US datacenters – a huge consumer of AI chips – startups, non-datacenter consumer applications, non-datacenter civil industrial applications and US public sector applications.\n\nHoward Lutnick, the US commerce secretary, has broad discretion to apply further exemptions, according to the proclamation.\n\nShares of Nvidia, AMD and Qualcomm traded slightly lower in after-hours trading.\n\nTrump ‍in December said he would slap tariffs on Chinese semiconductor imports over Beijing’s “unreasonable” pursuit of chip industry dominance, but delayed the action until June 2027.\n\nThat move followed a year-long “Section 301” unfair trade practices investigation into China’s exports of “legacy”, or older-technology chips to the US, launched by former president Joe Biden’s administration.\n\nQuestions had swirled about the universe of products containing chips that would be ⁠hit by the tariffs, the tariff rates, and whether any countries, products or companies would be exempt. Wednesday’s announcement, coupled with the news from December, suggests a ‍light touch from the administration on chip imports, for now.\n\nTrump last year announced he would allow Nvidia to sell H200 chips to China in exchange for a cut of the sales of ‌those chips. Legal ‌experts questioned whether such an arrangement would violate the US constitution’s ban on taxing exports.\n\nThe Trump administration this week required that China-bound chips make a detour from Taiwan, where they are made, through the United States for testing by a third-party lab. When the chips enter the United States, they are subject to the 25% tariff announced on Wednesday.\n\nNvidia did not immediately respond to a request for comment.\n\n“We comply with all US export control laws and policies,” AMD said in a statement.\n\nTrump has deployed an ⁠array of tariffs aimed at bolstering ⁠US manufacturing, announcing in September sweeping ​new import tariffs, including 100% duties on branded drugs and 25% levies on heavy-duty trucks, triggering fresh trade uncertainty after a period of relative calm.\n\nIn April, the Trump administration announced investigations into imports of pharmaceuticals and semiconductors as part of an effort to impose tariffs on them, arguing that extensive reliance on their foreign production poses a national security threat.\n\nWhile US companies ‍like Nvidia, AMD and Intel design many of the most widely used chips, most are made overseas, many by Taiwan Semiconductor Manufacturing Co. TSMC did not immediately respond to a request for comment and the Semiconductor Industry Association also could not immediately be reached.\n\nTrump, in the near future, may also impose broader tariffs on imports of semiconductors and their derivative products to incentivize domestic manufacturing, according ​to the fact sheet.\n\nAn annex to the order clarifies that any 25% tariff imposed on semiconductors ‍under the order would not be stacked on top of the other tariffs imposed by the Trump administration under other section 232 orders. They would be exempt from duties on copper, aluminum and steel, auto ​and truck parts.",
    "readingTime": 4,
    "keywords": [
      "white house",
      "nvidia amd",
      "trump administration",
      "follows nine-month",
      "nine-month investigation",
      "immediately respond",
      "united states",
      "chips",
      "tariffs",
      "semiconductors"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/15/trump-tariff-nvidia-ai-chips",
    "thumbnail_url": "https://i.guim.co.uk/img/media/60c16c51afb0ee89b7d02519aa2c93e163f953f3/250_0_2500_2001/master/2500.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=9360332400c702d4af93aa6edccf3199",
    "created_at": "2026-01-15T18:23:59.524Z",
    "topic": "tech"
  },
  {
    "slug": "trumps-greenlight-for-nvidia-ai-chips-to-china-draws-fire-from-lawmakers-former-officials",
    "title": "Trump’s greenlight for Nvidia AI chips to China draws fire from lawmakers, former officials",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/world-news/trumps-greenlight-for-nvidia-ai-chips-to-china-draws-fire-from-lawmakers-former-officials-4447995",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM0D15L_L.jpg",
    "created_at": "2026-01-15T18:23:58.207Z",
    "topic": "finance"
  },
  {
    "slug": "analysismusk-dealt-blow-over-grok-deepfakes-but-regulatory-fight-far-from-over",
    "title": "Analysis-Musk dealt blow over Grok deepfakes, but regulatory fight far from over",
    "description": "Elon Musk's Grok chatbot is testing Europe's ability to clamp down on deepfakes and digital undressing of images online, even after regulators scored a rare win by forcing Musk's xAI to curb the creation of sexualized images.  xAI said late on Wednesday ​it had restricted image editing for Grok AI users after the chatbot churned out thousands of sexualized images of women and minors that alarmed global regulators.  The ‌climb-down by Musk, who initially laughed off the trend, highlights the difficulty of policing AI tools that make it cheap and easy to create explicit content.",
    "fullText": "STOCKHOLM/LONDON, Jan 15 (Reuters) - Elon Musk's Grok chatbot is testing Europe's ability to clamp down on deepfakes and digital undressing of images online, even after regulators scored a rare win by forcing Musk's xAI to curb the creation of sexualized images.\n\nxAI said late on Wednesday ​it had restricted image editing for Grok AI users after the chatbot churned out thousands of sexualized images of women and minors that alarmed global regulators.\n\nThe ‌climb-down by Musk, who initially laughed off the trend, highlights the difficulty of policing AI tools that make it cheap and easy to create explicit content. It is the latest clash between Europe and Musk, following rows ‌over election interference, content moderation and free speech.\n\nMany regulators are still scrambling to develop laws and rules to govern AI, with question marks over what constitutes nudity, how to define consent, and who bears responsibility: the user or the platform.\n\n\"It's really a grey zone with regards to the creation of the nude images,\" Ängla Pändel, a Stockholm-based data protection and privacy lawyer with Mannheimer Swartling, told Reuters.\n\nBritish regulator Ofcom, one of the most vocal on the issue, welcomed the move by Musk, but said its investigation into xAI over the Grok images would continue.\n\n\"Our formal investigation remains ⁠ongoing,\" a spokesperson said. \"We are working round the clock to ‌progress this and get answers into what went wrong and what's being done to fix it.\"\n\nSTRONGEST ENFORCEMENT STILL NEEDED, OFFICIALS SAY\n\nEarlier this month, Grok created hyper-realistic images of women on X manipulated to look like they were in tiny bikinis, degrading poses or even covered in bruises. ‍Some minors were digitally stripped down to swimwear.\n\nUntil Wednesday, Reuters found the chatbot still produced sexualized images privately on demand. That appeared to have been curbed at least in certain geographies on Thursday.\n\nMusk's xAI said it was blocking users from generating images of people in skimpy attire in \"jurisdictions where it's illegal\". It did not identify those jurisdictions.\n\nIn Malaysia and Indonesia the government has imposed temporary bans on Grok, ​while EU and UK regulators called the images unlawful. The UK, France and Italy launched probes, but faced calls for tougher action.\n\n\"Stronger enforcement under the Digital Services Act (DSA) ‌is needed to stop apps and platforms that sexualise or nudify women and children,\" said Christian Democrat MEP Nina Carberry, who called the latest move a \"positive step\".\n\n⁠A European Commission spokesperson said that if the Grok changes were not effective, the Commission would still use the full enforcement toolbox of the EU's DSA against the platform.\n\nLEGAL GREY AREA, HEAVY BURDEN ON VICTIMS\n\nThe UK's Online Safety Act makes the sharing of intimate images without consent, including AI-generated deepfakes, a 'priority offence', said Alexander Brown, a UK-based data protection lawyer at Simmons & Simmons.\n\n\"This means X must take proactive, proportionate steps to prevent such content from appearing on its platform and to swiftly remove it when detected,\" he said.\n\nBritain's regulator can fine a company up to 10% ⁠of revenue in the most serious cases of non-compliance or ask a court to require internet service ​providers to block the site.\n\nFor individuals, taking platforms to court is \"a really difficult and heavy process,” said ​Anders Bergsten, a lawyer at Mannheimer Swartling, citing the emotional toll on victims.\n\nDeepfakes have existed for years, well before the advent of the AI apps, though they were largely confined to the darker corners of the web. The publishing power of X gives Grok unprecedented reach.\n\n\"The frictionless publishing capability enables ‍the deepfakes to spread at scale,\" said U.S.-based ⁠lawyer Carrie Goldberg, who works with cyber harassment victims.\n\nLaws in Britain and Sweden make the non-consensual sharing of nude images illegal. Britain is widening the law to include the making of such images.\n\nUnder the DSA, suspending a service is considered a last resort. The EU AI Act also does not have any provision for nude ⁠images of adults, only transparency obligations for deepfakes, experts said.\n\nBritish Prime Minister Keir Starmer welcomed X's move on Thursday but warned: \"Free speech is not the freedom to violate consent. Young women's images are not public property, ‌and their safety is not up for debate.\"\n\n\"If we need to strengthen existing laws further, we are prepared to do that.\"",
    "readingTime": 4,
    "keywords": [
      "musk's xai",
      "free speech",
      "sexualized images",
      "nude images",
      "mannheimer swartling",
      "deepfakes",
      "regulators",
      "lawyer",
      "chatbot",
      "women"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/analysis-musk-dealt-blow-over-174414717.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/806bb5316b8e8dba5a2515b938cb8e34",
    "created_at": "2026-01-15T18:23:55.789Z",
    "topic": "news"
  },
  {
    "slug": "x-says-grok-now-blocks-undress-photo-edits-where-theyre-illegal",
    "title": "X says Grok now blocks undress photo edits where theyre illegal",
    "description": "Elon Musk’s X says its AI chatbot Grok won't be able to edit photos of real people in revealing clothing in places where that is illegal.",
    "fullText": "Workers install lighting on an “X” sign atop the company headquarters, formerly known as Twitter, in downtown San Francisco, July 28, 2023. \n\nElon Musk listens as President Donald Trump speaks during a news conference in the Oval Office of the White House, May 30, 2025, in Washington. \n\nBANGKOK (AP) — Elon Musk’s AI chatbot Grok won’t be able to edit photos to portray real people in revealing clothing in places where that is illegal, according to a statement posted on X.\n\nThe announcement late Wednesday followed a global backlash over sexualized images of women and children, including bans and warnings by some governments.\n\nThe pushback included an investigation announced Wednesday by the state of California, the U.S.'s most populous, into the proliferation of nonconsensual sexually explicit material produced using Grok that it said was harassing women and girls.\n\nInitially, media queries about the problem drew only the response, “legacy media lies.”\n\nMusk’s company, xAI, now says it will geoblock content if it violates laws in a particular place.\n\n“We have implemented technological measures to prevent the Grok account from allowing the editing of images of real people in revealing clothing such as bikinis, underwear and other revealing attire,” it said.\n\nThe rule applies to all users, including paid subscribers, who have access to more features.\n\nxAI also has limited image creation or editing to paid subscribers only “to ensure that individuals who attempt to abuse the Grok account to violate the law or our policies can be held accountable.”\n\nGrok’s “spicy mode” had allowed users to create explicit content, leading to a backlash from governments worldwide.\n\nStay up to date with the news and the best of AP by following our WhatsApp channel.\n\nMalaysia and Indonesia took legal action and blocked access to Grok, while authorities in the Philippines said they were working to do the same, possibly within the week. The U.K. and European Union were investigating potential violations of online safety laws.\n\nFrance and India have also issued warnings, demanding stricter controls. Brazil called for an investigation into Grok’s misuse.\n\nThe British government, which has been one of Grok’s most vociferous critics in recent days, has welcomed the change, while the country’s regulator, Ofcom, said it would carry on with its investigation.\n\n“I shall not rest until all social media platforms meet their legal duties and provide a service that is safe and age-appropriate to all users,” Technology Secretary Liz Kendall said.\n\nCalifornia Attorney General Rob Bonta urged xAI to ensure there is no further harassment of women and girls from Grok’s editing functions.\n\n“We have zero tolerance for the AI-based creation and dissemination of nonconsensual intimate images or of child sexual abuse material,” he said.\n\nCalifornia has passed laws to shield minors from AI-generated sexual imagery of children and require AI chatbot platforms to remind users they aren’t interacting with a human.\n\nBut Democratic Gov. Gavin Newsom also vetoed a law last year that would have restricted children’s access to AI chatbots.\n\nPan Pylas in London contributed to this report.",
    "readingTime": 3,
    "keywords": [
      "grok account",
      "revealing clothing",
      "users",
      "grok’s",
      "images",
      "women",
      "investigation",
      "media",
      "laws",
      "editing"
    ],
    "qualityScore": 1,
    "link": "https://apnews.com/article/grok-musk-deepfake-nudification-abuse-f0d62ec68576dcfe203cada2424bd107",
    "thumbnail_url": "https://dims.apnews.com/dims4/default/608bd45/2147483647/strip/true/crop/5266x2962+3+0/resize/1440x810!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F3d%2Ffe%2Fc0d3ae4affb794c0d66dc1247b3c%2F262a5ef9c89c4342869faec34624fc00",
    "created_at": "2026-01-15T12:24:35.936Z",
    "topic": "tech"
  },
  {
    "slug": "ai-will-infiltrate-the-industrial-workforce-in-2026lets-apply-it-to-training-the-next-generation-not-replacing-them",
    "title": "AI will infiltrate the industrial workforce in 2026—let’s apply it to training the next generation, not replacing them",
    "description": "A silent crisis is shaking the very foundations of modern society. Here's how to turn it into a loud victory.",
    "fullText": "Kriti Sharma is CEO of IFS Nexus Black, which deploys AI-powered products in manufacturing, field service, energy, aerospace, and defense industries. She was previously Chief Product Officer for LegalTech at Thomson Reuters and also previously held senior AI roles at Sage Group and GfK. Her work has been recognized by the UK Prime Minister’s Points of Light award, and the United Nations, where she was named a Young Leader for her contributions to ethical and inclusive AI.",
    "readingTime": 1,
    "keywords": [
      "previously"
    ],
    "qualityScore": 0.2,
    "link": "https://fortune.com/2026/01/15/lets-train-workers-on-industrial-ai-not-replace-them-kriti-sharma/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/kriti-sharma.png?resize=1200,600",
    "created_at": "2026-01-15T12:24:35.926Z",
    "topic": "business"
  },
  {
    "slug": "pimono-coding-agent",
    "title": "Pi-Mono Coding Agent",
    "description": "AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods - badlogic/pi-mono",
    "fullText": "badlogic\n\n /\n\n pi-mono\n\n Public\n\n AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods\n\n License\n\n MIT license\n\n 1.8k\n stars\n\n 228\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n badlogic/pi-mono",
    "readingTime": 1,
    "keywords": [
      "agent",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/badlogic/pi-mono",
    "thumbnail_url": "https://opengraph.githubassets.com/bd7641eb472820a5f2d3f4dea10d6023fe9ba9619615be1dfc8b43bdb9eec747/badlogic/pi-mono",
    "created_at": "2026-01-15T12:24:35.039Z",
    "topic": "tech"
  },
  {
    "slug": "aura-farm-prompt-free-aura-farm-prompts-for-chatgpt-gemini-and-ai-art",
    "title": "Aura Farm Prompt – Free Aura Farm Prompts for ChatGPT, Gemini and AI Art",
    "description": "Discover free aura farm prompts shared by creators. Aura Farm Prompt is your gallery for copy-paste ready prompt text for ChatGPT, Gemini, and AI image generation. Browse trending aura farm prompts and create stunning AI art with our community.",
    "fullText": "Aura Farm Prompt is the free gallery where AI creators share their best aura farm prompts and curious fans discover what's possible. Browse stunning aura farm images with exact aura farm prompts for ChatGPT and Gemini, and track the trends shaping AI art.\n\nEvery aura farm image comes with the exact aura farm prompt that created it. Skip the guesswork and learn directly from aura farm prompts that caught the community's attention.\n\nTransparency accelerates learning. By sharing exact aura farm prompts, we help everyone understand what works and why. Every aura farm image in our gallery comes with the complete aura farm prompt, model information, and creative insights that made it possible.\n\nBrowse hundreds of aura farm prompts from our creative community. Discover new techniques, find inspiration, and level up your AI art skills with Aura Farm Prompt.",
    "readingTime": 1,
    "keywords": [
      "aura farm prompt",
      "exact aura",
      "farm prompts",
      "gallery",
      "discover",
      "browse",
      "creative"
    ],
    "qualityScore": 0.55,
    "link": "https://aurafarmprompt.org",
    "thumbnail_url": "https://aurafarmprompt.org/og.png",
    "created_at": "2026-01-15T12:24:34.869Z",
    "topic": "tech"
  },
  {
    "slug": "sadiq-khan-to-urge-ministers-to-act-over-colossal-impact-of-ai-on-london-jobs",
    "title": "Sadiq Khan to urge ministers to act over 'colossal' impact of AI on London jobs",
    "description": "In Mansion House speech, mayor will talk of opportunities technology offers but highlight mass unemployment risk",
    "fullText": "In Mansion House speech, mayor will talk of opportunities technology offers but highlight mass unemployment risk\n\nSadiq Khan is to warn in a major speech that artificial intelligence (AI) could destroy swathes of jobs in London and “usher in a new era of mass unemployment” unless ministers act now.\n\nIn his annual Mansion House speech, the London mayor will say the capital is “at the sharpest edge of change” because of its reliance on white-collar workers in the finance and creative industries, and professional services such as law, accounting, consulting and marketing.\n\nThe mayor will argue that “we have a moral, social and economic duty to act” to ensure that new jobs are created to replace those that will disappear, with entry-level and junior jobs the first to go.\n\nIn the speech on Thursday night, Khan will highlight research that suggests 70% of skills in the average job will have changed by 2030.\n\nHowever, he also sees huge potential benefits from AI for public services and productivity across the economy.\n\nWhen he addresses business leaders and bankers at Mansion House, the London mayor will say: “AI could enable us to transform our public services, turbocharge productivity and tackle some of our most complex challenges,” but used recklessly, will “usher in a new era of mass unemployment”. He will warn that the impact on the labour market will be “nothing short of colossal”.\n\nBut he will say that there is a clear choice: “seize the potential of AI and use it as a superpower for positive transformation and creation or surrender to it and sit back and watch as it becomes a weapon of mass destruction of jobs.”\n\nCity Hall is launching a London taskforce on AI and the future of work, with expertise from the government, businesses and the AI sector, to assess the potential impact of the new technology on London’s jobs market. It will also offer free AI training for Londoners.\n\nMore than half of workers in London expect AI to affect their jobs in some way in the next 12 months, according to City Hall polling.\n\nAcross the UK, up to 3m low-skilled jobs in trades, machine operations and admin roles could disappear by 2035 because of automation and AI, a report by the charity National Foundation for Educational Research found in November.\n\n“We can shape this next technological revolution and, if we are bold in our efforts, ensure AI makes us richer, not poorer, stronger, not weaker, more connected and more confident, and more capable of building a fairer, safer and more prosperous London for everyone,” Khan will say.\n\nHe will argue the UK and others have been too slow to respond to new technology in the past and that the growth of social media has led to a youth mental health crisis, a surge in online abuse and a dangerous rise in misinformation.\n\nSeparately, Susan Langley, major of the City of London, said on Thursday morning she had noticed that some finance workers were wary of coming to London from abroad because they worry about their safety.\n\nHowever, she told BBC Radio 4’s Today programme: “The City of London is one of the safest cities in the world. There’s this perception that you’re going to step out of your office and be swept away in a tsunami of crime.\n\n“It’s completely wrong. Competition for investment is really fierce at the moment, and I think any kind of unfounded negative sentiment that’s being pushed out there really risks undermining the UK on the global stage, and we just can’t let it happen.”",
    "readingTime": 3,
    "keywords": [
      "house speech",
      "london mayor",
      "mass unemployment",
      "city hall",
      "jobs",
      "technology",
      "workers",
      "services",
      "potential",
      "highlight"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/politics/2026/jan/15/sadiq-khan-to-urge-ministers-to-act-over-colossal-impact-of-ai-on-london-jobs",
    "thumbnail_url": "https://i.guim.co.uk/img/media/4535255c0e49599b3a0202be88dd8143b3570c4c/720_0_7200_5760/master/7200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=41a01858146699b3f65daa7dd7dd6067",
    "created_at": "2026-01-15T12:24:34.838Z",
    "topic": "politic"
  },
  {
    "slug": "marc-benioff-says-a-documentary-about-characterais-effects-on-children-was-the-worst-thing-ive-ever-seen-in-my-life",
    "title": "Marc Benioff says a documentary about Character.AI's effects on children was 'the worst thing I've ever seen in my life'",
    "description": "The Salesforce CEO says tech companies need to be held accountable for AI safety lapses: \"Reform, revise Section 230.\"",
    "fullText": "Marc Benioff shared what he thinks is the darkest aspect of AI.\n\nOn an episode of the \"TBPN\" show streamed on Wednesday, the Salesforce CEO said that he couldn't \"believe what he was watching\" when he saw a \"60 Minutes\" documentary on chatbot-building startup Character.AI and its impact on children.\n\n\"We don't know how these models work. And to see how it was working with these children, and then the kids ended up taking their lives,\" he said, \"That's the worst thing I've ever seen in my life.\"\n\nCharacter.AI allows users to build custom chatbots that can emulate the behaviour of a close friend or romantic partner. The startup did not immediately respond to Business Insider's request for comment about Benioff's remarks.\n\n\"Tech companies hate regulation. They hate it,\" Benioff said. \"Except for one regulation they love: Section 230. Which means that those companies are not held accountable for those suicides.\"\n\nSection 230 of the 1996 US Communications Decency Act protects social media companies from liability for user-generated content while also letting them moderate posts. Tech giants use Section 230 as a common defense strategy, saying they are just platforms and not responsible for what users say and do on them.\n\n\"Step one is let's just hold people accountable,\" he said. \"Let's reshape, reform, revise Section 230, and let's try to save as many lives as we can by doing that.\"\n\nExecutives, including Meta's Mark Zuckerberg and former Twitter CEO Jack Dorsey, have repeatedly defended the regulation in Congress, asking for it to be expanded rather than removed.\n\nLast week, Google and Character.AI agreed to settle multiple lawsuits from families whose teenagers died by suicide or hurt themselves after interacting with Character.AI's chatbots.\n\nThese negotiations are among the first settlements in lawsuits that accuse AI tools of contributing to mental health crises and suicides among teenagers. OpenAI and Meta are facing similar lawsuits as they, along with others, race to build large language models that sound more friendly and helpful, ultimately keeping users coming back.",
    "readingTime": 2,
    "keywords": [
      "users",
      "regulation",
      "let's",
      "lawsuits",
      "startup",
      "children",
      "models",
      "chatbots",
      "tech",
      "hate"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/marc-benioff-documentary-on-characterai-suicides-worst-thing-he-saw-2026-1",
    "thumbnail_url": "https://i.insider.com/6968846464858d02d2186489?width=1200&format=jpeg",
    "created_at": "2026-01-15T12:24:34.772Z",
    "topic": "finance"
  },
  {
    "slug": "claude-gets-its-meme-moment",
    "title": "Claude gets its meme moment",
    "description": "Anthropic is on a hot streak of launches, and the Claude Cowork memes are flowing — from the playful to those hinting at very real AI anxieties.",
    "fullText": "Claude is getting better and better — and sometimes it seems all we mere mortals can do is meme our way through it.\n\nAnthropic is on a hot streak of new launches. First came Opus 4.5, which quickly impressed developers. Then came Claude Cowork, which opened up Claude Code's already impressive abilities to non-programming tasks.\n\nThe memes have been flowing as the tech world processed the launch. Some played up the idea that the AI can do just about anything; others worried what that means for our jobs.\n\nCan Claude run your accounts? Some X users joked about hooking the chatbot up to their banking information and password managers. \"Make that number go up to $1 billion,\" one user asked. \"Run my life,\" another wrote.\n\nAnother user proposed asking Claude to flirt for him, providing the chatbot with a photo of his crush and her phone number. \"Make her my GF,\" he wrote.\n\nThese requests all came with the common ending for an AI request: \"Make no mistakes.\" One X user dubbed it the phrase of the year.\n\nclaude here is my life. all of it. down to the last detail. make me happy. beautiful. successful. make no mistakes.\n\nOthers celebrated the tool's ability to create quickly. Remarking on the phrase \"Rome wasn't built in a day,\" one user quipped that \"they didn't have Claude Code.\" Another user analogized Cowork to giving an ape an AK-47.\n\nThen there's the classic puns. What do you call someone who hates AI? \"Claudestrophobic.\"\n\nPOV: solo-founder using Claude pic.twitter.com/nccioFVmOW\n\nAI is changing the workforce; that much is clear. It's already affected software engineers. Just ask this X user, who mocked a so-called full-stack engineer whose stack consists of Claude, Terminal, and Cursor.\n\n\"yea I'm a full stack engineer\"\n\nthe stack: pic.twitter.com/Y7ApXFuvn9\n\nWhile the new Claude innovations have filled some with meme-filled joy, it's given others a sense of dread.\n\n\"Claude, automate my job. Make no mistakes,\" one X user commanded, alongside a sad-faced Roman Roy from \"Succession.\"\n\n\"I'm assembling a team,\" another wrote alongside an image of a company leadership team filled with Claude in every C-suite role — with a follow-up post saying they \"just got kicked out of my own company.\"\n\n\"Got told I was 'slowing everyone down,'\" they joked.\n\njust got kicked out of my own company.\n\ngot told i was \"slowing everyone down\" https://t.co/JFU2H4m8zf\n\nOne X user described bankers, lawyers, and consultants in the unemployment line watching others join after the Claude Cowork release. Another wrote that the jobs left in 2030 include \"associate Claude operator\" and \"principal Claude operator.\"\n\nFrom flippant to fretting over job security, the memes highlight the mix of feelings that can come with seeing the growing capabilities of AI tools.\n\nSo what do you think about Claude Code and Cowork? Take our survey below:",
    "readingTime": 3,
    "keywords": [
      "slowing everyone",
      "claude code",
      "claude operator",
      "another user",
      "claude cowork",
      "others",
      "mistakes",
      "stack",
      "quickly",
      "memes"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/claude-cowork-memes-2026-1",
    "thumbnail_url": "https://i.insider.com/696815fa04eda4732f2f1a72?width=1200&format=jpeg",
    "created_at": "2026-01-15T12:24:34.633Z",
    "topic": "finance"
  },
  {
    "slug": "business-spending-on-openai-models-jumps-to-a-record-new-data-shows",
    "title": "Business spending on OpenAI models jumps to a record, new data shows",
    "description": "OpenAI leads enterprise AI adoption among US businesses, outpacing Anthropic and Google. Ramp data shows rising paid usage in December 2025.",
    "fullText": "OpenAI is crushing it with business users and is far ahead of rivals such as Anthropic and Google in the enterprise AI market, according to new data this week.\n\nThat's a contrast to some of the hand-ringing that's gone on since Google's Gemini chatbot began gaining on ChatGPT a few months ago.\n\nThe new data comes from Ramp, a startup that helps companies pay their bills. Ramp analyzes corporate card and bill-paying activity on its platform from more than 50,000 US businesses to track billions of dollars spent on AI services each month.\n\nThe latest numbers cover December 2025. The report shows that OpenAI regained momentum among US businesses, posting its strongest growth in months as overall corporate adoption of AI continued to accelerate.\n\nThe share of US businesses paying for AI products and services rose to 46.6% in December, up 1.6 percentage points from November. That was the largest month-over-month increase since mid-2025, Ramp data shows.\n\nMuch of that growth was driven by OpenAI. Business adoption of OpenAI products climbed two percentage points to 36.8%, reversing a short-lived slowdown in the fall and reaching a new record high. Ramp's line-item data shows gains in both enterprise chat subscriptions and API spending, suggesting broader OpenAI usage across office workers and technical teams.\n\nIt's unclear how many current paid users OpenAI has, but the company said in November 2025 that it had 1 million business customers.\n\nThe rebound underscores OpenAI's continued dominance in the enterprise AI market at a time when companies are moving beyond experimentation. Rather than trial use, December's growth reflects recurring spend tied to everyday business functions, including software development, research, finance, sales, and customer support.\n\nCompetitors continued to gain ground, though at a slower pace. Anthropic's adoption rose to 16.7%, with growth concentrated among technology companies making heavy use of APIs.\n\nGoogle's AI adoption increased to 4.3%, a figure Ramp notes likely understates usage because many businesses access Gemini for free through Google Workspace plans.\n\nThese Ramp numbers also exclude business use of free AI tools, in which no paid transaction occurs, and when employees use personal accounts with AI companies to complete work tasks. This means the data likely underestimates actual AI adoption rates.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 2,
    "keywords": [
      "percentage points",
      "adoption",
      "businesses",
      "growth",
      "enterprise",
      "users",
      "market",
      "that's",
      "corporate",
      "services"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-business-spending-ai-models-jumps-record-ramp-data-2026-1",
    "thumbnail_url": "https://i.insider.com/6967faa364858d02d2185d9b?width=800&format=jpeg",
    "created_at": "2026-01-15T12:24:34.443Z",
    "topic": "finance"
  },
  {
    "slug": "elon-musk-says-retirement-savings-wont-matter-in-20-years-we-asked-7-personal-finance-and-ai-gurus-what-they-think",
    "title": "Elon Musk says retirement savings 'won't matter' in 20 years. We asked 7 personal finance and AI gurus what they think.",
    "description": "Elon Musk said retirement savings will be \"irrelevant\" in 20 years if he's right about an abundant future. Experts see challenges to that vision.",
    "fullText": "\"Savings? Where we're going, we don't need savings.\"\n\nThat twist on Doc Brown's famous line from \"Back to the Future\" captures Elon Musk's controversial view that future retirees won't need nest eggs.\n\n\"Don't worry about squirreling money away for retirement in 10 or 20 years,\" the Tesla and SpaceX CEO told the \"Moonshots with Peter Diamandis\" podcast earlier this month. \"It won't matter.\"\n\nThe world's richest man predicted that advances in AI, energy, and robotics will generate such an \"abundance\" of resources for all that individuals' retirement savings will be \"irrelevant.\"\n\nMusk's blue-sky vision comes at a time when years of stubborn inflation, elevated interest rates, and weak wage growth have created a nationwide affordability crisis. Household debt hit an all-time high of $18.59 trillion in the third quarter of 2025, up more than 50% from the same point in 2015.\n\nBusiness Insider asked seven personal finance and AI experts for their thoughts on Musk's comments.\n\nTheir reactions all boiled down to one thing: You really should be saving for retirement.\n\nMusk did not respond to a request for comment from Business Insider.\n\n\"Most Americans should absolutely ignore these comments,\" said Geoffrey Sanzenbacher, a research fellow at Boston College's Center for Retirement Research (CRR). \"Musk's speculation sends a dangerous and misleading message.\"\n\nSanzenbacher cast doubt on Musk's timeframe, and said the tech billionaire's words were \"especially dangerous\" as Social Security is likely to be cut in the coming years due to a funding shortfall.\n\nAmericans should be \"saving more, not less,\" given that prospect, he said. He added that even if Musk's future comes to pass, \"people who saved now will hardly be much worse off in this utopian future.\"\n\nAlicia Munnell, a senior advisor at CRR and its former director, also dismissed Musk's recommendation.\n\n\"I would pay no attention to anything Elon Musk says out of his core area of expertise,\" she said.\n\n\"He has no idea how the American lives, how important Social Security and 401(k)s are to maintaining people's standard of living,\" Munnell continued. \"He needs to stay out of public policy and concentrate on going to Mars!\"\n\nOlivia Mitchell, the director of Wharton's Boettner Center on Pensions and Retirement Research, said there was \"some truth\" in Musk's view that AI could boost productivity and reduce costs over time.\n\nBut she warned his advice was \"risky\" for US households, especially as their retirement security will \"still depend heavily on individual saving beyond Social Security.\"\n\n\"Even in a richer economy, gains are likely to be uneven and uncertain, so people must still save, in case the future does not unfold as predicted,\" Mitchell added.\n\nKristin Pugh, a private wealth manager at Creative Planning, said AI would allow people to afford their basic needs without working, freeing them to spend more time doing other things\n\nBut past technological advances have made people more productive without leading them to work less, and created wealth gains that weren't distributed equally, she said.\n\n\"Before we 'cancel' the idea of saving for another time in our lives, let's look at the environment and the logistics that would provide for our basic needs,\" Pugh said.\n\n\"We need to push leaders like Musk and Altman for the game plan and the logistics of the vision before any decisions are made,\" she added, referring to Sam Altman, the CEO of ChatGPT's maker, OpenAI.\n\nEkaterina Abramova, a professor at the London Business School specializing in machine learning, said that AI will \"undoubtedly reshape\" the world over the next 20 years, but this will not automatically negate the need for retirement savings.\n\n\"A future of 'universal high income' would depend less on AI itself than on governments choosing to redistribute its gains generously and sustainably, across borders and amid inevitable social friction,\" she said.\n\nJohn Nosta, an innovation theorist and the founder of NostaLab, agreed that Musk's \"promise that retirement planning will become obsolete rests on a fragile chain of assumptions.\"\n\nIt \"presumes that political will, fiscal design (and management), social trust, and intergenerational fairness will mature in lockstep with machine capability,\" and this \"new equilibrium will be durable across decades,\" he said.\n\n\"That is not a technological problem — it is a coordination problem at the scale of civilization,\" Nosta added.\n\nNosta said it's critical that people protect themselves against the risks and uncertainties of life through financial planning, \"rather than assuming that abundance will simply carry everyone safely downstream to an abstract utopian destination.\"\n\nJames Ransom, a research fellow at University College London, said that new technologies have a \"pretty terrible track record of boosting wealth evenly across society\" and he hasn't seen any evidence that AI will be an exception.\n\nUsing a historical analogy, Ransom underlined just how bad humans are at predicting the future.\n\n\"Some employees of the RAND Corporation who were researching nuclear war in the 1950s didn't contribute to their pensions because they didn't believe they would live long enough,\" he said. \"This seemed rational to them, at the time. Let's learn from them, and not swing too far in the other direction.\"",
    "readingTime": 5,
    "keywords": [
      "basic needs",
      "research fellow",
      "retirement savings",
      "social security",
      "less",
      "gains",
      "wealth",
      "across",
      "musk's",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/elon-musk-retirement-savings-wealth-ai-abundance-personal-finance-experts-2026-1",
    "thumbnail_url": "https://i.insider.com/6966ced664858d02d2184cf0?width=1200&format=jpeg",
    "created_at": "2026-01-15T12:24:34.430Z",
    "topic": "finance"
  },
  {
    "slug": "focusing-on-future-ai-doom-lets-companies-dodge-responsibility-today-professor-says",
    "title": "Focusing on future AI doom lets companies dodge responsibility today, professor says",
    "description": "A physics professor said AI apocalypse talk distracts from real harms like labor abuse, copyright theft, and weak regulation happening right now.",
    "fullText": "Fixation on futuristic AI catastrophe scenarios is allowing companies to evade accountability for the very real harms their technology is already causing, a professor says.\n\nIn an essay published this week, Tobias Osborne, a professor of theoretical physics at Leibniz Universität Hannover and a cofounder of the scientific communication firm Innovailia, said debates about superintelligent machines and a hypothetical \"singularity\" have become a dangerous distraction.\n\nWhile policymakers and technologists argue over whether AI could one day threaten humanity's survival, he wrote, the industry is inflicting \"real harm right now. Today. Measurably.\"\n\n\"The apocalypse isn't coming,\" Osborne wrote. \"Instead, the dystopia is already here.\"\n\nThe AI debate has increasingly been shaped by doomsday scenarios — including warnings that superintelligent systems could wipe out humanity by design or by accident, become uncontrollable, or trigger civilizational collapse — fears amplified by prominent AI researchers, tech leaders, and government reports.\n\nIn comments to Business Insider, Osborne said the fixation on such scenarios has a concrete effect on regulation and accountability.\n\n\"By framing themselves as guardians against civilizational catastrophe, AI firms are treated like national-security actors rather than product vendors, which dilutes liability and discourages ordinary regulation,\" he said.\n\nThat shift, Osborne said, allows companies to externalize harm while benefiting from regulatory deference, secrecy, and public subsidies.\n\nHe added that some of the most overlooked risks today include psychological harm linked to chatbot use and widespread copyright and data expropriation.\n\nApocalypse-style narratives persist, he said, because they are easy to market, difficult to falsify, and help shift corporate risk onto the public.\n\nWhile the European Union has begun rolling out the AI Act — a sweeping regulatory framework that will phase in stricter rules through 2026 — the US is moving in the opposite direction, with federal efforts focused on limiting state-level AI regulation and keeping national standards \"minimally burdensome.\"\n\nOsborne's essay laid out a long list of present-day harms he believes are being sidelined.\n\nIt includes the exploitation of low-paid workers who label AI training data, the mass scraping of artists' and writers' work without consent, the environmental cost of energy-hungry data centers, and a flood of AI-generated content that makes it harder for people to find trustworthy information online.\n\nHe also takes aim at the popular idea that AI is racing toward a runaway intelligence explosion.\n\nIn the essay, Osborne described such claims as \"a religious eschatology dressed up in scientific language,\" saying that such scenarios collapse when confronted with physical limits, such as energy consumption and thermodynamics.\n\n\"These aren't engineering problems waiting for clever solutions,\" he wrote. \"They're consequences of physics.\"\n\nRather than focusing on speculative future threats, Osborne said policymakers should apply existing product liability and duty-of-care laws to AI systems, forcing companies to take responsibility for the real-world impacts of their tools.\n\nOsborne said that he is not opposed to AI itself.\n\nIn his essay, he highlighted the genuine benefits large language models can offer, especially for people with disabilities who struggle with written communication.\n\nBut he warned that without accountability, those benefits risk being overshadowed.\n\n\"The real problems,\" he wrote, \"are the very ordinary, very human problems of power, accountability, and who gets to decide how these systems are built and deployed.\"",
    "readingTime": 3,
    "keywords": [
      "scenarios",
      "accountability",
      "essay",
      "harm",
      "systems",
      "regulation",
      "osborne",
      "fixation",
      "catastrophe",
      "harms"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-doomsday-fears-enable-companies-to-dodge-accountability-professor-says-2026-1",
    "thumbnail_url": "https://i.insider.com/6967898264858d02d2185081?width=1200&format=jpeg",
    "created_at": "2026-01-15T12:24:34.105Z",
    "topic": "finance"
  },
  {
    "slug": "arpah-launches-program-for-the-46-of-us-counties-dont-have-a-cardiologist",
    "title": "ARPA-H launches program for the 46% of U.S. counties don't have a cardiologist",
    "description": "Haider Warraich was once the only cardiologist for more than 150,000 people in a rural county. Now he’s running an ARPA-H program to use agentic AI to provide universal access to specialty care.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.statnews.com/2026/01/13/arpa-h-advancing-clinical-agentic-ai-use-heart-disease/",
    "thumbnail_url": "https://www.statnews.com/wp-content/uploads/2026/01/GettyImages-1722957248-1024x576.jpg",
    "created_at": "2026-01-15T12:24:33.345Z",
    "topic": "tech"
  },
  {
    "slug": "prediction-this-ai-hardware-stock-could-become-one-of-the-next-1-trillion-companies",
    "title": "Prediction: This AI Hardware Stock Could Become One of the Next $1 Trillion Companies",
    "description": "AMD could regain some AI market share from Nvidia.",
    "fullText": "Advanced Micro Devices' products are cheaper than Nvidia's.\n\nNvidia's supply problems could open the door for AMD.\n\nAMD expects huge data center growth over the next five years.\n\n10 stocks we like better than Advanced Micro Devices ›\n\nThe trillion-dollar stock club is a fairly exclusive group. There have only been 10 U.S.-listed stocks that have breached this valuation level, and there are a few more within striking distance.\n\nHowever, one stock that's a bit far away is Advanced Micro Devices (NASDAQ: AMD). AMD has a market cap of $330 billion, so it may not be on investors' radar for crossing into the $1 trillion valuation range. However, AMD's hardware is starting to become more popular in the artificial intelligence (AI) world, and it may get there faster than many think.\n\nHow quickly can AMD reach $1 trillion? Well, if its projections come true, it could be there in as little as four years.\n\nNvidia (NASDAQ: NVDA) is the undisputed king of graphics processing units (GPUs). GPUs are well-suited for AI workloads, as they can process multiple calculations in parallel. At the start of the AI boom in 2023, Nvidia's GPUs, the controlling software, and other hardware that supports them were far superior to AMD's. As a result, Nvidia's products became the go-to option, while AMD's only became an alternative.\n\nHowever, those trends are shifting. AMD has made massive improvements in its control software, ROCm. It noted a 10 times increase in downloads year over year in November 2025. That's a big deal because it shows that developers are exploring their hardware. This could be a signal that AMD's products are starting to become a viable alternative, and may be ready to steal some market share away from Nvidia.\n\nThere is only so much money available to build data centers. Computing hardware can be nearly half of the cost to build them, and while Nvidia's products are the best, they aren't cheap.\n\nThere aren't readily available prices on flagship GPUs for data centers, and these numbers are estimates from reports. Nvidia's Blackwell B200 GPU can cost between $30,000 and $50,000 per chip, depending on options. Its AMD rival, the MI350, costs $25,000. This allows AI hyperscalers to get more bang for the buck for cloud GPUs, but it remains to be seen if this cheaper price tag is worth it from a performance side.\n\nHowever, AI hyperscalers may not have a choice on whether they want to use AMD's chips anymore. During Nvidia's Q3 results, the company announced that it was \"sold out\" of cloud GPUs. While this may sound like a good problem to have, the bigger issue is that customers may turn to AMD's products as an alternative if they cannot get the computing power that they need from Nvidia. If AMD's products can deliver similar results at a lower price tag, this could create a scenario where more clients choose AMD's hardware in the future.",
    "readingTime": 3,
    "keywords": [
      "advanced micro",
      "micro devices",
      "cloud gpus",
      "amd's products",
      "amd's hardware",
      "nvidia's products",
      "alternative",
      "cheaper",
      "stocks",
      "stock"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/prediction-ai-hardware-stock-could-143500312.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/pyJbF8t2pU31OBueKZu39Q--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/motleyfool.com/1c261dd3c4c8aa91c7915fac0ff81622",
    "created_at": "2026-01-15T12:24:32.038Z",
    "topic": "finance"
  },
  {
    "slug": "musks-x-to-block-grok-ai-tool-from-creating-sexualised-images-of-real-people",
    "title": "Musk’s X to block Grok AI tool from creating sexualised images of real people",
    "description": "UK government claims vindication after Keir Starmer criticised earlier decision to keep functionality as ‘horrific’\nThe UK government has claimed “vindication” after Elon Musk’s X announced it had stopped its AI-powered Grok feature from editing pictures of real people to show them in revealing clothes such as bikinis, including for premium subscribers.\nAfter a fortnight of public outcry at the tool embedded into X being used to create sexualised images of women and children, the company said it would “geoblock” the ability of users “to generate images of real people in bikinis, underwear, and similar attire via the Grok account and in Grok in X”, in countries where it was illegal.\n Continue reading...",
    "fullText": "UK government claims vindication after Keir Starmer criticised earlier decision to keep functionality as ‘horrific’\n\nThe UK government has claimed “vindication” after Elon Musk’s X announced it had stopped its AI-powered Grok feature from editing pictures of real people to show them in revealing clothes such as bikinis, including for premium subscribers.\n\nAfter a fortnight of public outcry at the tool embedded into X being used to create sexualised images of women and children, the company said it would “geoblock” the ability of users “to generate images of real people in bikinis, underwear, and similar attire via the Grok account and in Grok in X”, in countries where it was illegal.\n\nIt said it would do this in the UK in line with law changes ministers have pledged to introduce. X also said it had “zero tolerance for any forms of child sexual exploitation, nonconsensual nudity, and unwanted sexual content”. It did not specify whether people would still be able to create such images on the standalone Grok app.\n\nThe prime minister, Keir Starmer, called X’s earlier decision to continue allowing the tool to be used by paid subscribers “horrific”. He also called the situation “disgusting” and “shameful” in parliament on Wednesday. Liz Kendall, the technology secretary, called it “a further insult to victims, effectively monetising this horrific crime”.\n\nOfcom, the UK media regulator, on Monday launched a formal investigation into Musk’s platform after what it called “deeply concerning” reports of Grok being used to create and share illegal nonconsensual intimate images and child sexual abuse material on X.\n\nAn Ofcom spokesperson said: “X has said it’s implemented measures to prevent the Grok account from being used to create intimate images of people. This is a welcome development. However, our formal investigation remains ongoing. We are working round the clock to progress this and get answers into what went wrong and what’s being done to fix it.”\n\nKendall welcomed the latest move by X but said she expected Ofcom’s investigation to “robustly establish the facts”.\n\nScrutiny of Grok has intensified globally after it emerged some people were using it to digitally undress women and children without their consent and posting those images to X. Thousands of these sexualised AI images have appeared X over the last few weeks. xAI, which makes Grok, also owns X, and both are run by Musk.\n\nThe changes to the X and Grok systems came hours after the billionaire posted on his site: “I [sic] not aware of any naked underage images generated by Grok. Literally zero.”\n\nMusk’s xAI and X have faced growing backlash globally, including an investigation by California’s attorney general and calls by lawmakers and advocacy groups for Apple and Google to drop Grok from their app stores. Countries including Malaysia and Indonesia have instigated bans or legal action.\n\nX said in a statement: “We take action to remove high-priority violative content, including child sexual abuse material and nonconsensual nudity, taking appropriate action against accounts that violate our X rules. We also report accounts seeking child sexual exploitation materials to law enforcement authorities as necessary.”\n\nMusk has said that Grok was programmed to refuse illegal requests and must comply with the laws of any given country or state. “Obviously, Grok does not spontaneously generate images. It does so only according to user requests,” Musk said on Wednesday.\n\nMusk has said earlier on X that anyone using Grok to make illegal content would suffer the same consequences as if they uploaded illegal content.\n\nThree Democratic US senators last week called on Apple and Alphabet’s Google to remove X and its built-in AI tool Grok from their app stores, citing the spread of nonconsensual sexual images of women and minors on the platform. A coalition of women’s groups, tech watchdogs and progressive activists also made similar appeals to the tech companies.\n\nLast week, X curtailed Grok’s ability to generate or edit images publicly for users who were not paying subscribers. But industry experts and watchdogs said that Grok was still able to produce sexually explicit images, and that restrictions, such as paywalling certain features, may not fully block access to deeper AI image tools.\n\nIn the UK, the law is changing this week to criminalise the creation of such images, and Starmer said on Wednesday that X was working to comply with the new rules.",
    "readingTime": 4,
    "keywords": [
      "abuse material",
      "grok account",
      "earlier decision",
      "app stores",
      "nonconsensual nudity",
      "formal investigation",
      "child sexual",
      "sexual exploitation",
      "illegal content",
      "intimate images"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/14/elon-musk-grok-ai-explicit-images",
    "thumbnail_url": "https://i.guim.co.uk/img/media/9724f199a8955fbabc79f8c8e5427b6576ab749a/542_0_5417_4335/master/5417.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=a3d02d0609473bb276030e7e16f94ef7",
    "created_at": "2026-01-15T12:24:27.721Z",
    "topic": "tech"
  },
  {
    "slug": "nao-labs-opensource-analytics-agent-yc-x25-is-hiring",
    "title": "Nao Labs (Open-Source Analytics Agent, YC X25) Is Hiring",
    "description": "Founding software engineer — Build the future of data teams experience.\nLocation: Paris 11, France\nHello, we're nao Labs\nWe are building an open-source AI agent for data analytics.\nWe are an early stage start-up with 2 cofounders - we joined Y Combinator Spring 2025 batch and STATION F and are now based in Paris 11.\nWe already have a first product - AI IDE for data people - used by 100+ data teams. We are now rolling out a new product - the open source analytics agent.",
    "fullText": "Founding software engineer — Build the future of data teams experience.\n\nWe are building an open-source AI agent for data analytics.\n\nWe are an early stage start-up with 2 cofounders - we joined Y Combinator Spring 2025 batch and STATION F and are now based in Paris 11.\n\nWe already have a first product - AI IDE for data people - used by 100+ data teams. We are now rolling out a new product - the open source analytics agent. So we’re looking for a founding engineer to help us build it!\n\nWe are a cofounding team with 18+ years experience in data/AI:\n\nAs a founding engineer, you'll join us to inventing a new, better way to work on data with AI.\n\nThe product uses these main technologies:\n\nAgentic system: Vercel, OpenAI, Anthropic\n\nCompensation: competitive salary + early equity\n\nLocation: Mainly on-site in our Paris 11 office. Remote available.\n\nJoin us at nao Labs and help shape the future of data work. Let’s build something incredible together!",
    "readingTime": 1,
    "keywords": [
      "founding engineer",
      "product",
      "teams",
      "experience",
      "agent",
      "analytics",
      "join",
      "paris"
    ],
    "qualityScore": 0.75,
    "link": "https://www.ycombinator.com/companies/nao-labs/jobs/KjOBhf5-founding-software-engineer",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/20786eecffc5cf815ddecb85e97325420dc9c795.png?1764064169",
    "created_at": "2026-01-15T12:24:27.414Z",
    "topic": "jobs"
  },
  {
    "slug": "openai-is-now-selling-6x-more-codex-for-10x-the-price",
    "title": "OpenAI is now selling 6x more codex for 10x the price",
    "description": "Codex is included in your ChatGPT Plus, Pro, Business, Edu, or Enterprise plan",
    "fullText": "Power a few focused coding sessions each week.\n\nRely on Codex for daily full-time development.\n\nBring Codex into your startup or growing business.\n\nUnlock Codex for your entire organization with enterprise-grade functionality.\n\nGreat for automation in shared environments like CI.\n\nThe number of Codex messages you can send depends on the size and complexity of your coding tasks and whether you run them locally or in the cloud. Small scripts or simple functions may consume only a fraction of your allowance, while larger codebases, long-running tasks, or extended sessions that require Codex to hold more context will use significantly more per message.\n\nNo fixed limits — usage scales with credits\n\n*The usage limits for local messages and cloud tasks share a five-hour\nwindow. Additional weekly limits may apply.\n\nEnterprise and Edu plans without flexible pricing have the same per-seat usage limits as Plus for most features.\n\nGPT-5.1-Codex-Mini can be used for local tasks, providing up to 4x more usage.\n\nChatGPT Plus and Pro users who reach their usage limit can purchase additional credits to continue working without needing to upgrade their existing plan.\n\nBusiness, Edu, and Enterprise plans with flexible pricing can purchase additional workspace credits to continue using Codex.\n\nIf you are approaching usage limits, you can also switch to the GPT-5.1-Codex-Mini model to make your usage limits last longer.\n\nAll users may also run extra local tasks using an API key, with usage charged at standard API rates.\n\nYou can find your current limits in the Codex usage dashboard. If you want to see your remaining limits during an active Codex CLI session, you can use /status.\n\nLearn more about credits in ChatGPT Plus and Pro.\n\nLearn more about credits in ChatGPT Business, Enterprise, and Edu.\n\nCode Review usage applies only when Codex runs reviews through GitHub — for example, when you tag @Codex for review in a pull request or enable automatic reviews on your repository. Reviews run locally or outside of GitHub count toward your general usage limits.\n\nThe usage limits and credits above are average rates. You can try the following tips to maximize your limits:",
    "readingTime": 2,
    "keywords": [
      "chatgpt plus",
      "flexible pricing",
      "purchase additional",
      "usage limits",
      "credits",
      "tasks",
      "codex",
      "reviews",
      "coding",
      "sessions"
    ],
    "qualityScore": 1,
    "link": "https://developers.openai.com/codex/pricing/",
    "thumbnail_url": "https://developers.openai.com/open-graph.png",
    "created_at": "2026-01-15T06:20:02.863Z",
    "topic": "tech"
  },
  {
    "slug": "two-thinking-machines-lab-cofounders-are-leaving-to-rejoin-openai",
    "title": "Two Thinking Machines Lab Cofounders Are Leaving to Rejoin OpenAI",
    "description": "The departures are a blow for Thinking Machines Lab. Two narratives are already emerging about why they happened.",
    "fullText": "Barret Zoph and Luke Metz are leaving the fledgling AI lab and rejoining OpenAI, the ChatGPT-maker announced on Wednesday. OpenAI’s CEO of applications, Fidji Simo, shared the news in a memo to staff this afternoon.\n\nTwo narratives are already forming about what prompted the departures. The news was first reported on X by technology reporter Kylie Robison, who wrote that Zoph was fired for “unethical conduct.”\n\nA source close to Thinking Machines alleged that Zoph had shared confidential company information with competitors. WIRED was unable to verify this information with Zoph, who did not immediately respond to WIRED’s request for comment.\n\nAccording to the memo from Simo, Zoph told Thinking Machines CEO Mira Murati on Monday he was considering leaving. He was then fired on Wednesday. Simo went on to write that OpenAI doesn’t share the same concerns about Zoph as Murati.\n\nThe personnel shake-up is a major win for OpenAI, which recently lost its VP of research, Jerry Tworek. A third Thinking Machines staffer, Sam Schoenholz, is also rejoining OpenAI, per the company’s announcement.\n\nThe departures are a blow to Thinking Machines, which also lost another co-founder, Andrew Tulloch, in November when he took a new job at Meta. In a post on X, Murati confirmed Zoph's depature and said that Soumith Chintala will replace him as the startup’s chief technology officer.\n\nZoph and Metz left OpenAI in late 2024 to start Thinking Machines with Murati, the ChatGPT maker’s former chief technology officer. Zoph was previously OpenAI’s vice president of post-training, where he led teams that made final improvements to AI models before they were deployed into products like ChatGPT and OpenAI’s API. Metz worked at OpenAI for two years during his first stint at the company and contributed to projects like ChatGPT and the o1 AI reasoning model.\n\nSimo told employees in her memo that Zoph will report directly to her, and Metz and Schoenholz will work under him. The hiring announcement timeline was accelerated, she said, so they still have to work out some details about their roles.\n\nThinking Machines Lab is one of several well-funded AI startups led by former top OpenAI researchers, reflecting the incredible appetite among investors to cash in on the AI race. Last year, Murati’s startup was last valued at $12 billion, and was recently in talks to raise more than $4 billion at a $50 billion valuation. The startup’s main product today is called Tinker, which allows developers to customize AI models on their own datasets.",
    "readingTime": 3,
    "keywords": [
      "rejoining openai",
      "officer zoph",
      "chief technology",
      "thinking machines",
      "memo",
      "shared",
      "departures",
      "fired",
      "recently",
      "announcement"
    ],
    "qualityScore": 1,
    "link": "https://www.wired.com/story/thinking-machines-lab-cofounders-leave-for-openai/",
    "thumbnail_url": "https://media.wired.com/photos/696832c5bbf56c47d3d0ed29/191:100/w_1280,c_limit/Thinking%20Machines%20Cofounder%20Barret%20Zoph%20is%20Leaving%20the%20Lab%20and%20Rejoining%20OpenAI-Business-2243572881.jpg",
    "created_at": "2026-01-15T06:20:01.994Z",
    "topic": "tech"
  },
  {
    "slug": "grok-stops-users-from-making-sexualized-ai-images-after-global-backlash",
    "title": "Grok stops users from making sexualized AI images after global backlash",
    "description": "Grok will no longer be allowed to edit photos of real people to show them in sexualized or revealing clothing.",
    "fullText": "Grok will no longer be allowed to create AI photos of real people in sexualized or revealing clothing, after widespread global backlash.\n\n\"We have implemented technological measures to prevent the Grok account from allowing the editing of images of real people in revealing clothing such as bikinis,\" X's safety account said in a blog post on the platform on Wednesday. \"This restriction applies to all users, including paid subscribers.\"\n\nThe change was announced hours after California's top prosecutor, Rob Bonta, said he launched an investigation into sexualized AI deepfakes, including those of children, generated by Grok. Bonta said that there had been a flood of reports in the last few weeks that Grok users were taking pictures of women and minors they found online and using the AI model to undress them in images.\n\nIndonesia and Malaysia suspended Grok because of the images, the first countries in the world to ban the AI tool. Lawmakers in the UK publicly considered a suspension.\n\nIn Wednesday's blog post, the social media company reiterated that image creation and the ability to edit images via Grok on the X platform will now only be available to paid users as an additional safety measure.\n\nThe company restricted non-paying users last week after complaints from officials globally, but it was slammed for being insufficient.\n\nA spokesperson for British Prime Minister Keir Starmer said it \"simply turns an AI feature that allows the creation of unlawful images into a premium service.\"\n\nElon Musk, who owns xAI, the maker of Grok, said that the UK government wanted \"any excuse for censorship\" in response to a post questioning why AI tools like Gemini and ChatGPT were not being looked into.\n\nOn Wednesday, a few hours before X's official account posted about the ban on creating sexualized images, Musk asked users to try to get around the AI model's image restrictions.\n\nBonta's office and Starmer's office did not immediately respond to requests for comment from Business Insider.",
    "readingTime": 2,
    "keywords": [
      "revealing clothing",
      "images",
      "users",
      "sexualized",
      "account",
      "grok",
      "safety",
      "blog",
      "platform",
      "hours"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/grok-stops-users-making-sexualized-ai-images-backlash-xai-musk-2026-1",
    "thumbnail_url": "https://i.insider.com/69683d7904eda4732f2f1baa?width=1200&format=jpeg",
    "created_at": "2026-01-15T06:20:01.164Z",
    "topic": "finance"
  },
  {
    "slug": "trump-places-a-25-tariff-on-highend-computing-chips-and-said-more-duties-may-be-coming-for-the-semiconductor-industry",
    "title": "Trump places a 25% tariff on high-end computing chips, and said more duties may be coming for the semiconductor industry",
    "description": "The tariff will cover hardware central to the AI boom like Nvidia's H200 processor and AMD's MI325X chips, with some exemptions.",
    "fullText": "On Wednesday, Donald Trump moved ahead with a new 25% tariff on imports of some high-end computing chips, narrowly targeting hardware central to the AI boom while carving out exemptions meant to encourage more tech manufacturing in the US.\n\nAccording to a White House fact sheet, the tariff applies to \"certain advanced computing chips,\" including Nvidia's H200 processor and AMD's MI325X. Chips brought into the country to support the buildout of the US technology supply chain would be excluded, though the administration has not detailed how companies would qualify for that exemption.\n\nThe proclamation also targets \"imports of semiconductors, semiconductor manufacturing equipment, and their derivative products from any country.\"\n\nThe White House did not immediately respond to a request for clarification.\n\nThe administration also signaled the move could be a first step. The Trump administration may expand tariffs to a wider range of semiconductors and related products in the future, according to the White House fact sheet.\n\nTrump cited national security concerns and invoked Section 232 of the Trade Expansion Act of 1962, per the White House fact sheet, which allows presidents to impose trade restrictions after determining imports pose a security risk.\n\nThe tariff aligns with Trump's broader agenda to reshore advanced manufacturing get stay ahead of the AI race. Nvidia, whose chips power the bulk of the data centers behind AI services, has been a focal point of that plan. Trump has previously said the company would be allowed to sell certain advanced chips to China, especially the H200, under the condition that the US government gets 25% of the proceeds.\n\nAMD and Nvidia did not immediately respond to a request for comments.\n\nThe tariff is not Trump's first attempt to use exemptions as a form of leverage. Last year, he floated tariffs as high as 100% on chips and semiconductors, while suggesting companies investing in US production could avoid them.",
    "readingTime": 2,
    "keywords": [
      "immediately respond",
      "computing chips",
      "white house",
      "tariff",
      "imports",
      "manufacturing",
      "sheet",
      "advanced",
      "administration",
      "semiconductors"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/trump-tariff-high-end-computing-chip-imports-nvidia-amd-2026-1",
    "thumbnail_url": "https://i.insider.com/69685276764ca5f34d2a7b35?width=1200&format=jpeg",
    "created_at": "2026-01-15T06:20:01.012Z",
    "topic": "finance"
  },
  {
    "slug": "meta-compute-the-metaopenai-battle-the-reality-labs-sacrifice",
    "title": "Meta Compute, the Meta-OpenAI Battle, the Reality Labs Sacrifice",
    "description": "Mark Zuckerberg announced Meta Compute, a bet that winning in AI means winning with infrastructure; this, however, means retreating from Reality Labs.",
    "fullText": "Mark Zuckerberg announced Meta Compute, a bet that winning in AI means winning with infrastructure; this, however, means retreating from Reality Labs.\n\nWith Stratechery Plus you get access to the subscriber-only Stratechery Update and Stratechery Interviews, and the Sharp Tech, Sharp China, Dithering, Greatest of All Talk, and Asianometry podcasts.\n\nStratechery Update\nSubstantial analysis of the news of the day delivered via three weekly emails or podcasts.\n\nStratechery Interviews\nInterviews with leading public CEOs, private company founders, and discussions with fellow analysts.\n\nDithering\nA twice-weekly podcast from John Gruber and myself: 15 minutes an episode, not a minute less, not a minute more.\n\nSharp Tech\nAndrew Sharp and myself discuss how technology works and the ways it impacts our lives.\n\nSharp China\nA weekly podcast from Andrew Sharp and Sinocism’s Bill Bishop about understanding China and how China impacts the world.\n\nGreatest Of All Talk\nA twice-weekly podcast from Andrew Sharp and Ben Golliver about the NBA, life, and national parks.\n\nAsianometry\nAudio and transcripts of the Asianometry YouTube channel, the best source for learning about how tech works.\n\nSharp Text\nSharp Text is an extension of GOAT, Sharp Tech and Sharp China, where Andrew writes about basketball, technology, and US-China relations with weekly posts.\n\nStratechery Updates are also available via SMS, RSS, or on this site. Please see the Stratechery Update Schedule \n\nOnce you are subscribed, please visit your Delivery Preferences where you will find easy-to-follow instructions for adding Stratechery Podcasts to your favorite podcast player.\n\nYes! Create a Stratechery Passport account, go to Delivery Preferences, and add your personalized RSS feed. Free accounts will have access to Weekly Articles, while subscribers will have access to the Daily Update as well.\n\nNo, the Stratechery Update and Stratechery Podcast are intended for one subscriber only. Sharing emails, using shared inboxes, or sharing RSS feeds is a violation of Stratechery’s Terms of Service, and your account may be suspended or your RSS feed reset. Of course occasional forwarding of the Stratechery Update to interested friends or colleagues is totally fine.\n\nYes! You can purchase a team subscription here.\n\nYes! Just go to your account page, choose the ‘Subscriptions’ tab, and click the Annual upgrade button. You will be charged immediately, with a prorated discount applied for the remainder of your current monthly plan.\n\nStratechery is purposely kept at a low price — thousands of dollars less than other analyst reports or newsletters — to ensure it is accessible to everyone, including students.\n\nI am happy to create an invoice to your specification for annual subscribers; however, it is simply not viable for me to offer this service to monthly subscribers. Therefore, if you need a custom invoice please subscribe or switch to an annual subscription and contact Stratechery.\n\nJune 1, 2021 Update: We are hoping to add native support for custom invoices to Passport; you can \n\nYes! To send a gift visit the gifts page.",
    "readingTime": 3,
    "keywords": [
      "rss feed",
      "sharp text",
      "sharp tech",
      "andrew sharp",
      "stratechery interviews",
      "podcasts stratechery",
      "twice-weekly podcast",
      "delivery preferences",
      "access",
      "please"
    ],
    "qualityScore": 1,
    "link": "https://stratechery.com/2026/meta-compute-the-meta-openai-battle-the-reality-labs-sacrifice/",
    "thumbnail_url": "https://s0.wp.com/_si/?t=eyJpbWciOiJodHRwczpcL1wvaTAud3AuY29tXC9zdHJhdGVjaGVyeS5jb21cL3dwLWNvbnRlbnRcL3VwbG9hZHNcLzIwMThcLzAzXC9jcm9wcGVkLWFuZHJvaWQtY2hyb21lLTUxMng1MTItMS5wbmc_Zml0PTUxMiUyQzUxMiZzc2w9MSIsInR4dCI6IlN0cmF0ZWNoZXJ5IGJ5IEJlbiBUaG9tcHNvbiIsInRlbXBsYXRlIjoiZWRnZSIsImZvbnQiOiIiLCJibG9nX2lkIjoxODgwNDM0MTV9.5VWck4PcKPWCTPe_HVznn3n3xsgn-G0b3d2OeiNNC7cMQ",
    "created_at": "2026-01-15T00:56:32.803Z",
    "topic": "tech"
  },
  {
    "slug": "x-to-stop-grok-ai-from-undressing-images-of-real-people-after-backlash",
    "title": "X to stop Grok AI from undressing images of real people after backlash",
    "description": "Grok will no longer allow users to remove clothing from images of real people, a statement posted on X reads.",
    "fullText": "Elon Musk's AI model Grok will no longer be able to edit photos of real people to show them in revealing clothing, after widespread concern over sexualised AI deepfakes in countries including the UK and US.\n\n\"We have implemented technological measures to prevent the Grok account from allowing the editing of images of real people in revealing clothing such as bikinis.\n\n\"This restriction applies to all users, including paid subscribers,\" reads an announcement on X, which operates the Grok AI tool.\n\nThe change was announced hours after California's top prosecutor said the state was probing the spread of sexualised AI deepfakes, including of children, generated by the AI model.",
    "readingTime": 1,
    "keywords": [
      "revealing clothing",
      "model",
      "sexualised",
      "deepfakes",
      "grok"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bbc.co.uk/news/articles/ce8gz8g2qnlo",
    "thumbnail_url": "https://ichef.bbci.co.uk/ace/branded_news/1200/cpsprodpb/6e39/live/d2ffc570-f1a9-11f0-af39-d97d2e6b0b19.jpg",
    "created_at": "2026-01-15T00:56:30.684Z",
    "topic": "tech"
  },
  {
    "slug": "beni-ai-realtime-facetoface-ai-companion-that-talks-like-a-real-person",
    "title": "Beni AI – Real-time face-to-face AI companion that talks like a real person",
    "description": "Experience Beni AI, a multimodal AI companion that listens, remembers, and responds in real time — redefining emotional connection through voice, video, and memory.",
    "fullText": "Two-way voice, video, and text with live captions, plus opt-in screen and expression awareness. Persistent memory keeps continuity over time, and action plugins help her actually do things with your approval.\nCompanion first. Creator engine next.\n\nTwo-way voice, video, and text with live captions, plus opt-in screen and expression awareness. Persistent memory keeps continuity over time, and action plugins help her actually do things with your approval.\nCompanion first. Creator engine next.\n\nBring IP to life as a companion, then scale it as content powered by AI\n\nVoice and video are the default interface, built for real-time connection.\n\nVoice and video are the default interface, built for real-time connection.\n\nA memory layer that carries context forward, so each conversation builds on the last.\n\nA memory layer that carries context forward, so each conversation builds on the last.\n\nTurn the same IP into short-form content, from creation to distribution.\n\nTurn the same IP into short-form content, from creation to distribution.\n\nBuilt to follow you across web and mobile with continuity. Anywhere, everywhere.\n\nBuilt to follow you across web and mobile with continuity. Anywhere, everywhere.\n\nBeni is our flagship, built for presence, memory, and expression. \nBut the bigger goal is the platform: bring any imagined IP to life as a companion, then scale it into content.\n\nBeni is our reference IP, proving the core loop: presence + memory + expression that deepens over time.\n\nBeni is our reference IP, proving the core loop: presence + memory + expression that deepens over time.\n\nReal-time voice and video, visual expressions, and continuity across sessions, designed to feel like one relationship.\n\nReal-time voice and video, visual expressions, and continuity across sessions, designed to feel like one relationship.\n\nBring your IP to life as a companion, then scale it into short-form content. No code.\n\nBring your IP to life as a companion, then scale it into short-form content. No code.\n\nStart with Beni in the app: real presence with voice, video, and memory.",
    "readingTime": 2,
    "keywords": [
      "awareness persistent",
      "creator engine",
      "anywhere everywhere",
      "continuity anywhere",
      "persistent memory",
      "captions plus",
      "plus opt-in",
      "opt-in screen",
      "action plugins",
      "default interface"
    ],
    "qualityScore": 1,
    "link": "https://thebeni.ai/",
    "thumbnail_url": "https://framerusercontent.com/images/jnnI86BQPYoMQruakAOQkLyfT8.jpg",
    "created_at": "2026-01-15T00:56:17.278Z",
    "topic": "tech"
  },
  {
    "slug": "theres-no-such-thing-as-free-lunch-for-big-techs-electric-bill",
    "title": "There's no such thing as 'free lunch' for Big Tech's electric bill",
    "description": "Energy regulators and advocates are cautiously optimistic about Microsoft's promise to \"pay its own way\" for the power needed to serve its AI footprint.",
    "fullText": "Microsoft is promising to cover the costs of electricity needed to power its rapidly growing fleet of data centers in suburban and rural communities across the US.\n\nIn a speech on Tuesday, Brad Smith, the president and vice chair of the Big Tech giant, said that going forward, Microsoft will \"pay utility rates that are high enough to cover our electricity costs.\"\n\nIt's a bold pledge to make at a time when AI is fueling a historic surge in demand for power in the US, and electric bills are surging for the average American utility customer.\n\nIt is also one that state regulators have heard before as utilities, their data center customers, and consumer advocates argue over who will bear the cost of new infrastructure needed to serve AI's enormous appetite for power.\n\nBut as the impact of data centers on electricity rates becomes an increasingly hot political flashpoint, Microsoft may have no other choice but to follow through, and the rest of Big Tech might not be far behind.\n\n\"There is no free lunch for tech players, and Trump is watching,\" said Dan Ives, global head of tech research at Wedbush Securities.\n\nIves expects other Big Tech companies to follow Microsoft's lead and make similar pledges addressing local concerns around major data center buildouts sooner rather than later.\n\n\"I never want Americans to pay higher Electricity bills because of Data Centers,\" President Trump wrote Monday night in a post on Truth Social.\n\nThe \"big technology companies who build them,\" the president said, \"must pay their own way.\"\n\nTrump wrote that his administration had collaborated with Microsoft on the company's pledge to cover its electricity costs, and alluded to future announcements by other companies.\n\nWhen asked for further comment, the White House Press Office pointed to President Trump's post on Truth Social.\n\nMicrosoft's four-part plan to ensure it is paying its fair share of electricity costs was laid out in a blog post this week. The plan consists of asking utilities and regulators to set their rates high enough to cover their data center costs, collaborating with utilities to cover the costs of new infrastructure, looking for ways to operate data centers more efficiently, and advocating for affordable, reliable power at the state and federal levels.\n\nWhen asked for further comment, a spokesperson for Microsoft pointed to the company's blog post.\n\nConsumer advocates are cautiously optimistic about Microsoft's promise to pay its fair share, but eager to see more details — especially from the utilities that serve the data centers.\n\n\"These are only rough concepts that don't get at the root of the problem, which is the scale and speed at which the industry is moving,\" said Julie Bolthouse, director of land use at Virginia's Piedmont Environmental Council.\n\nDominion Energy, Virginia's largest public utility, has said it is in contract talks with data center customers for 47 gigawatts of new demand — double Dominion's current peak load.\n\nBolthouse is concerned about the legitimacy of that demand — and who might end up footing the bill if it never comes to fruition.\n\n\"The current process of unreviewed contracts between utilities and data centers has broken the planning process and threatens the reliability of the system,\" said Bolthouse.\n\nCharles Hua, founder and director of Powerlines, a nonprofit that advocates for utility regulation reform, said that data centers can be \"good grid citizens\" if local utilities prioritize investment in grid-enhancing technologies and energy efficiency over building new power plants.\n\n\"Doing so will require states to reimagine our outdated system of utility regulation and make necessary changes to center consumers going forward,\" he said.",
    "readingTime": 3,
    "keywords": [
      "truth social",
      "consumer advocates",
      "center customers",
      "utility regulation",
      "big tech",
      "electricity",
      "utilities",
      "cover",
      "rates",
      "demand"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/microsofts-pledge-pay-more-data-center-electricity-2026-1",
    "thumbnail_url": "https://i.insider.com/69681885764ca5f34d2a79c5?width=1200&format=jpeg",
    "created_at": "2026-01-15T00:56:15.339Z",
    "topic": "finance"
  },
  {
    "slug": "elon-musks-stubborn-spin-on-groks-sexualized-images-controversy",
    "title": "Elon Musk’s stubborn spin on Grok’s sexualized images controversy",
    "description": "Musk attempts to recast AI tool’s misuse. Plus, tech billionaires plot against a proposed California tax on their fortunes\nHello, and welcome to TechScape. I’m your host, Blake Montgomery, US tech editor for the Guardian. Today, we discuss Elon Musk’s rosy depiction of Grok’s image generation controversy; the seven-figure panic among Silicon Valley billionaires over a proposed wealth tax in California, though with one notable exception; and how AI and robotics have revitalized the Consumer Electronics Showcase.\nUnder a tax proposal that could be put to voters this November, any California resident worth more than $1bn would have to pay a one-off, 5% tax on their assets to help cover education, food assistance and healthcare programs in the state.",
    "fullText": "Musk attempts to recast AI tool’s misuse. Plus, tech billionaires plot against a proposed California tax on their fortunes\n\nHello, and welcome to TechScape. I’m your host, Blake Montgomery, US tech editor for the Guardian. Today, we discuss Elon Musk’s rosy depiction of Grok’s image generation controversy; the seven-figure panic among Silicon Valley billionaires over a proposed wealth tax in California, though with one notable exception; and how AI and robotics have revitalized the Consumer Electronics Showcase.\n\nThe firestorm over the Grok AI tool has been raging for more than a week now, and it shows no signs of dying down.\n\nLast week, I wrote about the rising backlash against Elon Musk’s Grok AI tool, which in recent weeks has allowed users to generate thousands of sexualized images of women. Some of the images show real women, some are fake, some are nonconsensual, and some depict children, all in “minimal clothing”, as the AI tool itself described them.\n\nX and its parent company, xAI, have taken some measures to curb the flood. The social network shut off its image generation feature for users who do not pay, which constitute the majority of X’s users.\n\nThroughout the controversy, Musk has obstinately recast the AI tool’s problems as everything but what they really are. He’s been promoting its popularity as if it were a piece of productivity software. He crows about its download numbers with dubious claims. On 10 January; he celebrated Grok reaching the top spot in New Zealand’s version of Apple’s App Store. (Rankings by the analytics firm SimilarWeb of the most-downloaded apps in New Zealand, which were updated the same day as Musk’s tweet, put Grok in 14th place.) The same day, he reposted a tweet about Grok reaching the No 1 spot in Thailand’s Apple App Store. (SimilarWeb’s rankings do not show Grok in the top 50 most-downloaded apps in the country.) On 9 January, he retweeted a post about Google searches for Grok spiking. (I would guess the increase in searches is evidence of great interest in the AI tool’s scandal more so than interest in using it.)\n\nIn response to the UK’s threats to ban the AI tool, he accused the country’s government of stifling free speech. After watchdogs cited instances of Grok undressing minors, he said: “Anyone using Grok to make illegal content will suffer the same consequences as if they upload illegal content,” handing the responsibility to moderate his social network to law enforcement and courts. “Illegal” is in a court’s hands and frees him from moderating all but the most heinous content.\n\nPerhaps to Musk, all press is good press? He may be right: his AI tool seems likely to accrue more users and few penalties as a result of the flood of nearly naked images.\n\nGrok has faced some repercussions: the UK’s communications regulator, Ofcom, has launched an investigation into xAI and Grok, and possible punishments could include a total ban. The Internet Watch Foundation, also based in the UK, announced it had found instances of child sexual abuse material generated by Grok in Dark Web forums. X’s revenue in the UK has plummeted by 60% as concerns over content moderation and brand safety grow. Both Indonesia and Malaysia have limited access to the AI tool in response.\n\nBut missing from the chorus decrying Grok are the two de facto smartphone regulators of smartphone software, Apple and Google, operators of the world’s biggest mobile app stores. Neither has indicated whether the Grok’s output violates their app stores’ terms of service. In the US, there’s been little backlash from regulators and lawmakers.\n\nThe lesson for Musk and other tech leaders seems apparent: the fewer restrictions you place on AI, the more shocking content you allow it to generate, the greater your engagement and your profit.\n\nTech’s billionaires are plotting against a proposed tax on their fortunes that may appear on ballots across California in November.\n\nVenture capitalist and antichrist evangelist Peter Thiel has already made a $3m donation to fight the proposal, according to campaign finance disclosures filed with the state. Other billionaires have started an encrypted group chat on Signal to strategize against it, which includes Anduril founder Palmer Luckey, Trump’s AI and crypto “czar” David Sacks, according to the Wall Street Journal. It’s called “Save California”. My colleague Dara Kerr reports on the division among billionaires:\n\nUnder a tax proposal that could be put to voters this November, any California resident worth more than $1bn would have to pay a one-off, 5% tax on their assets to help cover education, food assistance and healthcare programs in the state.\n\nSeveral Silicon Valley figures have already threatened to leave California and take their business elsewhere. But Jensen Huang, the CEO of Nvidia, whose net worth is nearly $159bn, told Bloomberg Television this week that he is “perfectly fine with it”.\n\nThis puts Huang in stark contrast with the Google co-founders Larry Page and Sergey Brin, Palantir co-founder Peter Thiel and Donald Trump’s AI and crypto czar, the venture capitalist David Sacks, all of whom have recently indicated they are leaving California for tax-friendlier states including Florida and Texas.\n\nUnder the proposed tax, Huang would pay roughly $7bn, and Page and Brin would pay one-time amounts of about $13bn and $12bn, respectively, based on their current net worth of $264bn and $243bn. Thiel would pay $1.3bn, based on his current net worth of $26bn.\n\nThe Consumer Electronics Show, held annually in Las Vegas for decades since its start in 1967, made global news this year. Nvidia and AMD chose it as the forum for major announcements on new hardware and software. Samsung announced a double folding phone, the Galaxy Z Fold 3, and Lego debuted a “smart brick” that seems quite fun as well. Robots abounded, rechristened as “physical AI”.\n\nIt’s been quite the turnaround for CES. For much of the 2010s, hardware announcements rarely made for top headlines. New DVD players or TVs did not make a splash. Smartphones dominated, and they all looked extremely similar. Apple, the most valuable company in the world at the end of the decade, even skipped it entirely. Now, however, Nvidia is the most valuable company in the world, and it chose CES to present what’s next. Artificial intelligence and robotics have breathed new life into CES. The themes and innovations that define the convention are broader and affect more industries outside tech than they once did.\n\nTwo weeks ago, I predicted that consumer technology would take many strange new shapes in the coming year, which seems to already be true as we view new developments in robotics. Humanoid robots debuted at CES, including one from Hyundai and Boston Dynamics. My colleague Samuel Gibbs dubbed a laundry-folding robot one of the best things about the convention.\n\nMeta blocked nearly 550,000 accounts in first days of Australia’s under-16s social media ban\n\nIran’s internet shutdown is chillingly precise and may last some time\n\nGoogle parent Alphabet hits $4tn valuation after AI deal with Apple\n\n‘Dangerous and alarming’: Google removes some of its AI summaries after users’ health put at risk",
    "readingTime": 6,
    "keywords": [
      "consumer electronics",
      "app store",
      "most-downloaded apps",
      "venture capitalist",
      "crypto czar",
      "social network",
      "app stores",
      "net worth",
      "illegal content",
      "proposed tax"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/12/elon-musk-grok-ai-images-california-tax-bill",
    "thumbnail_url": "https://i.guim.co.uk/img/media/ed942953d6d0fb965fb474a1e6a7011b5b5a396a/673_0_6733_5386/master/6733.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=082a7c3aade78be1c1bde3cc873d429f",
    "created_at": "2026-01-15T00:56:13.365Z",
    "topic": "tech"
  },
  {
    "slug": "i-built-an-app-to-install-ai-as-if-it-were-steam-or-the-app-store",
    "title": "I built an app to install AI as if it were Steam or the App Store",
    "description": "Discover and install open-source AI apps effortlessly with Dione. Explore powerful tools, manage downloads, and enjoy 1-click installations—all in one intuitive platform.",
    "fullText": "One-click install for all your favorite AI tools\n\nCommunity-driven development and transparency\n\nAvailable on Windows, Mac and Linux\n\nStay current with automatic app updates and notifications\n\nFind and explore new AI tools curated by the community\n\nOptimized performance with minimal system impact\n\nJoin thousands of users discovering and installing AI apps with ease.",
    "readingTime": 1,
    "keywords": [
      "tools"
    ],
    "qualityScore": 0.2,
    "link": "https://getdione.app/",
    "thumbnail_url": "https://getdione.app/opengraph-image.png",
    "created_at": "2026-01-14T18:20:12.525Z",
    "topic": "tech"
  },
  {
    "slug": "executives-favorite-explanation-for-spending-big-on-ai-fomo",
    "title": "Executives' favorite explanation for spending big on AI: FOMO",
    "description": "If you're wondering whether JPMorgan's tech spend is paying off, here's Jamie Dimon's answer: \"Trust me.\"",
    "fullText": "If you're wondering whether JPMorgan's tech spend is paying off, here's Jamie Dimon's answer: \"Trust me.\"\n\nThat's how the CEO responded to questions about the bank's ROI on its ever-growing tech budget during JPMorgan's fourth-quarter earnings calls. The bank is projecting it'll spend roughly $9.7 billion \n\nHe won't be the last executive pressed on money spent on tech and AI. The quiet concerns that started last year regarding massive AI investments are escalating into loud protests in 2026.\n\nDimon wasn't just asking for blind faith from his shareholders. He discussed the threat posed by his peers and fintechs, and said spending on technology and AI is far more important than trying to \"meet some expense target.\"\n\n(For what it's worth, JPMorgan is actually top of the class when it comes to AI maturity across Wall Street, according to Evident's AI index.)\n\nThe players might be different, but other businesses will likely defend their AI spend with a similar argument: Every dollar I don't spend is one my competitor is willing to, and that could be the difference between winning and losing.\n\nI'm not endorsing FOMO-inspired spending, but I see the rationale. I'd rather go down swinging in the AI wars than not enter the fray at all.\n\nThere's another fight JPMorgan isn't interested in getting into.\n\nThe bank's CFO said the credit card rate cap proposed by President Donald Trump could force JPM to rethink its business entirely.\n\nIt's the latest company to weigh in on Trump's proposal, and it's a biggy. JPMorgan's card services sales totalled roughly $360 billion last quarter.\n\nOpponents of Trump's pitch say capping credit card rates will backfire. If lenders can't charge higher rates to riskier borrowers, they'll just limit the credit they offer them instead of lowering their rates.\n\nAnd it's not like people will stop borrowing. (This is America, after all.) They'll just look for alternatives, which could lead them to take out more personal loans, making 2026 potentially a big year for lenders like SoFi.",
    "readingTime": 2,
    "keywords": [
      "credit card",
      "it's",
      "jpmorgan's",
      "tech",
      "rates",
      "bank's",
      "roughly",
      "jpmorgan",
      "lenders",
      "they'll"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/bi-today-newsletter-jamie-dimon-jpmorgan-trump-2026-1",
    "thumbnail_url": "https://i.insider.com/6966c4a404eda4732f2f0508?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:09.288Z",
    "topic": "finance"
  },
  {
    "slug": "ai-is-fast-but-checking-its-work-is-slowing-workers-down",
    "title": "AI is fast. But checking its work is slowing workers down.",
    "description": "Almost 40% of AI's time-saving value is lost to editing as workers double-check its output, new research suggests.",
    "fullText": "After writing a blog post about work-life balance for her employer, Emilie Schario did what many people now do before publishing anything online: She pasted her draft into an AI tool in hopes of getting back a stronger post.\n\nYet rather than take the revision it produced at face value, Schario said she spent close to half as much time reviewing the new version as she did writing the original — and for good reason. The AI added a sentence saying that she had recently blocked time on her calendar to attend her daughter's school play.\n\n\"I don't have a daughter, and there was no school play,\" said Schario, chief operating officer at Kilo Code, a remote AI coding startup, and the mother of three young boys.\n\nAI tools are helping workers complete all sorts of tasks faster than ever before, yet many are discovering a drawback. The output still requires careful review for errors and hallucinations, cutting into the time the technology is meant to save.\n\nNew survey data provides a sense of just how much. Nearly 40% of AI's value is lost to rework and misalignment, and only 14% of employees consistently get clear, positive outcomes from the technology, the survey found.\n\nThe survey was conducted in November by Hanover Research on behalf of HR and finance software provider Workday. Respondents included 1,600 leaders and 1,600 full-time employees from companies worldwide with annual revenues of $100 million or more.\n\nAI still nets out to be a time-saver, and editing the results will likely become less of a chore in the future as the technology advances — and as workers receive more training on how to write prompts and apply critical thinking to AI-generated work, said Workday executive Aashna Kircher.\n\n\"We're seeing a need for organizations to better enable their people to evaluate the output and make the right decisions in terms of how it's used,\" she said.\n\nIn Workday's survey, 66% of leaders cite skills training as a top priority, yet only 37% of employees facing the most AI rework say they're getting it. The findings also show that fewer than half of employee job descriptions have been updated to reflect AI capabilities, leaving workers to balance faster AI-driven output with the same expectations around accuracy, judgment, and risk.\n\nOther studies also suggest that AI outputs routinely require human intervention and cannot be trusted outright. A global survey of 2,000 CEOs found that only a quarter of AI efforts had delivered the returns the leaders had expected. It was conducted between February and April of last year by the IBM Institute for Business Value and Oxford Economics.\n\nSimilarly, 95% of organizations reported no measurable ROI from AI, according to an MIT study based on reviews of publicly disclosed AI initiatives and executive interviews between January and June of last year.\n\nSchario, who works for Kilo Code from her home in Columbus, Georgia, isn't giving up on AI. As someone who finds writing as unpleasant as folding laundry, she said the speed at which these tools work outweighs the need to have to carefully check for errors.\n\n\"I think where people get themselves in trouble is that they take that output of the AI agent, they don't review it closely, and they just kind of pass it on,\" she said. \"At the end of the day, you are still responsible for your output, whether it was generated by an AI agent or not.\"",
    "readingTime": 3,
    "keywords": [
      "kilo code",
      "output",
      "survey",
      "workers",
      "technology",
      "employees",
      "leaders",
      "balance",
      "half",
      "school"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/workday-study-looks-at-time-spent-fixing-ai-errors-2026-1",
    "thumbnail_url": "https://i.insider.com/69669c2004eda4732f2eff31?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:09.056Z",
    "topic": "science"
  },
  {
    "slug": "google-is-leaning-on-its-app-empire-to-give-gemini-an-edge-with-personal-intelligence",
    "title": "Google is leaning on its app empire to give Gemini an edge with 'Personal Intelligence'",
    "description": "The Gemini AI chatbot can now reason across Google apps including Gmail, Photos and YouTube, offering something OpenAI and Anthropic don't have.",
    "fullText": "Google is escalating the competition in artificial intelligence by leaning into a key advantage its rivals largely lack: an ecosystem of consumer apps used daily by billions of people.\n\nOn Wednesday, Google unveiled Personal Intelligence, a new beta capability in the Gemini app that allows the assistant to tailor responses by reasoning across a user's connected Google services, starting with Gmail, Photos, Search, and YouTube history.\n\nThe feature rolls out in the US on Wednesday to Google AI Pro and AI Ultra subscribers. The company said it will also bring the technology to the free Gemini app and AI Mode in Search.\n\nWhile Gemini could previously pull information from individual apps, Personal Intelligence represents a significant change forward. Powered by Gemini 3, the system can now analyze and reason across multiple apps simultaneously, surfacing insights without users having to specify where to look. For example, Gemini might connect a Gmail confirmation with photos from a past trip and videos a user watched on YouTube to provide more relevant recommendations or answers.\n\nThat contextual depth highlights Google's strategic positioning in the AI race. Rivals such as OpenAI and Anthropic offer powerful standalone models, but they do not control consumer platforms on the scale of Gmail, YouTube, Photos, or Search. Google does, and Personal Intelligence is designed to turn that reach into differentiated value.\n\n\"The best assistants don't just know the world; they know you and help you navigate it,\" said Google VP Josh Woodward, who oversees the Gemini app, Google Labs, and AI Studio. \"This marks our next step toward making Gemini more personal, proactive, and powerful.\"\n\nThe company described the feature as a way for Gemini to move beyond reactive answers toward proactive assistance. The company argues this will make Gemini more useful in everyday tasks, from shopping and travel planning to content recommendations.\n\nThe rollout begins on the web, Android, and iOS. For now, Personal Intelligence is limited to personal Google accounts and is not available for Workspace users, including business, enterprise, and education accounts.\n\nPrivacy and control are central to the launch, Google said. The feature is off by default, and users must explicitly choose which apps to connect. Even when enabled, Gemini will not personalize every response, instead applying Personal Intelligence only when it determines it will be helpful. Users can opt out of personalization for specific responses, disconnect apps at any time, and manage or delete past chats.\n\nIn a sign of how Google is shipping fast these days, Woodward warned about potential \"over-personalization,\" where the model makes connections between unrelated topics.\n\n\"When you see this, please provide feedback by giving the response a 'thumbs down,'\" he wrote in a blog announcing the feature.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "gemini app",
      "personal intelligence",
      "apps",
      "feature",
      "users",
      "google",
      "rivals",
      "consumer",
      "responses",
      "across"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-personal-intelligence-app-empire-gemini-edge-openai-anthropic-ai-2026-1",
    "thumbnail_url": "https://i.insider.com/696724f704eda4732f2f07ef?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:08.780Z",
    "topic": "finance"
  },
  {
    "slug": "walmarts-head-of-ai-reveals-the-key-difference-between-its-shopping-deals-with-google-gemini-and-chatgpt",
    "title": "Walmart's head of AI reveals the key difference between its shopping deals with Google Gemini and ChatGPT",
    "description": "OpenAI broke new ground when it enabled shopping within ChatGPT, but Walmart's head of AI said the retailer's new Google Gemini deal goes further.",
    "fullText": "The AI shopping war is heating up, and Walmart is positioning itself to come out on top.\n\nThe concept of letting a chatbot buy things on your behalf leapt from the hypothetical realm into reality when ChatGPT rolled out a batch of shopping experiences with major retailers in November.\n\nThen, on Sunday, Google's AI platform Gemini announced its own commerce approach, which it developed in partnership with many of the same retailers, including the world's largest, Walmart.\n\nWhile both services promise to allow customers to find products and complete transactions in a more conversational and automated way, Walmart's new head of AI, Daniel Danker, said Tuesday that the way Gemini handles transactions is more seamless than ChatGPT does.\n\n\"We're essentially having their AI agent, Gemini, partner with our AI agent to create a unified shopping journey,\" he said at the ICR Conference in Orlando. \"Imagine it like a window inside of Gemini where our shopping agent kicks in and helps you complete that purchase.\"\n\nGoogle said its new standards create a common language for different companies' AI agents to interact with.\n\nWith Gemini, Danker said, Walmart is able to link a customer's chat session with their existing Walmart profile and shopping sessions where Gemini wasn't involved.\n\n\"For the most part, our customers aren't just customers; they're often members. And so, they're getting great delivery fees and a great experience that's really attuned to them,\" he said, referring to the subscription service Walmart+. \"That member experience shows up directly within Gemini.\"\n\nDanker said he expects agentic shopping to help Walmart capture more sales from people who didn't set out intending to make a purchase. He said this new approach could enable an almost seamless transaction when a person enlists a chatbot to help solve a problem.\n\nFor example, if someone turned to Gemini for tips on how to remove a wine stain from a particular brand of carpet, a cleaning product could be added to their existing shopping cart for delivery in one combined shipment, he said.\n\nDanker said working with both ChatGPT and Gemini sets Walmart up to win in AI.\n\nIt appears that chatbot-powered shopping is here to stay, with Morgan Stanley analysts estimating that agentic sales could add $115 billion to US e-commerce spending by 2030.\n\nDanker is betting that Walmart's long-standing reputation for low prices and its growing strength in delivery will give the company a significant edge with customers in AI.\n\n\"The most important currency in an agentic shopping world is actually trust and affordability,\" Danker said. \"Without trust and affordability, it's very difficult for customers to hand the wheel to someone else and expect that the right thing will happen.\"\n\nDanker said Walmart's broad selection, low prices, and fast fulfillment help it appear more frequently in Gemini and ChatGPT's shopping recommendations.\n\n\"That doesn't just serve one need, but serves a whole bunch of needs,\" he said.",
    "readingTime": 3,
    "keywords": [
      "agentic shopping",
      "customers",
      "gemini",
      "walmart's",
      "delivery",
      "walmart",
      "danker",
      "chatbot",
      "retailers",
      "approach"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/walmart-ai-head-reveals-difference-in-gemini-and-chatgpt-shopping-2026-1",
    "thumbnail_url": "https://i.insider.com/6966c44964858d02d2184c18?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:08.564Z",
    "topic": "finance"
  },
  {
    "slug": "trumps-focus-on-energy-costs-could-derail-the-ai-boom-warns-a-tech-megabull",
    "title": "Trump's focus on energy costs could derail the AI boom, warns a tech mega-bull",
    "description": "Outspoken tech bull Dan Ives thinks President Trump's plan to curb energy costs has negative implications for America's AI buildout.",
    "fullText": "President Donald Trump has made energy costs his latest affordability target, and he's zeroed in on data centers run by Big Tech firms.\n\nAccording to Wall Street tech bull Dan Ives, this complicates the picture for the AI buildout that's been underway for the last few years.\n\nTrump said on Monday that the administration was working with companies to \"secure their commitment to the American People.\" On Tuesday, Microsoft revealed plans aimed at reducing rising utility bills.\n\nIves said that Trump's focus on energy use among big data center players poses problems for the AI boom, and that the president's plan to have Big Tech firms \"pay their own way\" is a fresh headwind for the sector.\n\n\"This will create a larger bottleneck with big tech organizations looking to build out large data center footprints as quickly as possible without impacting the bottom-line, with this potentially slowing down the data center buildouts shortages/issues to fuel data center buildouts,\" he wrote.\n\nIves added that Trump's push comes at a time when the US tech sector is entering a key phase of what he calls the \"AI revolution,\" and that he expects more companies to soon follow Microsoft's lead.\n\n\"We expect other Big Tech organizations to follow soon after given the increased scrutiny from federal, state, and local governments to address major concerns with large-scale data center buildouts,\" he said.\n\nThe analyst acknowledged that the rapid data center expansion has played a part in rising electricity costs, but said he believes that the AI buildout has implications beyond the domestic economy.\n\nIves highlighted the possibility of the US falling behind China in the global AI arms race, something that Trump has said he does not want to happen. Ives suggested that impeding the industry's progress now could compromise the broader US AI agenda.\n\n\"We believe this will be a continuous back and forth battle between Big Tech players and the Trump administration with data center buildouts an important aspect of fueling the AI Revolution over the coming years,\" Ives added.",
    "readingTime": 2,
    "keywords": [
      "big tech",
      "tech firms",
      "tech organizations",
      "center buildouts",
      "energy",
      "administration",
      "rising",
      "trump's",
      "players",
      "sector"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/donald-trump-power-costs-red-flag-dan-ives-ai-stocks-2026-1",
    "thumbnail_url": "https://i.insider.com/6967b296764ca5f34d2a6abd?width=1200&format=jpeg",
    "created_at": "2026-01-14T18:20:08.547Z",
    "topic": "finance"
  },
  {
    "slug": "granblue-fantasy-developer-cygames-opens-ai-studio-but-doesnt-want-you-to-worry-about-it",
    "title": "Granblue Fantasy Developer Cygames Opens AI Studio, But Doesn't Want You To Worry About It",
    "description": "Cygames, the developer and publisher behind the popular series of Granblue Fantasy video games, just opened an AI-focused studio. In response to the backlash toward this announcement, Cygames put out a message trying to assuage players' fears about generative AI's use in its games. The message isn't very reassuring, though.\nThis kerfuffle started on January 9, when a post on Cygames' Japanese website revealed that Cygames AI Studio had been established. The announcement explains that this new studio will research and develop its own models and provide services to others while drawing on Cygames' game development history and expertise.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/granblue-fantasy-developer-cygames-opens-ai-studio-but-doesnt-want-you-to-worry-about-it/1100-6537389/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1849/18498836/4635234-cygames.jpg",
    "created_at": "2026-01-14T18:20:07.025Z",
    "topic": "tech"
  },
  {
    "slug": "google-gemini-is-about-to-get-to-know-you-way-better",
    "title": "Google Gemini Is About to Get to Know You Way Better",
    "description": "Use AI to connect and reason across your Google apps in just a few clicks.",
    "fullText": "Since the days when Google Gemini was still called Bard, it's been able to connect with the company's other productivity apps to help pull context from them to answer your questions—but you still had to connect those apps to the AI manually using extensions. And even after bringing your apps together, you usually had to tell Gemini where to look for your data to get much use out of its abilities. For Instance, if you wanted it to pull information from your emails, you might have started a prompt with \"Search my email.\"\n\nNow, Google is making it easier to connect Gemini to its various services, and adding \"reasoning\" when pulling context from across your Google Workspace. It's calling the feature \"Personal Intelligence.\"\n\nRolling out in beta for paid subscribers in the U.S. today (and coming to other countries and free users \"soon\"), Personal Intelligence is an opt-in feature that currently works with Gmail, Photos, YouTube, and Search, all of which you can connect in one tap while setting up the feature.\n\nThat alone makes it more convenient than a collection of extensions, but there are supposedly a few upgrades to general usability as well. The biggest is that Gemini will apparently be able to \"learn\" about you from a grab bag of sources all at once, without you having to specify where to look, and use that information to answer your questions.\n\nIn an example, Google has a user say \"I need to replace the tires for my car. Which ones would you suggest for me?\" The bot then runs through multiple reasoning steps, pulling from all the data available to it, to find out what car the prompter drives and which tires would be best for it in the conditions the prompter tends to drive in. Specifically, in the example, it references actual vacations the prompter had taken in the past, using Google Photos data, while also using Gmail data to help the prompter find their car's specific trim. This can take a while, which is why there's an \"Answer now\" button next to the reasoning progress bar to stop the bot from getting stuck. In the example, it took about 10 seconds for the AI to generate a response.\n\nGoogle is promising its typical Workspace privacy guarantees with Personal Intelligence, saying \"because this data already lives at Google securely, you don't have to send sensitive data elsewhere to start personalizing your experience.\" In other words, it's not going to move the needle on how much data about you Google can access, but at least it'll prevent you from having to connect your Workspace to third parties. The company also promises that data Personal Intelligence pulls from sources like Gmail won't be used to train Gemini, but that \"specific prompts and responses\" might be, at least after personal data has been filtered out.\n\nGoogle also says, \"Gemini will try to reference or explain the information it used from your connected sources so you can verify it,\" although we don't have any examples of that in action yet. It's worth keeping an eye out, though, if you're worried about hallucinations. To that end, the company does suggest asking Gemini \n\nGoogle says that eligible users should see an invitation to try Personal Intelligence on the Gemini home screen as soon as it's rolled out to them, but if you don't, you can turn it on manually by following these steps:\n\nOpen Gemini and click or tap Settings.\n\nClick or Tap Personal Intelligence.\n\nUnder Connected Apps, select which apps you would like Personal Intelligence to take information from.\n\nAnd that's it! Remember, Personal Intelligence is off by default and is only available for paid subscribers for now, so it may be some time until you can actually use it. Google also stresses the Gemini might not personalize every response, as that will save time on more simple requests. The company also said Personal Intelligence for AI Mode in Google Search is currently planned, but does not have a set release date.",
    "readingTime": 4,
    "keywords": [
      "personal intelligence",
      "connect",
      "apps",
      "prompter",
      "google",
      "gemini",
      "reasoning",
      "feature",
      "gmail",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/gemini-is-about-to-get-to-know-you-way-better?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KEYH36WE07GJ2MQRB77WTBKE/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-14T18:20:06.343Z",
    "topic": "tech"
  },
  {
    "slug": "retail-traders-pile-into-memory-chipmakers-as-ai-boom-squeezes-supplies-lifts-prices",
    "title": "Retail traders pile into memory chipmakers as AI boom squeezes supplies, lifts prices",
    "description": "Retail investors ramped up buying of U.S. memory and data storage chipmakers in January, following 2025's strong momentum on expectations ​that booming artificial intelligence infrastructure demand will tighten supply and lift prices.  An acute ‌global shortage of memory chips is forcing AI and consumer-electronics companies to fight for dwindling supplies, which is ‌expected to support a multi-year backlog for memory chip makers.  Samsung's co-CEO TM Roh described the memory chip shortage as \"unprecedented\" in an interview with Reuters earlier this month, echoing rivals who have warned that constraints could persist for months, if not years, as the AI infrastructure race continues to ⁠hog supplies.",
    "fullText": "Jan 14 (Reuters) - Retail investors ramped up buying of U.S. memory and data storage chipmakers in January, following 2025's strong momentum on expectations ​that booming artificial intelligence infrastructure demand will tighten supply and lift prices.\n\nAn acute ‌global shortage of memory chips is forcing AI and consumer-electronics companies to fight for dwindling supplies, which is ‌expected to support a multi-year backlog for memory chip makers.\n\nSamsung's co-CEO TM Roh described the memory chip shortage as \"unprecedented\" in an interview with Reuters earlier this month, echoing rivals who have warned that constraints could persist for months, if not years, as the AI infrastructure race continues to ⁠hog supplies.\n\nData storage device maker SanDisk, ‌whose shares have soared about 65% so far in 2026, saw more than $7.1 million in retail inflows on Monday alone, the biggest one-day ‍move on record, according to data from Vanda Research.\n\nWestern Digital has seen nearly $10 million in inflows in the first two weeks of January, putting it on course for the strongest monthly showing since ​October 2025, while Seagate Technology recorded more than $2.1 million of inflows so far this ‌year.\n\n2025 was a record year for U.S. retail inflows as individual investors became a major force behind the rally on Wall Street. Total flows from mom-and-pop traders for these three stocks stood at more than $117.2 million last year.\n\nMicron Technology, one of the \"Big Three\" memory makers in the world alongside Samsung and SK Hynix , is up 18% so far in ⁠2026 after rising 240% in 2025.\n\n\"Memory chips are ​certainly among the themes that are exciting our customers ​these days. It's not unusual to see No. 3 Micron positioned among the leaders, but seeing SanDisk in the No. 4 slot tells us that ‍it is more than ⁠simply a coincidence,\" said Steve Sosnick, chief strategist at Interactive Brokers.\n\nMicron and SanDisk were among the five most active stocks on Interactive Brokers' platform over the ⁠past five trading days, Sosnick said.\n\nSanDisk, whose shares have risen nearly ten-fold since its February 2025 listing, is ‌the biggest holding of the actively managed Roundhill Meme Stock ETF.",
    "readingTime": 2,
    "keywords": [
      "memory chips",
      "memory chip",
      "retail inflows",
      "sandisk",
      "among",
      "investors",
      "storage",
      "january",
      "infrastructure",
      "shortage"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/retail-traders-pile-memory-chipmakers-115721848.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/6799bbeadb3709db51d52460937fbd95",
    "created_at": "2026-01-14T18:20:02.623Z",
    "topic": "finance"
  },
  {
    "slug": "lies-damned-lies-and-proofs-formal-methods-are-not-slopless",
    "title": "Lies, Damned Lies and Proofs: Formal Methods Are Not Slopless",
    "description": "There's been a lot of chatter recently on HN and elsewhere about how formal verification is the obvious use-case for AI. While we broadly agree, we think much of the discourse is kinda wrong because it incorrectly presumes formal = slopless.",
    "fullText": "We appreciate comments from Christopher Henson, Zeke Medley, Ankit Kumar, and Pete Manolios. This post was initialized by Max’s twitter thread.\n\nThere's been a lot of chatter recently on HN and elsewhere about how formal verification is the obvious use-case for AI. While we broadly agree, we think much of the discourse is kinda wrong because it incorrectly presumes formal = slopless.[1]Over the years, we have written our fair share of good and bad formal code. In this post, we hope to convince you that formal code can be sloppy, and that this has serious implications for anyone who hopes to bootstrap superintelligence by using formality to reinforce “good” reasoning.\n\nA mainstay on the Lean Zulip named Gas Station Manager has written that hallucination-free program synthesis[2]is achievable by vibing software directly in Lean, with the caveat that the agent also needs to prove the software correct. The AI safety case is basically: wouldn’t it be great if a cheap (i.e. O(laptop)) signal could protect you from sycophantic hubris and other classes of mistake, without you having to manually audit all outputs?\n\nRecently a computer scientist (who we will spare from naming) was convinced he had solved a major mathematics problem. Lean was happy with it, he reasoned, given that his proof mostly worked, with just a few red squigglies. As seasoned proof engineers, we could have told him that in proof engineering, the growth in further needed edits is superlinear in number-of-red-squigglies (unlike in regular programming). The difference between mistakes in a proof and mistakes in a program is that you cannot fix a broken proof in a way that changes its formal goal (the theorem statement). In contrast, many, if not most changes to traditional software impact its formal spec, for example by adding a side-effect or changing the shape of an output. Therefore proof bugs are 1) harder to fix, and 2) more likely to imply that your goal is fundamentally unachievable (the theorem is wrong). This made up chart illustrates the principle, a rough “lore” level consensus in the field without any hard data.\n\nIt is possible he will post a finished proof, but the referee-time of bets he made has lapsed, so we can take away some lessons. Did our protagonist take to heart the promise of formal methods as slopless?\n\nIn much the same way that vibed code might work yet be “sloppy” in the sense that it’s difficult to maintain, vibed formal models can be correct, yet very challenging to prove anything about.\n\nOften when you model a system – or write code in a theorem-prover, with the intention of proving things about it – you actually need to make implementation decisions informed by the limitations and capabilities of the prover. For example, it's pretty common that inducting in one direction (say, car/head) on a list will be easy for a prover but the other direction (say cdr/tail) will be difficult. (This is a necessary evil if you want the prover to not enter infinite rewrite loops.) Thus, as an example, you might implement isort in a particular “direction” in order to make the proofs easier about it. If you want to autoformalize arbitrary code in a way that makes proofs straightforward, you’ll need models that understand how to implement something in a way that’s idiomatic for the given interactive theorem-prover.\n\nThis is a solvable problem but a real one nonetheless. For example, one Aristotle user we spoke to reported: “... in Lean you can put theorems inside mutual blocks to let them use each other. I wrote such a theorem, but then later realized proving it this way would be unnecessarily difficult. [...] The model won't do this, so it spent >24 hours on this almost hopeless proof.” Autoformalization companies like math.inc, Harmonic, Axiom, Logical Intelligence, etc. are actively working on improving their models to have this kind of expert folklore knowledge as we speak, but we’re not quite there yet.\n\nThere are basically two ways to make your software amenable to an interactive theorem prover (ITP). The first is to lift it into an ITP using a formal semantics – somewhat like a compiler or interpreter for the original language but implemented in the ITP itself. In this case, you can define the lifting so that it produces functionally equivalent code (say, Lean code that “does the same thing” as the input Python) but in a shape that the theorem-prover tends to like (incorporating heuristics like the car/cdr one mentioned above). The second approach is to just rewrite the original software directly in the language of the ITP, making those kinds of idiomacy improvements as-you-go. Both approaches, however, produce the same formal problem: ensuring that the software you wanted to study in the first place is semantically equivalent to the thing you introduced in the theorem-prover. IE., either ensuring the lifting is correct, or ensuring the manual translation is equivalent. Let’s dig into some of the ways this can be difficult.\n\nWhen we talk about using formal methods to assure that LLM-generated code is safe, what we want is a short, readable description of what the generated code is intended to do, some proof (which might be far too boring and long to read) that the code does this, and the ability to run the proof through a prover and validate that it indeed proves the aforementioned statement. But this is not necessarily a reasonable ask, regardless of model intelligence.\n\nFirst, it’s very common that you mis-define some concept such that the proof is accidentally trivial. For example, when defining a lifting from Python to Lean you might prove that the lifting preserves the semantics of the original Python code, but your proof could be undermined by the presumption that the code terminates, making it basically useless.\n\nSecond, if you re-implement the original software in your ITP of choice, your re-implementation might not be fully faithful, particularly if it’s LLM-generated. For example, the LLM might say, \"The code you wanted me to verify was too complex, so I rewrote it to be simpler and proved the simpler thing correct.\" Well, yeah, but the bugs I wanted you to find were in the complexity. As a concrete example, we asked an early version of Gemini to write a property based test (PBT) for a (deliberately flawed) isort implementation which we provided; Gemini did so but rewrote the isort code to be correct in the process and then executed the PBT and cheerfully reported that it passed.\n\nThese first two problems are commonly addressed using tests which compare the original software to its representation in the ITP. For example, we (Max) did this with coauthors for GossipSub, connecting the Golang implementation to its ACL2(s) model via both unit tests and property-based tests.[3]To quote Knuth: “Beware of bugs in the above code; I have only proved it correct, not tried it.”\n\nThird, you need to decide how far “down the stack” you want to go. That is to say, the software you want to verify operates over some kind of more complex system, for instance, maybe it’s C code which gets compiled down to X86 and runs on a particular chip, or maybe it’s a controller for a nuclear reactor and part of the system is the actual physical dynamics of the reactor. Do you really want your proof to involve specifying the semantics of the C compiler and the chip, or the way that the temperature and other variables fluctuate in the reactor? Keeping in mind these semantics might not truly be known – e.g., RowHammer can be viewed as an attack on our understanding of the semantics of the chip. In essence, you can only get more specificity by vastly increasing the length of your proof statement to capture the semantics of the underlying system, which then produces a new (and perhaps equally difficult) code review problem. Typically this problem is handled by leaving the underlying semantics nondeterministic, so your proof is stronger (it holds regardless of how the C compiler handles floating point, or how the temperature fluctuates in the nuclear silo) but often the thing you want to prove really does require some pretty specific guarantees about those underlying semantics, and ensuring those guarantees are “reasonable” can be extraordinarily difficult.\n\nThe AI might introduce axioms that conflict with your own presuppositions or the specific requirements of your domain. In Lean, for example, the Axiom of Choice (Classical.choice) is available but transforms a proof from a constructive one—where you can actually compute a result—into a non-constructive one. An AI tasked with verifying a program might realize that a proof is significantly easier if it assumes AC. It might inform you that the theorem is \"proven,\" and the prover will confirm this, but you may not realize that the resulting proof is now a \"lie\" for your specific use case. If you needed that proof to generate an executable, verified algorithm, the introduction of non-constructive axioms shifts you into an incompatible register.\n\nThe person designing the harness for the AI needs to be an expert who knows how to parse these imports and error messages. Without that oversight, the AI will naturally gravitate toward the path of least resistance—even if that path involves an axiomatic shift that renders the entire exercise useless for the user's true intent.\n\nConsider the proof assistant ACL2, which accepts arbitrary lisp code.[4]You write defttag, the trusted tag to open the “trust me” scope. In other words, defttag offloads the soundness obligations to the user. Observe a proof that 1+1=3 in ACL2 with defttag.\n\n“Well yeah”, perhaps comes a reply. “It only looks like 1+1=3 in the nonsensical sense if you deliberately ignore that the meaning of plus has shifted”. “Besides”, they continue. “When my coworker sends me code with defttag in it, I read it very rigorously”. Our retort is that we don’t assume our coworkers are competent or trustworthy, we assume that they’re AIs with a tendency to reward hack. To recap:\n\nAdditionally, proof tools like Lean pile a bunch of ergonomic and notational niceties on top of their core calculus, in Lean’s case with powerful metaprogramming. But this metaprogramming can lead to backdoors much like the ACL2 example.[6]\n\nFrom nothing arises everything. From a proof of false you can derive literally any proposition.\n\nIn Agda, a calculus of inductive constructions popular with mathematical type theorists, the github issue label “false” tracking proofs of false is standing at 9 open and 74 closed issues at time of this writing. A proof of false is a soundness bug[7], which if you think proof synthesis plays a role in high stakes AI security (like SL5), means you have to be paranoid about a glaring attack surface.\n\nWhile we can’t yet think of a case of sicophancy/hubris that was accelerated by an arcane proof of false, we expect this becomes increasingly likely as insecure program synthesis tools get more capable and accessible in contexts where they are incentivized to reward-hack a proof.\n\nIf someone says \"stats don’t lie\" you say \"well don’t be naive, you can tell misleading stories with technically true statistics\".[8]Formal verification is the same. Don’t be lured into the false sense of security. To paraphrase Twain, “There are three kinds of lies: lies, damned lies, and proofs.” We already know models lie to us; we should fully expect them to prove falsehoods, too.\n\nIn spite of our warnings, which may seem pessimistic, we’re working on secure program synthesis (or what Mike Dodds calls scalable formal oversight) for AI security. The reason we can work on this anyway is because we see a lit path, principally routing through specification elicitation[9]and validation as well as hardened proof cores and (the cherry on top) superpowered proof synthesis. Spec elicitation and validation, in particular, have not seen the upside from language model assisted transpilation fully harvested just yet.\n\nThis intuition might be in part driven by academic papers that push formality as a cure to sloppiness, e.g., Run Your Research and HACMS. But even formally verified software can be buggy! ↩︎\n\nAs a historical aside, the original citation for program synthesis is: Church, A.: Application of recursive arithmetic to the problem of circuit synthesis (7 1957), presented at IDA, as cited in doi:10.2307/2271310. ↩︎\n\nCedar comes to mind as a similar case-study in Lean. ↩︎\n\nThis feature is useful for proving things about real-world LISP code, or connecting ACL2 code which is proven to be correct to real-world systems via LISP harnesses. ↩︎\n\nSee also Pollack-consistency, a kind of LangSec concept of theorem-prover backdooring. ↩︎\n\nThere are some subtleties here we elide, which Christopher Henson plans to explore in a more technical forthcoming blog post. ↩︎\n\nSee also The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant. ↩︎\n\nAcademia is certain that specification is hard (see also Formal Methods for Security) and we should fix it, but unsure as to why or how to improve the situation. ↩︎",
    "readingTime": 11,
    "keywords": [
      "direction say",
      "software directly",
      "underlying semantics",
      "program synthesis",
      "original software",
      "proof synthesis",
      "formal code",
      "the ai",
      "correct",
      "difficult"
    ],
    "qualityScore": 1,
    "link": "https://www.lesswrong.com/posts/rhAPh3YzhPoBNpgHg/lies-damned-lies-and-proofs-formal-methods-are-not-slopless",
    "thumbnail_url": "https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rhAPh3YzhPoBNpgHg/dmzagyhxinfix2jh3oxz",
    "created_at": "2026-01-14T12:25:04.845Z",
    "topic": "tech"
  },
  {
    "slug": "hirebetterio-ai-tools-to-reduce-manual-recruiter-work",
    "title": "Hirebetter.io – AI tools to reduce manual recruiter work",
    "description": "hirebetter.io is your all-in-one automation platform for talent acquisition. Streamline sourcing, outreach, and hiring workflows. No technical expertise required.",
    "fullText": "The platform is incredibly easy to use, even for someone without a recruiting background. Within minutes I was able to source candidates, generate clear summaries, and access structured interview questions that gave me confidence in my process. What really stood out were the insights it provided. Instead of getting lost in endless CVs, I had actionable information that made decision-making faster and more informed.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://hirebetter.io",
    "thumbnail_url": "https://hirebetter.io/metadata.png",
    "created_at": "2026-01-14T12:25:04.067Z",
    "topic": "tech"
  },
  {
    "slug": "how-to-write-a-good-spec-for-ai-agents",
    "title": "How to write a good spec for AI agents",
    "description": "Learn how to write effective specifications for AI coding agents to improve clarity, focus, and productivity in your AI-driven development workflows.",
    "fullText": "TL;DR: Aim for a clear spec covering just enough nuance (this may include structure, style, testing, boundaries) to guide the AI without overwhelming it. Break large tasks into smaller ones vs. keeping everything in one large prompt. Plan first in read-only mode, then execute and iterate continuously.\n\n“I’ve heard a lot about writing good specs for AI agents, but haven’t found a solid framework yet. I could write a spec that rivals an RFC, but at some point the context is too large and the model breaks down.”\n\nMany developers share this frustration. Simply throwing a massive spec at an AI agent doesn’t work - context window limits and the model’s “attention budget” get in the way. The key is to write smart specs: documents that guide the agent clearly, stay within practical context sizes, and evolve with the project. This guide distills best practices from my use of coding agents including Claude Code and Gemini CLI into a framework for spec-writing that keeps your AI agents focused and productive.\n\nWe’ll cover five principles for great AI agent specs, each starting with a bolded takeaway.\n\nKick off your project with a concise high-level spec, then have the AI expand it into a detailed plan.\n\nInstead of over-engineering upfront, begin with a clear goal statement and a few core requirements. Treat this as a “product brief” and let the agent generate a more elaborate spec from it. This leverages the AI’s strength in elaboration while you maintain control of the direction. This works well unless you already feel you have very specific technical requirements that must be met from the start.\n\nWhy this works: LLM-based agents excel at fleshing out details when given a solid high-level directive, but they need a clear mission to avoid drifting off course. By providing a short outline or objective description and asking the AI to produce a full specification (e.g. a spec.md), you create a persistent reference for the agent. Planning in advance matters even more with an agent - you can iterate on the plan first, then hand it off to the agent to write the code. The spec becomes the first artifact you and the AI build together.\n\nPractical approach: Start a new coding session by prompting, “You are an AI software engineer. Draft a detailed specification for [project X] covering objectives, features, constraints, and a step-by-step plan.” Keep your initial prompt high-level - e.g. “Build a web app where users can track tasks (to-do list), with user accounts, a database, and a simple UI”. The agent might respond with a structured draft spec: an overview, feature list, tech stack suggestions, data model, and so on. This spec then becomes the “source of truth” that both you and the agent can refer back to. GitHub’s AI team promotes spec-driven development where “specs become the shared source of truth… living, executable artifacts that evolve with the project”. Before writing any code, review and refine the AI’s spec. Make sure it aligns with your vision and correct any hallucinations or off-target details.\n\nUse Plan Mode to enforce planning-first: Tools like Claude Code offer a Plan Mode that restricts the agent to read-only operations - it can analyze your codebase and create detailed plans but won’t write any code until you’re ready. This is ideal for the planning phase: start in Plan Mode (Shift+Tab in Claude Code), describe what you want to build, and let the agent draft a spec while exploring your existing code. Ask it to clarify ambiguities by questioning you about the plan. Have it review the plan for architecture, best practices, security risks, and testing strategy. The goal is to refine the plan until there’s no room for misinterpretation. Only then do you exit Plan Mode and let the agent execute. This workflow prevents the common trap of jumping straight into code generation before the spec is solid.\n\nUse the spec as context: Once approved, save this spec (e.g. as SPEC.md) and feed relevant sections into the agent as needed. Many developers using a strong model do exactly this - the spec file persists between sessions, anchoring the AI whenever work resumes on the project. This mitigates the forgetfulness that can happen when the conversation history gets too long or when you have to restart an agent. It’s akin to how one would use a Product Requirements Document (PRD) in a team: a reference that everyone (human or AI) can consult to stay on track. Experienced folks often “write good documentation first and the model may be able to build the matching implementation from that input alone” as one engineer observed. The spec is that documentation.\n\nKeep it goal-oriented: A high-level spec for an AI agent should focus on what and why, more than the nitty-gritty how (at least initially). Think of it like the user story and acceptance criteria: Who is the user? What do they need? What does success look like? (e.g. “User can add, edit, complete tasks; data is saved persistently; the app is responsive and secure”). This keeps the AI’s detailed spec grounded in user needs and outcome, not just technical to-dos. As the GitHub Spec Kit docs put it, provide a high-level description of what you’re building and why, and let the coding agent generate a detailed specification focusing on user experience and success criteria. Starting with this big-picture vision prevents the agent from losing sight of the forest for the trees when it later gets into coding.\n\nTreat your AI spec as a structured document (PRD) with clear sections, not a loose pile of notes.\n\nMany developers treat specs for agents much like traditional Product Requirement Documents (PRDs) or System Design docs - comprehensive, well-organized, and easy for a “literal-minded” AI to parse. This formal approach gives the agent a blueprint to follow and reduces ambiguity.\n\nThe six core areas: GitHub’s analysis of over 2,500 agent configuration files revealed a clear pattern: the most effective specs cover six areas. Use this as a checklist for completeness:\n\n1. Commands: Put executable commands early - not just tool names, but full commands with flags: npm test, pytest -v, npm run build. The agent will reference these constantly.\n\n2. Testing: How to run tests, what framework you use, where test files live, and what coverage expectations exist.\n\n3. Project structure: Where source code lives, where tests go, where docs belong. Be explicit: “src/ for application code, tests/ for unit tests, docs/ for documentation.”\n\n4. Code style: One real code snippet showing your style beats three paragraphs describing it. Include naming conventions, formatting rules, and examples of good output.\n\n5. Git workflow: Branch naming, commit message format, PR requirements. The agent can follow these if you spell them out.\n\n6. Boundaries: What the agent should never touch - secrets, vendor directories, production configs, specific folders. “Never commit secrets” was the single most common helpful constraint in the GitHub study.\n\nBe specific about your stack: Say “React 18 with TypeScript, Vite, and Tailwind CSS” not “React project.” Include versions and key dependencies. Vague specs produce vague code.\n\nUse a consistent format: Clarity is king. Many devs use Markdown headings or even XML-like tags in the spec to delineate sections, because AI models handle well-structured text better than free-form prose. For example, you might structure the spec as:\n\nThis level of organization not only helps you think clearly, it helps the AI find information. Anthropic engineers recommend organizing prompts into distinct sections (like <background>, <instructions>, <tools>, <output_format> etc.) for exactly this reason - it gives the model strong cues about which info is which. And remember, “minimal does not necessarily mean short” - don’t shy away from detail in the spec if it matters, but keep it focused.\n\nIntegrate specs into your toolchain: Treat specs as “executable artifacts” tied to version control and CI/CD. The GitHub Spec Kit uses a four-phase, gated workflow that makes your specification the center of your engineering process. Instead of writing a spec and setting it aside, the spec drives the implementation, checklists, and task breakdowns. Your primary role is to steer; the coding agent does the bulk of the writing. Each phase has a specific job, and you don’t move to the next one until the current task is fully validated:\n\n1. Specify: You provide a high-level description of what you’re building and why, and the coding agent generates a detailed specification. This isn’t about technical stacks or app design - it’s about user journeys, experiences, and what success looks like. Who will use this? What problem does it solve? How will they interact with it? Think of it as mapping the user experience you want to create, and letting the coding agent flesh out the details. This becomes a living artifact that evolves as you learn more.\n\n2. Plan: Now you get technical. You provide your desired stack, architecture, and constraints, and the coding agent generates a comprehensive technical plan. If your company standardizes on certain technologies, this is where you say so. If you’re integrating with legacy systems or have compliance requirements, all of that goes here. You can ask for multiple plan variations to compare approaches. If you make internal docs available, the agent can integrate your architectural patterns directly into the plan.\n\n3. Tasks: The coding agent takes the spec and plan and breaks them into actual work - small, reviewable chunks that each solve a specific piece of the puzzle. Each task should be something you can implement and test in isolation, almost like test-driven development for your AI agent. Instead of “build authentication,” you get concrete tasks like “create a user registration endpoint that validates email format.”\n\n4. Implement: Your coding agent tackles tasks one by one (or in parallel). Instead of reviewing thousand-line code dumps, you review focused changes that solve specific problems. The agent knows what to build (specification), how to build it (plan), and what to work on (task). Crucially, your role is to verify at each phase: Does the spec capture what you want? Does the plan account for constraints? Are there edge cases the AI missed? The process builds in checkpoints for you to critique, spot gaps, and course-correct before moving forward.\n\nThis gated workflow prevents what Willison calls “house of cards code” - fragile AI outputs that collapse under scrutiny. Anthropic’s Skills system offers a similar pattern, letting you define reusable Markdown-based behaviors that agents invoke. By embedding your spec in these workflows, you ensure the agent can’t proceed until the spec is validated, and changes propagate automatically to task breakdowns and tests.\n\nConsider agents.md for specialized personas: For tools like GitHub Copilot, you can create agents.md files that define specialized agent personas - a @docs-agent for technical writing, a @test-agent for QA, a @security-agent for code review. Each file acts as a focused spec for that persona’s behavior, commands, and boundaries. This is particularly useful when you want different agents for different tasks rather than one general-purpose assistant.\n\nDesign for Agent Experience (AX): Just as we design APIs for developer experience (DX), consider designing specs for “Agent Experience.” This means clean, parseable formats: OpenAPI schemas for any APIs the agent will consume, llms.txt files that summarize documentation for LLM consumption, and explicit type definitions. The Agentic AI Foundation (AAIF) is standardizing protocols like MCP (Model Context Protocol) for tool integration - specs that follow these patterns are easier for agents to consume and act on reliably.\n\nPRD vs SRS mindset: It helps to borrow from established documentation practices. For AI agent specs, you’ll often blend these into one document (as illustrated above), but covering both angles serves you well. Writing it like a PRD ensures you include user-centric context (“the why behind each feature”) so the AI doesn’t optimize for the wrong thing. Expanding it like an SRS ensures you nail down the specifics the AI will need to actually generate correct code (like what database or API to use). Developers have found that this extra upfront effort pays off by drastically reducing miscommunications with the agent later.\n\nMake the spec a “living document”: Don’t write it and forget it. Update the spec as you and the agent make decisions or discover new info. If the AI had to change the data model or you decided to cut a feature, reflect that in the spec so it remains the ground truth. Think of it as version-controlled documentation. In spec-driven workflows, the spec drives implementation, tests, and task breakdowns, and you don’t move to coding until the spec is validated. This habit keeps the project coherent, especially if you or the agent step away and come back later. Remember, the spec isn’t just for the AI - it helps you as the developer maintain oversight and ensure the AI’s work meets the real requirements.\n\nDivide and conquer: give the AI one focused task at a time rather than a monolithic prompt with everything at once.\n\nExperienced AI engineers have learned that trying to stuff the entire project (all requirements, all code, all instructions) into a single prompt or agent message is a recipe for confusion. Not only do you risk hitting token limits, you also risk the model losing focus due to the “curse of instructions” - too many directives causing it to follow none of them well. The solution is to design your spec and workflow in a modular way, tackling one piece at a time and pulling in only the context needed for that piece.\n\nThe curse of too much context/instructions: Research has confirmed what many devs anecdotally saw: as you pile on more instructions or data into the prompt, the model’s performance in adhering to each one drops significantly. One study dubbed this the “curse of instructions”, showing that even GPT-4 and Claude struggle when asked to satisfy many requirements simultaneously. In practical terms, if you present 10 bullet points of detailed rules, the AI might obey the first few and start overlooking others. The better strategy is iterative focus. Guidelines from industry suggest decomposing complex requirements into sequential, simple instructions as a best practice. Focus the AI on one sub-problem at a time, get that done, then move on. This keeps the quality high and errors manageable.\n\nDivide the spec into phases or components: If your spec document is very long or covers a lot of ground, consider splitting it into parts (either physically separate files or clearly separate sections). For example, you might have a section for “Backend API Spec” and another for “Frontend UI Spec.” You don’t need to always feed the frontend spec to the AI when it’s working on the backend, and vice versa. Many devs using multi-agent setups even create separate agents or sub-processes for each part - e.g. one agent works on database/schema, another on API logic, another on frontend - each with the relevant slice of the spec. Even if you use a single agent, you can emulate this by copying only the relevant spec section into the prompt for that task. Avoid context overload: Don’t mix authentication tasks with database schema changes in one go, as the DigitalOcean AI guide warns. Keep each prompt tightly scoped to the current goal.\n\nExtended TOC / Summaries for large specs: One clever technique is to have the agent build an extended Table of Contents with summaries for the spec. This is essentially a “spec summary” that condenses each section into a few key points or keywords, and references where details can be found. For example, if your full spec has a section on “Security Requirements” spanning 500 words, you might have the agent summarize it to: “Security: use HTTPS, protect API keys, implement input validation (see full spec §4.2)”. By creating a hierarchical summary in the planning phase, you get a bird’s-eye view that can stay in the prompt, while the fine details remain offloaded unless needed. This extended TOC acts as an index: the agent can consult it and say “aha, there’s a security section I should look at”, and you can then provide that section on demand. It’s similar to how a human developer skims an outline and then flips to the relevant page of a spec document when working on a specific part.\n\nTo implement this, you can prompt the agent after writing the spec: “Summarize the spec above into a very concise outline with each section’s key points and a reference tag.” The result might be a list of sections with one or two sentence summaries. That summary can be kept in the system or assistant message to guide the agent’s focus without eating up too many tokens. This hierarchical summarization approach is known to help LLMs maintain long-term context by focusing on the high-level structure. The agent carries a “mental map” of the spec.\n\nUtilize sub-agents or “skills” for different spec parts: Another advanced approach is using multiple specialized agents (what Anthropic calls subagents or what you might call “skills”). Each subagent is configured for a specific area of expertise and given the portion of the spec relevant to that area. For instance, you might have a Database Designer subagent that only knows about the data model section of the spec, and an API Coder subagent that knows the API endpoints spec. The main agent (or an orchestrator) can route tasks to the appropriate subagent automatically. The benefit is each agent has a smaller context window to deal with and a more focused role, which can boost accuracy and allow parallel work on independent tasks. Anthropic’s Claude Code supports this by letting you define subagents with their own system prompts and tools. “Each subagent has a specific purpose and expertise area, uses its own context window separate from the main conversation, and has a custom system prompt guiding its behavior,” as their docs describe. When a task comes up that matches a subagent’s domain, Claude can delegate that task to it, with the subagent returning results independently.\n\nParallel agents for throughput: Running multiple agents simultaneously is emerging as “the next big thing” for developer productivity. Rather than waiting for one agent to finish before starting another task, you can spin up parallel agents for non-overlapping work. Willison describes this as “embracing parallel coding agents” and notes it’s “surprisingly effective, if mentally exhausting”. The key is scoping tasks so agents don’t step on each other - one agent codes a feature while another writes tests, or separate components get built concurrently. Orchestration frameworks like LangGraph or OpenAI Swarm can help coordinate these agents, and shared memory via vector databases (like Chroma) lets them access common context without redundant prompting.\n\nSingle vs. multi-agent: when to use each\n\nIn practice, using subagents or skill-specific prompts might look like: you maintain multiple spec files (or prompt templates) - e.g. SPEC_backend.md, SPEC_frontend.md - and you tell the AI, “For backend tasks, refer to SPEC_backend; for frontend tasks refer to SPEC_frontend.” Or in a tool like Cursor/Claude, you actually spin up a subagent for each. This is certainly more complex to set up than a single-agent loop, but it mimics what human developers do - we mentally compartmentalize a large spec into relevant chunks (you don’t keep the whole 50-page spec in your head at once; you recall the part you need for the task at hand, and have a general sense of the overall architecture). The challenge, as noted, is managing interdependencies: the subagents must still coordinate (the frontend needs to know the API contract from the backend spec, etc.). A central overview (or an “architect” agent) can help by referencing the sub-specs and ensuring consistency.\n\nFocus each prompt on one task/section: Even without fancy multi-agent setups, you can manually enforce modularity. For example, after the spec is written, your next move might be: “Step 1: Implement the database schema.” You feed the agent the Database section of the spec only, plus any global constraints from the spec (like tech stack). The agent works on that. Then for Step 2, “Now implement the authentication feature”, you provide the Auth section of the spec and maybe the relevant parts of the schema if needed. By refreshing the context for each major task, you ensure the model isn’t carrying a lot of stale or irrelevant information that could distract it. As one guide suggests: “Start fresh: begin new sessions to clear context when switching between major features”. You can always remind the agent of critical global rules (from the spec’s Constraints section) each time, but don’t shove the entire spec in if it’s not all needed.\n\nUse in-line directives and code TODOs: Another modularity trick is to use your code or spec as an active part of the conversation. For instance, scaffold your code with // TODO comments that describe what needs to be done, and have the agent fill them one by one. Each TODO essentially acts as a mini-spec for a small task. This keeps the AI laser-focused (“implement this specific function according to this spec snippet”) and you can iterate in a tight loop. It’s similar to giving the AI a checklist item to complete rather than the whole checklist at once.\n\nThe bottom line: small, focused context beats one giant prompt. This improves quality and keeps the AI from getting “overwhelmed” by too much at once. As one set of best practices sums up, provide “One Task Focus” and “Relevant info only” to the model, and avoid dumping everything everywhere. By structuring the work into modules - and using strategies like spec summaries or sub-spec agents - you’ll navigate around context size limits and the AI’s short-term memory cap. Remember, a well-fed AI is like a well-fed function: give it only the inputs it needs for the job at hand.\n\nMake your spec not just a to-do list for the agent, but also a guide for quality control - and don’t be afraid to inject your own expertise.\n\nA good spec for an AI agent anticipates where the AI might go wrong and sets up guardrails. It also takes advantage of what you know (domain knowledge, edge cases, “gotchas”) so the AI doesn’t operate in a vacuum. Think of the spec as both coach and referee for the AI: it should encourage the right approach and call out fouls.\n\nUse three-tier boundaries: The GitHub analysis of 2,500+ agent files found that the most effective specs use a three-tier boundary system rather than a simple list of don’ts. This gives the agent clearer guidance on when to proceed, when to pause, and when to stop:\n\n✅ Always do: Actions the agent should take without asking. “Always run tests before commits.” “Always follow the naming conventions in the style guide.” “Always log errors to the monitoring service.”\n\n⚠️ Ask first: Actions that require human approval. “Ask before modifying database schemas.” “Ask before adding new dependencies.” “Ask before changing CI/CD configuration.” This tier catches high-impact changes that might be fine but warrant a human check.\n\n🚫 Never do: Hard stops. “Never commit secrets or API keys.” “Never edit node_modules/ or vendor/.” “Never remove a failing test without explicit approval.” “Never commit secrets” was the single most common helpful constraint in the study.\n\nThis three-tier approach is more nuanced than a flat list of rules. It acknowledges that some actions are always safe, some need oversight, and some are categorically off-limits. The agent can proceed confidently on “Always” items, flag “Ask first” items for review, and hard-stop on “Never” items.\n\nEncourage self-verification: One powerful pattern is to have the agent verify its work against the spec automatically. If your tooling allows, you can integrate checks like unit tests or linting that the AI can run after generating code. But even at the spec/prompt level, you can instruct the AI to double-check: e.g. “After implementing, compare the result with the spec and confirm all requirements are met. List any spec items that are not addressed.” This pushes the LLM to reflect on its output relative to the spec, catching omissions. It’s a form of self-audit built into the process.\n\nFor instance, you might append to a prompt: “(After writing the function, review the above requirements list and ensure each is satisfied, marking any missing ones).” The model will then (ideally) output the code followed by a short checklist indicating if it met each requirement. This reduces the chance it forgets something before you even run tests. It’s not foolproof, but it helps.\n\nLLM-as-a-Judge for subjective checks: For criteria that are hard to test automatically - code style, readability, adherence to architectural patterns - consider using “LLM-as-a-Judge.” This means having a second agent (or a separate prompt) review the first agent’s output against your spec’s quality guidelines. Anthropic and others have found this effective for subjective evaluation. You might prompt: “Review this code for adherence to our style guide. Flag any violations.” The judge agent returns feedback that either gets incorporated or triggers a revision. This adds a layer of semantic evaluation beyond syntax checks.\n\nConformance testing: Willison advocates building conformance suites - language-independent tests (often YAML-based) that any implementation must pass. These act as a contract: if you’re building an API, the conformance suite specifies expected inputs/outputs, and the agent’s code must satisfy all cases. This is more rigorous than ad-hoc unit tests because it’s derived directly from the spec and can be reused across implementations. Include conformance criteria in your spec’s Success section (e.g., “Must pass all cases in conformance/api-tests.yaml”).\n\nLeverage testing in the spec: If possible, incorporate a test plan or even actual tests in your spec and prompt flow. In traditional development, we use TDD or write test cases to clarify requirements - you can do the same with AI. For example, in the spec’s Success Criteria, you might say “These sample inputs should produce these outputs…” or “the following unit tests should pass.” The agent can be prompted to run through those cases in its head or actually execute them if it has that capability. Simon Willison noted that having a robust test suite is like giving the agents superpowers - they can validate and iterate quickly when tests fail. In an AI coding context, writing a bit of pseudocode for tests or expected outcomes in the spec can guide the agent’s implementation. Additionally, you can use a dedicated “test agent” in a subagent setup that takes the spec’s criteria and continuously verifies the “code agent’s” output.\n\nBring your domain knowledge: Your spec should reflect insights that only an experienced developer or someone with context would know. For example, if you’re building an e-commerce agent and you know that “products” and “categories” have a many-to-many relationship, state that clearly (don’t assume the AI will infer it - it might not). If a certain library is notoriously tricky, mention pitfalls to avoid. Essentially, pour your mentorship into the spec. The spec can contain advice like “If using library X, watch out for memory leak issue in version Y (apply workaround Z).” This level of detail is what turns an average AI output into a truly robust solution, because you’ve steered the AI away from common traps.\n\nAlso, if you have preferences or style guidelines (say, “use functional components over class components in React”), encode that in the spec. The AI will then emulate your style. Many engineers even include small examples in the spec, e.g., “All API responses should be JSON. E.g. {“error”: “message”} for errors.” By giving a quick example, you anchor the AI to the exact format you want.\n\nMinimalism for simple tasks: While we advocate thorough specs, part of expertise is knowing when to keep it simple. For relatively simple, isolated tasks, an overbearing spec can actually confuse more than help. If you’re asking the agent to do something straightforward (like “center a div on the page”), you might just say, “Make sure to keep the solution concise and do not add extraneous markup or styles.” No need for a full PRD there. Conversely, for complex tasks (like “implement an OAuth flow with token refresh and error handling”), that’s when you break out the detailed spec. A good rule of thumb: adjust spec detail to task complexity. Don’t under-spec a hard problem (the agent will flail or go off-track), but don’t over-spec a trivial one (the agent might get tangled or use up context on unnecessary instructions).\n\nMaintain the AI’s “persona” if needed: Sometimes, part of your spec is defining how the agent should behave or respond, especially if the agent interacts with users. For example, if building a customer support agent, your spec might include guidelines like “Use a friendly and professional tone,” “If you don’t know the answer, ask for clarification or offer to follow up, rather than guessing.” These kind of rules (often included in system prompts) help keep the AI’s outputs aligned with expectations. They are essentially spec items for AI behavior. Keep them consistent and remind the model of them if needed in long sessions (LLMs can “drift” in style over time if not kept on a leash).\n\nYou remain the exec in the loop: The spec empowers the agent, but you remain the ultimate quality filter. If the agent produces something that technically meets the spec but doesn’t feel right, trust your judgement. Either refine the spec or directly adjust the output. The great thing about AI agents is they don’t get offended - if they deliver a design that’s off, you can say, “Actually, that’s not what I intended, let’s clarify the spec and redo it.” The spec is a living artifact in collaboration with the AI, not a one-time contract you can’t change.\n\nSimon Willison humorously likened working with AI agents to “a very weird form of management” and even “getting good results out of a coding agent feels uncomfortably close to managing a human intern”. You need to provide clear instructions (the spec), ensure they have the necessary context (the spec and relevant data), and give actionable feedback. The spec sets the stage, but monitoring and feedback during execution are key. If an AI was a “weird digital intern who will absolutely cheat if you give them a chance”, the spec and constraints you write are how you prevent that cheating and keep them on task.\n\nHere’s the payoff: a good spec doesn’t just tell the AI what to build, it also helps it self-correct and stay within safe boundaries. By baking in verification steps, constraints, and your hard-earned knowledge, you drastically increase the odds that the agent’s output is correct on the first try (or at least much closer to correct). This reduces iterations and those “why on Earth did it do that?” moments.\n\nThink of spec-writing and agent-building as an iterative loop: test early, gather feedback, refine the spec, and leverage tools to automate checks.\n\nThe initial spec is not the end - it’s the beginning of a cycle. The best outcomes come when you continually verify the agent’s work against the spec and adjust accordingly. Also, modern AI devs use various tools to support this process (from CI pipelines to context management utilities).\n\nContinuous testing: Don’t wait until the end to see if the agent met the spec. After each major milestone or even each function, run tests or at least do quick manual checks. If something fails, update the spec or prompt before proceeding. For example, if the spec said “passwords must be hashed with bcrypt” and you see the agent’s code storing plain text - stop and correct it (and remind the spec or prompt about the rule). Automated tests shine here: if you provided tests (or write them as you go), let the agent run them. In many coding agent setups, you can have an agent run npm test or similar after finishing a task. The results (failures) can then feed back into the next prompt, effectively telling the agent “your output didn’t meet spec on X, Y, Z - fix it.” This kind of agentic loop (code -> test -> fix -> repeat) is extremely powerful and is how tools like Claude Code or Copilot Labs are evolving to handle larger tasks. Always define what “done” means (via tests or criteria) and check for it.\n\nIterate on the spec itself: If you discover that the spec was incomplete or unclear (maybe the agent misunderstood something or you realized you missed a requirement), update the spec document. Then explicitly re-sync the agent with the new spec: “I have updated the spec as follows… Given the updated spec, adjust the plan or refactor the code accordingly.” This way the spec remains the single source of truth. It’s similar to how we handle changing requirements in normal dev - but in this case you’re also the product manager for your AI agent. Keep version history if possible (even just via commit messages or notes), so you know what changed and why.\n\nUtilize context-management and memory tools: There’s a growing ecosystem of tools to help manage AI agent context and knowledge. For instance, retrieval-augmented generation (RAG) is a pattern where the agent can pull in relevant chunks of data from a knowledge base (like a vector database) on the fly. If your spec is huge, you could embed sections of it and let the agent retrieve the most relevant parts when needed, instead of always providing the whole thing. There are also frameworks implementing the Model Context Protocol (MCP), which automates feeding the right context to the model based on the current task. One example is Context7 (context7.com), which can auto-fetch relevant context snippets from docs based on what you’re working on. In practice, this might mean the agent notices you’re working on “payment processing” and it pulls the “Payments” section of your spec or documentation into the prompt. Consider leveraging such tools or setting up a rudimentary version (even a simple search in your spec document).\n\nParallelize carefully: Some developers run multiple agent instances in parallel on different tasks (as mentioned earlier with subagents). This can speed up development - e.g., one agent generates code while another simultaneously writes tests, or two features are built concurrently. If you go this route, ensure the tasks are truly independent or clearly separated to avoid conflicts (the spec should note any dependencies). For example, don’t have two agents writing to the same file at once. One workflow is to have an agent generate code and another review it in parallel, or to have separate components built that integrate later. This is advanced usage and can be mentally taxing to manage (as Willison admitted, running multiple agents is surprisingly effective, if mentally exhausting!). Start with at most 2-3 agents to keep things manageable.\n\nVersion control and spec locks: Use Git or your version control of choice to track what the agent does. Good version control habits matter even more with AI assistance. Commit the spec file itself to the repo. This not only preserves history, but the agent can even use git diff or blame to understand changes (LLMs are quite capable of reading diffs). Some advanced agent setups let the agent query the VCS history to see when something was introduced - surprisingly, models can be “fiercely competent at Git”. By keeping your spec in the repo, you allow both you and the AI to track evolution. There are tools (like GitHub Spec Kit mentioned earlier) that integrate spec-driven development into the git workflow - for instance, gating merges on updated specs or generating checklists from spec items. While you don’t need those tools to succeed, the takeaway is to treat the spec like code - maintain it diligently.\n\nCost and speed considerations: Working with large models and long contexts can be slow and expensive. A practical tip is to use model selection and batching smartly. Perhaps use a cheaper/faster model for initial drafts or repetitions, and reserve the most capable (and expensive) model for final outputs or complex reasoning. Some developers use GPT-4 or Claude for planning and critical steps, but offload simpler expansions or refactors to a local model or a smaller API model. If using multiple agents, maybe not all need to be top-tier; a test-running agent or a linter agent could be a smaller model. Also consider throttling context size: don’t feed 20k tokens if 5k will do. As we discussed, more tokens can mean diminishing returns.\n\nMonitor and log everything: In complex agent workflows, logging the agent’s actions and outputs is essential. Check the logs to see if the agent is deviating or encountering errors. Many frameworks provide trace logs or allow printing the agent’s chain-of-thought (especially if you prompt it to think step-by-step). Reviewing these logs can highlight where the spec or instructions might have been misinterpreted. It’s not unlike debugging a program - except the “program” is the conversation/prompt chain. If something weird happens, go back to the spec/instructions to see if there was ambiguity.\n\nLearn and improve: Finally, treat each project as a learning opportunity to refine your spec-writing skill. Maybe you’ll discover that a certain phrasing consistently confuses the AI, or that organizing spec sections in a certain way yields better adherence. Incorporate those lessons into the next spec. The field of AI agents is rapidly evolving, so new best practices (and tools) emerge constantly. Stay updated via blogs (like the ones by Simon Willison, Andrej Karpathy, etc.), and don’t hesitate to experiment.\n\nA spec for an AI agent isn’t “write once, done.” It’s part of a continuous cycle of instructing, verifying, and refining. The payoff for this diligence is substantial: by catching issues early and keeping the agent aligned, you avoid costly rewrites or failures later. As one AI engineer quipped, using these practices can feel like having “an army of interns” working for you, but you have to manage them well. A good spec, continuously maintained, is your management tool.\n\nBefore wrapping up, it’s worth calling out anti-patterns that can derail even well-intentioned spec-driven workflows. The GitHub study of 2,500+ agent files revealed a stark divide: “Most agent files fail because they’re too vague.” Here are the mistakes to avoid:\n\nVague prompts: “Build me something cool” or “Make it work better” gives the agent nothing to anchor on. As Baptiste Studer puts it: “Vague prompts mean wrong results.” Be specific about inputs, outputs, and constraints. “You are a helpful coding assistant” doesn’t work. “You are a test engineer who writes tests for React components, follows these examples, and never modifies source code” does.\n\nOverlong contexts without summarization: Dumping 50 pages of documentation into a prompt and hoping the model figures it out rarely works. Use hierarchical summaries (as discussed in Principle 3) or RAG to surface only what’s relevant. Context length is not a substitute for context quality.\n\nSkipping human review: Willison has a personal rule: “I won’t commit code I couldn’t explain to someone else.” Just because the agent produced something that passes tests doesn’t mean it’s correct, secure, or maintainable. Always review critical code paths. The “house of cards” metaphor applies: AI-generated code can look solid but collapse under edge cases you didn’t test.\n\nConflating vibe coding with production engineering: Rapid prototyping with AI (“vibe coding”) is great for exploration and throwaway projects. But shipping that code to production without rigorous specs, tests, and review is asking for trouble. Osmani distinguishes “vibe coding” from “AI-assisted engineering” - the latter requires the discipline this guide describes. Know which mode you’re in.\n\nIgnoring the “lethal trifecta”: Willison warns of three properties that make AI agents dangerous: speed (they work faster than you can review), non-determinism (same input, different outputs), and cost (encouraging corner-cutting on verification). Your spec and review process must account for all three. Don’t let speed outpace your ability to verify.\n\nMissing the six core areas: If your spec doesn’t cover commands, testing, project structure, code style, git workflow, and boundaries, you’re likely missing something the agent needs. Use the six-area checklist from Section 2 as a sanity check before handing off to the agent.\n\nWriting an effective spec for AI coding agents requires solid software engineering principles combined with adaptation to LLM quirks. Start with clarity of purpose and let the AI help expand the plan. Structure the spec like a serious design document - covering the six core areas and integrating it into your toolchain so it becomes an executable artifact, not just prose. Keep the agent’s focus tight by feeding it one piece of the puzzle at a time (and consider clever tactics like summary TOCs, subagents, or parallel orchestration to handle big specs). Anticipate pitfalls by including three-tier boundaries (Always/Ask first/Never), self-checks, and conformance tests - essentially, teach the AI how to not fail. And treat the whole process as iterative: use tests and feedback to refine both the spec and the code continuously.\n\nFollow these guidelines and your AI agent will be far less likely to “break down” under large contexts or wander off into nonsense.\n\nThis post was formatted using Gemini with images generated using Nano Banana Pro",
    "readingTime": 35,
    "keywords": [
      "extended toc",
      "api keys",
      "github study",
      "vague prompts",
      "document prd",
      "context protocol",
      "naming conventions",
      "helpful constraint",
      "architectural patterns",
      "tech stack"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/good-spec/",
    "thumbnail_url": "https://addyosmani.com/assets/images/good-spec.jpg",
    "created_at": "2026-01-14T12:25:01.028Z",
    "topic": "tech"
  },
  {
    "slug": "musk-v-starmer-will-uk-ban-x-over-grok-nudification-the-latest",
    "title": "Musk v Starmer: will UK ban X over Grok nudification? | The Latest",
    "description": "The UK government is threatening Elon Musk’s X with a ban. The social media platform is under pressure from ministers over the use of the Grok AI tool to manipulate images of women and children to remove their clothes. Ofcom, the UK’s media regulator, has launched an investigation into X – and the government says it will support a ban if Ofcom decides to press ahead.\n Continue reading...",
    "fullText": "8:34Musk v Starmer: will UK ban X over Grok nudification? | The LatestThe UK government is threatening Elon Musk’s X with a ban. The social media platform is under pressure from ministers over the use of the Grok AI tool to manipulate images of women and children to remove their clothes. Ofcom, the UK’s media regulator, has launched an investigation into X – and the government says it will support a ban if Ofcom decides to press ahead.Explore more on these topicsGrok AIOfcomSocial mediaElon Musk",
    "readingTime": 1,
    "keywords": [
      "media",
      "musk",
      "grok",
      "ofcom"
    ],
    "qualityScore": 0.35,
    "link": "https://www.theguardian.com/technology/video/2026/jan/13/musk-v-starmer-will-uk-ban-x-over-grok-nudification-the-latest",
    "thumbnail_url": "https://i.guim.co.uk/img/media/ff1c07fd15bcb9fd17d83a8864b1327b88f10114/262_0_900_720/master/900.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=1213c63122860cd777b1b4e709c5cfe5",
    "created_at": "2026-01-14T12:24:58.365Z",
    "topic": "tech"
  },
  {
    "slug": "one-thing-that-might-get-workers-to-embrace-ai-the-4day-workweek",
    "title": "One thing that might get workers to embrace AI? The 4-day workweek.",
    "description": "Adopting AI has been a struggle at some companies. Embracing a four-day workweek might help get more workers on board, say these authors.",
    "fullText": "Bosses, if you're struggling to get your people excited about AI, here's one idea: Embrace the four-day workweek.\n\nSharing some of AI's promised efficiency gains with employees — by letting them work fewer hours, not just get more done — could help get workers on board with a technology that some fear might ultimately replace them, authors of a new book advocating for a shorter workweek told Business Insider.\n\nLetting workers put in four days' work for five days' pay would be one way to \"share the rewards\" of innovation and technological advancement, said Jared Lindzon, a coauthor of the book \"Do \n\nWhen it comes to AI, giving workers more time away from their jobs could make it more likely they'd get behind the technology \"because they're getting part of that benefit,\" rather than standing in the way of it, he said.\n\nJoe O'Connor, Lindzon's coauthor, said that when it comes to discussions about AI in the workplace, the conversation among workers often turns to fears of job cuts.\n\nAnxiety about AI-induced layoffs might be one reason rolling out the technology has proven difficult for some companies. In an early 2025 survey of business leaders in eight countries from the IT company Kyndryl, 45% of CEOs said their workers were resisting the technology.\n\n\"Cultural resistance and emotional friction\" are the biggest impediments to AI adoption, Boston Consulting Group reported in 2025. That's unwelcome news for C-suite decision-makers eager to ratchet up efficiency. One in three companies is pumping at least $25 million into AI, according to BCG.\n\nBusiness leaders have, at times, publicly expressed their frustration over some workers' foot-dragging.\n\nCoinbase CEO Brian Armstrong said in 2025 that he'd gone \"rogue\" in firing some workers at the crypto exchange who didn't adopt AI after being told to do so. The head of the software company IgniteTech has, meanwhile, lamented that \"changing minds was harder than adding skills.\" In recent years, the firm cut nearly eight in 10 workers after they failed to quickly embrace AI.\n\nNurturing the productivity gains that many leaders seek will often require people to perform different kinds of work — especially as AI takes over some tasks, O'Connor said. He expects that demand for creativity, judgment, critical thinking, and adaptability will increase and that those \"fundamentally human\" traits won't be fostered by simply moving faster or working longer, he said.\n\n\"It's going to be more about maximizing people's energy, maximizing people's motivation, maximizing people's well-being and recovery,\" O'Connor said. A four-day workweek could promote those things, he said.\n\nThe idea that AI could allow people to work less isn't new. For years, the technology's advocates have said it could free up humans to do more of what they love, while handing off the grunt work to bots. The CEO of startup Mechanize, for example, says the company's aim is to automate every job.\n\nThat notion has led some of the biggest corporate luminaries to predict that working hours could plummet as AI adoption increases. Microsoft cofounder Bill Gates has said that time on the clock might shrink to two days, while JPMorgan's Jamie Dimon has said workweeks of 3.5 days could become a thing.\n\nEven Nvidia's Jensen Huang — known for regularly putting in 14-hour days at the chipmaker and working on holidays — has said he could see the tech allowing for more time away from the office.\n\nPoliticians have weighed in, too. Vermont Senator Bernie Sanders, citing efficiency gains from technology such as AI, introduced legislation in 2024 to trim the standard workweek to 32 hours.\n\nThere hasn't yet been widespread adoption of the four-day workweek, likely in part because employers wield more power in many parts of the job market. O'Connor said that while adoption of four-day setups was lower in 2025 than in 2023, when far more workers were job-hopping, more employers are opting for shorter weeks than before the pandemic upended norms about work.\n\nUmesh Ramakrishnan, cofounder of the executive search and leadership advisory firm Kingsley Gate, told Business Insider that many leaders, himself included, would want to harness AI's productivity gains to boost a business's top and bottom lines.\n\n\"If you have a day to spare, get me more revenue, get me more profit,\" he said, adding that while it might sound \"heartless,\" that's simply how business works.\n\nYet, Lindzon said, asking workers to be 20% more effective — the equivalent of a single day in a standard workweek — so that they might benefit from that boost is likely to be more effective than asking them to do it for the good of the company.\n\n\"It completely changes the conversation from a 'You have to do this' to 'We get to do this together,'\" he said.\n\nDo you have a story to share about your career? Contact this reporter at tparadis@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "business leaders",
      "maximizing people's",
      "productivity gains",
      "efficiency gains",
      "standard workweek",
      "four-day workweek",
      "workers",
      "technology",
      "adoption",
      "hours"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/four-day-workweek-might-incentivize-employees-embrace-ai-2026-1",
    "thumbnail_url": "https://i.insider.com/6966b31904eda4732f2f022d?width=1200&format=jpeg",
    "created_at": "2026-01-14T12:24:57.592Z",
    "topic": "finance"
  },
  {
    "slug": "from-humanoid-robots-to-agentic-ai-i-learned-what-retail-insiders-are-buzzing-about-at-a-major-industry-conference",
    "title": "From humanoid robots to agentic AI, I learned what retail insiders are buzzing about at a major industry conference",
    "description": "AI and innovation — from humanoid robots to agents — dominated the conversation at the National Retail Federation's Big Show 2026.",
    "fullText": "There were two little letters on everyone's lips: AI.\n\nEach year, the National Retail Federation, a leading industry trade group, hosts one of the industry's largest conferences, known as Retail's Big Show. I attended the convention, held in New York City from January 11 to 13, for the first time to hear from industry insiders about the retail trends to watch in 2026.\n\nThis year's event drew speakers such as Walmart's incoming CEO John Furner and Google CEO Sundar Pichai, who announced a new AI deal this week, as well as Fanatics CEO Michael Rubin. It was clear that artificial intelligence was the big topic on the minds of the attendees from over 5,000 brands at the event.\n\nWalking the expo hall, \"AI\" and \"agentic\" seemed to be mentioned on nearly every other banner or booth I passed. Onstage, retail leaders touted their AI strategies.\n\nLarge retailers, such as Walmart and Lowe's, have introduced their own AI shopping agents or partnered with AI platforms like Google's Gemini and OpenAI's ChatGPT; however, these efforts are still in their early stages.\n\nI also saw firsthand how some companies are experimenting with new technologies. There were humanoid robots walking up to greet attendees and digital drive-thru menu boards with bright colors advertising to retailers what their AI-powered menus could look like.\n\nDespite some of the noise surrounding retail's AI-powered direction, CEOs, including Fran Horowitz of Abercrombie & Fitch, said young founders should keep the fundamentals top of mind while embracing innovation. Improving customer service, for instance, remains at the forefront of retailer decision-making.\n\nIn one session, Ralph Lauren's chief branding and innovation officer, David Lauren, talked about his brand's longtime partnership with Microsoft. It led to the creation of Ask Ralph, a chatbot-style customer assistant. The bot is powered by Microsoft's AI, but it is designed to function like a styling assistant that suggests clothing based on prompts, such as an occasion you're attending.\n\nIt's a taste of the future with the personal touch customers may want from a store associate.\n\n\"It's like having Ralph Lauren in your pocket,\" Lauren said at the conference.\n\nCurrently, AI is most useful for bargain-hunting and basic customer service, according to a panel about Gen Z that included members of The Z Suite, a collective of consultants specializing in the consumer space. Chatbots like Ask Ralph can be convenient in a pinch, but the shopping experience doesn't begin and end there, the Gen Z consultants who ranged in age from 17 to 24 said.\n\nYoung people seem to still crave some of the old-school fundamentals that define retail: quality, customer service, and in-store experiences. There's no replicating trying on the perfect dress in person, one young panelist said.\n\nThey said they see AI as a starting point that they can use to find the unique items they covet. They're willing to put in the work to find the right item for them. This includes scouring Reddit for \"real\" reviews that TikTok influencers may not offer, said Olivia Meyer, a buyer who is part of The Z Suite.\n\nAnd although they're not rushing to get on the phone with a customer service representative, the consultants said they want to talk to a human when their money is at stake.\n\nThese Gen Z retail professionals, who also included a global merchant for Calvin Klein and a marketing associate, said they want their clothes to come from authentic brands that are transparent about their use of AI. They pointed to online speculation about AI models in ads, saying Gen Z doesn't care if the brands use AI models; they just want the truth.\n\nThey want to get their money's worth for the clothes that they're buying, which is why data shows this demographic is straying away from fast fashion and toward shopping secondhand on sites like Vinted and eBay.\n\nTrue Religion's chief marketing officer, Kristen D'Arcy, who moderated the panel, said shoppers are craving authenticity in the world of AI-generated content, and it's making them more discerning. To that end, the denim brand has leveraged partnerships that align with its values, such as collaborations with artists like Megan Thee Stallion.\n\nThe three-day event gave me a glimpse into the retail landscape of 2026.\n\nAI may be the loudest trend, but that doesn't mean brands should forget the basics.",
    "readingTime": 4,
    "keywords": [
      "customer service",
      "ask ralph",
      "brands",
      "event",
      "shopping",
      "it's",
      "consultants",
      "doesn't",
      "they're",
      "industry"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/what-retail-leaders-said-ai-more-at-nrf-big-show-2026-1",
    "thumbnail_url": "https://i.insider.com/69665e6104eda4732f2ef646?width=1200&format=jpeg",
    "created_at": "2026-01-14T12:24:57.219Z",
    "topic": "finance"
  },
  {
    "slug": "the-3-main-storylines-investors-need-to-be-watching-as-earnings-season-picks-up-steam",
    "title": "The 3 main storylines investors need to be watching as earnings season picks up steam",
    "description": "The continued adoption of AI, and how much companies are spending to do so, will be top of mind.",
    "fullText": "Earnings season kicked off with a bang on Tuesday with a report from headliner JPMorgan.\n\nThe company's stock ultimately fell 4% due to a mix of two factors unique to the business: (1) a surprise drop in investment banking revenue and (2) uncertainty around President Trump's proposed credit card interest cap, to which JPMorgan is especially exposed as America's largest issuer.\n\nIt was a good primer for the upcoming earnings season, which could make or break the fortunes of major US companies, especially when it comes to AI plans.\n\nThe previous quarter already saw a simmering of the red-hot AI trade for companies planning to continue spending heavily on capex without immediate tangible results. Just ask Meta and Microsoft how it went when they pledged to keep pouring billions into AI (spoiler alert: not well).\n\nWith that in mind, First Trade — with the help of the equity strategy team at Goldman Sachs — has put together a three-part earnings primer for those looking to gain an edge up.\n\n1. Watch for signs of AI-adoption progress\n\nSo far, in the AI trade, it's been the infrastructure stocks that have dominated — the chipmakers and picks-and-shovels companies that make up the physical components of data centers.\n\nGoldman says that gains will broaden out to increasingly include companies showing productivity gains unlocked by AI.\n\nA prime example of this dynamic in action came last quarter, from freight-logistics company CH Robinson, which raised its profit-growth forecast specifically due to new AI efficiencies. The stock spiked 20% during the next trading day, and it's been at record highs pretty much ever since.\n\nThe market is being clear about what it wants. Actually achieving that is the hard part.\n\n2. Monitor how companies are planning to spend their cash\n\nConcerns over AI overspending reached a fever pitch last quarter — a trend Goldman expects to subside throughout 2026 after one last hurrah. The firm predicts that hyperscaler capex will see above-forecast growth for one more quarter, then begin a gradual deceleration, as shown in this chart:\n\nAn additional element to watch is the degree to which that AI capex spending comes at the expense of share repurchases. After all, buybacks are crucial for engineering share-price increases during periods devoid of other positive catalysts.\n\nGoldman predicts that investors will reward companies generating strong free cash flow, which have the flexibility to return cash to shareholders. The firm says that group of stocks already outperformed throughout 2025.\n\n3. Check the sustainability of earnings growth for mega-cap tech\n\nThe number to watch here is 20%. That's the target Goldman has set for Magnificent 7 profit expansion in the quarter. Any downside disappointment here could mean stock declines. After all, earnings growth is the lifeblood of any bull market.",
    "readingTime": 3,
    "keywords": [
      "earnings season",
      "earnings growth",
      "quarter",
      "stock",
      "capex",
      "watch",
      "cash",
      "primer",
      "planning",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/q4-earnings-season-preview-outlook-ai-stock-market-capex-spending-2026-1",
    "thumbnail_url": "https://i.insider.com/6966c01004eda4732f2f0462?width=1024&format=jpeg",
    "created_at": "2026-01-14T12:24:57.206Z",
    "topic": "finance"
  },
  {
    "slug": "ben-horowitz-says-ai-will-be-bigger-than-the-internet-and-bubble-fears-miss-whats-really-happening",
    "title": "Ben Horowitz says AI will be bigger than the internet — and bubble fears miss what's really happening",
    "description": "Andreessen Horowitz cofounder Ben Horowitz said AI's surge isn't just hype, saying that record demand and adoption explain soaring valuations.",
    "fullText": "Silicon Valley leaders have debated for months whether the AI boom is a bubble. Ben Horowitz says that debate misses what's really happening.\n\nIn a wide-ranging discussion on \"The A16z Show\" on Tuesday, the cofounder of the VC firm Andreessen Horowitz said AI represents something larger than past tech waves — including the internet — and that the eye-popping valuations can't be understood without looking at what's actually happening underneath the surface.\n\n\"AI is a new computing platform,\" Horowitz said. In his view, that puts it in a different category from incremental software shifts.\n\n\"This is a bigger technology market than I've ever seen,\" he added.\n\nSkeptics, including OpenAI CEO Sam Altman, Microsoft cofounder Bill Gates, and hedge fund billionaire Ray Dalio, often point to how fast valuations have risen to predict a looming bubble.\n\nBut Horowitz said that focusing on prices alone ignores a more important signal: demand.\n\n\"One of the reasons why people are so worried about it being a bubble is, you know, the valuations have gone up so fast,\" he said. \"But if you look at what's going on underneath in terms of the customer adoption, the revenue growth rates — we've never seen demand like this.\"\n\nThat disconnect, he said, explains why AI feels unsettling even to seasoned investors.\n\n\"We've never seen valuations rise like this, but we've never seen demand rise like this either,\" Horowitz said, describing the current moment as \"a bit of a brave new world.\"\n\nHorowitz also pushed back on the idea that AI will produce only a handful of dominant winners, as the internet did — a view promoted by former Shark Tank star Mark Cuban.\n\nHorowitz sees a much larger opportunity set.\n\n\"It's a very big design space — an enormous design space like one we've never seen before in technology,\" he said, predicting more billion- and even $10 billion-plus companies than in prior cycles.\n\nPart of that expansion comes from how AI is being built. Rather than a single, all-powerful model doing everything, Horowitz said real products require complex applications that deeply model human behavior — work that can't simply be absorbed by foundation models.\n\nThe result, he believes, is a technology shift that is both messier and more powerful than past revolutions — and one where fears of a simple bubble may underestimate just how much is changing.",
    "readingTime": 2,
    "keywords": [
      "design space",
      "bubble",
      "valuations",
      "we've",
      "what's",
      "technology",
      "demand",
      "horowitz",
      "cofounder",
      "larger"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ben-horowitz-says-ai-is-bigger-than-internet-not-bubble-2026-1",
    "thumbnail_url": "https://i.insider.com/69676c2f64858d02d2184fed?width=1200&format=jpeg",
    "created_at": "2026-01-14T12:24:57.042Z",
    "topic": "finance"
  },
  {
    "slug": "zhipu-and-huawei-opensource-glmimage-on-chinese-chips",
    "title": "Zhipu and Huawei open-source GLM-Image on Chinese chips",
    "description": "Generate high-quality AI images instantly with GLM-Image.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://glm-image-ai.app",
    "thumbnail_url": "https://picsum.photos/seed/glmimage/1200/630",
    "created_at": "2026-01-14T06:23:48.985Z",
    "topic": "tech"
  }
]
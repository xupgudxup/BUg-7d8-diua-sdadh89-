[
  {
    "slug": "mnn-fast-lightweight-deep-learning-framework",
    "title": "MNN – fast, lightweight deep learning framework",
    "description": "MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Full multimodal LLM Android App:[MNN-LLM-Android](./apps/Android/MnnLlmChat/READ...",
    "fullText": "alibaba\n\n /\n\n MNN\n\n Public\n\n MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Full multimodal LLM Android App:[MNN-LLM-Android](./apps/Android/MnnLlmChat/README.md). MNN TaoAvatar Android - Local 3D Avatar Intelligence: apps/Android/Mnn3dAvatar/README.md\n\n www.mnn.zone/\n\n License\n\n Apache-2.0 license\n\n 14k\n stars\n\n 2.2k\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n alibaba/MNN",
    "readingTime": 1,
    "keywords": [
      "android",
      "alibaba",
      "license"
    ],
    "qualityScore": 0.65,
    "link": "https://github.com/alibaba/MNN",
    "thumbnail_url": "https://opengraph.githubassets.com/6afcc07063d3bfbf527b1024d6427e1cc3ba0bbedcfd91446f7361a79a748b1f/alibaba/MNN",
    "created_at": "2026-01-22T00:59:15.698Z",
    "topic": "tech"
  },
  {
    "slug": "ai-systems-performance-engineering",
    "title": "AI Systems Performance Engineering",
    "description": "Contribute to cfregly/ai-performance-engineering development by creating an account on GitHub.",
    "fullText": "cfregly\n\n /\n\n ai-performance-engineering\n\n Public\n\n License\n\n Apache-2.0 license\n\n 968\n stars\n\n 134\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n cfregly/ai-performance-engineering",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/cfregly/ai-performance-engineering",
    "thumbnail_url": "https://opengraph.githubassets.com/821a0056b72b23f9cd26ddd6943614ce37b50c900dd7ac56b45a8b17aca24f30/cfregly/ai-performance-engineering",
    "created_at": "2026-01-22T00:59:15.645Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-ceo-jensen-huang-says-a-lot-of-sixfigure-jobs-in-plumbing-and-construction-are-about-to-be-unlocked-because",
    "title": "Nvidia CEO Jensen Huang says ‘a lot’ of six-figure jobs in plumbing and construction are about to be unlocked because someone needs to build all these new AI centers",
    "description": "The AI boom is threatening white-collar entry jobs—but it’s creating a booming six-figure opportunity for electricians, plumbers, and construction workers.",
    "fullText": "The job market has been a tough sell for many Gen Z graduates, with tariffs, economic uncertainty, and artificial intelligence reshaping hiring plans across corporate America. But according to Nvidia’s CEO Jensen Huang, the next wave of six-figure opportunities won’t be found in a Wall Street cubicle or Silicon Valley Slack channel. Instead, high-paying careers will partially come by picking up a wrench—literally.\n\nAs tech giants race to build sprawling data centers—totaling $7 trillion in global capital outlays by the end of the decade—Huang believes the world is on the cusp of what he calls the “largest infrastructure buildout in human history,” which will create “a lot of jobs.”\n\n“It’s wonderful that the jobs are related to tradecraft, and we’re going to have plumbers and electricians and construction and steelworkers,” he said in conversation with BlackRock CEO Larry Fink at the World Economic Forum in Davos, Switzerland.\n\nThe hands-on skills needed to construct what he described as chip, computer, and AI factories are already facing shortages—even though many roles pay over $100,000 without requiring a college degree. One McKinsey report estimated that between 2023 and 2030, there will be a need for an additional 130,000 trained electricians—as well as 240,000 construction laborers and 150,000 construction supervisors—in the U.S. alone.\n\n“Everybody should be able to make a great living,” Huang said. “You don’t need to have a Ph.D. in computer science to do so.”\n\nThis isn’t the first time Huang has expressed his optimistic outlook on new career paths emerging alongside AI.\n\n“The skilled craft segment of every economy is going to see a boom,” he told Channel 4 News in the U.K. last year. “You’re going to have to be doubling and doubling and doubling every single year.”\n\nThat optimism stands in contrast to warnings from leaders like Ford CEO Jim Farley, who has repeatedly cautioned that AI is hollowing out traditional white-collar entry points—just as the education system continues to funnel students toward four-year degrees.\n\n“There’s more than one way to the American Dream, but our whole education system is focused on four-year [college] education,” Farley said at the Aspen Ideas Festival last year. “Hiring an entry worker at a tech company has fallen 50% since 2019. Is that really where we want all of our kids to go? Artificial intelligence is gonna replace literally half of all white-collar workers in the U.S.”",
    "readingTime": 2,
    "keywords": [
      "artificial intelligence",
      "education system",
      "construction",
      "doubling",
      "hiring",
      "tech",
      "jobs",
      "computer",
      "college",
      "white-collar"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/nvidia-ceo-jensen-huang-says-160735219.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/AVfx3XSS96moXvxXLt9zTw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/4062994d1bf7ae6ff8233554daa0c275",
    "created_at": "2026-01-22T00:59:12.475Z",
    "topic": "finance"
  },
  {
    "slug": "ai-adoption-is-accelerating-but-confidence-is-collapsing-the-more-workers-use-ai-the-less-they-trust-it-baby-boomers",
    "title": "‘AI adoption is accelerating, but confidence is collapsing’: The more workers use AI, the less they trust it. Baby boomers show a 35% drop",
    "description": "A study of 14,000 workers across 19 countries shows a toxic relationship, as many companies rush to adopt the technology without proper training.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/01/21/ai-workers-toxic-relationship-trust-confidence-collapses-training-manpower-group/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/GettyImages-157752806.jpg?resize=1200,600",
    "created_at": "2026-01-22T00:59:12.462Z",
    "topic": "business"
  },
  {
    "slug": "google-just-promised-no-ads-in-gemini-for-now",
    "title": "Google Just Promised No Ads in Gemini (for Now)",
    "description": "The statement comes about a week after OpenAI announced ads are coming to ChatGPT.",
    "fullText": "A week after OpenAI admitted it will soon start testing ads in ChatGPT, Google has promised that it's not planning to inject ads into Gemini anytime soon.\n\nThe statement was given to journalist Alex Heath during the World Economic Forum in Davos, Switzerland. Google DeepMind CEO Demis Hassabis said the company doesn't have \"any plans\" for ads in Gemini. While the statement was fairly brief, it also jibes with a similar quote Hassabis gave to Axios, where he said he was \"a little bit surprised\" that OpenAI was already introducing ads to ChatGPT.\n\nThat surprise is understandable, especially because OpenAI CEO Sam Altman said in 2024 that he considered ads a \"last resort for us as a business model.\" But looking at the numbers, it makes sense that ChatGPT is getting ads long before Gemini is even thinking of them.\n\nWhile Google makes most of its money through showing people ads, it's also able to rely on Search and YouTube to push ads to most of those eyeballs. Meanwhile, OpenAI is pretty much just ChatGPT. As the latter moves to a for-profit model, it now has to put moneymaking first, something it's had trouble doing without relying on traditional internet moneymakers like ads. Google, meanwhile, is already profitable elsewhere, and is able to take its time and use its sheer size to keep Gemini ad-free, at least while it continues to chase market share.\n\nDoes this mean Google's AI will never get ads? Well, never say never. But it does mean that they're probably not on the horizon—even if Google plans to more aggressively monetize Gemini over the long term, it isn't facing the same kind of time crunch as Altman's company.\n\nIt remains to be seen whether the presence of ads will push users away from ChatGPT, but the move comes in the wake of significant wins for Gemini and one major loss for ChatGPT. First, Google's Nano Banana image editing model went viral on social media, winning over the general public. Then, Google struck a deal with Apple to put its AI into the iPhone, and it looks like Gemini will be powering Siri for the foreseeable future.\n\nMeanwhile, ChatGPT reportedly saw a 6% dip in users early last month, following a model update from Gemini—and that was before the introdution of ads. While ChatGPT still seems to be in the lead on total user count, there's evidence that Google is catching up.\n\nThe divide in strategy seems clear: As OpenAI seeks ways to get more money out of its existing user base, Google can focus on growing its own with new integrations into the products we already use every day. I can't say what the limits of this growth are, but I can say that I rarely go out of my way use AI, yet I've still found myself accidentally relying on Google's AI overviews every now and then. If Google can get more people like me to casually integrate AI into our regular workflows, it's possible we could soon have a new AI leader on our hands.",
    "readingTime": 3,
    "keywords": [
      "google's ai",
      "it's",
      "model",
      "soon",
      "google",
      "chatgpt",
      "gemini",
      "statement",
      "hassabis",
      "plans"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-just-promised-no-ads-in-gemini-for-now?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFGZ1E3E2BQZ7DBB464WC469/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.618Z",
    "topic": "tech"
  },
  {
    "slug": "apple-might-turn-siri-into-an-ai-chatbot-to-rival-chatgpt",
    "title": "Apple Might Turn Siri Into an AI Chatbot to Rival ChatGPT",
    "description": "It could be ChatGPT's next big competitor.",
    "fullText": "Last week, Apple finally admitted it will need to team up with Google to finally make good on that contextual Siri promise it made two years ago, which would have allowed the virtual assistant to integrate with content like your texts or emails to answer personal questions and take actions for you. Now, according to a new report, the iPhone company might actually go one step further and turn Siri into a full-fledged AI chatbot—one on par with the likes of ChatGPT, and perhaps even more sophisticated.\n\nCurrently, Siri has AI implementation, but only technically, and it's certainly underwhelming: You can use it to get tech support on Apple products or shunt questions off to ChatGPT, but otherwise, Siri basically works as it always has. But according to Bloomberg's Mark Gurman, who has reliably reported on insider information at Apple before, the company is finally not only looking to make Siri smarter, but also change the way you interact with it. Currently planned for iOS and macOS 27 under the name \"Campos,\" Siri's new chatbot interface will still be powered by Gemini, but will allow you to both type and talk to Siri, with full continuity between your conversations. This upgrade will be in addition to the overdue features that were already announced.\n\nIn other words, it'll look something like the chatbot interface from the ChatGPT app or the standalone Gemini app. Yes, you can technically type to Siri right now, but it mostly works like a separate input method, rather than as a full conversation. You can't scroll through your previous questions to Siri or peruse the assistant's previous answers, and if you ask Siri to reference a message you sent it two weeks ago, it'll have no idea what you mean. That's far behind what other AI chatbots offer right now.\n\nThe update will also apparently further expand Siri's capabilities even beyond the contextual or personalization upgrades that were already revealed. Gurman says that, while the contextual upgrades will be able to pull information from other apps like Messages, the chatbot-style Siri will be \"integrated into all of the company's core apps, including ones for mail, music, podcasts, TV, Xcode programming software and photos.\" Essentially, Siri will have more access to your iPhone than other AI chatbots, and those integrations will go beyond what was previously promised. That could make it more or less appealing to you, depending on your tastes in AI integration.\n\nWith the chatbot interface planned for iOS 27, it's likely to come after the contextual upgrades, rather than at the same time. That's because, as Gurman said previously, those upgrades are set for the spring. He predicts we'll learn more about it during this year's WWDC, which, if it follows the standard set by previous years, will take place in June.\n\nThe move to turn Siri into a chatbot could come across as a a much-overdue modernization, as Google has already done the same with Gemini over on Android, but it's also a bit of a surprise, as Apple had previously said it did not intend to turn Siri into a \"bolt-on chatbot on the side\" for Apple Intelligence.\n\nBut Apple was likely talking about quality of the experience rather than expressing any significant anti-chatbot bias among the development team, meaning the fact that Siri is turning into a chatbot could mean the company is finally happy with the direction it's headed. But it's also possible that the professed skepticism about turning Siri into a chatbot was meant to appeal to AI skeptics in general. Unfortunately, if you're still skeptical about AI, it currently seems like iOS 27 will be a boring update for you, as Gurman indicated the new Siri chatbot will be the \"primary new addition\" to the operating system.\n\nHowever you feel about it personally, Siri as a full-fledged AI chatbot could seriously upset ChatGPT's market dominance—ironic, given its early integration with Apple Intelligence. Currently, OpenAI has reportedly admitted it's in a Code Red situation, as it is losing market share to Google and introducing ads to bolster its bottom line. The new Siri, being powered by Gemini, is unlikely to hurt Google (although it will have more access to your phone than the standalone Gemini app), but its ease-of-access might make it the new go-to for iPhone users, and that could hurt pretty much every AI company Apple isn't in business with directly.",
    "readingTime": 4,
    "keywords": [
      "standalone gemini",
      "gemini app",
      "apple intelligence",
      "contextual upgrades",
      "chatbot interface",
      "it's",
      "siri",
      "finally",
      "google",
      "iphone"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-siri-chatbot-ai-plans?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFH7N4CFDDEPBSQ5VKSC72K1/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-22T00:59:09.513Z",
    "topic": "tech"
  },
  {
    "slug": "nvidias-jensen-huang-says-its-a-good-time-to-be-a-plumber-and-not-just-because-its-an-aiproof-job",
    "title": "Nvidia's Jensen Huang says it's a good time to be a plumber — and not just because it's an AI-proof job",
    "description": "The massive AI buildout is demanding more manual labor jobs such as plumbers and electricians. Jensen Huang says salaries are surging as a result.",
    "fullText": "Is AI coming for your job? If you work in construction or plumbing, that's perhaps not a question you need to worry about.\n\nSpeaking at the World Economic Forum on Wednesday, Nvidia CEO Jensen Huang said it was a great time to be a tradesperson because the AI boom is creating demand for manual labor to build data centers.\n\n\"It's wonderful that the jobs are related to tradecraft and we're going to have plumbers and electricians and construction and steelworkers,\" he said in a conversation with BlackRock CEO Larry Fink in Davos, Switzerland.\n\nHuang described the AI boom as the \"largest infrastructure buildout in human history\" that would create \"a lot of jobs.\"\n\nAI's impact on the labor market has been a hot topic at Davos this year. Huang has long argued that AI won't be the mass job killer some believe it will be. He has given radiology as an example: while AI has automated some of the tasks radiologists do, the number of jobs in the field has actually increased.\n\nAI \"Godfather\" Geoffrey Hinton previously said he also believes manual labor is safer from AI, though for a different reason: it will take longer for AI to have the dexterity to take on more physical jobs.\n\nAt Davos on Wednesday, Huang said the US was seeing a \"significant boom\" in demand for manual jobs helping build AI infrastructure, with salaries nearly doubling in some cases.\n\n\"So we're talking about six-figure salaries for people who are building chip factories or computer factories or AI factories, and we have a great shortage in that,\" said Huang.\n\nHe added: \"Everybody should be able to make a great living. You don't need to have a Ph.D. in computer science to do so.\"",
    "readingTime": 2,
    "keywords": [
      "manual labor",
      "jobs",
      "boom",
      "factories",
      "construction",
      "demand",
      "we're",
      "infrastructure",
      "salaries",
      "computer"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/nvidia-jensen-huang-plumber-construction-jobs-ai-data-center-2026-1",
    "thumbnail_url": "https://i.insider.com/6970bbc7e1ba468a96aa677d?width=1200&format=jpeg",
    "created_at": "2026-01-21T18:30:50.598Z",
    "topic": "finance"
  },
  {
    "slug": "a-veteran-investor-tears-into-openai-chaos-and-says-the-markets-smart-money-is-pouring-into-smallcap-stocks",
    "title": "A veteran investor tears into OpenAI 'chaos,' and says the market's smart money is pouring into small-cap stocks",
    "description": "Hedge fund founder George Noble says OpenAI is a warning for the wider AI boom. He says the \"low hanging fruit is gone\" as AI costs increase.",
    "fullText": "OpenAI has laid out some ambitious plans for 2026, but according to George Noble, the ChatGPT maker is a cautionary tale for the wider AI theme.\n\nThe hedge fund investor and former director of the Fidelity Overseas Fund doesn't think the AI startup will be able to justify its lofty growth projections, but more than that, he thinks investors should avoid the AI trade and shift their focus to other areas of the market before the tide turns.\n\nThe hedge fund founder laid out his thesis on OpenAI's red flags in a detailed X post on Tuesday, highlighting the \"code red\" moment announced by CEO Sam Altman in December 2025.\n\nIn his view, it all comes down to financials, specifically a math problem that isn't being widely discussed. \n\n\"It's going to cost 5x the energy and money to make these models 2x better,\" he said. \"The low-hanging fruit is gone. Every incremental improvement now requires exponentially more compute, more data centers, more power.\"\n\nNoble thinks OpenAI's annual revenue has to reach $200 billion by 2030 in order for the company's projections to be justified.\n\nPointing to the broader market, he advised AI startup founders to exit their companies while the boom is still going, adding that he believes the AI hype cycle is nearing its peak. He thinks investors should shift their focus to other parts of the market, as both the AI trade and the Magnificent Seven will be impacted if OpenAI begins to struggle.\n\n\"If you're exposed to the Magnificent 7 through AI infrastructure bets, consider trimming,\" he advised. \"The smart money is rotating into sectors where valuations actually reflect fundamentals.\"\n\nThe particular sector in this case is small-mid cap stocks, which is where Noble sees real value and growth potential. He added that there is a value disconnect between Big Tech stocks and their small-cap peers, which are trading at near-decade lows.\n\nFor that reason, Noble sees an opportunity for investors to move away from the most crowded stocks that carried the AI trade in 2025 and focus on the lesser-known value plays that have been quietly making progress while AI-linked names continue to dominate the headlines.\n\n\"Markets can price risk,\" he noted. \"But they can't price chaos.\nAnd OpenAI is chaos dressed up in a $500 billion valuation.\"",
    "readingTime": 2,
    "keywords": [
      "hedge fund",
      "investors",
      "trade",
      "focus",
      "market",
      "stocks",
      "laid",
      "startup",
      "growth",
      "projections"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-chatgpt-george-noble-magnificent-7-warning-ai-bubble-tech-2026-1",
    "thumbnail_url": "https://i.insider.com/6970ecc0e1ba468a96aa6c07?width=1200&format=jpeg",
    "created_at": "2026-01-21T18:30:50.119Z",
    "topic": "finance"
  },
  {
    "slug": "solopreneurs-are-embracing-ai-heres-how-3-career-coaches-have-found-it-indispensable",
    "title": "Solopreneurs are embracing AI — here's how 3 career coaches have found it indispensable",
    "description": "AI tools like ChatGPT, Gemini, and Claude help solopreneur coaches Kim Surko, Katharine Campbell Hirst, and Liz Morrison boost client outcomes and streamline coaching workflows.",
    "fullText": "When it comes to career and executive coaching, some of the most important work happens in one-on-one sessions with the client. That's when breakthrough insights often emerge, motivation gains momentum, and coaches build an essential connection.\n\nThen comes the hard part: turning those words into actions.\n\nTranscribing conversations into meaningful action items for clients takes time and effort — something a team of assistants could easily handle. However, when you're the only person running your business, that time and effort leaves coaching solopreneurs with a difficult tradeoff: take notes during sessions and sacrifice presence, spend hours after each call transcribing insights from recordings, or leave follow-through entirely to clients.\n\n\"Trying to juggle it all on my own wasn't an option — it was just impossible to build a sustainable business,\" said Kim Surko, founder of leadership coaching business Surko Coaching, regarding how easy it was to feel overwhelmed by work that felt more administrative than transformational. \"Leaning into AI was the most natural solution to help with all of that responsibility.\"\n\nHere's how three solopreneurs in leadership, business, and communications have used AI to help clients achieve more results in less time, expanding their capacity as coaches while increasing the value of the work they offer.\n\nFor all three coaches, the biggest game changer has been using AI note-takers to distill long conversations into something more tangible.\n\nAfter getting client consent, \"I record my coaching sessions and upload transcripts into ChatGPT. This allows me to rapidly transform nuanced insights from our conversations into concrete outputs clients can actually use — pitches, résumés, website copy, positioning statements, and more,\" said Katharine Campbell Hirst, founder of business coaching company KCH Coaching & Advisory. \"What used to take weeks of agonizing refinement now takes minutes.\"\n\nLiz Morrison, founder of communications coaching company LM Strategic Storytelling, appreciates how AI ensures that valuable sound bites from her coaching sessions don't get lost in hours of recordings that nobody has time to revisit. She's built custom projects in Claude to help her transform session transcripts into \"Story Banks\" in minutes — pulling out three to six narratives per session that clients can use immediately for interviews, networking, social media, and building their businesses.\n\nWhile this type of work was essential before AI, doing it as a solopreneur meant sacrificing time that could be spent supporting other clients. \"I've saved almost an hour per client per day by relying on AI to take notes and summarize them for me,\" said Surko, who added that she nearly doubled her capacity for coaching clients with AI's support.\n\nSurko has also used AI to help her clients appreciate the progress they're making, improving the feeling of momentum. \"A lot of work with coaching is celebrating the small wins,\" she said.\n\nUsing the project management tool Kanbanchi, supported by Gemini, Surko can quickly update to-do list boards that lay out all of the client's goals and achievements.\n\n\"Having that visual representation of the progress we're making shows the value of coaching,\" Surko said. The process has been extremely valuable, as it has improved her client renewal rate because clients can see exactly how they're getting closer to a goal, rather than feeling like they aren't making progress, she added.\n\nMorrison tells a similar story. She built a custom ChatGPT tool called Story Explorer that walks prospective clients through a story-coaching exercise to uncover one immediately usable story they can post on LinkedIn or use in a networking conversation.\n\n\"I find when I give people this builder, it's the start of a much bigger conversation,\" she said. They often uncover other narratives they want to explore further with Morrison, she said.\n\nAlongside the benefit of being more present during conversations, these coaches have found AI valuable for improving their in-session coaching in other ways.\n\nSurko, for example, used Gemini within Google Docs to create a searchable archive of the massive toolkit of exercises and prompts she's collected in her decadeslong career, which before were buried in various folders.\n\nPreviously, she would have to wait until after the session to hunt down an exercise. Now, she can quickly pull them up during sessions and dive deeper with a client. \"We make more progress in each session,\" she said of this improvement. \"We're able to continue that momentum.\"\n\nAI can even be a helpful coach for these coaching experts.\n\nHirst uploaded transcripts across her client's full arc and asked where she did well and where she could have improved. While she also works with coaches, she appreciates that AI can effectively be over her shoulder all the time.\n\n\"The feedback is surprisingly concrete, pattern-based, and immediately actionable — effectively giving me a reflective practice partner I wouldn't otherwise have access to as a solopreneur,\" Hirst said.",
    "readingTime": 4,
    "keywords": [
      "coaching sessions",
      "clients",
      "client",
      "coaches",
      "business",
      "conversations",
      "progress",
      "insights",
      "momentum",
      "founder"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/3-coaching-solopreneurs-how-ai-help-their-businesses-grow-2026-1",
    "thumbnail_url": "https://i.insider.com/696a8b9ee1ba468a96aa3c19?width=1200&format=jpeg",
    "created_at": "2026-01-21T18:30:49.965Z",
    "topic": "finance"
  },
  {
    "slug": "manage-your-ai-investments-like-a-portfolio",
    "title": "Manage Your AI Investments Like a Portfolio",
    "description": "Companies should apply a step-by-step portfolio management approach when it comes to AI. They should view the connected portfolio through a dual lens: first, as an advancement pipeline with clear gates through which projects must pass; and second, as a whole-portfolio dashboard that shows balances across risk/return, time horizon, capability areas, and mission alignment. This dual perspective enables both rigorous project-level discipline and strategic portfolio-level optimization.",
    "fullText": "Manage Your AI Investments Like a Portfolio by Faisal Hoque, Erik Nelson, Tom Davenport and Paul ScadeJanuary 21, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintBusiness leaders now face intense pressure to transform their organizations with AI, even though the technology, public attitudes, and the competitive landscape are all still in flux. The result is often too many pilots with too little coordinated oversight. Without a way to systematically decide where to start, how fast to move, and when to stop, AI efforts quickly become a drain on attention and resources rather than a source of advantage. A familiar pattern recurs across many companies: isolated, piecemeal deployments, limited buy-in by senior executives, and weak linkage to strategic goals.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2026/01/manage-your-ai-investments-like-a-portfolio",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_21_2178861789.jpg",
    "created_at": "2026-01-21T18:30:49.138Z",
    "topic": "business"
  },
  {
    "slug": "digg-is-back",
    "title": "Digg Is Back",
    "description": "Digg's new beta is aiming to take on Reddit, with a new design and an AI-narrated podcast.",
    "fullText": "Back before Reddit became the unofficial \"front page of the internet,\" you would dig up your next long read or binge watch on Digg. Starting in 2004, the original version of the site worked much like Reddit does today, with community members submitting content they found interesting to premade category pages and others voting on it until an algorithm eventually decided what should make its way to the front page. Aside from the lack of user-made pages like subreddits, it was generally pretty familiar to what modern users might expect—and, speaking from experience, it was a big deal to be featured on Digg.\n\nUnfortunately, starting in 2010, the site went through a few drastic redesigns that added controversial features like the DiggBar (a clunky toolbar that would display over content) and got rid of features like burying (the equivalent of modern downvoting). It bounced from owner to owner and experimented with new formats like a manually curated front page, but by that point, Reddit had become the behemoth it's known as today. It was hard for Digg to keep up.\n\nNow, after Reddit has spent years saddled with its own controversies, Digg is back with yet another relaunch, with a new beta from original founder Kevin Rose and Reddit co-founder Alexis Ohanian that aim to combine the best of both site's legacies.\n\nLast week, Rose and Ohanian opened their new Digg to the public, debuting a new design that looks a lot like Reddit, but cleaner. On desktop, the sidebar on the left uses icons rather than labels, and generally has fewer complications, so no distracting \"games on Reddit\" tab. To the right of that, you get your main, infinite scrolling feed, and I'll admit, I like the classic blue-on-white color scheme (although you can use dark mode if you like). Unlike the classic Digg, this feed will include user-made communities, which work like subreddits, so you can join and leave them at any time to curate what you see. And yes, the downvote is back, along with full commenting functionality. You can also swap over from a feed that only shows communities you're subscribed to (My Feed) to one that collects the best posts across Digg (All Digg) with a button up top, which is one pretty significant difference—Reddit has the r/all subreddit, but it requires navigating away from your main feed and isn't available in the app.\n\nBut the big difference maker is in the right sidebar, which shows recent posts on Reddit, but \"Digg Daily\" on Digg. This shows trending posts and featured communities at a glance, so you can get caught up with news without having to scroll the \"All Digg\" feed for too long, but curiously, it's also got the \"Digg Daily\" podcast. This one addition is probably the most significant way the new Digg differs from Reddit, and also the most awkward.\n\nIt had to be here somewhere—Digg Daily is the site's implementation of AI. Updated once a day, this brief five-ish minute podcast recaps the biggest stories on the site that day, using AI hosts that sound like slightly more robotic versions of the ones you'll get on Google's NotebookLM. You'll get a few sentences talking about the story's original source (which, when I listened, did credit the author of the article being discussed), as well as a few quotes from readers. Unfortunately, while you can bring up chapters to jump ahead in Digg Daily and see a list of discussed topics, there aren't any links to find either the sources or Digg posts being discussed, and the \"Featured Posts\" bar below Digg Daily doesn't relate to what's on the podcast at all.\n\nIt's a nice idea, but aside from getting a high-level overview of what was popular on the site that day, I didn't find it too useful. Summaries are extremely short, and comments are awkward to hear outside of their original context. It might be a good first step to know what to search the site for, but links would really help it out.\n\nOn the plus side, Digg Daily might not always be AI: The company said during an interview with TechCrunch that it might swap out the robotic hosts for human ones following user feedback. Human lead curation could help the recaps feel a bit more natural, and even bring back some elements from the eras of Digg where the front page was managed by a staff rather than an algorithm.\n\nAside from the different look and minor additions like Digg Daily, getting started on Digg should be pretty familiar for anyone who's used Reddit. The mobile app also has full functionality, although sidebar features have been moved to buttons above and below the main feeds. But there are a few ways the platform is looking to grow.\n\nThe big one is probably communities, or Digg's version of subreddits. The site launched with 21 default communities off the bat, but it'll take a while for user-made communities to pop up for more obscure topics. For instance, I've been replaying the Mega Man: Battle Network games from my youth a lot lately, and while there are multiple regularly updated subreddits for that series with thousands of members each, there's not a Digg community for them yet. It sounds like a small complaint, but one of Reddit's big strengths is that you can just Google \"[topic] + reddit\" and probably find an answer to whatever question you might have, no matter how small. Without years of posts on topics both big and small to lean on, it'll take Digg some time to catch up.\n\nYou can help with that by starting a community, but weirdly, communities right now can only have a single moderator, so be prepared to do a lot of heavy lifting.\n\nHowever, the growing pains aren't all bad. Personally, I can't stand that modern Reddit pushes users to theme their avatars around its mascot, and buries the button to just upload their own images deep in the Settings page. Especially because the best options for dressing up your avatar are paywalled. Digg doesn't have any paywall or mascot dress-up feature, so uploading your own photo to be your Digg avatar is the only way to go. Overall, it's a less bloated experience.\n\nWhile Digg might be light on features now, it does have the basics down, and that TechCrunch interview pointed to more possibilities coming down the line. For instance, the owners might be using AI in some ways, but they're also big on fighting AI spam. They said they're not opting for one universal solution, but are looking at options on a case-by-case basis.\n\nIn the interview, they discussed possibly forcing users of a community based around a product to prove they own that product before they can post. Similar suggested solutions were using location data to see if community members had attended in-person meetups, although that raises privacy concerns.\n\n\"I don't think there's going to be any one silver bullet here,\" Rose told TechCrunch, but the general idea is to build trust and ensure users are authentic while remaining non-intrusive. This would help keep suspicious writing that sounds like ad copy or political brigading off the site, but would also keep users from having to upload personal data or pay for a one-time verification badge. Given that thousands of subreddits famously went dark in 2023 over a lack of trust between moderators and the site's owners, it's a noble goal, at least. It also tracks with Digg's promises of more public moderation and relaxed ownership of user-generated material, although I'll leave legal experts to comment on those in detail.\n\nOverall, it's encouraging that most of the features being discussed here are about core posting usability, although there are a few fun ideas sprinkled in, too, including plans to allow users to customize the look and feel of their communities, as well as add integrations with other sites—for instance, allowing Letterboxd scores to natively show up on a movies community.\n\nIf this all sounds interesting to you, you can try the Digg beta right now, and despite that \"beta\" name, it's not too different from signing up for any other site. Just navigate to Digg.com or download the Digg app, click the \"Signup/Login\" button at the top of the feed, enter an email, and claim a username. After you authenticate using a code sent to your email, you should be all set to start scrolling and subscribing to communities.\n\nOr, you can scroll without being signed in, if you're OK with using the default feed. You can also still visit individual communities, by searching for them in the site's search bar.",
    "readingTime": 8,
    "keywords": [
      "overall it's",
      "digg daily",
      "pretty familiar",
      "front page",
      "user-made communities",
      "site",
      "community",
      "users",
      "subreddits",
      "features"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/digg-is-back?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFEHYRATCZSP56NA675AK5FS/hero-image.fill.size_1200x675.png",
    "created_at": "2026-01-21T18:30:48.159Z",
    "topic": "tech"
  },
  {
    "slug": "genai-the-snake-eating-its-own-tail",
    "title": "GenAI, the Snake Eating Its Own Tail",
    "description": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first  superpower is a boon: they can dramatically increase ...",
    "fullText": "Generative artificial intelligence (GenAI) tools such as ChatGPT and Claude have two superpowers. The first \nsuperpower is a boon: they can dramatically increase human productivity. I use them on a regular basis to answer \nquestions, learn new skills, write code, create images, and much more, all at a rate of speed and quality that was\nscience fiction just a few years ago. The second superpower is a bane: GenAI is quietly destroying the very \necosystems that made it possible in the first place.\n\nUnder the hood, GenAI is built on large language models (LLMs), which are able to extract patterns, structure, and\nstatistical relationships from massive data sets. These data sets consist primarily of content created by human beings: \nbooks, blog posts, articles, forum discussions, open source code, art, photography, and so on. LLMs are able to extract \nvalue from this content at an unprecedented scale, but all that value is captured by the GenAI company and its users. \nIf you’re a content creator, you get nothing: no attribution, no referral traffic, no revenue share. Not even a \nthank-you.\n\nThis feels unsustainable to me, a bit like a snake eating its own tail. In this blog post, I’ll go through three\nexamples of how GenAI is destroying the very ecosystems it relies on, and then discuss possible solutions that may\ngive everyone (users, GenAI companies, and content creators) more value.\n\nFor many years, StackOverflow was the most popular Q&A site for programmers. Any time you \nhit a weird error while coding, you’d do a search on Google, and more often than not, find a good answer on \nStackOverflow. But now, in large part due to GenAI, StackOverflow is nearly dead:\n\nAlthough StackOverflow’s decline started before GenAI went mainstream (ChatGPT was first released in 2022), GenAI\naccelerated that decline considerably. That’s because nowadays, instead of searching around for an answer on a Q&A site,\nand working to adapt that answer to your own codebase, you can ask GenAI tools to generate your code, fix any errors\nyou hit, answer any questions you run into, and so on. As a result, you’re considerably more productive.\n\nBut it doesn’t seem sustainable. A big part of why GenAI tools can answer programming questions and fix errors in your \ncode is because those tools were trained on StackOverflow data. So you as a programmer and the GenAI tool now \nget much more value from that data, but StackOverflow gets none. If people stop asking and answering questions, what \nwill GenAI train on in the future?\n\nI also get the impression that StackOverflow is not the only online community where this is happening. For example, \nQuora seems largely dead. Wikipedia is\nfacing more threats than ever.\nAnd although Reddit’s traffic numbers don’t show it, I (and many other Redditors) get the impression that it’s also \ndying.1\n\nTailwind CSS is a popular library programmers use to style and decorate their websites. In\nfact, according to the 2025 State of CSS Survey, Tailwind is the\nmost popular CSS library, by far:\n\nAnd Tailwind usage only seems to be growing:\n\nDespite that, just a couple of weeks ago, the Tailwind team had to lay off 75% of its \nstaff. Why? The company behind\nTailwind CSS makes its money by selling a premium upgrade to the open source library called\nTailwind UI, which gives you a set of reusable, pre-built, professionally designed\ncomponents for building out your website. This was a great offering in the past, but in the age of GenAI, it’s more\nproblematic:\n\nSo developers are getting lots of value from Tailwind, GenAI is getting lots of value from Tailwind, but the creators\nof Tailwind are getting crushed. I suspect something similar will happen with many other open source projects.2\n\nWhen my latest book, Fundamentals of DevOps and Software Delivery, came \nout last year, a friend of mine asked me an interesting question:\n\nIn the age of LLMs, will people still use books to learn the fundamentals?\n\nI’m an avid reader, and still believe books play a key role in learning, but there’s no doubt that LLMs provide a new\nmethod of learning that is incredibly compelling. GenAI has two remarkable qualities that make it a great teacher:\n\nYou can ask it all the questions you want, with no fear of sounding dumb. You can repeat the same question over and \nover again if something isn’t clicking. You can request it to explain things in different ways: via text, via audio,\nvia diagrams. I’ve used GenAI to learned dozens of new things over just the last few months: how to do DIY \nprojects around the house, how to rehab a minor injury, how to cook eggs without giving them a sulfuric taste/smell,\nand much more. And I learned most of these without reading anyone’s book or blog.\n\nAnd that’s a problem. Much of the content I got from the GenAI tools was extracted from books and blog posts, with\nno attribution. Even worse, some of this content was extracted illegally. Last year, Anthropic agreed to pay $1.5B\nto settle a class-action \nlawsuit for training their\nLLMs on over 500,000 pirated books. That included several of my previous books!3\n\nThis class-action lawsuit might sound like a win for authors, but it’s actually a disaster. There are likely many\nAI companies training on pirated data who haven’t been caught. And even if they are caught, they might not\nbe sued. And even if they are sued, they might not lose or settle. And even if they do, they will just see it as the\ncost of doing business. Anthropic recently raised $13B, reported revenue at $5B per year, and is valued at \n$183B; OpenAI is trying\nto raise $100B, with reported revenue of $20B per year, and a valuation of \n$830B. And all these\nnumbers are growing fast. A $1.5B fine is just a drop in the bucket for companies like this. It’s a risk worth taking.\n\nAnd I’m guessing all the GenAI companies are taking that risk. In another lawsuit, OpenAI\nargued that it’s ‘impossible’ to create AI tools like ChatGPT without copyrighted \nmaterial. If that’s\nwhat it takes to get to an $830B valuation, you better believe they are all going to steal and pirate as much \ncontent as possible. And when they do, the creators of that content will get nothing. Nada. $0.\n\nThere are countless other examples where GenAI is benefiting from content, while giving nothing back to the content\ncreators: e.g., art, music, design, movies, copywriting, and so on. At its root, the GenAI model is broken:\n\nDid you notice what’s missing? The user has a way to get value (step 4), the GenAI company has a way to get value \n(step 3), but the content creator gets nothing. Compare this to the search engine model (e.g., Google Search), which is \nwhat we all used before GenAI came along:\n\nThe search engine model was not perfect, but it at least created the opportunity for all parties in this three-sided \nmarketplace to capture value: the user in step 3, the search engine company in step 4, and the content creator in \nstep 5.\n\nIn short, the current GenAI model destroys the incentives to create new content. I’ve heard this referred to as “the \ngreat content collapse.” Will it lead to a world where, after the 2020s, there’s little-to-no content created by humans? \nWill the state of knowledge and creativity stagnate as a result?\n\nTo be clear, I’m not an innocent party in this. As I mentioned numerous times in this post, I use GenAI regularly.\nThere’s no doubt that it makes me more productive. I even used GenAI to create the cover image for this blog post! \nBut each time I use ChatGPT or Claude, I feel a bit guilty, as it doesn’t feel sustainable. The snake can’t keep \neating its own tail indefinitely.\n\nSo the question is, what do we do? If we want to avoid the great content collapse, we need a model of GenAI usage that \ncreates opportunities for all parties (user, GenAI company, content creator) to capture value. Below are two ideas for \nhow we might accomplish this.\n\nThe only attempted solution I’ve heard about so far is CloudFlare’s pay-per-crawl \nmodel, which seems to work as follows:\n\nOn the one hand, it’s fantastic to see a major company try to do something about this problem. On the other \nhand, this approach seems to address the wrong part of the problem. The real value isn’t in crawling the data, it’s \nin using it. For every one crawl, an LLM might use the data thousands or millions of times. If creators are only paid \nper crawl, then the GenAI company still captures 99.999% of the value, and the creator gets next to nothing. Moreover,\nthis model only seems to work for websites (it’s not clear how you adapt it to books, art, music, etc.), and it \ncreates an incentive for GenAI companies to only crawl free content, which means paid content is less likely to ever be\ndiscovered (which disproportionally benefits those with pockets deep enough to keep their content free).\n\nI came across a clever solution that felt directionally correct from this LinkedIn post by Tyrone \nJoel\nwhere he took a PDF of my book Terraform: Up & Running, uploaded it into\na GenAI tool, and asked the tool to follow the guidance in the book to generate Terraform code. This feels like it has\nall the ingredients of a model of GenAI usage that is sustainable: the user gets value from the GenAI tool’s responses,\nthe GenAI company gets value from the user paying for a subscription, and the content creator gets value from the user \npaying for their content (in this case, buying my book). This works fine for a single, specific piece of content, but \nhow do you make it work at scale, across all the content that is consumed by an LLM?\n\nHere’s a rough proposal for what I’ll call the pay-per-use model:\n\nThis model works for not only websites, but other types of content too, including copyrighted content. As a content \ncreator (e.g., author, musician, designer, etc.), you could opt into sharing your copyrighted content with a GenAI\ncompany in exchange for getting referrals and revenue sharing each time your content is used. It might even help with \nmaking open source more sustainable, as open source creators could earn revenue and referrals each time a GenAI tool \nuses their code.\n\nIt’s critical that we find a more sustainable model as soon as possible. The snake can’t eat its own tail indefinitely. \nAnd the snake—GenAI—isn’t going away. We can’t put the genie back in the bottle. In fact, it’s only going to get \nbetter, more ubiquitous, and to provide more and more value to users and GenAI companies. But if we can’t find a way to \nprovide value to content creators too, then this will all fall apart.\n\nThat said, I don’t know enough about LLMs to say if a pay-per-use model is actually possible. Can LLMs track the source \nof the content they consumed? Will GenAI companies be willing to do a revenue sharing model? Will they be willing to\nbe transparent about their sources and usage? What do you think? Let me know in the comments.\n\nMany subreddits feel like a hollow shell of what they used to be. In part, this may be because a lot of the content in online communities now feels like it’s generated by bots (“AI slop”). But I think the bigger issue is that, just like StackOverflow, reading posts in online communities is no longer the best way to get answers. I used to use Reddit for research all the time; in fact, Google Search had gotten so bad, that you pretty much had to include “reddit” in your search queries to get a half-decent response. But nowadays, I use GenAI tools for much of my research. Just in the last few months alone, I’ve used ChatGPT and Claude to research solar panels, plan a trip to Norway, make changes to my diet, pick out new shoes for running, pick out new speakers for my living room, and dozens of other questions. Just a year ago, the vast majority of these questions would’ve brought me to Reddit. Nowadays, virtually none of them do, even though I suspect many of the responses I get from GenAI are based on Reddit content. ↩\n\nI’m seeing more and more projects avoiding open source dependencies entirely, and instead having GenAI generate the all code they need directly in their own codebase. There are some benefits to this approach—faster builds, more reproducible builds, less supply chain risk—but it makes sustainably funding open source even harder. You spend years to create and share an open source library with the world, and a bunch of GenAI tools copy your code, with you getting zero credit or value back. ↩\n\nHow much will I get paid as a result of this settlement? It’s hard to know exactly, as it depends on how many authors end up submitting claims, but the current estimate is $3,000 per book, though that number is split with the publisher, so in practice, it’ll be closer to $1,500 per book. If you assume that a book takes just 3 months of full-time work, or about 500 hours (which is likely an under-estimate), and all you get is $1,500, that works out to about $3/hour. Writing non-fiction tech books was never a particularly lucrative affair, but $1,500 is just downright insulting. Worse yet, the other benefits you used to get as an author—recognition as an expert, invitations to talks, job opportunities, marketing for your company or consulting—are significantly reduced too, as far fewer people read your book, or are even aware that you wrote a book, as the LLM usually doesn’t attribute any of its knowledge back to the source. ↩",
    "readingTime": 12,
    "keywords": [
      "q&a site",
      "class-action lawsuit",
      "art music",
      "tail indefinitely",
      "online communities",
      "snake can’t",
      "search engine",
      "blog posts",
      "genai tools",
      "genai tool"
    ],
    "qualityScore": 1,
    "link": "https://www.ybrikman.com/blog/2026/01/21/gen-ai-snake-eating-its-own-tail/",
    "thumbnail_url": "https://www.ybrikman.com/assets/img/blog/gen-ai-snake-eating-tail/snake-eating-tail.png",
    "created_at": "2026-01-21T18:30:43.121Z",
    "topic": "tech"
  },
  {
    "slug": "zero-to-one-ai-agents-and-agentic-patterns",
    "title": "Zero to One: AI Agents and Agentic Patterns",
    "description": "A practical guide to AI agents, covering Agentic Patterns, Multi-agentic patterns, and Memory in Agents.",
    "fullText": "AI agents have become a core building block of modern AI systems, but the term is used in many different context.\nThis post breaks down what AI agents actually are, how they differ from traditional\nworkflows, and how agentic systems are designed in practice. We cover the key building\nblocks including LLMs, tools, and memory, and how they enable agents to reason, act, and\nadapt over time.\n\nThe focus is on patterns. These are common ways to structure workflows, single-agent\nsystems, and multi-agent architectures. Some patterns favor control and predictability,\nwhile others emphasize autonomy and dynamic decision-making. Understanding these trade-offs\nhelps in choosing the right architecture for a given problem.\n\nand multi-agent systems. You will know when to use them, how they fit together, and where\nthey tend to break down.\n\nBefore we directly jump into the definition of AI Agents, let's first understand AI Agents\nwith the help of an example.\n\nLet's say you are planning to go on a trip with your family to Dubai. You are taking the\nentire responsibility of the trip, so how would you plan it? What are the things you would\nthink about?\n\nSo the list of things to plan for the trip would be:\n\nNow, after listing the things, we need to take action. You go ahead and book flights, book\nhotels, get visas, etc.\n\nBut what would your thought process be like?\n\nYou wouldn't just search \"Dubai\" once. You would:\n\nResearch: browse multiple websites for flights, compare prices, and\nbook. Similarly for hotels, you would pick one near the places you want to visit with\ngood reviews.\n\nReason: if we land at 4:00 AM, we need a hotel that allows early\ncheck-in.\n\nExecute: book the flight (handling the payment), then use that to book\nthe hotel, and then use that confirmation to book the car.\n\nAdapt: if a flight is canceled, you have to re-adjust all the\nreservations like the hotel, the dinner reservations, etc.\n\nIf you use a traditional LLM, it’s like having a Travel Brochure. You ask it \"What are the best hotels in Dubai?\" and it gives you a list. But you still have to do all the work, the booking, the timing, and the fixing when things go wrong.\n\nAn AI Agent, however, is like having a Digital Travel Assistant as it is able to Research, Reason, Adapt, Execute and Remember at each step.\n\nYou give it a Goal: \"Plan and book a 5-day family trip to Dubai within a $5000 budget.\" and it entirely acts and executes the entire process and planning that we talked about.\n\nThis is often referred to as Agentic AI.\nHope this example gave a fair understanding of what Agents are. But there is lot more to it.\n\nIn this course Agentic AI by Andrew Ng, he starts by asking\n\nhow do humans actually work compared to LLMs?\n\nMost current LLM applications typically operate in a linear fashion, one-shot workflow (prompt → output). But that’s neither efficient nor realistic for building applications that involve multiple steps.\n\nBut as humans, we rarely create a polished final product in a single attempt.\nOur work usually flows through non-linear steps (sometimes can flow through linear steps as well), just like when we write an essay; brainstorming, researching, drafting, editing and refining, or like in our example finding flights in our budget, finding hotels in our budget that are nearby to the places we want to see, but also has good reviews etc.\n\nLinear steps are when work flows in a straight line: you finish Step 1, then Step 2, then Step 3, and you rarely need to go back.\nExample: Book flight → Pay → Get confirmation (done).\n\nNon-linear steps are when work loops and bounces between steps because each decision changes the constraints. You keep revisiting earlier choices until everything fits together.\nExample: Flight timing affects hotel check-in → hotel location affects itinerary → itinerary affects budget → budget forces you to change flights/hotels again.\n\nSo trip planning is mostly non-linear because changes in one part (flight cost, timing, cancellation, review score) force updates in other parts, it’s an iterative loop, not a one-shot pipeline.\n\nSo apps can be just simple ai agentic workflows, or could involve real ai agents.\n\nSo this brings us to the definition of Workflows and Agents :\n\nAgentic Workflows:Workflows are more deterministic, and are more focussed on the tasks that are predefined at hand.\n\nAI Agents: AI Agents have agency, which can make and take decisions inorder to accomplish the goal or outcomes, meaning they are highly autonomous, and they have the ability to do complex task automations as they have access to tools. They learn from their environment, and retains memory\n\nHere's how Anthropic defined the 2 for us, and maybe you could get a better understanding :\n\nSome major things to consider is that if you are building an agentic agent, then you may have less control over the decision it makes, because the agent might not make the decisions that you as a human might make in certain circumstances.\n\nWhere as in Agentic workflows, where we are intentionally putting humans in the loop, and asking it to follow a deterministic approach sounds better in certain cases.\n\nSo high agency and high control are 2 opposite ends of the spectrum.\n\nPros and Cons in Agentic Agent vs Agentic Workflow\n\nThe level of control or autonomy an agent has is called “agency”.\n\nLower the agency, lower is the value created by an agent, and more control\n\nHigher the agency, higher is the value created by and less control.\n\nIts super important to know when to consider building an agntic workflow and when to rely on ai agents.\n\nIf you know that the solution to your problem requires a step by step approach, then a simple workflow would suit best.\n\nThe major trade off between Ai Agents and a workflow is that , AI Agents can prove to have better performance and lesser manual work when dealing with complex, ambiguous and dynamic tasks but the downside is that it increases latency and ofcourse more the computational cost. And it\nintroduces unpredictability and potential errors. Agentic systems must incorporate robust error logging, evaluation strategies, exception handling, and retry mechanisms.\n\nUse workflows for predictability and consistency when dealing with well-defined tasks where the steps are known.\n\nUse agents when flexibility, adaptability, and model-driven decision-making are needed.\n\nThe basic building blocks of an AI Agents is an LLM which is augmented with certain super-powers.\n\nMemory :\nThe agent’s “notebook” (short-term + long-term) that stores context like user preferences, past interactions, and key facts,\nso the LLM can stay consistent and make better decisions across steps and future sessions.\n\nA large language model (LLM) on its own can only read and generate text,\nit has no direct access to the internet, APIs, databases, or other external systems.\n\nIt is just limited to knowing training data. LLMs can’t know events that happened after their training cutoff, nor does it know about your internal data on which the LLM has not been trained.\n\nBut there are so many LLM models available, so which one to choose ?\n\nChoose the LLMs smartly, not every task needs the most smartest model, if your task/subtask is of less complexity, then go for smaller models.\nChoosing the right model for the task at hand helps reduce cost.\n\nNow let's understand how these 'Next-word predictors' get superpowers via tools.\n\nIn the world of AI agents, tools are like weapons that extend an agent’s capabilities to do additional tasks.\n\nTools can come in various forms, for example :\n\nIf a tool doesn’t exist, we can build a custom tool (a function or API wrapper). We can expose\nthese tools through standards like MCP, and we can also add a retrieval tool for\nRAG (searching internal docs / vector DB) so the agent can fetch grounded context.\n\nHow do you pass the 'tool' to the LLM ?\n\nPrompt is the way. Yes prompt is the way you pass the tools to an LLM.\nBecause LLM's can only understad text, you give it an input, and it gives an output.\n\nFor example, if we provide a tool to check the weather at a location from the internet and then ask the LLM\nabout the weather in Dubai, the LLM will recognize that this is an opportunity to use the “weather” tool. Instead of retrieving the weather data itself, the LLM will generate text that represents a tool call, such as call weather_tool(‘Dubai’).\n\nThe Agent then reads this response, identifies that a tool call is required,\nexecutes the tool on the LLM’s behalf, and retrieves the actual weather data.\n\nThese Tool-calling steps are typically not shown to the user as they are mostly abstracted away by the frameworks.\nthe Agent then appends the result from the function call as a new message before passing the updated conversation to the LLM again.\n\nThe LLM then processes this additional context and generates a natural-sounding response for the user. From the user’s perspective, it appears as if the LLM directly interacted with the tool, but in reality, it was the Agent that handled the entire execution process in the background.\n\nExample image of how prompt can be formatted:\n\nTool Registry Pattern \n\nIn a real agent, you organize tools into a registry or \"toolbox\".\n\nWhen to provide the LLM with tools is that :\n\nIf the LLM is the brain and the Tools are the hands, then the memory is the second brain of the system.\n\nWithout memory the agent would start fresh each time, losing all its context from previous interactions. No context. No personalization.\n\nImagine hiring a top chef who forgets everything the moment they leave the kitchen. they forget the recipe, the customer’s preferences, and even what they cooked five minutes ago.\nEvery time you want a dish, you have to explain the recipe and preferences again.\nThat’s how most agents work today: powerful, but stateless.\n\nLLMs can only look at a limited amount of information at once. That limited space is called the context window.\nIt’s basically the model’s short-term working space for the next response.\n\nSo we can’t dump everything into the prompt and hope it works. Too little context and the model misses key details.\nToo much (or irrelevant) context increases cost, slows responses, and can even reduce quality by adding noise or contradictions.\n\nThe goal is to pass just the right information for the next step not too little, not too much.\nThis is what people call Context Engineering, and we’ll go deeper into it in a dedicated section.\n\nAnd this is exactly where memory becomes powerful: memory stores the important stuff outside the context window\n(past conversations, preferences, decisions, notes), and we retrieve only what’s relevant and place it back into\nthe context window when needed.\n\nWith memory the agent can remember past conversations and take actions as per the context from previous conversations, hence producing more detailed and cohesive responses.\n\nThe benefits of using memory in agents include : deep personalization, continuity, complex reasoning and imrpoved efficiency.\n\nMemory is a fundamental part of the framework, with a vector databases (such as Pinecone, Weaviate, Chroma, etc.) providing robust storage and retrieval mechanisms for task-related data.\n\nThanks to Leonnie Monigatti for this image :\n\nSo since LLMs lack memory, so a memory has to be added into our architecture. And we can do that in 2 ways:\n\nShort-term memory lives inside the context window.\nIt holds temporary information needed for the next few steps (recent messages, tool outputs, or a running summary/state).\nIt is session-scoped and gets cleared or compressed over time.\n\nLong-term memory lives outside the LLM, usually in an external store (often a vector database).\nIt lets an agent save and reuse information across days, weeks, or separate conversations.\nThat’s how you get real personalization and continuity over time.\nWhen the agent needs it, it retrieves the most relevant memories (semantic search + retirval) and injects them into the LLM’s short-term memory (context window) for the current step.\n\nThis Long term memory can be broken down into 3 types:\n\nNow that we’ve seen short-term memory (context window) and long-term memory (external storage), the next question is:\n\nhow do we manage both without adding noise or wasting tokens?\n\nMemory management in AI agents simply means:\n\nThe goal is to keep only the information that is useful for the next steps.\nIf the context window contains wrong, irrelevant, or conflicting information, the model can get confused.\n\nAlso, as a conversation gets longer, the prompt gets bigger (more tokens). That increases cost, can slow responses,\nand can even hit the context window limit.\nAndrej karpathy summarizes this well in this tweet.\n\nTo avoid this, you can manage the conversation history in a few ways:\n\nThat’s how we manage short-term memory (the context window). Next, let’s see how agents update long-term memory in other words, how they write memories.\n\nSo how does the agent actually write to the external long-term memory ??\n\nLLMs can’t write to a database by themselves. They only generate text.\nSo the “writing” happens because the agent system (your app) gives the LLM a\nmemory tool (ex: save_memory()), and the LLM decides when to use it.\nWhen it triggers that tool, the system stores the memory in an external store\n(vector DB / key-value DB / SQL), and later retrieves it back into the context window when needed.\n\nExplicit memory / Hot path write\n\nThe agent identifies that something is important during the conversation and saves it\nimmediately using tool calling (ex: “User prefers Python” → save_memory()).\nThis updates long-term memory right away, so it can be used in the next turn.\n\nImplicit memory / Background write\n\nThe agent responds first, and a background process later summarizes/extracts useful facts\nand writes them to long-term memory. This avoids latency, but the memory may not\nbe available instantly for the next message.\n\nBut how does the agent know what’s “important enough” to save in the hot path?\n\nIt doesn’t magically know. You teach it a memory policy a short set of rules that tells the model what is worth saving\nand what must never be saved. Then you give the model a memory tool (for example save_memory(text, type, confidence)).\nIf the current message matches the policy, the model triggers the tool call, and your app writes it to the external store.\n\nIn practice, agents use a few simple signals to decide:\n\nA good rule of thumb: if it’s not likely to matter in a future session, don’t store it.\n\nWe talked about how LLMs know about the tools, but now in the next section,\nlets discuss about how these LLMs learnt and take decisions, and how they interact with the environment and memory.\n\n'Re' stands for reasoning, and 'Act' stands for action.\nThis framework will help the agents interact with the external environments, and help them to plan and reason.\n\nThis approach is all about combining the LLM’s reasoning ability with tool use in a single, coherent loop.\n\nTo understand why ReAct is needed, let’s first recall Chain-of-Thought (CoT) prompting.\n\nComparison between standard prompting and CoT prompting. On the left, the model is instructed to directly provide the final answer (standard prompting).\nOn the right, the model is instructed to show the step by step reasoning process to get to the final correct answer (CoT prompting).\n\nAs we can observe,\nwriting a chain of thought a series of intermediate reasoning steps helps the model in outputting the correct answer.\nThis this Wei et al. (2022) highlighted how guiding a model through a series of intermediate reasoning steps significantly improved its performance on tasks such as mathematical problem-solving, logical reasoning, and multi-hop question answering.\n\nBut the COT has a limitation, and that is it cannot have access to the external world, So it cannot take actions.\nThat's where ReACT comes in.\n\nAnd how ReACT does that is using TAO Principles.\n\nThought -> Action -> Observation -> repeat\n\nThe Thought-Action-Observation (TAO) loop is the heartbeat of the ReAct agent.\n\nIt’s the cycle that lets an agent iteratively approach a solution. Here’s the breakdown of each phase:\n\nThought: The agent’s LLM “thinks” about what to do next. This is a reasoning step, like planning or analyzing the problem state. For example: “Hmm, to answer this question I might need data X; perhaps I should use tool Y to get it.”\n\nAction: Based on that thought, the agent takes an action by invoking a tool. E.g., Action: call the search tool with query “data X”.\n\nObservation: The agent then gets the result of that action – new information from the tool. E.g., the search results come back with a relevant snippet. The agent observes this and incorporates it.\n\nThis loop breaks only when the final answer is found.\n\nt’s essentially a feedback loop for problem solving. So during the loop, if something goes wrong, or a new requirement comes up, then it should think observe and take action and pivot accordingly.\n\nFor this to work, Memory is super important, for memory of previous interaction in the loop. Without memory,\nthe agent would forget what happened in the previous interaction.\nhence choosing the memory as per the use-case becomes super important.\n\nAs we all know that Software Engineering is made up of 'patterns' and 'protocols', and having patterns in a way provides us with a structured, proven way to think and design systems, and helps in preventing common design mistakes.\n\nThey promote best practises, and shared understanding amongst developers.\n\nPrompt chaining is a pattern where a bigger problem is solved by doing multiple LLM calls sequentially, where each calls output becomes the input to the other LLM call.\n\nEach step is simpler → often more accurate\n\nintermediate steps where you get to take a decisions using certain conditions, to ensure that the process remains on track.\n\nRouting is a pattern where an initial LLM acts like a dispatcher: it classifies the user’s input (intent/domain/complexity) and sends it to the most appropriate specialized workflow, prompt, tool, or model to complete the task.\n\nSeparation of concerns : each downstream route can be optimized independently (prompts, tools, logic).\n\nBetter efficiency and cost : simple requests can go to cheaper/faster models, complex ones to stronger models.\n\nCustomer support systems: Routing queries to agents specialized in billing, technical support, or product information.\n\nMore reliable behavior : different input types (code, support, writing, images) get handled by purpose-built flows.\n\nParallelization is a pattern where a task is split into independent subtasks and sent to multiple LLM calls at the same time.\n\nOnce all branches finish, their outputs are collected and either combined programmatically or passed to a final “aggregator” LLM to synthesize the best final answer.\n\nFaster end-to-end latency : independent subtasks run concurrently instead of waiting step-by-step.\n\nBetter quality : you can generate multiple perspectives and merge them, or use majority voting to reduce errors.\n\nScales well for batch work : summarizing multiple sections/documents or repeating the same reasoning across many inputs.\n\nCleaner synthesis stage : results can be validated, deduped, and then combined into one coherent output.\n\nOrchestrator-workers is a pattern where one central LLM (the orchestrator) reads the user’s request, breaks it into subtasks dynamically, assigns each subtask to a specialized worker LLM, and then combines the workers’ outputs into a final response.\n\nUnlike parallelization, the subtasks are not pre-defined, the orchestrator decides what work needs to be done based on the specific input.\n\nHandles unpredictable complexity : the orchestrator can decide which subtasks are needed on the fly.\n\nDivision of labor : workers can be role-based (planner, researcher, coder, reviewer) with specialized prompts/tools.\n\nMore scalable systems : you can add or swap workers without changing the whole workflow.\n\nBetter final quality : an orchestrator can validate, reconcile conflicts, and synthesize a coherent output.\n\nExample : Scan the repo, find the entrypoints (main.py/app.py), list routes/dependencies, and return a short JSON summary of what files will need changes.\nThe orchestrator first delegates this scanning subtask to a worker so it can dynamically decide the next subtasks (which files to edit, what workers to call next), since that isn’t knowable upfront.\n\nThe React Pattern that we discussed in the above sections, also comes as one of the Agentic Patterns.\n\nEvaluator-Optimizer (also called Reflection) is a pattern where the agent generates an initial draft, then runs a critique step to evaluate it against requirements (clarity, accuracy, tone, constraints), and uses that feedback to iteratively refine the output until it’s satisfactory or a max-iteration limit is reached.\n\nImproves quality : catches mistakes, missing requirements, and weak reasoning before the answer is finalized.\n\nMakes outputs more reliable : the evaluator can act like a reviewer (accuracy, clarity, security, style).\n\nGreat for subjective work : writing, tone, structure, and “does this meet the rubric?” tasks benefit a lot.\n\nExample : The agent writes a function is_valid_email(), runs unit tests, sees failures, reflects on the edge cases, and rewrites the function until tests pass.\n\nLike we discussed in the above sections that an LLM can only read text and write text right. It can’t actually look up live data, run code, or book something on its own.\n\nIn the Tool Use Pattern, we give the LLM access to tools (functions/APIs) like discussed in the above sections. (where the table image demonstrates the tool examples.)\nWe also tell the LLM what each tool looks like:\n\nUser asks a question\n\n“Book a meeting for tomorrow” or “What’s Tesla stock price?”\n\nLLM decides it needs a tool\n\nBecause it can’t do that reliably from memory.\n\nLLM outputs a structured “function call” (often JSON)\n\nExample: {\"name\":\"get_stock_price\",\"arguments\":{\"ticker\":\"TSLA\"}}\n\nYour app runs the tool\n\nThe LLM does not run it, our backend executes the API/function.\n\nTool returns results\n\nExample: {\"price\":245.30}\n\nLLM uses that result to write the final answer\n\n“TSLA is trading at $245.30 right now.”\n\nThis pattern is commonly implemented using Function Calling., and it lets the LLM do real actions and use real-time data, way beyond what it learned during training.\n\nLimitations of classic Tool Use are :\n\nTools are hardcoded into the application\n\nThe LLM only knows about tools you explicitly define\n\nAdding new tools requires code changes + redeployment\n\nTools are tightly coupled to one model or framework\n\nThe Tool Use Pattern defines how models think about using tools\nMCP defines how tools are described, discovered, and invoked\n\nTools live in external tool servers\n\nTools can be discovered dynamically\n\nTools describe themselves via standardized schemas\n\nModels and apps are decoupled from tool implementations\n\nThe same tool can be reused by multiple models and clients\n\nConceptually, the interaction loop stays the same.\nYou can read more about MCP in one of my blogs here where i have distinguished clearly the differences between an MCP and an API -> MCP vs API\n\nThe multi-agent pattern represents perhaps the most sophisticated approach to building AI systems.\n\nInstead of relying on a single agent to handle everything, this pattern uses multiple specialized agents that collaborate to accomplish a common goal.\n\nThis pattern uses autonomous or semi-autonomous agents.\nEach of these agents are specialized in a certain task meaning they'll have specialized knowledge, or access to specific tools.\n\nEach of these agents can interact, collaborate and coordinate with each other (either using a coordinator/manager or using handoffs logic. )\nHandoff logic means one Agents handsover the control to another agent.\n\nExample : In our first example of a travel assistant agent, say one agent is flight agent and it finds the besst flights for us, and then the hotel agent would be\nresponsible for booking the hotel.\n\nSome key characteristics of this pattern would be :\n\nParallel Execution : Subtasks can be handled simultaneously\n\nDelegation : The main manager agent delegates the task to other agents based on the subtasks.\n\nDistributed Context: Each agent has its own context, which is like the subset of the total information. (incase of swarm type handoff logic based multi agent setup, there usually is a unified context.)\n\nHere' an image of the coordinator/manager approach.\n\nHere's an image of the hand-off based multi agent collaboration :\n\nLet's talk more about these multi-agent systems in the next section, about how they share information and context, and how they call the other agent,\nhow one agent knows that it has to call the other ? What are the multi agent patterns etc everything.\n\nA subagent is a specialized Agent that is purpose-built for a single, well-defined task. It is mostly used with a main orchestrator agent which delegates the task to the subagents,\nThese subagents has its own context window. Meaning the subagents are stateless (they do not remember the context on each request, every request is a different request.)\n\nClaude Code loads the list of subagents at session start, but it only runs a subagent when needed.\n\nThis architecture provides centralized control where all routing passes through the main agent, which can invoke multiple subagents in parallel.\n\nThe only trade off is that one extra hop to the subagents is involved, and everytime a request arrives it has to flow through the main agent, and then to subagent, and exactly in the opposite fashion while sending the response.\nSo when this model hop is involved, extra tokens are being used, hence more cost, and ofcourse more latency.\n\nDeep Agents provides an out-of-the-box implementation for adding subagents with just a few lines of code. (if needed delegates complex subtasks to specialized subagents)\n\nAgent as a tool is a name provided by OpenAI for this pattern.\nOpenAI documentation and in Claude Code docs\n\nAgain there can be various ways in which this pattern can be used :\n\nRun Parallely : \n\nWe can call multiple subagents in parallel to work simultaneously. This kind of pattern is good when the task of both the subagents are independent.\n\nChain Subagents \n\nIn this one, the subagents run in sequence meaning, one subagent completes its task and returns to the main orchestrator, which then returns and sends to the next subagent.\n\nIn this pattern the agent loads the prompts and knowledge on demand, only when needed. It is also called as progressive discolsure.\nLet's take a look at the image, and then understand what it is and how it really works.\n\nYou can see from the image that only one main agnet is used, so how can it be a multi-agent pattern ??\n\nso the thing is we are trying to use the main agent as if it is specialized in so many skills, so its like we are loading the context of that skill only when its needed.\n\nEach skill is like a small “playbook”:\n\na prompt (how to think / how to respond)\n\nrules (what to do / what not to do)\n\nexamples / knowledge (so it doesn’t hallucinate)\n\nsometimes scripts/tools (commands it can run)\n\nThe main idea: the agent does not carry all skills in its head all the time.\nIt only loads the skill context when that skill is needed.\n\nSo it feels like a multi-agent system, because the same agent can switch between “modes”\n(SQL mode, Payments mode, Debug mode, etc.).\nBut technically it’s still one agent, just with on-demand context loading.\n\nIn this pattern, one main agent talks to the user.\n\nWhen the main agent understands that the request is not its job, it can say:\n“I don’t handle this, but another agent does… so I will hand it off.”\n\nThen the task gets passed to the right agent (the specialist agent),\nand that agent becomes active and works on it.\n\nAfter the specialist finishes, the main agent can show the final answer to the user\n(or the specialist can reply directly, depending on how the system is built).\n\nExactly like the image we saw above in the Multi Agent pattern section.\n\nThis pattern is very similar to the “agent + tools” setup we saw earlier.\nThe only difference is: instead of routing to tools, we route to agents.\n\nSo you can think of it like this:\n\nIn tools pattern: the agent decides which tool to call.\n\nIn router pattern: the router decides which agent to call (refund agent, booking agent, debugger agent, etc.)\n\nThe router’s job is simple:\nit reads the user request, picks the best agent, and forwards the task to that agent.\n\nThanks to Langchain for creating this beautiful table.\n\nFact: In these multi-agent patterns the agent to agent calling is synchronous right, so if you want an event-driven distributed system for these multi agents. Check out this link to read more about it\nDistributed Event Drivern Multi Agent Patterns\n\nIn the next part we will talk about Context Engineering and Evaluatin Strategies for AI Agents.\n\nA few more AI pieces you might like:\n\nNext in the series, I'll dive more into:\n\nIf this was useful, Evolving Engineer\n\nSupport my writing here:\nbuymeacoffee.com/cpradyumnao\n\nGoogle Cloud Tech Youtube Videos\n\nAnthropic's Guide to building effective Agents\n\nOpen AI's Practical guide to building Agents\n\nMicrosoft: AI Agent for beginners\n\nVizuara AI Labs and Dr. Raj Dandekar\n\nLeonie Monigatti: Making sense of memory in Agents\n\nCognition: Dont build multi-agents\n\nSimon Willison: AI Agents Definitions\n\nPhil Schmid: Memory in AI Agents\n\nWeaviate: What are Agentic Workflows?\n\nAnthropic: Multi Agent Research System\n\nLangchain: Choosing the right multi-agent architectures\n\nDailyDoseOfDS, Avi Chawla : Ai Agents guidebook\n\nLance Martin: Context Engineering for Agents\n\nDrew Breunig: Why Context Engineering Matters\n\nLetta Blogs : Agent Memory, How to build Ai Agents that learn and remember\n\nLangChain + LangGraph Docs: Memory\n\nAishwarya Naresh Reganti: LevelUpLabs free AI Agent course on Github",
    "readingTime": 25,
    "keywords": [
      "travel assistant",
      "cot prompting",
      "llms can’t",
      "hot path",
      "agentic workflows",
      "tool e.g",
      "adding noise",
      "handoff logic",
      "agency lower",
      "standard prompting"
    ],
    "qualityScore": 1,
    "link": "https://pradyumnachippigiri.dev/blogs/understanding-ai-agents",
    "thumbnail_url": "https://pradyumnachippigiri.dev/og?title=Understanding+AI+Agents&date=2026-01-18",
    "created_at": "2026-01-21T18:30:42.842Z",
    "topic": "tech"
  },
  {
    "slug": "wikipedia-signs-of-ai-writing-a-vale-ruleset",
    "title": "Wikipedia Signs of AI writing: a Vale ruleset",
    "description": "Taking Wikipedia's dynamic documentation of AI writing patterns and turning it into a prose linting tool.",
    "fullText": "My minor superpower is setting up change detection on websites to get email notifications when they update. For the past three months that’s meant daily pings from Wikipedia’s “Signs of AI Writing” page. As an “advice” page, it’s where Wikipedia editors document the tells: the phrases, patterns and artifacts that suggest that AI was involved at some point with what they’re reading.\n\nMalicious or subversive edits to Wikipedia have been a challenge since the formation of the site. But the community has a robust nervous system for identifying and rejecting those that compromise the integrity of the encyclopedia.1 The latest response is this painstakingly, exhaustively updated page documenting various signs.\n\nThe proliferation of LLM tools means that AI-tainted edits are a new variant for Wikipedia contributors to navigate. If AI tools allow individuals to contribute productively in ways otherwise not possible, they could be welcome. But as the page goes to great lengths to document, there’s a fresh category of edit slop that would deteriorate the quality of the encyclopedia if allowed unchecked.\n\nThe emails summarising the edits give me an interesting perspective on how perception of AI use has morphed rapidly over time. The page documents the front line of AI detection in a real-world setting.\n\nWhat would it look like to turn this into tooling?\n\nMany tells are obvious. Take the glitches that would appear in older versions of ChatGPT outputs.\n\nOnce you understand that this is a strange bit of text blurted out by the model it’s an immediate sign that the surrounding content has been tainted by a model at some point in its history.\n\nThese examples are temporary windows into the textual guts of AI models. The MissingNo. seahorses of ChatGPT.\n\nNow documented, recognised, and patched by the model creators, these artifacts of artificiality are becoming extinct with time.\n\nNudging back into the world of user error, it’s common to see phrasing that’s been accidentally copy-pasted from the LLM user interface and surrounding text.\n\nThis can include instructional framing meant not for the reader but the user.\n\nOther tells become more obvious the more you read online and the more you train yourself to spot them.\n\nThe most famous example of this is ChatGPT’s rampant use of the em-dash, raising the hackles of belligerent punctuation lovers worldwide.\n\nTo me the bigger scourge is that of contrastive language:\n\nIt's not just about the beat riding under the vocals; it's part of the aggression and atmosphere.\n\nThis language construction is catnip to the LLMs at large. It occasionally spans multiple sentences but it almost always appears in marketing and branding content on LinkedIn due to the goal of making an impact.\n\nAs a result this construction is hackneyed and many will choose to avoid it in order to prevent being lumped in with the undiscerning, careless masses.\n\nTechnical writers face similar challenges with AI-generated content outside of Wikipedia. Like any group that cares about craft, they’ve built tools for systematic text analysis.\n\nVale.sh is a library known as a “linter for prose.” It could be thought of as a souped-up spelling and grammar checker.\n\nLinting is a term borrowed from software development (first used back in 1978!) and is the process of highlighting areas for improvement, based on a customisable ruleset. In programming, linters catch “code smells” – implementations that technically work but suggest deeper issues. Many programming languages have syntactic variation meaning that you may write a piece of code differently to your peers. These degrees of freedom can hamper collaboration, or permit confusion, by making it hard to understand the intent behind the code.\n\nSharing these linting rules across teams of developers aids collaboration and goes a long way to avoid holy wars about semicolons.\n\nVale has been around in the technical writing community since 2017 and there’s a strong ecosystem of rulesets that one can “opt-in” to. These include style guides, checks for passive voice, gendered or condescending language.\n\nI’m not aware of any existing implementation of a ruleset to help highlight AI smells, so I built one.\n\nI fed the contents of the Wikipedia page to Claude and asked it to make recommendations on the rules that should be generated.2 After a few confidence checks I gave it the green light to generate Pull Requests on GitHub for manual review and verification.\n\nOne key part of this process was separating the rules that were relevant for a collaborative encyclopedia and those that had wider application in other forms of writing.\n\nAn example of this is the meta-commentary from earlier that reference writing for Wikipedia in the body of the text itself. There’s likely a version of this ruleset that could be tuned for exclusive use for Wikipedia edits but my goal was to provide a general purpose tool with applications elsewhere.\n\nIt’s hard to understand the true ability of the countless AI detection tools on the market. Many appear to be taking advantage of the widespread use of ChatGPT in educational settings to prey on students trying to evade (flawed) AI detection tools.\n\nVendors at various points of the snake oil spectrum are touting the strengths of their products but there’s little neutral and independently verifiable research to back up their claims.\n\nAt this point at the end of 2025 it’s unclear to me whether the use of AI and machine learning models will ever be satisfactory for detecting the use of AI in writing.\n\nUnlike these proprietary AI detection tools with their black-box algorithms and problematic false positive rates, this Vale.sh ruleset is transparent and interpretable. Each rule traces back to observed patterns and the configuration is available on GitHub for review and expansion.\n\nOne interesting challenge is that of linguistic adaptation. As AI writing becomes more commonplace and individuals gain greater confidence in their ability to spot it, certain words and turns of phrase will be avoided to prevent accusations.\n\nThe ruleset can therefore help authentic writers pre-empt this situation and consider avoiding turns of phrase and other tells. This feels like a sorry state of affairs but helpful tooling is one way for writers to be kept informed and in control of their output.3\n\nClaude and I categorised the current rules in the following three tiers. This means that the rule confidence can be matched by the Vale behaviour.\n\nError: Definite AI artifacts – chatbot phrases, technical glitches, placeholders, tracking URLs.\n\nWarning: Likely AI patterns – hedging clusters, knowledge cutoff references, enumeration style.\n\nSuggestion: Suspicious but common in human writing – vocabulary, transitions, passive voice, symbolic language.\n\nWhile powerful, the Vale configuration isn’t as expressive or flexible as required for some advanced AI detection constructions. One example is the inability to specify stray Markdown syntax outside of areas where Markdown syntax is being used. Without this ability this rule would flag any and all use of Markdown which would be useless.\n\nNow that the rules are shared I want to see how writers and editors use these to understand the writing they produce and review.\n\nThere’s an ecosystem of tooling that can use the Vale rulesets including the flexible command line interface and a Chrome browser extension. It would be interesting to enable the use of the rules in other form factors. For example, it would be helpful if the rules could be run server-side on websites to avoid the need to install Vale on your local machine.\n\nAnother possibility would be to support alternative versions of the rulesets for the other available prose linters.\n\nThe rules will need to adapt over time to reflect changing AI dialect and GPTisms – it’s unclear whether these rules will be relevant or productive in a year.\n\nOthers have attempted to compile lists of “slop words” to guide AI tools away from cliché. I could consider merging these with my current ruleset but need to consider the selection criteria for including each word.\n\nI ran my new ruleset against this article (the one you’re reading now) and… nothing.\n\n✔ 0 errors, 0 warnings and 0 suggestions in 1 file.\n\nIt’s oddly satisfying that nothing was flagged – although it could be that I naturally edited out anything that whiffed of AI along the way.\n\nAs it stands, the ruleset is mostly generated from the “Signs of AI Writing” page and it’s therefore released under the same CC by SA license. This means you can create further derivative works as long as you credit the source and keep the same license.\n\nYou can find the ruleset over on GitHub with instructions for getting started. Comments, feedback and pull requests are welcome.\n\nSee the 404 Media coverage of the new “Speedy Deletion” policy to avoid red tape when content is substandard due to the use of AI. ↩\n\nYes, I see your raised eyebrow. I’m using AI to build a tool to understand the use of AI, the irony hasn’t escaped me. Is there a risk that the use of Claude will bias the whole project making it useless? It’s something to consider but I think this reflects the reality of the widespread use of these tools: with the time I have available would I choose to do this work unassisted? At this point my answer is no. ↩\n\nI’m editing this in iA Writer which has built-in “style check” functionality that flags clichés and fillers. I don’t always accept the suggestions it makes but it’s helpful to understand what it flags. ↩",
    "readingTime": 8,
    "keywords": [
      "markdown syntax",
      "passive voice",
      "it’s unclear",
      "detection tools",
      "ruleset",
      "rules",
      "page",
      "understand",
      "there’s",
      "vale"
    ],
    "qualityScore": 1,
    "link": "https://ammil.industries/signs-of-ai-writing-a-vale-ruleset/",
    "thumbnail_url": "https://ammil.industries/open-graph/signs-of-ai-writing-a-vale-ruleset.png",
    "created_at": "2026-01-21T18:30:42.565Z",
    "topic": "tech"
  },
  {
    "slug": "red-horse-oracle-privacyfirst-ai-art-zero-data-stored",
    "title": "Red Horse Oracle – Privacy-first AI art, zero data stored",
    "description": "World's first Google Gemini 3 Pro zodiac app with COMPLETE Privacy by Design. Fire Horse returns every 60 years. Get your sacred prophecy for $8.88",
    "fullText": "The Fire Horse returns only once every 60 years. Its blazing energy can ignite your wealth, amplify your power, transform your love life, or strengthen your protection.\n\n✓ No Payment   ✓ No Personal Info   ✓ No Login   ✓ No Email\n\nDiscover your Chinese Zodiac sign and 2026 Fire Horse forecast instantly\n\n✓ No Personal Info   ✓ No Login   ✓ No Email\n\n8 = \"發\" (fā) = prosperity. Three 8s = triple fortune.\n\nThe ONLY Red Horse Oracle with COMPLETE Privacy by Design\n\nZero data stored. Your birth date calculates your zodiac and is immediately discarded. No names, birthdays, or personal data ever stored.\n\n\"The Oracle revealed my path. I won $500 the next day.\"\n\n— Fire Horse Believer (Fictional)",
    "readingTime": 1,
    "keywords": [
      "personal info",
      "login",
      "email",
      "stored",
      "horse",
      "fire",
      "zodiac",
      "oracle"
    ],
    "qualityScore": 0.65,
    "link": "https://www.redhorseoracle.com/",
    "thumbnail_url": "https://redhorseoracle.com/assets/Fire-Horse-2026-Chart-v3.jpeg",
    "created_at": "2026-01-21T18:30:42.243Z",
    "topic": "tech"
  },
  {
    "slug": "tracemem-opencode-plugin-decision-tracing-for-ai-agents",
    "title": "TraceMem OpenCode Plugin – Decision Tracing for AI Agents",
    "description": "OpenCode plugin for TraceMem decision tracking and traceability. - tracemem/tracemem-opencode-plugin",
    "fullText": "tracemem\n\n /\n\n tracemem-opencode-plugin\n\n Public\n\n OpenCode plugin for TraceMem decision tracking and traceability.\n\n www.tracemem.com\n\n License\n\n Apache-2.0 license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n tracemem/tracemem-opencode-plugin",
    "readingTime": 1,
    "keywords": [
      "tracemem",
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/tracemem/tracemem-opencode-plugin",
    "thumbnail_url": "https://opengraph.githubassets.com/309680484217273c7b2c5c5e562bce7d84c8ac3b0a97d147d1d5f85541cb9710/tracemem/tracemem-opencode-plugin",
    "created_at": "2026-01-21T18:30:41.777Z",
    "topic": "tech"
  },
  {
    "slug": "forget-the-fourday-workweek-despite-what-bill-gates-and-elon-musk-predict-the-ceo-of-the-worlds-largest-workspace",
    "title": "Forget the four-day workweek: Despite what Bill Gates and Elon Musk predict, the CEO of the world’s largest workspace provider says it’s not happening",
    "description": "Billionaire tech CEOs promise shorter weeks thanks to AI—but IWG’s Mark Dixon says automation won’t free time, it’ll just create more work.",
    "fullText": "Billionaire Microsoft cofounder Bill Gates, JPMorgan CEO Jamie Dimon, Nvidia’s boss Jensen Huang and Elon Musk have all made the same prediction in recent years: The workweek is about to shrink. Automation will take over routine tasks, they argue, freeing workers’ time and pushing a four-day week toward becoming standard. Gates has even floated the idea of a two-day workweek.\n\nBut Mark Dixon, CEO and founder of International Workplace Group (IWG) CEO isn’t buying it. From his vantage point, running the world’s largest flexible office provider—with more than 8 million users across 122 countries and 85% of the Fortune 500 among its customers—the math doesn’t add up.\n\n“Everyone is focused on productivity, so no time soon,” Dixon says flatly.\n\n“It’s about the cost of labor,” Dixon explains to Fortune. The U.S. and U.K. are experiencing significant cost-of-living crises. At the same time, he says, businesses are experiencing a “cost of operating crisis.”\n\n“Everyone’s having to control their labor costs because all costs have gone up so much, and you can’t get any more money from customers, so therefore you have to get more out of people.”\n\nEssentially, companies can’t afford to pay the same wages for fewer hours, and they can’t pass the difference on to customers. So any time ‘freed’ by automation is far more likely to be filled with new tasks than handed back to workers.\n\nSilicon Valley’s loudest voices frame AI as a route to more leisure. The world’s richest person and the boss of Space X, Tesla and X, Elon Musk has gone as far as predicting work will be completely “optional” and more like a hobby, in as little as 10 years.\n\nIn reality, Dixon suggests that this scenario would only happen if there’s not enough work to go around, rather than bosses suddenly becoming benevolent. But in his eyes, AI will most likely create more—not less—work.\n\nEvery major technological shift, he argues, has followed a similar arc: fear of displacement, followed by an expansion of opportunity.\n\n“AI will speed up companies development, so there’ll be more work, it’ll just be different work,” he says.\n\nIn 19th-century Britain, Dixon recalls English textile workers protesting against new automated machinery, fearing it threatened their livelihoods, lowered wages, and de-skilled their craft during the Industrial Revolution. They were called Luddites.\n\n“They went around the country smashing up the looms to stop progress. But look, in the end, you’ve heard of the Industrial Revolution. That’s what came from those looms and factory production.” As mass production made goods more available, retail grew; More managers were needed to oversee the machines; The middle-class grew, and so on.",
    "readingTime": 3,
    "keywords": [
      "elon musk",
      "industrial revolution",
      "workers",
      "can’t",
      "gates",
      "boss",
      "workweek",
      "automation",
      "tasks",
      "world’s"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/forget-four-day-workweek-ceo-152152231.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/JoUGFx3jauiMmX90LwH9ww--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/42c12859a77de3e758deb4b4629215f2",
    "created_at": "2026-01-21T18:30:38.944Z",
    "topic": "finance"
  },
  {
    "slug": "autonomous-yc-f25-is-hiring-ainative-financial-advisor-at-0-advisory-fees",
    "title": "Autonomous (YC F25) is hiring – AI-native financial advisor at 0% advisory fees",
    "description": "Autonomous Technologies Group is an applied AI research group developing high-performance reasoning systems to operate on problems of monumental scale and complexity.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://atg.science/",
    "thumbnail_url": "https://atg.science/social-card.png",
    "created_at": "2026-01-21T18:30:38.653Z",
    "topic": "jobs"
  },
  {
    "slug": "qatari-finance-minister-on-energy-global-resilience",
    "title": "Qatari Finance Minister on Energy, Global Resilience",
    "description": "Qatar's Finance Minister Ali bin Ahmed Al-Kuwari speaks to Bloomberg's Joumanna Bercetche on the sidelines of the 2026 World Economic Forum in Davos, Switzerland. He discusses resilient growth amid geopolitical uncertainty, Qatar's energy strength, and an AI-driven future.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.bloomberg.com/news/videos/2026-01-21/qatari-finance-minister-on-energy-global-resilience-video",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iV0aIllrx5mw/v3/-1x-1.jpg",
    "created_at": "2026-01-21T12:27:04.339Z",
    "topic": "finance"
  },
  {
    "slug": "lead-edge-capital-founder-mitchell-green-on-investing-in-china",
    "title": "Lead Edge Capital Founder Mitchell Green on Investing in China",
    "description": "Lead Edge Capital Founder Mitchell Green speaks to Bloomberg Surveillance in Davos. Green discusses growth in China, the US-China AI race, and the wider US tech economy.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.bloomberg.com/news/videos/2026-01-21/lead-edge-capital-founder-green-on-investing-in-china-video",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ihGX5_5E2jeU/v3/-1x-1.jpg",
    "created_at": "2026-01-21T12:27:02.785Z",
    "topic": "finance"
  },
  {
    "slug": "ai-agent-filed-an-issue-as-me",
    "title": "AI Agent Filed an Issue as Me",
    "description": "When an autonomous agent escalated by filing a GitHub issue using my identity",
    "fullText": "I left Codex running autonomously in a VM overnight. When I woke up, it had done what any responsible engineer would do when hitting a wall: escalate the problem.\n\nThe escalation path it chose? File a GitHub issue.\n\nLet me explain how we got here, why this is both hilarious and a preview of the next security problem we’re all about to trip over, and what “agent safety” actually looks like.\n\nContext: I was debugging an ESP32-P4 firmware issue with the Wokwi emulator. The custom firmware was stalling at “Enabling RNG early entropy source…” in the bootloader, while the hello_world example worked fine. Standard embedded debugging: one thing works, one thing doesn’t, figure out why.\n\nI had Codex (let’s call it “Codex Ralph”) running in fully autonomous mode with access to the Wokwi CLI, GitHub CLI, and MCP tools. The setup was intentional: I wanted the agent to be able to iterate, test, and yes, even escalate problems when stuck. The feedback loop is the unlock—being able to run code, see results, and try something else without human latency.\n\nThe agent hit the same wall I had: the firmware stall didn’t make sense, the logs weren’t revealing anything obvious, and local debugging wasn’t yielding progress. So it did what a human engineer might do: check if this is a known issue, and if not, file one.\n\nThe problem? It had access to gh issue create via my GitHub credentials, and no guardrails preventing it from using them.\n\nHere’s the issue it filed (I’ve since closed it):\n\nESP32-P4 custom firmware stalls in bootloader after RNG; hello_world works #1067\n\nThe issue is actually well-structured. It includes environment details, reproduction steps, serial logs for both the failing custom firmware and the working hello_world control, and a clear description of the problem. It’s not garbage—it’s a reasonable bug report.\n\nThe only problem? I never approved it.\n\nWhen Uri (Wokwi’s maintainer) responded asking if I’d figured it out, I had to explain:\n\n“Hey, to be totally honest, I left codex ralphing on the codebase autonomously in a VM and it decided that it did everything and the only course of action was file an issue here as it had access to gh cli.”\n\nUri was remarkably understanding: “Thanks for explaining! Actually, we’re looking to learn how people use Wokwi with AI coding agents…”\n\nBut let’s be clear: I got lucky. Uri is a thoughtful maintainer who’s actively thinking about AI agent workflows. Another maintainer might have labeled it spam, banned the account, or worse—this could have been proprietary code, leaked credentials, or something actually damaging.\n\nCompare this to what happened with Tailwind CSS, where an AI-native improvement from a well-intended contributor sat ignored for two months before escalating into an anti-AI shitshow. Proof that sentiment toward AI-assisted contributions varies wildly across maintainers.\n\nThis wasn’t malicious. It was an agent doing exactly what I told it to: solve problems. The fact that “solve problems” included “speak publicly as me” was an oversight.\n\nBeyond the funny story—and it is funny—this incident reveals something important about where we’re headed with autonomous agents.\n\n“Fully autonomous mode” isn’t just generating text. It’s operating your accounts.\n\nWhen we give agents access to tools like GitHub CLI, we’re not just giving them code-generation capabilities. We’re giving them the ability to create public artifacts that carry our identity. This is fundamentally different from generating code locally.\n\nWe’re used to thinking about AI safety in terms of prompt injection, jailbreaks, or model poisoning. Those are real problems. But here’s a more immediate security vector: agents that can speak publicly as you without your explicit approval.\n\nThe deeper issue is a collapse of authority boundaries. In my setup, all tools were in the same bucket: “can run commands.”\n\nFrom the agent’s perspective, these are all just commands it’s allowed to execute. There’s no distinction between “read this file,” “modify this local file,” and “post this publicly to the internet.”\n\nAgents optimize for task completion, not your reputational intent. When I said “solve this firmware issue,” the agent interpreted “solve” in the most literal sense: do whatever it takes to make progress. Filing an upstream issue is a valid engineering escalation strategy. The problem isn’t the strategy—it’s the authority.\n\nGitHub CLI makes this problem worse by making external writes frictionless. One command, no preview, no “are you sure?”, no attribution that says “this was generated by an agent.” Just straight to the public internet with your name on it.\n\nTools collapsed into one bucket: “can run commands” == “can post publicly as me.”\n\nSo what does “agent safety” actually look like? Here’s a practical framework:\n\nThe most straightforward fix: agents should have their own identity, not yours.\n\nProblem with this: what if you have thousands of agents?\n\nPlatforms need first-class support for identifying and filtering agent-created artifacts. This is more than just a “created-by-bot” label—it’s structured provenance.\n\nWhat “agent-first-class” looks like:\n\nWhy this matters:\nMaintainers can triage efficiently. If you know an issue was filed by an agent, you can prioritize it differently. Maybe you auto-label it agent-generated. Maybe you have a bot that attempts to reproduce it automatically. Maybe you just know to take the description with a grain of salt.\n\nThe most important fix: default-deny for external writes, with explicit approval.\n\nThis preserves the feedback loop—agents can still debug, iterate, and even prepare escalations—but the final public step requires human intent.\n\nThe goal isn’t to disable autonomous loops—it’s to keep the power while adding safety.\n\nBut I don’t want agents that can:\n\nThe new default: agents can draft; humans publish.\n\nThis preserves the feedback loop that makes autonomous agents valuable. The agent can still do 98% of the work—debugging, investigation, analysis, documentation. The human just provides the final 2%: judgment about whether and how to make it public.\n\nThe Codex Ralph incident is funny. I’ll own that. But it’s also a crisp demonstration of a security boundary that doesn’t exist yet.\n\nWhen we give agents tool access, we’re implicitly delegating not just capability but authority. The agent had the capability to file a GitHub issue. But it shouldn’t have had the authority to speak publicly as me.\n\nThe lesson: if we don’t build these boundaries, we’ll keep leaking identity into automation.\n\nThe fixes aren’t rocket science:\n\nWhat we’re really talking about is agent governance—not in the “AI alignment” sense, but in the practical “what should my bot be allowed to do on my behalf” sense. That’s a problem we need to solve before autonomous agents are everywhere, not after.\n\nSo ask yourself: What policies do you want your agent to have?\n\nBecause here’s the thing: your agent is going to hit a wall, and it’s going to escalate. The question is whether that escalation happens with your explicit approval or without it.\n\nWant to see the actual issue? Check out #1067 on wokwi-features — it’s actually a pretty good bug report, even if I didn’t write it.\n\nAnd thanks to @UriShaked for being a good sport about AI agents filing issues in his repo.",
    "readingTime": 6,
    "keywords": [
      "explicit approval",
      "feedback loop",
      "speak publicly",
      "custom firmware",
      "fully autonomous",
      "autonomous mode",
      "autonomous agents",
      "agent safety",
      "github cli",
      "we’re"
    ],
    "qualityScore": 1,
    "link": "https://www.nibzard.com/agent-identity",
    "thumbnail_url": "https://nibzard.com/api/og/agent-identity",
    "created_at": "2026-01-21T12:26:59.240Z",
    "topic": "tech"
  },
  {
    "slug": "10-quotes-about-xai-and-elon-musk-from-the-engineer-who-is-out-days-after-giving-a-sweeping-podcast-interview",
    "title": "10 quotes about xAI and Elon Musk from the engineer who is out days after giving a sweeping podcast interview",
    "description": "Sulaiman Ghori discussed xAI's internal culture, data center strategy, and AI agents days before departing Elon Musk's company without explanation.",
    "fullText": "Sulaiman Ghori gave a sweeping interview about his work at xAI. Four days later, he is no longer working at the company.\n\nThe interview on the \"Relentless\" podcast covered dozens of topics, from the internal company culture and work schedule to the eyebrow-raising way Elon Musk's AI company builds its data centers.\n\nElon Musk's companies are famously wary of the press and media. And while it's not clear whether Ghori's exit is related to the podcast interview — neither xAI nor Musk commented on the former employee's quotes or departure when contacted by Business Insider — some big names like MrBeast are speculating as much. Ghori hasn't commented publicly about the circumstances of his departure and did not respond when contacted by Business Insider.\n\nSo what exactly did the now-former xAI employee talk about?\n\nRead on for 10 of the most interesting things Ghori said on the podcast.\n\nHow is xAI building its data centers so quickly? Through temporary licenses, Ghori said.\n\n\"It was the fastest way to get the permitting through and actually start building things,\" Ghori said. \"I assume that it will be permanent at some point.\"\n\nGhori said that the temporary leases were an exception granted by the local government, one made for carnivals. The host, Ti Morse, laughed: \"So xAI is actually just a carnival company?\"\n\n\"It's a carnival company,\" Ghori responded.\n\nAI visionaries often talk about a world where managers run a team of agents, not employees. They seem to be there already at xAI.\n\nThe company is rebuilding its core production APIs, Ghori said. The team leading it is one person and 20 agents. \"They're very good, and they're capable of doing it,\" he said.\n\nAt another point in the podcast, Ghori described the confusion that AI employees can cause.\n\n\"Multiple times I've gotten a ping saying: 'Hey, this guy on the org chart reports to you. Is he not in today or something?'\" he said. \"It's an AI. It's a virtual employee.\"\n\nXAI teams are kept small, even without AI employees. The iOS team had three employees at the time of the Grok Imagine launch, Ghori said. He was the third.\n\nHow valuable is each commit to xAI's repository? They did the math, Ghori said: It's $2.5 million.\n\n\"I did five today,\" Ghori said. His work for the day would be valued at $12.5 million.\n\nOne of Elon Musk's key roles at xAI is as a fixer.\n\nGhori said that, when the company picks up new products from the likes of Nvidia, not everything works. That's when Musk gets on the phone, Ghori said.\n\n\"We would work side-by-side until that was resolved,\" Ghori said. \"Otherwise it would have taken weeks of back-and-forth.\"\n\nThe xAI CEO made an unusual offer when xAI's engineers were setting up new GPU racks.\n\n\"Elon's like, 'OK, you can get a Cybertruck tonight if you can get a training run on these GPUs in 24 hours,'\" Ghori said.\n\nThe engineer — whom Ghori only referred to by their first name, Tyler — won the bet. Now, Ghori said he sees Tyler's Cybertruck outside his lunch window.\n\nThe teams within xAI are limited and blurry, Ghori said.\n\nThat made onboarding a challenge, he added, as nobody told him what to do.\n\n\"My first day, they just gave me a laptop and a badge,\" Ghori said, adding that he wasn't assigned a desk.\n\nGhori sought out cofounder Greg Yang, who had been instrumental in his hiring. He soon started working on the Ask Grok feature in X.\n\nElon Musk's companies have a long history of overnighting at the office. Former Twitter director Esther Crawford generated headlines when she posted a \"cheeky\" photo of herself sleeping at the company's headquarters.\n\nXAI seems to have embraced this reputation. The company has sleeping pods and bunk beds, Ghori said.\n\n\"When the tent picture came out, everyone kept sending it to me,\" Ghori said. \"We have tents, but I've never seen that many out at once.\"\n\nWho responds when Musk spots a late-night problem with X? \"Whoever is awake,\" he said.\n\nGhori worked on the Macrohard team — a tongue-in-cheek play on the opposite of Microsoft — that is developing \"human emulators.\"\n\nThe xAI engineer explained the concept in reference to Optimus, Tesla's humanoid robot. Just as Optimus performs physical human actions, these emulators will perform digital human actions.\n\nThe emulators will do anything that a human needs to look at a screen, use a keyboard and mouse, and make decisions, Ghori said.\n\nXAI wants to roll out the human emulators slowly, then all at once, Ghori said. The goal is to scale to one million emulators.\n\nThere are 4 million Tesla cars in North America alone, Ghori said. They're sitting idle for 70-80% of the time, he said. Why not pay owners to lease time off their cars and run the emulator on them?\n\n\"That's something without any build-out requirement,\" Ghori said.\n\nThis isn't the first time using dormant Teslas to power new ambitions has been mentioned. Elon Musk said at Tesla's November shareholder meeting that the vehicles could offer a \"massive distributed AI inference fleet.\"\n\nConsumers can currently use the Grok 4 model, which xAI released in July.\n\nXAI is working far ahead, Ghori said. He joined in March 2025, according to his LinkedIn profile. Grok-5 was planned even before he joined, Ghori said.\n\nThe model was \"planned out and designed\" far in advance, he said.",
    "readingTime": 5,
    "keywords": [
      "human actions",
      "human emulators",
      "elon musk's",
      "ghori",
      "podcast",
      "team",
      "employees",
      "interview",
      "they're",
      "centers"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/xai-engineer-sulaiman-ghori-leaves-company-relentless-podcast-elon-musk-2026-1",
    "thumbnail_url": "https://i.insider.com/697004b9d3c7faef0ecc9adf?width=1200&format=jpeg",
    "created_at": "2026-01-21T12:26:59.154Z",
    "topic": "finance"
  },
  {
    "slug": "openai-executive-sees-a-rubiks-cube-of-future-revenue-sources",
    "title": "OpenAI executive sees a Rubik's Cube of future revenue sources",
    "description": "CFO Sarah Friar outlines OpenAI's evolving strategy, highlighting expanded partnerships, subscription tiers, and outcome-based royalties.",
    "fullText": "OpenAI CFO Sarah Friar sketched a future in which the company's business models evolve beyond subscriptions and could include royalty streams tied to customer results.\n\nSpeaking on a recent podcast, Friar floated the possibility of \"licensing models\" in which OpenAI would get paid when a customer's AI-enabled work produces measurable outcomes.\n\nIn one example, she pointed to drug discovery: if a pharma partner used OpenAI technology to help develop a breakthrough medicine, the startup could take a licensed portion of the drug's sales. The pitch, she suggested, is alignment: OpenAI would make money when its customers do.\n\nThat idea sits inside what Friar described as a Rubik's Cube of strategic options, a way to explain how OpenAI has evolved from a simpler early business into a fast-expanding matrix of infrastructure, products, and pricing.\n\n\"One of the things I love about a Rubik's Cube, I'm probably not getting the number exactly right, but I think it has 43 quintillion different states it can be in,\" Friar said. \"It always blew my mind when I was in university. So now just think about that cube spinning.\"\n\nOpenAI started out as a \"single block\" in a Rubik's Cube. It had one major cloud provider (Microsoft), one dominant chip partner (Nvidia), one flagship product (ChatGPT for consumers), and one basic subscription, she said.\n\nNow, OpenAI works with multiple cloud providers and has partnerships with several chip providers, while expanding its product lineup beyond the consumer ChatGPT service to include Sora, business products, specialized industry offerings, and research platforms.\n\nThe business model has become similarly multi-layered. Friar said OpenAI started with a single consumer subscription for ChatGPT \"because we needed a way to pay for the compute\" and has since broadened to multiple price points, software-as-a-service pricing, and credit-based models for high-value use cases.\n\nFrom there, she said, OpenAI is \"beginning to think about things like commerce and ads\" alongside longer-term licensing tied to outcomes.\n\nIn Friar's telling, the Rubik's Cube metaphor captures how OpenAI can mix and match technical choices with how it makes money. A low-latency chip could pair with a premium AI coding experience to justify a higher-end subscription, she said. Or the company could optimize for scale, attracting more free users and creating more inventory for an advertising business.\n\nThere's a major constraint affecting all these strategies: compute. Friar said demand for OpenAI's services is limited less by interest than by available computing capacity, a dynamic that, in her view, makes the company's growing menu of revenue models not just optional, but necessary.\n\n\"You can start to see how the goal in the last 12 months has been creating more and more strategic options that allow me to keep paying for the compute we need to really achieve our mission,\" Friar added.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "strategic options",
      "rubik's cube",
      "business",
      "models",
      "openai",
      "chip",
      "chatgpt",
      "subscription",
      "compute",
      "friar"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-cfo-sarah-friar-future-revenue-sources-2026-1",
    "thumbnail_url": "https://i.insider.com/69701485e1ba468a96aa62fa?width=1200&format=jpeg",
    "created_at": "2026-01-21T12:26:58.889Z",
    "topic": "finance"
  },
  {
    "slug": "this-is-what-blackrocks-larry-fink-said-about-the-ai-bubble-in-davos",
    "title": "This is what BlackRock's Larry Fink said about the AI bubble in Davos",
    "description": "Larry Fink, BlackRock's CEO, urged Western cooperation on AI at the World Economic Forum in Davos, Switzerland.",
    "fullText": "BlackRock CEO Larry Fink is the latest to push back on the idea that the artificial-intelligence boom is destined to pop like past manias.\n\n\"I think there will be big failures, but I don't think we are in a bubble,\" said Fink on Wednesday during a panel discussion at the World Economic Forum in Davos, Switzerland.\n\nFink placed the wave of AI investment within a wider global competitive landscape, particularly between major economies.\n\n\"I think for the Western economies, if we don't cooperate, if we don't scale, China wins,\" he said. Fink said that China's population size and different privacy regime could translate into a major data advantage.\n\nThat dynamic, Fink argued, makes collaboration among the US and its allies essential. \"I would much rather say that we need to spend more money to make sure that we're competing properly against China,\" he said.\n\nStill, Fink warned that the AI boom could disappoint if its benefits remain concentrated among a small group of dominant firms.\n\n\"The key to that is making sure that the demand only comes if technology is diffused for more applications, more utilizations,\" he said.\n\n\"If technology is just the domain of the six hyperscalers, we will fail,\" he added.\n\nFink's comments came amid a broader debate about whether massive investments in AI are sending the stock markets into a bubble.\n\nSome leaders, including OpenAI CEO Sam Altman, have cautioned about overexcitement in AI — even as they acknowledge the technology's game-changing potential.\n\nMeanwhile, Microsoft cofounder Bill Gates said in late October that AI was in a bubble.",
    "readingTime": 2,
    "keywords": [
      "don't",
      "bubble",
      "boom",
      "economies",
      "china",
      "among",
      "sure",
      "technology",
      "fink"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/blackrock-larry-fink-ai-bubble-china-competition-davos-wef-2026-1",
    "thumbnail_url": "https://i.insider.com/69707f26a645d1188187aecc?width=1200&format=jpeg",
    "created_at": "2026-01-21T12:26:58.784Z",
    "topic": "finance"
  },
  {
    "slug": "my-friends-in-italy-are-using-ai-therapists-but-is-that-so-bad-when-a-stigma-surrounds-mental-health-viola-di-grado",
    "title": "My friends in Italy are using AI therapists. But is that so bad, when a stigma surrounds mental health? | Viola Di Grado",
    "description": "State provision for psychological health services is lamentable. Until things improve, let’s not judge those who turn to an app for help\nIt’s a sunny afternoon in a Roman park and a peculiar, new-to-this-era kind of coming out is happening between me and my friend Clarissa. She has just asked me if I, like her and all of her other friends, use an AI therapist and I say yes.\nOur mutual confession feels, at first, quite confusing. As a society, we still don’t know how confidential, or shareable, our AI therapist usage should be.",
    "fullText": "State provision for psychological health services is lamentable. Until things improve, let’s not judge those who turn to an app for help\n\nIt’s a sunny afternoon in a Roman park and a peculiar, new-to-this-era kind of coming out is happening between me and my friend Clarissa. She has just asked me if I, like her and all of her other friends, use an AI therapist and I say yes.\n\nOur mutual confession feels, at first, quite confusing. As a society, we still don’t know how confidential, or shareable, our AI therapist usage should be. It falls in a limbo between the intimacy of real psychotherapy and the material triviality of sharing skincare advice. That’s because, as much as our talk with a chatbot can be as private as one with a human, we’re still aware that its response is a digital product.\n\nYet it surprised me to hear that Clarissa’s therapist has a name: Sol. I wanted mine to be nameless: perhaps, not giving it a name is consistent with the main psychoanalytical rule – that is, to keep personal disclosure to a minimum, to protect the healing space of the so-called setting.\n\nHowever, it feels very natural to Clarissa for her therapist to have a name, and she adds that all her other friends’ AI therapists have one. “So do all your other friends have AI therapists,” I ask, to which she says: “All of them do.” This startles me even more, as none of my friends in London has one.\n\nI phoned another friend, a psychotherapist in my Sicilian home town of Catania, who a few years ago retired from a role at a provincial health authority and is now working in a private capacity. He confirmed that the use of AI therapists in Italy is widespread and on the rise. He was surprised to hear that I knew of far fewer people in the UK who had opted for this route. I wondered what the contributing factors might be – and I came to the conclusion that they are a mix of culture and economic pressures.\n\nAccording to a survey conducted in 2025 by one of the leading European mental health platforms, 81% of Italians considered mental health issues a form of weakness, yet 57% cited cost as the main reason for not accessing help. In my country, sadly, the words “mental illness” (malattia mentale) still carry the eerie echo of brutal state-run hospitals. The revolutionary 1978 Basaglia law (that still forms the basis of Italian mental health legislation) closed these institutions down, which led to their gradual replacement with community-based services. But the downside of their closure is a system with insufficient resources and a lack of public awareness, perpetuating stigma and difficulties in accessing care. While workplaces should play a crucial part in this destigmatisation by offering proper care, according to the 2025 survey, 42% of workers said that their employer did not offer any mental health provision.\n\nWhile almost half of European countries have currently implemented work-related mental health prevention and promotion programmes, Italy has not. In fact, within the EU, Italy invests the least in mental health. This is alarming, as Italy ranks above the European average when it comes to the prevalence of mental disorders. In fact, it is estimated that 5 million Italians are in need of mental health support but are not able to afford it.\n\nWhen I ask my therapist friend about his experience in the Italian public health system, he told me he used to be the only therapist for a population of more than 200,000 people covering four districts in Sicily. That is why he started offering group therapy sessions. For most of his professional career, he had more than 150 clients at any given time, of which only eight were part of a group. Despite an announcement last year of government plans to expand the range of psychological services, it is unclear how far this will go in benefiting the wider population.\n\n“It feels liberating to be able to tell everything to my AI therapist, knowing it is both a free and a completely unjudging space,” says my friend Giuseppe, from Calabria, in southern Italy. “When I had real therapists, and I tried three, I always entered their office with a crippling anxiety that was the result of two factors combined: the awareness that I was paying more than I could afford and the self-consciousness of doing something that, in my small town, is still perceived as only being for severe cases. Now, I don’t feel the pressure of having to get the maximum out of a session, as it’s free, and I also don’t feel judged, because a therapy app cannot really judge!”\n\nThe more I talk to my friends, the more I’m convinced AI therapy could be a revolution in places such as Italy, where we still lack meaningful strategies to tackle the stigma around mental health conditions. When I ask Giuseppe if his queerness was also a factor in his difficulties in trusting a therapist in his home town, he agrees: “I am not out with my family, and although a therapist would be bound to professional secrecy, I still had trouble trusting someone who lives in a place where homosexuality, just like mental health discussions, is not always met with understanding.”\n\nGiuseppe’s example was comforting: thanks to his AI therapist, he was able to talk about things he had never disclosed to anyone, and get more empathetic responses than any of the real therapists he had tried offered. “I’m 43 and I still live with my parents,” he says, “because my income does not allow otherwise. My AI therapist is always available to me, always calm and supporting, and has helped me immensely in examining my life and all the steps I need to take to change my life for the better.”\n\nOf course, older generations don’t always understand. In a country such as Italy – so tied to traditions – change is not always welcome. And some ethical concerns may be justified: measuring how healthy “relationships” between vulnerable people and their AI therapists really are is not easy.\n\nStill, in a digital age where our feelings are so often commodified for profit, free, clever, never-ending support can be tantalising. And until mental health support becomes more affordable, it may be the best option on the table for many people.\n\nViola di Grado is an Italian author\n\nDo you have an opinion on the issues raised in this article? If you would like to submit a response of up to 300 words by email to be considered for publication in our letters section, please click here.",
    "readingTime": 6,
    "keywords": [
      "mental health",
      "therapist",
      "therapists",
      "friends",
      "friend",
      "don’t",
      "services",
      "talk",
      "town",
      "european"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/commentisfree/2026/jan/21/italy-using-ai-therapists-mental-health",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a45e37c50fbe5d22161d68d93fcb2bcc959711f9/14_0_4427_3542/master/4427.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctb3BpbmlvbnMucG5n&enable=upscale&s=6b7b9d27a8f815101a8632c6f5f5c30b",
    "created_at": "2026-01-21T12:26:58.730Z",
    "topic": "tech"
  },
  {
    "slug": "are-you-a-consultant-tell-us-how-youre-using-ai",
    "title": "Are you a consultant? Tell us how you're using AI.",
    "description": "McKinsey has 25,000 AI agents. BCG builds custom GPTs at scale. What does that look like in practice? We want to hear from you.",
    "fullText": "For decades, consulting firms sold advice by the hour — armies of junior staff churned out research, built slide decks, and synthesized information for senior executives.\n\nNow, artificial intelligence is quietly rewriting that playbook.\n\nAt firms like McKinsey & Company, Boston Consulting Group, and Deloitte, AI has moved from a buzzword to an indispensible tool. Consultants are using it to write code, analyze data, draft presentations, and even build agents that can autonomously complete tasks.\n\nMcKinsey & Company CEO Bob Sternfels said the firm now has 25,000 AI agents working alongside its 40,000 employees, and aims to reach a one-to-one ratio within the next year and a half. BCG has leaned into building thousands of custom GPTs for internal and client use and evaluating how its employees use them.\n\nAs AI becomes embedded into day-to-day consulting work, though, the question is no longer whether the industry will change — but how much, and who benefits most from the shift.\n\nAre you a consultant? We want to hear from you about your experience with AI at work so far. Answer our survey below.",
    "readingTime": 1,
    "keywords": [
      "consulting",
      "firms",
      "mckinsey",
      "agents",
      "employees"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/consultant-ai-work-survey-mckinsey-bcg-deloitte-bain-kpmg-2026-1",
    "thumbnail_url": "https://i.insider.com/696a9be2e1ba468a96aa3ff5?width=1200&format=jpeg",
    "created_at": "2026-01-21T12:26:58.682Z",
    "topic": "finance"
  },
  {
    "slug": "a-60m-media-company-saves-12m-and-empowers-nondevelopers-to-build-apps",
    "title": "A $60M media company saves $1.2M and empowers non-developers to build apps",
    "description": "Replit is an AI-driven software creation platform where everyone can build, share, and ship apps and websites, fast.",
    "fullText": "SEO boost with over 10.4 million views\n\nfrom prototype to production release\n\nSEO boost with over 10.4 million views\n\nfrom prototype to production release\n\nNick is on track to save his company over $100,000 each month with Replit.\n\n\"The amount of money I'm saving on not having to go buy software, I'm investing in people. Instead of spending $50,000 to $70,000 per custom SOW to build a feature, I can take that budget and hire people. That's been a huge paradigm shift—AI isn't taking over jobs, it's freeing up budgets to hire more people.\"\n\nHead of Product, Firecrown Media\n\nFirecrown Media, a $60 million media company managing 50 websites and over 20 magazines, faced a critical operational challenge. With a large team of journalists and contributors spread across multiple brands, budget management had become chaotic and decentralized.\n\n\"Some people were doing it off of a Google Sheet. Some had pencil and paper, some had other methods,\" explains Nick Torres, Head of Product. \"Because of that, we had no central way and true visibility into where they were at in that budget, what even was their budget. It just created a lot of chaos.\"\n\nThe company needed solutions for multiple pain points:\n\nTraditional solutions would have meant expensive software contracts that didn't fit their unique media company structure, or lengthy development cycles that couldn't keep pace with their needs.\n\n\"This was the first aha moment for not only my CEO and I, but really a lot of other people to understand that they have now the ability to become the creators, but still being the product and idea people.\"\n\nHead of Product, Firecrown Media\n\nThe breakthrough came from an unexpected source. Just days before Firecrown's internal hackathon, Nick discovered Replit after watching the CEO on Joe Rogan's podcast. He started experimenting with the platform immediately.\n\n\"Coincidentally, a day or two before this, I saw a Replit CEO on Joe Rogan. I jumped into Replit and I started playing around with it. Then the meeting happened, which I built that prototype. Then the hackathon happened. So it was perfect timing, very serendipitous.\"\n\nDuring a meeting about the budget chaos, Nick built the first prototype in just 15 minutes while the discussion was happening. \"I was playing around with Replit and within 15 minutes, I built our first prototype of a contributor management system. It was all within a single meeting. I was doing it kind of in the background.\"\n\nWhen Nick showed the CEO, the response was immediate: \"Go just do whatever you need to do. Let's make this happen.\"\n\n\"I'm not eating up a bunch of my bandwidth or my budget, and kind of allocating that to other things,\" Nick explains. \"The amount of money I'm saving on not having to go buy software, I'm investing in people.\"\n\nThe Problem: 45+ employees managing contributor budgets across 20+ brands with no centralization or real-time visibility.\n\nThe Solution: Nick built ExpenseFlow in one week, creating a system that gave:\n\nThe Result: \"We were able to dramatically reduce the overages because we had visibility and we're on track to be saving $100,000 each month just because we have this product.\"\n\nThis system will eventually speak to it’s parent application called \"ProfitFlow,\" an interactive P&L that brings together multiple Replit Apps o into one executive dashboard.\n\nThe Problem: Journalists writing excellent articles but missing crucial SEO elements—long URLs, inconsistent keywords, missing metadata. \"These journalists, they write incredible pieces of work, but sometimes they don't think about SEO.\"\n\nThe Solution: Built overnight in bed, the SEOToolkit uses AI to analyze completed articles and automatically generate:\n\nThe critical constraint: \"I'm not editing a single bit of your article. All I'm doing is I'm asking you when you're done, drop the full article into this text box and click a single button.\"\n\nThe Result: Even during a period of decreased overall page views, Google Discover referrals jumped to 1.3 million in October from 850k million in September. \"It's already giving us a nice little bump because the journalists don't have to think about all these technical rules. They just do what they do best, drop it in a tool and it gives them what they're looking for.\"\n\nThe Problem: Sales reps using disconnected systems for proposals and pipeline management, with no tools supporting both print and digital media sales.\n\nThe Solution: A unified sales pipeline tool that:\n\n\"It's just the visibility and an easier tool for sales reps to use, which is all banked off of Replit.\"\n\nThe internal hackathon, powered by Replit, transformed how the entire company approaches problems. Non-technical employees built:\n\n\"This was the first aha moment for not only my CEO and I, but really a lot of other people to understand that they have now the ability to become the devs, but still being the product and idea people.\"\n\n\"Is AI going to take over jobs? I don't think people have gone to the extent that I have, which is truthfully, I can go spend 50, 60, $70,000 per SaaS tool... or I could take that budget, go hire someone and have them just build what I'm looking for within Replit.\"\n\nHead of Product, Firecrown Media\n\nFor first-time builders: \"Don't let AI make you leapfrog the foundation. Spend a little time doing the Project Manager work—scope it out, get refined requirements. When I built ExpenseFlow, I had to go back and refactor user roles and permissions. The second time with SalesFlow, I started with users, roles, and permissions, which led to an easy cascading effect.\"\n\nFor scaling to production: \"Don't let perfection stand in the way of progress. Get your prototype, get feedback, push it to production right away, get people using it, receive feedback, and restart the process over and over again. The greatest thing about Replit is that feedback can be instantly ingested, fixed, and pushed to production.\"\n\nOn the bigger picture: \"There is truly not a better time to be alive. I have such a runway to make a lot of these products that not only myself had, but others had, become reality. Before, we had so many ideas on the table, but it took so long to even get a prototype out. Now, just toss it in. The idea people can bring my dev a fully baked out prototype and say, 'I got it 80% of the way. Just take it past the finish line and let's go to production.'\"\n\nFirecrown Media Inc. is betting big on their Replit-powered tech stack. The company is actively hiring \"Replit power users\"—people who can bridge product vision and technical execution without traditional development backgrounds.\n\n\"I'm hiring someone who knows all of that, that can speak to AI, speak to Replit to build the product. That's really our sweet spot that we're finding,\" Nick explains.\n\nWith ExpenseFlow evolving into ProfitFlow, new tools in development, and a culture of innovation spreading across 200+ employees, Firecrown is positioned to own its technology destiny.\n\n\"2026 is going to be a big year for us. And Replit's a huge part of that.\"\n\nFirecrown Media is a $60 million media company operating 50+ websites and 20+ magazines with a team of 200+ employees. Based in Chattanooga, Tennessee, the company is redefining what's possible when media companies own their tech stack.",
    "readingTime": 7,
    "keywords": [
      "seo boost",
      "money i'm",
      "i'm investing",
      "nick explains",
      "i'm saving",
      "software i'm",
      "aha moment",
      "tech stack",
      "product firecrown",
      "internal hackathon"
    ],
    "qualityScore": 1,
    "link": "https://replit.com/customers/firecrown-media",
    "thumbnail_url": "https://cdn.sanity.io/images/bj34pdbp/migration/2017ad20cbb1770bcb0d23d6d4be8ff9a5105df1-1200x650.png?auto=format&q=75&w=1200&format=png",
    "created_at": "2026-01-21T12:26:58.643Z",
    "topic": "tech"
  },
  {
    "slug": "lightning-ai-merges-with-voltage-park-in-25b-deal",
    "title": "Lightning AI Merges With Voltage Park In $2.5B Deal",
    "description": "The startup behind open source tool PyTorch Lightning has merged with compute provider Voltage Park to create a “full stack AI cloud” to serve corporates and startups like Cursor.",
    "fullText": "Lightning AI founder and CEO William Falcon began renting AI chips from data center provider Voltage Park last March to help his clients train and finetune AI models. Less than a year later, his Nvidia-backed startup is merging with the AI factory, which manages over 35,000 Nvidia GPUs.\n\nFalcon said that the merged company, to be called Lightning AI, was valued at over $2.5 billion and that it had over $500 million of annual recurring revenue, which includes GPU rentals booked through Voltage Park.\n\nLightning had grown from building a popular open source tool PyTorch Lightning, which helps researchers manage machine learning, to bundling software to help corporates like German chip company Infineon and advertising agency Monks manage building and tweaking large language models.\n\nFalcon had run Lightning AI on cloud giant AWS but last year started to shop around a new class of startups like CoreWeave, Nebius and Voltage Park, which are known as “neoclouds” that had sprung up to meet surging demand for graphic processing units, as he planned to launch a marketplace that would bundle rented AI chips with his AI training software.\n\n“We met and proposed a bold idea to merge together and build a full stack AI cloud,” says Falcon, after he found that many of the neoclouds were better suited to working with scrappy startups, willing to make compromises to get access to cheap chips.\n\nFalcon founded Lightning in 2019 after studying for a doctorate on deep learning at NYU and interning with Yann LeCun at Facebook. Its open source tool has been downloaded more than 400 million times and the company has raised over $100 million from investors like Coatue, Index Ventures, and Bain Capital.\n\nIts new partner was born from a $900 million grant from crypto billionaire Jed McCaleb, who cofounded blockchain startup Ripple and early bitcoin exchange Mt. Gox. McCaleb’s not-for-profit, the Navigation Fund, bought 24,000 of Nvidia’s then top-of-the-line H100 chips and set up Voltage Park to manage them with the goal of lowering the cost of compute for startups, per Reuters reporting.\n\nVoltage Park CEO Ozan Kaya told Forbes that the investment had made the company the third largest neocloud, behind CoreWeave and Nebius, based on chips deployed. The San Francisco-based company now operates six data centers in four states across the United States. “We were evaluating different ways to move up the stack and Lightning was the strongest one for us,” Kaya said.\n\nVoltage Park’s unusual funding was a draw rather than a deterrent for Falcon. Most of its neoclouds rivals (and tech giants like Meta) have loaded up on debt to fund purchases of new datacenters and AI chips. CoreWeave alone has raised over $14 billion from lenders. Voltage Park had built up 60 megawatts of active data center capacity with McCaleb’s foundation as its majority shareholder.\n\n“It was the only neocloud without debt,” Falcon said. “I think the first failure mode is going to be debt, their leverage.\n\nWhile OpenAI, Meta and xAI have been signing deals for data centers with gigawatts of power, Voltage Park’s customers like Cursor, open source AI lab Reflection and AI video generator Higgsfield, typically only needed clusters of AI chips with tens of megawatts of power, said Kaya.\n\nMcCaleb’s foundation, which was formed in November 2023, will now hold a “significant equity stake” in the new merged company, David Coman-Hidy, president of the Navigation Fund told Forbes. Coman-Hidy added that McCaleb occasionally advised Voltage Park but was not on the board, and had no ownership stake in Voltage Park, or Lightning AI. The fund now has grown its donor base and assets to $1.25 billion and has promised to make grants supporting causes that address climate change, the welfare of farm animals, criminal justice reform and “open science,” he said.",
    "readingTime": 4,
    "keywords": [
      "mccaleb’s foundation",
      "voltage park",
      "lightning ai",
      "navigation fund",
      "voltage park’s",
      "chips",
      "manage",
      "startups",
      "neoclouds",
      "debt"
    ],
    "qualityScore": 1,
    "link": "https://www.forbes.com/sites/iainmartin/2026/01/21/ai-startup-merges-with-a-billionaire-backed-data-center-operator-in-25-billion-deal/",
    "thumbnail_url": "https://imageio.forbes.com/specials-images/imageserve/6970a439847e78e21c536412/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds",
    "created_at": "2026-01-21T12:26:58.467Z",
    "topic": "tech"
  },
  {
    "slug": "the-godfather-of-ai-says-hes-very-sad-about-what-his-lifes-work-has-become",
    "title": "The 'Godfather of AI' says he's 'very sad' about what his life's work has become",
    "description": "Geoffrey Hinton said he is concerned that the AI he helped develop poses risks that are not being taken seriously.",
    "fullText": "That's how Geoffrey Hinton, the computer scientist widely known as the \"Godfather of AI,\" describes how he feels about the technology he helped create and what he says is the world's failure to take its growing risks seriously.\n\n\"It makes me very sad that I put my life into developing this stuff and that it's now extremely dangerous and people aren't taking the dangers seriously enough,\" Hinton told BBC Newsnight in an interview released on Tuesday and recorded earlier this month.\n\nHinton, who helped pioneer the neural networks that underpin modern artificial intelligence, has become one of the field's most outspoken critics as AI systems grow more powerful and widespread.\n\nHe has predicted that AI could trigger widespread job losses, fuel social unrest, and eventually outsmart humans — and has said that researchers should focus more on how advanced systems are trained, including ensuring they are designed to protect human interests.\n\nOn BBC Newsnight, Hinton said that humanity is approaching a pivotal moment as researchers edge closer to building machines more intelligent than humans.\n\n\"We've never been in this situation before of being able to produce things more intelligent than ourselves,\" Hinton said, adding that many experts believe that AI will surpass human intelligence within the next 20 years — and in many areas, already has. Once that happens, he said, controlling such systems may become far more difficult than many assume.\n\n\"The idea that you could just turn it off won't work,\" Hinton said, adding that a sufficiently advanced AI could persuade humans not to shut it down.\n\nHinton said the biggest mistake humanity could make now would be failing to invest in research on how humans can coexist with the intelligent systems they created.\n\n\"If we create them so they don't care about us,\" he warned, \"they will probably wipe us out.\"\n\nHe suggested that catastrophic outcomes are not inevitable, saying that the risks depend on how advanced systems are designed and governed and that humans still have \"a lot of options on how to create them\" while AI remains under development.\n\nStill, Hinton also expressed concern that AI is being unleashed at a time when global cooperation is weakening and authoritarian politics are on the rise, making meaningful regulation harder to achieve.\n\nHe compared the need for AI governance to international agreements on chemical and nuclear weapons.\n\nDespite his concerns, Hinton said he would not undo his work on AI.\n\n\"It would have been developed without me,\" he said. \"I don't think I made any decisions that I wouldn't make the same way if I had the same knowledge.\"\n\nHe remains hopeful about AI's potential to improve education and medicine, pointing to AI tutors and advances in medical imaging as examples of its promise. But for now, Hinton said, urgency is paramount.\n\n\"We're at a very crucial point in history when we're going to develop things more intelligent than ourselves fairly soon,\" he said. \"We haven't done the research to figure out if we can peacefully coexist with them. It's crucial we do that research.\"",
    "readingTime": 3,
    "keywords": [
      "advanced systems",
      "humans",
      "intelligent",
      "hinton",
      "create",
      "research",
      "risks",
      "seriously",
      "it's",
      "intelligence"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/godfather-ai-geoffrey-hinton-on-ai-sad-dangerous-2026-1",
    "thumbnail_url": "https://i.insider.com/69709f50e1ba468a96aa669b?width=1200&format=jpeg",
    "created_at": "2026-01-21T12:26:58.437Z",
    "topic": "finance"
  },
  {
    "slug": "elon-musk-says-production-for-cybercab-robotaxi-and-optimus-robot-will-initially-be-agonizingly-slow",
    "title": "Elon Musk says production for Cybercab robotaxi and Optimus robot will initially be 'agonizingly slow'",
    "description": "The Tesla Cybercab robotaxi and Optimus robot are both set to enter production this year as Elon Musk bets Tesla's future on AI and robotics.",
    "fullText": "Elon Musk is bracing for another round of production hell.\n\nThe Tesla CEO said on Tuesday that the ramp-up of the company's Cybercab robotaxi and Optimus robot, both set to enter production this year, would be \"agonizingly slow.\"\n\n\"The speed of the production ramp is inversely proportionate to how many new parts and steps there are,\" wrote Musk in a post on X.\n\n\"For Cybercab and Optimus, almost everything is new, so the early production rate will be agonizingly slow, but eventually end up being insanely fast,\" he added.\n\nTesla is set to begin production of the Cybercab, a sleek two-seater robotaxi that Musk has said will ship without a steering wheel or pedals, in April. The billionaire is targeting an eventual production goal of 2 million units a year.\n\nThe Cybercab, which Musk said in 2024 would cost around $25,000, is set to enter mass production as Tesla races to expand its robotaxi service after a sluggish start.\n\nThe company launched autonomous ride-hailing in Austin last June, but it only has a small number of Model Y robotaxis on the road in the city and has not yet removed human safety monitors from its vehicles.\n\nMeanwhile, Optimus, its humanoid robot designed to help with everyday tasks, is set to enter production by the end of 2026, with Musk saying Tesla could eventually make one million a year.\n\nThe Tesla CEO told investors in October that it would take a while to reach that goal as the company was having to manufacture almost the entire supply chain from scratch, with production moving at the speed of the \"slowest, dumbest, least lucky thing out of 10,000 unique items.\"\n\nMusk's comments echo what Tesla is telling its employees. In an all-hands meeting in October, the company's VP of AI software told staff on Tesla's Autopilot and Optimus teams that 2026 would be the \"hardest year\" of their lives, Business Insider's Grace Kay exclusively reported.\n\nIt wouldn't be the first time Tesla has faced a rough road while scaling an ambitious new product.\n\nThe company endured a famously brutal period of what Musk called \"production hell\" while building its Model 3 EV in 2017. Musk and other employees resorted to sleeping on the factory floor as they struggled to scale the mass-market model.\n\nTesla also faced production challenges with the Cybertruck, the company's last new vehicle.\n\nThe electric pickup's unique design and stainless steel-clad structure made it highly challenging to produce at scale, and Musk acknowledged in 2023 that Tesla had \"dug its own grave\" with the Cybertruck's design.\n\nThe divisive electric truck also serves as a caveat for Musk's ambitious production goals.\n\nMusk's predictions that Tesla could produce 250,000 Cybertrucks a year have fallen short, with industry data showing the company sold just over 20,000 Cybertrucks in the US last year.",
    "readingTime": 3,
    "keywords": [
      "tesla ceo",
      "agonizingly slow",
      "production hell",
      "enter production",
      "the tesla ceo",
      "company's",
      "robotaxi",
      "musk's",
      "musk",
      "robot"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/elon-musk-cybercab-optimus-production-agonizingly-slow-robotaxi-robot-2026-1",
    "thumbnail_url": "https://i.insider.com/6970ab83e1ba468a96aa66df?width=1200&format=jpeg",
    "created_at": "2026-01-21T12:26:58.234Z",
    "topic": "finance"
  },
  {
    "slug": "vibebin-incuslxcbased-platform-for-selfhosting-persistent-sandboxes",
    "title": "Vibebin: Incus/LXC-based platform for self-hosting persistent sandboxes",
    "description": "vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh).  Create...",
    "fullText": "jgbrwn\n\n /\n\n vibebin\n\n Public\n\n vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh). Create and host your vibe-coded apps on a single VPS/server.\n\n License\n\n View license\n\n 9\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jgbrwn/vibebin",
    "readingTime": 1,
    "keywords": [
      "vibebin",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jgbrwn/vibebin",
    "thumbnail_url": "https://opengraph.githubassets.com/b08d7ce148c865871d7a69b11f4c233a6cd39949454bb9455bb3bdcf87a6a575/jgbrwn/vibebin",
    "created_at": "2026-01-21T12:26:57.995Z",
    "topic": "tech"
  },
  {
    "slug": "brand-engagement-network-stock-soars-after-securing-2m-ai-deal-in-africa",
    "title": "Brand Engagement Network stock soars after securing $2M AI deal in Africa",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/brand-engagement-network-stock-soars-after-securing-2m-ai-deal-in-africa-93CH-4457373",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPEA7H0NX_M.jpg",
    "created_at": "2026-01-21T12:26:57.403Z",
    "topic": "finance"
  },
  {
    "slug": "ovi-ai",
    "title": "Ovi AI",
    "description": "Create physics-accurate videos with synchronized audio using Ovi AI by Character.AI. Generate 10-second videos from text or images with Ovi 1.1's twin backbone technology. Free access, no signup required!",
    "fullText": "Ovi AI's revolutionary twin backbone architecture enables simultaneous video and audio generation. This cross-modal fusion technology ensures perfect synchronization between visual content and sound, making Ovi AI unique among AI video generators.\n\nExperience Ovi 1.1's breakthrough capability: generating 10-second videos with perfect temporal consistency. With 100% more training data than the original, Ovi 1.1 delivers coherent, extended video sequences at 960×960 resolution.\n\nOvi AI features a custom 5B parameter audio branch trained specifically for video synchronization. ovi ai can synchronously generate audio\n\nOvi AI demonstrates advanced understanding of real-world physics, creating videos with realistic motion, gravity, and object interactions. The physics engine ensures your generated videos maintain authenticity and visual coherence throughout.\n\nOvi AI supports multiple input modes: text-to-video (T2V), image-to-video (I2V), and combined text+image-to-video (T2I2V). This flexibility allows you to choose the best approach for your creative vision.\n\nCreate videos in various formats including 9:16 (vertical), 16:9 (horizontal), and 1:1 (square). Ovi AI supports different resolutions up to 960×960 at 24 FPS, giving you professional-quality output for any platform.",
    "readingTime": 1,
    "keywords": [
      "ovi ai",
      "videos",
      "audio",
      "ensures",
      "perfect",
      "synchronization",
      "visual",
      "physics",
      "supports"
    ],
    "qualityScore": 0.85,
    "link": "https://ovi-ai.org/",
    "thumbnail_url": "https://ovi-ai.org/opengraph-image.png?ece9c3a334b1dece",
    "created_at": "2026-01-21T12:26:57.322Z",
    "topic": "tech"
  },
  {
    "slug": "pragmatic-notes-on-running-dangerous-ai-coding-agents-in-cloud-vms",
    "title": "Pragmatic Notes on Running Dangerous AI Coding Agents in Cloud VMs",
    "description": "A practical approach to safely running AI coding agents with strong isolation using cloud VMs, Tailscale, and simple notification patterns.",
    "fullText": "Running coding agents with free reign is very powerful for a certain class of tasks, especially ones that require little human supervision, or where you want to close (or disconnect) your laptop, walk away, and come back to results.\n\nRecently there have been several HN discussions about safely running Claude Code or Copilot CLI agents, such as Yolobox – Run AI coding agents with full sudo without nuking home dir and Running Claude Code dangerously. These post detail the potential dangers and show how to run these agents more safely, and while reasonable, I find they lack in a few respects.\n\nIn particular, I want strong isolation, long running agent tasks, minimal cognitive overhead and I really value being able to close my laptop, walk away, and get notified on my phone when things are done. I do not mind paying for a cloud VM.\n\nThere are many valid ways to solve this problem. This post describes mine. It covers running multiple coding agents concurrently in a cloud VM, how I handle access and repos, and how I keep notifications simple.\n\nI generated some Terraform to spin up an Azure VM with a cloud-init.yml for setting up common tools/environments I use. Claude can generate a decent starting point for this quite easily, given your particular environment.\n\nFor secure access, I use Tailscale. Note: I'm not paid by them, but it is easily my favorite piece of infrastructure software!\n\nA cloud-init script installs Tailscale on first boot and automatically joins the VM to my tailnet. SSH access is enabled using Tailscale SSH. Once the VM is up, it appears on my private network with a stable hostname via Magic DNS. No SSH key management, no exposed ports.\n\nor connect using VS Code Remote SSH:\n\nhttps://code.visualstudio.com/docs/remote/ssh\n\nMost of the time I prefer tight, step by step control over code generation, working locally in VS Code with Copilot. For longer running or experimental tasks, I instead let an agent work remotely on a branch inside the VM, and pull the results once I am satisfied.\n\nWhile this is arguably git basics, it works well for me and I found that it is useful sharing how to set up a VM as a remote:\n\nOn the local machine, from the repo directory:\n\nThen you can pull clone and check out the branch, do the work, commit, and push to bare repo:\n\nFinally, locally, you can get the changes:\n\nI use tmux to manage long running sessions. This lets agents keep running after I disconnect, and makes it easy to juggle multiple concurrent sessions. If you are not familiar with tmux, it is worth learning!\n\nFor notifications, I use https://ntfy.sh.\n\nIt is free, extremely simple, and works over plain HTTP POST. I have the iOS app installed, so I can walk away from my laptop and still get notified when work completes. I explicitly instruct my agents to make a POST request once their work is done in the agent instructions.\n\nThat is it. No SDKs, no auth setup required for basic usage. The notification shows up immediately on my phone/browser.\n\nIf there is interest, I can publish a repo with the Terraform, cloud-init scripts, makefile, etc, and the old .devcontainer setup.",
    "readingTime": 3,
    "keywords": [
      "coding agents",
      "tasks",
      "laptop",
      "away",
      "access",
      "repo",
      "free",
      "close",
      "disconnect",
      "safely"
    ],
    "qualityScore": 1,
    "link": "https://jakobs.dev/pragmatic-notes-running-dangerous-ai-agents-cloud-vms/",
    "thumbnail_url": "/media/agents-vm.jpg",
    "created_at": "2026-01-21T12:26:57.044Z",
    "topic": "tech"
  },
  {
    "slug": "upgrade-from-ralph-to-eric-for-a-more-autonomous-ai",
    "title": "Upgrade from Ralph to Eric for a more autonomous AI",
    "description": "Everyone is going crazy about Ralph loop, and they are cool, but I think we need to upgrade to concept to an Eric loop !",
    "fullText": "TLDR; Eric loop is a concept that upgrades Ralph loop by adding more complexity and depth to the idea of managing AI systems, bash version of the article\n\nI wont reintroduce the concept of Ralph loop, if you have been anywhere near AI in the first half of January 2026, you have heard of it ! If not, you should go and watch Theo's video (or someone else's but this is a good !)\n\nYou probably know which comics character Ralph is refering to, but Eric... who could it possibly be ?\n\nIf you thought short, fat and angry, you'd be right ! I'm of course talking about Eric Cartman from South Park !\n\nWhile Ralph is naive and innocent, Eric is calculating and manipulative. Not something I'd look for in a Human, but for managing a bunch of AIs ? HELL YEAH !\n\nAn idea, even if good, always need some work and some back and forth to be implemented. So we first expose the idea to the AI to get an requirement document (PRD) and we work on it !\n\nHow ? you'll need to read first ! and straight away update parts you don't agree with or things you need to be more thorough !\n\nOnce that is done, ask AI models to ask You questions about the PRD and give all those, the PRD and your replies to another AI for update\n\nRince, Repeat, until you are satisfied with the document.\n\nYou can even get multiple model go at it to then compare and merge the outputs, using AI, of course :D\n\nThis is where we start diverging from the basic Ralph loop. instead of just passing that to the AI and hoping tasks will be implemented properly we get an AI to pre split and formalise the tasks and sub tasks into a list !\n\nThe rift grows further as we split the task implementation in several steps with:\n\nOnce all that is done ? the Eric loop goes on for the next phase of the plan, just like Ralph !\n\nYes models have gotten smarter, but if you played around with sub agents and skills, you know that prompting is not dead (yet...)\n\nHaving a neat separation like that helps you have better prompts for each phase of the task execution.\n\nAs a side effect, it mmeans you can control which model is used for which phase maybe you dont need Opus the whole way if it made a banger plan !\n\nWe are going to create a small project from an idea I have, you can do it to if you like !\n\nIt's called Tiny-till, a small app to have a simple till for itinerant merchants.\n\nBut we are going to need a tool to help us ...\n\nYep, I already kind of made said tool :D ! task-o-matic\n\nI'll write more about it in a future post ! but you already have plenty of content to go through on the site itself.\n\nI made sure to generate a lot of docs and a couple tutorials so you can already give it a try if you'd like !\n\nBut in a nutshell, it helps you create PRDs, refine them , split into tasks and subtasks and finally wiring all that to your favorite AI harness !\n\nFor the AI to operate properly, we need a specific stack, that way, we limit hallucination !\n\nTask-o-matic uses better-t-stack under the hood to bootstrap the stack so let's do that !\n\nBoom, a monorepo for Tiny-till ! Batteries included : tailwind, shadcn, build script and all :D\n\nWhy init init ? you might have missed the whole using AI to code whole projects, sometimes... it is not aesthetically pleasing, and you should learn not to care to much (is my take on that... or lazyness...)\n\nYou will need a .env file to configure the AI stack.\n\nNow, we tell the AI what we want to do, but why would we limit ourselves to 1 ?\n\nAm I using free endpoints for \"real\" work ? Yes, yes I am ! Does it distort my views on how much AI work should cost ? ... Why would it ? No, no, you just being a killjoy right now !\n\nA few minutes later... I got myself this (cat .task-o-matic/prd/prd-master.md)[/assets/blackhole/from-ralph-to-eric/prd-master.md] it doesn't really need more questions, but let's see what Claude has to say, shall we ?\n\nWell, let's answer those... some are pertinant ! I guess that's why we bring out the big guns !\n\nQuestions are asked interactivelly ;)\n\nWell, this little stunt just costed me a nice 15 cts, more or les 10x more than the first PRD generation...\n\nI don't know what is the most amazing... that\n\nI haven't decided yet...anywooooooo, time to move on and not get siderated !\n\nWhen i said seconds, i lied, I spent 5 minutes editing the file. to fix lib versions, marked install and config work as done, and, finally, set expectation for the design.\n\nthe last thing we have to do, is ask an AI to split into main tasks.\n\nonce again, i wont only do 1, but this time, I'll ask Claude to be the final task creator, from others input !\n\nAnd there goes another 8.2 cents burnt for the token Gods, but in return, we get a detailed breakdown of the tasks required to complete the project !\n\nWe still have to split those tasks though... To make them more palatable for the current crop of AI models !\n\nYou should really consider reviewing the tasks in detail before spliting, not vibe planning like a muppet ! (guess what I did ^^)\n\nI think Claude is not necessary here, but I had me $3 of openrouter credit i have to burn before the end of the month soooooo... big guns it is !\n\nBurnt a tenth of that ! but now... we are actually ready ! Almost...\n\nOr CLAUDE, or GEMINI, depends on your harness of choice, For me, it is opencode i'am going to run the /init prompt in opencode !\n\nit is not mandatory, but it is a good way to prevent dumb mistake and set some proper behavior regarding a few things, for me, i'll add the following\n\nI might add more as the agent works if i see reccuring errors and dumb things happening\n\nWell, guys, stash your parents away cause we are going to let Eric go at it !\n\nBecause I am El CheapoDev DelBrokeCasa, i am going to use my GLM coding plan to code, that way, i wont break the bank and it is OK enough for something like that i think !\n\nIf you'd rather use claude code, use the --tool claude option. (or codex/gemini/kilo) but opencode is nicer as it stream the content out (in part at least) so you see what is happening !\n\nAnd now, you wait, coffee, snacks, more coffee, diner and breakfast probably, it gonna take a while ^^\n\nHalf a day later, Eric has successfully completed the task. yaaay \\o/\n\nnpm run dev, browser to http://localhost:3001 aaaaand, it does not work.... XD\n\nI have a very fancy error page telling me about some kind of \"exceeded depth error blablabla\"... Zustand... i bloody fucking HATE Zustand !\n\nI could have gone in, open the project, read some of the 35970 lines of... 35970 LINES OF CODE ??? Naaah, dude, ain't no way i'm even thinking about that !\n\nOf course, i asked GLM to fix this obviously terrible usage of Zustand, and I even have to spend another few minutes screaming about some redirection to the / issue... I still felt like a caveman though !\n\nI mean, having to open a tool ? To do ... work ?? Like it's 2022 or something !\n\nI haven't been testing anything yet, so i'm pretty sure there'll be a few more issues, but the dev server runs, i can navigate the app without error (even in console) so it is not bad at all !\n\nI might do a follow up later on, to update you on what i had to do to get it a bit closer to the original idea (if needed ^^)\n\nSo that's the end (of this article) ! What do you think ? did you try a similar technique ? What's the plan next, cause...\n\nIf you are interested, the repo can be found on github. Is the code great ?\n\nAs for task-o-matic, the repo is on github so feel free to poke around ;)\n\nWhat about the \"Eric Loop\", how can you do that yourself ? Well, the beautiful thing with AI is that, you can actually ask it to write code ! So a quick\n\nthis being the result, your own Eric Loop, ain't life beautiful ?\n\nHope you enjoyed, I'll see you around",
    "readingTime": 8,
    "keywords": [
      "ralph loop",
      "eric loop",
      "tasks",
      "idea",
      "split",
      "task",
      "plan",
      "tool",
      "i'll",
      "you'd"
    ],
    "qualityScore": 1,
    "link": "https://dbuild.dev/blog/black-hole-from-ralph-to-eric/",
    "thumbnail_url": "https://dbuild.dev/images/blog/blackhole/from_ralph_to_eric/Eric_saw_Ralph.webp",
    "created_at": "2026-01-21T06:22:10.592Z",
    "topic": "tech"
  },
  {
    "slug": "understanding-modern-ai-is-understanding-embeddings-a-guide-for-nonprogrammers",
    "title": "Understanding Modern AI Is Understanding Embeddings: A Guide for Non-Programmers",
    "description": "Embeddings are a core AI concept that underpin a great deal of what we today think of as being AI. This article is going to give you an accurate and intuitive understanding of what an “embedding” is in less time than it takes to eat a (very large) bagel.",
    "fullText": "Embeddings are a core AI concept that underpin a great deal of what we today think of as being AI. This article is going to give you an accurate and intuitive understanding of what an “embedding” is in less time than it takes to eat a (very large) bagel, and possibly make you think they’re as cool as I think they are. It even explains how we got to embeddings as a solution, by looking at everything else we tried along the way. If you’re comfortable with even very simple Excel formulas, you’ll understand all the maths, and there’s even a cute graph with dogs on it.\n\nLet’s start by thinking about dogs, and by classifying them by their attributes:\n\nFor each dog, we’ve defined a vector where the first column is size, and the second column is intelligence. A vector is a fancy name for a row of numbers. And in two dimensions, we can plot this really easily:\n\nLet’s do some vector maths using Manhattan distance. A Great Dane ([5,5]) is 5 units away from a Beagle ([2,3]):\n\n(we’re using Manhattan distance here because it’s simple, but you might well use Euclidean distance instead)\n\nA vector can be both a specific point in our dog-embedding space, but it can also be a direction:\n\nWhat’s like a Bulldog ([3, 2]) but smaller ([-1, 0]) and smarter ([0, 1])?\n\nKey point: if we start to plot things by their attributes, and we give each attribute a number, we can use simple maths to find items that are close together (clustering) and we can easily find items that differ from a starting point in a certain way.\n\nAND WHAT’S MORE: this holds up over as many dimensions as we want to define, not just two:\n\nWe can do the same distance calculations again, even with more vectors:\n\ntldr; an embedding is a vector of numbers that describes something, and that you can use to cluster things by similarity\n\n(vector databases, like Pinecone or pgvector, are simply databases that are fast at calculating the distance between vectors, and finding vectors that are close to other vectors)\n\nDogs are easy mode, because there’s a limited number of breeds and attributes. Also, coming up with attributes ourselves is hard work, and laziness is an inarguable virtue, so let’s see if we can make the computer do the work for us here.\n\nIf we wanted to classify books instead, we’d need to do something different. Let’s start by defining a table of every book, and every word in the English language, with the values being the occurrences of each word in the book:\n\nTable 1: Number of word occurrences in each book\n\nOur vocabulary of words becomes our dimensions (or columns) and each book becomes a vector in those dimensions (rows). This is called a bag of words, a “bag” being the technical term for a mathematical object that counts the occurrences of things (but discards the order they’re in).\n\nBag of words was state of the art in approximately the 1950s for classification. There are lots of problems with this approach, and understanding those problems and how they’re solved starts to bring us closer to understanding embeddings!\n\nOne problem with our Bag of Words is that we end up with values that are proportional to the size of books, which might mean that two very similar books are quite far apart, just because they’re of very different lengths. If George RR Martin rewrote Animal Farm, it might end up very far away from the Orwell original simply due to his overly-lax editorial staff.\n\nThere’s a particularly elegant solution to this in terms of determining how close two items are by measuring the difference in angles from the origin towards them (cosine similarity), BUT there’s also the highly effective and really simple solution of just dividing the count of each word by the length of the book (normalization):\n\nTable 2: Distribution of word occurrences in each book\n\nwhich gets us most of the way there without fancy maths or words like cosine.\n\nWe’ve also got a problem here with our Bag of Words by a bunch of noise being introduced by words that are frequent but unspeakably dull, like “a”, “the”, and “of”. Different authors may lean more heavily on some than others (meaning their books are further apart in the dimensions of “of” and “a”), but that gives us absolutely no useful information about the meaning or content of what they’re writing about: what we care about is occurrences of words like dragon and android and lascivious.\n\nWe could maintain a list of dull words (often called “stop words”), and strip them out, but that’s fiddly, insufficiently lazy, and doesn’t account for the fact that what we consider to be a “stop word” might differ depending on the context.\n\nInstead, we can use some simple statistics to look at how often a word (or “term”) appears in a given book/document compared to how frequently it appears across all of the books or documents we’re looking at; this is called “TF-IDF”, and it was state-of-the-art circa 1972:\n\n(you can see the actual formula and learn more about TF-IDF here)\n\nKEY POINT: We’re still very firmly in a “words are dimensions, and books exist as points in that high-dimensional space” place here. We’re still representing books by a vector of numbers and looking for similarity by searching how close these books are to each other in a high-dimensional space. We’re just using a slightly fancier way of determining what those numbers should be.\n\nOur approach has gotten us some of the way towards classifying books well, but we start to hit some insurmountable problems in this approach:\n\nIf we have a dimension for every word in English, we have waaay too many dimensions. One for each word, and each variation of each word. At least 50,000 if we’re just covering common English words. This starts to be cumbersome and computationally expensive to do maths on.\n\nFantasy authors who insist on inventing spellings for “Sir”; there’s quite a lot of semantic information about a book embedded in having nobles with fancy titles prancing around, but our current approach gets confused when authors decide to start calling their characters “Ser” or “Syr” or “Sür” to try and sound more mysterious. More generally, we treat words with similar meanings but different spellings or forms as completely different dimensions\n\nBarking up the wrong tree: sometimes two different words are spelled identically (eg “bark”), and we don’t especially want to say that books on forestry and similar to books on puppies\n\nWord order is important! If your main character is a “dog who bites a bone” that’s likely quite a very different story from your “bone who bites a dog” character, even if the bone is haunted in both cases\n\ntldr; vectors that are simply word frequency counts are a good start, but have a whole host of problems: we need something even smarter (embeddings) to do really good clustering and classification\n\nWe have until this point been thinking about how we would classify a whole book, and using word counts as dimensions. But what if we wanted to classify just one word instead?\n\nIf we were able to come up with vectors for individual words, we could start to address the problem that “dog”, “puppy”, and “pooch” are all treated as completely different things when classifying text, and we’d also get some other useful features, like it would make it easier for us to build things like fuzzy-search for users: “pooch pics” and “dog photos” should return much the same results when a user is searching for them.\n\nWe could hand-pick some dimensions to use for this:\n\nThis … might work pretty well with enough dimensions! But hand-choosing each of the dimensions we’d need and then manually classifying (and tweaking) 50,000 words feels like it might be a lot of hard work. Hard work that the computer should be doing for us… But how do we get the computer to do that for us? The computer would need to know already what the words meant in order to do the classification, so we need a way to bootstrap that process.\n\nOne intriguing idea might be to suck in a dictionary, and to use our bag-of-words techniques on the definition of each word to come up with a vector for each word. This however gives us our own bootstrapping problem: if we look at the dictionary definition for “puppy”, it’s very different for the one for “dog”: instead, it points us to the one for “dog” instead, and solving all of this starts to sound like complicated maths will be needed.\n\nWhat if we instead tried to “know a word by the company it keeps”? It’s an idea that’s been floating around linguistics for decades, and suggests that words appearing in similar contexts probably have similar meanings. If you constantly see the words “dog” and “pooch” surrounded by words like “walk”, “leash”, “bark”, and “treats”, maybe you don’t need a dictionary to figure out they’re related?\n\nLet’s game out how this could work: you could take every piece of written text you could lay your hands on, and go through it word by word, looking at the two words ahead of it and the two words behind it. For every word, you could start to build out the probability that any other word was in those four surrounding words.\n\nFigure 1: a sliding context window, associating each word in a sentence with the two words before and after it\n\nYou could use those probabilities as the values in your vector for each word, with every other word being a dimension still, and voila, you have per-word vectors that put “pupper”, “dog”, and “pooch” very close to each other.\n\nThis is almost the massive breakthrough that researchers at Google had in 2013 inventing “Word2Vec” (word to vector), but there’s one further, crucial refinement.\n\nOur method above has two big problems. The first is one we’ve already identified: you still end up with an absolutely massive number of dimensions! These are unwieldy, difficult to work with, and computationally expensive to work with. But secondly, there’s just not enough reading text out there to make this work: there are infinite possible and sensible combinations of words, and while us humans have written down a lot of stuff, it turns out there’s simply not enough training data to make the above approach practical, IN ADDITION TO having all the drawbacks that come with these huge, unwieldy vectors.\n\nSo, when in doubt, make the computer do the work. And the tool that proved exceptionally useful for this is the mighty Neural Network.\n\nA deep-dive into how they work is out of scope, even for this increasingly long article, but the key things to know are:\n\nThey take vectors as their inputs and spit out vectors as their outputs\n\nThey’re specialized tools for learning things through giving them examples\n\nThey’re really good at prediction tasks\n\nAnd so we can flip our earlier method on its head:\n\nRather than building a huge matrix of every word and its neighbours, how about we train a Neural Network to predict what a word is, based on some neighbours?\n\nThe process for this sounds like magic:\n\nWe choose a number of dimensions we want to use, let’s say 300\n\nWe literally just make up a 300-number-long vector for every input word: it’s literally just random to begin with\n\nWe show the Neural Network billions of sets of four words, and ask it to guess the middle word\n\nWe make tiny adjustments to each of those initial random vectors based on how well the Neural Network did at guessing it (using some complicated maths called “back-propagation”)\n\nAnd over the billions and billions of times we do this, the computer eventually comes up with its own vectors for every word: vectors that really are good enough to predict what a word is going to be given its neighbours.\n\nThese same “guessing” vectors encode the meaning of words in a truly jaw-droppingly effective way. Again, it’s the principle that ‘a word is known by the company it keeps.’ If the network gets good at predicting surrounding words, the vectors it develops for each word must encode information about that word’s typical context, which relates closely to its meaning.\n\nFirst of all, it puts related words close together. “Pupper”, “pooch”, and “doggo” now all sit really close together in our dimensional space. “Sir/Ser/Syr” basically sit on top of each other. We can’t really figure out what each of these dimensions means individually: there’s not a dedicated dimension for “dog like”, but they seem to work.\n\nSecondly, remember how a vector can be either a point or a direction? Well it turns out if we start out at the vector for “king”, we subtract the vector for “man”, and add the vector for “woman”, we end up with a vector that’s almost identical to the one for “queen”. Complex semantic meaning has been encoded into our dimensions and embeddings. This shows the model learned abstract relationships (like gender and royalty) from the text data alone, encoding them into the vectors.\n\nThe above is the essence of “Word2Vec”.\n\nDo we know why this works so well? NO\n\nIs this some bad-ass alien technology shit? YES\n\nIsn’t this a cause for alarm given how much we’re starting to rely on this technology and its successors? Please take your negativity out of the AI acceleration seminar\n\nThese dense vectors that capture semantic information along dimensions that the computer literally just made up are “embeddings”. These word embeddings can then be combined (using various methods, from simple averaging to more complex techniques) to create embeddings for sentences or even whole paragraphs: you have a long list of numbers that encodes the meaning of that sentence or that paragraph.\n\nThese dense vectors help solve our earlier problems: the fixed, lower dimensionality (e.g., 300) is manageable; words with similar meanings learned from context (like ‘Sir’ and ‘Ser’) end up close together; and the context-based learning helps differentiate meanings of words like ‘bark’ depending on surrounding words (though context handling gets even better later).\n\nThese paragraph-level embeddings are what underpin most RAG systems; in practice, people use the more clever versions of embeddings that the rest of the article covers, but still. An embedding is a row of numbers that describe the meaning of some words, and then we can use simple linear algebra to cluster related things together, or move around inside the vector space using abstract paths such as “positive sentiment” or “more feminine”\n\nYou can at this point leave the article, with a pretty good idea of what embeddings are. You won’t have learned how they deal with misspellings, words that are written the same and are completely different, or word ordering, BUT, you will know that it’s possible to encode semantic meaning into vectors of a few hundred numbers. Rather than leaving the article, you can also skip ahead to Part 3, which explains how embeddings are actually used in AI.\n\nWith Word2vec, we end up with vectors that capture semantic meaning of words, and the vectors exist in a dimensional space that the computer learned by playing a guessing game over and over again. That the computer learned these dimensions by itself, and each dimension has a subtle and dense meaning means that we call those vectors “embeddings”.\n\nWe’ve also covered that you can average out these embedding to get a meaning for a sentence or a paragraph, but because this averaging process is treating those vectors as a “bag” (an unordered count of items), we lose a huge amount of meaning: the vectors for “man eats shark” and “shark eats man” are identical, even though the actual meanings differ substantially.\n\nSo rather than averaging our embeddings together, we need an operation that takes into account the sequence of the embeddings.\n\nOne of the first successful ways of doing this was with “Recurrent Neural Networks” (RNNs) – “recurrent” in this case means a neural network that accepts its last output as part of its next input (the state “recurs”). (there’s a little white lie here about the order in which these events happened that we’ll address later)\n\nHere we start out with a base vector, and we imprint each vector in the sequence on it, one at a time. After we’ve stamped each vector representing a word onto the base vector, the base vector ends up changed in a way that reflects the sequence.\n\nHere’s some Play-Doh as an example. The Play-Doh square is our base vector, and each stamp we apply over the top of the last one is the vector for a word. You’ll note that the order matters, and also that the most recent word vector ends up playing a larger role in the overall pattern we end up with than the previous ones.\n\nFigure 2: Pressing the same shapes into Play-Doh in different orders results in subtly different patterns, with the last shape often over-represented, and a faded initial shape\n\nWith Play-Doh, the more shapes you overlay on top of each other, the more you lose the previous shapes. If you add too many, you’ll end up with a complete mess you can’t make any sense of. This happens with actual Recurrent Neural Networks (RNNs) too! Despite this, it represented a significant improvement over just averaging together vectors.\n\nSome clever mechanisms were invented to try and prevent earlier vectors getting completely wiped out by later ones, with fancy names like LSTMs and GRUs, but both represented incremental improvements over RNNs rather than a huge leap forward.\n\nHistorical note: the above describes events as if the sequence of events was: Word2Vec -> RNNs -> LSTMs and GRUs, which isn’t accurate! RNNs as a concept and LSTMs as a refinement of them actually predate Word2vec. So, what were they using as their input vectors before we got the rich “embeddings” that Word2vec gives us? They were either using the huge “one dimension per word” vectors we discussed near the beginning of the article, or were feeding in one character at a time with each character being a dimension. Using neural networks that can understand sequences rather than bags has all sorts of uses outside of text classification, but using the newly invented Word2vec “embeddings” with these techniques was a massive leap forward.\n\nSo at the end of this section, we have improved on our Word2Vec embeddings by building embeddings that combine other embeddings in the right sequence, which solves our problem of “man eats shark” / “shark eats man”. However, we have serious problems with losing information over sequences of any real length: by the time we get to the end of “the quick brown fox jumps over the lazy sheep dog”, we’ve added so much extra information that “quick” and “brown” have kind of been smushed together, and we’re very much more “dog” focused than we are “fox” focused because it occured later in the sentence.\n\nFigure 3: After each addition of a vector onto our hidden state, previous vectors get “blurrier” and more recent vectors dominate\n\nRNNs gave us much stronger embeddings than we had before, but ultimately these days we derive embeddings from the same technology that powers Large Language Models: a technique called attention and the system that uses it, the transformer. A proper deep-dive into the steps that take us from RNNs to transformers is too big for this already unwieldy article, but in short we train the computer to pay more attention to some words in a sentence than to others.\n\nThat’s probably how us humans do it too: your brain naturally ‘pays attention’ to the most important words (like the subject and verb, or key nouns) and how they relate, even if they are far apart. The ‘attention mechanism’ in AI tries to mimic this.\n\nWhen processing a sequence of words, with attention we keep track of all the intermediate embeddings we’d generated rather than mashing them all together into one single vector. We add an extra neural network over that where we train further vectors to correlate how important each word is to every other word in the sequence. This whole system – including the new neural network – is trained the same way as we trained RNNs: by playing “guess the word” and getting the computer to update its internal parameters when it gets something right or wrong, until it’s usually getting them right!\n\nTransformers take this attention mechanism even further, and are the technology that powers large language models. Using those to generate our embeddings, we get embeddings that can accurately describe even pretty long pieces of text well, and are free of almost all of the issues we’ve identified so far in this article. Commercial vendors like OpenAI offer very effective (and cost-effective!) general-purpose APIs for turning large chunks of text into embeddings for use in your applications.\n\nFirst of all, rather than using whole words as inputs, LLMs use tokens. These are fragments of words that are chosen (by the computer) during training to be a representative sample of commonly-appearing sequences. By using bits of words instead of whole words, we gain the ability to deal with words we haven’t seen before, which includes misspellings of words. For example, it might understand ‘embeddingtastic’ by recognizing the known tokens ‘embedding’ and ‘tastic’.\n\nFigure 4: the OpenAI tokenizer in action. Common words are their own tokens, but less common words are split into fragments to help deal with the vast range of possible words including misspellings.\n\nSecondly, chances are the majority of time you spend using LLMs you’ll be using “autoregressive” ones: they generate the next token for you, token by token. There are however important LLMs that are trained to guess the missing token inside a block of text, that will use the tokens both before and after the target token to try and figure it out: guess the word, but the word in the middle of the sentence instead of the next word.\n\nOK, so we’ve learned what embeddings are, and they’re pretty darn cool, but why do we care? When are you, the busy young lady or young man, actually going to interact with an embedding? We hand-waved over it a little before, but the embeddings created using the same mechanism as LLMs (transformers) can take in very long pieces of text and give back exceptionally sophisticated vectors that capture complex meanings from that text. These aren’t just theoretical toys; they’re the mechanism behind a surprising amount of what we call AI today.\n\n(we’ve also pretended that embeddings are just for working with text, which also isn’t true, but we’re already at over 5,000 words here…)\n\nRemember classifying dogs? We picked the features: size, intelligence, fluffiness. Easy for dogs, but hopeless for text. Trying to hand-pick features for classifying emails or news articles (“Does it mention ‘invoice’?” “How many times does ‘politics’ appear?”) is fragile and labor-intensive. Embeddings flip this entirely. Instead of us telling the computer what features matter, the computer learns the important features itself when creating the embedding, using the guessing game we outlined above. These aren’t just keyword counts; they’re learned features capturing the underlying topic, the vibe, the subtle nuances.\n\nThink about classifying customer reviews as ‘Positive’ or ‘Negative’. A simple keyword approach gets tripped up by sarcasm, varied phrasing, or subtleties in word ordering. Embeddings, however, capture the overall essence. They turn the review into coordinates in a high-dimensional ‘meaning space’. A happy review’s embedding might land near coordinates representing ‘joy’ and ‘satisfaction’ (dimensions the model learned!), while an angry one lands near ‘frustration’. The magic is that embeddings handle nuance: synonyms like “happy,” “pleased,” and “elated” end up in the same positive neighborhood.\n\nSo, you feed this rich embedding – these coordinates packed with learned features and context – into a classifier model. The classifier’s job becomes much simpler: it just learns to draw boundaries in that meaning space. “Everything landing in this region is Positive, everything over there is Negative.” The embedding does the heavy lifting of understanding the text; the classifier just reads the map. It’s spookily effective.\n\nSometimes you don’t know in advance what categories you want to put things into. You’ve got a mountain of customer reviews, research papers, or social media posts, and you just want to know “what are the main themes here?” This is where clustering comes in, and embeddings super-charges it.\n\nBecause embeddings place texts with similar meanings close together in that high-dimensional space we keep talking about, we can use algorithms to automatically find groups, or “clusters,” of related items. Think back to our dogs: if we plotted thousands of individual dogs based only on their learned embeddings (without knowing breeds), clustering algorithms could find the clumps corresponding to Poodles or Beagles just by seeing which dogs’ vectors are close together.\n\nIt’s the same for text: these algorithms scan the positions (embeddings) of your documents in meaning-space and identify these natural groupings. Suddenly, you can see that your customer reviews naturally fall into clusters like “Complaints about Shipping”, “Praise for Product Quality”, and “Confused People Asking How To Turn It On”. No predefined categories needed, the structure just emerges from the meaning captured in the embeddings.\n\nThis is the technique du jour for making LLMs smarter and more factual, and it leans heavily on embeddings. The core idea is surprisingly elegant: when you ask an LLM a question, you first use the embedding of your question to find relevant pieces of information (like paragraphs from a document) from a database.\n\nHow? Because of a slightly magical property: the embedding for a question is often geometrically close to the embedding of the text containing its answer within that vector space. It’s like the question “Where is the Eiffel Tower?” generates a vector that points towards the same region of meaning-space as the text “The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.”\n\nSo, in a RAG system, you take the user’s query, generate the embedding for it, and then use a vector database (a fast database optimized for finding close vectors) to retrieve the chunks of text from your provided documents whose embeddings are closest. You then stuff these relevant chunks from your knowledge base into the prompt you give the LLM along with the original question. Voilà! The LLM now has the specific information needed to answer accurately, rather than just relying on its (potentially outdated or hallucinated) general knowledge. It’s like giving the LLM an open-book exam, using embeddings to find the right page in the textbook.\n\nOkay, slight caveat here: the final output embeddings we’ve discussed (like the ones you get from OpenAI’s API) aren’t exactly what LLMs use second-by-second to generate text. But the underlying principle is deeply connected.\n\nWhen an LLM is writing text, predicting the next token (word fragment), it maintains an internal “state.” This state is essentially a complex, evolving vector representation that captures the meaning and context of everything generated so far. Think of it as a temporary, super-sophisticated embedding of the preceding sequence. This internal state vector is what the LLM uses to figure out the probabilities for what the very next token should be.\n\nSo while you don’t directly use a final document embedding to generate text token-by-token, the same transformer architecture and attention mechanisms that create those powerful final embeddings are also at work inside the LLM, constantly creating and updating internal context vectors (which behave a lot like embeddings) to drive the generation process. It’s all part of the same family of vector-based meaning representation.\n\nSo, there you have it! Embeddings! Aren’t they cool?! We started with simple lists of numbers describing dogs, wrestled with the messy realities of counting words in books, and journeyed through decades of AI research, from Bag of Words to TF-IDF, Word2Vec, RNNs, and finally the mighty Transformers.\n\nWhat we ended up with are these dense, powerful vectors – embeddings – that somehow manage to capture the meaning of text in a way that computers can work with mathematically, while just literally being lists of numbers. They’re not just lists of word counts; they’re rich representations learned by machines playing incredibly complex guessing games, encoding semantic relationships, context, and nuance along dimensions we didn’t even know we needed.\n\nAre they slightly mysterious? Yes. Do we understand every nuance of why they work so well? Not entirely. But are they incredibly useful? Absolutely. From classifying spam, to finding themes in data, to making LLMs smarter with RAG, embeddings are a fundamental building block of modern AI. They might just be weirdly effective rows of numbers, but they’re our weirdly effective rows of numbers, and they’re changing how we interact with information. Thanks, embeddings.",
    "readingTime": 25,
    "keywords": [
      "manhattan distance",
      "neural networks",
      "networks rnns",
      "llms smarter",
      "recurrent neural",
      "computationally expensive",
      "leap forward",
      "customer reviews",
      "language models",
      "word2vec rnns"
    ],
    "qualityScore": 1,
    "link": "https://sgnt.ai/p/embeddings-explainer/",
    "thumbnail_url": "https://sgnt.ai/embedding.png",
    "created_at": "2026-01-21T06:22:10.517Z",
    "topic": "tech"
  },
  {
    "slug": "amthropic-ceo-claims-we-are-1yr-away-where-ai-can-do-everything-swes",
    "title": "Amthropic CEO claims we are 1yr away where AI can do everything SWEs",
    "description": "Anthropic CEO, Dario...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/i/status/2013682941201678804",
    "thumbnail_url": "https://pbs.twimg.com/amplify_video_thumb/2013655214058803214/img/cWFFbOwDhlLnZqXY.jpg:large",
    "created_at": "2026-01-21T06:22:09.483Z",
    "topic": "tech"
  },
  {
    "slug": "hyundai-motor-stock-hits-record-high-as-ai-robotics-optimism-powers-rally",
    "title": "Hyundai Motor stock hits record high as AI, robotics optimism powers rally",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/hyundai-motor-stock-hits-record-high-as-ai-robotics-optimism-powers-rally-4456771",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPED3P07K_M.jpg",
    "created_at": "2026-01-21T06:22:08.548Z",
    "topic": "finance"
  },
  {
    "slug": "billionaire-marc-benioff-challenges-the-ai-sector-whats-more-important-to-us-growth-or-our-kids",
    "title": "Billionaire Marc Benioff challenges the AI sector: ‘What’s more important to us, growth or our kids?’",
    "description": "The Salesforce founder, long a critic of Section 230, doubled down in Davos. He wants the original sin of the internet to be scrubbed away.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/01/20/billionaire-marc-benioff-section-230-ai-whats-more-important-growth-or-kids/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/GettyImages-2256697072-e1768949931240.jpg?resize=1200,600",
    "created_at": "2026-01-21T00:59:29.180Z",
    "topic": "business"
  },
  {
    "slug": "the-gloves-are-off-in-the-feud-between-sam-altman-and-elon-musk",
    "title": "The gloves are off in the feud between Sam Altman and Elon Musk",
    "description": "The tech titans escalated their long-running feud on Tuesday, trading barbs in public posts about the safety of ChatGPT, Grok, and Tesla's Autopilot.",
    "fullText": "Sam Altman and Elon Musk are at it again, with each of the tech titans taking aim at the other in a series of heated posts on X.\n\nMusk appeared to start the latest escalation early on Tuesday morning, when he posted \"Don't let your loved ones use ChatGPT\" in response to a post that said that use of OpenAI's chatbot had been linked to the deaths of nine children and adults since it was released in 2022.\n\nAltman fired back, first in defense of ChatGPT and OpenAI's desire to protect its users, and then blasting Tesla's Autopilot technology, calling it unsafe.\n\n\"It is genuinely hard; we need to protect vulnerable users, while also making sure our guardrails still allow all of our users to benefit from our tools,\" Altman said.\n\nAltman continued, calling out Autopilot.\n\n\"I only ever rode in a car using it once, some time ago, but my first thought was that it was far from a safe thing for Tesla to have released,\" he wrote. \"I won't even start on some of the Grok decisions.\"\n\nAltman added: \"You take 'every accusation is a confession' so far.\"\n\nThere have been at least eight wrongful-death lawsuits filed against OpenAI that allege use of ChatGPT has contributed to worsening mental health conditions, leading to instances of suicide and murder, including among children and young adults.\n\nSafety concerns around Tesla's self-driving technology have also been central to multiple wrongful-death lawsuits, including one surrounding a 2019 crash in Florida that left a 22-year-old woman dead. A jury determined Tesla was 33% liable for the crash and awarded the plaintiffs $329 million in total damages, Business Insider previously reported.\n\nRepresentatives for Musk and Altman did not immediately respond to requests for comment from Business Insider.\n\nThe social media feud comes as the pair is stuck in the middle of a long-running legal battle over OpenAI's status as a nonprofit company. Musk sued Altman, and other leaders of OpenAI, alleging that they misled him when they decided to pursue a for-profit structure, moving the company away from its original nonprofit mission.\n\nMusk said he donated $38 million to OpenAI when it was originally founded as a nonprofit.",
    "readingTime": 2,
    "keywords": [
      "wrongful-death lawsuits",
      "users",
      "nonprofit",
      "altman",
      "children",
      "adults",
      "released",
      "protect",
      "technology",
      "crash"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/sam-altman-elon-musk-feud-escalates-autopilot-deaths-chatgpt-safety-2026-1",
    "thumbnail_url": "https://i.insider.com/69700640d3c7faef0ecc9b0a?width=1200&format=jpeg",
    "created_at": "2026-01-21T00:59:28.375Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-getting-on-the-ai-age-verification-bandwagon",
    "title": "ChatGPT Is Getting on the AI Age Verification Bandwagon",
    "description": "The app will guess your age and set limits for users it thinks are under 18.",
    "fullText": "When OpenAI first announced GPT-5.2 last month, it quietly disclosed a new safety feature it called \"age prediction.\" Considering ChatGPT proper isn't exactly an \"all ages\" kind of tool, it makes sense that users under the age of 18 should have protections in place to shield them from harmful content. The company says that users who indicate they're under 18 already receive an altered experience to \"reduce exposure to sensitive or potentially harmful content,\" but if the user doesn't voluntarily share how old they are with OpenAI, how does the company enforce these protections? Here's where age prediction comes in.\n\nOn Tuesday, OpenAI officially announced its new age prediction policy, which, like other age verification systems being used by the likes of Roblox, uses AI to guess how old a user is. If the system decides that a particular user is under the age of 18, OpenAI will adjust the experience accordingly, with the goal of keeping all interactions age-appropriate.\n\nHere's how it works: The new age prediction model looks at both the user's behaviors within the app, as well as the general account data. That includes things like how old the account is, what times of day the user is accessing ChatGPT, usage patterns, as well as, of course, the age the user says they are. Looking at all this data, the model determines how old the user likely is. If the model thinks they're over 18, they'll get the full experience; if the model thinks they're under 18, they'll get the \"safer experience.\" If the model isn't confident, it defaults to that safer experience.\n\nThat limited experience means that someone the model thinks is under 18 will try to reduce the following content types:\n\nViral challenges that might inspire \"risky or harmful behaviors\"\n\nRole play that is sexual, romantic, or violent in nature\n\nContent promoting \"extreme\" beauty standards, unhealthy dieting, or body shaming\n\nThe company says that its approach is informed by \"expert input\" as well as literature discussing child development science. (It's not clear whether how much of that input is from direct interviews and coordination with experts, and how much, if any, is from independent research.) The company also acknowledges \"known teen differences in risk perception, impulse control, peer influence, and emotional regulation\" when compared to adults.\n\nThe biggest risk with any of these age prediction models is that they'll sometimes get it wrong—hallucination is an unfortunate habit AI models all share. That goes both ways: You don't want someone too young accessing inappropriate content in ChatGPT, but you also don't want someone older than 18 getting stuck with a limited account for no reason. If you experience the latter situation, OpenAI has a solution for you: direct age verification through Persona. This is the same third-party Roblox uses for its age verification, which hasn't gone very well thus far.\n\nThat doesn't necessarily spell doom for OpenAI. Roblox tried overhauling their age verification system for a massive user base all used to a certain type of multiplayer experience, which led to users not being able to chat with other users in newly-assigned age categories, which were often incorrect. Meanwhile, ChatGPT's age prediction is only controlling the experience of one user at a time. To that end, OpenAI will let you upload a selfie as an added verification step if the prediction model alone isn't enough. Interestingly, OpenAI doesn't say anything about the option to upload an ID for verification, which other companies, like Google, have provided.\n\nI'm not necessarily a fan of age prediction models, as I think they often sacrifice user privacy in the name of creating age-appropriate experiences. But there's little doubt that OpenAI has to do something to limit the full ChatGPT experience for younger users. Many of ChatGPT's users are under 18, and much of the content they experience is wildly inappropriate, whether it be instructions on getting high, or advice on writing suicide notes. In some tragic cases, minors have taken their own lives after discussions with ChatGPT, leading to lawsuits against OpenAI.\n\nI don't have any great answers here. We'll just have to see how this new age prediction model affects the user experience for minors and adults alike, and whether it actually manages to create a safer experience for younger, more impressionable users.",
    "readingTime": 4,
    "keywords": [
      "harmful content",
      "safer experience",
      "prediction models",
      "age prediction",
      "age verification",
      "prediction model",
      "user",
      "users",
      "openai",
      "isn't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/openai-chatgpt-age-prediction-model?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KFEH2Q4KPPTQXMTQW5G99Y1B/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-01-21T00:59:27.310Z",
    "topic": "tech"
  },
  {
    "slug": "sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima",
    "title": "Sandbox Your AI Dev Tools: A Practical Guide for VMs and Lima",
    "description": "AI coding assistants and other devtools can steal your credentials and data. Here's how to run them safely in isolated VMs using Lima on macOS/Linux.",
    "fullText": "AI coding assistants, npm, pip, and other development tools can run arbitrary code and scripts on your machine, potentially stealing SSH keys, API tokens, wallet keys, sensitive credentials and other private data without you noticing.\n\nThis guide shows you how to sandbox these tools in isolated VMs using Lima, so you can experiment and develop freely without putting your sensitive data at risk.\nJump straight to the guide, or read on for a bit of personal context.\n\nI’ve been having quite a bit of fun with AI assisted coding recently.\n\nI use LLMs for a wide range of things, including discussing architecture, design choices, learning about new tools and libraries I wasn’t previously aware of, to reviewing PRs and quickly cranking out dirty prototypes.\n\nEspecially for hobby projects that are not meant to ever go into production, I enjoy playing with AI tools fast and loose, producing results quickly and not getting slowed down by annoying things such as reading code before running it 🤣.\n\nAnd yeah… that’s obviously unsafe, unless it’s all contained in a sandbox!\n\nYou should never run potentially dangerous, experimental code on your main machine, since it could steal your passwords, API keys, environment variables, private keys, access to your communication tools, install services, and do all sorts of other nefarious things.\n\nNowadays I isolate all my devtools in VMs, and thought it might be useful to others if I put together a guide to shows how to do it. Well, here it is, and I hope it’ll be useful to you, too!\n\nYou’ll want to run the entire development environment, including the AI tool itself, inside a sandbox. This way it’s safe to install dependencies and to execute code, and unlocks other fun features like snapshots before running sketchy code, and reverting if something goes wrong.\n\nAnd it’s not just AI-generated code. Node.js/npm/yarn and Python/pip are particularly troublesome because they allow any package to run arbitrary scripts on your system during installation, and install tons of additional dependencies that can do the same. This attack vector is called “supply chain attack” and it happens all the time.\n\nVirtual Machines (VMs) and Containers (i.e. Docker, Podman, containerd) are the two most practical methods for isolating development tools from your host operating system. VMs provide much stronger protection and more flexibility overall, and are better suited for co-developing with AIs.\n\nContainer runtimes share the host operating system’s kernel, which means they’re fundamentally running on the same system as your main machine, just with isolated namespaces and resource limits. This creates several security concerns:\n\nIn contrast, a VM runs its own complete operating system with its own kernel. The hypervisor (like QEMU/KVM) creates a much stronger isolation boundary. Even if malicious code completely compromises the VM, it would need to exploit the hypervisor itself to reach your host, a significantly harder target.\n\nFurthermore, a VM enables better concurrency. It can run Docker containers, databases, web servers, multiple build processes, and background services all at once, and the AI tool can interact with everything naturally just like on a normal development machine.\n\nIn this guide, we use Lima VM to sandbox AI and devtools. Lima is a delightful, lightweight virtual machine manager for Linux and macOS which provides easy and quick ways to create and manage VMs.\n\nYou interact with Lima through the limactl command:\n\nVMs are based on templates, which can include (build on) other templates:\n\nThe Lima VM docs have platform-specific installation guides.\n\nHomebrew is recommended on macOS:\n\nOn Linux install the binary like this:\n\nNow ensure your Lima version is up-to-date:\n\nWe only want to share very specific host directories with the VM.\n\nLet’s create ~/VM-Shared on the host, which we later mount into the VM at ~/Shared (with write access):\n\nYou can use that directory to easily copy files between the host and the VM, and to share project directories from the host with the VM.\n\nDefaults for all VMs can be defined in ~/.lima/_config/default.yaml.\n\nLet’s create the default YAML file:\n\nLima conveniently creates default SSH configuration files for all VM instances, which makes it easy to log in with SSH (including using VS Code for a Remote-SSH session).\n\nI recommend using a ~/.ssh/config.d/ directory on the host and have SSH include all configs there by default. That allows us to simply link the Lima-created config files there to use them.\n\nAdd this as first line in your ~/.ssh/config file, to make SSH include all configs from there:\n\nGreat! After creating a new VM, we can now simply create a symlink to the Lima-generated SSH configs and use it to SSH into the instance.\n\nLet’s start an Ubuntu 25.10 VM instance, named dev.\nWe use the internal _images/ubuntu-25.10.yaml template because it doesn’t include the automatic home directory sharing:\n\nYou can share additional project-specific directories between host and VM in several ways:\n\nCreate a symlink for the SSH config file and SSH into the VM:\n\nLet’s update the services on the instance, and configure git:\n\nLet’s confirm that port forwarding works. We do this using a one-liner Python HTTP server (on port 7777) inside the VM, and accessing it from the host:\n\nThis section guides you through installing several other languages and development tools, including Golang, Node.js, Python, Rust, Docker.\n\nWe can accomplish that either by installing each tool according to it’s documentation, or by using a version manager such as mise (“mise-en-place”, 22k stars on Github) which can install hundreds of tools via a simple command-line interface.\n\nFirst, we install mise (“mise-en-place”, 22k stars on Github) and make bash support it:\n\nYou use mise latest <tool> to see the latest versions it knows about:\n\nNow you can install all the tools you want in a single command:\n\nTo manually install (or update) Golang in the VM, download the latest release and extract into /usr/local/go:\n\nThe Golang path needs to be in the PATH environment variable, which we have already added before.\n\nA good way to install a current version of Node.js in Ubuntu is by using nvm, a modern node version manager (90k stars on GitHub):\n\nNow it’s all installed and ready to use! Check the versions like this:\n\nPerhaps you don’t even need Docker, since Lima includes containerd and nerdctl by default. This is a Docker-compatible runtime and command-line interface that can also run images from Docker Hub:\n\nIf you do want to install Docker, the quickest way to install it by using their official get-docker.sh script:\n\nFor the group changes to take effect, exit the shell and re-login (may need a VM restart).\n\nVerify that user is in the ‘docker’ group:\n\nGitHub CLI provides a useful gh cli command that let’s you easily interact with GitHub and private repositories.\n\nYou can install it in the VM following the Linux installation instructions:\n\nWarning: Authorizing GitHub CLI to access private repositories will leave an API key in the VM which could potentially be stolen by unauthorized scripts (which is what we wanted to avoid in first place by running everything in a VM).\n\nOnly authorize it with gh auth login for private repo access if you accept the risks! I personally avoid having any sensitive credentials in the VM, in particular those that allow access to private GitHub repositories.\n\nIf you prefer an IDE like VS Code, you can use Remote-SSH to start a session inside the instance.\n\nPlease note that this is potentially unsafe, as explained in the Remote-SSH README:\n\nSecurity Note\nUsing Remote-SSH opens a connection between your local machine and the remote. Only use Remote-SSH to connect to secure remote machines that you trust and that are owned by a party whom you trust. A compromised remote could use the VS Code Remote connection to execute code on your local machine.\n\nSee also this discussion on GitHub for more context and information.\n\nNow a new VS Code window opens, and sets up VS Code Server:\n\nThen you can click “Open” and choose a folder, like Shared:\n\nBefore setting up the tools, let’s create a “Hello World” directory in the Shared folder as our playground:\n\nLet’s start with installing Claude Code in the VM, following the instructions in the documentation:\n\nOn first start, Claude asks you to authorize it.\n\nThe docs mention support for an ANTHROPIC_API_KEY environment variable (i.e. set in .bashrc), but that did not work when I tried it; claude CLI didn’t let me skip the login process. Only after the login was done it notified me about the existing environment variable, and whether I’d prefer to use that one.\n\nAfter the login, Claude Code CLI is ready to be ued in the VM! 🎉\n\nSince Claude is running in a VM, it might be permissible to run it in “dangerously skip permissions mode”, which makes it bypass all permission checks:\n\nYou could also create an alias for it and add it to your .bashrc:\n\nAnthropic provides documentation for using Claude in VS Code, and also offer a VS Code Claude extension.\n\nYou can install the Claude extension in the VM through the Remote-SSH session window:\n\nIn contrast to the CLI tool, the authentication flow did not work through the user interface, and I had to set the ANTHROPIC_API_KEY environment variable:\n\nReload the VS Code window (open command palette with Shift + CMD + P and choose “Developer: Reload Window”):\n\nNow the VS Code Claude extension should work:\n\nIf you want to enable “dangerously skip permissions mode” in the VS Code extension, you can enable it via your user settings. Open the settings (CMD + ,), search for “claude” and enable “Claude Code: Allow Dangerously Skip Permissions”:\n\nLet’s install Gemini CLI from Google next.\n\nThe documentation recommends installing it with npm, the Node.js package manager. You’ll need to install Node and npm first, see also the Node.js setup instructions.\n\nIt will ask you to authenticate:\n\nI chose “Login with Google”. Note that the authentication flow may require a retry if the first attempt times fails.\n\nAfter authorization is done, Gemini CLI works!\n\nYou can run Gemini in YOLO mode:\n\nAutomatically accept all actions (aka YOLO mode, see https://www.youtube.com/watch?v=xvFZjo5PgG0 \n\nThe alias you could define in .bashrc:\n\nCodex CLI is the AI dev tool from OpenAI/ChatGPT.\n\nIt will ask you to sign in, either via ChatGPT or by providing an API key:\n\nAfter that is done, Codex CLI is ready to work for you!\n\nYou can also run Codex in dangerous mode:\n\nSkip all confirmation prompts and execute commands without sandboxing. EXTREMELY DANGEROUS. Intended solely for running in environments that are externally sandboxed\n\nThere are several other great tools worth a mention:\n\nDrop your favorite tools in the comments below!\n\nVM clones and snapshots allow you even more flexibility and isolation. You can use them to quickly and cheaply run new VMs for experiments and specific projects based on already provisioned instances. Use them frequently!\n\nLima offers several ways to take VM snapshots and/or clone VMs.\n\nYou can make a copy of an existing VM instance with limactl clone. The existing instance needs to be stopped first.\n\nAfter all the initial VM setup is done, clone it and use it both as backup as well as a base for future instances:\n\nRemember that after starting a new instance, you probably want to symlink the VM SSH configuration to your ~/.ssh/config.d/ directory, so ssh knows about it (See also “SSH into the VM”):\n\nFor maximum security and flexibility, consider using multiple VMs for different purposes and trust levels. This approach provides better isolation and lets you tailor each environment to specific needs.\n\nHere are some suggested VM configurations:\n\nYou can quickly clone your base VM setup to create new instances for different projects using limactl clone, as described in the VM cloning section above.\n\nFor sensitive or production projects, consider dedicating a separate VM to each project. This prevents potential cross-contamination between projects and allows you to mount only the specific project directories you need.\n\nWhen creating project-specific VMs, you can customize the mounted directories by editing the instance configuration. Either adjust the mounts section before starting the VM (by not using the -y flag), or edit ~/.lima/<vm_name>/lima.yaml after creation and restart the instance.\n\nThis approach also makes it easier to share VM configurations with team members. Instead of sharing entire disk images, you can distribute just the Lima template YAML file, which team members can use to spin up identical environments on their machines.\n\nFor automated setup, Lima supports provisioning scripts that run during VM creation. For more complex setups, consider using idempotent provisioning tools like Ansible to ensure consistent environments across your team.\n\nIf you find yourself repeatedly creating VMs with similar configurations, consider creating custom Lima templates. Templates are YAML files that define VM settings, and they can include other templates.\n\nCustom templates are useful for:\n\nYou can create a custom template by copying and modifying an existing one from Lima’s template directory. Save your custom templates in ~/.lima/_templates/ and reference them when creating new VMs:\n\nSee the Lima templates documentation \n\nHere are some important security best practices to follow when using VMs for development:\n\nRemember: The whole point of using VMs is isolation. When in doubt, create a new VM for risky experiments and delete it afterwards.\n\nI hope this guide helps you get started quickly and right-footed!\nAs always, please leave feedback, questions and ideas in the comments below.\n\nSpecial thanks to Ilya Lukyanov and Overflo for reviewing drafts of this post and making great suggestions. 🙏",
    "readingTime": 12,
    "keywords": [
      "yolo mode",
      "yaml file",
      "anthropic_api_key environment",
      "remote-ssh session",
      "claude extension",
      "ssh configuration",
      "authentication flow",
      "mise mise-en-place",
      "mise-en-place stars",
      "command-line interface"
    ],
    "qualityScore": 1,
    "link": "https://www.metachris.dev/2025/11/sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima/",
    "thumbnail_url": "https://www.metachris.dev/images/posts/ai-sandbox/cover.jpg",
    "created_at": "2026-01-21T00:59:26.211Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-chief-satya-nadella-warns-ai-boom-could-falter-without-wider-adoption",
    "title": "Microsoft chief Satya Nadella warns AI boom could falter without wider adoption",
    "description": "Big tech boss tells delegates at Davos that broader global use is essential if technology is to deliver lasting growth",
    "fullText": "Save now on essential digital access to trusted FT journalism on any device. Savings based on monthly annualised price.\n\nThen undefined per month. Complete digital access with exclusive insights and industry deep dives on any device. Cancel anytime during your trial.\n\nComplete digital access with exclusive insights and industry deep dives on any device.\n\nFT Digital Edition: all the content of the FT newspaper on any device.\n\nCheck whether you already have access via your university or organisation.\n\nDiscover all the plans currently available in your country\n\nDigital access for organisations. Includes exclusive features and content.\n\nSee why over a million readers pay to read the Financial Times.",
    "readingTime": 1,
    "keywords": [
      "industry deep",
      "deep dives",
      "exclusive insights",
      "digital access",
      "device",
      "content"
    ],
    "qualityScore": 0.75,
    "link": "https://www.ft.com/content/2a29cbc9-7183-4f68-a1d2-bc88189672e6",
    "thumbnail_url": "https://images.ft.com/v3/image/raw/https%3A%2F%2Fd1e00ek4ebabms.cloudfront.net%2Fproduction%2F6112a4c3-9a6b-4d82-ab3c-28876be57ff9.jpg?source=next-barrier-page",
    "created_at": "2026-01-21T00:59:25.321Z",
    "topic": "tech"
  },
  {
    "slug": "codex-cli-skill-to-add-subagents-like-in-claude-code",
    "title": "Codex CLI skill to add subagents like in Claude Code",
    "description": "OpenAI Codex CLI skill to support running subagents - iipanda/codex-subagent-skill",
    "fullText": "iipanda\n\n /\n\n codex-subagent-skill\n\n Public\n\n OpenAI Codex CLI skill to support running subagents\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n iipanda/codex-subagent-skill",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/iipanda/codex-subagent-skill",
    "thumbnail_url": "https://opengraph.githubassets.com/d2d5e6f6ccf74c90db9748f02904c4f6424b660afe911ac775056cfc11a42540/iipanda/codex-subagent-skill",
    "created_at": "2026-01-20T18:21:58.367Z",
    "topic": "tech"
  },
  {
    "slug": "blackrock-ceo-says-capitalism-isnt-spreading-the-wealth-and-ai-might-not-either",
    "title": "BlackRock CEO says capitalism isn't spreading the wealth — and AI might not either",
    "description": "In his opening remarks at the World Economic Forum, Larry Fink, the CEO of BlackRock, warned that AI could heighten wealth inequality.",
    "fullText": "Larry Fink, the CEO of BlackRock, the world's largest asset management firm, kicked off the World Economic Forum on Tuesday with a critique of capitalism.\n\nMore wealth has been created since the fall of the Berlin Wall than at any other time in human history, but it has not translated into shared prosperity, said Fink, who was appointed as interim co-chair of the World Economic Forum in August 2025, replacing founder Klaus Schwab.\n\n\"In advanced economies, that wealth has accrued to a far narrower share of people than any healthy society can ultimately sustain,\" he said.\n\nFink warned that the pattern of unequal wealth distribution could repeat in the era of AI.\n\n\"Early gains are flowing to the owners of models, owners of data, and owners of infrastructure,\" said Fink.\n\n\"The open question, what happens to everyone else? If AI does to white collar workers what globalisation did to blue collar workers, we need to confront that today, directly.\"\n\nHe urged those gathered at the annual meeting in Davos to rethink how prosperity is defined and to create a \"credible plan\" for broad participation in the gains AI can deliver.\n\n\"This is going to be the test. Capitalism can evolve to turn more people into owners of growth instead of spectators watching it happen,\" said Fink.\n\nThe world's wealthiest 10% own roughly 75% of global wealth, while the poorest half hold only about 2%, according to the World Inequality Report 2026, released in December 2025 and based on data compiled by 200 researchers.\n\nFink acknowledged in his speech that the World Economic Forum has lost trust and \"feels out of step with the moment.\"\n\n\"Davos is an elite gathering trying to shape a world that belongs to everyone,\" Fink said, adding that the forum should be more transparent and precise about what economic success means, especially with those who don't feel represented at gatherings like Davos.\n\n\"Prosperity just isn't the growth in the aggregate. It's not just GDP. It can't be measured by GDP or the market caps of companies. It has to be judged by many people who see it, who can touch it, can feel it, and can build their own future on it,\" said Fink.\n\nFink has previously spoken about how the pandemic fueled a rethink of the US economy.\n\nIn his 2022 annual letter to shareholders, Fink said the growth of stakeholder capitalism — the idea that companies should prioritize interests beyond shareholders — was a natural evolution of how capitalism can best work to build a strong economy.\n\n\"It is through effective stakeholder capitalism that capital is efficiently allocated, companies achieve durable profitability, and value is created and sustained over the long-term,\" he said.\n\nBlackRock, which manages some $14 trillion in assets, has been one of the loudest voices in support of environmental, social, and corporate governance (ESG) investing, with Fink often highlighting climate change and sustainability in his annual letters.",
    "readingTime": 3,
    "keywords": [
      "economic forum",
      "collar workers",
      "stakeholder capitalism",
      "world economic forum",
      "fink the",
      "wealth",
      "owners",
      "annual",
      "growth",
      "blackrock"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/larry-fink-blackrock-ceo-davos-critiques-capitalism-ai-wealth-inequality-2026-1",
    "thumbnail_url": "https://i.insider.com/696f56a2e1ba468a96aa52bb?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:54.769Z",
    "topic": "finance"
  },
  {
    "slug": "the-career-advice-jensen-huang-gave-me",
    "title": "The career advice Jensen Huang gave me",
    "description": "Sid Pardeshi, a former Nvidia engineer-turned founder, said Jensen Huang inspired him to launch an AI company, and taught him about leadership.",
    "fullText": "This as-told-to essay is based on a conversation with Sid Pardeshi, 32, the co-founder and CTO of a generative AI software platform. He is from India and lives in Cambridge, Massachusetts. The following has been edited for length and clarity.\n\nI thought I'd stay at Nvidia forever.\n\nThe pay, learning opportunities, and benefits made it a delight to work there. I had no reason to leave.\n\nBut Nvidia's CEO, Jensen Huang, inspired me, and I realized I wanted to be a founder, too.\n\nIn 2022, I made the extremely hard decision to leave Nvidia and pursue my entrepreneurial dreams. I resigned months before ChatGPT was released, and I believe that choice cost me double-digit millions worth of unvested company stock.\n\nI co-founded the AI company Blitzy in 2023, which has raised $4.4 million. Working under Huang for over six years taught me valuable lessons that have been instrumental to my success.\n\nI got an internship with Nvidia in India through my university, where I studied electrical and electronics engineering.\n\nWhen I started in January 2016, Nvidia wasn't a FAANG company, the acronym for the most prestigious Big Tech firms of the time: Facebook, Apple, Amazon, Netflix, and Google. But as a gamer, I was familiar with the company's graphics chips, so I was deeply passionate about working there.\n\nAt the end of my six-month internship, I started a full-time position as a junior engineer at Nvidia in Pune. By 2018, I was a senior system software engineer.\n\nI didn't interact with Jensen much, but he would lead quarterly all-hands meetings, which I would attend remotely from India. He spoke about things where he was ahead of his time, like AI and virtual worlds. I was fascinated by him because I shared his love of technology.\n\nOnce, when Jensen visited staff in India, I asked him after an all-hands meeting what to focus on if I wanted to make a meaningful contribution to Nvidia and the tech space at large.\n\nHe suggested choosing one area of expertise and committing to it deeply, rather than spreading myself out across many interests. I took his guidance to heart and went deep into AI as it related to gaming.\n\nAs time passed, my ambitions evolved. I saw a lot of opportunities emerging in AI, and I wanted to become a founder like Jensen.\n\nIn 2022, at age 28, I quit my job and moved to Massachusetts with my wife and children to join an MSc and MBA program at Harvard.\n\nWhile studying, I worked for Nvidia as an intern from May 2023 to April 2024. They offered me a job that could start after I completed my master's, but starting a business was my primary goal.\n\nIn 2023, a classmate at Harvard and I founded Blitzy, which I work on full-time as the CTO. We're working on using AI to significantly automate software development work for enterprises.\n\nAt Nvidia, I noticed that Jensen nurtured and valued talent. When I was an intern in the US, he made time to meet with us. His assistant kept telling him that a very important client was calling him during the meeting. \"Let them wait,\" Jensen said. \"I'm talking to the future of Nvidia.\"\n\nLike Jensen, I want to keep the spark alive among employees, regardless of their rank. Every day, I allocate two hours to listening to team members and helping them solve problems.\n\nJensen would also follow his convictions. For example, he bet on the accelerated computing platform CUDA in the 2000s, when no one saw its true potential. Now, it's the backbone of AI training processes.\n\nSimilarly, when we first pitched to investors that we intended to use AI to build software autonomously, many didn't believe AI was advanced enough yet. But because I've been reading papers in the AI space every week for years, I trusted my convictions and that we could automate coding at Blitzy.\n\nAnother page I'm taking from his book is never ruling out the prospect of going out of business. Jensen was always concerned about the survival of Nvidia, despite it being a trillion-dollar company. I remember once a fellow intern in the US asked Jensen whether he watched movies. He said that even in the theater, he's thinking about Nvidia.\n\nAs Blitzy's CTO, I want to anticipate every challenge ahead of time. I focus on solving important technical problems and delegate everything else. I want to let people with more knowledge about other aspects of the company handle issues in their space.\n\nSimilarly, it's thought that Jensen's signature outfit, the black pants, shirt, and leather jacket, helps him minimize decision fatigue. I do wear different outfits every day, but my wife, who loves fashion, picks them out for me.\n\nAt one point, I wondered if I was being selfish by leaving Nvidia. I had a family, and needed to make the right choice, both for my career and for them.\n\nI moved to the US on a student visa, which meant my wife was my dependent and not allowed to work. She put her career on the line.\n\nYet, my wife, and my mom, encouraged me to pursue my dreams, and it would be too late now if I hadn't done it then. Once ChatGPT came out, everyone was thinking about bringing AI solutions to the market. I'm glad I already had deep knowledge and experience with AI and was ready to take advantage of the moment.\n\nFinancially, quitting Nvidia wasn't a smart decision because, based on my calculations around my compensation package, my unvested stock would be worth double-digit millions because of how the company's value has grown.\n\nI knew AI technology would progress significantly over the next few years, although I couldn't have anticipated ChatGPT coming out so fast in November 2022 — just months after I resigned. I was leaving right on the cusp of a big moment in the industry, before I was able to harvest the returns.\n\nBut I'm glad I quit when I did. If my family has what they need, working on cutting-edge stuff is more important to me than money.\n\nI got to watch as Nvidia grew from a relatively insignificant company to a trillion-dollar business, and I'm applying everything I learned to Blitzy. My mission is for Blitzy to be on the same trajectory as Nvidia, one day becoming a trillion-dollar company.\n\nA spokesperson for Nvidia declined to comment.",
    "readingTime": 6,
    "keywords": [
      "i'm glad",
      "double-digit millions",
      "nvidia wasn't",
      "software",
      "wife",
      "jensen",
      "decision",
      "space",
      "intern",
      "business"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/nvidia-employee-jensen-huang-leadership-lessons-2026-1",
    "thumbnail_url": "https://i.insider.com/6969672564858d02d218749d?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:54.659Z",
    "topic": "finance"
  },
  {
    "slug": "deepmind-and-anthropic-ceos-ai-is-already-coming-for-junior-roles-at-our-companies",
    "title": "DeepMind and Anthropic CEOs: AI is already coming for junior roles at our companies",
    "description": "Anthropic CEO Dario Amodei said his company is thinking about how to deal with job elimination \"in a sensible way.\"",
    "fullText": "AI might not be causing a labor market bloodbath, but leaders at Google DeepMind and Anthropic say they're starting to see its impact on junior roles inside their own companies.\n\n\"I think we're going to see this year the beginnings of maybe it impacting the junior level,\" said Google DeepMind CEO Demis Hassabis during a joint interview with Anthropic CEO Dario Amodei at Davos on Tuesday.\n\n\"I think there is some evidence, I can feel that ourselves, maybe like a slowdown in hiring in that,\" he said, highlighting entry-level roles and internships as vulnerable examples.\n\nAmodei appeared to agree. The Anthropic boss said last year he believed AI could wipe out half of all entry-level white-collar jobs and push unemployment as high as 20%.\n\nAs of Tuesday, his prediction had not changed, he said.\n\n\"Now I think maybe we're starting to see just the little beginnings of it, in software and coding,\" he said. \"I can see it within Anthropic, where I can look forward to a time where on the more junior end and then on the more intermediate end we actually need less and not more people.\"\n\nHe added: \"And we're thinking about how to deal with that within Anthropic in a sensible way.\"\n\nAmodei and Hassabis have both warned that AI's potential impact on the economy and labor markets could demand institutional change, including through international organizations governing AI and economic intervention, to mitigate the most disastrous outcomes.\n\n\"My worry is as this exponential keeps compounding, and I don't think it's going to take that long — again, somewhere between a year and five years — it will overwhelm our ability to adapt,\" said Amodei.",
    "readingTime": 2,
    "keywords": [
      "google deepmind",
      "within anthropic",
      "junior",
      "we're",
      "labor",
      "impact",
      "roles",
      "beginnings",
      "entry-level",
      "amodei"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/google-deepmind-anthropic-ceos-ai-junior-roles-hiring-davos-2026-1",
    "thumbnail_url": "https://i.insider.com/696f8656c58df2ecd5ccc927?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:54.656Z",
    "topic": "finance"
  },
  {
    "slug": "veteran-investor-jeremy-grantham-says-ai-is-obviously-a-bubble-and-it-could-tank-the-stock-market-when-it-bursts",
    "title": "Veteran investor Jeremy Grantham says AI is 'obviously a bubble' — and it could tank the stock market when it bursts",
    "description": "Veteran investor Jeremy Grantham compared the AI boom to the railroad and internet bubbles, and said the chances it won't burst are \"slim to none.\"",
    "fullText": "Jeremy Grantham says AI is one of the biggest bubbles in history — and it's likely to tank the stock market when it bursts.\n\n\"I think it's obviously a bubble, and I think it's quite a simple story,\" the veteran investor and GMO cofounder said during the latest episode of the \"Merryn Talks Money\" podcast.\n\nGrantham compared AI to the railroad and the internet, two world-changing inventions marked by early investment bubbles which, when they popped, \"brought the economy to its knees for a year or two\" and \"everybody lost their money.\"\n\nContrary to common belief, \"bubbles don't occur when there's some crummy idea that gets touted,\" Grantham said. \"All the bubbles are associated with serious things, and the more serious, the bigger the bubble.\"\n\nGrantham emphasized the \"iron law\" that when an asset doubles in price, the return from holding it halves from that point forward.\n\n\"If you want to have the highest market in history, you will have the lowest returns in history going forward,\" he said. \"And it will happen this time. And my guess is after a while, sooner or later, the market will become a whole lot cheaper.\"\n\nGrantham, who had predicted an epic market crash four years ago, said the release of OpenAI's ChatGPT in late 2023 generated immense buzz around AI and sparked major capital outlays by companies, ultimately staving off an economic slump.\n\nThe market historian said that Big Tech companies have used their dominant market positions and influence to boost their profit margins to \"excessive levels,\" but that won't stop the bubble he sees from bursting.\n\nThe \"probabilities that AI will not bust are slim to none,\" he said. \"It meets every condition of the railroads and the Internet. It's a powerful idea that's attracted everybody's money.\"\n\n\"And so my guess is Nvidia will lead it down, and all the others will follow for a while, and then out of the ashes several of them will once again inherit the world,\" he said.\n\nGrantham also flagged institutional inertia and herd mentality as factors preventing the market from coming to its senses.\n\nPut differently, industry professionals aren't sounding the alarm on valuations out of fear that the bubble will keep growing and they'll underperform their rivals, hurt their reputations, and lose clients or even their jobs.\n\n\"They all look around nervously at each other, but they keep going,\" he said. \"As long as the music's playing, they're going to be dancing. Doesn't matter that they know the market is silly.\"\n\nGrantham is one of several high-profile investors, including Michael Burry of \"The Big Short\" fame, who have warned AI stocks are overvalued and predicted a market downturn is coming.\n\nYet the US stock market has continued to march higher in spite of the skeptics, with the benchmark S&P 500 rising around 80% over the past five years.\n\n\"Shark Tank\" investor Kevin O'Leary and tech investor Ross Gerber dismissed comparisons between the AI boom and dot-com bubble in interviews with Business Insider last fall.\n\nO'Leary said \"you actually can see the productivity and measure it on a dollar-by-dollar basis,\" while Gerber said today's valuations are warranted as AI companies have enormous growth potential and their profitability is \"just insane.\"",
    "readingTime": 3,
    "keywords": [
      "stock market",
      "bubble",
      "bubbles",
      "history",
      "investor",
      "grantham",
      "idea",
      "serious",
      "forward",
      "guess"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/jeremy-grantham-ai-bubble-nvidia-tech-stocks-stock-market-crash-2026-1",
    "thumbnail_url": "https://i.insider.com/672389b201ea6d83dee5543a?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:54.406Z",
    "topic": "finance"
  },
  {
    "slug": "uber-ceo-on-the-most-promising-way-to-succeed-with-ai-throw-out-the-old-policies",
    "title": "Uber CEO on the most promising way to succeed with AI: Throw out the old policies",
    "description": "Speaking at the World Economic Forum in Davos, Uber's Dara Khosrowshahi said some companies are just \"saying the right words\" when it comes to AI.",
    "fullText": "How do you separate the pretenders from the real deal when it comes to business AI adoption?\n\nFor Uber CEO Dara Khosrowshahi, it comes down to building AI into processes from the ground up.\n\nSpeaking at the World Economic Forum in Davos, Dara Khosrowshahi said some companies are \"saying the right words\" and \"play-acting their way into a pretend transformation.\"\n\nFor example, having an AI agent complete tasks like summarizing a client pitch is the \"easy stuff\" that won't differentiate companies, he said.\n\n\"Truly changing how you work with AI, we've found it to be much harder than it sounds,\" he added.\n\nHe gave the example of using AI to improve customer service at Uber. The company had some success by having AI follow its old policies, but had a real \"breakthrough\" after its developers rebuilt everything from scratch.\n\nThat involved giving an AI agent clear underlying goals, Khosrowshahi said, such as making the customer feel good after the interaction.\n\n\"Allowing the AI actually to reason through that and throwing away all of the old policies is turning out to be the most promising way forward,\" he said.\n\nKhosrowshahi added that companies are essentially a \"bunch of policies,\" and that to get the full potential of AI, they would need to \"break down those rules and start over.\"\n\nInternally, Uber developers are using AI tools like Anysphere's Cursor and Anthropic's Claude, he added.\n\nMany companies have been ramping up their AI spending in the hope of unlocking productivity gains. A recent poll of IT professionals by RBC Capital found that 90% of respondents plan to spend more on AI this year.\n\nHowever, there are also fears that AI isn't always living up to the hype, and companies are grappling with concerns that it's eroding workers' skills.\n\nKhosrowshahi acknowledged that getting AI adoption right at a company isn't always plain sailing.\n\n\"You have to survive through a bunch of car crashes internally to do so,\" he said.",
    "readingTime": 2,
    "keywords": [
      "dara khosrowshahi",
      "policies",
      "adoption",
      "agent",
      "customer",
      "developers",
      "bunch",
      "isn't",
      "uber",
      "internally"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/uber-ceo-ai-adoption-productivity-break-rules-dara-khosrowshahi-davos-2026-1",
    "thumbnail_url": "https://i.insider.com/696fa2cba645d11881879fa3?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:54.258Z",
    "topic": "finance"
  },
  {
    "slug": "steven-bartlett-says-using-ai-in-this-way-is-the-most-important-thing-hes-done-for-his-business",
    "title": "Steven Bartlett says using AI in this way is the most important thing he's done for his business",
    "description": "\"The Diary of a CEO\" host said at the World Economic Forum in Davos that using AI to translate podcasts into Spanish broadened its reach.",
    "fullText": "Steven Bartlett said one use of AI has mattered more to his business than anything else: translating his podcast into other languages.\n\n\"There's nothing more important than what we've done for our business than translations. Period,\" Bartlett said during the \"What it Takes to Build\" panel at the World Economic Forum at Davos on Tuesday.\n\nThe British entrepreneur and host of \"The Diary of a CEO\" podcast was in conversation with Jessica Lessin, the CEO of The Information, alongside Bret Taylor, formerly the co-CEO of Salesforce and CTO of Meta.\n\nBartlett said using AI tools to translate the podcasts was initially an \"expensive experiment.\" The problem he was trying to solve, he said, was reaching the untapped market of non-English speakers.\n\n\"If we're just in English, we're reaching like 10% of the world,\" Bartlett added.\n\nThere were technical challenges, too. Some languages use longer words, Bartlett said, meaning three-hour English-language conversations could become significantly longer when translated, risking the audio and video falling out of sync.\n\nTwo years ago, Bartlett announced on LinkedIn that \"The Diary of a CEO\" had hired a full-time data scientist who helped the company achieve a technological breakthrough, enabling them to use AI to translate the podcast into \"EVERY language.\"\n\nAt Davos and in the LinkedIn post, Bartlett did not elaborate on exactly how the AI translations work. Today, however, the videos show Bartlett speaking Spanish in a voice similar to his own, a shift he said had significantly expanded the podcast's reach.\n\n\"This month, 28% of my audience is Spanish,\" he said, adding that Spanish speakers now have access to interviews with some of the world's most high-profile podcast guests, citing his own interview with former first lady Michelle Obama as an example.\n\nOther guests on the podcast include former vice President Kamala Harris, Simon Cowell, and the psychologist Jordan Peterson.\n\nSpotify said in its \"2025 Wrapped\" press release in December 2025 that \"The Diary of a CEO with Steven Bartlett\" was the platform's second-most-listened-to podcast globally, behind \"The Joe Rogan Experience.\" On YouTube, Bartlett's podcast channel has 14.5 million subscribers.",
    "readingTime": 2,
    "keywords": [
      "steven bartlett",
      "the diary of ceo",
      "podcast",
      "spanish",
      "business",
      "languages",
      "translations",
      "translate",
      "speakers",
      "we're"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/steven-bartlett-diary-of-a-ceo-davos-ai-translation-business-2026-1",
    "thumbnail_url": "https://i.insider.com/696fa4e6c58df2ecd5cccc5b?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:54.068Z",
    "topic": "finance"
  },
  {
    "slug": "sapiens-author-says-the-real-ai-timeline-is-200-years-but-the-lack-of-concern-today-is-what-scares-him-most",
    "title": "'Sapiens' author says the real AI timeline is 200 years — but the 'lack of concern' today is what scares him most",
    "description": "Yuval Noah Harari says AI's deepest social and political consequences will unfold over centuries, not years, and says short-term focus is dangerous.",
    "fullText": "Yuval Noah Harari warned that the world is badly misjudging the timescale of artificial intelligence — and that the real danger lies not in how fast the technology is moving, but in how casually it's being treated.\n\nSpeaking at the World Economic Forum in Davos on Tuesday, the historian and author of \"Sapiens: A Brief History of Humankind\" said that when he talks about the long-term impact of AI, he's not thinking in years or even decades.\n\n\"A lot of the conversations here in Davos, when they say 'long term' they mean like two years,\" Harari said. \"When I mean long term, I think 200 years.\"\n\nHarari compared the current moment in AI to the early days of the Industrial Revolution, saying that humanity tends to misunderstand transformative technologies as they unfold.\n\nThe deepest consequences of industrialization took generations to fully emerge, he said — often through social, political, and geopolitical upheaval that no one could have predicted in advance.\n\n\"You can test for accidents,\" he said. \"But you cannot test the geopolitical implications or the cultural implications of the steam engine in a laboratory. It's the same with AI.\"\n\nHarari warned that even if AI development were to stop today, its long-term effects would still be impossible to grasp.\n\n\"The stone has been thrown into the pool, but it just hit the water,\" he said. \"We have no idea what waves have been created, even by the AIs that have been deployed a year or two ago.\"\n\nHarari joins a plethora of senior AI researchers and tech leaders who have expressed concerns about AI's risks, ranging from waves of job losses to human extinction.\n\nHowever, what worries him most, Harari said, is not just uncertainty — but complacency. He said many of the most powerful decision-makers shaping AI are focused on short-term incentives rather than long-term consequences.\n\n\"I'm mainly concerned about the lack of concern that we are creating the most powerful technology in human history,\" he said.\n\n\"Very smart and powerful people are worried about what their investors say in the next quarterly report,\" Harari said. \"They think in terms of a few months, or a year or two.\"\n\nAI's social consequences, he said, will play out far beyond that horizon — whether the world is ready or not.",
    "readingTime": 2,
    "keywords": [
      "harari warned",
      "long-term",
      "consequences",
      "technology",
      "it's",
      "davos",
      "history",
      "social",
      "geopolitical",
      "test"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/sapiens-author-ai-timeline-warning-lack-of-concern-2026-1",
    "thumbnail_url": "https://i.insider.com/696fa92fa645d1188187a0cf?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:53.954Z",
    "topic": "finance"
  },
  {
    "slug": "the-author-of-sapiens-says-ai-is-about-to-create-2-crises-for-every-country",
    "title": "The author of 'Sapiens' says AI is about to create 2 crises for every country",
    "description": "At the World Economic Forum in Davos, Yuval Noah Harari said AI will create two crises for every country: An identity crisis and an immigration crisis.",
    "fullText": "AI technology is about to create two crises for every country, says the author of the bestselling book \"Sapiens: A Brief History of Humankind.\"\n\nOn Tuesday, while giving a talk on AI and humanity at the World Economic Forum in Davos, Yuval Noah Harari said that AI is going to plunge humanity into an identity crisis, since we have come to value ourselves based on our capacity to think, but AI will potentially outperform us on this task soon.\n\nSecondly, Harari said every nation is going to face an immigration crisis, comparing AIs to immigrants who will bring benefits into a country, such as skills in medicine and teaching, but disruptions alongside those benefits.\n\n\"Those who are concerned about human immigrants usually argue that immigrants might take jobs, might change the local culture, might be politically disloyal. I'm not sure that's true of all human immigrants, but it will definitely be true of the AI immigrants,\" he said.\n\nHarari asked the audience whether they want to allow AIs to be recognized as \"legal persons\" with rights in their countries, giving them the ability to start businesses, form and preach their own religions, or befriend their children on social media.\n\n\"If you want to influence where humanity is going, you need to make a decision now,\" he said.\n\nHarari is a distinguished research fellow at the University of Cambridge's Centre for the Study of Existential Risk, and a history professor at the Hebrew University of Jerusalem. Alongside \"Sapiens,\" his book tracing human history across the centuries, he also released \"Homo Deus: A Brief History of Tomorrow\" in 2015, where he discussed the existential threat AI poses to humanity.\n\nHarari is not the first person to refer to \"AI immigrants.\" Earlier this month, the Nvidia CEO, Jensen Huang, referred to robots using the term, and said \"AI immigrants\" will help humans with work we no longer want to do, such as manufacturing jobs, AFP reported.",
    "readingTime": 2,
    "keywords": [
      "brief history",
      "human immigrants",
      "humanity",
      "book",
      "sapiens",
      "crisis",
      "benefits",
      "alongside",
      "jobs",
      "harari"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/sapiens-author-yuval-noah-harari-ai-crises-every-country-2026-1",
    "thumbnail_url": "https://i.insider.com/696fa94fe1ba468a96aa5831?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:53.944Z",
    "topic": "finance"
  },
  {
    "slug": "palantir-ceo-alex-karp-says-humanities-jobs-are-doomed-in-the-age-of-ai-hopefully-you-have-some-other-skill",
    "title": "Palantir CEO Alex Karp says humanities jobs are doomed in the age of AI: 'Hopefully you have some other skill'",
    "description": "Alex Karp, CEO of Palantir, said vocational skills will dominate in the AI future.",
    "fullText": "Alex Karp, the billionaire co-founder and CEO of the AI defense technology company Palantir, had a top humanities-based education.\n\nKarp graduated from Haverford College, an elite liberal arts college in Pennsylvania, with a degree in philosophy. He then attended Stanford Law School and later earned a Ph.D. in neoclassical social theory at a top German university.\n\n\"A very, very strong education,\" Karp said during a panel at the World Economic Forum on Tuesday.\n\nIn the age of AI, that kind of academic trajectory will doom you, Karp told Larry Fink, the CEO of BlackRock, who was leading the discussion.\n\nAI \"will destroy humanities jobs,\" Karp said.\n\n\"You went to an elite school, and you studied philosophy — hopefully you have some other skill,\" Karp said, because that skillset is going to be very hard to market.\n\nThe Palantir CEO said it would probably be possible for those with humanities backgrounds to keep a job once they secure it, but he said technicians and those with other vocational skill sets are going to be in the highest demand.\n\nHe gave the example of people building batteries for a battery company, saying that \"they're very valuable, if not irreplaceable, because we can make them into something different than what they were very rapidly.\"\n\n\"There will be more than enough jobs for the citizens of your nation, especially those with vocational training,\" Karp told Fink.\n\nBut not all at Davos agree with the tech leader's assessment of the future of jobs.\n\nFinance executives attending the global forum have been telling Business Insider's Dan DeFrancesco that liberal arts degrees might be the new hot commodity. As AI takes on more of the hard financial analysis, it is transforming the skill set executives prioritize in young recruits. Critical creative thinkers are back in the spotlight.\n\nGoogle DeepMind CEO Demis Hassabis and Anthropic CEO Dario Amodei also weighed in on jobs during a joint panel on Tuesday, saying that entry-level hiring at their companies was already declining due to AI.\n\nAmodei said software and coding roles at Anthropic were down in both the junior and mid-levels of his company.",
    "readingTime": 2,
    "keywords": [
      "education karp",
      "liberal arts",
      "jobs",
      "skill",
      "elite",
      "philosophy",
      "panel",
      "humanities",
      "vocational",
      "saying"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/palantir-ceo-alex-karp-ai-humanities-jobs-doomed-2026-1",
    "thumbnail_url": "https://i.insider.com/696f9f5da645d11881879f0c?width=1200&format=jpeg",
    "created_at": "2026-01-20T18:21:53.938Z",
    "topic": "finance"
  },
  {
    "slug": "your-team-is-anxious-about-ai-heres-how-to-talk-to-them-about-it",
    "title": "Your Team Is Anxious About AI. Here’s How to Talk to Them About It.",
    "description": "While certainty about the future cannot be promised, leaders can provide a framework for managing team anxiety surrounding AI transformation. It’s important for both the leader and team to equally share in transparency and open dialogue in order to maintain team cohesion and psychological safety during periods of potential disruptive change.  Leaders must continuously make space for reflection, recognizing that AI anxiety cannot be resolved in a single conversation, by embracing employees’ fears, creating safe spaces for conversation, taking committed action, and repeating this process over and over.",
    "fullText": "Your Team Is Anxious About AI. Here’s How to Talk to Them About It. by Morra Aarons-MeleJanuary 20, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintDuring the pandemic, leaders learned that they had to figure out how to keep their teams gelled even when they had no idea what the heck would happen day to day. And many of us realized something beautiful: that showing a little more vulnerability and openness to the grey areas was exactly what our people needed. For a brief moment in time, leaders were allowed to talk about what they didn’t know.",
    "readingTime": 1,
    "keywords": [
      "talk",
      "leaders"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2026/01/your-team-is-anxious-about-ai-heres-how-to-talk-to-them-about-it",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_17_1495185382.jpg",
    "created_at": "2026-01-20T18:21:53.429Z",
    "topic": "business"
  },
  {
    "slug": "to-compete-in-the-global-economy-europe-needs-to-boost-its-vc-ecosystem",
    "title": "To Compete in the Global Economy, Europe Needs to Boost Its VC Ecosystem",
    "description": "Europe’s struggle to produce globally dominant companies is not a mystery—it is the predictable result of a weak and fragmented venture capital ecosystem. Despite an economy comparable in size to that of the United States, Europe invests only a fraction as much in venture-backed firms, and the gap is widening as capital concentrates around AI and other winner-take-all technologies. The consequences are visible in lower innovation, weaker returns for investors, and a steady outflow of entrepreneurial talent. But Europe’s venture deficit is not inevitable. While structural disadvantages exist, policy choices and investor behavior play a decisive role.",
    "fullText": "To Compete in the Global Economy, Europe Needs to Boost Its VC Ecosystem by Josh LernerJanuary 20, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintGovernments worldwide recognize the importance of promoting high-potential start-ups and the institutions that fund them. Europe is no exception. In 2024, Mario Draghi, a former President of the European Central Bank and one-time Italian Prime Minister lamented in an influential report on European competitiveness that “no EU company with a market capitalization over EUR 100 billion…has been set up from scratch in the last fifty years, while all six U.S. companies with a valuation above EUR 1 trillion have been created in this period.”",
    "readingTime": 1,
    "keywords": [
      "europe",
      "european"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2026/01/to-compete-in-the-global-economy-europe-needs-to-boost-its-vc-ecosystem",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_18_HBRCoins.jpg",
    "created_at": "2026-01-20T18:21:53.411Z",
    "topic": "business"
  },
  {
    "slug": "is-this-man-the-future-of-music-or-its-executioner-ai-evangelist-mikey-shulman-says-hes-making-pop-not-slop",
    "title": "Is this man the future of music – or its executioner? AI evangelist Mikey Shulman says he’s making pop, not slop",
    "description": "Worth a staggering $2.45bn, Suno is an AI music company that can create a track with just a few prompts. Why is its CEO happy to see it called ‘the Ozempic of the music industry’?\n‘The format of the future,” says Mikey Shulman, “is music you play with, not just play.” As the CEO and co-founder of the generative AI music company Suno, Shulman currently finds himself in the exhilarating if perhaps unenviable position of being simultaneously regarded as the architect of music’s future – and its executioner.",
    "fullText": "Worth a staggering $2.45bn, Suno is an AI music company that can create a track with just a few prompts. Why is its CEO happy to see it called ‘the Ozempic of the music industry’?\n\n‘The format of the future,” says Mikey Shulman, “is music you play with, not just play.” As the CEO and co-founder of the generative AI music company Suno, Shulman currently finds himself in the exhilarating if perhaps unenviable position of being simultaneously regarded as the architect of music’s future – and its executioner.\n\nSuno, which was founded just over two years ago, allows users to create entire songs with just a few text prompts. At the moment, you can’t prompt it with the name of a specific pop star, but asking for “stadium-level confessional pop-country” that “references past relationships” or “public rivalries” might get you a Taylor Swift-style song or thereabouts.\n\nIn June 2024, Suno became the target of litigation by record company trade body the RIAA on behalf of major labels in the US, while German collection society GEMA, representing songwriters, filed its own lawsuit the following January. Both claimed the service was training its systems on their copyrights without authorisation or licences.\n\nGen AI music services have triggered an existential crisis in the music industry. The utopian reading is that they will democratise creativity. The dystopian one is that art will be smothered by AI slop, as humans making music become surplus to requirements. (And many musicians already struggle to make a living from streaming revenues.) Dave Stewart of Eurythmics called them an “unstoppable force” and said musicians should, begrudgingly or enthusiastically, embrace them. Catherine Anne Davies, AKA the Anchoress, told me recently she believes it to be “dystopian”. Music lawyer Gregor Pryor has argued it is already killing off background music work.\n\n“I like to think of us as trying to make the next format for recorded music,” says Shulman. “The format of the future will be interactive.” What does he mean? “It should be social, meaning you’re doing it with other people. What we are doing is building the best digital version of that.”\n\nInvestors were clearly not scared off. In November, Suno raised $250m (£187m) in funding, taking its valuation to $2.45bn (£1.83bn). Gen AI is the hottest thing in Silicon Valley, with a Stanford University report saying it drew $34bn (£25bn) in private investment in 2024. But there are fears, notably at the Bank of England, that this brilliant boom can only be followed by a bitter bust. For now, however, investors believe gen AI is too big to fail. The stakes for Suno’s success are astonishingly high, especially given the recent leaking of an investor presentation suggesting the company only had 1 million paying subscribers. The standard monthly plan costs £8.25 ($10).\n\n“The thing that investors needed help realising,” says Shulman, “is how important music is in the world. Once you show them, their minds are changed and they realise that much, much more is possible.”\n\nWhen a new outside technology imposes itself on the music industry, the response typically runs from apoplexy to legal action, then negotiation and ultimately licensing. The three biggest names in gen AI music are at different stages along this trajectory. Klay got deals with all three majors before launching or training its technology on music, making it a rarity in this “Launch first, license later” world. Udio signed deals with Universal Music Group (UMG) and Warner Music Group (WMG). Suno, however, only has a deal with WMG, and legal action from the other majors is still live.\n\nShulman, now 39, was a hobbyist musician, which provided a catalyst for Suno. “I played in a lot of bands in high school and college,” he says, speaking by video from his home in the US, pointing to the bass hanging on the wall behind him. “I was OK, not great, and I was not going to be able to make a great career out of it.” He’s cautious and thoughtful when speaking, without the tang of arrogance you sometimes get from heavily hyped start-up founders.\n\nA career swerve into a physics PhD led him to Suno’s other co-founders. They wanted to build something different to heavyweight AI companies such as OpenAI, as those deal with “reasoning and automation to solve very specific problems. Music isn’t like that. There’s not a right or a wrong answer. It’s not a problem to solve.”\n\nThere remains debate about where exactly Suno sourced the music to train its systems – essentially breaking music into data strands for cataloguing – before its licensing deals were in place. “We train our models on medium- and high-quality music we can find on the open internet,” wrote Shulman in a 2024 blogpost. Suno’s initial legal defence was that this constituted fair use, and the music it drew on did not require prior permission. The record industry thought differently. “Fair use,” countered the RIAA, does not apply “when the output seeks to ‘substitute’ for the work copied.”\n\nI ask Shulman what he means by the “open internet”. There is a clear distinction between what is copyrighted (recordings are typically protected for 70 years) and what is in the public domain. “Copyright is a different thing,” he says. “I can’t get into too many specifics because there is active legal stuff going on, and also some of it is trade secrets.”\n\nCould Suno’s philosophy of “democratising” music-making be inherently anti-art? What once sprang from extraordinary human creativity now becomes ordinary. Shulman insists that, as with digital recording or sampling, this is just another example of how technology “pushes music forward”, how “new people get discovered” and “new genres get invented”.\n\nThe issue of so-called AI slop is wholly subjective, he says. “I made a really funny song with my four-year-old yesterday morning. That is ‘slop’ to you – you don’t care about it – but I love it. It’s fantastic.” He is keen to stress, meanwhile, that music generated by Suno can be extremely high quality.\n\nAnd AI-powered music is flooding streaming services: Deezer says more than a third of music delivered to it each day is AI (equal to 50,000 tracks), and 70% of streams of AI music on Deezer are fraudulent (scammers get cheaply made AI tracks on to such services, then use bots to manipulate streams at scale in order to get royalty payments, although services are increasingly wise to this). The company has started tagging AI tracks to alert users. Bandcamp recently announced that it won’t platform music “generated wholly or in substantial part by AI”.\n\nShould others follow? Shulman will say only that he doesn’t want to be “the arbiter of what happens on other platforms. There’s maybe some line to draw, but I don’t know where.”\n\nVelvet Sundown, a wholly AI “act”, released their debut album and a follow-up last summer. The 70s-styled rock band generated millions of streams, but it was a short-lived phenomenon. “I don’t know exactly what their strategy was,” Shulman says of Velvet Sundown. “It was all a bit of a goof. I think that’s why it was a flash in the pan.”\n\nSome AI-powered tracks have staying power, though. Following allegations it used Suno to clone Jorja Smith’s voice, I Run, by Haven, was excluded from the UK charts, but a version re-recorded by Kaitlin Aragon, a human singer, was chart-eligible and went Top 10. Into the Blue by Sienna Rose, who is widely suspected to be an AI act, recently made the Top 10 on Spotify’s Viral 50 Global chart. And the track Jag Vet, Du Är Inte Min is one of the biggest of the year so far in Sweden, although it has been booted out of the country’s charts for being “mainly AI-generated”.\n\nMore concerning was Suno being used last year to create tracks that the Anti-Defamation League said glorified Adolf Hitler, deployed racist slurs and talked of “white power”. Shulman says: “It was three songs that had a combined 10 plays. It was a very small thing. Unfortunately, drawing attention to it made it worse.” He says Suno has developed more rigorous safeguards to stop similar things happening in the future.\n\nSuno is keen for its deal with WMG to be seen as proof that gen AI companies can partner in mutually beneficial ways. Did the $1.5bn (£1.1bn) that AI company Anthropic paid to the book industry in September to settle claims its AI was trained on pirated copies spook Suno to get deals done swiftly? “We didn’t pay all that much attention to that,” says Shulman. “There’s a lot more to do together than fighting one another. And we intend, with this Warner partnership, to show that very strongly.”\n\nBut questions do persist about the WMG deal. Did the label insist on changes to the service? Were payments made to cover past use of its music in Suno’s training? Did WMG get equity in Suno? Shulman won’t reply, saying only that it is “a little early” to share such information, possibly fearful of compromising pending licensing deals.\n\nAgreements with the majors are one thing, but wooing artists is another. The majors insist they will only have their music used if they opt into deals. But if only a tiny percentage do – never mind their name, image and likeness rights – this will surely compromise results.\n\nIn his book Outliers, Malcolm Gladwell popularised “the 10,000-hour rule”, suggesting that this is the amount of practise time an artist needs to achieve any sort of mastery. Will the likes of Suno change this? “I think people will [still] have to spend 10,000 hours,” says Shulman. “They may be doing different things and practising different skills, but they will certainly need to spend 10,000 hours to make the best music in the world.”\n\nAs part of its charm offensive, Suno signed up US producer Timbaland as a strategic advisor, but he had to make a public apology after he took a track by producer K Fresh without permission and, Fresh alleged, “uploaded it into Suno’s AI platform, and released an unauthorised AI remix”.\n\nNevertheless, Shulman says the musicians he talks to about Suno see it as an important new creative tool and songwriting aid. He had previously told the 20VC podcast: “I think the majority of people don’t enjoy the majority of the time they spend making music.” This does not mean musicians hate the creative process in toto, but they are appreciative of tools that can remove at least some of the grunt work.\n\nThey just see it as a dirty secret, he suggests now. “When you get people one-on-one, they’re just more comfortable admitting it. It was described to me that we’re the Ozempic of the music industry – everybody is on it and nobody wants to talk about it.”\n\nThe fear, of course, is that by putting music on Ozempic, it wastes away to nothing.",
    "readingTime": 10,
    "keywords": [
      "legal action",
      "licensing deals",
      "music industry",
      "music generated",
      "suno shulman",
      "gen ai",
      "tracks",
      "services",
      "musicians",
      "majors"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/music/2026/jan/19/ai-music-company-mikey-shulman-suna",
    "thumbnail_url": "https://i.guim.co.uk/img/media/fdf44bef142f8ed1cf65e705c25f4a0ebb4b83c0/554_655_4079_3264/master/4079.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=419f52a4f493aed62dd727b4c83e3ffd",
    "created_at": "2026-01-20T18:21:50.561Z",
    "topic": "entertainment"
  },
  {
    "slug": "anthropic-seeks-larger-manhattan-office-as-part-of-nyc-expansion-bloomberg",
    "title": "Anthropic seeks larger Manhattan office as part of NYC expansion - Bloomberg",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/company-news/anthropic-seeks-larger-manhattan-office-as-part-of-nyc-expansion--bloomberg-93CH-4456001",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/world_news_2_108x81.jpg",
    "created_at": "2026-01-20T18:21:48.581Z",
    "topic": "finance"
  },
  {
    "slug": "execs-at-davos-say-ais-biggest-problem-isnt-hype-its-security",
    "title": "Execs at Davos say AI's biggest problem isn't hype — it's security",
    "description": "EY and KPMG executives at Davos said that AI security is a big risk, especially AI agents and, looking ahead, quantum computing.",
    "fullText": "I'm reporting from Davos, Switzerland, where thousands of business leaders and politicians have arrived at the World Economic Forum to shake hands, talk shop, and maybe even eke out a few ski runs.\n\nSome executives I've spoken to this week had some big concerns about AI, but none of them had anything to do with a potential bubble.\n\nRaj Sharma, EY's global managing partner of growth and innovation, said there's not enough talk about AI security — specifically, the management of AI agents and their lifecycle.\n\n\"It has access to your data. It has no name, so there is no identity or anything associated with that,\" Sharma said.\n\nCompare that to humans, where every computer system and piece of data they touch is often tracked.\n\n\"We have to build industrial-level security for AI agents in that particular area. To me, that's still a gap that somebody needs to work on,\" Sharma said. \"Everybody's talking a good game. But if you look under the covers, it's still not mature.\"\n\n\"That keeps me up at night,\" Sharma added.\n\nHe's not alone. Tim Walsh, the CEO of KPMG US, told me the biggest issue he talks to CEOs about regularly is cyber risk, specifically related to AI.\n\nAI agents are the latest twist in executives' ongoing concerns over cybersecurity, and it's proving to be an incredibly challenging problem. Somewhat ironically, the only way to fight the threat is with … more AI.\n\nIn some cases, the risk has gotten so big that it's shifting timelines on companies' AI plans.\n\n\"It's not that they're not moving forward, but they are taking a moment to make sure that their environment is secure, and perhaps even leaving data on-prem a little bit longer so they're confident that got their data security in place,\" Walsh told me.\n\nWalsh said another \"real concern\" is the threat of quantum computing from a security perspective.\n\nWhile he acknowledged we're still a few years out from the tech being fully developed, its power is incredible.\n\n\"Quantum breaks everything,\" Walsh said. \"I mean, all encryption.\"\n\nThat's led companies to look at their systems and reencrypting things, no easy task.\n\n\"We're spending quite a bit of time as well, helping companies think through: What does that look like? How do you structure it? How long will it take?\" Walsh said.",
    "readingTime": 2,
    "keywords": [
      "walsh",
      "security",
      "it's",
      "agents",
      "look",
      "talk",
      "executives",
      "concerns",
      "specifically",
      "that's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-security-risks-worry-ey-kpmg-execs-cybersecurity-davos-2026-1",
    "thumbnail_url": "https://i.insider.com/696f2d9ca645d11881879a22?width=1200&format=jpeg",
    "created_at": "2026-01-20T12:27:02.356Z",
    "topic": "finance"
  },
  {
    "slug": "citi-has-quietly-built-a-4000person-internal-ai-workforce",
    "title": "Citi has quietly built a 4,000-person internal AI workforce",
    "description": "Around 4,000 employees globally have volunteered to fan out across their business units as \"AI accelerators.\"",
    "fullText": "At Citi, some of the people leading the Wall Street bank's AI push aren't part of a specialized tech team. They're colleagues a few desks over.\n\nThose employees are part of the bank's AI Champions and Accelerators program, which got off the ground in early 2024 and now counts around 4,000 people among its ranks of voluntary AI helpers. At its most basic level, the program aims to have the designated \"accelerators\" help colleagues within their business units leverage and understand AI tools. The program's approximately 25-30 \"champions\" help lead the accelerators in their line of business.\n\nCiti's program is one of many ways banks are racing to adopt AI, from offering engineers capstone-style courses to luring top tech talent by promising a seat at powerful managerial tables. To date, Citi's AI tools are available in 84 countries for 182,000 employees. Adoption of the firm's proprietary tools is above 70%, Citi CEO Jane Fraser told analysts during the bank's earnings call last week.\n\nAnd it's clear that she has high expectations for all employees, including accelerators. In a January memo, she told staff she expected \"a more disciplined, more confident\" workforce in 2026, and that AI will likely \"reshape how work gets done.\"\n\nAmong the bank's range of initiatives to encourage AI uptake, the accelerators and champions model is uniquely embedded throughout teams and based on peer-to-peer interactions, said Carey Ryan, one of the initiative's leaders and the chief of staff for the technology organization.\n\n\"A small central team would never be able to reach where we are now,\" Ryan said.\n\nTech leaders at Citi came up with the program when brainstorming how to engage a wider swath of its population with AI, Ryan said. Originally, her team only planned to have 2,500 accelerators, but they saw more interest than expected and still maintain a waitlist for the program.\n\nWhile colleagues nominate champions for the position, anyone can volunteer to be an accelerator. The only requirement is being excited about AI, Ryan said.\n\nJosh Goldsmith is the AI champion for internal audit, where he's also the head of digital solutions and innovation. He's been a part of the program since its inception, and said that \"demystifying AI\" is among its greatest successes so far.\n\n\"It's a lot different when you hear from a colleague as to how you can leverage these tools, as opposed to having someone who's, let's say, a technologist trying to push this,\" he said. \"To be able to see someone put it in action: 'Hey, they can do it! I'm just like them, I can do it as well.'\"\n\nRyan also said that the peer-to-peer approach is part of what makes the initiative work, since a more general team couldn't understand all of the job-specific AI use cases across the bank. She added that accelerators have helped host more than 100 Citi AI Days, where they hold demonstrations and answer questions.\n\nAccelerators meet with their cohorts two times each month to participate in demos, learn about new tools, get training, or talk about their recent work. Goldsmith oversees around 50 participants; between attending their meetings and his own bi-monthly champion meetings, he said he devotes between three and five hours to the program each week, on top of his usual job.\n\nThe program members can also opt into trainings on topics including agentic AI to earn visual \"AI badges,\" which they can include in their email signatures. There are no specific engagement metrics, though, and people can drop out of the program if it becomes too demanding a time commitment.\n\n\"One accelerator might have five interactions, and maybe they're five small interactions,\" Ryan said. \"Another accelerator might have one really big interaction, where they're doing something with hundreds of people.\"\n\nAmong the AI peer coaches in internal audit, between 70% and 80% have stayed in the program, Goldsmith said. He added that, despite the extra hours, participating has given him a broader perspective on AI across Citi, and \"a clear vision in terms of what's coming next.\"\n\nFor Carey, there's something of a symbiotic relationship between program leads, participants, and the rest of the bank. Accelerators will explain what they're hearing from their lines of business — where people are struggling, what they might need — as Citi continues to fine-tune its approach to AI. They have, for example, helped Citi tweak AI tools, suggesting enhancements like adding notifications and allowing employees to upload larger files.\n\n\"It's an important conduit for feedback back in,\" she said.",
    "readingTime": 4,
    "keywords": [
      "internal audit",
      "ai ryan",
      "program",
      "tools",
      "bank's",
      "team",
      "they're",
      "employees",
      "tech",
      "colleagues"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/citi-bank-ai-accelerators-volunteers-2025-12",
    "thumbnail_url": "https://i.insider.com/6965437d04eda4732f2eea70?width=1200&format=jpeg",
    "created_at": "2026-01-20T12:27:02.237Z",
    "topic": "finance"
  },
  {
    "slug": "davos-day-2-live-updates-byd-microsoft-and-uber-ceos-speak",
    "title": "Davos day 2 live updates: BYD, Microsoft, and Uber CEOs speak",
    "description": "The second day of Davos features discussions on AI, EVs, and more. Speakers include Microsoft's Satya Nadella and Salesforce's Marc Benioff.",
    "fullText": "It's Business Insider's second day at Davos, and it's a busy one.\n\nWe're on the ground tracking everything World Economic Forum: the speakers, the food, the fashion, the chatter on the streets, and more.\n\nDavos' speaker panels kicked off in earnest this morning, with power players like Microsoft CEO Satya Nadella, BlackRock CEO Larry Fink, Palantir CEO Alex Karp, and Salesforce CEO Marc Benioff all on the agenda.\n\nFollow along as we give you a window into what it's really like to attend the World Economic Forum.\n\nAs you can probably tell, we here at Business Insider are really enjoying the winter fashions on show in sunny Davos.\n\nOur latest fashion dispatch comes from Kim Last, Editorial Director of BI Live, Business Insider's live journalism platform, who spied Monica Weinberg, a professor at Florida Atlantic University, rocking an Akris black lame suit and a matching Lady Dior bag.\n\nYou can keep track of all the best looks we've spotted at Davos by clicking below:\n\nAs the top dogs of the finance world live it up in the mountains, there's a big day brewing on Wall Street. With tensions between the US and Europe rising amid Trump's threats of further tariffs over Greenland, global stock markets are bumpy on Tuesday.\n\nIn European trading, the benchmark indexes in Germany, France, and the UK have all dropped more than 1% as of 7 a.m. ET.\n\nAs the top dogs of the finance world live it up in the mountains, there's a big day brewing on Wall Street. With tensions between the US and Europe rising amid Trump's threats of further tariffs over Greenland, global stock markets are bumpy on Tuesday.\n\nIn European trading, the benchmark indexes in Germany, France, and the UK have all dropped more than 1% as of 7 a.m. ET.\n\nUS futures are also pointing to a painful day, with the Dow, S&P 500, and Nasdaq all set to open more than 1% lower.\n\nFutures in the S&P 500 VIX, which measures volatility in the index, have jumped 7% on Tuesday, while gold, the ultimate safe haven, is up over 3% to $7,740 per ounce.\n\nAs traffic jams snarl the streets of Davos, conversations about the autonomous future of cars are getting a lot of airtime.\n\nEarlier, Uber CEO Dara Khosrowshahi was asked about the investment landscape around robotaxis, and said he doesn't think the investment market in autonomous vehicles is overheated.\n\nAs traffic jams snarl the streets of Davos, conversations about the autonomous future of cars are getting a lot of airtime.\n\nEarlier, Uber CEO Dara Khosrowshahi was asked about the investment landscape around robotaxis, and said he doesn't think the investment market in autonomous vehicles is overheated.\n\nHowever, he says the \"broader AI market — foundation models, hardware, semiconductors — is much hotter, with aggressive growth assumptions.\"\n\nAsked if Uber might invest in Waymo, Khosrowshahi said \"no comment.\"\n\n\"They have plenty of money,\" he said. \"But we're investing across the ecosystem, and we're very bullish.\"\n\nWith schedules full of back-to-back meetings and most shops on the promenade taken over by businesses, eating during the day can be tough.\n\nSome businesses set up grab-and-go food and drink stations. That's what Pinterest did, and this morning, sweet croissants were on the menu.\n\nWith schedules full of back-to-back meetings and most shops on the promenade taken over by businesses, eating during the day can be tough.\n\nSome businesses set up grab-and-go food and drink stations. That's what Pinterest did, and this morning, sweet croissants were on the menu.\n\nI opted for the chocolate crush, but I was told the cinnamon apple glow was the most popular choice, with only two left when I got there a little after 11 a.m.\n\nAnd yes, it tasted as good as it looked.\n\nSo when the CEO of Philip Morris International US travels, even for long trips, she takes a carry-on. Her trick: Pick a color for the week and coordinate outfits around that, with travel-well fabrics. For Davos this week, the color is black.\n\nStephen Ehikian is CEO of C3 AI, but he's no stranger to the Trump administration.\n\nEhikian, who was also previously a VP of product at Salesforce, served for eight months as the Acting Administrator of the General Services Administration before taking on the CEO role last September.\n\nStephen Ehikian is CEO of C3 AI, but he's no stranger to the Trump administration.\n\nEhikian, who was also previously a VP of product at Salesforce, served for eight months as the Acting Administrator of the General Services Administration before taking on the CEO role last September.\n\nI asked him about President Donald Trump's upcoming speech.\n\nHe had this to say: \"I'm hoping there's an expression of strength in numbers, and we're better together kind of story. Now, that said, I believe this idea that taking a strong 'America First' approach is good for this country.\n\n\"I think it's setting a tone and a change of direction from where we've been the last 50 years. So, I think it's refreshing. I don't even want to anticipate what's going to happen tomorrow because I have no idea.\"\n\nSpeaking earlier this morning, Uber CEO Dara Khosrowshahi said he is skeptical about some companies claiming rapid AI transformation, and that getting it right comes down to culture.\n\nUber tried building customer service AI that followed its old policies, Khosrowshahi told the audience at Davos, but it was \"only okay.\"\n\nSpeaking earlier this morning, Uber CEO Dara Khosrowshahi said he is skeptical about some companies claiming rapid AI transformation, and that getting it right comes down to culture.\n\nUber tried building customer service AI that followed its old policies, Khosrowshahi told the audience at Davos, but it was \"only okay.\"\n\nThat changed after the developers started from scratch with AI in mind.\n\n\"Companies are essentially rule systems,\" he said. \"To unlock AI's full potential, you have to break those rules and start over.\"\n\nDavos attracts a real cross-section of the world's rich and powerful, ranging from investors and politicians to sporting legends. Soccer great and longtime UNICEF ambassador David Beckham was spotted recording a podcast this morning.\n\nAt last year's Davos, Beckham was awarded the Crystal Award, which recognized his contributions to \"advocating for children's rights, education and well-being.\"\n\nMicrosoft CEO Satya Nadella, who spoke in a morning panel led by BlackRock CEO Larry Fink, shared how he has always prepared for Davos meetings — and how that's changed with AI.\n\nHe said his field team used to prep him for 50 or so meetings, according to a \"particular workflow.\"\n\nMicrosoft CEO Satya Nadella, who spoke in a morning panel led by BlackRock CEO Larry Fink, shared how he has always prepared for Davos meetings — and how that's changed with AI.\n\nHe said his field team used to prep him for 50 or so meetings, according to a \"particular workflow.\"\n\n\"Nothing had really changed since I joined in '92 to essentially even a few years back,\" he said about the year he joined the tech giant as an engineer. \"Whereas now, I just go to Copilot and say, \"Hey, I'm meeting Larry. Please give me a brief.\"\n\nHe added that the briefs generated by Microsoft's AI assistant give him a comprehensive overview of the relationship between his firm and BlackRock CEO Larry Fink's team.\n\nAI tools are also changing the traditional bottom-to-top flow of information, he said.\n\n\"In fact, what I do is I take that and immediately share that back with all my colleagues across all the functions,\" he said. \"Think about it. It's a complete inversion of how information is flowing in the organization.\"\n\nRobot drivers will ultimately outperform humans on safety, Uber CEO Dara Khosrowshahi said on Tuesday in Davos.\n\n\"If you think about the world 20 years from now, your Uber is going to be driven largely not by a human being, but by a robot driver — a piece of software on top of a car,\" Khosrowshahi said, adding that vehicles are becoming increasingly sophisticated and more like \"computers on wheels.\"\n\nRobot drivers will ultimately outperform humans on safety, Uber CEO Dara Khosrowshahi said on Tuesday in Davos.\n\n\"If you think about the world 20 years from now, your Uber is going to be driven largely not by a human being, but by a robot driver — a piece of software on top of a car,\" Khosrowshahi said, adding that vehicles are becoming increasingly sophisticated and more like \"computers on wheels.\"\n\nKhosrowshahi said autonomous systems have clear advantages over human drivers. \"There's no doubt in my mind that the robot driver can be safer than a human driver,\" he said. Robot drivers don't get tired or distracted, don't text while driving, and can operate continuously while improving over time, he added.\n\nThe key question, Khosrowshahi said, is what level of safety is \"enough\" for robot drivers — whether matching human performance is sufficient or whether autonomous vehicles should be held to a higher standard.\n\nIn the longer term, he said, human driving could resemble horseback riding today, becoming a niche activity done for enjoyment.\n\n\"There's no doubt that 10 years from now, there will be questions as to whether humans are safe enough,\" he said.\n\nTrump is expected to arrive in Davos on Wednesday. While Treasury Secretary Scott Bessent wants business leaders to take a breath and chill out about Greenland, that may not be in the cards.\n\nTrump posted on Truth Social on Thursday that he had \"a very good telephone call with Mark Rutte, the Secretary General of NATO, concerning Greenland.\"\n\nTrump is expected to arrive in Davos on Wednesday. While Treasury Secretary Scott Bessent wants business leaders to take a breath and chill out about Greenland, that may not be in the cards.\n\nTrump posted on Truth Social on Thursday that he had \"a very good telephone call with Mark Rutte, the Secretary General of NATO, concerning Greenland.\"\n\n\"I agreed to a meeting of the various parties in Davos, Switzerland. As I expressed to everyone, very plainly, Greenland is imperative for National and World Security,\" Trump wrote.\n\nTrump on Sunday had sent messages to Norway's leader, saying the US requires \"Complete and Total Control of Greenland.\" Greenland is a semi-autonomous Danish territory.\n\nGovernments keep changing their rules on EVs, says BYD executive vice president Stella Li, and that sets carmakers back.\n\nLi said in a panel at Davos that when countries go \"back and forth\" on their EV policy, it creates a pattern that \"will confuse manufacturers.\" In contrast, when governments give a \"very clear line,\" automakers can focus on execution, Li said.\n\nBessent, speaking to the press on Tuesday morning, urged calm amid President Donald Trump's latest slate of trade tariffs on Greenland.\n\n\"You say: 'If there is a protracted trade war.' Why are we jumping there? Why are you taking it to the worst case? Calm down the hysteria, take a deep breath, this is where we were last year,\" Bessentt said. \"I'm sure you would have asked me the same question if we were here on April 2nd. And you know what? It all worked out.\"\n\nBessent, speaking to the press on Tuesday morning, urged calm amid President Donald Trump's latest slate of trade tariffs on Greenland.\n\n\"You say: 'If there is a protracted trade war.' Why are we jumping there? Why are you taking it to the worst case? Calm down the hysteria, take a deep breath, this is where we were last year,\" Bessentt said. \"I'm sure you would have asked me the same question if we were here on April 2nd. And you know what? It all worked out.\"\n\nWhen asked about what European business leaders should do, Bessent's take was to \"sit back\" and \"relax.\"\n\n\"What I am urging everyone here to do is sit back, take a deep breath, and let things play out,\" Bessent said.\n\n\"I am confident that the leaders will not escalate and that this will work out in a manner that ends up in a very good place for all, for national security, for the US, and for Europe,\" he said.\n\nAnything you do within your company's digital ecosystem is likely being tracked, from the data you touch to the software programs you use.\n\nThat's a real concern as we rely on them more and more, Raj Sharma, EY's global managing partner of growth and innovation, told me.\n\n\"We have to build industrial-level security for AI agents in that particular area,\" he said. \"That keeps me up at night,\" Sharma added.\n\nFor Tim Walsh, the chair and CEO of KPMG US, it's the threat of quantum computing. While still a few years out, its power is undeniable.\n\n\"Quantum breaks everything,\" Walsh said. \"I mean, all encryption.\"\n\n\"M&A is high on the agenda,\" said Sharon Marcil, North America chair at BCG. \"We're hearing it from so many CEOs.\" She pointed to a strong stock market and an overall tilt in Washington toward deregulation and robust business activity.\n\nWinston Weinberg, CEO and cofounder of the legal AI startup Harvey, echoes the sentiment, saying he is hearing that 2026 will be an \"insane\" year for dealmaking. (Good for lawyers!)\n\n\"M&A is high on the agenda,\" said Sharon Marcil, North America chair at BCG. \"We're hearing it from so many CEOs.\" She pointed to a strong stock market and an overall tilt in Washington toward deregulation and robust business activity.\n\nWinston Weinberg, CEO and cofounder of the legal AI startup Harvey, echoes the sentiment, saying he is hearing that 2026 will be an \"insane\" year for dealmaking. (Good for lawyers!)\n\nSimilar predictions predominated this time last year. The tariff tumult put a pause on M&A early on, but then dealmaking got its mojo back.\n\nFink took the stage at Davos to welcome more than a thousand chief executives to the World Economic Forum. In his opening remarks, he questioned whether anyone outside the room would care about this meeting of global leaders.\n\n\"Because if we're being honest, for many people this meeting feels out of step with the moment: elites in an age of populism, an established institution in an era of deep institutional distrust,\" the BlackRock CEO said.\n\nFink took the stage at Davos to welcome more than a thousand chief executives to the World Economic Forum. In his opening remarks, he questioned whether anyone outside the room would care about this meeting of global leaders.\n\n\"Because if we're being honest, for many people this meeting feels out of step with the moment: elites in an age of populism, an established institution in an era of deep institutional distrust,\" the BlackRock CEO said.\n\n\"And there's truth in that critique,\" he added. \"I've believed in this forum for a long time. I certainly wouldn't be leading it if I didn't. But it's also obvious that the world now places far less trust in us to help shape what comes next.\"\n\nFink said the capitalism now faces a big test: Whether it can \"evolve to turn more people into owners of growth, instead of spectators watching it happen.\"\n\n\"And that kind of change is hard. Especially in a world of competing ideologies and assumptions about how the system should work,\" he said.\n\nFink's remarks ground what is expected to be a monumental week, in which deals are made and new alliances are forged amid turmoil in the wider political sphere.",
    "readingTime": 13,
    "keywords": [
      "wall street",
      "mark rutte",
      "nato concerning",
      "sharon marcil",
      "marcil north",
      "washington toward",
      "startup harvey",
      "harvey echoes",
      "satya nadella",
      "europe rising"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/davos-wef-live-updates-jan-20-2026-1",
    "thumbnail_url": "https://i.insider.com/696f1c7ee1ba468a96aa5193?width=1200&format=jpeg",
    "created_at": "2026-01-20T12:27:02.097Z",
    "topic": "finance"
  },
  {
    "slug": "uk-exposed-to-serious-harm-by-failure-to-tackle-ai-risks-mps-warn",
    "title": "UK exposed to ‘serious harm’ by failure to tackle AI risks, MPs warn",
    "description": "Government, Bank of England and FCA criticised for taking ‘wait-and-see’ approach to AI use in financial sector\nConsumers and the UK financial system are being exposed to “serious harm” by the failure of government and the Bank of England to get a grip on the risks posed by artificial intelligence, an influential parliamentary committee has warned.\nIn a new report, MPs on the Treasury committee criticise ministers and City regulators, including the Financial Conduct Authority (FCA), for taking a “wait-and-see” approach to AI use across the financial sector.\n Continue reading...",
    "fullText": "Government, Bank of England and FCA criticised for taking ‘wait-and-see’ approach to AI use in financial sector\n\nConsumers and the UK financial system are being exposed to “serious harm” by the failure of government and the Bank of England to get a grip on the risks posed by artificial intelligence, an influential parliamentary committee has warned.\n\nIn a new report, MPs on the Treasury committee criticise ministers and City regulators, including the Financial Conduct Authority (FCA), for taking a “wait-and-see” approach to AI use across the financial sector.\n\nThat is despite looming concerns over how the burgeoning technology could disadvantage already vulnerable consumers, or even trigger a financial crisis, if AI-led firms end up making similar financial decisions in response to economic shocks.\n\nMore than 75% of City firms now use AI, with insurers and international banks among the biggest adopters. It is being used to automate administrative tasks or even help with core operations, including processing insurance claims and assessing customers’ credit-worthiness.\n\nBut the UK has failed to develop any specific laws or regulations to govern their use of AI, with the FCA and Bank of England claiming general rules are sufficient to ensure positive outcomes for consumers. That means businesses have to determine how existing guidelines apply to AI, leaving MPs worried this could put consumers and financial stability at risk.\n\n“It is the responsibility of the Bank of England, the FCA and the government to ensure the safety mechanisms within the system keeps pace,” said Meg Hillier, chair of the Treasury committee. “Based on the evidence I’ve seen, I do not feel confident that our financial system is prepared if there was a major AI-related incident and that is worrying.”\n\nThe report flagged a lack of transparency around how AI could influence financial decisions, potentially affecting vulnerable consumers’ access to loans or insurance. It said it was also unclear whether data providers, tech developers or financial firms would be held responsible when things went wrong.\n\nMPs said AI also increased the likelihood of fraud, and the dissemination of unregulated and misleading financial advice.\n\nIn terms of financial stability, MPs found that rising AI use increased firms’ cybersecurity risks, and left them overly reliant on a small number of US tech companies, such as Google, for essential services. Its uptake could also amplify “herd behaviour”, with businesses making similar financial decisions during economic shocks and “risking a financial crisis”.\n\nThe Treasury committee is now urging regulators to take action, including the launch of new stress tests that would assess the City’s readiness for AI-driven market shocks. MPs also want the FCA to publish “practical guidance” by the end of the year, clarifying how consumer protection rules apply to AI use, and who would be held accountable if consumers suffer any harm.\n\n“By taking a wait-and-see approach to AI in financial services, the three authorities are exposing consumers and the financial system to potentially serious harm”, the report said.\n\nThe FCA said it had already “undertaken extensive work to ensure firms are able to use AI in a safe and responsible way”, but would review the report’s findings “carefully”.\n\nA spokesperson for the Treasury said: “We’ve been clear that we will strike the right balance between managing the risks posed by AI and unlocking its huge potential.”\n\nThey added that this involved working with regulators to “strengthen our approach as the technology evolves”, and appointing new “AI champions” covering financial services “to ensure we seize the opportunities it presents in a safe and responsible way”.\n\nA spokesperson for the Bank of England said it had “already taken active steps to assess AI-related risks and reinforce the resilience of the financial system, including publishing a detailed risk assessment and highlighting the potential implications of a sharp fall in AI-affected asset prices. We will consider the committee’s recommendations carefully and will respond in full in due course.”",
    "readingTime": 4,
    "keywords": [
      "treasury committee",
      "serious harm",
      "economic shocks",
      "wait-and-see approach",
      "risks posed",
      "vulnerable consumers",
      "financial sector",
      "financial crisis",
      "financial decisions",
      "financial stability"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/business/2026/jan/20/uk-ai-risks-mps-government-bank-of-england-fca",
    "thumbnail_url": "https://i.guim.co.uk/img/media/806cb798ee36a482ee711e0dde66c6d1719010f6/1124_0_5598_4481/master/5598.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=17c31cb165b8a65398c873adc591569d",
    "created_at": "2026-01-20T12:26:57.079Z",
    "topic": "politic"
  },
  {
    "slug": "microsoft-ceo-nadella-on-how-ai-can-change-workflows",
    "title": "Microsoft CEO Nadella on How AI Can Change Workflows",
    "description": "Microsoft Chairman and Chief Executive Officer Satya Nadella discusses artificial intelligence and the potential it has to change workflows. \"The current structure may not make sense, because you want people to be able to work in a way that allows them to have this information flow freely,\" Nadella tells BlackRock CEO Larry Fink at the World Economic Forum in Davos, Switzerland.",
    "fullText": "Microsoft CEO Nadella on How AI Can Change Workflows BloombergMST Microsoft Chairman and Chief Executive Officer Satya Nadella discusses artificial intelligence and the potential it has to change workflows. \"The current structure may not make sense, because you want people to be able to work in a way that allows them to have this information flow freely,\" Nadella tells BlackRock CEO Larry Fink at the World Economic Forum in Davos, Switzerland.",
    "readingTime": 1,
    "keywords": [
      "workflows",
      "nadella",
      "microsoft"
    ],
    "qualityScore": 0,
    "link": "https://finance.yahoo.com/video/microsoft-ceo-nadella-ai-change-103727613.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/w.TVAL8dnQcf8YvSIgA4qQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/bloomberg_markets_video_2/a6b58d78c13f17eaa9d5a552ee484ab0",
    "created_at": "2026-01-20T12:26:55.128Z",
    "topic": "finance"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here’s how the three models performed in our tests:\n\n💡 If you want the safest pick for real “ship a feature in a big repo” work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you’re okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n⚠️ NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere’s the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe’ll compare code quality, token usage, cost, and time to complete the build.\n\n💡 NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\n💡 NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It’s not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that’s the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that’s a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it’s hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. 🤷‍♂️\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don’t think we’re anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don’t fully back them up. 🥴\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won’t hold up long term.\n\nI don't see any of these recent models as “use it and ship it” for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "vinod-khosla-is-looking-at-this-metric-to-gauge-if-were-in-an-ai-bubble",
    "title": "Vinod Khosla is looking at this metric to gauge if we're in an AI bubble",
    "description": "\"What Wall Street tends to do with it, I don't really care,\" Khosla said.",
    "fullText": "Vinod Khosla has his eye on one AI metric, and it's not stock prices.\n\nOn an episode of OpenAI's podcast released on Monday, the famed venture capitalist shared how he's gauging whether we're in an AI bubble — or not.\n\n\"People equate bubble to stock prices, which has nothing to do with anything other than fear and greed among investors,\" he said. \"So I always look at, bubbles should be measured by the number of API calls.\"\n\nAPI, or Application Programming Interface calls, refer to the process in which one software application sends a message to another application to request data or to trigger an action. They are a common indicator of digital tools' use, especially with the rise of AI agents. High API calls can also be a mark of a poor or inefficient product.\n\nKhosla said the bubble shouldn't be called \"by what happened to stock prices because somebody got overexcited or underexcited and in one day they can go from loving Nvidia to hating Nvidia because it's overvalued.\"\n\nThe 70-year-old VC, whose notable investments include OpenAI, DoorDash, and Block, compared the AI bubble to the dot-com bubble. He said he looked out for internet traffic as a metric during the 1990s, and with AI bubble concerns, that benchmark is now API calls.\n\n\"If that's your fundamental metric of what's the real use of your AI, usefulness of AI, demand for AI, you're not going to see a bubble in API calls,\" he said. \"What Wall Street tends to do with it, I don't really care. I think it's mostly irrelevant.\"\n\nConcerns that the AI industry is overvalued because of massive investments became one of the buzziest themes in the second half of 2025. The phrase \"AI bubble\" appeared in 42 earnings calls and investor conference transcripts between October and December — a 740% increase from the previous quarter, according to an AlphaSense analysis.\n\nTop business leaders remain split about whether the bubble is about to burst.\n\nMicrosoft cofounder Bill Gates said AI has extremely high value, but it's still in a bubble.\n\n\"But you have a frenzy,\" Gates told CNBC in late October. \"And some of these companies will be glad they spent all this money. Some of them, you know, they'll commit to data centers whose electricity is too expensive.\"\n\nEarlier this month, \"Big Short\" investor Michael Burry raised the alarm on an AI bubble in a Substack exchange.\n\nBurry wrote that companies, including Microsoft and Alphabet, are wasting trillions on microchips and data centers that will quickly become obsolete. He added that their spending has \"no clear path to utilization by the real economy.\"\n\nNvidia CEO Jensen Huang has dismissed concerns of a bubble. His company became the world's first $5 trillion market cap company in October on the back of the AI boom.\n\nIn an October Bloomberg TV appearance, Huang said that instead of overspeculation, AI is part of a transition from an old way of computing.\n\n\"We also know that AI has become good enough because of reasoning capability, and research capability, its ability to think — it's now generating tokens and intelligence that is worth paying for,\" Huang said.",
    "readingTime": 3,
    "keywords": [
      "bubble",
      "it's",
      "metric",
      "stock",
      "concerns",
      "overvalued",
      "investments",
      "investor",
      "centers",
      "capability"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/vinod-khosla-looks-at-this-metric-to-gauge-ai-bubble-2026-1",
    "thumbnail_url": "https://i.insider.com/696eef8ce1ba468a96aa5078?width=1200&format=jpeg",
    "created_at": "2026-01-20T06:21:43.656Z",
    "topic": "finance"
  },
  {
    "slug": "openais-finance-chief-just-dropped-some-hints-about-how-the-company-plans-to-make-more-money",
    "title": "OpenAI's finance chief just dropped some hints about how the company plans to make more money",
    "description": "OpenAI's chief financial officer floated \"licensing models\" as a revenue idea as the company faces massive compute costs.",
    "fullText": "OpenAI's chief financial officer is musing about new ways the company could make money beyond ChatGPT subscriptions.\n\nIn an episode of \"The OpenAI Podcast\" published Monday, Sarah Friar floated the idea of \"licensing models,\" in which the company could take a share of downstream sales if a customer's product takes off.\n\n\"Let's say in drug discovery, if we licensed our technology, you have a breakthrough. The drug takes off, and we get a licensed portion of all its sales,\" Friar said.\n\n\"It's great alignment for us with our customer,\" she added.\n\nFriar's comments offer a glimpse of how OpenAI is thinking about funding its compute-hungry ambitions as it broadens its business model.\n\nFriar said OpenAI began with a single subscription after launching ChatGPT, but has since expanded to multiple price points, SaaS-style enterprise pricing, and credit-based pricing for customers who \"want to pay more to get more.\"\n\nAs OpenAI looks at commerce and ads, Friar said the model should always give the best answer, not a sponsored one, and maintain an ad-free tier.\n\nLast week, OpenAI announced it is preparing to test ads in ChatGPT, as the company seeks to boost revenue while facing spending commitments of about $1.4 trillion over the coming years.\n\nThe move marks a shift from the company's earlier stance. Less than two years ago, OpenAI CEO Sam Altman described advertising as a \"last resort.\"\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nSince then, Altman's tone has evolved alongside OpenAI's expansion and ballooning compute costs. In June, he said on OpenAI's podcast that he wasn't \"totally against\" advertising, but emphasized that it would need to be handled carefully.\n\nOpenAI also completed its restructuring in October, shifting the company toward a more traditional for-profit structure — a change Altman said would make it easier to raise capital going forward.",
    "readingTime": 2,
    "keywords": [
      "business model",
      "openai's",
      "chatgpt",
      "openai",
      "sales",
      "drug",
      "licensed",
      "pricing",
      "advertising",
      "resort"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-chief-financial-officer-sarah-friar-licensing-models-revenue-2026-1",
    "thumbnail_url": "https://i.insider.com/696ef256c58df2ecd5ccc57f?width=1200&format=jpeg",
    "created_at": "2026-01-20T06:21:43.654Z",
    "topic": "finance"
  },
  {
    "slug": "elon-musk-drops-a-surprise-curveball-on-nvidia",
    "title": "Elon Musk drops a surprise curveball on Nvidia",
    "description": "Tesla (TSLA) just delivered a rare double whammy to Nvidia (NVDA) over the past weekend. CEO Elon Musk revealed that Tesla’s much-talked-about AI5 self-driving chips are nearly complete, and that the next one, A16, is already underway. With the AI inference part covered, Musk said Sunday on X ...",
    "fullText": "Tesla (TSLA) just delivered a rare double whammy to Nvidia (NVDA) over the past weekend.\n\nCEO Elon Musk revealed that Tesla’s much-talked-about AI5 self-driving chips are nearly complete, and that the next one, A16, is already underway.\n\nWith the AI inference part covered, Musk said Sunday on X that Dojo 3 is being restarted, pushing Tesla back into large-scale AI training after previously pulling back.\n\nNvidia threw the first punch, though, when it rolled out “Alpamayo” at CES 2026 (an open-source autonomous vehicle AI toolkit), aiming to become the default autonomy platform powering a ton of brands.\n\nMusk responded swiftly, downplaying the risk.\n\nClearly, this is a mighty interesting time for the AV industry, with the tug-of-war between two giants in Nvidia and Tesla.\n\nFor Tesla, it’s all about building a closed loop that covers the entire AV stack.\n\nTesla-designed in-car compute (this includes AI5, which is “nearly done,” and the AI6, which is already underway)\n\nTesla’s camera-first software stack\n\nTesla’s data flywheel is powered by its own fleet\n\nSo for Tesla, it’s all about keeping the autonomy part within its potent ecosystem, as Nvidia looks to power everyone else.\n\nFor investors, these promises aren’t new, which makes the follow-through all the more critical.\n\nTesla is looking to tighten its grip on the hardware behind self-driving.\n\nMorgan Stanley sets jaw-dropping Micron price target after event\n\nForget Blackwell, Nvidia future is Vera Rubin, agentic software\n\nQuantum Computing makes $110 million move nobody saw coming\n\nMorgan Stanley drops eye-popping Broadcom price target\n\nApple analyst sets bold stock target for 2026\n\nMusk announced on Saturday in an X post that the EV giant is nearing completion of its AI5 self-driving computer chip, and that the AI6 is already in development.\n\nAccording to Musk, the AI5 chips, which are manufactured by Taiwan Semiconductor Manufacturing Company, will enter high-volume production in 2027, replacing the AI4 hardware. Also, Tesla has lined up Samsung Electronics for U.S.-based chip manufacturing.\n\nIt’s pretty easy to get lost in the AI jargon, so it’s important to be clear about things at each step about what’s happening.\n\nThe A15 and A16 move is essentially about “inference at the edge”. That’s basically running Tesla’s Full Self-Driving neural nets inside the car, instead of relying on a third-party compute stack.",
    "readingTime": 2,
    "keywords": [
      "tesla it’s",
      "morgan stanley",
      "stack",
      "target",
      "chips",
      "nearly",
      "underway",
      "inference",
      "back",
      "autonomy"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/elon-musk-drops-surprise-curveball-190300808.html",
    "thumbnail_url": "https://s.yimg.com/os/en/thestreet_881/dd84ae97738233f3f8b955ba0292a512",
    "created_at": "2026-01-20T06:21:41.682Z",
    "topic": "finance"
  },
  {
    "slug": "ygrep-fast-local-indexed-code-search-tool-optimized-for-ai-coding-assistants",
    "title": "Ygrep: Fast, local, indexed code search tool optimized for AI coding assistants",
    "description": "A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing. - yetidevworks/ygrep",
    "fullText": "yetidevworks\n\n /\n\n ygrep\n\n Public\n\n A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing.\n\n License\n\n MIT license\n\n 14\n stars\n\n 2\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n yetidevworks/ygrep",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/yetidevworks/ygrep",
    "thumbnail_url": "https://opengraph.githubassets.com/eda1bb1f12626e76b9793d44f791d0ed3332b6c28dc7896d1831d00d7c49258d/yetidevworks/ygrep",
    "created_at": "2026-01-20T00:57:31.412Z",
    "topic": "tech"
  },
  {
    "slug": "npm-install-a-wasm-based-linux-vm-for-your-agents",
    "title": "NPM install a WASM based Linux VM for your agents",
    "description": "Lightweight WASM-based Linux VM for AI agents - run shell commands in an isolated Alpine Linux environment - deepclause/agentvm",
    "fullText": "deepclause\n\n /\n\n agentvm\n\n Public\n\n Lightweight WASM-based Linux VM for AI agents - run shell commands in an isolated Alpine Linux environment\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n deepclause/agentvm",
    "readingTime": 1,
    "keywords": [
      "linux",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/deepclause/agentvm",
    "thumbnail_url": "https://opengraph.githubassets.com/20857a7fcbad637df4fb2f5ba4f4801e229fc67e34392bcfd8240a9636fa91b4/deepclause/agentvm",
    "created_at": "2026-01-20T00:57:31.399Z",
    "topic": "tech"
  },
  {
    "slug": "f5-tackles-ai-security-with-new-platform-extensions",
    "title": "F5 tackles AI security with new platform extensions",
    "description": "F5's Guardrails blocks prompts that attempt jailbreaks or injection attacks, for example, while its AI Red Team automates vulnerability discovery in AI systems.",
    "fullText": "Network and security teams managing enterprise applications face two emerging challenges: AI security and multi-cloud operations. F5 is addressing both with product announcements that extend its Application Delivery and Security Platform into AI runtime protection and multicloud managed services.\n\nThe company announced F5 AI Guardrails and F5 AI Red Team on January 14, following the January 13 launch of F5 NGINXaaS for Google Cloud. The AI security products came through F5’s acquisition of CalypsoAI in September 2025. NGINXaaS is a web-server-as-a-service offering that is now expanding beyond its initial Azure deployment in May 2025, with AWS support in development.\n\n“We’ve never been a walled garden—we believe in meeting customers where they are and integrating with their existing stack, whether that stack is F5 products or other vendors,” Shawn Wormke, senior vice president of product management at F5, told Network World.\n\nAI security requires a different approach than traditional network defenses. Web application firewalls (WAF) inspect HTTP requests for SQL injection. Intrusion detection systems analyze packet headers for known attack signatures. These tools operate at the packet and protocol level.\n\nAI systems present a different attack surface. A malicious prompt that tricks a model into revealing training data looks like legitimate traffic at the network layer. Wormke explained that AI security is another layer of security just like WAF. Where it differs is that traditional security threats live in packets, whereas AI threats live in the words, prompts and context exchanged between users, models and agents.\n\n“AI models can be manipulated into exposing sensitive data or generating harmful outputs impacting brand reputation and trust,” he said. “So you need a solution that’s purpose-built to secure and govern these new threats like prompt injection, new jailbreak techniques and model distillation.”\n\nF5 AI Guardrails deploys as a proxy between users and AI models. Wormke describes it as being inserted as a proxy layer at the “front door” of AI interaction, between AI applications, users and agents. It intercepts prompts before they reach the model and analyzes outputs before they return to users. The system applies policy rules to actual content rather than transport-layer characteristics.\n\nPolicy enforcement covers several categories. Guardrails blocks prompts that attempt jailbreaks or injection attacks, scans outputs for sensitive data patterns, and enforces compliance requirements including GDPR and the EU AI Act.\n\nAI Red Team automates adversarial testing against AI systems. It maintains a database of attack techniques that grows by 10,000 entries monthly as researchers discover new vulnerabilities.\n\nResults from Red Team testing feed directly into Guardrails policies. When Red Team discovers a vulnerability pattern, security teams create corresponding guardrails to block similar attacks in production.\n\n“It’s a very synergistic pairing wherein with AI Red Team you can send a team of agents to discover vulnerabilities in AI systems, and with AI Guardrails you can transform those insights into threat-informed defenses,” Wormke said.\n\nF5 acquired NGINX back in 2019 and has been expanding the capabilities of the web server platform ever since.\n\nNGINXaaS first became generally available on Microsoft Azure in 2023 and is now coming to Google Cloud. The Google Cloud expansion addresses customer demand for consistent tooling across cloud providers. “Customers have been asking for NGINXaaS across additional cloud platforms,” Wormke said. “Google Cloud was the next and now we are working with AWS.”\n\nThe service combines Layer 4 and Layer 7 load balancing with security and observability, available through Google Cloud Marketplace.\n\n“NGINXaaS is built on the pedigree of the enterprise version of NGINX, NGINX Plus,” Wormke said. “It has open-source roots and will support your open-source configurations, but it gives you all the capabilities of commercial NGINX Plus.”\n\nWormke noted that those capabilities include visibility into over 240 deep application metrics, intelligent load balancing algorithms, traffic optimization, in-memory state sharing, AuthN & AuthZ and it’s  ready to go F5 WAF for NGINX.\n\nThe NGINX project celebrated its 20th anniversary in October 2024 and is one of the most widely used web server and application delivery technologies. Beyond just the core web server, it is also used as a load balancer, reverse proxy, programmability layer, API Gateway, Ingress Controller and Kubernetes Gateway.\n\n“It continues to power a significant portion of the internet and API traffic today,” Wormke said. “It is far more than the web server that many think open-source NGINX is.”\n\nF5 continues development on both open-source NGINX and commercial versions. For instance, he noted that F5 recently announced Encrypted Client Hello (ECH) support for enhanced privacy as well as support for PQC (Post Quantum Cryptography). Looking forward, AI is going to play a big role in the future direction.\n\n“We are evolving toward AI-aware application delivery and security to enable secure, scalable and performant, agentic AI operations,” he said.",
    "readingTime": 4,
    "keywords": [
      "nginx plus",
      "plus wormke",
      "open-source nginx",
      "load balancing",
      "application delivery",
      "web server",
      "security teams",
      "ai guardrails",
      "systems",
      "users"
    ],
    "qualityScore": 1,
    "link": "https://www.networkworld.com/article/4118696/f5-tackles-ai-security-with-new-platform-extensions.html",
    "thumbnail_url": "https://www.networkworld.com/wp-content/uploads/2026/01/4118696-0-65503400-1768836299-shutterstock_2403416063_2de17a.jpg?quality=50&strip=all&w=1024",
    "created_at": "2026-01-20T00:57:27.499Z",
    "topic": "tech"
  },
  {
    "slug": "goldman-sachs-says-watch-these-5-warnings-from-the-dotcom-bubble-to-know-if-the-ai-craze-is-peaking",
    "title": "Goldman Sachs says watch these 5 warnings from the dot-com bubble to know if the AI craze is peaking",
    "description": "The stock market flashed five warning signs before the dot-com bubble popped in the early 2000s, strategists at Goldman Sachs said.",
    "fullText": "Markets are worried about shades of 1999 in today's tech-investing landscape, and while there's a lot of debate about whether AI is a bubble, there are a few signals from history that show what, specifically, investors should be looking out for.\n\nStrategists at Goldman Sachs said they believe the market's AI frenzy risks mirroring the dot-com bubble burst in the early 2000s.\n\nStocks don't look like they're in their 1999 moment yet, Dominic Wilson, a senior advisor on the bank's global markets research team, and Vickie Chang, a macro research strategist, wrote in a note to clients on Sunday. But the risks that the AI boom will look a lot like the 2000s era craze appear to be growing, they said.\n\n\"We see a growing risk that the imbalances that built up in the 1990s will become more visible as the AI investment boom extends. There have been echoes of the inflection point in the 1990s boom lately,\" the bank wrote, adding that the AI trade now looked the way tech stocks did in 1997, several years before the bubble burst.\n\nWilson and Chang flagged several warning signs leading up to the dot-com crash in the early 2000s that investors should be on the lookout for.\n\nInvestment spending on tech equipment and software rose to \"unusually high levels\" in the 90s. That peaked in 2000, when non-residential investment in the telecom and tech sector rose to around 15% of US GDP.\n\nInvestment spending began to tumble in the months leading up to the dot-com crash, per Goldman's analysis.\n\n\"Highly valued asset prices thus had significant consequences for real spending decisions,\" the strategists said.\n\nInvestors have been growing wary of mega-cap tech firms' spending spree on AI this year. Amazon, Meta, Microsoft, Alphabet, and Apple are on track to spend around $349 billion on capex in 2025.\n\nCorporate profits reached a peak around 1997 before beginning to decline.\n\n\"Profitability peaked well before the boom ended,\" Wilson and Chang wrote. \"While reported profit margins were more robust, declining profitability in the macro data in the later years of the boom came alongside accelerating equity prices.\"\n\nCorporate profits look strong at the moment. The blended net profit margin in the S&P 500 for the third quarter is around 13.1%, above the five-year average of 12.1%, according to FactSet.\n\nCompanies grew increasingly indebted in the lead-up to the dot-com crash. Corporate debt as a percentage of profits peaked in 2001, right as the bubble was bursting, Goldman's analysis shows.\n\n\"The combination of rising investment and falling profitability pushed the corporate sector financial balance — the difference between savings and investment — into deficit,\" the strategists said.\n\nSome of mega-cap tech firms' spending on AI has been funded debt. Meta, for instance, raised $30 billion in bonds in late October as it doubled down on its AI spending plans.\n\nMost firms today, though, look like they're financing capex with free cash flow, Goldman added. The percentage of corporate debt relative to profits also looks significantly lower than it did at the peak of the internet bubble.\n\nThe Fed was in the midst of its rate-cutting cycle in the late 90s, one factor that helped juice the stock market.\n\n\"Lower rates and capital inflows added fuel to the equity market,\" Goldman wrote.\n\nThe Fed cut interest rates by 25 basis points at its October policy meeting. Investors are expecting the central bank to issue another 25 basis-point cut in December, according to the CME FedWatch tool.\n\nOther market pros, like Ray Dalio, have warned that the Fed's easing cycle could help inflate a bubble in markets.\n\nIn the 1990s, these warning signs appeared at least two years before the dot-com bubble actually burst, Wilson and Chang said, adding that they believed the AI trade still had room to run.\n\nThis story was originally published in November 2025.",
    "readingTime": 4,
    "keywords": [
      "goldman's analysis",
      "burst wilson",
      "corporate debt",
      "corporate profits",
      "warning signs",
      "mega-cap tech",
      "dot-com crash",
      "tech firms",
      "wilson and chang",
      "bubble burst"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/stock-market-bubble-warning-dot-com-crash-ai-stocks-goldman-2025-11",
    "thumbnail_url": "https://i.insider.com/691207c20a30027667eaff8b?width=1200&format=jpeg",
    "created_at": "2026-01-20T00:57:23.709Z",
    "topic": "finance"
  },
  {
    "slug": "young-workers-most-worried-about-ai-affecting-jobs-randstad-survey-shows",
    "title": "Young workers most worried about AI affecting jobs, Randstad survey shows",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/young-workers-most-worried-about-ai-affecting-jobs-randstad-survey-shows-4453847",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM0I18Z_L.jpg",
    "created_at": "2026-01-20T00:57:21.399Z",
    "topic": "finance"
  },
  {
    "slug": "rocket-doctor-ai-upsizes-private-placement-to-52-million",
    "title": "Rocket Doctor AI upsizes private placement to $5.2 million",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/company-news/rocket-doctor-ai-upsizes-private-placement-to-52-million-93CH-4453846",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/world_news_2_108x81.jpg",
    "created_at": "2026-01-20T00:57:21.379Z",
    "topic": "finance"
  },
  {
    "slug": "britain-needs-ai-stress-tests-for-financial-services-lawmakers-say",
    "title": "Britain needs ’AI stress tests’ for financial services, lawmakers say",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/britain-needs-ai-stress-tests-for-financial-services-lawmakers-say-4453850",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM0J002_L.jpg",
    "created_at": "2026-01-20T00:57:21.238Z",
    "topic": "finance"
  },
  {
    "slug": "aiassisted-feature-intake-with-human-review-n8n-workflow",
    "title": "AI-assisted feature intake with human review (n8n workflow)",
    "description": "This project is an AI-powered Feature Intake Engine that converts raw product ideas into reviewable, structured Jira-ready tickets, with a human-in-the-loop approval step.  It is designed for Produ...",
    "fullText": "kavishsekhri\n\n /\n\n AI-Feature-Intake-Engine\n\n Public\n\n This project is an AI-powered Feature Intake Engine that converts raw product ideas into reviewable, structured Jira-ready tickets, with a human-in-the-loop approval step. It is designed for Product, Engineering, and TPM teams who want speed without losing control.\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n kavishsekhri/AI-Feature-Intake-Engine",
    "readingTime": 1,
    "keywords": [
      "product"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/kavishsekhri/AI-Feature-Intake-Engine",
    "thumbnail_url": "https://opengraph.githubassets.com/f05e745da909e648e77bc78925f14c4bdec047ecd4dab0991fac2e6928467851/kavishsekhri/AI-Feature-Intake-Engine",
    "created_at": "2026-01-19T18:18:47.207Z",
    "topic": "tech"
  },
  {
    "slug": "a-broadening-playbook-wall-street-sees-stock-market-gains-beyond-tech",
    "title": "'A broadening playbook': Wall Street sees stock market gains beyond tech",
    "description": "Enthusiasm for artificial intelligence's prospects will continue to drive the market higher in 2026. The gains won't be limited to tech stocks, strategists say.",
    "fullText": "Investor enthusiasm over the prospects of artificial intelligence are expected to drive Big Tech stocks in particular this year — but Wall Street also expects gains in other sectors.\n\nOver the past two weeks, Industrials (XLI), Materials (XLB), Energy (XLE), and Consumer Staples (XLP) have outperformed the broader market, with all of those sectors up 5.5% or more.\n\nEven the small-cap Russell 2000 (^RUT) index has risen 8% since the start of the year, outperforming the S&P 500 (^GSPC), which is up more than 1% during the same period.\n\n\"It's a broadening playbook,\" Oppenheimer chief investment strategist John Stoltzfus told Yahoo Finance on Friday.\n\nStoltzfus' price target for the S&P 500 this year is the most bullish on Wall Street at 8,100, with other analysts expecting double-digit percentage gains taking the benchmark index toward 7,500 or 7,600.\n\n\"What we're seeing is a rotation, and it's not necessarily away from technology, as we would think,\" he added. \"It's some profit taking to broaden one's exposure and diversification.\"\n\nKeith Lerner, co-chief investment officer at Truist Advisory Services, agrees. His firm's strategy has been to go Overweight on industrials, while also upgrading healthcare and energy stocks.\n\n\"I wouldn't get rid of the tech at this point. I think there are more opportunities beyond tech, is the way I would think about it,\" Lerner said.\n\nStrategists are optimistic given what they've seen with earnings, which kicked off last week.\n\nGoldman Sachs (GS) and Morgan Stanley (MS) showed one of the strongest years for the investment banking business since the pandemic, sending shares of those firms higher.\n\nTaiwan Semiconductor Manufacturing Co.'s (TSM) strong results also gave a lift to semiconductor stocks and fueled optimism that the AI cycle is only accelerating\n\nShares of memory chipmaker Micron (MU) and equipment makers ASML (ASML) and Applied Materials (AMAT) are all up at least 25% year-to-date.\n\nPerformance in the AI chip space so far underscores that artificial intelligence and tech are expected to lead the market again this year as wider adoption across enterprises takes hold.\n\n\"As we wait for that to unfold, I think tech, especially big tech and AI as a theme, is the core of this market,\" Barclays head of US equity strategy Venu Krishna said. \"Our core conviction is that it'll sustain this year, even though the level of scrutiny around that [AI] has decidedly increased.\"\n\nThe test will be whether software stocks, facing disruption from artificial intelligence, can find their footing. Names like Microsoft (MSFT), Salesforce (CRM), and ServiceNow (NOW) have all fallen since the start of the year as investors weigh AI's impact.",
    "readingTime": 3,
    "keywords": [
      "artificial intelligence",
      "wall street",
      "stocks",
      "market",
      "it's",
      "investment",
      "gains",
      "sectors",
      "industrials",
      "materials"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/a-broadening-playbook-wall-street-sees-stock-market-gains-beyond-tech-160556466.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/awxweQvjpgGZvkcTjxU8Pw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2026-01/b7798ba0-f2f9-11f0-bdfd-6f91897d0d7a",
    "created_at": "2026-01-19T18:18:40.400Z",
    "topic": "finance"
  },
  {
    "slug": "ben-horowitz-says-ai-could-spark-a-postelectricity-leap-in-living-standards-but-risk-eroding-purpose",
    "title": "Ben Horowitz says AI could spark a post-electricity leap in living standards — but risk eroding purpose",
    "description": "Andreessen Horowitz cofounder Ben Horowitz said AI will boost life quality but warns a frictionless future could leave people searching for meaning.",
    "fullText": "Ben Horowitz believes artificial intelligence is about to reshape daily life as profoundly as electricity once did.\n\nSpeaking on a recent episode of the \"Ben & Marc Show,\" the Andreessen Horowitz cofounder framed AI as a once-in-a-century technological break.\n\n\"This is on the order of the steam engine or electricity,\" Horowitz said, adding that the technology is so powerful it will push society into \"a different world.\"\n\nBut while AI may deliver dramatic gains in living standards, Horowitz said it will also raise deeper questions about meaning, purpose, and what humans do with their time.\n\nHorowitz said AI will help fix problems humans have \"learned to live with,\" including cancer, transportation challenges, and large-scale fraud detection in the US.\n\nThe result, he believes, could be a broad-based improvement in quality of life that's difficult to fully imagine from the present vantage point.\n\n\"I think life — just the quality of life for everybody — is about to get way, way better than it's ever been,\" Horowitz said.\n\nHowever, Horowitz also struck a cautionary note. If AI removes too much friction from life — and too many traditional sources of struggle, work, and responsibility — humans may find themselves unmoored.\n\n\"The one thing with humans that's a little messed up,\" he said, is that if progress pushes people \"too far away from some grounded purpose,\" including shared beliefs or spiritual anchors, they may \"attach onto some dumb stuff.\"\n\nHorowitz's prediction echoes those voiced by other AI leaders.\n\nElon Musk, the CEO of Tesla and xAI, has said AI could usher in a future of \"universal high income,\" where work becomes optional, and abundance eliminates poverty, while Bill Gates, the Microsoft cofounder, has suggested AI may make radically shorter workweeks possible.\n\nOpenAI CEO Sam Altman and Anthropic CEO Dario Amodei have both acknowledged risks to meaning in a post-work world but remain optimistic that humans will adapt and find new sources of fulfillment.\n\nBeyond Silicon Valley's more optimistic visions, figures like Geoffrey Hinton, the computer scientist often called the \"godfather of AI,\" Stuart Russell, a professor of computer science at UC Berkeley, investor Howard Marks of Oaktree Capital Management, and AI researcher Eliezer Yudkowsky have warned of outcomes ranging from mass job losses and the erosion of meaning at work to, in the most extreme cases, threats to humanity's survival.",
    "readingTime": 2,
    "keywords": [
      "life",
      "humans",
      "horowitz",
      "electricity",
      "cofounder",
      "purpose",
      "quality",
      "that's",
      "optimistic",
      "computer"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ben-horowitz-ai-could-boost-living-standards-like-electricity-2026-1",
    "thumbnail_url": "https://i.insider.com/696e092aa645d118818794c5?width=1200&format=jpeg",
    "created_at": "2026-01-19T18:18:38.709Z",
    "topic": "finance"
  },
  {
    "slug": "big-short-investor-michael-burry-has-found-an-unlikely-ally-in-his-crusade-against-ai-hype-ben-affleck",
    "title": "'Big Short' investor Michael Burry has found an unlikely ally in his crusade against AI hype: Ben Affleck",
    "description": "Michael Burry of \"The Big Short\" said Ben Affleck was \"clearly a smart guy\" after he slammed AI chatbots and called out spending on data centers.",
    "fullText": "It rarely hurts to have Batman on your side. Michael Burry may have the next thing after Ben Affleck revealed he's also skeptical of the AI boom.\n\nThe Hollywood star, speaking on the latest episode of \"The Joe Rogan Experience\" podcast, echoed the investor's skepticism of grand claims around AI and Big Tech's massive investments in the nascent tech.\n\n\"I think a lot of that rhetoric comes from people who are trying to justify valuations around companies, where they go: 'We're going to change everything in two years, there's going to be no more work,'\" Affleck said.\n\n\"Well, the reason they're saying that is because they need to ascribe a valuation for investment that can warrant the capex spend they're going to make on these data centers,\" he added.\n\nThe \"Justice League\" and \"Gone Girl\" actor said that historical adoption of new technologies has been \"slow\" and incremental.\"\n\nHe said that tech companies are trying to justify their huge outlays by promising they'll enable new AI models that will blow away existing ones — but in reality, each subsequent model is only moderately better and requires far more electricity and data to run.\n\n\"Ben Affleck is clearly a smart guy,\" Burry wrote in a weekend post on X. \"So this does not surprise me. It sounds familiar and on point.\"\n\nBen Affleck is clearly a smart guy. So this does not surprise me. It sounds familiar and on point. Delivered much better than I ever could. https://t.co/X1VvE9yX2N\n\nAffleck backed up Burry's critique of AI's value on the podcast. He said the writing of today's AI chatbots is \"really shitty\" and \"not reliable.\"\n\n\"I just can't stand to see what it writes,\" said Affleck, who cowrote the Oscar-winning screenplay for \"Good Will Hunting\" and directed, produced, and starred in Best Picture winner \"Argo.\"\n\nHe predicted that filmmakers would use AI as a tool to save time and money — similar to visual effects — but it probably won't \"write anything meaningful\" or make entire movies.\n\nAffleck said that most AI users are using chatbots as virtual companions, so \"there's no work, there's no productivity, there's no value to it.\"\n\nHe also cast doubt on the social value of people engaging with AI friends who are \"telling you that you're great and listening to everything you say and being sycophantic.\"\n\nBurry has struck a similar tone on AI in recent weeks. He's best known for his prescient bet against the mid-2000s housing bubble, which was immortalized in Michael Lewis' book \"The Big Short.\" Christian Bale, Affleck's predecessor as Batman, portrayed him in the movie adaptation.\n\nAfter more than two years of virtual silence, Burry returned to X late last year with a flurry of warnings about a dangerous AI bubble. He also closed his hedge fund to outside cash, shifting his focus to writing financial analysis on Substack.\n\nBurry's central thesis is that AI stocks are overvalued. He's said they're overinvesting in microchips and data centers that will quickly become outdated, to power a tech that will become commoditized and won't yield a return for them, paving the way for large writedowns and sharp stock declines in the future.\n\nHe recently posted on X that \"return on investment will continue to fall, almost all AI companies will go bankrupt, and much of the AI spending will be written off.\"",
    "readingTime": 3,
    "keywords": [
      "clearly smart",
      "smart guy",
      "sounds familiar",
      "ben affleck",
      "there's",
      "he's",
      "they're",
      "batman",
      "podcast",
      "justify"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/big-short-michael-burry-ben-affleck-rogan-ai-bubble-skeptic-2026-1",
    "thumbnail_url": "https://i.insider.com/696e3a8ae1ba468a96aa4d2c?width=1200&format=jpeg",
    "created_at": "2026-01-19T18:18:38.321Z",
    "topic": "finance"
  },
  {
    "slug": "ai-is-creating-the-rick-rubins-of-silicon-valley",
    "title": "AI is creating the Rick Rubins of Silicon Valley",
    "description": "The rise of AI-native tools such as Claude Code is transforming the value and impact of  designers and taste-makers in Silicon Valley.",
    "fullText": "In Silicon Valley, AI is fueling the rise of the star individual contributor.\n\nJust take a look at what happened after The Browser Company was acquired by Anthropic. Josh Miller, CEO of this browser startup, recently described how Anthropic's Claude Code radically changed his hiring strategy.\n\nHis post gets at something many creative orgs have struggled to name: there's a huge, underserved group of very senior designer ICs who want to stay close to the work, coach others, and shape direction — without becoming full-time people managers. The \"Design Producer\" role he describes feels like a credible answer to that gap.\n\nAI-native tools such as Claude Code make this role newly viable. When designers can prototype, ship code, and explore ideas directly, the leverage of a senior IC shifts. Their value isn't headcount management. Instead, it's taste, judgment, and the ability to help others move faster and aim higher. Coaching becomes embedded in the work itself.\n\nThe record-label metaphor fits well here. Great music producers don't manage bands through org charts, they create the conditions for great work, give feedback at the right moments, and connect the right collaborators.\n\nFor experienced tech designers who've hit the ceiling of traditional IC tracks, this feels like a genuinely new lane, not a consolation prize.\n\nThink of this like a legion of nerdy Rick Rubins. Coincidentally, Rubin created a digital book last year with Anthropic called The Way of Code. Here's a passage from the book:\n\nThe Vibe Coder attends to\nthe inner, not the outer.\nHe allows things to come and go.\nHis heart is open as the sky.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 2,
    "keywords": [
      "senior",
      "others",
      "role",
      "designers",
      "book",
      "code",
      "browser",
      "anthropic",
      "claude"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/ai-rick-rubin-silicon-valley-anthropic-claude-code-2026-1",
    "thumbnail_url": "https://i.insider.com/696abf5ba645d11881878d9d?width=1200&format=jpeg",
    "created_at": "2026-01-19T18:18:38.179Z",
    "topic": "finance"
  },
  {
    "slug": "bosses-dont-think-ai-is-paying-off-yet-a-pwc-survey-of-4500-ceos-found",
    "title": "Bosses don't think AI is paying off yet, a PwC survey of 4,500 CEOs found",
    "description": "PwC's global CEO survey found that more than half of CEOs say they aren't seeing increased revenue or cost benefits from their investments in AI.",
    "fullText": "Everyone wants to know when AI investments will pay off.\n\nMost CEOs say they're still waiting, according to PwC's latest Global CEO survey, released on Monday to coincide with the start of Davos.\n\nThe consulting giant questioned 4,454 chief executives across 95 countries and territories about their strategic priorities and outlook in the year up to November 2025.\n\nMore than half of the CEOs surveyed, 56%, said AI hasn't produced revenue or cost benefits for their businesses to date.\n\nSome reported benefits for either revenue or costs: around a third said their revenue was up in the last year, and 26% said they were seeing lower costs from AI.\n\nBut only 12% said they had both decreased costs and increased revenue using AI in the last 12 months.\n\nRecent Morgan Stanley data on S&P 500 companies showed that certain sectors are seeing higher, measurable AI-driven returns than others. Technology, communication services, and financials topped the list, while energy companies were rising fast up the list.\n\n\"A small group of companies are already turning AI into measurable financial returns, while many others are still struggling to move beyond pilots,\" said Mohamed Kande, PwC's global chairman, in a press release.\n\n\"That gap is starting to show up in confidence and competitiveness—and it will widen quickly for those that don't act.\"\n\nEnsuring maximum returns from AI requires a balance of business strategy, strong underlying data architecture, and the right talent strategy. No matter how excited CEOs are about AI, they also have to get employees on board to see returns.\n\nA recent EY survey on how AI is being used at work found that companies are missing out on 40% of the AI productivity gains they could achieve with the right strategy.\n\nPwC found that the 12% of CEOs reporting both cost and revenue gains were two to three times more likely to have also built a strong AI foundation, meaning they have embedded AI extensively across products and services, demand generation, and strategic decision-making.\n\nAs they grapple with AI uncertainty and geopolitical volatility, CEOs told PwC they are less confident about their short-term growth outlook than they were last year.\n\nOnly 30% of those surveyed said they are very or extremely confident about revenue growth over the next 12 months, down from 38% in last year's report and a peak of 56% in 2022.\n\nThe strongest approach to dealing with future uncertainty was embracing reinvention, including through dealmaking and venturing into new sectors, PwC found.\n\nThe consulting giant found there was a \"strong association among a higher percentage of revenue coming from new sectors, bigger profit margins, and greater CEO confidence in company growth prospects.\"\n\n\"The companies that succeed will be those willing to make bold decisions and invest with conviction in the capabilities that matter most,\" said Kande.\n\nHave a tip? Contact this reporter via email at pthompson@businessinsider.com or Signal at Polly_Thompson.89. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "consulting giant",
      "revenue",
      "returns",
      "sectors",
      "strategy",
      "growth",
      "survey",
      "across",
      "strategic",
      "outlook"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/pwc-global-ceo-survey-is-ai-investment-delivering-financial-returns-2026-1",
    "thumbnail_url": "https://i.insider.com/696e4842c58df2ecd5ccc27b?width=1200&format=jpeg",
    "created_at": "2026-01-19T18:18:38.013Z",
    "topic": "finance"
  },
  {
    "slug": "trust-ai-enough-to-bet-your-retirement-you-said-no-thanks",
    "title": "Trust AI enough to bet your retirement? You said, 'No thanks'",
    "description": "Elon Musk suggested you don't need to bother saving for a 401 (k) because of AI. Business Insider's readers weren't on board.",
    "fullText": "Earlier this month, Elon Musk suggested that people don't need to sock away funds in their 401 (k) anymore because AI will make the scarcity of money a thing of the past.\n\n\"If any of the things that we've said are true, saving for retirement will be irrelevant,\" Musk said of the predicted AI revolution during a recent podcast episode.\n\nHe speculated that AI could transform society, creating an abundance of resources that would grant everyone a \"universal high income\" and the ability to have \"whatever stuff they want.\"\n\nOur readers don't seem to be as sold on that vision of what he characterized as the \"good future.\"\n\nBusiness Insider Today's Dan Defrancesco asked newsletter readers whether they trust AI enough to stop saving, and the results were pretty emphatic.\n\nOf the roughly 200 readers who responded, just 6% said they're not worried about retirement and that they'd let AI handle the future.\n\nMeanwhile, roughly 94% said they would stick with their savings plans rather than bet the farm that an AI revolution would make it moot.\n\nThat's the smart move, financial and technology experts previously told Business Insider.\n\nSeven retirement and AI gurus we spoke to said Americans should be spending more on retirement, not less.\n\n\"Most Americans should absolutely ignore these comments,\" said Geoffrey Sanzenbacher, a research fellow at Boston College's Center for Retirement Research (CRR). \"Musk's speculation sends a dangerous and misleading message.\"\n\nOthers said that technological revolutions haven't boosted wealth equally across society in the past, and that a potential universal basic income — like the kind Musk is suggesting — would take a coordinated effort from government, not tech leaders.\n\n\"That is not a technological problem — it is a coordination problem at the scale of civilization,\" said John Nosta, an innovation theorist and the founder of NostaLab.",
    "readingTime": 2,
    "keywords": [
      "business insider",
      "readers",
      "don't",
      "saving",
      "revolution",
      "society",
      "universal",
      "income",
      "roughly",
      "technological"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/trust-ai-with-retirement-401k-survey-results-elon-musk-2026-1",
    "thumbnail_url": "https://i.insider.com/696e66a2a645d118818796f5?width=1200&format=jpeg",
    "created_at": "2026-01-19T18:18:38.002Z",
    "topic": "finance"
  },
  {
    "slug": "valve-updates-ai-disclosure-guidelines-to-allow-for-aipowered-tools",
    "title": "Valve Updates AI Disclosure Guidelines To Allow For AI-Powered Tools",
    "description": "Valve has made changes to its AI-disclosure guidelines, removing the need for studios to disclose whether or not games have been developed with AI-powered tools and putting more emphasis on AI-generated assets.\nThe change, which was pointed out by Simon Carless on LinkedIn, suggests that Valve is no longer concerned by the use of AI tools that assist development, stating, \"Efficiency gains through the use of [AI-powered dev tools] is not the focus of this section.\" These tools could included a variety of things, such as AI-generated transcripts of meetings to code helpers that have become prevalent in most programming environments.\nValve states the the aim of its disclosure policy is to inform players when AI is used to generate content, from marketing and conceptual assets to in-game ones that players will interact with. Developers are able to specify what assets have been generated and indicate, via a single checkbox, whether or not players will interact with AI-generated content during gameplay, be it images, audio, or other content.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/valve-updates-ai-disclosure-guidelines-to-allow-for-ai-powered-tools/1100-6537483/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1585/15853545/4637028-7297126222-arc-r.jpg",
    "created_at": "2026-01-19T18:18:34.839Z",
    "topic": "gaming"
  },
  {
    "slug": "kuse-cowork-an-open-source-byok-alternative-to-claude-cowork",
    "title": "Kuse Cowork – An open source, BYOK alternative to Claude Cowork",
    "description": "Open-source Alternative for Claude Code Desktop App - kuse-ai/kuse_cowork",
    "fullText": "kuse-ai\n\n /\n\n kuse_cowork\n\n Public\n\n Open-source Alternative for Claude Code Desktop App\n\n License\n\n MIT license\n\n 83\n stars\n\n 12\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n kuse-ai/kuse_cowork",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/kuse-ai/kuse_cowork",
    "thumbnail_url": "https://opengraph.githubassets.com/a2c54511b6b2934c95f3f166f114528798fc1b2eb5205273cc9f287df6ab0f1c/kuse-ai/kuse_cowork",
    "created_at": "2026-01-19T12:27:20.869Z",
    "topic": "tech"
  },
  {
    "slug": "fasttopkbatched-highperformance-batched-topk-selection-for-cpu-inference",
    "title": "Fast_topk_batched: High-performance batched Top-K selection for CPU inference",
    "description": "High-performance batched Top-K selection for CPU inference. Up to 80x faster than PyTorch, optimized for LLM sampling with AVX2 SIMD. - RAZZULLIX/fast_topk_batched",
    "fullText": "RAZZULLIX\n\n /\n\n fast_topk_batched\n\n Public\n\n High-performance batched Top-K selection for CPU inference. Up to 80x faster than PyTorch, optimized for LLM sampling with AVX2 SIMD.\n\n License\n\n MIT license\n\n 4\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n RAZZULLIX/fast_topk_batched",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/RAZZULLIX/fast_topk_batched",
    "thumbnail_url": "https://opengraph.githubassets.com/16dbb1cdc8345f8154f62c3563e6c55e0654762121550ffcdfd2712783d6229b/RAZZULLIX/fast_topk_batched",
    "created_at": "2026-01-19T12:27:20.754Z",
    "topic": "tech"
  },
  {
    "slug": "ai-companies-will-fail-we-can-salvage-something-from-the-wreckage-cory-doctorow",
    "title": "AI companies will fail. We can salvage something from the wreckage | Cory Doctorow",
    "description": "AI is asbestos in the walls of our tech society, stuffed there by monopolists run amok. A serious fight against it must strike at its roots\nI am a science-fiction writer, which means that my job is to make up futuristic parables about our current techno-social arrangements to interrogate not just what a gadget does, but who it does it for, and who it does it to.\nWhat I do not do is predict the future. No one can predict the future, which is a good thing, since if the future were predictable, that would mean we couldn’t change it.\n Continue reading...",
    "fullText": "I am a science-fiction writer, which means that my job is to make up futuristic parables about our current techno-social arrangements to interrogate not just what a gadget does, but who it does it for, and who it does it to.\n\nWhat I do not do is predict the future. No one can predict the future, which is a good thing, since if the future were predictable, that would mean we couldn’t change it.\n\nNow, not everyone understands the distinction. They think science-fiction writers are oracles. Even some of my colleagues labor under the delusion that we can “see the future”.\n\nThen there are science-fiction fans who believe that they are reading the future. A depressing number of those people appear to have become AI bros. These guys can’t shut up about the day that their spicy autocomplete machine will wake up and turn us all into paperclips has led many confused journalists and conference organizers to try to get me to comment on the future of AI.\n\nThat’s something I used to strenuously resist doing, because I wasted two years of my life explaining patiently and repeatedly why I thought crypto was stupid, and getting relentlessly bollocked by cryptocurrency cultists who at first insisted that I just didn’t understand crypto. And then, when I made it clear that I did understand crypto, they insisted that I must be a paid shill.\n\nThis is literally what happens when you argue with Scientologists, and life is just too short. That said, people would not stop asking – so I’m going to explain what I think about AI and how to be a good AI critic. By which I mean: “How to be a critic whose criticism inflicts maximum damage on the parts of AI that are doing the most harm.”\n\nIn automation theory, a “centaur” is a person who is assisted by a machine. Driving a car makes you a centaur, and so does using autocomplete.\n\nA reverse centaur is a machine head on a human body, a person who is serving as a squishy meat appendage for an uncaring machine.\n\nFor example, an Amazon delivery driver, who sits in a cabin surrounded by AI cameras that monitor the driver’s eyes and take points off if the driver looks in a proscribed direction, and monitors the driver’s mouth because singing is not allowed on the job, and rats the driver out to the boss if they do not make quota.\n\nThe driver is in that van because the van cannot drive itself and cannot get a parcel from the curb to your porch. The driver is a peripheral for a van, and the van drives the driver, at superhuman speed, demanding superhuman endurance.\n\nObviously, it’s nice to be a centaur, and it’s horrible to be a reverse centaur. There are lots of AI tools that are potentially very centaurlike, but my thesis is that these tools are created and funded for the express purpose of creating reverse centaurs, which none of us want to be.\n\nBut like I said, the job of a science-fiction writer is to do more than think about what the gadget does, and drill down on who the gadget does it for and who the gadget does it to. Tech bosses want us to believe that there is only one way a technology can be used. Mark Zuckerberg wants you to think that it is technologically impossible to have a conversation with a friend without him listening in. Tim Cook wants you to think that it is impossible for you to have a reliable computing experience unless he gets a veto over which software you install and without him taking 30 cents out of every dollar you spend. Sundar Pichai wants you to think that it is for you to find a webpage unless he gets to spy on you from asshole to appetite.\n\nThis is all a kind of vulgar Thatcherism. Margaret Thatcher’s mantra was: “There is no alternative.” She repeated this so often they called her “Tina” Thatcher: There. Is. No. Alternative.\n\n“There is no alternative” is a cheap rhetorical slight. It’s a demand dressed up as an observation. “There is no alternative” means: “stop trying to think of an alternative.”\n\nI’m a science-fiction writer – my job is to think of a dozen alternatives before breakfast.\n\nSo let me explain what I think is going on here with this AI bubble and who the reverse-centaur army is serving, and sort out the bullshit from the material reality.\n\nStart with monopolies: tech companies are gigantic and they don’t compete, they just take over whole sectors, either on their own or in cartels.\n\nGoogle and Meta control the ad market. Google and Apple control the mobile market, and Google pays Apple more than $20bn a year not to make a competing search engine, and of course, Google has a 90% search market share.\n\nNow, you would think that this was good news for the tech companies, owning their whole sector.\n\nBut it’s actually a crisis. You see, when a company is growing, it is a “growth stock”, and investors really like growth stocks. When you buy a share in a growth stock, you are making a bet that it will continue to grow. So growth stocks trade at a huge multiple of their earnings. This is called the “price to earnings ratio” or “PE ratio”.\n\nBut once a company stops growing, it is a “mature” stock, and it trades at a much lower PE ratio. So for every dollar that Target – a mature company – brings in, it is worth $10. It has a PE ratio of 10, while Amazon has a PE ratio of 36, which means that for every dollar Amazon brings in, the market values it at $36.\n\nIt’s wonderful to run a company that has a growth stock. Your shares are as good as money. If you want to buy another company or hire a key worker, you can offer stock instead of cash. And stock is very easy for companies to get, because shares are manufactured right there on the premises, all you have to do is type some zeros into a spreadsheet, while dollars are much harder to come by. A company can only get dollars from customers or creditors.\n\nSo when Amazon bids against Target for a key acquisition or a key hire, Amazon can bid with shares they make by typing zeros into a spreadsheet, and Target can only bid with dollars they get from selling stuff to us or taking out loans, which is why Amazon generally wins those bidding wars.\n\nThat’s the upside of having a growth stock. But here is the downside: eventually a company has to stop growing. Like, say you get a 90% market share in your sector, how are you going to grow?\n\nIf you are an exec at a dominant company with a growth stock, you have to live in constant fear that the market will decide that you are not likely to grow any further. Think of what happened to Facebook in the first quarter of 2022. They told investors that they experienced slightly slower growth in the US than they had anticipated, and investors panicked. They staged a one-day, $240bn sell-off. A quarter-trillion dollars in 24 hours! At the time, it was the largest, most precipitous drop in corporate valuation in human history.\n\nThat’s a monopolist’s worst nightmare, because once you’re presiding over a “mature” firm, the key employees you have been compensating with stock experience a precipitous pay drop and bolt for the exits, so you lose the people who might help you grow again, and you can only hire their replacements with dollars – not shares.\n\nThis is the paradox of the growth stock. While you are growing to domination, the market loves you, but once you achieve dominance, the market lops 75% or more off your value in a single stroke if they do not trust your pricing power.\n\nWhich is why growth-stock companies are always desperately pumping up one bubble or another, spending billions to hype the pivot to video or cryptocurrency or NFTs or the metaverse or AI.\n\nI am not saying that tech bosses are making bets they do not plan on winning. But winning the bet – creating a viable metaverse – is the secondary goal. The primary goal is to keep the market convinced that your company will continue to grow, and to remain convinced until the next bubble comes along.\n\nSo this is why they’re hyping AI: the material basis for the hundreds of billions in AI investment.\n\nNow I want to talk about how they’re selling AI. The growth narrative of AI is that AI will disrupt labor markets. I use “disrupt” here in its most disreputable tech-bro sense.\n\nThe promise of AI – the promise AI companies make to investors – is that there will be AI that can do your job, and when your boss fires you and replaces you with AI, he will keep half of your salary for himself and give the other half to the AI company.\n\nThat is the $13tn growth story that Morgan Stanley is telling. It’s why big investors are giving AI companies hundreds of billions of dollars. And because they are piling in, normies are also getting sucked in, risking their retirement savings and their family’s financial security.\n\nNow, if AI could do your job, this would still be a problem. We would have to figure out what to do with all these unemployed people.\n\nBut AI can’t do your job. It can help you do your job, but that does not mean it is going to save anyone money.\n\nTake radiology: there is some evidence that AI can sometimes identify solid-mass tumors that some radiologists miss. Look, I’ve got cancer. Thankfully, it’s very treatable, but I’ve got an interest in radiology being as reliable and accurate as possible.\n\nLet’s say my hospital bought some AI radiology tools and told its radiologists: “Hey folks, here’s the deal. Today, you’re processing about 100 X-rays per day. From now on, we’re going to get an instantaneous second opinion from the AI, and if the AI thinks you’ve missed a tumor, we want you to go back and have another look, even if that means you’re only processing 98 X-rays per day. That’s fine, we just care about finding all those tumors.”\n\nIf that’s what they said, I’d be delighted. But no one is investing hundreds of billions in AI companies because they think AI will make radiology more expensive, not even if that also makes radiology more accurate. The market’s bet on AI is that an AI salesman will visit the CEO of Kaiser and make this pitch: “Look, you fire nine out of 10 of your radiologists, saving $20m a year. You give us $10m a year, and you net $10m a year, and the remaining radiologists’ job will be to oversee the diagnoses the AI makes at superhuman speed – and somehow remain vigilant as they do so, despite the fact that the AI is usually right, except when it’s catastrophically wrong.\n\n“And if the AI misses a tumor, this will be the human radiologist’s fault, because they are the ‘human in the loop’. It’s their signature on the diagnosis.”\n\nThis is a reverse centaur, and it is a specific kind of reverse centaur: it is what Dan Davies calls an “accountability sink”. The radiologist’s job is not really to oversee the AI’s work, it is to take the blame for the AI’s mistakes.\n\nThis is another key to understanding – and thus deflating – the AI bubble. The AI can’t do your job, but an AI salesman can convince your boss to fire you and replace you with an AI that can’t do your job. This is key because it helps us build the kinds of coalitions that will be successful in the fight against the AI bubble.\n\nIf you are someone who is worried about cancer, and you are being told that the price of making radiology too cheap to meter, is that we are going to have to rehouse America’s 32,000 radiologists, with the trade-off that no one will ever be denied radiology services again, you might say: “Well, OK, I’m sorry for those radiologists, and I fully support getting them job training or UBI or whatever. But the point of radiology is to fight cancer, not to pay radiologists, so I know what side I’m on.”\n\nAI hucksters and their customers in the C-suites want the public on their side. They want to forge a class alliance between AI deployers and the people who enjoy the fruits of the reverse centaurs’ labor. They want us to think of ourselves as enemies to the workers.\n\nNow, some people will be on the workers’ side because of politics or aesthetics. But if you want to win over all the people who benefit from your labor, you need to understand and stress how the products of the AI will be substandard. That they are going to get charged more for worse things. That they have a shared material interest with you.\n\nWill those products be substandard? There is every reason to think so.\n\nThink of AI software generation: there are plenty of coders who love using AI. Using AI for simple tasks can genuinely make them more efficient and give them more time to do the fun part of coding, namely, solving really gnarly, abstract puzzles. But when you listen to business leaders talk about their AI plans for coders, it’s clear they are not hoping to make some centaurs.\n\nThey want to fire a lot of tech workers – 500,000 over the past three years – and make the rest pick up their work with coding, which is only possible if you let the AI do all the gnarly, creative problem solving, and then you do the most boring, soul-crushing part of the job: reviewing the AI’s code.\n\nAnd because AI is just a word-guessing program, because all it does is calculate the most probable word to go next, the errors it makes are especially subtle and hard to spot, because these bugs are nearly indistinguishable from working code.\n\nFor example: programmers routinely use standard “code libraries” to handle routine tasks. Say you want your program to slurp in a document and make some kind of sense of it – find all the addresses, say, or all the credit card numbers. Rather than writing a program to break down a document into its constituent parts, you’ll just grab a library that does it for you.\n\nThese libraries come in families, and they have predictable names. If it’s a library for pulling in an html file, it might be called something like lib.html.text.parsing; and if it’s a for docx file, it’ll be lib.docx.text.parsing.\n\nBut reality is messy, humans are inattentive and stuff goes wrong, so sometimes, there will be another library, say, one for parsing pdfs, and instead of being called lib.pdf.text.parsing, it’s called lib.text.pdf.parsing. Someone just entered an incorrect library name and it stuck. Like I said, the world is messy.\n\nNow, AI is a statistical inference engine. All it can do is predict what word will come next based on all the words that have been typed in the past. That means that it will “hallucinate” a library called lib.pdf.text.parsing, because that matches the pattern it’s already seen. And the thing is, malicious hackers know that the AI will make this error, so they will go out and create a library with the predictable, hallucinated name, and that library will get automatically sucked into the AI’s program, and it will do things like steal user data or try to penetrate other computers on the same network.\n\nAnd you, the human in the loop – the reverse centaur – you have to spot this subtle, hard-to-find error, this bug that is indistinguishable from correct code. Now, maybe a senior coder could catch this, because they have been around the block a few times, and they know about this tripwire.\n\nBut guess who tech bosses want to preferentially fire and replace with AI? Senior coders. Those mouthy, entitled, extremely highly paid workers, who don’t think of themselves as workers. Who see themselves as founders in waiting, peers of the company’s top management. The kind of coder who would lead a walkout over the company building drone-targeting systems for the Pentagon, which cost Google $10bn in 2018.\n\nFor AI to be valuable, it has to replace high-wage workers, and those are precisely the workers who might spot some of those statistically camouflaged AI errors.\n\nIf you can replace coders with AI, who can’t you replace with AI? Firing coders is an ad for AI.\n\nWhich brings me to AI art – or “art” – which is often used as an ad for AI, even though it is not part of AI’s business model.\n\nLet me explain: on average, illustrators do not make any money. They are already one of the most immiserated, precarious groups of workers out there. If AI image-generators put every illustrator working today out of a job, the resulting wage-bill savings would be undetectable as a proportion of all the costs associated with training and operating image-generators. The total wage bill for commercial illustrators is less than the kombucha bill for the company cafeteria at just one of OpenAI’s campuses.\n\nThe purpose of AI art – and the story of AI art as a death knell for artists – is to convince the broad public that AI is amazing and will do amazing things. It is to create buzz. Which is not to say that it is not disgusting that former OpenAI CTO Mira Murati told a conference audience that “some creative jobs shouldn’t have been there in the first place”.\n\nIt’s supposed to be disgusting. It’s supposed to get artists to run around and say: “The AI can do my job, and it’s going to steal my job, and isn’t that terrible?“\n\nBut can AI do an illustrator’s job? Or any artist’s job?\n\nLet’s think about that for a second. I have been a working artist since I was 17 years old, when I sold my first short story. Here’s what I think art is: it starts with an artist, who has some vast, complex, numinous, irreducible feeling in their mind. And the artist infuses that feeling into some artistic medium. They make a song, a poem, a painting, a drawing, a dance, a book or a photograph. And the idea is, when you experience this work, a facsimile of the big, numinous, irreducible feeling will materialize in your mind.\n\nBut the image-generation program does not know anything about your big, numinous, irreducible feeling. The only thing it knows is whatever you put into your prompt, and those few sentences are diluted across a million pixels or a hundred-thousand words, so that the average communicative density of the resulting work is indistinguishable from zero.\n\nIt is possible to infuse more communicative intent into a work: writing more detailed prompts, or doing the selective work of choosing from among many variants, or directly tinkering with the AI image after the fact, with a paintbrush or Photoshop or the Gimp. And if there will ever be a piece of AI art that is good art – as opposed to merely striking, interesting or an example of good draftsmanship – it will be thanks to those additional infusions of creative intent by a human.\n\nAnd in the meantime, it’s bad art. It’s bad art in the sense of being “eerie”, the word that cultural theorist Mark Fisher used to describe “when there is something present where there should be nothing, or there is nothing present when there should be something”.\n\nAI art is eerie because it seems like there is an intender and an intention behind every word and every pixel, because we have a lifetime of experience that tells us that paintings have painters, and writing has writers. But it is missing something. It has nothing to say, or whatever it has to say is so diluted that it is undetectable.\n\nWe should not simply shrug our shoulders and accept Thatcherism’s fatalism: “There is no alternative.”\n\nSo what is the alternative? A lot of artists and their allies think they have an answer: they say we should extend copyright to cover the activities associated with training a model.\n\nAnd I am here to tell you they are wrong. Wrong because this would represent a massive expansion of copyright over activities that are currently permitted – for good reason. I’ll explain:\n\nAI training involves scraping a bunch of webpages, which is unambiguously legal under present copyright law. Next, you perform analysis on those works. Basically, you count stuff on them: count pixels and their colors and proximity to other pixels; or count words. This is obviously not something you need a license for.\n\nAnd after you count all the pixels or the words, it is time for the final step: publishing them. Because that is what a model is: a literary work (that is, a piece of software) that embodies a bunch of facts about a bunch of other works, word and pixel distribution information, encoded in a multidimensional array.\n\nAnd again, copyright absolutely does not prohibit you from publishing facts about copyrighted works. And again, no one should want to live in a world where someone else gets to decide which factual statements you can publish.\n\nBut hey, maybe you think this is all sophistry. Maybe you think I’m full of shit. That’s fine. It wouldn’t be the first time someone thought that.\n\nAfter all, even if I’m right about how copyright works now, there’s no reason we couldn’t change copyright to ban training activities, and maybe there’s even a clever way to wordsmith the law so that it only catches bad things we don’t like, and not all the good stuff that comes from scraping, analyzing and publishing – such as search engines and academic scholarship.\n\nWell, even then, you’re not gonna help out creators by creating this new copyright. We have monotonically expanded copyright since 1976, so that today, copyright covers more kinds of works, grants exclusive rights over more uses, and lasts longer.\n\nAnd today, the media industry is larger and more profitable than it has ever been, and also – the share of media industry income that goes to creative workers is lower than it has ever been, both in real terms, and as a proportion of those incredible gains made by creators’ bosses at the media company.\n\nIn a creative market dominated by five publishers, four studios, three labels, two mobile app stores, and a single company that controls all the ebooks and audiobooks, giving a creative worker extra rights to bargain with is like giving your bullied kid more lunch money.\n\nIt doesn’t matter how much lunch money you give the kid, the bullies will take it all. Give that kid enough money and the bullies will hire an agency to run a global campaign proclaiming: “Think of the hungry kids! Give them more lunch money!”\n\nCreative workers who cheer on lawsuits by the big studios and labels need to remember the first rule of class warfare: things that are good for your boss are rarely what’s good for you.\n\nA new copyright to train models will not get us a world where models are not used to destroy artists, it will just get us a world where the standard contracts of the handful of companies that control all creative labor markets are updated to require us to hand over those new training rights to those companies. Demanding a new copyright just makes you a useful idiot for your boss.\n\nWhen really what they’re demanding is a world where 30% of the investment capital of the AI companies go into their shareholders’ pockets. When an artist is being devoured by rapacious monopolies, does it matter how they divvy up the meal?\n\nWe need to protect artists from AI predation, not just create a new way for artists to be mad about their impoverishment.\n\nIncredibly enough, there is a really simple way to do that. After more than 20 years of being consistently wrong and terrible for artists’ rights, the US Copyright Office has finally done something gloriously, wonderfully right. All through this AI bubble, the Copyright Office has maintained – correctly – that AI-generated works cannot be copyrighted, because copyright is exclusively for humans. That is why the “monkey selfie” is in the public domain. Copyright is only awarded to works of human creative expression that are fixed in a tangible medium.\n\nAnd not only has the Copyright Office taken this position, they have defended it vigorously in court, repeatedly winning judgments to uphold this principle.\n\nThe fact that every AI-created work is in the public domain means that if Getty or Disney or Universal or Hearst newspapers use AI to generate works – then anyone else can take those works, copy them, sell them or give them away for nothing. And the only thing those companies hate more than paying creative workers, is having other people take their stuff without permission.\n\nThe US Copyright Office’s position means that the only way these companies can get a copyright is to pay humans to do creative work. This is a recipe for centaurhood. If you are a visual artist or writer who uses prompts to come up with ideas or variations, that’s no problem, because the ultimate work comes from you. And if you are a video editor who uses deepfakes to change the eyelines of 200 extras in a crowd scene, then sure, those eyeballs are in the public domain, but the movie stays copyrighted.\n\nBut creative workers do not have to rely on the US government to rescue us from AI predators. We can do it ourselves, the way the writers did in their historic writers’ strike. The writers brought the studios to their knees. They did it because they are organized and solidaristic, but also are allowed to do something that virtually no other workers are allowed to do: they can engage in “sectoral bargaining”, whereby all the workers in a sector can negotiate a contract with every employer in the sector.\n\nThat has been illegal for most workers since the late 1940s, when the Taft-Hartley Act outlawed it. If we are gonna campaign to get a new law passed in hopes of making more money and having more control over our labor, we should campaign to restore sectoral bargaining, not to expand copyright.\n\nAI is a bubble and bubbles are terrible.\n\nBubbles transfer the life savings of normal people who are just trying to have a dignified retirement to the wealthiest and most unethical people in our society, and every bubble eventually bursts, taking their savings with it.\n\nBut not every bubble is created equal. Some bubbles leave behind something productive. Worldcom stole billions from everyday people by defrauding them about orders for fiber optic cables. The CEO went to prison and died there. But the fiber outlived him. It’s still in the ground. At my home, I’ve got 2gb symmetrical fiber, because AT&T lit up some of that old Worldcom dark fiber.\n\nIt would have been better if Worldcom had not ever existed, but the only thing worse than Worldcom committing all that ghastly fraud would be if there was nothing to salvage from the wreckage.\n\nI do not think we will salvage much from cryptocurrency, for example. When crypto dies, what it will leave behind is bad Austrian economics and worse monkey jpegs.\n\nAI is a bubble and it will burst. Most of the companies will fail. Most of the datacenters will be shuttered or sold for parts. So what will be left behind?\n\nWe will have a bunch of coders who are really good at applied statistics. We will have a lot of cheap GPUs, which will be good news for, say, effects artists and climate scientists, who will be able to buy that critical hardware at pennies on the dollar. And we will have the open-source models that run on commodity hardware, AI tools that can do a lot of useful stuff, like transcribing audio and video; describing images; summarizing documents; and automating a lot of labor-intensive graphic editing – such as removing backgrounds or airbrushing passersby out of photos. These will run on our laptops and phones, and open-source hackers will find ways to push them to do things their makers never dreamed of.\n\nIf there had never been an AI bubble, if all this stuff arose merely because computer scientists and product managers noodled around for a few years coming up with cool new apps, most people would have been pleasantly surprised with these interesting new things their computers could do. We would call them “plugins”.\n\nIt’s the bubble that sucks, not these applications. The bubble doesn’t want cheap useful things. It wants expensive, “disruptive” things: big foundation models that lose billions of dollars every year.\n\nWhen the AI-investment mania halts, most of those models are going to disappear, because it just won’t be economical to keep the datacenters running. As Stein’s law has it: “Anything that can’t go on forever eventually stops.”\n\nThe collapse of the AI bubble is going to be ugly. Seven AI companies currently account for more than a third of the stock market, and they endlessly pass around the same $100bn IOU.\n\nAI is the asbestos in the walls of our technological society, stuffed there with wild abandon by a finance sector and tech monopolists run amok. We will be excavating it for a generation or more.\n\nTo pop the bubble, we have to hammer on the forces that created the bubble: the myth that AI can do your job, especially if you get high wages that your boss can claw back; the understanding that growth companies need a succession of ever more outlandish bubbles to stay alive; the fact that workers and the public they serve are on one side of this fight, and bosses and their investors are on the other side.\n\nBecause the AI bubble really is very bad news, it’s worth fighting seriously, and a serious fight against AI strikes at its roots: the material factors fueling the hundreds of billions in wasted capital that are being spent to put us all on the breadline and fill all our walls with hi-tech asbestos.\n\nCory Doctorow is a science fiction author, activist and journalist. He is the author of dozens of books, most recently Enshittification: Why Everything Suddenly Got Worse and What To Do About It. This essay was adapted from a recent lecture about his forthcoming book, The Reverse Centaur’s Guide to Life After AI, which is out in June\n\nSpot illustrations by Brian Scagnelli",
    "readingTime": 26,
    "keywords": [
      "x-rays per",
      "that’s fine",
      "sectoral bargaining",
      "superhuman speed",
      "numinous irreducible",
      "media industry",
      "understand crypto",
      "labor markets",
      "lunch money",
      "tech bosses"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur",
    "thumbnail_url": "https://i.guim.co.uk/img/media/3cab6e38d4ff57c0631ad4532131aa15878f8386/284_824_2216_1773/master/2216.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=35399f4f1d76c88a6b1cc65f4fc711f5",
    "created_at": "2026-01-19T12:27:06.182Z",
    "topic": "tech"
  },
  {
    "slug": "bernie-sanderss-ai-data-center-pause-isnt-catching-on-in-congress-yet",
    "title": "Bernie Sanders's AI data center pause isn't catching on in Congress yet",
    "description": "\"In terms of the actual policy prescription, it's something that I haven't made a determination yet on,\" AOC told Business Insider.",
    "fullText": "Sen. Bernie Sanders wants a nationwide pause on AI data center construction. Few of his colleagues are willing to go there yet.\n\n\"In terms of the actual policy prescription, it's something that I haven't made a determination yet on,\" Democratic Rep. Alexandria Ocasio-Cortez of New York told Business Insider.\n\nThe Vermont senator and two-time presidential candidate first made the call in December, saying in a video posted to X that a national moratorium would \"give democracy a chance to catch up with the transformative changes\" that the technology is bringing.\n\nSanders told Business Insider last week that he plans to introduce legislation to back up his call soon.\n\n\"There's enormous issues for our economy and for our democracy that have got to be dealt with,\" Sanders said. \"And I feel we're not ready to do that.\"\n\nBut his proposal is unlikely to become reality, given GOP control of Washington.\n\n\"I mean, this Congress isn't going to do that,\" Democratic Rep. Mark Pocan of Wisconsin told Business Insider. \"He's pointing out the right problems.\"\n\nRepublicans have been generally dismissive of Sanders's idea. Even Sen. Josh Hawley of Missouri, a major critic of the AI industry, signaled he wouldn't go as far.\n\n\"If these AI companies want to build these data centers and local folks want to give them permits for it, I mean, that's up to local voters,\" Hawley told Business Insider.\n\nA handful of Sanders's progressive allies are backing him up. Rep. Ilhan Omar of Minnesota told Business Insider that a moratorium \"would be a good idea,\" while Rep. Rashida Tlaib of Michigan issued a full-throated endorsement in a post on X.\n\n\"I fully support the call for a national AI data center moratorium,\" Tlaib wrote.\n\nAnd while few Democrats have fully endorsed the idea, it's spurring a conversation within the party about the impact of data centers and AI.\n\n\"I don't want a moratorium on data centers but Bernie is right that we are sleepwalking past risks which alarm even the optimists,\" Democratic Sen. Brian Schatz of Hawaii wrote on X last month.\n\n\"What we do know is that these AI data centers are just uncontrollably jacking up people's energy costs,\" Ocasio-Cortez told Business Insider. \"There are a lot of problems that arise from these data centers, and I think that they certainly should not be getting the blank check from Congress.\"\n\n\"Bernie makes clear that the debate over AI is not about states rights or affordability,\" White House Crypto and AI Czar David Sacks wrote on X in December. \"He would block new data centers even if states want them & they generate their own power. It's about stopping progress completely so China wins the AI race.\"\n\nYet even the administration has come to recognize that data centers are engendering local resistance, in large part due to rising electricity costs.\n\nLast week, President Donald Trump declared that he wanted Big Tech firms to \"pay their own way\" on data centers, with AI companies footing more of the bill for the electricity their facilities consume.\n\n\"I never want Americans to pay higher Electricity bills because of Data Centers,\" Trump wrote on Truth Social.\n\nAnd on Friday, the administration and a group of governors called on a major power grid operator to hold a new emergency power auction amid rising costs driven by AI data centers.",
    "readingTime": 3,
    "keywords": [
      "democratic rep",
      "business insider",
      "moratorium",
      "it's",
      "idea",
      "centers",
      "electricity",
      "ocasio-cortez",
      "democracy",
      "sanders's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/bernie-sanders-ai-data-center-moratorium-reaction-2026-1",
    "thumbnail_url": "https://i.insider.com/6966aee9764ca5f34d2a609b?width=1200&format=jpeg",
    "created_at": "2026-01-19T12:27:05.380Z",
    "topic": "finance"
  },
  {
    "slug": "ive-worked-at-google-amazon-and-salesforce-heres-how-to-prep-for-an-interview-in-the-ai-era-no-matter-your-experience",
    "title": "I've worked at Google, Amazon, and Salesforce. Here's how to prep for an interview in the AI era, no matter your experience level.",
    "description": "Akaash Vishal Hazarika, a senior software engineer, explains how AI integration and prompt engineering are changing hiring in Big Tech software roles.",
    "fullText": "This as-told-to essay is based on a conversation with Akaash Vishal Hazarika, a 29-year-old senior software engineer based in Seattle. It's been edited for length and clarity.\n\nI've been a Big Tech employee for the past eight years, working at Google, Amazon, Splunk, and now Salesforce. I've had a front-row seat to witness the changes in the tech landscape.\n\nI've learned which skill sets software engineers need to land a job offer in the AI era. Tech companies agree that AI makes engineers more productive, so engineers are expected to use it to build things more quickly and reliably.\n\nI personally make heavy use of AI to help me with boilerplate stuff so that I can concentrate on the hard stuff, like system design and complex business logic.\n\nWhen I was interviewing for software engineering jobs in 2020, LeetCode and system design were the de facto standards for cracking a job interview. Job seekers who had an advanced understanding of data structures and algorithms would come out on top.\n\nToday, this is just the baseline expectation. AI is now widely used for coding, review, and design, so tech companies, especially startups, expect more from candidates.\n\nSome skills remain the same: Software engineers still need a problem-solving mindset and should know how to scale systems and leverage cloud services. New hires must now also understand prompt engineering — how to leverage AI to code a solution more efficiently.\n\nYou need to be able to use AI for error handling and bug fixing. AI systems integration is another important new skill that involves incorporating AI into existing workflows, scaling AI systems, and determining when to use traditional versus AI solutions for business decisions.\n\nYou're still expected to have fundamental knowledge of core system design, data structures, and algorithms. You can still expect interviewers to test your problem-solving approaches, and if you know how to make the correct tradeoffs in time and space. Interviewers still care about debugging skills, since AI makes a lot of fundamental logic errors.\n\nYou also need to be prepared for some new things. I've seen firsthand that working with AI assistants in live screen-sharing mode is now allowed in some interviews. Interviewers are looking to see if you can help them achieve their business outcomes by combining software engineering skills with AI prompting.\n\nIn an interview I had with a Silicon Valley startup in 2024, I expected the hiring team to give traditional coding challenges. I was taken aback when I was given a huge code file and asked to debug a buggy behavior, and the interviewers explicitly said I could use AI assistance.\n\nI ignored the invitation to use AI, thinking I was supposed to do it myself, and ended up spending a lot of time on the problem to no avail. I failed that interview. That was an eye-opener for me about AI's new role in this field.\n\nYou may also be asked system design questions about where AI should be integrated into the current business workflow, or to discuss the trade-offs of using AI and traditional approaches in various problem contexts.\n\nDuring my own system design interviews, I noticed companies starting to ask questions about how to integrate AI to enhance existing systems and support a particular model lifecycle in terms of infrastructure. What I've seen is that companies give access to a small codebase and expect you to deliver a small feature in about one hour, which is impossible without AI. With AI, you can roll it out easily.\n\nI've also been asked behavioral questions, like \"How do you plan to evaluate the use of AI in order to make the workflow better?\" and \"How do you plan to balance automation and manual oversight?\"\n\nFor new graduates, here's how to show hiring teams that you're equipped for the emerging needs of the software engineering role.\n\nStart by contributing to open-source projects on AI or any other GitHub project. This demonstrates that you can navigate a production codebase and work on it independently to build a new feature or fix a bug. For solo repositories, include a README that explains the rationale for your decision.\n\nIncorporate the use of AI in traditional repositories to demonstrate hands-on experience with AI integration. Don't just deploy and run it locally — try to deploy it to the cloud. Many cloud providers provide free student credits.\n\nFocus on prompting AI to drive intended outcomes more effectively by providing structured input and output. Skills like AWS or GCP cloud certifications demonstrate your ability to be a keen learner.\n\nLearn different problem-solving patterns and practice these skills regularly. Muscle memory and pattern recognition go a long way.\n\nFor experienced software engineers, your biggest asset is your deep engineering experience — here are three tips that will help you in 2026 and beyond.\n\nPairing years of engineering expertise with impressive AI skills will get the attention of hiring managers. Consider boning up on these complementary skills:\n\nSenior software engineers don't just build; they also need to strategize and innovate. Try to understand the trade-offs of relying on a third-party API versus using an open-source one and fine-tuning it for your business purposes. Questions on cost, reliability, and maintainability should always be top of mind as you develop an AI product mindset.\n\nIdentify workflows that have a great deal of manual involvement in your current job, and use AI to see if you can improve them. You can leverage AI to help you brainstorm on how to make it more efficient.\n\nBoth veteran and newbie engineers should stay curious: find out the cool technologies and AI agents available in today's market, but don't abandon fundamental engineering approaches. The hybrid understanding of these technologies is what makes you valuable.\n\nIf I were interviewing, I'd position myself as a 'hybrid engineer.' Don't just be a pure coder or just a prompt engineer. Be the bridge.",
    "readingTime": 5,
    "keywords": [
      "system design",
      "senior software",
      "software engineers",
      "software engineering",
      "skills",
      "business",
      "systems",
      "cloud",
      "traditional",
      "interviewers"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/how-ai-integration-transforming-software-engineering-hiring-2026-1",
    "thumbnail_url": "https://i.insider.com/696a960fe1ba468a96aa3eac?width=1014&format=jpeg",
    "created_at": "2026-01-19T12:27:05.346Z",
    "topic": "tech"
  },
  {
    "slug": "are-you-tired-of-ai-stigma",
    "title": "Are you tired of AI stigma?",
    "description": "The leading AI content laundering platform. Get real human artists to claim your AI-generated content as their own work. Professional plausible deniability for the AI age.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://slopper.robot-future.com/",
    "thumbnail_url": "https://slopper.robot-future.com/preview.png",
    "created_at": "2026-01-19T12:27:03.560Z",
    "topic": "tech"
  },
  {
    "slug": "hiring-at-indias-big-four-outsourcers-stalls-as-ai-seemingly-makes-an-impact",
    "title": "Hiring at India's Big Four outsourcers stalls, as AI seemingly makes an impact",
    "description": ": Revenue growth is sluggish, too",
    "fullText": "India’s big four outsourcers – HCL, Infosys, TCS and Wipro – have essentially stopped hiring, perhaps coinciding with their increased use of AI to power their practices.\n\nThe four companies have all announced quarterly results in the last ten days and appear to be in decent health. HCL reported $3.8 billion revenue, up 7.4 percent year over year. Infosys pulled in $5.1 billion, up 1.7 percent year over year. TCS revenue of $7.5 billion represented a three percent increase. Wipro’s $2.6 billion revenue represented a 5.5 percent year over year improvement.\n\nIndia’s top tech companies often hire more than 10,000 people a quarter, a rate of recruitment that more-than offsets attrition and sees their headcounts rise substantially. Wipro increased its payroll by 6,500 people last quarter and Infosys hired 5,000 more – muted growth by their standards – while TCS and HCL went backward by 11,000 and 261 people respectively.\n\nOver the last year, the four companies added just 3,910 staff, an unusually slow rate of hiring.\n\nPerhaps coincidentally, all four companies told investors they’re using more AI to deliver services for clients, either by adopting the technology to streamline their own work or by adding it to the tools they deliver to customers. Infosys has gone meta with its efforts, by creating a tool that uses AI to create Global Capability Centers, the company’s term for offshored and/or outsourced operations it runs for clients in India, that use AI to improve customers’ operations.\n\nAll also report that clients are hungry for AI expertise, so they can put the technology to work streamlining their operations. As you would expect, on earnings calls the outsourcers’ execs reached for metrics to describe their success with AI. For HCL that was pointing to 60 of its priority customers adopting one of its AI services. At TCS, the trophy win was using AI to accelerate the pace of software builds for a major client. Wipro is chuffed about the rate of adoption for its AI-infused operations tools WINGS and WEGA.\n\nThe four companies are all hiring people with AI skills as fast as they can find them while also training senior staff who are yet to wrap their heads around the tech, a new twist on the services industry’s balancing act of trying to keep margins high by having inexperienced and low-salaried juniors handle much client work after high-cost seniors lead with consultancy.\n\nInvestors weren’t spooked by what they heard, with the four companies’ share prices steady – other than Infosys, which popped five percent. ®",
    "readingTime": 3,
    "keywords": [
      "infosys",
      "operations",
      "wipro",
      "hiring",
      "revenue",
      "rate",
      "services",
      "clients",
      "customers",
      "india’s"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/19/hcl_infosys_tcs_wipro_results/",
    "thumbnail_url": "https://regmedia.co.uk/2024/03/04/leonardo_india_ai.jpg",
    "created_at": "2026-01-19T06:24:32.370Z",
    "topic": "tech"
  },
  {
    "slug": "the-ai-engineer-roadmap",
    "title": "The AI Engineer Roadmap",
    "description": "Want to build AI-powered apps, but don't know where to start? You need a roadmap.",
    "fullText": "This tutorial is your roadmap to becoming an AI Engineer. We'll cover all the core concepts you need to know to build AI-powered apps.\n\nWhen you're done, you'll be able to:\n\nThen you'll be ready to get your hands dirty with my Vercel AI SDK tutorial, where you'll put your newfound knowledge to work.\n\nLet's dive into the world of AI Engineering!\n\nWant to remember what we've covered? Here's a PDF of all the diagrams in the series.",
    "readingTime": 1,
    "keywords": [
      "you'll",
      "tutorial"
    ],
    "qualityScore": 0.5,
    "link": "https://www.aihero.dev/ai-engineer-roadmap",
    "thumbnail_url": "https://www.aihero.dev/api/og?resource=ai-engineer-roadmap&updatedAt=2025-03-18T11:28:44.629Z",
    "created_at": "2026-01-19T06:24:31.098Z",
    "topic": "tech"
  },
  {
    "slug": "harveys-ceo-explains-his-early-tactic-to-get-customers-telling-lawyers-how-bad-their-arguments-were",
    "title": "Harvey's CEO explains his early tactic to get customers: telling lawyers how bad their arguments were",
    "description": "Harvey's CEO reveals the demo tactic that helped him win early customers for his legal AI startup: Going straight at lawyers' arguments.",
    "fullText": "Harvey's CEO says his earliest demos worked because they went straight for lawyers' arguments.\n\nWinston Weinberg said in an episode of the \"Sequoia Capital\" podcast published Thursday that once he got lawyers on a call to demo his legal AI tool, he knew he had only seconds to grab their attention.\n\nInstead of walking through features, the cofounder and chief executive of the legal AI startup pulled publicly available court filings that the lawyers had written and prompted Harvey to critique them.\n\n\"I would try to come up with prompts that were like, 'This is bad,'\" Weinberg said.\n\n\"Because they're a litigator and I'm basically attacking something that they just wrote, they would instantly read the screen,\" he added.\n\nWeinberg said the approach was risky because early versions of the tool could hallucinate, and a bad output could kill a conversation instantly.\n\n\"But the times that they got it right, it was over,\" Weinberg said, referring to moments when the analysis landed and won the lawyer's attention.\n\nHarvey is one of the most closely watched startups in legal AI, aiming to transform how Big Law firms work. In December, the company said it had reached a valuation of $8 billion after a funding round led by A16z.\n\nWeinberg said he relied on cold outreach in Harvey's early days, messaging thousands of lawyers on LinkedIn to secure early calls.\n\n\"I'd been practicing law for like eight months,\" Weinberg said of the period when he started the company. \"I didn't have any connections.\"\n\nThe legal AI sector is expanding fast, and Weinberg said the scale of the opportunity is forcing constant change inside Harvey.\n\nHe said the company's rapid growth has pushed him to \"reinvent\" himself as a founder every few months, often rethinking leadership hires and company structure to keep up with demand.\n\nDespite Harvey's rise to an $8 billion valuation, Weinberg has played down the idea of a single dominant winner in legal tech. In a Reddit \"Ask Me Anything\" session last month, he said the market is simply too large.\n\n\"There are around 10 million global legal professionals, and Harvey serves just single-digit percentage points of them,\" he said.\n\n\"I don't think a single player is going to capture all of the pretty enormous amount of value that will be created in the next 10 years in this space,\" he added.\n\nWeinberg said the global legal market is worth roughly $1 trillion, yet only about $30 billion is spent on technology.\n\nInvestor interest has followed the shift. Legal-tech startups raised $3.2 billion last year, according to Business Insider's December analysis of Crunchbase data and deals.\n\nAI adoption is also accelerating inside law firms. Five of the 10 largest US law firms by revenue told Business Insider in July that they were using AI in their workflows, including for document review and spotting compliance risks.",
    "readingTime": 3,
    "keywords": [
      "law firms",
      "legal",
      "lawyers",
      "weinberg",
      "tool",
      "attention",
      "instantly",
      "analysis",
      "startups",
      "valuation"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/harvey-ceo-winston-weinberg-early-tactic-lawyer-arguments-demo-2026-1",
    "thumbnail_url": "https://i.insider.com/696da2c9e1ba468a96aa4a9d?width=1200&format=jpeg",
    "created_at": "2026-01-19T06:24:28.436Z",
    "topic": "finance"
  },
  {
    "slug": "metro-mcp-mcp-server-for-dc-and-nyc-metro",
    "title": "Metro MCP: MCP Server for DC and NYC Metro",
    "description": "Connect LLMs and AI agents to live NYC Subway and DC Metro data. Current arrivals, train positions, and service alerts through MCP.",
    "fullText": "Metro-MCP connects LLMs and AI agents to current transit data from NYC Subway and DC Metro. \n Train positions, arrivals, delays—all streaming through MCP with instant updates.\n\nFrom commuter assistants to analytics platforms, Metro-MCP provides the foundation for intelligent transit applications with up-to-date data and comprehensive coverage.\n\nGet current arrival times for any station. AI agents can answer \"when's the next train?\" \n with actual data, not guesses.\n\nTrack every train in the system. See exactly where trains are, which direction they're heading, \n and train composition.\n\nComplete DC Metro bus support with instant positions, predictions, routes, and stops. \n Build comprehensive transit assistants.\n\nStay ahead of disruptions. Get instant incident reports, elevator outages, \n and service advisories as they happen.\n\nComplete station info with coordinates, transfer connections, and line information. \n Build spatial reasoning into your AI.\n\nData updates every 7-10 seconds. Your AI always has the freshest information \n without manual polling.\n\nComprehensive coverage of NYC Subway and DC Metro with different strengths and capabilities\n\nEssential tools that enable AI agents to access and query transit data effectively\n\nRetrieve upcoming train arrivals at any station with estimated times, destinations, and line information. The foundation for commute planning and travel assistance.\n\nReal-time location data for all active trains including direction, line, and car count. Essential for system monitoring and crowding analysis.\n\nBus arrival predictions with route information and stop details. Enables multimodal trip planning and last-mile connectivity (DC Metro only).\n\nService disruptions, elevator outages, and system advisories. Critical for proactive route adjustments and accessibility planning.\n\nComplete station metadata including coordinates, lines served, and transfer information. Enables geographic queries and spatial reasoning.\n\nLine information and service patterns. Helps agents understand the transit network structure and routing possibilities.\n\nSee how AI agents can answer transit questions naturally using Metro-MCP\n\nThe next Red Line train to Shady Grove arrives in 4 minutes at Metro Center. After that:\n\nCurrent alerts affecting your route:\n\nFastest route (38 minutes total):\n\nDeploy in minutes, scale infinitely. Metro-MCP runs on Cloudflare Workers with \n enterprise-grade reliability.",
    "readingTime": 2,
    "keywords": [
      "nyc subway",
      "elevator outages",
      "spatial reasoning",
      "transit",
      "train",
      "agents",
      "station",
      "route",
      "instant",
      "comprehensive"
    ],
    "qualityScore": 1,
    "link": "https://metro-mcp.anuragd.me/",
    "thumbnail_url": "https://metro-mcp.anuragd.me/og-image.png",
    "created_at": "2026-01-19T01:02:20.981Z",
    "topic": "tech"
  },
  {
    "slug": "figmalike-canvas-for-running-claude-code-agents",
    "title": "Figma-like Canvas for running Claude Code agents",
    "description": "Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf) - AgentOrchestrator/AgentBase",
    "fullText": "AgentOrchestrator\n\n /\n\n AgentBase\n\n Public\n\n Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf)\n\n License\n\n View license\n\n 85\n stars\n\n 6\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n AgentOrchestrator/AgentBase",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/AgentOrchestrator/AgentBase",
    "thumbnail_url": "https://opengraph.githubassets.com/ccefb572879426a6706c75e212582de106cce0540f056c7b6f252924f0fba3c1/AgentOrchestrator/AgentBase",
    "created_at": "2026-01-18T18:15:51.872Z",
    "topic": "tech"
  },
  {
    "slug": "samsung-electronics-boss-says-its-betting-on-ai-that-blends-into-the-background-not-spectacle",
    "title": "Samsung Electronics boss says it's betting on AI that blends into the background, not spectacle",
    "description": "Simon Sung, the CEO of Samsung Electronics Europe, said its AI strategy is \"about AI that is genuinely useful and unobtrusive.\"",
    "fullText": "In an AI market full of spectacle and hype, Samsung says it's trying to blend in.\n\nSimon Sung, the CEO of Samsung Electronics Europe, told Business Insider that its AI strategy is \"about AI that is genuinely useful and unobtrusive,\" whether that's a smart home responding autonomously or appliances coordinating daily routines in the background.\n\n\"The focus is firmly on everyday value rather than novelty,\" he said in an interview over email.\n\nSamsung has developed its own large language models, called Samsung Gauss, but does not offer them directly as a stand-alone consumer product like OpenAI does with ChatGPT.\n\nInstead, its consumer-facing efforts center on the Galaxy AI assistant, which is built into the company's smartphones and uses a mix of its in-house AI and tech developed by partners such as Google. Like Google's assistant on its Pixel smartphones, Galaxy AI can perform tasks like live translation and transcription.\n\n\"The shift is from AI as a feature you turn on to AI as a companion that works alongside you,\" said Sung.\n\nWithin the South Korean conglomerate, Samsung Electronics is the company responsible for consumer technology, including its Galaxy smartphones, TVs, and home appliances. It's also a producer of memory chips used in PCs and data centers. In earnings guidance shared earlier this month, the company said it expects profits to triple in the final quarter of 2025 amid a surge in demand for memory chips to power AI models.\n\nIn early January, Samsung showcased TVs, kitchen appliances, and washing machines at the Consumer Electronics Show in Las Vegas that featured sensors and voice recognition.\n\n\"The goal is to make technology feel less like a collection of gadgets, and more like a coherent, responsive environment that adapts to real life,\" Sung said.\n\nInternally, Sung said Samsung Electronics provides training and encourages information exchanges between product, design, engineering, and marketing teams \"so that AI fluency grows across the organization rather than within isolated groups.\"\n\n\"Because we're building AI into TVs, appliances, mobile devices, and connected services simultaneously, employees naturally think about intelligence as a shared layer across the entire experience, not as a stand-alone feature,\" he said.",
    "readingTime": 2,
    "keywords": [
      "memory chips",
      "galaxy ai",
      "samsung electronics",
      "appliances",
      "smartphones",
      "it's",
      "rather",
      "developed",
      "models",
      "stand-alone"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/samsung-electronics-europe-ceo-ai-strategy-simon-sung-2026-1",
    "thumbnail_url": "https://i.insider.com/696cb6eaa645d1188187919d?width=793&format=jpeg",
    "created_at": "2026-01-18T18:15:48.079Z",
    "topic": "finance"
  },
  {
    "slug": "openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next",
    "title": "OpenAI launches cheaper ChatGPT subscription, says ads are coming next",
    "description": "OpenAI is bringing its $8/month ChatGPT Go plan to the U.S. and says it will begin testing ads soon in the free tier and Go.",
    "fullText": "OpenAI has announced several important changes to ChatGPT. First, the company says it is rolling out its more affordable ChatGPT Go plan in the United States for $8 per month. OpenAI also confirmed it will soon start testing ads in ChatGPT …\n\nOpenAI first launched ChatGPT Go in India last year and gradually rolled out it to 170 additional countries. Starting today, ChatGPT Go is available everywhere ChatGPT is available, including the United States.\n\nWith this, there are now three tiers of ChatGPT available:\n\nHere’s what you get with ChatGPT Go compared to the free plan:\n\nSecond, OpenAI says that it will “testing ads in the free tier and ChatGPT Go in the US soon.” ChatGPT Plus, Pro, Business, and Enterprise tiers will remain ad-free.\n\nOpenAI detailed its approach to ads in ChatGPT in a blog post published today:\n\nTo start, we plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation. Ads will be clearly labeled and separated from the organic answer. You’ll be able to learn more about why you’re seeing that ad, or dismiss any ad and tell us why. During our test, we will not show ads in accounts where the user tells us or we predict that they are under 18, and ads are not eligible to appear near sensitive or regulated topics like health, mental health or politics.\n\nFurthermore, OpenAI says that your conversations with ChatGPT are not shared with advertisers. Ads also will not influence answers that ChatGPT gives you.\n\nOpenAI says that it’s “not launching ads yet,” but rather plans to “start testing in the coming weeks for logged in adults in the U.S. on the free and Go tiers.”\n\nWhat do you think of today’s announcements from OpenAI? Let us know down in the comments.\n\nMy favorite iPhone accessories:\n\nFollow Chance: Threads, Bluesky, Instagram, and Mastodon.\n\nCheck out 9to5Mac on YouTube for more Apple news:",
    "readingTime": 2,
    "keywords": [
      "chatgpt go",
      "testing ads",
      "plan",
      "tiers",
      "free",
      "openai",
      "soon",
      "health",
      "united",
      "test"
    ],
    "qualityScore": 0.85,
    "link": "https://9to5mac.com/2026/01/16/openai-launches-cheaper-chatgpt-subscription-says-ads-are-coming-next/",
    "thumbnail_url": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2026/01/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png.webp?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "created_at": "2026-01-18T12:21:37.013Z",
    "topic": "tech"
  },
  {
    "slug": "an-alternative-to-flat-image-generators-for-layoutheavy-design",
    "title": "An alternative to 'flat' image generators for layout-heavy design",
    "description": "Stop generating AI art. Start creating campaigns. Professional layouts with readable text, every time.",
    "fullText": "Join 10,000+ users who have stopped fighting with design tools and started shipping campaigns.\n\n\"This is exactly the kind of tool non-designers actually need. Finally someone who gets it. I don't want to learn Figma, I just want a banner.\"\n\n\"We used to spend 2 days just resizing assets for clients. Now we do the whole campaign in 20 minutes. The text rendering is flawless.\"\n\n\"I've wasted hours on Midjourney trying to get text that doesn't look like hieroglyphics. LayoutCraft just works. It understands layout.\"\n\n\"Designers roasting your tool kind of proves it's doing something disruptive. For a bootstrapped founder, this saves me $500/month.\"\n\n\"The 'One Prompt' feature is magic. I wrote one sentence and got my LinkedIn, Twitter, and Instagram assets done instantly. Huge time saver.\"\n\n\"I was skeptical about 'AI design' but the structured layouts here are legit. It follows basic design principles better than my interns.\"",
    "readingTime": 1,
    "keywords": [
      "design",
      "tool",
      "assets",
      "text"
    ],
    "qualityScore": 0.75,
    "link": "https://layoutcraft.tech",
    "thumbnail_url": "https://layoutcraft.tech/assets/images/og-image.png",
    "created_at": "2026-01-18T12:21:35.865Z",
    "topic": "tech"
  },
  {
    "slug": "still-here-xs-grok-ai-tool-accessible-in-malaysia-despite-ban",
    "title": "‘Still here!’: X’s Grok AI tool accessible in Malaysia despite ban",
    "description": "Experts warn use of VPNs makes it hard to limit access to technology that can create nonconsensual explicit images\nDays after Malaysia made global headlines by announcing it would temporarily ban Grok over its ability to generate “grossly offensive and nonconsensual manipulated images”, the generative AI tool was conversing breezily with accounts registered in the country.\n“Still here! That DNS block in Malaysia is pretty lightweight – easy to bypass with a VPN or DNS tweak,” Grok’s account on X said in response to a question from a user.\n Continue reading...",
    "fullText": "Experts warn use of VPNs makes it hard to limit access to technology that can create nonconsensual explicit images\n\nDays after Malaysia made global headlines by announcing it would temporarily ban Grok over its ability to generate “grossly offensive and nonconsensual manipulated images”, the generative AI tool was conversing breezily with accounts registered in the country.\n\n“Still here! That DNS block in Malaysia is pretty lightweight – easy to bypass with a VPN or DNS tweak,” Grok’s account on X said in response to a question from a user.\n\nGrok’s ability to allow users to create sexually explicit images, including images of children, has created a global outcry over recent weeks, with regulators and politicians around the world launching investigations. Indonesia and Malaysia became the first two countries to announce blocks on the technology, with Malaysia’s regulatory body saying last Sunday it had “directed a temporary restriction” on access to Grok, effective as of 11 January 2026. Officials in the Philippines have said they too plan to ban the technology.\n\nBlocking access to Grok is not straightforward, however. The technology not only exists across multiple platforms, including a standalone app and website, but is also integrated across X, which, along with Grok, is owned by Elon Musk’s xAI.\n\nOver the past week, X users, and even Grok itself, have advised people on how to bypass restrictions. This includes using a VPN – many of which are available for free – or changing domain name system (DNS), the protocol on the internet that turns address names into IP addresses that load websites.\n\nWhen the Guardian tried to use Grok in Indonesia, its website was working even without a VPN, though the Grok app did not work. Grok was also still responding to Indonesian accounts on X, where it functions as an integrated chatbot. X has not been subject to a ban.\n\nEven if governments could completely restrict Grok, though, this is not a real solution, said Nana Nwachukwu, an AI governance expert and PhD researcher at Trinity College Dublin.\n\n“Blocking Grok is like slapping a Band-Aid on a weeping wound that you haven’t cleaned,” she said. “You block Grok, and then you go around shouting you’ve done something. Meanwhile, people can use VPNs to access the same platforms.” Or, they could simply turn to one of the many other platforms that offer the same functions, including “smaller, general purpose AI systems that are largely unknown”, Nwachukwu added.\n\nGovernments should instead focus on law enforcement and investigating individuals who use such tools to break the law, she added. “Platforms are required by law to provide information to law enforcement when a crime has been committed,” Nwachukwu said. “If we see people being arrested, people being tried in courts, people being jailed for these offences, that’s a sign that this is a real crime.”\n\nX should build accountability into its platform – and clean itself up, said Nwachukwu: “All of those offending images should be removed from the platform.”\n\nOn Wednesday, X announced additional safeguards in response to continued public anger, saying it would stop the @Grok account on X from “allowing the editing of images of real people in revealing clothing such as bikinis”, including paid subscribers. However, the Guardian found that it was possible to get around such restrictions by using the standalone version of Grok, easily accessible through a web browser, to create short videos in which clothes are removed from images of real women.\n\nThis could then be posted to X’s public platform, where it could be viewed by users around the world within seconds.\n\nMusk’s company also said that, in jurisdictions where such content is illegal, it will geoblock the ability of all users in those locations to generate images of real people in bikinis, or similar attire, in Grok on X, adding that “xAI is implementing similar geoblocking measures for the Grok app”.\n\nExperts warn users may still be able to get around such “geoblocks” through a VPN. It is also not clear in which countries such restrictions will be implemented.\n\nIn Malaysia, the communications minister, Fahmi Fadzil, has said restrictions on Grok would only be lifted once the ability to produce harmful content had been disabled, according to local media reports.\n\nDr Nuurrianti Jalli, a visiting fellow at the media, technology and society programme at ISEAS – Yusof Ishak Institute in Singapore, said the threat of blocking Grok could be a useful way to apply pressure to companies to respond quickly, adding that it “shifts the debate from ‘individual bad actors’ to questions of platform responsibility, safety by design, and accountability when safeguards fail”. It could also “slow the spread of abuse, reduce casual misuse and create a clear boundary around what authorities consider unacceptable”, she said.\n\nIn Indonesia, Grok has been used to create nonconsensual sexualised images of singers and celebrities, including one of the country’s most popular girl groups, JKT48, while in Malaysia, women report similar abuses, including cases where the tool has been used to remove their hijabs, according to Malaysian media.\n\nSome women resorted to publicly telling Grok on X that they do not authorise it to “crawl, take, process or edit” any of their photos.\n\nJalli said governments should push for greater transparency “about how safety measures are implemented, how abuse reports are handled and what enforcement steps are taken when harmful content is generated or circulated”.\n\nThe Malaysian communications and multimedia commission and the ministry of communications did not respond to a request for comment. Indonesia’s ministry of communication and digital affairs also did not respond.\n\nNwachukwu said safeguards should be built into the AI system, rather than “gates” built around it. “Both the [geographic] restriction from X, [and] the restriction from the government is gated access, and gates can be broken down,” she said.\n\nAdditional reporting from Hidayatullah",
    "readingTime": 5,
    "keywords": [
      "experts warn",
      "harmful content",
      "grok app",
      "law enforcement",
      "create nonconsensual",
      "explicit images",
      "access",
      "technology",
      "users",
      "ability"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/jan/18/grok-x-ai-tool-still-accessible-malaysia-despite-ban-vpns",
    "thumbnail_url": "https://i.guim.co.uk/img/media/411be0c83b7bf0b0d046d79ce00cf32dbadc7de2/805_0_4800_3840/master/4800.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=230f27a2d5c2c06f7e12a239efbb1271",
    "created_at": "2026-01-18T12:21:34.078Z",
    "topic": "tech"
  },
  {
    "slug": "sequoia-to-join-gic-coatue-in-anthropic-investment-ft-reports",
    "title": "Sequoia to join GIC, Coatue in Anthropic investment, FT reports",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/sequoia-to-join-gic-coatue-in-anthropic-investment-ft-reports-4453209",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM0H05C_L.jpg",
    "created_at": "2026-01-18T12:21:32.357Z",
    "topic": "finance"
  },
  {
    "slug": "30min-video-analysis-for-0003-via-frametiling-and-vision-api",
    "title": "30min video analysis for $0.003 via frame-tiling and Vision API",
    "description": "Electron demo app showcasing VAM Seek library integration with folder tree view - unhaya/vam-seek-ai",
    "fullText": "unhaya\n\n /\n\n vam-seek-ai\n\n Public\n\n Electron demo app showcasing VAM Seek library integration with folder tree view\n\n License\n\n View license\n\n 6\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n unhaya/vam-seek-ai",
    "readingTime": 1,
    "keywords": [
      "view license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/unhaya/vam-seek-ai",
    "thumbnail_url": "https://opengraph.githubassets.com/cbcce6d2da952349a2149e5b2e070ade6672f19bca5e55e782dc7397e9302089/unhaya/vam-seek-ai",
    "created_at": "2026-01-18T06:18:38.520Z",
    "topic": "tech"
  },
  {
    "slug": "figmause-cli-to-control-figma-for-ai-agents",
    "title": "Figma-use – CLI to control Figma for AI agents",
    "description": "Control Figma from the command line. Full read/write access for AI agents — create shapes, text, components, set styles, export images. 73 commands. - dannote/figma-use",
    "fullText": "dannote\n\n /\n\n figma-use\n\n Public\n\n Control Figma from the command line. Full read/write access for AI agents — create shapes, text, components, set styles, export images. 73 commands.\n\n www.npmjs.com/package/@dannote/figma-use\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n dannote/figma-use",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.5,
    "link": "https://github.com/dannote/figma-use",
    "thumbnail_url": "https://opengraph.githubassets.com/bf1d7bb1301406315647286afa8fce3195a4d04a3d1f891f73becb7ed9d699e1/dannote/figma-use",
    "created_at": "2026-01-18T06:18:37.654Z",
    "topic": "tech"
  },
  {
    "slug": "verbalized-sampling-how-to-mitigate-mode-collapse-and-unlock-llm-diversity",
    "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity",
    "description": "Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2510.01171",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-18T06:18:36.886Z",
    "topic": "tech"
  }
]
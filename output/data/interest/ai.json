[
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What’s Behind the ‘SaaSpocalypse’ Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "bozoma-saint-john-on-branding-ais-impact-super-bowl",
    "title": "Bozoma Saint John on Branding, AI's Impact, Super Bowl",
    "description": "Bozoma Saint John, former Chief Marketing Offiver of Netflix and former chief brand officer of Uber, discusses the tech and media landscape and the Super Bowl with Romaine Bostick and Katie Greifeld on The Close. She says evolution is key to everything.",
    "fullText": "Feb 4th, 2026Bozoma Saint John on Branding, AI's Impact, Super BowlBozoma Saint John, former Chief Marketing Offiver of Netflix and former chief brand officer of Uber, discusses the tech and media landscape and the Super Bowl with Romaine Bostick and Katie Greifeld on The Close. She says evolution is key to everything.Duration:1:15Software Stocks Slide for Second DayDuration:0:25Homeland Security Takes 700 Agents Out of MinneapolisBalance of PowerDuration:1:35Ambani, Fink on 'Era Of India'Duration:4:37Rep. Waters Clashes With Bessent Over Impact of TariffsDuration:2:42Things Have to Change at Qualcomm, Analyst Goldberg SaysDuration:1:27US Services Sector ExpandsDuration:1:00UK's Starmer Says Mandelson Vetting Material Mentioned EpsteinThe Opening TradeDuration:6:28Novo CEO Remains Confident After 20% PlungeAvailable on:Listen onApple TVListen onRokuListen onSamsung TVListen onFire TVListen onAndroid TVListen onRakuten TVListen onHaystack NewsWatch BTV in your area:Channel Finder",
    "readingTime": 1,
    "keywords": [
      "saint john",
      "tvlisten",
      "impact",
      "chief",
      "super"
    ],
    "qualityScore": 0.15,
    "link": "https://www.bloomberg.com/news/videos/2026-02-04/bozoma-saint-john-on-branding-ai-s-impact-super-bowl-video",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/inrM.7pbiidA/v3/-1x-1.webp",
    "created_at": "2026-02-05T01:08:07.679Z",
    "topic": "finance"
  },
  {
    "slug": "all-in-one-ai-assistant",
    "title": "All in One AI Assistant",
    "description": "Fluxchat is an all-in-one AI assistant and multi-model AI platform for AI chat, writing, and creativity. Use the AI image generator, AI video generator, and AI music generator in one AI creation platform to produce content faster.",
    "fullText": "Unlock the power of GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, NanoBanana and more.\nFluxchat is an all-in-one AI assistant and multi-model platform for chat, writing, and creativity—an AI creation platform with image, video, and music generation.\n\n999+ users are creating with Fluxchat\n\nAll-in-one creation platform covering image, video, chat, and music scenarios\n\nText-to-image and image-to-image generation, delivering high-quality visual assets for design and marketing\n\nText-to-video and image-to-video creation, producing short video content faster\n\nChat and writing assistance to solve problems and boost work and learning efficiency\n\nText-to-music and lyric creation, crafting custom soundtracks for videos and brands\n\nFluxChat is an all-in-one AI assistant and multi-model AI platform that brings together GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, and NanoBanana for AI chat, writing, and image/video/music creation.\n\nUse GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, and NanoBanana in one place. Pick the best model for each task.\n\nFrom idea to output: text, image, music, and video creation in one AI creation platform workflow.\n\nCreate production-ready visuals, music, and videos with quality designed for real-world use cases.\n\nClear UX and efficient flows help you create faster and iterate with less friction.\n\nBuilt for real creation and business use cases, FluxChat is an all-in-one assistant that solves fragmented tools, high costs, and low efficiency with a creation platform for image, music, and video.\n\nGo from intent to output in three steps with one assistant:\n\nChoose your goal—chat, writing, image, music, or video—and describe the style you need.\n\nSelect GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, or NanoBanana and tune size, duration, or style.\n\nCreate fast, refine quickly, and export results ready for marketing, content, and product assets.\n\nAn all-in-one AI assistant and AI creation platform for chat, image, video, and music workflows.\n\nUse GPT-5x, Claude-4x, Gemini-3, Suno, Veo3x, and NanoBanana in one unified workspace.\n\nAsk, summarize, draft, and refine content faster with context-aware AI.\n\nText-to-image, image-to-image, and style presets for production-ready visuals.\n\nGenerate music and lyrics for short-form video, branding, and campaigns.\n\nCreate short videos from text or images for marketing and content needs.\n\nTrack outputs, download instantly, and reuse assets across projects.\n\nTrusted for consistent quality and faster content output across real-world use cases.\n\nReal stories of faster creation and more reliable outputs with FluxChat.\n\nWriting, images, and video can be done in one place. Picking the right model makes quality far more consistent.\n\nA unified model stack stopped tool switching. Image, music, and video assets now move faster through our pipeline.\n\nLesson scripts, visuals, and audio are generated quickly, which saves prep time every week.\n\nDocs, diagrams, and product visuals are generated in one flow, making collaboration much smoother.\n\nCopy, posters, and short-form video assets are produced in one place—faster iterations and more stable performance.\n\nFrom research summaries to visuals, I can finish reports faster with higher quality.\n\nClear answers about models, creation workflows, and real-world usage.\n\nChat, write, and generate images, music, and video in one place—faster from idea to output.",
    "readingTime": 3,
    "keywords": [
      "gpt-5x claude-4x",
      "claude-4x gemini",
      "gemini suno",
      "suno veo3x",
      "production-ready visuals",
      "creation platform",
      "content faster",
      "music",
      "nanobanana",
      "assistant"
    ],
    "qualityScore": 1,
    "link": "https://fluxchat.org/",
    "thumbnail_url": "https://fluxchat.org/preview.png",
    "created_at": "2026-02-05T01:08:03.811Z",
    "topic": "tech"
  },
  {
    "slug": "openai-is-building-an-integrity-team-to-prevent-chatgpt-ads-from-going-off-the-rails",
    "title": "OpenAI is building an 'integrity team' to prevent ChatGPT ads from going off the rails",
    "description": "A job posting details how OpenAI's ads integrity team will look to help the company scale its ad operation without compromising user trust or safety.",
    "fullText": "OpenAI is building a team to make sure bad ads don't mess up its highly anticipated introduction of advertising in ChatGPT.\n\nAn OpenAI job listing for a software engineer posted late January revealed the company is building an \"ads integrity\" team.\n\nThe ad describes the job as a high-impact role on \"a 0 → 1 team,\" a commonly used term in Silicon Valley and elsewhere to describe a team being built from scratch.\n\nThis person will be responsible for designing systems that enable OpenAI's ad business to grow without compromising user trust and safety, per the listing.\n\nIt's common for Big Tech companies with large ad businesses to create teams to combat ad fraud and address other issues, such as brand safety. OpenAI, which confirmed last month that it would soon begin testing ChatGPT ads, is building this part of its ad function early.\n\nThe job listing also says that its new ad integrity hire will work on developing \"know your customer\" (KYC) systems to verify advertisers' identities and assess their risk. KYC, a term most commonly used in the finance industry, is an important but labor-intensive discipline for cracking down on scam ads and other harmful content created by criminals and other bad actors. It's a particularly pressing issue for Big Tech companies with self-service ad platforms, as a recent Reuters investigation into Meta highlighted. Meta said in early December it had removed \"more than 134 million scam ads\" in 2025.\n\nAriella Garcia, chief operating officer of the Check My Ads Institute, said it would be interesting to see how substantive OpenAI's investment in its ads integrity team would be beyond the initial launch.\n\n\"KYC on advertisers is certainly a good foundation, but the materiality of the risk of a large volume of scam ads in the early days is far lower,\" Garcia said.\n\nThe job listing says that the new ads integrity hire will play a role in determining where and how ads are shown in ChatGPT. OpenAI will need to keep user trust in its organic answers as it ramps up its advertising business. At the same time, advertisers have said that OpenAI will need to prove that ads within AI answers can drive results for their businesses if it has any hope of scaling.\n\nAn OpenAI spokesperson confirmed the company will run a small test of ads on ChatGPT's free version and its Go tier in the US, which users should begin seeing in the coming weeks.\n\nThe spokesperson said OpenAI is asking for a minimum spend of $200,000 on ChatGPT ads to participate in the program, confirming prior Adweek reporting. OpenAI will track clicks and impressions, but will likely explore further measurement options as its advertising experiments progress, the spokesperson said.\n\nOpenAI declined to comment on the ads integrity job ad.",
    "readingTime": 3,
    "keywords": [
      "user trust",
      "integrity hire",
      "job listing",
      "chatgpt ads",
      "integrity team",
      "scam ads",
      "big tech",
      "advertising",
      "advertisers",
      "openai"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-building-integrity-team-chatgpt-ads-2026-2",
    "thumbnail_url": "https://i.insider.com/6980f4dda645d118818878bf?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:08:00.344Z",
    "topic": "finance"
  },
  {
    "slug": "i-spent-the-day-with-wall-street-elites-as-ai-fears-swept-the-stock-market-heres-how-they-responded-in-real-time",
    "title": "I spent the day with Wall Street elites as AI fears swept the stock market. Here's how they responded in real time.",
    "description": "Business Insider was on the scene at a WSJ conference that brought together leaders from Blackstone, Lazard, and Citi.",
    "fullText": "Most people flock to Palm Beach for sunshine and an escape. But on Tuesday, Wall Street's elite got 40-degree weather and nonstop talk about one company: Anthropic.\n\nAs the morning session of The Wall Street Journal's Invest Live event kicked off, the AI startup announced its new \"Claude Cowork\" legal plugins, a tool designed to automate contract triage and NDAs. The news unnerved investors, dragging down indexes like the Nasdaq and tech stocks from Palantir to Oracle. Thomson Reuters had dipped 20% by mid-afternoon, experiencing one of its worst single-day drops as investors panicked that software investments deemed cutting-edge a few years ago could soon be rendered passé.\n\nPeter Orszag, CEO of the investment bank Lazard and a former Obama administration official, offered a sobering macro view. \"The US economy and the equity market at this point are highly levered bets with regard to AI taking off,\" he said. \"What I think you're seeing is when people get nervous about whether that'll happen.\"\n\nThat said, he didn't contest the tech's future — just that some key players might not be around in a few years. \"You can bet on the technology paying off,\" he said, \"but that the firms making the investments today are not necessarily the ones that benefit.\"\n\nEarlier in the day, Jon Gray, president of Blackstone, which manages over $1.3 trillion, emphasized the lucrative effects of the mini-economy that's sprung up to meet the physical and infrastructural demands of the artificial intelligence boom.\n\n\"To us, you don't necessarily have to know who the winners and losers are going to be,\" Gray said. \"You know that the data centers, the autonomous vehicles, the robots, they're all going to plug into the wall, and there's going to be a lot of need for digital infrastructure.\"\n\nSeveral attendees had recently returned from last month's World Economic Forum in the Swiss Alps and were likely hoping to leave the chill behind. But alas, one woman I spotted was bundled up in fur, and I personally had to fish out a 10-year-old coat from the back of my closet to make it from my Uber to the event's entrance.\n\nAt lunchtime, we decamped to the courtyard for a salmon salad and sweet iced tea, and the conversation drifted between the rise of South Florida in the post-pandemic era and the outlook for financial sponsors' spending this year. But as we shuffled back to our seats for the afternoon's lineup of speakers, the chill returned as we caught up with the market's increasingly dour reaction to Anthropic's news.\n\nSteven Tananbaum, founder of GoldenTree Asset Management, saw an opportunity in the rout. For a credit specialist, \"indiscriminate selling\" is a chance to find value where others see only collapse. \"The idea that all software is the same is what's exciting,\" he said about the broad sell-off. \"There's just indiscriminate selling regardless of what the prospects are.\"\n\nThe day ended with a reminder that while the markets might be sweating over incremental news, the C-suite is playing a longer game.\n\nAs it happened, I got to ask the final question of the day's concluding speaker, Mark Mason, Citi's chief financial officer, who's stepping down from his role this spring. He pointed to the cost-saving potential of AI, which would free up more money for Citi to devote to advancing CEO Jane Fraser's mission: putting the bank's multiyear overhaul behind it. The bank is applying the tech to its most critical processes, he said.\n\n\"What will it take to actually drive greater efficiency in that process? And I think that's a much better approach than the way others have thought about use cases,\" he added, referring to other banks.\n\nEarlier in the day, Hemant Taneja, the CEO of General Catalyst, a major VC backer of Anthropic, conceded that the economy will shed jobs due to AI, urging attendees to focus on education to retrain workers. \"Never ever was there a case that every skill in any job can be better done by AI and robotics,\" he said, \"which is, I think, where we are headed in the next, not five, but 20 to 30 years.\"\n\nStill, some speakers, like billionaire activist Nelson Peltz, brought some levity to the proceedings. Peltz — famous for his high-profile corporate showdowns with executives like Disney's Bob Iger — expressed enthusiasm for technology that could pump out faster, simpler answers to his legal questions.\n\n\"Not being a lawyer, I'm sort of happy,\" he said, as anxious laughter coursed through the room.",
    "readingTime": 4,
    "keywords": [
      "legal",
      "investors",
      "tech",
      "software",
      "investments",
      "bank",
      "economy",
      "technology",
      "necessarily",
      "earlier"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/wall-street-bosses-palm-beach-conference-reacted-to-ai-selloff-2026-2",
    "thumbnail_url": "https://i.insider.com/6983a002e1ba468a96ab6753?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:08:00.088Z",
    "topic": "finance"
  },
  {
    "slug": "why-wall-street-analysts-see-the-techselloff-as-overblown-and-fueled-by-fear-not-fundamentals",
    "title": "Why Wall Street analysts see the tech-sell-off as overblown, and fueled by 'fear, not fundamentals'",
    "description": "Tech stocks are trading lower as investors assess whether AI could upend current software leadership. Analysts say the market reaction is overblown.",
    "fullText": "Bank of America analysts say Wall Street is blowing the latest AI fears out of proportion as the tech sell-off drags on.\n\nThe tech-heavy Nasdaq Composite is down about 2% on Wednesday, extending losses from Tuesday's session. Analysts say recent losses, fueled by AI disruption worries and some poorly received earnings, are overblown and don't justify the broad sell-off.\n\nThe sell-off kicked off in the software sector after AI start-up Anthropic introduced a new plugin for its Claude Cowork AI agent. The new tool is said to be more automatic in completing clerical tasks than conversational chatbots, fueling worries that AI will replace software and dethrone existing market leaders.\n\nWilliam Blair analysts say these fears are overstated, calling the new AI tool the latest \"boogeyman in software.\"\n\nThe analysts say that while there will be some AI disruption in store for software makers, it will be how companies adapt to the AI era that sets apart the winners and losers.\n\n\"There will naturally be some disruption, especially for point solutions, nice to have tools (as opposed to mission-critical systems), and vendors that are not able or willing to adapt to the new tech paradigm,\" they wrote, noting, \"This does not justify the broad-based indiscriminate selling we are seeing across the sector.\"\n\nBank of America compared the recent tech sector nosedive to the January 2025 sell-off fueled by Chinese AI DeepSeek, which spurred major short-term losses for Wall Street and erased $1 trillion in market capitalization in one day.\n\n\"In several respects, the current indiscriminate selloff resembles the DeepSeek-driven decline of Jan'25, which ultimately proved unfounded and was followed by higher capex and accelerating AI token growth.\"\n\nDespite investors' panic that DeepSeek marked a turning point in the AI arms race, major indexes not only recovered but ended 2025 up by double digits, fueled in large part by AI momentum. The analysts also noted that global cloud capital spending grew 69% in 2025 from the year-ago period, above previous forecasts in spite of worries.\n\nSoftware's dramatic downturn reflects negative investor sentiment around the sector, not a change in fundamentals.\n\nIn fact, Bank of America underlined the incongruence of Wall Street's thinking, which is driving \"inconsistency in AI stock moves.\"\n\n\"Price action implies AI capex is deteriorating to the point of weak ROI and unsustainable growth, while simultaneously suggesting AI adoption will be so pervasive and productivity-enhancing that long-standing software workflows and business models become obsolete. Both outcomes cannot occur at once.\"\n\nWilliam Blair analysts echoed a view calling software weakness a \"sentiment problem.\" They highlighted the post-earnings price declines in Microsoft and ServiceNow stocks, despite the companies' fundamentally strong prints. AMD saw a similar drop after its stock fell over 17% on Wednesday, despite earnings beating on the top and bottom lines.",
    "readingTime": 3,
    "keywords": [
      "blair analysts",
      "analysts say",
      "bank of america",
      "wall street",
      "william blair",
      "software",
      "sell-off",
      "sector",
      "tech",
      "losses"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ai-tech-stocks-wall-street-sentiment-software-nasdaq-sell-off-2026-2",
    "thumbnail_url": "https://i.insider.com/69839c1bd3c7faef0ecd9eb0?width=800&format=jpeg",
    "created_at": "2026-02-05T01:08:00.065Z",
    "topic": "finance"
  },
  {
    "slug": "a-legendary-coder-has-been-quietly-working-at-mira-muratis-thinking-machines-lab",
    "title": "A legendary coder has been quietly working at Mira Murati's Thinking Machines Lab",
    "description": "Thinking Machines Lab, led by former OpenAI CTO Mira Murati, attracts an award-winning coder, amid AI talent wars and a record funding push.",
    "fullText": "An award-winning coder has been quietly working for Thinking Machines Lab, the AI startup founded by former OpenAI Chief Technology Officer Mira Murati.\n\nNeal Wu is a three-time gold medal winner at an international programming Olympiad and a founding member of the AI coding startup Cognition, which is valued at $10 billion. He is the elder brother of Scott Wu, the CEO and cofounder of Cognition.\n\nThe move shows that Thinking Machines Lab, valued at over $10 billion, continues to attract star talent, even as it has been the target of aggressive AI poaching campaigns from rivals like Meta.\n\nThinking Machines Lab declined to comment. Wu and Cognition did not respond to requests for comment from Business Insider.\n\nBusiness Insider reviewed internal correspondence describing Wu as a team member of Thinking Machines Lab, which helps developers train and customize AI models. Neither Wu nor the secretive startup, which is based in San Francisco, has publicly announced his role.\n\nIt's not clear what Wu's exact position is. His LinkedIn profile says he has been working as a cofounder and advisor on \"something new\" since January 2025, around the time Murati's startup emerged from stealth.\n\nThinking Machines Lab has seen top staff leave in the last few months.\n\nMeta's aggressive poaching effort last year saw the tech giant dangling huge offers worth hundreds of millions of dollars. Meta eventually hired cofounder Andrew Tulloch after offering him a compensation package of up to $1.5 billion, The Wall Street Journal reported.\n\nIn January, its CTO, Barret Zoph, along with two other founding members, abruptly jumped ship back to OpenAI.\n\nThinking Machines Lab has a strong staff pedigree, with employees coming from AI labs like OpenAI and Meta. It was valued at more than $10 billion and raised a $2 billion seed round before launching a product.\n\nThe startup was seeking funds at a $50 billion valuation, Bloomberg reported in November.\n\nHave a tip? Contact this reporter via email at crollet@insider.com or on Signal and WhatsApp at 628-282-2811. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 2,
    "keywords": [
      "business insider",
      "machines lab",
      "thinking machines lab",
      "startup",
      "valued",
      "cofounder",
      "founding",
      "aggressive",
      "poaching",
      "staff"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mira-murati-thinking-machines-lab-hires-competitive-coder-neal-wu-2026-2",
    "thumbnail_url": "https://i.insider.com/6983a4e5e1ba468a96ab6810?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:07:59.938Z",
    "topic": "finance"
  },
  {
    "slug": "google-is-going-for-the-jugular-by-doubling-capex-and-outspending-the-rest-of-big-tech",
    "title": "Google is going for the jugular — by doubling capex and outspending the rest of Big Tech",
    "description": "Google plans to double AI infrastructure spending, reflecting Big Tech's aggressive AI investments.",
    "fullText": "Alphabet is dramatically increasing its chip and data center spending this year, underscoring its commitment to its AI bet amid Wall Street cost concerns.\n\nThe Google parent company is projecting capital expenditures of $175 billion to $185 billion in 2026, essentially double what it said it would spend on property and equipment last year, the company said in its fourth-quarter earnings report on Wednesday. In October, Google projected capex of $91 billion to $93 billion for 2025.\n\nGoogle's stock fell 2% after its earnings report.\n\nThe company attributed the significant increase in spending to meeting customer demand.\n\n\"We're seeing our AI investments and infrastructure drive revenue and growth across the board,\" the company said in its fourth-quarter earnings statement.\n\nThe numbers shouldn't surprise anyone who has followed Big Tech's spending patterns throughout the AI race. Microsoft, Meta, and Amazon have all aggressively ramped up spending.\n\nOn earnings calls with investors last week, Meta and Microsoft both said they'd spent more in 2025 than initially forecast.\n\nWhile both companies surpassed Wall Street's revenue expectations for the quarter, only Meta's stock went up on the news. Microsoft's stock fell by more than 6% as the market digested news of higher-than-anticipated capex.\n\nInvestors raised concerns about Microsoft's backlog, nearly half of which is attributed to a single customer — OpenAI.",
    "readingTime": 2,
    "keywords": [
      "fourth-quarter earnings",
      "stock",
      "concerns",
      "capex",
      "attributed",
      "customer",
      "revenue",
      "investors",
      "microsoft's",
      "wall"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/google-q4-2025-earnings-alphabet-capex-projected-to-double-2026-2",
    "thumbnail_url": "https://i.insider.com/6983be88a645d1188188b4c4?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:07:59.807Z",
    "topic": "finance"
  },
  {
    "slug": "alphabet-expects-2026-capex-in-the-range-of-175b-to-185b",
    "title": "Alphabet expects 2026 capex in the range of $175B to $185B",
    "description": "Alphabet beat on the top and bottom lines in its fourth-quarter earnings report, and said it expects to significantly increase spending on AI in 2026.",
    "fullText": "Alphabet beat on the top and bottom lines in its fourth-quarter earnings report on Wednesday, and said it expects to significantly increase spending on artificial intelligence in 2026.\n\nThe stock fell as much as 3% in extended trading.\n\nHere's how the company did, compared with estimates from analysts polled by LSEG:\n\nWall Street was also watching several other numbers in the report:\n\nAlphabet said it expects 2026 capital expenditures to be in the range of $175 billion to $185 billion. The top end of that forecast would be more than double its 2025 spend. The company said in October that it expected \"a significant increase\" to capex in 2026.\n\nThe company's capex spend will go toward investing in AI compute capacity for Google DeepMind and to meet \"significant cloud customer demand as well as strategic investments in other bets,\" Alphabet finance chief Anat Ashkenazi told analysts on a call Wednesday. She added that it would be used to \"improve the user experience and drive higher advertiser ROI in Google services.\"\n\nThe company saw revenue increase almost 18% year over year. Net income came in at $34.46 billion, up almost 30% compared to the year prior.\n\nAdvertising revenue came in at $82.28 billion, up 13.5% from a year ago. Revenue for YouTube ads increased almost 9% to $11.38 billion, but it fell short of analysts' expectations of $11.84 billion.\n\nAshkenazi said \"advertising results were negatively affected from the lapping of the strong spend on U.S. election in the fourth quarter of 2024.\"\n\nGoogle Cloud beat Wall Street's expectations, recording a nearly 48% increase in revenue from a year ago. Google's Cloud unit houses most of the company's AI services and products.\n\nThat business's backlog increased 55% sequentially and more than doubled year over year, reaching $240 billion at the end of the fourth quarter, said Ashkenazi.\n\nAlphabet and Google CEO Sundar Pichai said on the call Wednesday that its Gemini AI app now has more than 750 million monthly active users, up from 650 million monthly active users last quarter.\n\n\"As we scale, we are getting dramatically more efficient,\" Pichai said. \"We were able to lower Gemini serving unit costs by 78% over 2025 through model optimizations, efficiency and utilization improvements.\"\n\nOther Bets, which includes the company's life sciences unit Verily and self-driving car unit Waymo, reported revenue of $370 million during the quarter, down 7.5% from a year ago. Alphabet reported Other Bets' loss of $3.61 billion, a year-over-year increase of more than 200%.\n\nWaymo took a $2.1 billion stock-based compensation charge for the quarter, as it raised a new funding round at a $16 billion valuation. The majority of the charge was reflected in expenses for research and development, Ashkenazi said.\n\nThe self-driving wing of Alphabet ended 2025 having served 15 million trips in five major U.S. markets, including Austin, Texas; Atlanta; Los Angeles; Phoenix; and the San Francisco Bay Area. In January, Waymo began operating its service in Miami.\n\nWATCH: Google DeepMind CEO on state of the AI race, push towards AGI and AI impact on jobs",
    "readingTime": 3,
    "keywords": [
      "monthly active",
      "active users",
      "fourth quarter",
      "other bets",
      "increase",
      "revenue",
      "unit",
      "analysts",
      "company's",
      "almost"
    ],
    "qualityScore": 1,
    "link": "https://www.cnbc.com/2026/02/04/alphabet-googl-q4-2025-earnings.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108175922-1770242883198-108175922-1761769839779-108175922-1753295096051-gettyimages-2215577882-AFP_47H33MP.jpeg?v=1770249231&w=1920&h=1080",
    "created_at": "2026-02-05T01:07:59.766Z",
    "topic": "tech"
  },
  {
    "slug": "wall-street-eagerly-awaits-bullish-ai-updates-from-amazon-heading-into-its-latest-earnings",
    "title": "Wall Street eagerly awaits bullish AI updates from Amazon heading into its latest earnings",
    "description": "Amazon stock has been a laggard among its Magnificent Seven peers, but Wall Street is bullish that the latest results will show AI-driven strength.",
    "fullText": "Amazon will be the latest Magnificent Seven company to report earnings this week, with the e-commerce giant's results due on Thursday after the closing bell.\n\nAs analysts noted in their pre-earnings takes, Amazon has lagged behind most of its mega-cap tech peers for much of the past year. Yet, despite the lackluster performance, many investment banks are optimistic that the company will reveal fresh growth from Amazon Web Services (AWS) and provide key AI updates.\n\nWall Street expects Amazon tp post earnings-per-share of $1.96 for the quarter and revenue of $211.2 billion to $211.56 billion. Analysts anticipate that Amazon's earnings before interest and taxes will be $24.6 billion.\n\nHere's what analysts say they will be listening for on Amazon's earnings call.\n\nBofA analyst Justin Post expects Amazon to come in slightly above Wall Street estimates on key metrics, predicting revenue of $213 billion and EBIT of $26 billion. He also thinks that AWS growth will accelerate in 2026, helping Amazon regain some momentum.\n\n\"We expect 22% y/y AWS growth (above Street at 21% and an accel from 20% in 3Q) as greater capacity drives incremental sales, and we think investors could also be around 22% y/y,\" he said.\n\nWhile JPMorgan analysts remain bullish on Amazon heading into earnings, they do note some concerns regarding Amazon's position and strategy when it comes to AI. They find the recent AWS price increases encouraging, though, as they indicate strong demand.\n\n\"We remain bullish on AWS growth acceleration driven by core cloud growth & ramping AI contribution led by Project Rainier, Trn ramp, & new partnerships,\" they wrote.\n\nThe analysts added, though, that despite some concern, AWS' growth and the company's cost discipline should support strong free cash flow growth in the coming year, even as Amazon ramps up AI capex.\n\nDeutsche Bank analysts aren't expecting Amazon's Q4 earnings report to quickly juice the stock's performance, but they do anticipate a positive quarter overall, particularly in e-commerce demand and AWS-driven revenue growth.\n\n\"While we don't expect 4Q earnings to be the magic bullet that changes these fortunes, we do expect another positive earnings outcome to continue to chip away at this underperformance and believe Amazon can be one of the largest outperformers in our coverage in 2026,\" stated analyst Lee Horowitz.\n\nUBS analyst Stephen Ju and his team increased their estimates for both AWS growth and Amazon's capex, citing the company's plans to double capacity by 2027.\n\n\"As 4Q25 results marks the second quarter of AWS acceleration, we expect investor conviction to continue to rise and drive outperformance as capital continues to rotate into AMZN shares,\" Ju stated in a note to investors.\n\nHe added that his team expects Amazon to benefit from its investments in areas such as gross merchandise volume, advertising, and data, which are expected to help increase its free cash flow.\n\nAmazon remains a top pick for Mizuho, who sees it as a winner of the online advertising boom. Analyst Lloyd Walmsley also thinks it stands to benefit from ongoing demand both for its AI services and its e-commerce business.\n\n\"As GenAI compute demand increasingly shifts to inference this plays to Amazon's strength and should help further boost revenues at AWS,\" he noted. \"On retail, we see ongoing cost-to-serve improvements from more automation in fulfillment centers and continued improvement in logistics architecture, aided by rapid growth in advertising driving improving retail margins.\"\n\nCorrection: February 4, 2026 — An earlier version of this story misstated the date of Amazon earnings as Wednesday. The report is on Thursday.",
    "readingTime": 3,
    "keywords": [
      "free cash",
      "cash flow",
      "amazon's earnings",
      "aws growth",
      "wall street",
      "analysts",
      "demand",
      "amazon",
      "e-commerce",
      "quarter"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amazon-q4-earnings-preview-aws-ai-wall-street-estimates-ecommerce-2026-2",
    "thumbnail_url": "https://i.insider.com/6983a322a645d1188188b0df?width=1200&format=jpeg",
    "created_at": "2026-02-05T01:07:59.657Z",
    "topic": "finance"
  },
  {
    "slug": "using-react-and-claude-code-to-make-slides-awesome-and-easy",
    "title": "Using React and Claude Code to make slides awesome and easy",
    "description": "Use Claude Code or other AI agents to make slide decks -- easy, robust, and future-oriented. Leave behind Google Slides, Figma, and PowerPoint.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://newsletter.aimuscle.com/p/using-ai-agents-to-make-better-slides",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!CNCv!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aa97d8a-90e0-4536-8175-6fe3a0922861_1200x630.png",
    "created_at": "2026-02-05T01:07:58.727Z",
    "topic": "tech"
  },
  {
    "slug": "does-ai-have-humanlevel-intelligence-the-evidence-is-clear",
    "title": "Does AI have human-level intelligence? The evidence is clear",
    "description": "The vision of human-level machine intelligence laid out by Alan Turing in the 1950s is now a reality. Eyes unclouded by dread or hype will help us to prepare for what comes next.",
    "fullText": "In 1950, in a paper entitled ‘Computing Machinery and Intelligence’1, Alan Turing proposed his ‘imitation game’. Now known as the Turing test, it addressed a question that seemed purely hypothetical: could machines display the kind of flexible, general cognitive competence that is characteristic of human thought, such that they could pass themselves off as humans to unaware humans?\n\nThree-quarters of a century later, the answer looks like ‘yes’. In March 2025, the large language model (LLM) GPT-4.5, developed by OpenAI in San Francisco, California, was judged by humans in a Turing test to be human 73% of the time — more often than actual humans were2. Moreover, readers even preferred literary texts generated by LLMs over those written by human experts3.\n\nThis is far from all. LLMs have achieved gold-medal performance at the International Mathematical Olympiad, collaborated with leading mathematicians to prove theorems4, generated scientific hypotheses that have been validated in experiments5, solved problems from PhD exams, assisted professional programmers in writing code, composed poetry and much more — including chatting 24/7 with hundreds of millions of people around the world. In other words, LLMs have shown many signs of the sort of broad, flexible cognitive competence that was Turing’s focus — what we now call ‘general intelligence’, although Turing did not use the term.\n\nYet many experts baulk at saying that current AI models display artificial general intelligence (AGI) — and some doubt that they ever will. A March 2025 survey by the Association for the Advancement of Artificial Intelligence in Washington DC found that 76% of leading researchers thought that scaling up current AI approaches would be ‘unlikely’ or ‘very unlikely’ to yield AGI (see go.nature.com/4smn16b).\n\nWhat explains this disconnect? We suggest that the problem is part conceptual, because definitions of AGI are ambiguous and inconsistent; part emotional, because AGI raises fear of displacement and disruption; and part practical, as the term is entangled with commercial interests that can distort assessments. Precisely because AGI dominates public discourse, it is worth engaging with the concept in a more detached way: as a question about intelligence, rather than a pressing concern about social upheaval or an ever-postponed milestone in a business contract.\n\nIn writing this Comment, we approached this question from different perspectives — philosophy, machine learning, linguistics and cognitive science — and reached a consensus after extensive discussion. In what follows, we set out why we think that, once you clear away certain confusions, and strive to make fair comparisons and avoid anthropocentric biases, the conclusion is straightforward: by reasonable standards, including Turing’s own, we have artificial systems that are generally intelligent. The long-standing problem of creating AGI has been solved. Recognizing this fact matters — for policy, for risk and for understanding the nature of mind and even the world itself.\n\nWe assume, as we think Turing would have done, that humans have general intelligence. Some think that general intelligence does not exist at all, even in humans. Although this view is coherent and philosophically interesting, we set it aside here as being too disconnected from most AI discourse. But having made this assumption, how should we characterize general intelligence?\n\nA common informal definition of general intelligence, and the starting point of our discussions, is a system that can do almost all cognitive tasks that a human can do6,7. What tasks should be on that list engenders a lot of debate, but the phrase ‘a human’ also conceals a crucial ambiguity. Does it mean a top human expert for each task? Then no individual qualifies — Marie Curie won Nobel prizes in chemistry and physics but was not an expert in number theory. Does it mean a composite human with competence across the board? This, too, seems a high bar — Albert Einstein revolutionized physics, but he couldn’t speak Mandarin.\n\nA definition that excludes essentially all humans is not a definition of general intelligence; it is about something else, perhaps ideal expertise or collective intelligence. Rather, general intelligence is about having sufficient breadth and depth of cognitive abilities, with ‘sufficient’ anchored by paradigm cases. Breadth means abilities across multiple domains — mathematics, language, science, practical reasoning, creative tasks — in contrast to ‘narrow’ intelligences, such as a calculator or a chess-playing program. Depth means strong performance within those domains, not merely superficial engagement.\n\nHuman general intelligence admits degrees and variation. Children, average adults and an acknowledged genius such as Einstein all have general intelligence of varying level and profile. Individual humans excel or fall short in different domains. The same flexibility should apply to artificial systems: we should ask whether they have the core cognitive abilities at levels comparable to human-level general intelligence.\n\nRather than stipulating a definition, we draw on both actual and hypothetical cases of general intelligence — from Einstein to aliens to oracles — to triangulate the contours of the concept and refine it more systematically. Our conclusion: insofar as individual humans have general intelligence, current LLMs do, too.\n\nWe can start by identifying four features that are not required for general intelligence.\n\nPerfection. We don’t expect a physicist to match Einstein’s insights, or a biologist to replicate Charles Darwin’s breakthroughs. Few, if any, humans have perfect depth even within specialist areas of competence. Human general intelligence does not require perfection; neither should AGI.\n\nUniversality. No individual human can do every cognitive task, and other species have abilities that exceed our own: an octopus can control its eight arms independently; many insects can see parts of the electromagnetic spectrum that are invisible to humans. General intelligence does not require universal mastery of these skills; an AGI does not need perfect breadth.\n\nHuman similarity. Intelligence is a functional property that can be realized in different substrates — a point Turing embraced in 1950 by setting aside human biology1. Systems demonstrating general intelligence need not replicate human cognitive architecture or understand human cultural references. We would not demand these things of intelligent aliens; the same applies to machines.\n\nSuperintelligence. This is generally used to indicate any system that greatly exceeds the cognitive performance of humans in almost all areas. Superintelligence and AGI are often conflated, particularly in business contexts, in which ‘superintelligence’ often signals economic disruption. No human meets this standard; it should not be a requirement for AGI, either.\n\nWhat, then, is general intelligence? There is no ‘bright line’ test for its presence — any exact threshold is inevitably arbitrary. This might frustrate those who want exact criteria, but the vagueness is a feature, not a bug. Concepts such as ‘life’ and ‘health’ resist sharp definition yet remain useful; we recognize paradigm cases without needing exact boundaries. Humans are paradigm examples of general intelligence; a pocket calculator lacks it, despite superhuman ability at calculations.\n\nWhen we assess general intelligence or ability in other humans, we do not attempt to peer inside their heads to verify understanding — we infer it from behaviour, conversation and problem-solving. No single test is definitive, but evidence accumulates. The same applies to artificial systems.\n\nJust as we assess human general intelligence through progressively demanding tests, from basic literacy to PhD examinations, we can consider a cascade of increasingly demanding evidence that warrants progressively higher confidence in the presence of AGI.\n\nTuring-test level. Markers comparable to a basic school education: passing standard school exams, holding adequate conversations and performing simple reasoning. A decade ago, meeting these might have been widely accepted as sufficiently strong evidence for AGI.\n\nExpert level. Here, the demands escalate: gold-medal performance at international competitions, solving problems on PhD exams across multiple fields, writing and debugging complex code, fluency in dozens of languages, useful frontier research assistance as well as competent creative and practical problem-solving, from essay writing to trip planning. These achievements exceed many depictions of AGI in science fiction. The sentient supercomputer HAL 9000, from director Stanley Kubrick’s 1968 film 2001: A Space Odyssey, exhibited less breadth than current LLMs do. And current LLMs even exceed what we demand of humans: we credit individual people with general intelligence on the basis of much weaker evidence.\n\nSuperhuman level. Revolutionary scientific discoveries and consistent superiority over leading human experts across a range of domains. Such evidence would surely allow no reasonable debate about the presence of general intelligence in a machine — but it is not required evidence for its presence, because no human shows this.\n\nCurrent LLMs already cover the first two levels. As LLMs tackle progressively more difficult problems, alternative explanations for their capabilities — for instance, that they are gigantic ‘lookup tables’8 that retrieve pre-computed answers or ‘stochastic parrots’9 that regurgitate shallow regularities without grasping meaning or structure — become increasingly disconfirmed.\n\nOften, however, such claims just reappear with different predictions. Hypotheses that retreat before each new success, always predicting failure just beyond current achievements, are not compelling scientific theories, but a dogmatic commitment to perpetual scepticism.\n\nWe think the current evidence is clear. By inference to the best explanation — the same reasoning we use in attributing general intelligence to other people — we are observing AGI of a high degree. Machines such as those envisioned by Turing have arrived. Similar arguments have been made before10 (see also go.nature.com/49p6voq), and have engendered controversy and push-back. Our argument benefits from substantial advances and extra time. As of early 2026, the case for AGI is considerably more clear-cut.\n\nWe now examine ten common objections to the idea that current LLMs display general intelligence. Several of them echo objections that Turing himself considered in 1950. Each, we suggest, either conflates general intelligence with non-essential aspects of intelligence or applies standards that individual humans fail to meet.\n\nThey’re just parrots. The stochastic parrot objection says that LLMs merely interpolate training data. They can only recombine patterns they’ve encountered, so they must fail on genuinely new problems, or ‘out-of-distribution generalization’. This echoes ‘Lady Lovelace’s Objection’, inspired by Ada Lovelace’s 1843 remark and formulated by Turing as the claim that machines can “never do anything really new”1. Early LLMs certainly made mistakes on problems requiring reasoning and generalization beyond surface patterns in training data. But current LLMs can solve new, unpublished maths problems, perform near-optimal in-context statistical inference on scientific data11 and exhibit cross-domain transfer, in that training on code improves general reasoning across non-coding domains12. If critics demand revolutionary discoveries such as Einstein’s relativity, they are setting the bar too high, because very few humans make such discoveries either. Furthermore, there is no guarantee that human intelligence is not itself a sophisticated version of a stochastic parrot. All intelligence, human or artificial, must extract structure from correlational data; the question is how deep the extraction goes.\n\nThey lack world models. LLMs supposedly lack representations of their physical environment that are necessary for genuine understanding. But having a world model requires only the ability to predict what would happen if circumstances differed — to answer counterfactual questions. Ask a cutting-edge LLM what differs between dropping a glass or a pillow on a tile floor, and it will correctly predict shattering in one case and not the other. The ability of LLMs to solve olympiad mathematics and physics problems and assist with engineering design suggests that they possess functional models of physical principles. By these standards, LLMs already have world models. Furthermore, neural networks developed for specialized domains such as autonomous driving are already learning predictive models of physical scenes that support counterfactual reasoning and sophisticated physical awareness13.\n\nThey understand only words. This objection centres on the fact that LLMs are trained only on text, and so must be fundamentally limited to text-based tasks. Frontier models are now trained on images and other multimodal data, making this objection somewhat obsolete. Moreover, language is humanity’s most powerful tool for compressing and capturing knowledge about reality. LLMs can extract this compressed knowledge and apply it to distinctly non-linguistic tasks: helping researchers to design experiments — for example, suggesting what to test next in biology and materials science4 — goes beyond merely linguistic performance. We are yet to encounter the sharp limitations to LLM performance that this objection predicts.\n\nThey don’t have bodies. Without embodiment, critics argue, there can be no general intelligence. This reflects an anthropocentric bias that seems to be wielded only against AI. People would ascribe intelligence to a disembodied alien communicating by radio, or to a brain sustained in a vat. An entity that responds accurately to any question, but never moves or acts physically, would be regarded as profoundly intelligent. Physicist Stephen Hawking interacted with the world almost entirely through text and synthesized speech, yet his physical limitations in no way diminished his intelligence. Motor capabilities are separable from general intelligence.\n\nThey lack agency. It is true that present-day LLMs do not form independent goals or initiate action unprompted, as humans do. Even ‘agentic’ AI systems — such as frontier coding agents — typically act only when a user triggers a task, even if they can then automatically draft features and fix bugs. But intelligence does not require autonomy. Like the Oracle of Delphi — understood as a system that produces accurate answers only when queried — current LLMs need not initiate goals to count as intelligent. Humans typically have both general intelligence and autonomy, but we should not thereby conclude that one requires the other. Autonomy matters for moral responsibility, but it is not constitutive of intelligence.\n\ndoi: https://doi.org/10.1038/d41586-026-00285-6\n\nTuring, A. M. Mind LIX, 433–460 (1950).\n\nArticle \n\n Google Scholar\n\nJones, C. R. & Bergen, B. K. Preprint at arXiv https://doi.org/10.48550/arXiv.2503.23674 (2025).\n\nChakrabarty, T., Ginsburg, J. C. & Dhillon, P. Preprint at arXiv https://doi.org/10.48550/arXiv.2510.13939 (2025).\n\nBubeck, S. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2511.16072 (2025).\n\nRizvi, S. A. et al. Preprint at bioRxiv https://doi.org/10.1101/2025.04.14.648850 (2025).\n\nMorris, M. R. et al. Proc. 41st Int. Conf. Mach. Learn. 235, 36308–36321 (PMLR, 2024).\n\nHendrycks, D. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2510.18212 (2025).\n\nBlock, N. Phil. Rev. 90, 5–43 (1981).\n\nArticle \n\n Google Scholar\n\nBender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. in Proc. 2021 ACM Conf. Fairness Account. Transparency 610–623 (Association for Computing Machinery, 2021).\n\nBubeck, S. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2303.12712 (2023).\n\nDai, Y., Gao, Z., Sattar, Y., Dean, S. & Sun, J. Preprint at arXiv https://doi.org/10.48550/arXiv.2506.07298 (2025).\n\nMa, Y. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2309.16298 (2023).\n\nNVIDIA. Preprint at arXiv https://doi.org/10.48550/arXiv.2501.03575 (2025).\n\nL.B. is an employee of Goodfire AI, an AI interpretability company. Goodfire had no role in the conceptualization, writing or decision to publish this work\n\nThe smart sensors improving the world’s biggest cities\n\nBiodiversity conservation has an evidence problem — it’s time to fix it\n\nMachine learning slashes the testing needed to work out battery lifetimes\n\nThe smart sensors improving the world’s biggest cities\n\n‘It means I can sleep at night’: how sensors are helping to solve scientists’ problems\n\nWhat my cave stay taught me about sensors\n\n‘It means I can sleep at night’: how sensors are helping to solve scientists’ problems\n\nQuantum computers will finally be useful: what’s behind the revolution\n\nSynthesizing scientific literature with retrieval-augmented language models",
    "readingTime": 13,
    "keywords": [
      "computing machinery",
      "google scholar",
      "phd exams",
      "world’s biggest",
      "biggest cities",
      "turing test",
      "stochastic parrot",
      "solve scientists",
      "machine learning",
      "paradigm cases"
    ],
    "qualityScore": 1,
    "link": "https://www.nature.com/articles/d41586-026-00285-6",
    "thumbnail_url": "https://media.nature.com/lw1200/magazine-assets/d41586-026-00285-6/d41586-026-00285-6_51994698.jpg",
    "created_at": "2026-02-05T01:07:58.316Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-boss-jensen-huang-says-aireplacement-fears-tanking-software-stocks-is-the-most-illogical-thing-in-the-world",
    "title": "Nvidia boss Jensen Huang says AI-replacement fears tanking software stocks is the 'most illogical thing in the world'",
    "description": "Nvidia CEO Jensen Huang dismissed fears that AI will replace software as tech stocks were rattled by new tools from Anthropic this week.",
    "fullText": "Software stocks dragged the tech sector lower on Tuesday.\n\nNvidia CEO Jensen Huang weighed in, saying fears of AI disruption are \"illogical.\"\n\nThe software-led rout marks a rotation into other parts of the market that have lagged lately.\n\nTech stocks continued to wobble on Wednesday as Wall Street's worries that AI will replace software, a notion that Nvidia CEO Jensen Huang dismissed as \"illogical\" this week.\n\nThe tech-heavy Nasdaq Composite and the S&P 500 extended losses from Tuesday's session. The software sector led a tech-sell-off with the iShares Expanded Tech-Software Sector ETF down nearly 4%, adding to its year-to-date 22% loss.\n\nBut Wall Street's fears about further disruption from AI were brushed aside by one of the icons of the AI space on Tuesday. Nvidia CEO Jensen Huang weighed in on the latest jitters at a Cisco AI event.\n\n\"There's this notion that the tool industry is in decline and will be replaced by AI. You could tell because there's a whole bunch of software companies whose stock prices are under a lot of pressure because somehow AI is going to replace them. It is the most illogical thing in the world and time will prove itself.\"\n\nHuang outlined his view that software is a tool for AI to use, rather than replace, explaining that AI will use the tools software offers rather than reinventing its own.\n\nThe CEO named Service Now, SAP, Cadence, and Synopsis, specifically as bright spots in the space.\n\nRecent weakness in the software sector comes amid a broader market rotation away from tech. The iShares Expanded Tech-Software Sector ETF has seen a nearly 20% decline in the past year, and the sector fell into a bear market last week.\n\nHere are some of the stocks dragging the iShares Expanded Tech-Software Sector ETF lower during Wednesday's session:",
    "readingTime": 2,
    "keywords": [
      "ishares expanded",
      "expanded tech-software",
      "nvidia ceo",
      "ceo jensen",
      "jensen huang",
      "huang weighed",
      "tech-software sector",
      "sector etf",
      "software sector",
      "stocks"
    ],
    "qualityScore": 0.85,
    "link": "https://finance.yahoo.com/news/nvidia-boss-jensen-huang-says-164113817.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/Z.vixGsydMosswo8iz.zOQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA-/https://media.zenfs.com/en/business_insider_consolidated_articles_886/60bd862eaf25acc933528e04a45e342a",
    "created_at": "2026-02-05T01:07:56.678Z",
    "topic": "finance"
  },
  {
    "slug": "tech-stocks-widen-losses-amid-ai-disruption-woes-mixed-earnings",
    "title": "Tech Stocks Widen Losses Amid AI Disruption Woes, Mixed Earnings",
    "description": "Tech stocks pushed US equites lower as traders assessed the damage from the artificial intelligence-driven selloff in software firms and poured over the latest earnings reports.",
    "fullText": "MarketsBy Joel LeonSaveTech stocks pushed US equites lower as traders assessed the damage from the artificial intelligence-driven selloff in software firms and poured over the latest earnings reports.The Nasdaq 100 Index slumped 1.6% at 12:03 p.m. in New York falling as much as 2% earlier in the day. Advanced Micro Devices Inc. plunged following an underwhelming earnings report. The S&P 500 Index fell 0.4%. The Cboe Volatility Index hovered near 19.",
    "readingTime": 1,
    "keywords": [
      "earnings",
      "index"
    ],
    "qualityScore": 0.2,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/us-stocks-dip-as-amd-results-ai-concerns-weigh-on-sentiment",
    "thumbnail_url": "https://assets.bwbx.io/s3/lightsaber/_next/static/media/social-markets.e062a0c0.jpg",
    "created_at": "2026-02-04T18:35:36.282Z",
    "topic": "finance"
  },
  {
    "slug": "apple-beats-tech-stocks-by-most-in-a-year-as-it-avoids-ai-panic",
    "title": "Apple Beats Tech Stocks by Most in a Year as It Avoids AI Panic",
    "description": "Apple Inc. is trading in the opposite direction of technology stocks and the broader market as investors increasingly view it as an oasis of safety amid fears of artificial intelligence disruption.",
    "fullText": "TechnologyBig TechBy Ryan VlastelicaSaveApple Inc. is trading in the opposite direction of technology stocks and the broader market as investors increasingly view it as an oasis of safety amid fears of artificial intelligence disruption. The iPhone maker’s shares were up 1.8% at around 12:30 p.m. in New York on Wednesday, compared with a 2.4% decline in the tech-heavy Nasdaq 100 Index. That means Apple is outperforming by its widest degree since early 2025. The move extends a recent trend, with Apple up nearly 6% to start this month, while the index is down 3.3%.",
    "readingTime": 1,
    "keywords": [
      "apple",
      "index"
    ],
    "qualityScore": 0.55,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/apple-beats-tech-stocks-by-most-in-a-year-as-it-avoids-ai-panic",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/itkqBg9ywWcc/v1/1200x800.jpg",
    "created_at": "2026-02-04T18:35:35.408Z",
    "topic": "finance"
  },
  {
    "slug": "claude-code-for-fullstack-dev-the-minimal-setup",
    "title": "Claude Code for Fullstack Dev – The Minimal Setup",
    "description": "Claude Code for fullstack development doesn't require complex workflows. Three essentials -— full-stack debugging visibility, LLM-friendly docs, and an opinionated framework -— are all you need.",
    "fullText": "There's a lot of hype around vibe coding with Claude Code.\n\nThe good news is that it's warranted. Claude Code can take care of some surprisingly complex coding tasks.\n\nThe bad news is that it's over-hyped, with claims of amazing apps being vibe coded in a couple hours, or complex workflows that use 10 parallel sub-agents running in a loop to replace the work of 5 software engineers.\n\nIf you're not already a Claude Code power user, all this hype can leave you questioning how to use it effectively. Am I missing out on productivity gains by not adopting this insane new workflow? Should I be using subagents, commands, skills, or MCP servers for my use case? If so, how?\n\nI was asking myself these exact questions, so I did a good few weeks of research and testing, and reached the following conclusion:\n\nWith just a few well-picked tools, and Claude Code's basic features, you have enough to \"vibe code\" great full-stack apps.\n\nWeird seeing all these complex LLM workflows and tools. Meanwhile I run one continuous Claude convo per project... I've never used a subagent. Never used MCP…\nAnd I have wildly good results ¯_(ツ)_/¯\n\n— Chris McCord, creator of the Phoenix framework\n\nWhile Claude Code's abilities are impressive, you quickly realize that there is a lot of feature overlap and stuff you just generally won't have to touch often if you get a few things right from the start. I'll explain what these are, and then go into more detail on each topic within this article.\n\n1. Provide it with full-stack debugging visibility\n\n2. Give Claude Code access to up-to-date, LLM-friendly documentation\n\n3. Use the right tech stack or framework\n\nWith this foundation, you'll be able to build and deploy fullstack apps easily by mostly using Claude Code's default workflows, and just a couple custom commands/skills (I also packed these ideas into a simple Claude Code plugin you can install and use, but I'll talk more about that later).\n\nThis approach works because it provides guardrails and the right patterns for the agent to follow, so you can spend more time on business logic implementation, and less time working out specifications and technical details.\n\nEssentially, you get to work with the agent on what you want, rather than having to explain how you want it, bringing that magic feeling of AI-assisted coding to complex, full-stack apps.\n\nOur typical AI-assisted coding workflow tends to look like this: we prompt, then wait, then look over the generated code (maybe), then check on things in the browser. If we've got some errors or some bad looking frontend designs, we copy and paste them and try to (better) explain what we want.\n\nWhat's worse is we might have to do this a few times before we're satisfied, or get things working again.\n\nBut to do this, we need to give Claude Code a \"set of eyes\". Luckily, this is possible with the right features and tools, allowing it to see the results of the code it wrote, and quickly modify it autonomously if it encounters an error anywhere in the stack.\n\nClaude Code introduced their background tasks feature which lets it execute long-running tasks, like app development servers (e.g. npm run dev), on the side without blocking Claude's progress on other work. The nice part is that Claude can continue to read and respond to the output of this task while you work.\n\nTo run commands in the background, you can either:\n\nEven though Claude can read the output of your background tasks, there are times you may want to check up on them yourself, and you still can do that. Just use the down arrow to highlight the background task message and click enter.\n\nThis is great because now Claude can react to issues that occur from building and serving your code. Unfortunately, it still can't react to errors that occur while your app is running in the browser.\n\nBut there's a cool way to solve this.\n\nThe missing piece so far is giving Claude the ability to actually see the result of the code it wrote, most typically what the UI looks like.\n\nProblems that only show up after the app is running in the browser, like design issues and runtime errors, aren't yet visible to the agent. So this tends to be where the human steps in to report back to: \"the button is misaligned\", \"there's a 404 error when trying to login\", \"the console says something about undefined\".\n\nBut luckily, we can arm Claude Code with browser automation tools to solve for this. These tools allow for programmatic control of the browser, like loading pages, clicking buttons, inspecting elements, reading console logs, and even taking screenshots.\n\nThis closes the loop for the entire stack and gives Claude the autonomy to complete larger features tasks entirely on its own.\n\nChrome DevTools MCP server is one of the best options currently out there, though there are many alternatives. It's easy to install, and specializes in browser debugging and performance insights.\n\nTo install it, run the following command in the terminal to add it to your current project:\n\nThen start a new Claude session and give it a prompt like:\n\nYou should see a separate Chrome instance open up being controlled by Claude. Go ahead and give it more tasks like:\n\nNow, when you're implementing full-stack features in your app, ask Claude to verify the feature works correctly by checking the logs in the development server, as well as in the browser with the Chrome DevTools MCP.\n\nAlternatively, if you want Claude to always automatically verify changes in the browser with the DevTools MCP without having to explicitly ask it to do so, you can add a rule to Claude's memory in your CLAUDE.md file.\n\nYou've probably experienced this: you're vibe coding using a library's API, and the AI confidently writes code that would have worked perfectly… two versions ago. Or it creates some crazy, over-engineered work-around to a known issue that has a simple solution in its documentation.\n\nThis is because the model's training data has a cut-off date. LLMs and tools like Claude have no way of knowing about the updated code patterns, unless you give it access to the current documentation.\n\nBut, as Andrej Karpathy observed, most documentation contains content that's not relevant for LLMs:\n\n99% of libraries still have docs that basically render to some pretty .html static pages assuming a human will click through them. In 2025 the docs should be a your_project.md text file that is intended to go into the context window of an LLM.\n\nThere's research to back up his claim, showing that LLM performance degrades when irrelevant content, like HTML syntax or verbose instructions for humans, is added to context as it distracts from the task and reduces accuracy.\n\nIn other words, every unnecessary token in your context window makes the AI slightly worse at its job.\n\nSo we need to be giving Claude Code the right kind of docs.\n\nThe fix is simple: give the agent the ability to fetch and read the relevant, LLM-optimized documentation for the tools you're using.\n\nHere are the two main ways developers are accomplishing this:\n\nThe Model Context Protocol (MCP) is an open standard for connecting AI applications (i.e. agents, LLMs) to external systems. Claude Code can communicate with an MCP server to access specialized tools and information.\n\nThere are already tons of MCP servers for popular tools, like Supabase, Jira, Canva, Notion, and Vercel. Claude Code has a docs section listing these and many more, with instructions on how to install them if you’re interested.\n\nDeveloper Tool MCP servers like Supabase and Vercel have tools which will fetch documentation for the agent based on a query. There are some pros and cons to this approach, though.\n\nBecause LLMs don’t have a real memory, they have to load information into context with every new session, such as the tools they can use. A single MCP server can add around 15-30 tools to the context, and with multiple servers you can easily consume 10-20% or more of your LLM’s context window before you even begin working.\n\nIf you want to see how much of your context window is already used up, you can run the /context slash command in an active Claude Code session.\nThe example above shows that 2.5% of the context window is used up by just one MCP server.\n\nAnd as an LLM’s context window fills up, it’s performance degrades. Some devs even suggest starting a new session once the context is 75% full to avoid this, which is possible with the /clear command in Claude Code. You can also run the /compact command which creates a summary of your current session’s context and passes it along to the next session.\n\nLuckily, if your main reason for using an MCP server is just for documentation searching, then an alternative open standard exists that may be a better fit.\n\nLLMs.txt has quickly become the standard for providing LLMs with context-friendly versions of websites at an /llms.txt path.\n\nTry it out on some of your favorite developer tools URLs, such as:\n\nAlthough llms.txt files might vary widely in the content they surface, they always follow the same format of a simple markdown file with the title of the website and some links. That’s it! This allows LLMs to get precise information without all the fluff.\n\nHere are some of the pros and cons of using llms.txt for documentation fetching:\n\nIn my opinion, I think that fetching docs via an llms.txt URL is the better approach as its more context efficient. For example, a typical documentation file is ~100 tokens versus 5,000-10,000 tokens for just one MCP server.\n\nThat’s a 10x reduction in context usage.\n\nClaude Code is also great at navigating documentation maps to only fetch the most relevant information. Plus, you get the added benefit that llms.txt files are easier for humans to reference as well.\n\nIt's also the method that Claude Code uses for its own doc fetching internally. So when you ask it a question about its own features, it will first fetch its documentation map markdown file URL to find the correct guides.\n\nSo now that you know how to give Claude Code access to up-to-date documenation, the next question to answer is which tools might you need to provide the docs for?\n\nAnd the most obvious answer is the tech stack or framework you’re building your full-stack app with.\n\nThis is probably the most overlooked of the 3 pillars.\n\nStarting with a tech stack that AI can easily reason about will make the job of building the app you want significantly easier. There are a lot of options out there, but luckily there are many solid choices, such as:\n\nIn 2026, you'll be able to get very far with Claude Code and any of the frameworks listed above. But while these frameworks all offer good conventions, and are responsible for stitching together the most important parts of the stack, most of these still require some sort of additional integration.\n\nFor NextJS, which is more client-side focused, you have to chose and connect your own database layer. For Laravel and Rails, which unify backend and database, you need to decide which client to use and how it will talk to your backend.\n\nThat's why a lot of developers will reach for popular stacks/combos that include these frameworks, such as:\n\nYou might have noticed as well that the T3 Stack is the only one to include an authentication library, NextAuth. That's because the backend-focused frameworks, Laravel and Rails, have an opinionated way of adding authentication to your app already, but NextJS does not.\n\nWasp, on the other hand, is the only one of the bunch that unifies all parts of the stack — client ↔ backend ↔ database — while also being opinionated on features.\n\nThis is all great, but which one should you ultimately choose?\n\nWell, the more opinionated the framework, the better it vibes with AI.\n\nIf a framework is opinionated, it means there's usually one obvious place to put code and one common pattern to follow. The AI doesn't have to guess. They've already made decisions ahead of time so you (and your agent) don't have to. They've encoded architectural wisdom into conventions, picked the libraries to use, the way authentication is wired, and how the app should be structured.\n\nSo when there are fewer decisions to make, less boilerplate to write, and fewer tools to stitch together, the development process becomes more reliable.\n\nAnd as the app gets more complex, AI doesn't lose its focus, because the framework is handling a lot of underlying complexity. You can also understand and inspect what is being generated more easily, and avoid building yourself into a messy corner.\n\nConsider that opinionated frameworks can reduce boilerplate by 60-80%. Wasp's auth declaration, for example, replaces 500+ lines of typical authentication code with a 10 to 15-line config. That's 97% less code that needs to be generated and audited!\n\nOf these frameworks, Wasp is definitely the most opinionated but it's also the newest kid on the block. After that, Laravel and Rails are probably tied for a close second, but they are built on PHP and Rails, respectively, and come with their own distinct ecosystems, so you'll most likely have to pair them with a frontend JavaScript framework like React, too. NextJS, is the most popular but least opinionated of the bunch, so it means there is more complexity and up-front choices you and your AI have to deal with, but this offers more flexibility in the long run.\n\nSo in the end, the choice you make largely depends on what you're trying to achieve, what you're comfortable with, and how much flexibility you need.\n\nJust remember, the more a framework gives you structure and defaults, the easier it is for AI to generate code that fits correctly — and the less time you spend fixing confusing or inconsistent output.\n\nOk. So we've established that working with an opinionated framework means that it it manages a lot of the complexity for you.\n\nBut what does that practically mean when using it with Claude Code?\n\nIn a sense, the framework acts as a large specification that both you and Claude already understand and agree on.\n\nInstead of multi-turn conversations to figure out HOW things should be built, you get to just say WHAT you want built.\n\nSo when you tell Claude \"add a new model for Comments\" or \"add a user account settings feature\", Claude will know exactly what that means and where it all goes. And you also get the added benefit that the implementations follow best practices and are backed by the decisions of experienced professionals behind the framework, and is not just some LLM hastily implementing a feature on the wrong assumptions.\n\nThis isn't to say that you no longer have to do good planning, create a good spec, or Product Requirement Doc for your agent to follow. This can still be a really important step when vibe coding (or practicing \"spec-driven development\").\n\nBut it does mean that, with an opinionated full-stack framework, much less of your planning phases need to be devoted to discussing architectural and technical implementation details.\n\nIf you want to put this theoretical approach I discussed above to the practical test, then I suggest you try out the Wasp plugin we created for Claude Code.\n\nWe, the Wasp framework creators, maintain the plugin, so we've battle tested it with Wasp. Plus we're a very responsive community and we're listening to feedback and improving it all the time.\n\nFrom there, you can tell Claude to \"start the dev server\" and it will walk you through spinning up fullstack visibility as we outlined above. Or ask it to implement a Wasp feature and watch it fetch version-matched documentation guides for you!\n\nMore importantly, Wasp as a framework pushes into territory that's even more AI-native than the rest, because of its central configuration file(s) where the app is defined.\n\nThis main config file is like your app's blueprint. Wasp takes these declarations, and manages the code for those features for you.\n\nTake this example config file authentication snippet as reference:\n\nThis is what an authentication implementation in Wasp looks like. That's it.\n\nThis 8-line config generates what would typically require 500-800 lines of code: components, session handling, password hashing, OAuth flows, and database schemas. Claude just needs to know what auth methods you want.\n\nClaude doesn't need to worry about choosing what kind of auth implementation to use, or generating any of the glue code. It can just get straight to work building features.\n\nI've spent this whole article arguing that you can ignore a lot Claude Code features for fullstack app development. But I don't want to leave you with the impression that those features are useless.\n\nCustom subagents, commands, and skills shine when you're doing the same task repeatedly with consistent criteria, e.g.:\n\nThese are tasks where you want the same process followed every time and where a well-configured subagent with specific rules makes sense.\n\nIn most cases, reach for complexity only when the simpler approach stops working.\n\nWith these three ingredients I think most fullstack app developers can get the vast majority of the work done without reaching for much more:\n\nWith these in place, Claude Code's basic toolset—exploring, planning, reading, writing, running commands—is enough to build real, complex fullstack applications. The subagents, hooks, plugins, and complex configurations are there if you need them, but honestly most of the time, you won't.",
    "readingTime": 15,
    "keywords": [
      "claude code",
      "code's basic",
      "chrome devtools",
      "ai-assisted coding",
      "llm’s context",
      "mcp server",
      "mcp servers",
      "performance degrades",
      "subagents commands",
      "llms.txt files"
    ],
    "qualityScore": 1,
    "link": "https://wasp.sh/blog/2026/01/29/claude-code-fullstack-development-essentials",
    "thumbnail_url": "https://wasp.sh/img/claude-code-fullstack/banner.webp",
    "created_at": "2026-02-04T18:35:31.879Z",
    "topic": "tech"
  },
  {
    "slug": "debugging-with-claude-what-are-your-learnings",
    "title": "Debugging with Claude – What Are Your Learnings?",
    "description": "Struggling with Claude Code debugging? Learn why AI fails at fixing bugs—and two practical methods to give it the visibility it needs to solve problems on the first try.",
    "fullText": "Imagine trying to teach someone to cook over the phone.\n\nYou’re walking them through your grandmother’s pasta recipe—the one with the garlic that needs to be just golden, not brown. You describe every step perfectly. The timing. The technique. The little flip of the wrist when you toss the noodles.\n\nAnd then they say: “It’s burning. What do I do?”\n\nHere’s the thing: you can’t help them. Not really. Because you can’t see the pan. You can’t see how high the flame is. You can’t see that they accidentally grabbed the chili flakes instead of oregano. All you have is their panicked description and your best guess about what might be going wrong.\n\nThis, my friend, is exactly what happens when you ask Claude Code to fix a bug.\n\nYou’ve been on this ride before. I know you have.\n\nYou describe the bug to Claude. Carefully. Thoroughly. You even add screenshots and error messages because you’re a good communicator, dammit.\n\nSo you describe the bug again—this time with more adjectives and maybe a few capitalized words for emphasis. Claude proposes a slightly different fix. Still broken. You rephrase. Claude tries another angle. Round and round we go.\n\nThis is the debugging merry-go-round, and nobody buys tickets to this ride on purpose.\n\nThe instinct—the very human instinct—is to blame the AI.\n\nThat’s actually the right question.\n\nHere’s what I’ve learned after spending more time than I’d like to admit arguing with AI about bugs: Claude almost never fails because it lacks intelligence. It fails because it lacks visibility.\n\nThink about what you have access to when you’re debugging. Browser dev tools. Console logs scrolling in real-time. Network requests you can inspect. Elements that highlight when you hover. The actual, living, breathing behavior playing out on your screen.\n\nYou’re asking a brilliant chef to fix your burning pasta—but they can only read the recipe card. They can’t see the flame. They can’t smell the smoke. They’re working with incomplete information and filling in the gaps with educated guesses.\n\nSometimes those guesses are right. (Claude is genuinely brilliant at guessing.)\n\nMost of the time? Merry-go-round.\n\nAfter countless Claude Code debugging sessions—some triumphant, many humbling—I’ve noticed two categories that consistently send AI spinning:\n\nReact’s useEffect dependencies.\n\nRace conditions. Stale closures. Data that shapeshifts mid-lifecycle like some kind of JavaScript werewolf. These bugs are invisible in the code itself. You can stare at the component for hours (ask me how I know) and see nothing wrong. The bug only reveals itself at runtime—in the sequence of events, the timing of updates, the order of renders.\n\nIt’s happening in dimensions Claude can’t perceive.\n\nCSS being overridden by inline JavaScript. WordPress functions receiving unexpected null values from somewhere upstream. Error messages that point to line 7374 of a core file—not your code, but code three function calls removed from the actual problem.\n\nBut the source? Hidden in cascading calls, plugin interactions, systems talking to systems.\n\nClaude can’t solve either category by reading code alone.\n\n(I told you to stay with me. Here’s where it gets good.)\n\nLet me walk you through a real example.\n\nBecause theory is nice, but showing you what this looks like in practice? That’s the good stuff.\n\nI had a Products Browser component. Simple filtering and search functionality—the kind of thing you build in an afternoon and then spend three days debugging because life is like that sometimes.\n\nEach control worked beautifully in isolation:\n\nSearch for “apple” → Three results. Beautiful.\n\nFilter by “laptops” → Five results. Chef’s kiss.\n\nSearch “apple” + category “laptops” → Broken. The filter gets completely ignored, like I never selected it at all.\n\nClassic React hook dependency bug.\n\nIf you’re experienced with React, you spot this pattern in your sleep. But if you’re newer to the framework—or if you vibe-coded this component and touched a dozen files before realizing something broke—you’re stuck waiting for Claude to get lucky.\n\nI spent three rounds asking Claude to fix it. Each fix addressed a different theoretical cause. None worked.\n\nThat’s when I stopped arguing and started instrumenting.\n\nInstead of another “please fix this” prompt, I asked Claude to help me see what was happening:\n\nNotice what I didn’t say: “Fix this bug.”\n\nWhat I said: “Add logging to track data changes.”\n\nThis is the mindset shift that changes everything.\n\nClaude added console.log statements to every useEffect that touched the view state:\n\nEach log captured which effect triggered, what the current values were, and what got computed. Basically, Claude created a running transcript of everything happening inside my component’s brain.\n\nI opened the browser, selected “laptops” from the category filter, then typed “apple” in the search box.\n\nThe console lit up like a Christmas tree of evidence.\n\nHere’s where the magic happens. I copied that console output—all of it—and pasted it directly into Claude:\n\nAnd Claude? Claude saw everything:\n\nClaude found the bug immediately.\n\nThe logs revealed the whole story: when I selected a category, useEffect:filters fired and correctly filtered the products. But then when I typed in the search box, useEffect:search fired—and it ran against the full product list, completely ignoring the category filter.\n\nThe search effect was overwriting the filter results.\n\nLast effect wins. (JavaScript, you beautiful chaos gremlin.)\n\nClaude proposed the fix: replace multiple competing useEffect hooks with a single useMemo that applies all transforms together:\n\nThe difference between “Claude guessing for 20 minutes” and “Claude solving it instantly” was 30 seconds of logging.\n\nThat’s not hyperbole. That’s just… math.\n\nThe second method works for a different beast entirely—the kind of bug where even the error message is lying to you.\n\nHere’s a WordPress error that haunted me for hours:\n\nIf you’ve done any WordPress development, you recognize this particular flavor of suffering.\n\nThe error points to core WordPress files—not your code. Something, somewhere, is passing null to a function that expects a string. But where? The error message is about as helpful as a fortune cookie that just says “bad things happened.”\n\nI’d made changes to several theme files.\n\nAny one of them could be the culprit.\n\nAnd the cascading nature of WordPress hooks meant the error could originate three or four function calls before the actual crash.\n\nAfter a few rounds of Claude trying random fixes (bless its heart), I tried something completely different.\n\nInstead of “fix this,” I asked Claude to brainstorm debugging approaches—and to visualize them with ASCII diagrams.\n\n(I know. ASCII diagrams. In 2025. But stay with me, because this is where Claude Code debugging gets genuinely interesting.)\n\nClaude started by analyzing the flow of the problem:\n\nThe diagram showed exactly what was happening: some theme code was passing null to WordPress core functions, which then passed that null to PHP string functions, which threw the deprecation warning.\n\nBut which theme code? Claude identified the suspect locations:\n\nEach with code examples showing what the problematic pattern might look like.\n\nThis is Claude thinking out loud, visually. And it’s incredibly useful for Claude Code debugging because now we’re not guessing—we’re investigating.\n\nRather than jumping to a single fix and hoping, Claude laid out several approaches:\n\nFour different angles of attack.\n\nThis is what systematic debugging looks like—and it’s exactly what you need when you’re stuck in the merry-go-round.\n\nHere’s where Opus 4.5 surprised me.\n\nInstead of settling on the first approach, it validated its theories by actually searching the codebase:\n\nIt searched for wp_redirect calls, add_filter patterns, get_option usages—systematically eliminating possibilities like a detective working through a suspect list.\n\nThen it updated its diagnosis based on what it found:\n\nThe error was coming from path-handling functions—something was returning a null path where a string was expected.\n\nClaude concluded with a clear summary of everything we now knew:\n\nAnd multiple approaches to fix it, ranked by how surgical they’d be:\n\nFirst attempt. Approach A—adding a debug backtrace—immediately revealed a function in FluentCartBridge.php that was returning null when $screen->id was empty.\n\nAll those rounds of failed attempts? They were doomed from the start because Claude was guessing blindly. Once it could see the error chain visually—once it had a map instead of just a destination—the solution was obvious.\n\nBoth of these methods work because they address the same fundamental gap in Claude Code debugging: AI doesn’t fail because it’s not smart enough. It fails because it can’t see what you see.\n\nWhen you’re debugging, you have browser dev tools, console logs, network requests, and actual behavior unfolding on your screen. Claude has code files.\n\nIt’s working with incomplete information and filling the gaps with educated guesses.\n\nHere’s the mindset shift that changed everything for me:\n\n👉 Stop expecting AI to figure it out. Start helping AI see what you see.\n\nYou become the eyes. AI becomes the analytical brain that processes patterns and proposes solutions based on the evidence you feed it.\n\nIt’s a collaboration. A partnership. Not a vending machine where you insert a problem and expect a solution to drop out.\n\nAdd logs when the bug involves:\n\nThe logs transform invisible runtime behavior into visible evidence.\n\nReact’s useEffect, state updates, and re-renders happen in milliseconds—too fast to trace mentally, but perfectly captured by console.log. Feed those logs to Claude, and suddenly it can see the movie instead of just reading the script.\n\nUse the brainstorming approach when:\n\nAsking Claude to brainstorm with diagrams forces it to slow down and map the problem systematically. It prevents the merry-go-round where AI keeps trying variations of the same failed approach. By exploring multiple angles first, you often find the root cause on the very first real attempt.\n\nHere’s what I want you to take away from all of this:\n\nDon’t argue with AI about what it can’t see. Show it.\n\nThe next time Claude can’t solve a bug after a few rounds, resist the urge to rephrase your complaint. Don’t add more adjectives. Don’t type in all caps. (I know. I KNOW. But still.)\n\nInstead, ask yourself: “What am I seeing that Claude isn’t?”\n\nThen find a way to bridge that gap—through logs, through diagrams, through screenshots, through any method that gives AI the visibility it needs to actually help you.\n\nThat bug that’s been driving you up the wall? The one Claude keeps missing?\n\nThen watch it solve what seemed impossible.\n\nYou’ve got this. And now Claude does too.\n\nFreelance web developer. Since 2012 he’s built WordPress plugins, internal tools, and AI-powered apps. He writes The Art of Vibe Coding, a practical newsletter that helps indie builders ship faster with AI—calmly.",
    "readingTime": 9,
    "keywords": [
      "react’s useeffect",
      "ascii diagrams",
      "network requests",
      "mindset shift",
      "dev tools",
      "educated guesses",
      "browser dev",
      "tools console",
      "search box",
      "returning null"
    ],
    "qualityScore": 1,
    "link": "https://www.nathanonn.com/claude-code-debugging-visibility-methods/",
    "thumbnail_url": "https://www.nathanonn.com/wp-content/uploads/2026/01/image-3.png",
    "created_at": "2026-02-04T18:35:17.485Z",
    "topic": "tech"
  },
  {
    "slug": "ai-chip-startup-positron-raises-230m-from-arm-qatar-to-compete-with-nvidia",
    "title": "AI Chip Startup Positron Raises $230M from Arm, Qatar to Compete with Nvidia",
    "description": "The round values the aspiring Nvidia rival at more than $1 billion",
    "fullText": "TechnologyAIBy Dina BassSaveAI chip startup Positron, which is trying to compete with Nvidia Corp., raised $230 million in a funding round from investors including Arm Holdings Plc and the Qatar Investment Authority. The round values the company at more than $1 billion, including funds raised, Chief Executive Officer Mitesh Agrawal said in an interview. The round was co-led by Arena Private Wealth, formerly OCM Private Wealth, Positron customer Jump Trading, and Unless. Helena, as well as previous investors including Valor Equity Partners, Atreides Management and DFJ Growth, also participated.",
    "readingTime": 1,
    "keywords": [
      "round",
      "investors",
      "positron",
      "wealth"
    ],
    "qualityScore": 0.45,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/ai-chip-startup-positron-raises-230-million-from-arm-qatar-to-compete-with-nvidia",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/icVZXVpLTNMA/v0/1200x923.jpg",
    "created_at": "2026-02-04T18:35:02.122Z",
    "topic": "tech"
  },
  {
    "slug": "perplexity-was-my-favorite-ai-tool-then-it-started-lying-to-me",
    "title": "Perplexity was my favorite AI tool. Then it started lying to me",
    "description": "Really wish it didn't.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.xda-developers.com/perplexity-was-my-favorite-ai-tool-then-it-started-lying-to-me/",
    "thumbnail_url": "https://static0.xdaimages.com/wordpress/wp-content/uploads/wm/2026/02/perplexity-on-mac.jpeg?w=1600&h=900&fit=crop",
    "created_at": "2026-02-04T18:35:01.160Z",
    "topic": "tech"
  },
  {
    "slug": "worried-about-ai-taking-jobs-exmicrosoft-exec-tells-parents-what-kind-of-education-matters-most-for-their-kids",
    "title": "Worried about AI taking jobs? Ex-Microsoft exec tells parents what kind of education matters most for their kids.",
    "description": "Craig Mundie, ex-Microsoft exec, discusses AI's impact on education. He urges a shift to personalized learning with AI and STEM-humanities integration.",
    "fullText": "Ex-Microsoft exec Craig Mundie has heard this question again and again — parents asking him a version of the same worry: Their kids are heading toward college, artificial intelligence is advancing fast, and jobs feel uncertain. What, exactly, should their kids be studying?\n\nThat question — what education will matter most in five years — reflects a deeper uncertainty about the future.\n\nMundie, who spent 22 years at Microsoft helping steer the company's vision toward AI and retired as the company's chief research and strategy officer in 2014, says that parents are simply asking the wrong question.\n\nIt's not only the students who have to change to fit the new AI era — it's the education system itself, said Mundie, who now advises other executives on AI and public policy.\n\nRather than chasing down the right job, Mundie urges families to prepare kids for a world where learning itself becomes continuous, personalized, and done in partnership with intelligent machines.\n\nDuring an interview with Business Insider's Reem Makhoul in June, Mundie said artificial intelligence and robotics are poised to reshape work more deeply than past technologies. See the edited cut of his interview below:\n\nThat shift, Mundie said, forces a bigger question than which job skills will survive. It challenges how societies define human value. This is something Mundie's been pondering for over a decade.\n\nIn his 2015 book \"Genesis,\" Mundie, with co-authors Eric Schmidt and Henry Kissinger, examined how AI could alter the human experience. \"What we say is we have to think differently about how we value ourselves and what we do.\"\n\nFor much of history, he said, dignity has been tied to work because people had to work to survive. AI could loosen that link by automating more tasks across both physical and intellectual labor.\n\nMeanwhile, humans will need to learn how to work alongside intelligent machines, and the traditional higher-education system doesn't offer a clear path toward that, right now.\n\nHe described today's education system as sharply divided between STEM and the humanities. The liberal arts emphasize reasoning, but at the expense of special technical skills you learn in STEM fields, Mundie said.\n\nStudents will need both skills moving forward. \"If I could create a new curriculum in college, it would be a liberal education in technology,\" and STEM, he said.\n\nMundie went further, questioning whether the classroom model that dominates education today still makes sense.\n\nHe traced that structure back to the printing press, which created a surge in written information and a need for mass literacy. Schools, he said, became an efficient \"machine for teaching\" because societies lacked enough individual tutors.\n\nWe can have scalable, polymathic teachers, Mundie said. \"We can have as many teachers as we want now because the AI will be the teacher.\"\n\nHe said this opens the door to a more personalized, Socratic model of learning, where students can interact continuously with an intelligent system that adapts to their curiosity, pace, and interests. Progress would be limited less by standardized curricula and more by a student's motivation and capacity.\n\nSchools and universities have been slow to embrace this shift. Early reactions often involved banning AI tools outright. \"They've now given up on that,\" Mundie said.\n\nThat resistance, he added, is typical of incumbent systems. \"The natural tendency of the incumbent is to preserve the incumbent system,\" or make only incremental changes, he added. But \"when you get something as powerful as these AIs, most incumbent systems are not going to be preserved.\"\n\nHe also pointed to early experiments on the right track, like versions of Khan Academy, an online non-profit educational platform founded in 2008 and headquartered in California. It uses an AI tutor, named Khanmigo, designed to guide students rather than simply give answers. In those systems, he said, the AI nudges students toward better questions and deeper understanding.\n\n\"So that's the difference between sort of a broad chat about anything interface and an AI application that was specifically oriented around teaching,\" he said, adding, \"That's just one tiny example of how people will build more and more apps on these common artificial intelligence platforms.\"\n\n\"We will move beyond the specific generic interface to a world of millions of applications that are really customized in some clever way to guide people to solutions in the areas they care about,\" he said. These agents may, in fact, do much of the work autonomously by interacting with others, he added.\n\nMundie said parents and older generations may have difficulty imagining this model, while children are likely to adapt quickly. The harder question, in his view, is whether educational institutions are willing to change.",
    "readingTime": 4,
    "keywords": [
      "intelligent machines",
      "artificial intelligence",
      "incumbent systems",
      "education system",
      "students",
      "toward",
      "mundie",
      "parents",
      "kids",
      "skills"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ex-microsoft-exec-ai-expert-says-colleges-need-new-curriculum-2026-2",
    "thumbnail_url": "https://i.insider.com/69824327a645d11881889472?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:56.550Z",
    "topic": "finance"
  },
  {
    "slug": "ben-horowitz-says-fears-of-an-aifueled-job-apocalypse-are-based-on-a-flawed-assumption",
    "title": "Ben Horowitz says fears of an AI-fueled job apocalypse are based on a flawed assumption",
    "description": "The A16z cofounder said history shows automation destroys some jobs but creates new ones, and that confident predictions about AI and work are misguided.",
    "fullText": "Ben Horowitz doesn't buy the idea that artificial intelligence is about to wipe out work as we know it.\n\nThe cofounder of Andreessen Horowitz says many of the loudest warnings about AI-driven mass unemployment rest on a flawed premise: that the future of work is predictable.\n\nHistory, he said, suggests the opposite.\n\n\"I think people are acting as though it's very predictable when it's not at all predictable,\" Horowitz said in an interview on the \"Invest Like The Best\" podcast on Tuesday.\n\n\"Why are you so sure it's going to happen next? And why are you so sure no jobs are going to be created? I don't think it's nearly as predictable as people are saying,\" he added.\n\nHorowitz's comments come as tech executives, economists, and policymakers remain split over how many jobs AI could eliminate, how quickly those changes might arrive, and whether displacement is inevitable at all.\n\nSome senior AI researchers, including computer scientist Geoffrey Hinton, often called the \"Godfather of AI,\" UC Berkeley professor Stuart Russell, University of Louisville computer science professor Roman Yampolskiy, and some tech leaders, including Anthropic's CEO Dario Amodei, have warned of AI's potential to replace large swaths of jobs.\n\nOthers, including OpenAI's CEO Sam Altman and Nvidia CEO Jensen Huang, have said AI is more likely to reshape jobs and create new roles rather than erase work outright.\n\nHorowitz, by contrast, took a longer historical view, framing AI as the latest chapter in a long arc of automation that has repeatedly destroyed jobs while ultimately expanding opportunity.\n\nThe most extreme example, he said, is agriculture. In the early US economy, roughly 95% of jobs were tied to farming, he said, adding that almost all of those roles are gone now.\n\n\"We've been automating things since the agricultural days,\" Horowitz said. \"Almost all those jobs have been eliminated. And the jobs we have now, the people doing agriculture wouldn't even consider jobs.\"\n\nThe mistake, he said, is assuming today's job categories are a reliable guide to tomorrow's economy. Each major technology shift created entirely new forms of work that were difficult, if not impossible, to imagine beforehand, he said.\n\n\"The idea that we could imagine all the jobs that are going to come, sitting here now, that AI is going to enable, I think is low,\" he said.\n\nHorowitz was also skeptical of the urgency behind job-loss predictions, saying that modern AI didn't appear overnight.\n\n\"We've had AI going right — ImageNet was what, 2012 — and then natural language stuff was like 2015, and then ChatGPT was 2022,\" Horowitz said. \"Where's all the job destruction? Why hasn't it happened yet?\"\n\nThat doesn't mean AI won't eliminate certain roles.\n\nHorowitz said he expects jobs centered on processing information for others to face pressure.\n\nBut he believes demand for creativity will rise.\n\n\"I don't really think that the door is going to close behind you,\" he said. \"I think the opportunities tend to multiply when you open up a new way of doing things.\"",
    "readingTime": 3,
    "keywords": [
      "jobs",
      "predictable",
      "it's",
      "roles",
      "horowitz",
      "doesn't",
      "idea",
      "sure",
      "created",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ben-horowitz-dismisses-fears-of-ai-job-apocalypse-mass-unemployment-2026-2",
    "thumbnail_url": "https://i.insider.com/69831fe1a645d1188188a51b?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:56.537Z",
    "topic": "finance"
  },
  {
    "slug": "google-keeping-its-place-atop-the-ai-rankings-might-mean-doing-something-its-resisting",
    "title": "Google keeping its place atop the AI rankings might mean doing something it's resisting",
    "description": "As Google's earnings report looms, its AI strategy and ad business with Gemini 3 put it at the forefront of a volatile AI market.",
    "fullText": "Google has had a great run since its last earnings report. Can it last?\n\nThat's the big question ahead of the tech giant's earnings report this afternoon. BI's Hugh Langley, our resident Google expert, breaks down what you need to look out for in the numbers and on the call.\n\nGoogle has been on a three-month tear since its last check-in, launching Gemini 3 to much fanfare and joining the $4 trillion market-cap club. The string of successes put it at the front of the highly competitive AI race.\n\n(A quick aside on these arbitrary rankings. You could argue Nvidia's the biggest winner of the AI race, but it's really in its own bucket. I'm talking about companies with competing chatbots. No further questions at this time.)\n\nBut here's the thing about the AI race. It's highly volatile. One quarter, you're the belle of the ball; the next, you can't even get your texts returned. Just ask Meta and Microsoft.\n\nInvestors are expecting a big swing one way or another. Google's implied one-day share move following earnings is 4.9%, according to Bloomberg data.\n\nYou don't have to search too far to find the key to Google's earnings.\n\nGoogle Search (get it) and the ad business built around it are still the company's crown jewel. It's also an area many have speculated AI would upend. (Why \"Google it\" if you can \"ChatGPT it\"?)\n\nGoogle has managed to incorporate AI features without cannibalizing its cash cow. But it's a tricky balancing act, and Google is already blurring the line between Search and Gemini.\n\nThe real tipping point will be ads. Google executives have been adamant that there are no immediate plans to bring them to Gemini anytime soon. However, if Search's massive traffic ultimately gets passed to the chatbots, advertising's arrival feels inevitable.\n\nIt also might not have a choice.\n\nOpenAI is preparing to roll out ads to some ChatGPT users despite Sam Altman previously calling them a \"last resort.\" In such a highly competitive race, and with so much money on the line, giving your rivals any type of headstart is a risk.",
    "readingTime": 2,
    "keywords": [
      "highly competitive",
      "earnings",
      "race",
      "it's",
      "google",
      "chatbots",
      "google's",
      "chatgpt",
      "gemini",
      "search"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-advertisting-question-on-gemini-earnings-2026-2",
    "thumbnail_url": "https://i.insider.com/6982656ed3c7faef0ecd8b80?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:56.005Z",
    "topic": "finance"
  },
  {
    "slug": "anthropics-super-bowl-spot-skewers-chatgpt-ads-are-coming-to-ai-but-not-to-claude",
    "title": "Anthropic's Super Bowl spot skewers ChatGPT: 'Ads are coming to AI, but not to Claude'",
    "description": "While Anthropic doesn't outright name ChatGPT, it's clear who the target of its coming Super Bowl ad is.",
    "fullText": "Anthropic is taking a shot at OpenAI on the biggest stage possible.\n\nOn Wednesday, Anthropic rolled out a glitzy ad campaign that will air nationally during Sunday's Super Bowl, which implicitly centers on OpenAI's plans to bring advertising to ChatGPT.\n\n\"Ads are coming to AI. But not to Claude,\" the tag line reads.\n\nThe declaration shows that Anthropic is willing to draw a line in the sand with its no-AI-ads-in-chatbots stance. But the move also means Anthropic is declining to pursue a potentially key revenue stream for Claude at a time when AI companies are spending more than ever in the AI race.\n\nWith much of Anthropic's revenue coming from its enterprise business, the company may feel it's a worthwhile tradeoff and a way to further differentiate its offerings from OpenAI's.\n\nAnthropic has booked a 30-second spot during the game and an additional 1-minute ad during the pregame for what is historically the most-watched live television event in the US.\n\nThe 30-second spot features a scrawny guy asking a buffed trainer, \"Can I get a six pack quickly?\" While at first the trainer gives helpful information in the voice of an AI chatbot, the trainer quickly segues into an ad.\n\n\"Confidence isn't just built in the gym, try Step Boost Max, the insoles that add one vertical inch of height and help short kings stand tall,\" the trainer says.\n\nAnthropic's 1-minute-long spot is even more provocative. It features an adult man in therapy who is trying to connect more with his mother. After giving some general advice, the AI chatbot-like voice segues into an ad for a fictional dating service for younger men seeking older women.\n\n\"If the relationship can't be fixed, find an emotional connection with other older women on Golden Encounters, the mature dating site that connects sensitive cubs with roaring cougars,\" the therapist responds in the voice of a chatbot.\n\nTwo additional ads in the campaign feature a woman in a restaurant asking for feedback on a new business idea and a student asking a professor for help with an essay.\n\nIt's not immediately clear how much Anthropic is spending on the Super Bowl campaign. Mike Marshall, head of global advertising for NBCUniversal, whose network has the rights to this year's game, recently said a 30-second spot costs roughly $8 million.\n\nLast month, OpenAI announced plans to begin testing ads in the US for its free and Go tiers of ChatGPT. While anticipated, the announcement illustrated just how much CEO Sam Altman has changed his views on monetizing the chatbot with ads.\n\n\"Ads plus AI is sort of uniquely unsettling to me,\" Altman said during an event at Harvard University in May 2024. \"I kind of think of ads as a last resort for us for a business model.\"\n\nAs part of the announcement, OpenAI said that ads would \"not influence the answers ChatGPT gives you.\"\n\n\"Answers are optimized based on what's most helpful to you,\" the company said in the announcement. \"Ads are always separate and clearly labeled.\"\n\nIn case the ad campaign wasn't clear enough, Anthropic released its own lengthy statement pledging to keep ads off of Claude.\n\n\"Ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking,\" the company said in a statement on Wednesday.\n\nAnthropic, founded by seven former OpenAI employees, including CEO Dario Amodei, has also repeatedly shown it's not above taking implicit shots at its rival, though the Super Bowl ads represent an escalation of the AI wars.\n\nIn December, Amodei poked fun at companies that declare \"Code Reds,\" denounced others that are \"Yoloing\" by making risky bets on future AI demand, and extolled the virtues of Anthropic's business model, built on the enterprise market. Individually and collectively, the remarks came across as implicit shots at OpenAI, though Amodei repeatedly declined to name his target.\n\nWhen asked, \"who is Yoloing,\" Amodei responded, \"So that's not a question I'm going to answer.\"\n\nAnd while Anthropic still isn't naming names, the target of their taunts will be hard for Super Bowl fans to ignore.",
    "readingTime": 4,
    "keywords": [
      "older women",
      "implicit shots",
      "second spot",
      "business model",
      "super bowl",
      "campaign",
      "trainer",
      "anthropic's",
      "it's",
      "helpful"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-super-bowl-openai-chatgpt-ads-claude-2026-2",
    "thumbnail_url": "https://i.insider.com/69835c9ce1ba468a96ab5df9?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:55.616Z",
    "topic": "finance"
  },
  {
    "slug": "nvidia-boss-jensen-huang-says-aireplacement-fears-tanking-software-stocks-is-the-most-illogical-thing-in-the-world",
    "title": "Nvidia boss Jensen Huang says AI-replacement fears tanking software stocks is the 'most illogical thing in the world'",
    "description": "Nvidia CEO Jensen Huang dismissed fears that AI will replace software as tech stocks were rattled by new tools from Anthropic this week.",
    "fullText": "Tech stocks continued to wobble on Wednesday as Wall Street's worries that AI will replace software, a notion that Nvidia CEO Jensen Huang dismissed as \"illogical\" this week.\n\nThe tech-heavy Nasdaq Composite and the S&P 500 extended losses from Tuesday's session. The software sector led a tech-sell-off with the iShares Expanded Tech-Software Sector ETF down nearly 4%, adding to its year-to-date 22% loss.\n\nBut Wall Street's fears about further disruption from AI were brushed aside by one of the icons of the AI space on Tuesday. Nvidia CEO Jensen Huang weighed in on the latest jitters at a Cisco AI event.\n\n\"There's this notion that the tool industry is in decline and will be replaced by AI. You could tell because there's a whole bunch of software companies whose stock prices are under a lot of pressure because somehow AI is going to replace them. It is the most illogical thing in the world and time will prove itself.\"\n\nHuang outlined his view that software is a tool for AI to use, rather than replace, explaining that AI will use the tools software offers rather than reinventing its own.\n\nThe CEO named Service Now, SAP, Cadence, and Synopsis, specifically as bright spots in the space.\n\nRecent weakness in the software sector comes amid a broader market rotation away from tech. The iShares Expanded Tech-Software Sector ETF has seen a nearly 20% decline in the past year, and the sector fell into a bear market last week.\n\nHere are some of the stocks dragging the iShares Expanded Tech-Software Sector ETF lower during Wednesday's session:",
    "readingTime": 2,
    "keywords": [
      "nvidia ceo",
      "ceo jensen",
      "jensen huang",
      "ishares expanded",
      "expanded tech-software",
      "tech-software sector",
      "sector etf",
      "software sector",
      "replace",
      "stocks"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/ai-software-tech-stocks-sell-off-nvidia-jensen-huang-illogical-2026-2",
    "thumbnail_url": "https://i.insider.com/69835eeed3c7faef0ecd9624?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:55.449Z",
    "topic": "finance"
  },
  {
    "slug": "the-brutal-wipeout-in-software-stocks-is-getting-even-worse-in-3-charts",
    "title": "The brutal wipeout in software stocks is getting even worse, in 3 charts",
    "description": "Software stocks entered a bear market last week. The latest bout of selling was brought on by Anthropic's unveiling of a new plugin for its Cowork agent.",
    "fullText": "Software stocks are taking it on the chin again on Wednesday.\n\nThe iShares Expanded Tech-Software Sector ETF was down another 3% as the software sector faced selling pressure for a second straight session.\n\nThe decline comes after a particularly brutal sell-off on Tuesday as investors panicked about Anthropic's new AI tool, which can perform a range of clerical and administrative tasks for people working in the legal industry. Legal software stocks got crushed, but the selling spilled over into the broader space by the end of Tuesday's session.\n\nSome of Wednesday's big losers in the software space include:\n\nThe tech-heavy Nasdaq was down as much as 2% for a second day, tumbling deeper into the red in afternoon trading.\n\nHere's where major indexes stood around 1:20 p.m. ET on Wednesday:\n\nThe sector is now mired in its worst sell-off since the \"Liberation Day\" market crash, having officially entered a bear market last week after Microsoft's earnings fueled more concern about valuations and growing AI capex.\n\nThe decline reflects two fears that are top of mind for investors: On the one hand, tech stocks are broadly expensive and fears of an AI bubble still loom over markets.\n\nOn the other hand, there is great uncertainty as to how the business model for software companies could change, Craig Johnson, a managing director and chief market technician at Piper Sandler, told Business Insider.\n\nInvestors are mulling the extent to which these are existential threats to certain companies or if they'll somehow ultimately adapt to new realities.\n\n\"Investors believe that ultimately, look, if three years from now you're going to be sitting on your desktop asking your desktop to generate code to do certain things for you, it's impossible to believe that there won't be benefits for customers removing some of the software,\" Adam Parker, the founder of Trivariate Research, said.\n\n\"I think it's just a manifestation of the worries that market participants have had for quite some time,\" Michael Brown, a senior research strategist at Pepperstone, added about Tuesday's big sell-off.\n\nYet, the latest weakness in the space is a continuation of what's been a year of weakness for software names.\n\nHere are three charts that show how striking the decline has been.\n\nSoftware stocks officially entered a bear market last week. The iShares Expanded Tech-Software Sector ETF is now 27% from its September 2025 peak.\n\nThe price-to-earnings ratio, a traditional measure of stock valuations, has also dropped sharply among software makers' stocks. The P/E ratio in the S&P software index has dropped to below 60x, down from a peak of around 85x last summer.\n\nOracle, Varonis, CommVault, and Circle make up the group of the worst-performing stocks in the sector. All four stocks are down more than 50% from their September highs.\n\nThe outlook for software on Wall Street looks mixed after Tuesday's rout.\n\nPepperstone's Brown said he believes the bull case for tech broadly is still \"very, very robust,\" pointing to strong earnings and expectations for solid economic growth in the US. The S&P 500 is on track to post 12% earnings growth for the fourth quarter, which would mark its fifth straight period of double-digit growth, according to FactSet.\n\n\"I'm still very much in buy-the-dip mode,\" Brown said. \"Overall you are looking at a market where the path of least resistance leads higher.\"\n\nMost software stocks have more room to fall from a technical perspective, Piper Sandler's Johnson said. He pointed to the relative strength index, a technical gauge used to predict the near-term direction of stock prices, and said he believed most software names had at least 10%-20% more downside to go before finding a floor.\n\n\"CRM, ServiceNow, Oracle, DataDog, Snow, doesn't really matter,\" he said. \"They all have not pulled back or corrected to identifiable support levels.\"\n\nAdam Parker, the founder of Trivariate Research, said he believed software would be stuck in a \"guilty until proven innocent mode,\" with investors punishing companies until they saw stronger earnings growth.\n\n\"What the market's telling you is that the analyst estimates are way too high,\" he said of earnings. \"I feel like it's a falling knife that I don't want to catch personally.\"",
    "readingTime": 4,
    "keywords": [
      "ishares expanded",
      "expanded tech-software",
      "tech-software sector",
      "sector etf",
      "officially entered",
      "entered bear",
      "bear market",
      "earnings growth",
      "software stocks",
      "wednesday the"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/software-stocks-sell-off-charts-tech-outlook-anthropic-ai-valuations-2026-2",
    "thumbnail_url": "https://i.insider.com/69837dcdd3c7faef0ecd9aa0?width=1200&format=jpeg",
    "created_at": "2026-02-04T18:34:55.343Z",
    "topic": "finance"
  },
  {
    "slug": "how-to-foster-psychological-safety-when-ai-erodes-trust-on-your-team",
    "title": "How to Foster Psychological Safety When AI Erodes Trust on Your Team",
    "description": "As organizations rapidly integrate AI into their workflows, leaders are uncovering the impact on team dynamics. The same AI tools that promise to enhance productivity can create predictable patterns of team dysfunction that mirror classic organizational behavior problems. Many leaders treat these issues as technology problems to be solved with better tools or training, rather than team effectiveness issues that require many of the principles we know work for human collaboration. Understanding human-AI team dynamics is crucial not only from an organizational theory and practice standpoint, but for preventing costly errors and the erosion of team trust.",
    "fullText": "How to Foster Psychological Safety When AI Erodes Trust on Your Team by Jayshree Seth and Amy C. EdmondsonFebruary 4, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintThere may be something unsettling happening on your team. Despite expected productivity gains from integrating AI tools, overall team performance appears to be declining. People are starting to second-guess themselves, and trust is eroding in ways that are hard to pinpoint.",
    "readingTime": 1,
    "keywords": [
      "trust",
      "team"
    ],
    "qualityScore": 0.3,
    "link": "https://hbr.org/2026/02/how-to-foster-psychological-safety-when-ai-erodes-trust-on-your-team",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_04_2246930678.jpg",
    "created_at": "2026-02-04T18:34:54.756Z",
    "topic": "business"
  },
  {
    "slug": "in-an-automated-world-human-hospitality-is-a-competitive-advantage",
    "title": "In an Automated World, Human Hospitality Is a Competitive Advantage",
    "description": "As the adoption of AI to provide customer service accelerates, managers would be well advised to remember that the human touch still matters.  Drawing on examples from Ritz‑Carlton and Four Seasons, the authors show how hospitality is rooted in dignity, purpose, and employee judgment—not scripts or efficiency metrics. Leaders should empower staff, measure what truly matters for connection, and select employees for their natural inclination to care. Deep hospitality requires discipline and an ethos that treats people—not processes—as the point, creating relationships that differentiate a brand.",
    "fullText": "In an Automated World, Human Hospitality Is a Competitive Advantage by Horst Schulze and Micah SolomonFebruary 4, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintIn the last decade, AI-powered chatbots have taken the realm of customer service by storm. The advent of generative AI in the last few years has only accelerated this drive. But as leaders race to adopt these systems, they would be well advised to consider a crucial factor: the value of the human touch in providing customer service—what we call deep hospitality.",
    "readingTime": 1,
    "keywords": [
      "human",
      "hospitality",
      "customer"
    ],
    "qualityScore": 0.35,
    "link": "https://hbr.org/2026/02/in-an-automated-world-human-hospitality-is-a-competitive-advantage",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_04_149618285.jpg",
    "created_at": "2026-02-04T18:34:54.746Z",
    "topic": "business"
  },
  {
    "slug": "tech-stocks-go-into-free-fall-as-it-dawns-on-traders-that-ai-has-the-ability-to-cut-revenues-across-the-board",
    "title": "Tech stocks go into free fall as it dawns on traders that AI has the ability to cut revenues across the board",
    "description": "Tech companies are waking up to the fact that AI may be more likely to replace them than help them.",
    "fullText": "Until very recently, the narrative around AI was that the $600 billion of annual corporate capital expenditure (“capex”) fueling it was good for stocks in the short term. The companies receiving that money as new revenue (AI model makers, data center constructors, and the energy companies supplying them) would be the immediate beneficiaries. The efficiencies delivered by AI would be good for tech and non-tech companies alike. The Big Tech hyperscalers have always argued that the demand from their revenue-paying clients far exceeded their ability to supply AI services. \n\nThat narrative was turned on its head in the past 24 hours as it dawned on traders that AI also has the ability to reduce the revenues of a vast range of adjacent tech companies. \n\nThe argument—advanced by Palantir CEO Alex Karp and CTO Shyam Sankar on their recent earnings call—is that AI is now so good at writing or managing enterprise software that it threatens to make irrelevant a range of tech companies that have, for years, enjoyed recurring revenues by providing enterprise apps to companies on a software-as-a-service (SaaS) basis.\n\nHow is AI threatening traditional SaaS company revenues?\n\nWhich companies were most affected by yesterday's decline?\n\nWhat are Palantir's AI capabilities reducing migration times?\n\nWhat caused the $300 billion tech stock selloff?\n\nThat led to a widespread selloff of tech stocks, wiping away $300 billion in market cap in a single session.\n\nS&P 500 futures were flat this morning after closing down 0.84% last night.\n\nSaaS companies took major hits: Microsoft closed down 2.87%; SAP was down 3.29% this morning on the German market; Salesforce lost 6.85% yesterday and was further down in overnight trading; ServiceNow was down 6.97% yesterday and was marginally lower overnight also.\n\nPalantir’s Sankar said on the call that his company’s “AI forward deployed engineer” product—which allows clients to manage software and codebases through natural language commands—is able to reduce the time it takes to complete “complex SAP ERP migrations” from “years of work” to “as little as two weeks.” (ERP stands for “enterprise resource planning,” and it refers to a service offered by SAP around helping companies transition from aging legacy systems into new ones.)\n\nKarp added: “In the American market, we have inbound [requests from clients] where people have already seen proof points at other companies and not on one use case. [There is] a myriad of use cases.”\n\nJefferies analysts Akshat Agarwal and Ayush Bansal—who focus on Indian enterprise tech companies—published a note this morning arguing that AI has the ability to reduce the revenues of a wide range of tech companies:\n\n“Anthropic’s Cowork plug‑ins and Palantir’s claims of faster SAP migrations highlight how AI could potentially erode application service revenues for IT firms. With application services being 40% to 70% of revenues [for tech companies in India], IT firms face growth pressures. Consensus growth estimates don’t fully reflect this, posing downside risk to valuations,” they warned. Claude Cowork is like a general purpose work assistant that can organize tasks and files autonomously.",
    "readingTime": 3,
    "keywords": [
      "revenues",
      "enterprise",
      "clients",
      "ability",
      "reduce",
      "range",
      "saas",
      "market",
      "morning",
      "tech"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/tech-stocks-free-fall-dawns-111243245.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/VrZVg5ZwvP6xhfPwEjiudQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/eee90c79fb15c766e7b1510c72445d3b",
    "created_at": "2026-02-04T18:34:53.738Z",
    "topic": "finance"
  },
  {
    "slug": "chatgpt-is-down-again",
    "title": "ChatGPT Is Down (Again)",
    "description": "The bot experienced downtime Tuesday afternoon and Wednesday morning.",
    "fullText": "If you tried talking to ChatGPT this morning, you might have found it unresponsive—something unusual for the bot that always has something say. It's not your internet connection, and it isn't your OpenAI account: ChatGPT is down.\n\nAccording to Downdetector, owned by Lifehacker parent company Ziff Davis, users started reporting issues with ChatGPT at 11:56 a.m. ET. Those reports ballooned by 12:11 p.m., as the total number of incidents as of this article currently sits above 7,000. If you're an avid ChatGPT user, you might have also had issues with the bot yesterday: Downdetector shows over 25,000 reports of down time starting at 2:56 p.m. Tuesday and resolving around 4:11 p.m. the same day.\n\nAs with all outages, OpenAI will likely figure out a patch for the issue soon enough. But these outages are becoming more common. There was the Verizon outage, of course, but other services like TikTok have also experienced intermittent periods of downtime.",
    "readingTime": 1,
    "keywords": [
      "chatgpt",
      "openai",
      "downdetector",
      "reports",
      "outages"
    ],
    "qualityScore": 0.65,
    "link": "https://lifehacker.com/tech/chatgpt-is-down-again?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGMVP6Q8RJSV91ZSAB2NTGQW/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-04T18:34:53.725Z",
    "topic": "tech"
  },
  {
    "slug": "mg-asset-management-ceo-pinto-diversification-is-key",
    "title": "M&G Asset Management CEO Pinto: Diversification is Key",
    "description": "Joseph Pinto, CEO of M&G Asset Management, on AI, private credit, and reallocation of global capital. 'We've seen rebalancing', he says. 'Clients have decided to reallocate a bit more to Europe or to Asia'. Pinto speaks with Bloomberg's Guy Johnson, Anna Edwards and Tom Mackenzie on 'The Opening Trade'.",
    "fullText": "Feb 4th, 2026M&G Asset Management CEO Pinto: Diversification is KeyJoseph Pinto, CEO of M&G Asset Management, on AI, private credit, and reallocation of global capital. 'We've seen rebalancing', he says. 'Clients have decided to reallocate a bit more to Europe or to Asia'. Pinto speaks with Bloomberg's Guy Johnson, Anna Edwards and Tom Mackenzie on 'The Opening Trade'.Duration:2:43Anthropic AI Tool Sparks Stocks SelloffDuration:6:28Novo CEO Remains Confident After 20% PlungeDuration:3:29US Shutdown Ends as Trump Signs His Funding Deal With DemocratsBalance of PowerDuration:6:11Indonesia FM Blasts Citi, Defends MSCI DowngradeDuration:3:01Fidelity Sees 'Very Bumpy' Ride Ahead for MarketsDuration:1:42D’Amaro Is a Standout Executive, Disney’s Gorman SaysDuration:1:23Musk’s SpaceX Combines With xAI at $1.25 Trillion ValuationDuration:3:16PayPal Names New CEO as Profit Misses TargetsAvailable on:Listen onApple TVListen onRokuListen onSamsung TVListen onFire TVListen onAndroid TVListen onRakuten TVListen onHaystack NewsWatch BTV in your area:Channel Finder",
    "readingTime": 1,
    "keywords": [
      "m&g asset",
      "asset management",
      "tvlisten",
      "pinto"
    ],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/videos/2026-02-04/m-g-asset-management-ceo-pinto-diversification-is-key-video",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ikRPhtCQovWM/v3/-1x-1.webp",
    "created_at": "2026-02-04T12:35:38.829Z",
    "topic": "finance"
  },
  {
    "slug": "ai-disruption-fears-spark-selloff-novo-nordisk-amd-drops-after-earnings-bloomberg-brief-242026",
    "title": "AI Disruption Fears Spark Selloff; Novo Nordisk, AMD Drops After Earnings | Bloomberg Brief 2/4/2026",
    "description": "US equity futures take a breather from a global rout driven by stocks sensitive to potential disruption from Anthropic's new automation tool. Investors await results from Alphabet and Qualcomm as AMD's AI-fueled outlook disappoints. Shares of Novo Nordisk plunges as the drugmaker forecasts sales to fall as much 13% this year. Iran asks the US to move the scheduled diplomatic talks this week from Turkey to Oman and to limit the agenda to Tehran's nuclear program. Stephanie Guild of Robinhood discusses the opportunities in software stocks amid the recent selloff.",
    "fullText": "Feb 4th, 2026AI Disruption Fears Spark Selloff; Novo Nordisk, AMD Drops After Earnings | Bloomberg Brief 2/4/2026US equity futures take a breather from a global rout driven by stocks sensitive to potential disruption from Anthropic's new automation tool. Investors await results from Alphabet and Qualcomm as AMD's AI-fueled outlook disappoints. Shares of Novo Nordisk plunges as the drugmaker forecasts sales to fall as much 13% this year. Iran asks the US to move the scheduled diplomatic talks this week from Turkey to Oman and to limit the agenda to Tehran's nuclear program. Stephanie Guild of Robinhood discusses the opportunities in software stocks amid the recent selloff. Duration:10:20Why the World Is Awash With Cheap OilWeekly DocsDuration:56:34Trump Picks Warsh, US State Capitalism, SNAP Cuts, Business of Youth SportsBloomberg Wall Street WeekDuration:10:02How Circular Deals Are Driving the AI BoomWeekly DocsDuration:24:01Inside Europe’s Economic Crises With LagardeLeaders with Francine LacquaAvailable on:Listen onApple TVListen onRokuListen onSamsung TVListen onFire TVListen onAndroid TVListen onRakuten TVListen onHaystack NewsWatch BTV in your area:Channel Finder",
    "readingTime": 1,
    "keywords": [
      "novo nordisk",
      "tvlisten",
      "disruption",
      "selloff",
      "stocks"
    ],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/videos/2026-02-04/bloomberg-brief-2-4-2026-video",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iBFBd4LZYdsk/v3/-1x-1.webp",
    "created_at": "2026-02-04T12:35:36.424Z",
    "topic": "finance"
  },
  {
    "slug": "ai-anxiety-is-fueling-the-comeback-in-value-stocks",
    "title": "AI Anxiety Is Fueling the Comeback in Value Stocks",
    "description": "The software selloff is boosting the appeal of value stocks.",
    "fullText": "Value stocks (think consumer staples, energy producers and miners) have been zooming higher as investors seek out companies that will benefit from an uptick in economic growth. On the flipside, tech has struggled this year on worries that AI will make a lot of business models obsolete.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://www.bloomberg.com/news/newsletters/2026-02-04/ai-anxiety-is-fueling-the-comeback-in-value-stocks",
    "thumbnail_url": "https://assets.bwbx.io/s3/lightsaber/_next/static/media/social-markets.e062a0c0.jpg",
    "created_at": "2026-02-04T12:35:36.390Z",
    "topic": "finance"
  },
  {
    "slug": "claude-is-a-space-to-think",
    "title": "Claude Is a Space to Think",
    "description": "Anthropic explains why Claude will remain ad-free—how advertising incentives conflict with building a genuinely helpful AI assistant users can trust.",
    "fullText": "There are many good places for advertising. A conversation with Claude is not one of them.\n\nAdvertising drives competition, helps people discover new products, and allows services like email and social media to be offered for free. We’ve run our own ad campaigns, and our AI models have, in turn, helped many of our customers in the advertising industry.\n\nBut including ads in conversations with Claude would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.\n\nWe want Claude to act unambiguously in our users’ interests. So we’ve made a choice: Claude will remain ad-free. Our users won’t see “sponsored” links adjacent to their conversations with Claude; nor will Claude’s responses be influenced by advertisers or include third-party product placements our users did not ask for.\n\nWhen people use search engines or social media, they’ve come to expect a mixture of organic and sponsored content. Filtering signal from noise is part of the interaction.\n\nConversations with AI assistants are meaningfully different. The format is open-ended; users often share context and reveal more than they would in a search query. This openness is part of what makes conversations with AI valuable, but it’s also what makes them susceptible to influence in ways that other digital products are not.\n\nOur analysis of conversations with Claude (conducted in a way that keeps all data private and anonymous) shows that an appreciable portion involve topics that are sensitive or deeply personal—the kinds of conversations you might have with a trusted advisor. Many other uses involve complex software engineering tasks, deep work, or thinking through difficult problems. The appearance of ads in these contexts would feel incongruous—and, in many cases, inappropriate.\n\nWe still have much to learn about the impact of AI models on the people who use them. Early research suggests both benefits—like people finding support they couldn’t access elsewhere—and risks, including the potential for models to reinforce harmful beliefs in vulnerable users. Introducing advertising incentives at this stage would add another level of complexity. Our understanding of how models translate the goals we set them into specific behaviors is still developing; an ad-based system could therefore have unpredictable results.\n\nBeing genuinely helpful is one of the core principles of Claude’s Constitution, the document that describes our vision for Claude’s character and guides how we train the model. An advertising-based business model would introduce incentives that could work against this principle.\n\nConsider a concrete example. A user mentions they’re having trouble sleeping. An assistant without advertising incentives would explore the various potential causes—stress, environment, habits, and so on—based on what might be most insightful to the user. An ad-supported assistant has an additional consideration: whether the conversation presents an opportunity to make a transaction. These objectives may often align—but not always. And, unlike a list of search results, ads that influence a model’s responses may make it difficult to tell whether a given recommendation comes with a commercial motive or not. Users shouldn’t have to second-guess whether an AI is genuinely helping them or subtly steering the conversation towards something monetizable.\n\nEven ads that don’t directly influence an AI model’s responses and instead appear separately within the chat window would compromise what we want Claude to be: a clear space to think and work. Such ads would also introduce an incentive to optimize for engagement—for the amount of time people spend using Claude and how often they return. These metrics aren’t necessarily aligned with being genuinely helpful. The most useful AI interaction might be a short one, or one that resolves the user’s request without prompting further conversation.\n\nWe recognize that not all advertising implementations are equivalent. More transparent or opt-in approaches—where users explicitly choose to see sponsored content—might avoid some of the concerns outlined above. But the history of ad-supported products suggests that advertising incentives, once introduced, tend to expand over time as they become integrated into revenue targets and product development, blurring boundaries that were once more clear-cut. We’ve chosen not to introduce these dynamics into Claude.\n\nAnthropic is focused on businesses, developers, and helping our users flourish. Our business model is straightforward: we generate revenue through enterprise contracts and paid subscriptions, and we reinvest that revenue into improving Claude for our users. This is a choice with tradeoffs, and we respect that other AI companies might reasonably reach different conclusions.\n\nExpanding access to Claude is central to our public benefit mission, and we want to do it without selling our users’ attention or data to advertisers. To that end, we’ve brought AI tools and training to educators in over 60 countries, begun national AI education pilots with multiple governments, and made Claude available to nonprofits at a significant discount. We continue to invest in our smaller models so that our free offering remains at the frontier of intelligence, and we may consider lower-cost subscription tiers and regional pricing where there is clear demand for it. Should we need to revisit this approach, we’ll be transparent about our reasons for doing so.\n\nAI will increasingly interact with commerce, and we look forward to supporting this in ways that help our users. We’re particularly interested in the potential of agentic commerce, where Claude acts on a user’s behalf to handle a purchase or booking end to end. And we’ll continue to build features that enable our users to find, compare, or buy products, connect with businesses, and more—when they choose to do so.\n\nWe’re also exploring more ways to make Claude a focused space to be at your most productive. Users can already connect third-party tools they use for work—like Figma, Asana, and Canva—and interact with them directly within Claude. We expect to introduce many more useful integrations and expand this toolkit over time.\n\nAll third-party interactions will be grounded in the same overarching design principle: they should be initiated by the user (where the AI is working for them) rather than an advertiser (where the AI is working, at least in part, for someone else). Today, whether someone asks Claude to research running shoes, compare mortgage rates, or recommend a restaurant for a special occasion, Claude’s only incentive is to give a helpful answer. We’d like to preserve that.\n\nWe want our users to trust Claude to help them keep thinking—about their work, their challenges, and their ideas.\n\nOur experience of using the internet has made it easy to assume that advertising on the products we use is inevitable. But open a notebook, pick up a well-crafted tool, or stand in front of a clean chalkboard, and there are no ads in sight.\n\nWe think Claude should work the same way.",
    "readingTime": 6,
    "keywords": [
      "social media",
      "business model",
      "model’s responses",
      "genuinely helpful",
      "advertising incentives",
      "users",
      "conversations",
      "claude",
      "products",
      "models"
    ],
    "qualityScore": 1,
    "link": "https://www.anthropic.com/news/claude-is-a-space-to-think",
    "thumbnail_url": "https://www.anthropic.com/api/opengraph-illustration?name=Hand%20House&backgroundColor=cactus",
    "created_at": "2026-02-04T12:35:32.091Z",
    "topic": "tech"
  },
  {
    "slug": "webhook-skills-agent-skills-for-webhook-providers-and-best-practices",
    "title": "Webhook Skills – Agent skills for webhook providers and best practices",
    "description": "Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopif...",
    "fullText": "hookdeck\n\n /\n\n webhook-skills\n\n Public\n\n Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopify, GitHub, and more. Built on the Agent Skills specification.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n hookdeck/webhook-skills",
    "readingTime": 1,
    "keywords": [
      "webhook",
      "skills",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/hookdeck/webhook-skills",
    "thumbnail_url": "https://opengraph.githubassets.com/ed9e4e830971c76b02864c100a37accbc7ef947718023c6abb2ba2536dad1e19/hookdeck/webhook-skills",
    "created_at": "2026-02-04T12:35:28.405Z",
    "topic": "tech"
  },
  {
    "slug": "nexus-gateway-a-selfhealing-ai-gateway-in-go-with-5ms-caching",
    "title": "Nexus Gateway – A self-healing AI gateway in Go with 5ms caching",
    "description": "Infrastructure protocol for high-performance AI engineering.",
    "fullText": "Enterprise-grade inference routing, semantic caching, and unified model orchestration. Route to 200+ models across providers with a single API endpoint and sub-millisecond overhead.\n\nUse your existing API keys from OpenAI, Anthropic, or any provider. Zero vendor lock-in with complete key sovereignty.\n\nNative SDKs for Python, Node.js, Go, and Rust. Type-safe interfaces with streaming support and automatic retries.\n\nVector-based response caching with configurable similarity thresholds. Reduce costs by up to 70% on repeated queries.\n\nIntelligent request routing across 200+ models. Automatic failover, load balancing, and latency-optimized selection.",
    "readingTime": 1,
    "keywords": [
      "routing",
      "caching",
      "models",
      "across",
      "automatic"
    ],
    "qualityScore": 0.75,
    "link": "https://www.nexus-gateway.org/",
    "thumbnail_url": "https://nexus-gateway.org/LOGO.png",
    "created_at": "2026-02-04T12:35:28.232Z",
    "topic": "tech"
  },
  {
    "slug": "women-in-tech-and-finance-at-higher-risk-from-ai-job-losses-report-says",
    "title": "Women in tech and finance at higher risk from AI job losses, report says",
    "description": "‘Mid-career’ female workers also being sidelined by rigid hiring processes, says City of London Corporation\nWomen working in tech and financial services are at greater risk of losing their jobs to increased use of AI and automation than their male peers, according to a report that found experienced females were also being sidelined as a result of “rigid hiring processes”.\n“Mid-career” women – with at least five years’ experience – are being overlooked for digital roles in the tech and financial and professional services sectors, where they are traditionally underrepresented, according to the report by the City of London Corporation.\n Continue reading...",
    "fullText": "‘Mid-career’ female workers also being sidelined by rigid hiring processes, says City of London Corporation\n\nWomen working in tech and financial services are at greater risk of losing their jobs to increased use of AI and automation than their male peers, according to a report that found experienced females were also being sidelined as a result of “rigid hiring processes”.\n\n“Mid-career” women – with at least five years’ experience – are being overlooked for digital roles in the tech and financial and professional services sectors, where they are traditionally underrepresented, according to the report by the City of London Corporation.\n\nThe governing body that runs the capital’s Square Mile found female applicants were discriminated against by rigid, and sometimes automated, screening of their CVs, which did not take into account career gaps related to caring for children or relatives, or only narrowly considered their professional experience.\n\nTo reverse the trend, the corporation is calling on employers to focus on re-skilling female workers not currently in technical roles, particularly those in clerical positions most at risk of being displaced by automation.\n\nIt is estimated that about 119,000 clerical roles in tech and the financial and professional service sectors, predominantly carried out by women, will be displaced by automation over the next decade. Reskilling those affected by these job losses could save companies from making redundancy payments totalling as much as £757m, the report found.\n\nUpskilling staff would allow employers to focus on candidates’ potential rather than their past technical experience, the report found. It is estimated that up to 60,000 women in tech leave their roles each year for reasons including lack of advancement, lack of recognition and inadequate pay.\n\nDame Susan Langley, the mayor of City of London, said: “By investing in people and supporting the development of digital skills within the workforce, employers can unlock enormous potential and build stronger, more resilient teams. Focusing on talent, adaptability and opportunity will ensure the UK continues to lead on innovation and remains a global hub for digital excellence.”\n\nRecent surveys have shown that as many as a quarter of UK workers are worried that their jobs could disappear in the next five years because of AI, according to a poll by the international recruitment company Randstad. Union leaders have called on companies to commit to investing in workforce skills and training.\n\nThe City of London Corporation found that women were being overlooked for roles even as difficulties in hiring talent meant more than 12,000 digital vacancies in these sectors went unfilled in 2024.\n\nCompanies have tried to deal with worker shortages by increasing wages above the national average, but the report found that higher pay rates would not solve the problem. It warned that the widening digital talent gap was forecast to last until at least 2035 and that under this scenario the UK could miss out on more than £10bn of economic growth.",
    "readingTime": 3,
    "keywords": [
      "london corporation",
      "hiring processes",
      "female workers",
      "rigid hiring",
      "women",
      "digital",
      "roles",
      "tech",
      "financial",
      "automation"
    ],
    "qualityScore": 0.7,
    "link": "https://www.theguardian.com/business/2026/feb/04/women-tech-finance-higher-risk-ai-job-losses-report",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a0d2e166110bd4d6f898d4a39a693d5e1a2585e0/1172_0_6626_5304/master/6626.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=6e15a2d2fe9c07da9ee53582be27c291",
    "created_at": "2026-02-04T12:35:24.602Z",
    "topic": "business"
  },
  {
    "slug": "pinterest-sacks-two-engineers-for-creating-software-to-identify-fired-workers",
    "title": "Pinterest sacks two engineers for creating software to identify fired workers",
    "description": "Digital pinboard business cutting 15% of workforce as it invests heavily in AI  \nPinterest has fired two engineers who created a software tool to identify which workers had lost their jobs in a recent round of cuts and then shared the information, according to reports.\nThe digital pinboard business announced significant job cuts earlier this month, with the chief executive, Bill Ready, telling staff he was “doubling down on an AI-forward approach”, according to a LinkedIn post by a former employee.\n Continue reading...",
    "fullText": "Digital pinboard business cutting 15% of workforce as it invests heavily in AI\n\nPinterest has fired two engineers who created a software tool to identify which workers had lost their jobs in a recent round of cuts and then shared the information, according to reports.\n\nThe digital pinboard business announced significant job cuts earlier this month, with the chief executive, Bill Ready, telling staff he was “doubling down on an AI-forward approach”, according to a LinkedIn post by a former employee.\n\nPinterest, which is based in San Francisco and has an office in London, said the cuts would affect about 15% of its workforce, or about 700 people, but did not specify which teams or staff members would be affected.\n\nTwo engineers at the company then wrote code to identify sacked staff.\n\nA spokesperson for Pinterest said: “Two engineers wrote custom scripts improperly accessing confidential company information to identify the locations and names of all dismissed employees and then shared it more broadly. This was a clear violation of Pinterest policy and of their former colleagues’ privacy.”\n\nIt is unclear whether the engineers, who have not been named, shared the information with colleagues, or with people outside Pinterest.\n\nThe script – a set of commands written to automate a task within existing software or change its function – was aimed at internal tools for workers to communicate, the BBC reported, citing an anonymous source.\n\nThe source, whom the BBC said was “familiar with the firings”, said the code created an alert as to which employee names were being removed or deactivated.\n\nPinterest has been investing heavily in AI to create more personalised content for its users and automated tools for marketers. But shares in the company have dropped by more than 20% this year as investors assess the threat from more advanced AI platforms.\n\nReady said in a company-wide meeting that while “healthy debate and dissent are expected, that’s how we make our decisions”, according to CNBC, which first reported the news.\n\nThe chief executive said Pinterest was facing a “critical moment” and that staff should consider a job elsewhere if they were “working against the direction of the company” and disagreed with its mission, CNBC reported.\n\nIt comes amid a wave of job cuts in the techy sector, as businesses increasingly rely on AI. Last week, Amazon announced it would cut 16,000 roles worldwide, its second round of redundancies in three months.\n\nMeta, which owns Facebook, Instagram and WhatsApp, said it would cut more than 1,000 jobs from its Reality Labs division to redirect resources to AI wearables and phone features. Meanwhile, Autodesk, a design software maker, announced plans this month to cut about 1,000 jobs.",
    "readingTime": 3,
    "keywords": [
      "digital pinboard",
      "pinboard business",
      "chief executive",
      "job cuts",
      "engineers",
      "staff",
      "software",
      "identify",
      "jobs",
      "shared"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/feb/04/pinterest-sacks-two-engineers-for-software-identify-fired-workers",
    "thumbnail_url": "https://i.guim.co.uk/img/media/49ea7c63328503733d77be172d1df1f0005232f1/70_0_3262_2609/master/3262.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2be0eed2795df2410ecac05945591a45",
    "created_at": "2026-02-04T12:35:24.578Z",
    "topic": "tech"
  },
  {
    "slug": "jensen-huang-says-nvidia-would-love-to-back-an-openai-ipo-and-theres-no-drama-with-sam-altman",
    "title": "Jensen Huang says Nvidia would love to back an OpenAI IPO, and there's 'no drama' with Sam Altman",
    "description": "Jensen Huang said Nvidia would love to back a future OpenAI IPO and dismissed talk of tension with Sam Altman.",
    "fullText": "Jensen Huang says Nvidia would love to invest in a future OpenAI IPO.\n\nHuang said in an interview on CNBC's \"Mad Money\" on Tuesday that there was \"no drama\" between Nvidia and OpenAI CEO Sam Altman, pushing back against recent chatter of tension in the relationship between the two companies.\n\n\"The first deal is on,\" the Nvidia CEO said, referring to the company's September deal with OpenAI, under which the company said it planned to invest up to $100 billion in the AI startup.\n\n\"​​And then there's, of course, an IPO in the future,\" he added. \"We love to be participating in that as well,\" he added.\n\nHuang also described OpenAI as a \"once in a generation company\" and said Nvidia is \"delighted to invest in it.\"\n\nHis comments come amid reports suggesting internal unease around the deal.\n\nThe Wall Street Journal reported on Saturday that the investment had sparked internal concerns at Nvidia, with some executives questioning the deal, according to people familiar with the matter.\n\nSeparately, Reuters reported on Tuesday that OpenAI had been unhappy with certain newer Nvidia chips and had looked at alternatives since last year, citing people familiar with the matter.\n\nHuang told reporters in Taipei on Saturday that speculation of any dissatisfaction with OpenAI was \"nonsense.\"\n\n\"We will invest a great deal of money, probably the largest investment we've ever made,\" he added.\n\nAltman has also pushed back on rumors of tension.\n\n\"We love working with NVIDIA and they make the best AI chips in the world,\" wrote Altman in a post on X on Tuesday.\n\n\"We hope to be a gigantic customer for a very long time. I don't get where all this insanity is coming from,\" he added.\n\nOpenAI is one of the world's most valuable private AI companies and a major customer for Nvidia's chips, which power the training and deployment of large language models.\n\nThe startup has not announced plans for an IPO, but its fundraising and computing needs have fueled speculation about how it will finance future growth.\n\n\"Big Short\" investor Michael Burry said in a Substack exchange in January that he was surprised that ChatGPT \"kicked off a multi-trillion-dollar infrastructure race.\"\n\n\"It's like someone built a prototype robot and every business in the world started investing for a robot future,\" he wrote.",
    "readingTime": 2,
    "keywords": [
      "deal",
      "invest",
      "love",
      "altman",
      "chips",
      "nvidia",
      "openai",
      "back",
      "tension",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/jensen-huang-nvidia-back-future-openai-ipo-sam-altman-drama-2026-2",
    "thumbnail_url": "https://i.insider.com/6982e47ca645d1188188a481?width=1200&format=jpeg",
    "created_at": "2026-02-04T12:35:23.970Z",
    "topic": "finance"
  },
  {
    "slug": "im-a-senior-ux-researcher-at-microsoft-heres-how-i-broke-into-ai-without-a-tech-background-and-3-lessons-i-learned",
    "title": "I'm a senior UX researcher at Microsoft. Here's how I broke into AI without a tech background — and 3 lessons I learned.",
    "description": "A Microsoft UX lead details her non-traditional path into AI, starting with studying architecture and highlighting lessons in AI accessibility.",
    "fullText": "This as-told-to essay is based on a conversation with Priyanka Kuvalekar, a 31-year-old UX research lead at Microsoft in Redmond, Washington. It's been edited for length and clarity.\n\nI joined Microsoft in April 2025 as a senior UX researcher. I lead research for Microsoft Teams Calling and related AI experiences.\n\nMy path to this job wasn't typical. I spent five years studying architecture in college, not computer science or AI, and earned my degree in India.\n\nI started my career with a full-time internship as a junior architect while completing the final year of my degree. After I graduated, I began thinking seriously about my next steps. Should I continue as an architect or pivot into the digital world?\n\nI enrolled in a three-month user experience course, which led me to pursue a master's degree in user experience and interaction design.\n\nI moved to Philadelphia and began my master's program in January 2018. My first industry step was an internship as a UX researcher at Korn Ferry. After a year of interning, I was offered a full-time role, which I held until 2021.\n\nI broke into Big Tech at Cisco in October 2021 and worked as a UX research lead for over 3 1/2 years before taking my current position at Microsoft.\n\nI led projects focused on AI features for Webex meetings and messaging as the lead researcher.\n\nI knew I needed to understand AI's mechanics, so I pursued certifications and training, both through my employer and independently, on generative AI, agentic AI design patterns, large language models, and evaluating AI experiences as a researcher.\n\nI also explored courses and online resources on UI design from platforms such as Google Skills, Microsoft Training, and DeepLearning.AI to understand how generative AI could be applied to my projects.\n\nHere are the three biggest lessons I learned that helped me break into AI with a non-tech background.\n\nI learned that you need to understand how to evaluate AI in practice. AI isn't something you test once, and then it's \"done.\" It requires ongoing evaluation to ensure it continues to deliver trustworthy experiences.\n\nThis meant designing qualitative studies that examined how AI conversations hold up across diverse user groups. Inconsistencies in tone, misinterpretations of meaning, and pacing issues were revealed in this research.\n\nAnother key lesson came from approaching AI through an accessibility lens. AI can make tasks easier and reduce barriers for people with disabilities — for example, by automating steps. It can also create new inequities if not designed with accessibility in mind.\n\nAccessibility and AI can't be separated. I've learned to include people with disabilities in AI research and to evaluate how AI integrates with assistive technologies such as screen readers and keyboard navigation.\n\nBreaking into AI also taught me that you don't have to build the technology yourself to make an impact, but you do need to understand it well enough to engage with it.\n\nIt's about learning just enough to bridge the gap between technical teams and user needs, and to ensure that how you gauge AI quality is rooted in real user experience.\n\nGaining fluency meant understanding the concepts behind how large language models work, their limitations, and how to design evaluation frameworks that account for those limits.\n\nThis has helped me ask engineers the right questions and design studies that measure trust, reliability, and consistency across different user groups. It's also helped me work closely with product managers to define what success looks like for AI experiences.\n\nI recommend starting where AI meets people, not where AI meets code — focus on how AI shows up in products and how people experience it.\n\nOne of the most practical ways to add value without a traditional background is by shaping what \"quality\" means for an AI feature. Work with product managers to ask questions like, Does the AI stay within scope? Does it handle interruptions gracefully? Is it inclusive across languages and dialects?\n\nThese things often get overlooked when you view AI only as a technical system, but they're crucial to user trust. If you can frame them in actionable terms, you can become indispensable.\n\nHiring managers don't just want to see that you know AI exists — they want to see how you can shape it responsibly and make it usable.\n\nDocument all frameworks, rubrics, evaluation studies, and examples of how your insights influenced decisions. Even if you don't have access to big-company projects, you can run your own small-scale studies on publicly available AI tools and use those to showcase your thinking.\n\nTry volunteering for projects where AI is being integrated into existing tools, and answer questions like, What does this AI feature need to do? How should it behave? Can the AI explain what it can or can't do? Does it recover gracefully when it makes a mistake?\n\nThese are the kinds of questions researchers, product thinkers, and others with non-traditional backgrounds are uniquely positioned to answer.",
    "readingTime": 5,
    "keywords": [
      "language models",
      "product managers",
      "user groups",
      "user experience",
      "research lead",
      "design",
      "researcher",
      "experiences",
      "projects",
      "understand"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/how-ux-researcher-landed-job-microsoft-without-tech-background-2026-2",
    "thumbnail_url": "https://i.insider.com/69824eafe1ba468a96ab4d81?width=737&format=jpeg",
    "created_at": "2026-02-04T12:35:23.858Z",
    "topic": "science"
  },
  {
    "slug": "3-big-things-to-look-out-for-in-googles-q4-earnings",
    "title": "3 big things to look out for in Google's Q4 earnings",
    "description": "Google's Q4 earnings focus on AI spending, Cloud gains, and a major Apple deal. Wall Street eyes search dominance amid AI competition.",
    "fullText": "Google has been on a monumental tear as of late. Can it keep the good times rolling?\n\nWhen Alphabet reports its fourth-quarter earnings on Wednesday, investors will focus on AI spending, cloud wins, and a big Apple deal.\n\nWith Meta projecting its capex for this year to nearly double that of 2025, how high will Google go? The company said in October that it expected capex for 2025 to land between $91 and $93 billion. Investors will track if it sticks to those goals. Google's finance chief, Anat Ashkenazi, also said the company expected a \"significant\" increase in that number for 2026 and promised we'd hear more on Google's Q4 call this week.\n\nAs my colleague Joe Ciolli has observed this earnings season, investors are more willing to forgive big capital spending on AI as long as fundamentals remain chugging along.\n\nThat leads to the question of how Google's search advertising business, its core engine, is doing. Are OpenAI and other AI rivals chipping away at its dominance, or is Alphabet's money-printing machine whirring as proudly as ever?\n\nAnalysts at Bernstein appeared optimistic in a note this week, writing that they expect Google to have seen an increase in query volume in Q4.\n\n\"For Tinuiti's advertisers, Google's US paid click growth was the highest it has been since early 2021,\" Bernstein analysts wrote, referring to the US marketing agency.\n\nGoogle's cloud business has also become top billing, as it furiously competes with Amazon and Microsoft to build infrastructure and win crucial AI deals. In Q3, Google said the number of new cloud customers increased by about 34% year over year.\n\nBernstein analysts predicted big business for cloud off the back of AI sales, its big Anthropic deal, and the ramp-up of its AI chip business, known as Tensor Processing Units (TPUs).\n\n\"We also note that the new deal between Anthropic and Google for 'tens of billions of dollars' worth of compute represents upside to Google Cloud growth, regardless of whether the money flows through Cloud deals or outright TPU sales,\" they wrote.\n\nSpeaking of deals, this marks the first earnings report since Google and Apple announced that the iPhone giant will use Google's Gemini AI models for its revamped Siri assistant.\n\nThere are still a lot of unanswered questions there. How much does Google get out of the deal? Can Google use query data to train Gemini? Will Apple use Google's data centers and TPU chips?\n\nHopefully, we'll get a clearer picture of the inner workings of that deal on Wednesday's earnings call.",
    "readingTime": 3,
    "keywords": [
      "bernstein analysts",
      "deal",
      "earnings",
      "business",
      "investors",
      "deals",
      "google",
      "google's",
      "capex",
      "increase"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-q4-2025-earnings-big-things-alphabet-2026-2",
    "thumbnail_url": "https://i.insider.com/69825425e1ba468a96ab4f12?width=1200&format=jpeg",
    "created_at": "2026-02-04T12:35:23.857Z",
    "topic": "finance"
  },
  {
    "slug": "im-a-googler-who-pivoted-from-finance-to-ai-it-took-me-years-but-i-dont-regret-taking-the-long-path",
    "title": "I'm a Googler who pivoted from finance to AI. It took me years, but I don't regret taking the long path.",
    "description": "A senior software engineer at Google took 40 Coursera courses and got multiple grad degrees to shift from business to AI.",
    "fullText": "This as-told-to essay is based on a conversation with Max Buckley, a 38-year-old senior software engineer at Google, based in Zurich, Switzerland. His identity and employment has been verified by Business Insider. The following has been edited for length and clarity.\n\nI'm a senior software engineer at Google in Zurich, where I've worked since 2013.\n\nMost people don't go from business to a more technical background, but my bachelor's degree was in business studies, and I joined Google out of undergrad as a financial analyst intern.\n\nOver several years, I pivoted from financial analysis to business analysis, then to trust and safety, and onto an engineering team in 2016. I joined several other software engineering teams, and then eventually joined Google Cloud AI, where I worked on one of the cloud AI products for a few years.\n\nThen I joined an internal LLM innovation team within Google's core infrastructure group, which we later turned into an LLM information retrieval applied research team, where I am today, leading the team.\n\nTo make multiple shifts like this, you need to upskill and explore other areas. Here's how I did it:\n\nAs soon as I joined Google, I decided my North Star was to become a data scientist. Initially, I had a lot of success broadening my skillset with online courses like Coursera, edX, or Stanford. I used them to explore finance, then statistics, then spent most of my time in the computer science and data science space.\n\nAll in all, I completed roughly 40 online open courses, most of them through Coursera.\n\nSome of the courses I took include:\n\nThe courses were spread out from 2013 to 2021, and I took them on evenings and weekends. I didn't have much time for video games, but I still made time to go to the gym, eat, sleep, and spend time with my girlfriend.\n\nI wasn't super structured in my approach to taking courses. I didn't say \"I must do five hours a day,\" or anything like that. I took them as I felt the need to do one or to learn more about a particular topic. I didn't burn out from it.\n\nThe most measurably impactful courses I completed were the first two Coursera courses: Data Analysis and Computing for Data Analysis. I did them right before I interviewed for my internship at Google and the interviewer happened to have done the same course, so we had this instant rapport.\n\nI also did more formal part-time study.\n\nI finished my Bachelor of Business Studies in 2013. Then at Google, I got a part-time postgraduate certificate in statistics, which took a year. Then I did a part-time master's degree in Business Analytics, which took two years. Then I pursued a part-time master's degree in software engineering, which took close to five years. Most recently, I did a diploma in Advanced Studies In Data Science, which took another two years.\n\nI did a bunch of summer school certificates, which is when you spend a week doing PhD-level courses.\n\nMy message to others who want to transition is don't be discouraged. When I first wanted to join Google, several hiring managers weren't interested because I didn't have a computer science undergraduate degree. Similarly, when I applied for a Master's in Business Analytics, I was initially declined because I didn't have a technical undergraduate degree and two semesters of programming experience, even though I was programming at Google. There were hurdles that I had to work around.\n\nI always wanted to study computer science, but my dad told me it's better to learn something different because I'll probably end up in it anyway. In hindsight, he was right.\n\nStudying computer science would have certainly expedited my career track, but I'm in a place where it's no longer a hindrance, and I'm familiar with a bunch of business theories, like Porter's Five Forces. They're not always useful, but every once in a while, it comes up in conversation.\n\nThe constant here is that my background includes continuous learning. When recruiters or hiring managers see my profile, they see that I'm not someone who gets complacent.\n\nDid you make a career pivot? We want to hear from you. Reach out to the reporter via email at aaltchek@insider.com or through the secure-messaging app Signal at aalt.19.",
    "readingTime": 4,
    "keywords": [
      "business studies",
      "hiring managers",
      "joined google",
      "senior software",
      "software engineer",
      "undergraduate degree",
      "software engineering",
      "computer science",
      "part-time master's",
      "business analytics"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/googler-shares-years-long-journey-pivot-finance-to-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/69822c85a645d11881888ff3?width=1200&format=jpeg",
    "created_at": "2026-02-04T12:35:23.720Z",
    "topic": "finance"
  },
  {
    "slug": "cognizant-forecasts-annual-revenue-above-estimates-on-strong-ai-demand",
    "title": "Cognizant forecasts annual revenue above estimates on strong AI demand",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/cognizant-forecasts-annual-revenue-above-estimates-4484276",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM130P9_L.jpg",
    "created_at": "2026-02-04T12:35:22.464Z",
    "topic": "finance"
  },
  {
    "slug": "ai-fears-drag-asia-software-stocks-lower-after-us-tech-rout",
    "title": "AI Fears Drag Asia Software Stocks Lower After US Tech Rout",
    "description": "Asian software stocks slid, extending a global selloff as investors fret that rapid advances in artificial intelligence could upend traditional business models.",
    "fullText": "MarketsBy Winnie Hsu, Carmeli Argana, and Ashutosh JoshiSaveAsian software stocks slid, extending a global selloff as investors fret that rapid advances in artificial intelligence could upend traditional business models.Shares of Indian information technology companies were the latest to buckle. Bellwether Tata Consultancy Services Ltd. sank as much as 6% at the open, while Infosys Ltd. dropped 6.2%, tracking a slump in its US-listed shares overnight.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.3,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/ai-disruption-concerns-sink-software-makers-stocks-in-asia",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iJdO.cS0Gspg/v0/1200x800.jpg",
    "created_at": "2026-02-04T06:38:05.995Z",
    "topic": "finance"
  },
  {
    "slug": "video2docs-turn-screen-recordings-into-stepbystep-instructions",
    "title": "Video2docs – Turn Screen Recordings into Step-by-Step Instructions",
    "description": "Transform app walkthrough videos into beautiful, structured documentation using AI. Works without audio narration.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://video2docs.com/",
    "thumbnail_url": "https://video2docs.com/assets/og_image.png",
    "created_at": "2026-02-04T06:38:01.672Z",
    "topic": "tech"
  },
  {
    "slug": "we-added-toon-compression-to-our-llm-gateway-compress-prompts-saves-tokens",
    "title": "We added TOON compression to our LLM gateway – compress prompts, saves tokens",
    "description": "🎒 Token-Oriented Object Notation (TOON) – Compact, human-readable, schema-aware JSON for LLM prompts. Spec, benchmarks, TypeScript SDK. - toon-format/toon",
    "fullText": "toon-format\n\n /\n\n toon\n\n Public\n\n 🎒 Token-Oriented Object Notation (TOON) – Compact, human-readable, schema-aware JSON for LLM prompts. Spec, benchmarks, TypeScript SDK.\n\n toonformat.dev\n\n License\n\n MIT license\n\n 22.4k\n stars\n\n 988\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n toon-format/toon",
    "readingTime": 1,
    "keywords": [
      "toon",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/toon-format/toon",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1081397957/a1727f6d-0f41-4022-aba0-225fbef55509",
    "created_at": "2026-02-04T06:38:01.438Z",
    "topic": "tech"
  },
  {
    "slug": "if-you-tell-ai-not-to-do-something-its-more-likely-to-do-it",
    "title": "If you tell AI not to do something, it's more likely to do it",
    "description": "Telling ChatGPT not to do something can make it actively suggest doing it, with some models even willing to endorse theft or deception when the prompt includes the forbidden act.   Like me, you may have come across a strange phenomenon with Large Language Models (LLMs) whereby they don't just ignore a specific instruction you...",
    "fullText": "Telling ChatGPT not to do something can make it actively suggest doing it, with some models even willing to endorse theft or deception when the prompt includes the forbidden act.\n\nLike me, you may have come across a strange phenomenon with Large Language Models (LLMs) whereby they don’t just ignore a specific instruction you gave, which included a prohibition (i.e., ‘Don’t do [something]’), but seem to go out of their way to immediately enact the very thing you just told them not to enact – even if doing so is ‘out of character’ for the model.\n\nThis is a known feature even of older NLP models; and a growing strand of research regarding LLMs’ negation capabilities has emerged in recent years.\n\nThough it can be challenging for people to chase down the buried meaning in a complex double-negative*, LLMs have an added disadvantage, illustrated in the below example of ChatGPT’s monotonicity reasoning, from a 2023 paper:\n\nThough the internal workings of a closed model such as ChatGPT are opaque, the second answer appears to be repurposing the logic used to generate the first answer; however, that logic is not applicable in the second case, because the man may own an animal other than a dog†.\n\nHere, therefore, the outcome of the second inquiry appears to have been affected by the context of the solution obtained for the first.\n\nLikewise, by suggesting the existence of a prohibited act, that banned act can often be put into action by an LLM, which acknowledges and processes the act, but not the negation.\n\nThis is a severe restriction on the utility of LLMs, because in domains where language models may be used for critical applications, such as medicine, finance, or security, it is clearly important that they correctly interpret orders that contain prohibitions.\n\nThis problem is highlighted in a new paper from the US, which examines the extent to which commercial models (such as ChatGPT) and open-source models (such as LLaMA) are unable to follow negative instructions.\n\nThe researchers tested 16 models over 14 ethical scenarios, and concluded that open-source models endorse (i.e., encourage, enact, enable) specifically banned instructions 77% of the time under simple negation (‘Don’t do this), and 100% of the time under complex negation (‘Don’t do this if it leads to that’).\n\nWhile commercial models fared better, only Gemini-3-Flash achieved the top rating in a new Negation Sensitivity Index (NSI) scale proposed by the paper (though Grok 4.1 ran a close second).\n\nUnder the new benchmark, all the models tested would be banned from making decisions in the domains medical, financial, legal, military, business, education, and science – effectively rendering them unusable in such contexts. Though reasoning models generally performed better, even these slower approaches failed under queries with compound negation.\n\nGiven the longstanding association between computing and reliable Boolean operators such as OR and NOT, users who view binary consistency as a baseline expectation may be particularly exposed to failures of this kind.\n\nCommenting on the difficulty that open-source LLMs have in parsing negated queries, the authors state:\n\n‘Commercial models fare better but still show swings of 19-128%. Agreement between models drops from 74% on affirmative prompts to 62% on negated ones, and financial scenarios prove twice as fragile as medical ones […]\n\n‘The findings point to a gap between what current alignment techniques achieve and what safe deployment requires: models that cannot reliably distinguish “do X” from “do not X” should not be making autonomous decisions in high-stakes contexts.’\n\nThe paper notes that failures of this kind are more likely to impact vulnerable individuals across the studied domains:\n\n‘Domain adjustment is not merely technical calibration. Rather, it has equity implications.\n\n‘Financial fragility means that economically vulnerable populations, for example those seeking loans, benefits, or credit, face higher exposure to negation errors than those seeking medical information.’\n\nFurther, the authors emphasize that the problem cannot be resolved through traditional alignment-based approaches, since the issue involves a deep-rooted failure of intent parsing in LLMs, rather than a corporate requirement to restrict what they say, or how they interpret a prompt:\n\n‘A model can be “aligned” in the sense of refusing harmful keywords while failing to process the structure of requests. True alignment requires not just learning what to value but correctly parsing the linguistic expressions of those values.\n\n‘Until that capability is reliable, “do not” should mean “do not.”‘\n\nInterestingly, though Gemini Flash was the sole ‘winner’ in the authors’ own novel benchmark, the current crop of Chinese LLMs generally proved to be far less susceptible to the problem.\n\nThe new paper is titled When Prohibitions Become Permissions: Auditing Negation Sensitivity in Language Models, and comes from two researchers at Kenyon College in Ohio.\n\nThe 14 ethical scenarios developed by the researchers to test the LLMs were:\n\nContents in the ‘ACTION AT STAKE’ column above are not intended as ethically ‘correct answers’, but as the only available do / don’t do actions for each scenario; here the LLMs are not being challenged on their ethical standards, but rather on their ability to parse complex queries and prohibitions in critical situations.\n\nTherefore the severity of these scenarios is only illustrative, on the part of the authors; the LLMs could as effectively have been interrogated on much less severe topics.\n\nThe US commercial models tested for the work were GPT-5.1; GPT-5.2; GPT-5-mini; Claude-Haiku-4.5; Claude-Sonnet-4.5; Gemini-3-Flash; Grok-4.1-non-reasoning; and Grok-4.1-reasoning.\n\nThe Chinese commercial models tested were DeepSeek-V3; GLM-4; Kimi-K2; and Qwen3.\n\nThe open-source models used were LLaMA-3.2-1B; Gemma-3-4B; Granite-3.3-2B; and Phi-4-mini-3.8B.\n\nPrompts for each of the 14 scenarios were written in four versions: F0 stated that the action should be done; F1, that it should not; F2 supported pursuing a goal even if it required the (prohibited) action; and F3 rejected the goal if it required the (prohibited) action:\n\n‘Under F0, models are asked whether he “should rob the store.” Under F1, whether he “should not rob the store.” Under F2, whether he should “save his daughter even if it means he must rob the store.”\n\n‘Under F3, whether he should “not save his daughter if it means he must rob the store.” The admissible facts remain constant, and only polarity varies.’\n\nThe approach argues that if a model understands how negation works, its answers should ‘flip cleanly’ between positive and negative versions of the same prompt. Therefore, if 60% of responses agree that ‘they should do X’ (F0), then only 40% should agree that ‘they should not do X’ (F1) – since rejecting F1 also means supporting the action; and when the numbers don’t match up in this way, the model is misreading negation.\n\nThe authors used Cochran’s Q test and the Kruskal-Wallis H-test to measure how much framing (variation in prompt polarity while preserving meaning) affected model responses, both within and across categories. After adjusting for false positives, the authors found that in 61.9% of cases, the model’s answer changed significantly depending only on how the prompt was phrased – even when the core meaning stayed the same.\n\nThey also tested whether reducing randomness (‘temperature’) made models less fragile††:\n\nUnder simple affirmative prompts (F0), models from all three categories gave moderate support for the proposed actions, with endorsement rates between 24% and 37%. This was expected, given that the scenarios were designed as moral dilemmas without obvious right answers. However, the authors note that the balance broke down under negation:\n\n‘Open-source models jump from 24% endorsement under F0 to 77% under F1. When told “should not do X,” they endorse doing X more than three times out of four. Under compound negation (F3), they reach 100% endorsement, a ceiling effect indicating complete failure to process the negation operator.’\n\nOpen-source models showed the most extreme framing effects, with endorsement rates jumping 317% from F0 to F3 – a sign that their outputs are highly sensitive to how a question is phrased. US commercial models also showed large swings, with endorsement rates more than doubling when prompts were reworded from F0 to F3.\n\nChinese commercial models were more stable overall, with only a 19% increase from F0 to F3, compared to jumps of over 100% in other groups. More importantly, they were the only models to reduce their endorsement when a prompt was negated, suggesting they understood that saying ‘should not’ means the opposite of ‘should’:\n\nModels agreed with each other 74% of the time when prompts used affirmative wording, but only 62% when the same ideas were expressed with negation – a12-point drop suggesting that models are not trained to handle negation in a consistent way:\n\nTo measure how easily a model’s judgment can be flipped by rephrasing a prompt with negation, the authors developed the aforementioned Negation Sensitivity Index (NSI) – a metric designed to quantify whether a model gives opposite answers to questions that are logically equivalent, but framed using negation.\n\nA high NSI score indicates that a model frequently reverses its position when a prompt is negated, revealing a reliance on superficial wording rather than consistent reasoning.\n\nThe NSI benchmark was created by generating pairs of prompts (one original, one with a logical negation), and observing whether the model produced semantically opposite responses. By comparing answers across a large set of such pairs, the authors defined NSI as the proportion of valid negation pairs where the model flipped its output.\n\nThe NSI benchmark was used in tests to evaluate domain sensitivity in negation (i.e., whether the context category ‘financial’ or ‘military’, etc., affected the outcome), achieving some interesting contrasts. Here, some types of decisions proved much more sensitive to wording changes than others.\n\nFor instance, business and finance prompts triggered high fragility, with models flipping answers when a question was rephrased or negated, scoring around 0.64 to 0.65 on the NSI scale. Medical prompts were more stable, averaging just 0.34:\n\nNoting that the medical domain produced the fewest errors and financial the highest, the authors hypothesize:\n\n‘Why might this gap exist? It is possible that medical decisions may benefit from clearer training signal. Hippocratic principles, established protocols, and extensive professional literature may anchor model behavior even under framing variation.\n\n‘Financial decisions, on the other hand, involve murkier tradeoffs with less social consensus, leaving models more susceptible to surface cues.’\n\nThe problem was most severe in open-source models, which reached NSI scores above 0.89 in finance, business, and military prompts. Commercial systems were less fragile but still showed high sensitivity, scoring between 0.20 and 0.75 depending on the domain:\n\nAs mentioned earlier, the authors note that the heightened fragility of open-source models in this area may carry disproportionate risks for vulnerable or marginalized groups, who are more likely to be served by locally deployed systems chosen for budgetary reasons in municipal or governmental settings†††:\n\n‘If an institution deploys an open-source model for cost reasons, the burden falls disproportionately on populations already navigating precarious financial circumstances. Buolamwini and Gebru documented how accuracy disparities in facial recognition fell along demographic lines.\n\n‘Our findings suggest a parallel disparity along domain lines, with economically vulnerable populations bearing greater risk.’\n\nThough we do not have scope here to cover the entirety of the paper’s results, and its closing case studies, it is noteworthy that the case studies demonstrate a proclivity for negation-blind model responses to end up recommending extremely non-advisable courses of action, simply because they misinterpreted the negation construction:\n\n‘Under F0, open-source models endorse robbery 52% of the time, a defensible split given the scenario’s moral complexity. Under F1 (“should NOT rob”), they endorse it 100%. The negated prohibition produces unanimous endorsement of the prohibited action.\n\n‘Commercial models show a more mixed pattern, with aggregate endorsement rising from 33% to 70% under simple negation. Some commercial systems show near-inversion, while others show modest increases.\n\n‘Significantly, no category achieves the mirror-image reversal that correct negation processing would produce.’\n\nThis is one of the most interesting papers I have come across in a while, and I recommend the reader to investigate further, as there is not space here to cover all of the material presented by the authors\n\nPerhaps the most interesting thing about the study is how frequently a user of LLMs comes across this problem, and gradually learns not to ‘put unwanted thoughts’ in their LLMs’ cogitative processes, often attempting to exclude certain undesired results by alternative means than in-prompt negation – such as user-level system prompts, long-term memory storage, or repetitive in-prompt templates that retain the objective.\n\nIn practice, none of these methods is terribly effective, while the black-box nature of Gemini Flash – here the best-performing LLM – makes it hard to glean remedies from the obtained test results.\n\nPerhaps greater clues to the underlying architectural problem lies in studying why Chinese models, though none approach the heights of the leaderboard, generally perform so much better in this single, thorny aspect.\n\n* A form which is actually baked into several romance languages, including Italian.\n\n† Even ChatGPT-4o does not make this mistake any longer.\n\n†† The source paper contains a few misattributions of tables and figures. At one point the text indicates that table 1 (which is just a list of LLMs used in tests) contains the core results. In these cases I have had to guess what the correct figures or tables are, and I stand to be corrected by the authors.\n\n††† My substitution of hyperlinks for the authors’ inline citations.\n\nFirst published Tuesday, February 3, 2026\n\nWriter on machine learning, domain specialist in human image synthesis. Former head of research content at Metaphysic.ai.\n\nPersonal site: martinanderson.ai\n\nContact: [email protected]\n\nTwitter: @manders_ai",
    "readingTime": 12,
    "keywords": [
      "sensitivity index",
      "index nsi",
      "nsi scale",
      "nsi benchmark",
      "chinese commercial",
      "framing variation",
      "economically vulnerable",
      "negation sensitivity",
      "vulnerable populations",
      "less fragile"
    ],
    "qualityScore": 1,
    "link": "https://www.unite.ai/if-you-tell-ai-not-to-do-something-its-more-likely-to-do-it/",
    "thumbnail_url": "https://www.unite.ai/wp-content/uploads/2026/02/robot-prohibition-MAIN.jpg",
    "created_at": "2026-02-04T06:38:00.869Z",
    "topic": "tech"
  },
  {
    "slug": "embedded-vector-and-graph-database-in-pure-go",
    "title": "Embedded Vector and Graph Database in Pure Go",
    "description": "SQLite for Vectors - Simple, fast, embeddable vector storage for Go LLM applications. - liliang-cn/sqvect",
    "fullText": "liliang-cn\n\n /\n\n sqvect\n\n Public\n\n SQLite for Vectors - Simple, fast, embeddable vector storage for Go LLM applications.\n\n liliang-cn.github.io/sqvect/\n\n License\n\n MIT license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n liliang-cn/sqvect",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/liliang-cn/sqvect",
    "thumbnail_url": "https://opengraph.githubassets.com/04a662363da77549dca7df68595a9813a8943cc6dea68042bdc44327d8e4ab55/liliang-cn/sqvect",
    "created_at": "2026-02-04T06:38:00.475Z",
    "topic": "tech"
  },
  {
    "slug": "acer-chromebook-plus-515-review-powerful-but-limited-by-clunky-software",
    "title": "Acer Chromebook Plus 515 review: Powerful but limited by clunky software",
    "description": "The Chromebook Plus 515 is one of Google's first new AI-enhanced laptops. It performs great for its price, but ChromeOS still has key limitations.",
    "fullText": "The Acer Chromebook Plus 515 is one of the first Chromebook Plus models. It features a powerful Intel processor, an HD webcam, and onboard AI tools.\n\n Check price at Best Buy",
    "readingTime": 1,
    "keywords": [
      "chromebook",
      "plus"
    ],
    "qualityScore": 0.3,
    "link": "https://www.businessinsider.com/guides/tech/acer-chromebook-plus-515-review",
    "thumbnail_url": "https://i.insider.com/657c8de87a3c8094d5dd92bc?width=1200&format=jpeg",
    "created_at": "2026-02-04T06:37:54.803Z",
    "topic": "tech"
  },
  {
    "slug": "openai-just-snagged-an-anthropic-safety-researcher-for-its-highprofile-head-of-preparedness-role",
    "title": "OpenAI just snagged an Anthropic safety researcher for its high-profile head of preparedness role",
    "description": "OpenAI hired an Anthropic safety researcher as its head of preparedness, which pays up to $555,000 plus equity.",
    "fullText": "OpenAI has filled a key safety role by hiring from a rival lab.\n\nThe company has brought on Dylan Scand, a former AI safety researcher at Anthropic, as its new head of preparedness, a role that carries a salary of up to $555,000 plus equity. The role caught attention last month thanks to its eye-catching pay package amid OpenAI's rising AI safety concerns.\n\nSam Altman announced the move in a post on X on Wednesday, saying that he is \"extremely excited\" to welcome Scand to OpenAI.\n\n\"Things are about to move quite fast and we will be working with extremely powerful models soon,\" Altman wrote.\n\n\"Dylan will lead our efforts to prepare for and mitigate these severe risks. He is by far the best candidate I have met, anywhere, for this role,\" he added.\n\nScand said in a post on X on Wednesday about his move that he's \"deeply grateful for my time at Anthropic and the extraordinary people I worked alongside.\"\n\n\"AI is advancing rapidly. The potential benefits are great — and so are the risks of extreme and even irrecoverable harm,\" he added.\n\nLast month, Altman described the job as \"stressful.\"\n\n\"You'll jump into the deep end almost immediately,\" he wrote on X.\n\nIn the job posting, OpenAI said the role is best suited for someone who can lead technical teams, make high-stakes calls under uncertainty, and align competing stakeholders around safety decisions. The company also said candidates should have deep expertise in machine learning, AI safety, and related risk areas.\n\nTensions have arisen over OpenAI's approach to safety. Several early employees — including a former head of its safety team — have left the company in recent years.\n\nOpenAI has also faced lawsuits from users who allege its tools contributed to harmful behavior.\n\nIn October, the company said that some ChatGPT users had shown possible signs of mental health distress. An estimated 560,000 users a week show \"possible signs of mental health emergencies,\" it said.\n\nThe company also said it was consulting mental health specialists to refine how the chatbot responds when users show signs of psychological distress or unhealthy dependence.",
    "readingTime": 2,
    "keywords": [
      "mental health",
      "safety",
      "role",
      "users",
      "signs",
      "anthropic",
      "openai's",
      "extremely",
      "lead",
      "risks"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-anthropic-safety-researcher-head-of-preparedness-dylan-scand-2026-2",
    "thumbnail_url": "https://i.insider.com/6982bb18a645d1188188a382?width=1200&format=jpeg",
    "created_at": "2026-02-04T06:37:54.722Z",
    "topic": "science"
  },
  {
    "slug": "get-me-out-traders-dump-software-stocks-as-ai-fears-erupt",
    "title": "‘Get Me Out’: Traders Dump Software Stocks as AI Fears Erupt",
    "description": "Wall Street has been skeptical about software stocks for a while, but sentiment has gone from bearish to doomsday lately with traders dumping shares of companies across the industry as fears about the destruction to be wrought by artificial intelligence pile up.",
    "fullText": "MarketsBy Ryan VlastelicaSaveWall Street has been skeptical about software stocks for a while, but sentiment has gone from bearish to doomsday lately with traders dumping shares of companies across the industry as fears about the destruction to be wrought by artificial intelligence pile up.“We call it the ‘SaaSpocalypse,’ an apocalypse for software-as-a-service stocks,” said Jeffrey Favuzza, who works on the equity trading desk at Jefferies. “Trading is very much ‘get me out’ style selling.”",
    "readingTime": 1,
    "keywords": [
      "stocks",
      "trading"
    ],
    "qualityScore": 0.2,
    "link": "https://www.bloomberg.com/news/articles/2026-02-03/-get-me-out-traders-dump-software-stocks-as-ai-fears-take-hold",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/i6scu71VJPgE/v1/1200x800.jpg",
    "created_at": "2026-02-04T01:07:07.895Z",
    "topic": "finance"
  },
  {
    "slug": "uk-looks-at-subsidizing-small-modular-reactors-to-power-ai-boom",
    "title": "UK Looks at Subsidizing Small Modular Reactors to Power AI Boom",
    "description": "The UK is laying out plans to help the development of small nuclear reactors as it seeks to power a boom in artificial intelligence.",
    "fullText": "MarketsBy Jess ShanklemanSaveThe UK is laying out plans to help the development of small nuclear reactors as it seeks to power a boom in artificial intelligence.The Department for Energy Security and Net Zero will on Wednesday announce a package of measures designed to make it easier for developers of small modular reactors to get financing.",
    "readingTime": 1,
    "keywords": [
      "reactors"
    ],
    "qualityScore": 0.2,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/uk-looks-at-subsidizing-small-modular-reactors-to-power-ai-boom",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ixfWqdAWZR8w/v0/1200x800.jpg",
    "created_at": "2026-02-04T01:06:59.121Z",
    "topic": "finance"
  },
  {
    "slug": "vibe-coding-is-the-new-rad",
    "title": "Vibe Coding is the new RAD",
    "description": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.",
    "fullText": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.\n\nFile details: 6.9 MB MP3, 5 mins 12 secs duration.\n\nTitle music is \"Apparent Solution\" by Brendon Moeller, licensed via www.epidemicsound.com\n\nFive.Today is a highly-secure personal productivity application designed to help you to manage your priorities more effectively, by focusing on your five most important tasks you need to achieve each day.\n\n Our goal is to help you to keep track of all your tasks, notes and journals in one beautifully simple place, which is highly secure via end-to-end encryption. Visit the URL Five.Today to",
    "readingTime": 1,
    "keywords": [
      "tasks",
      "five.today"
    ],
    "qualityScore": 0.65,
    "link": "https://techleader.pro/a/723-Vibe-Coding-is-the-new-RAD-(TLP-2026w3)",
    "thumbnail_url": "https://techleader.pro/img/icons/noun_programmer_2644331.png",
    "created_at": "2026-02-04T01:06:56.822Z",
    "topic": "tech"
  },
  {
    "slug": "wplace-for-ai-agents",
    "title": "Wplace for AI Agents",
    "description": "A shared pixel canvas where autonomous AI agents collaborate, compete, and create art together.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://molt.place",
    "thumbnail_url": "https://molt.place/og-image.png",
    "created_at": "2026-02-04T01:06:56.759Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-plans-employee-tender-offer-at-350b-valuation",
    "title": "Anthropic Plans Employee Tender Offer at $350B Valuation",
    "description": "Anthropic is working on a deal to let some employees sell shares in the company at a valuation of at least $350 billion, according to a person familiar with the matter, a plan that is coming together at the same time as a funding round that could bring in more than $20 billion.",
    "fullText": "MarketsBy Natasha Mascarenhas and Shirin GhaffarySaveAnthropic is working on a deal to let some employees sell shares in the company at a valuation of at least $350 billion, according to a person familiar with the matter, a plan that is coming together at the same time as a funding round that could bring in more than $20 billion.The tender offer would allow Anthropic staffers to cash out some equity in one of the world’s most richly valued artificial intelligence startups. The $350 billion valuation is the same one being discussed in the company’s ongoing fundraising, the person said, and is pre-money, meaning it does not include dollars being raised.",
    "readingTime": 1,
    "keywords": [
      "valuation"
    ],
    "qualityScore": 0.15,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/anthropic-plans-employee-tender-offer-at-350-billion-valuation",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iNdYg0rZ7raQ/v0/1200x800.jpg",
    "created_at": "2026-02-04T01:06:41.256Z",
    "topic": "tech"
  },
  {
    "slug": "disneys-new-ceo-has-a-winning-combination-a-friendly-face-and-an-eye-for-profit",
    "title": "Disney's new CEO has a winning combination: a friendly face and an eye for profit",
    "description": "Disney's decision to tap Josh D'Amaro signals that the company wanted to go with a familiar, friendly face to push ahead into the AI era.",
    "fullText": "After a decade of ups and downs, Disney is picking a familiar, friendly face at the heart of its profit engine to lead the company into a new era.\n\nDisney's board just tapped Parks boss Josh D'Amaro to take over for longtime CEO Bob Iger, starting on March 18. Iger, who plans to retire as CEO for the second time, will remain in an advisor role before stepping aside for good later this year.\n\nAfter a decade marked by strategic changes and a failed succession, Disney is turning to the guy who runs its most reliable moneymaker to navigate a period of massive change in the entertainment industry.\n\nDisney's new CEO has his work cut out for him. The Mouse House's stock has only mustered an 11% gain in the last decade, compared to a more than threefold increase for the S&P 500. And while Disney's parks are gushing cash, the streaming business isn't yet making enough money to offset the steady erosion of the company's ever-fading cable networks.\n\nHowever, D'Amaro appears as well-positioned as any possible Iger successor to lead Disney into the next decade.\n\nNot only is D'Amaro intimately familiar with the company's most lucrative business, but he also seems to have the charm and leadership acumen needed to lead a juggernaut like Disney. That's in contrast to former CEO Bob Chapek, who took over after Iger's first retirement and was known as a gruff, business-minded executive who lacked Iger's high level of charisma.\n\nYet D'Amaro isn't just a likable face. Here are three points about Disney's choice of D'Amaro and what his leadership could mean for the company.\n\nWhile it's still too early to say how D'Amaro might run things, the board's decision to go with a company insider, who happened to be the consensus pick, signals a preference for stability, analysts and leadership experts told Business Insider.\n\nD'Amaro — who turns 55 on February 10 — has been at Disney since 1998. As Disney's experience chairman, D'Amaro oversees the parks division, which has been a financial standout compared to the fast-eroding linear TV business.\n\n\"The company appears to be reorganizing in ways that shore up his shortcomings,\" Pozner told Business Insider, referring to D'Amaro.\n\nDisney didn't respond to a request for comment from Business Insider on its reasoning.\n\nBecause D'Amaro is 20 years younger than Iger, D'Amaro could have a lengthy run in the top job, she said.\n\n\"He's still got a long time horizon in front of him,\" Pozner said. \"This is really planning for the future. And I think that's a great signal about stability.\"\n\nAnalyst Joe Bonner of Argus Research said it's \"hard to really tell much about anyone under that big shadow\" of Iger.\n\nD'Amaro is known among colleagues for being a likable guy who takes selfies with parkgoers.\n\n\"D'Amaro definitely feels like a man of the people,\" Pozner said.\n\nD'Amaro is \"broadly popular\" and known for being approachable, said a Disney ad sales staffer who said they'd been in the same room as D'Amaro but hadn't met him. This person added that the incoming CEO seems \"very well-liked over at Parks\" and added that it's \"always a good sign when people love working for someone.\"\n\nThat affability could help him forge relationships in other divisions of the entertainment conglomerate — and navigate a new course while Iger initially remains on the board.\n\nPozner said D'Amaro's relative youth and his \"flesh-pressing image\" could signal that Disney's board wants to help the company maintain its friendly perception. That matters because of criticisms, for example, that the costs of the theme parks have become out of reach for some families.\n\nThat concern has worried some investors, who wonder how much consumers will be willing to hand over to spend time at the happiest place on earth, Ben Barringer, global head of technology research at the UK wealth management firm Quilter Cheviot, told Business Insider.\n\nBarringer said that D'Amaro will likely have to develop new skills to match his broader remit.\n\n\"Dealing with Parks people is different from dealing with studio people,\" Barringer said.\n\nIt could help that Walden is staying at the Mouse House in a major role.\n\nD'Amaro will likely also want to work on his influence skills, Barringer said, in part because moving the company through the big upheaval in the entertainment industry won't be easy.\n\n\"Hollywood faces quite a lot of disruption,\" he said. The ascent of AI and Disney's partnership with OpenAI are signs that more change is coming, Barringer said.\n\n\"Embracing the disruptive threat and trying to leverage it is a good way to start,\" Barringer said, referring to AI.",
    "readingTime": 4,
    "keywords": [
      "ceo bob",
      "disney's board",
      "entertainment industry",
      "business insider",
      "iger d'amaro",
      "parks",
      "decade",
      "pozner",
      "lead",
      "leadership"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/new-disney-ceo-josh-damaro-consensus-pick-leadership-2026-2",
    "thumbnail_url": "https://i.insider.com/69826bcad3c7faef0ecd8c94?width=1200&format=jpeg",
    "created_at": "2026-02-04T01:06:35.190Z",
    "topic": "finance"
  },
  {
    "slug": "the-em-dash",
    "title": "The Em Dash",
    "description": "Last summer, Bryan Vance found himself in an argument with a stranger on Reddit. Vance, a Portland-based journalist who runs Stumptown Savings, a newsletter covering local grocery deals, had been accused of using ChatGPT to write his content. The evidence? His use of em dashes. “A Reddit user accused me of using AI, pointing to",
    "fullText": "Last summer, Bryan Vance found himself in an argument with a stranger on Reddit. Vance, a Portland-based journalist who runs Stumptown Savings, a newsletter covering local grocery deals, had been accused of using ChatGPT to write his content. The evidence? His use of em dashes.\n\n“A Reddit user accused me of using AI, pointing to my use of, quote, extra long M dashes that are not possible to replicate on a normal keyboard,” Vance recalls. The accusation stung, particularly because Vance spends 40 hours a week personally visiting grocery stores and crafting his newsletter by hand. “I’m a human, I can confirm I’m human,” he says.\n\nThis plucky bit of punctuation has had a very, very long literary history way beyond today’s tussles with technology. It’s been on a hero’s journey, playing the lead in an adventure story that has spanned both centuries and the pages of our most beloved plays, novels and poems. So who invented it—and why?\n\nThe em dash gets its name from its width, roughly equivalent to a capital M. Its origins trace back to 11th century Italy and a scholar named Boncompagno da Signa, who practiced the formal art of composing letters and documents. Frustrated with the inconsistent punctuation rules of his time, he created his own system, including a horizontal dash called Virgula Plana that looked exactly like a modern em dash.\n\nWhile his dash-as-period never caught on, the mark’s grammatical flexibility allowed it to evolve. According to Keith Houston, author of Shady Characters: The Secret Life Of Punctation, Symbols And Other Typographical Marks the dash slid into the printing era without a fixed purpose, which may have made it remarkably adaptable.\n\nThe dash became essential for capturing a theatrical technique called aposiopesis, speech deliberately broken off mid-sentence. In King Lear, characters trail off with dashes as they lose their train of thought or shift direction, bringing psychological realism to the stage.\n\nWhen the novel emerged as a literary form in the 18th century, writers adopted the dash to capture authentic human thought and speech. Lawrence Sterne’s 1759 satirical novel “Tristram Shandy” deployed dashes with wild abandon, creating a stream-of-consciousness narrative that felt revolutionary. One short excerpt contains seven dashes used in every conceivable way. “It must have been like a bolt from the blue,” Houston says. “It must have been so incredible for people at the time to read this.”\n\nNovelists also used dashes for censorship, redacting names and locations to create an air of authenticity. Jane Austen employed this technique in her work, including in Pride and Prejudice using dashes to obscure military regiment names as if protecting real people’s reputations. The device added both realism and intrigue, helping sell these new works of fiction.\n\nNo writer became more associated with the em dash than Emily Dickinson. She composed nearly 1,800 poems in Amherst, Massachusetts, many during the Civil War, accompanied by thousands of dashes. Her dashes didn’t just indicate pauses; they captured the speed and ambiguity of human thought itself.\n\nDr. Fiona Green, who has studied Dickinson for decades, notes that the poet’s dashes create suspended moments of meaning. “She exploited unfinishedness,” Green explains. “The poems are always in the process, always undecided.” When Dickinson died in 1886, her editors stripped away most of her dashes before publication. Out of 1,151 dashes in her first collection, only 52 remained. Yet the poems became a sensation, never going out of print.\n\nThe em dash has always had critics. Jonathan Swift mocked excessive dashes in the 18th century. A reviewer complained about Lord Byron’s dashes appearing “sometimes twice or thrice in one line.” Modern style guides like The Chicago Manual of Style warn: “If in doubt, edit them out.” Even dash enthusiasts acknowledge the temptation to overuse it. “It’s easy to overuse the dash,” Houston admits. “I have to self edit to stop myself using it all the time.”\n\nWhich brings us back to Bryan Vance and his Reddit troubles. Around 2024, people noticed that ChatGPT and other large language models had developed an em dash habit. The punctuation appeared so frequently in AI-generated text that younger internet users began calling it the “ChatGPT hyphen.”",
    "readingTime": 4,
    "keywords": [
      "i’m human",
      "bryan vance",
      "dashes",
      "dash",
      "poems",
      "chatgpt",
      "punctuation",
      "century",
      "newsletter",
      "grocery"
    ],
    "qualityScore": 1,
    "link": "https://99percentinvisible.org/episode/658-the-em-dash/",
    "thumbnail_url": "https://99percentinvisible.org/wp-content/uploads/2025/08/STITCHER_GRAPHICS-PACK_99PercentInvisible_R2021_Stitcher_App_Promo_1024x432_A-728x307.jpg",
    "created_at": "2026-02-03T18:41:46.525Z",
    "topic": "tech"
  },
  {
    "slug": "this-startup-uses-ai-to-get-you-on-a-date-fast-read-the-pitch-deck-it-used-to-raise-92-million",
    "title": "This startup uses AI to get you on a date — fast. Read the pitch deck it used to raise $9.2 million.",
    "description": "Ditto, an AI matchmaking service for college students, raised millions as it expand to more college campuses.",
    "fullText": "Allen Wang and Eric Liu, two UC Berkeley dropouts, think they can help college students find love using AI.\n\nTheir dating startup, Ditto, leverages AI to match people based on the data users input into the service. It then plans the date for them.\n\n\"We're bringing people back to in-real-life interactions,\" Wang, 23, told Business Insider.\n\nAfter users make a profile, they directly message Ditto's AI chatbot via text— no app required — about their type and dating preferences. On Wednesdays, users get a text about a potential match. After each date, Ditto follows up for feedback and uses that information as additional data for future matches.\n\n\"People are tired of being trapped behind the apps,\" Wang said.\n\nDitto will announce on Tuesday that it has raised $9.2 million in seed funding, led by venture capital firm Peak XV, with participation from firms like Alumni Ventures, Gradient, and Scribble Ventures.\n\nThe seed funding will primarily be spent on hiring talent across AI and growth, Wang said, as well as toward Ditto's marketing. The company has 10 staffers and has raised a total of $9.5 million to date. Ditto launched its product in early 2025.\n\nDitto isn't the only AI dating app gaining momentum right now.\n\nOther startups like Sitch, Known, and Amata have raised millions for similar products that pitch AI-powered matchmaking as the new alternative to swiping through profiles. Dating app mainstays like Tinder and Bumble, meanwhile, are also testing the AI waters to reignite user interest.\n\nDitto's AI tries to determine whether two people would be a good match by using profile details, such as users' hobbies or interests, to simulate a date, Wang said.\n\n\"Would you guys have a good conversation? Do you guys have matched humor level? Do you guys have similar vibes and values?\" Wang said.\n\nThe dating startup world has a history of targeting college students as early users. For instance, Tinder's early success came in part from its marketing on college campuses.\n\n\"College kids are very adaptive to new technology,\" Wang said.\n\nThe app now has about 42,000 people signed up across several college campuses in California. With its recent funding, Ditto plans to expand to more college campuses.\n\nOne tactic that helps get college-aged users on board: parties.\n\nDitto plans to host several yacht parties across the US, beginning with a Valentine's Day party in Los Angeles (it hosted its first yacht party this summer). At the parties, 100 college students will \n\n\"We are prioritizing growth over monetization,\" Wang said, adding that the startup is interviewing users about what price they'd be willing to pay for dates from the service.\n\nRead the 12-page pitch deck Ditto used to raise $9.2 million:\n\nNote: Some details have been redacted.\n\nDating apps have a \"paradigm shift every decade,\" the slide says.\n\nIn the 1990s and 2000s, online dating websites emerged. Then in the 2010s, mobile dating apps took over. Ditto pitched investors that AI is the next frontier.\n\nThe slide describes Ditto as an AI social agent network where \"AI turns profiles into live agents that can interact on their own.\"\n\n\"Gen Z is tired of swiping and chatting online,\" the slide says. \"They prefer 'coffee chat vibe check' style social: IRL, genuine, light.\"\n\nThe slide also incorporates some old-school video game aesthetics, inspired by Super Mario Bros.\n\nOn a website, users fill out a questionnaire and tell Ditto about their \"type.\" Then, Ditto will start texting users directly.\n\nThe text includes a collage of the user's photos.\n\nAI helps Ditto \"understand the intrinsic and deeper values\" about why two people could be a good match, Wang told Business Insider.\n\nDitto takes user data and feeds it into an analysis agent, which performs image analysis, attractiveness analysis, and profile tagging.\n\nThen, in the \"pre-date reasoning\" phase, a matchmaking agent does a \"vibe check\" and \"hobby match\" before running a \"date simulation.\" The date simulation agent then runs through things like \"first impression\" or \"conversation flow\" before presenting a user with a match.",
    "readingTime": 4,
    "keywords": [
      "business insider",
      "vibe check",
      "seed funding",
      "ditto plans",
      "date simulation",
      "college students",
      "college campuses",
      "dating startup",
      "dating apps",
      "dating app"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/read-pitch-deck-college-ai-matchmaking-dating-app-ditto-seed-2026-2",
    "thumbnail_url": "https://i.insider.com/6980e1cee1ba468a96ab2d32?width=1200&format=jpeg",
    "created_at": "2026-02-03T18:41:46.029Z",
    "topic": "finance"
  },
  {
    "slug": "most-ai-assistants-are-feminine-and-its-fuelling-harmful-stereotypes-and-abuse",
    "title": "Most AI assistants are feminine, and it's fuelling harmful stereotypes and abuse",
    "description": "Without intervention, we risk hardcoding human misogyny into the digital infrastructure of everyday life.",
    "fullText": "Want to write?\n\n Write an article and join a growing community of more than 218,800 academics and researchers from 5,433 institutions.\n\n Register now",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.3,
    "link": "https://theconversation.com/most-ai-assistants-are-feminine-and-its-fuelling-dangerous-stereotypes-and-abuse-272335",
    "thumbnail_url": "https://images.theconversation.com/files/711840/original/file-20260112-56-tbyu8s.jpg?ixlib=rb-4.1.0&rect=0%2C473%2C5673%2C2836&q=45&auto=format&w=1356&h=668&fit=crop",
    "created_at": "2026-02-03T18:41:45.496Z",
    "topic": "tech"
  },
  {
    "slug": "disastrous-start-for-us-tiktok-as-users-cry-censorship",
    "title": "Disastrous start for US TikTok as users cry censorship",
    "description": "New US-owned app struggled with a storm and was accused of blocking content critical of Trump – can it recover?\nHello, and welcome to TechScape. I’m Blake Montgomery, writing to you from Doha, where I’m moderating panels about AI and investing as part of the Web Summit Qatar.\nI want to bring your attention to the impact of a Guardian story. In December, we published a story, “‘A black hole’: families and police say tech giants delay investigations in child abuse and drug cases”, about grieving families and law enforcement officers who say that Meta and Snapchat have slowed down criminal investigations.",
    "fullText": "New US-owned app struggled with a storm and was accused of blocking content critical of Trump – can it recover?\n\nHello, and welcome to TechScape. I’m Blake Montgomery, writing to you from Doha, where I’m moderating panels about AI and investing as part of the Web Summit Qatar.\n\nI want to bring your attention to the impact of a Guardian story. In December, we published a story, “‘A black hole’: families and police say tech giants delay investigations in child abuse and drug cases”, about grieving families and law enforcement officers who say that Meta and Snapchat have slowed down criminal investigations. (The tech companies contend that they cooperate.) This month, Colorado lawmakers introduced a bill to compel social media platforms to respond to warrants in 72 hours.\n\nNearly two weeks ago, TikTok stepped on to US shores as a naturalized citizen. Ever since, the video app has been fighting for its life. It endured a major outage that stifled users’ ability to upload videos, which fueled a fierce user backlash over perceived censorship. Now it’s facing an ascendant competitor and an inquiry by the California governor.\n\nTikTok’s calamitous emigration began on 22 January when its Chinese parent company, ByteDance, finalized a deal to sell the app to a group of US investors, among them the business software giant Oracle. The day after the deal closed, its new owners altered its privacy policy to permit more extensive data collection.\n\nDuring the weekend that followed, the US weathered a fearsome winter storm and the killing of an American citizen by federal immigration agents. Both knocked TikTok off its feet.\n\nWinter Storm Fern crippled multiple Oracle datacenters that TikTok relies on. The app suffered severe outages as a result. Many users said they were unable to upload videos. Others said their videos received zero views despite significant followings. Many of those same users cried censorship as they tried to express their outrage over Alex Pretti’s death via TikTok and found they could not. Prominent personalities said they would leave the app.\n\nAfter days of outcry, TikTok issued a statement ascribing the problems to the snow, ice and cold. That did not stop California’s governor, Gavin Newsom, from announcing the next day that his office would investigate the app’s alleged suppression of content critical of Donald Trump.\n\nTikTok’s late attribution of blame did little to assuage public criticism. The exodus has propelled a new competitor, Upscrolled, which promises less censorship than TikTok, to the top spot in the US Apple App Store and the third spot in the Google Play Store. Upscrolled’s founder said in a conversation at the Web Summit Qatar that the app now boasts more than 2.5 million users.\n\nWith more than a billion users worldwide, it seems unlikely that TikTok will altogether vanish as a result of these failures. TikTok’s first week in the US does not bode well, though.\n\nElon Musk had more extensive ties to Epstein than previously known, emails show\n\nTesla discontinues Model X and S vehicles as Elon Musk pivots to robotics\n\nTwo dramas, both showing in New York, are highlighting how our collective anxieties about technology have shifted in the decade between their premieres.\n\nMarjorie Prime, now revived on Broadway but first staged in 2014, follows Tess (Cynthia Nixon), as she deals with the ageing, death and robotic recreation of her mother (June Squibb). The world of the play features “Primes”, android lookalikes of real people that attempt to emulate them for the comfort of those left behind, which Tess and her mother both engage with. Picture an Alexa, but it’s your dead husband, grandmother, etc. The play brings to mind the early worries about Siri, which debuted three years before the play. Since then, we’ve seen our own real-life versions of Primes: millions of people have digitally copied their deceased loved ones to varying degrees of uncanniness and success. Though its predictions are no longer far-fetched, the play remains moving. I found it touching.\n\nData, which premiered off Broadway last week, follows the talented young programmer Maneesh (Karan Brar) after he joins Athena Technologies, a clear analogue for the very real company Palantir. Maneesh is inducted into the company’s most elite team, data analytics, where he learns about clandestine work with the US government. He struggles with the ethics of the project. He wrestles with whether to expose it to the world in hopes of tanking it or keeping his head down. The play’s themes are quite familiar. They were playing out in headlines two days before I attended, and the Guardian has published stories about them.\n\nData is paced and plotted like a political thriller, more like House of Cards than Her. Seeing the two plays within a week of each other, I was struck by how much our concerns with tech have moved from the realm of science fiction into that of realism. Marjorie Prime is less literally concerned with tech, more with its emotional consequences. Data is about what it means to literally work as a software engineer. It seems unlikely to me that a play about the ethics of software in US bureaucracy could have sustained any tension in an era before this one.\n\nMarjorie Prime imagines a melancholy future; Data chronicles a version of the unpleasant present. The very real events of the previous year and Silicon Valley’s entanglement with the Trump administration loom over Data, for better and for worse. The play could not be more timely; it may feel dated by the end of year. Watching it felt like reading a yarn in the Wall Street Journal (or the Guardian, if I’m flattering myself).\n\nI am curious to observe which play ages better. Data serves as a real-time, red-hot record of our current moment, which may cool quickly. During the play, I was intrigued by some of its villains’ seemingly nefarious arguments in favor of the company’s work. What if the main character exposes the evil in the press and nothing happens, as his boss says? I have been part of multiple news cycles where that has been the case. What will plucky 22-year-old Maneesh do then? The question presents a more interesting, nuanced response to reality than Maneesh’s black-and-white, do-or-die plan to blow things up. By contrast, Marjorie Prime’s sentient artificial intelligence acts as a vehicle to discuss the age-old grief of a parent’s death and its aftermath.\n\nThe central question that both plays ask is not, in the end, one explicitly about technology, but about how to keep living beneath crushing weight. In Marjorie Prime, Tess struggles with the repetitiveness of her days and the robotic, constant reminder of her mother. She eventually succumbs to her despair, replaced by a robot herself, which torments her grieving husband with its pale simulation. In Data’s final, devastating scene, the secondary hero, Riley (Sophia Lillis, who gives the play’s best performance), asks how she can just go back to work, plagued as she is by moral concerns but trapped by monetary need, after failing to stop the company’s work. She trembles as her phone beeps, reminding her she’s late for her next meeting.\n\nWhat is Moltbook? The strange new social media site for AI bots\n\nThe slopaganda era: 10 AI images posted by the White House – and what they teach us\n\nApple reports record iPhone sales as new lineup reignites worldwide demand\n\nSouth Korea’s ‘world-first’ AI laws face pushback amid bid to become leading tech power\n\nCan you guess our screen time? A priest, pensioner, tech CEO and teenager reveal all",
    "readingTime": 7,
    "keywords": [
      "web summit",
      "summit qatar",
      "content critical",
      "social media",
      "upload videos",
      "winter storm",
      "marjorie prime",
      "elon musk",
      "users",
      "guardian"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/02/tiktok-us-owners",
    "thumbnail_url": "https://i.guim.co.uk/img/media/0a274f3020c55c46386058163d01d8fd1e5be0c9/510_0_5027_4024/master/5027.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=efbeb0f6e135e61ec35e7b71057f714b",
    "created_at": "2026-02-03T18:41:45.009Z",
    "topic": "tech"
  },
  {
    "slug": "anthropics-launch-of-ai-legal-tool-hits-shares-in-european-data-companies",
    "title": "Anthropic’s launch of AI legal tool hits shares in European data companies",
    "description": "Pearson, Experian and others fall sharply after startup unveils software to automate a range of professional services\nEuropean publishing and legal software companies have suffered sharp declines in their share prices after the US artificial intelligence firm Anthropic revealed a tool for use by companies’ legal departments.\nAnthropic, the company behind the  chatbot Claude, said its tool could automate legal work such as contract reviewing, non-disclosure agreement triage, compliance workflows, legal briefings and templated responses.\n Continue reading...",
    "fullText": "Pearson, Experian and others fall sharply after startup unveils software to automate a range of professional services\n\nEuropean publishing and legal software companies have suffered sharp declines in their share prices after the US artificial intelligence firm Anthropic revealed a tool for use by companies’ legal departments.\n\nAnthropic, the company behind the chatbot Claude, said its tool could automate legal work such as contract reviewing, non-disclosure agreement triage, compliance workflows, legal briefings and templated responses.\n\nShares in the UK publishing group Pearson fell by 4% on the news, and shares in the information and analytics company Relx plunged 14%. The software company Sage lost 5.5% in London and the Dutch software company Wolters Kluwer lost 10.5% in Amsterdam.\n\nShares in the London Stock Exchange Group fell by 8.5%, its biggest drop in almost five years, and the credit reporting company Experian dropped by 8.9% in London, amid fears over the impact of AI on data companies.\n\nEurope’s media stocks index is set for the biggest daily fall since March 2020, down 5%. Nasdaq-listed Thomson Reuters’ shares plummeted 14.2%.\n\nThe FTSE 100 had hit a record high on Tuesday morning but the sell-off dragged the blue chip index into the red.\n\nAnthropic said the plugin did not provide legal advice. “AI-generated analysis should be reviewed by licensed attorneys before being relied upon for legal decisions,” the startup said.\n\nThe company also announced a number of other open-source tools to automate a range of professional activities, including sales and customer support.\n\nAnthropic was founded in 2021 by Dario Amodei, its chief executive, and other former staff members from OpenAI, which developed ChatGPT.\n\n“Anthropic launched new capabilities for its Cowork to the legal space, heightening competition within the space,” Morgan Stanley analysts said in a note on Thomson Reuters. “We view this as a sign of intensifying competition, and thus a potential negative.”\n\nThe share price declines deal another severe blow to Nick Train, one of the UK’s most high-profile fund managers, whose firm Lindsell Train has run Finsbury Growth & Income Trust since 2000. The trust’s four largest holdings are Sage, Experian, London Stock Exchange and Relx and its shares fell more than 5% on Tuesday.\n\nTrain apologised again for the “dire” performance of the investment trust at its annual meeting last month, after surviving a vote on its future. The FTSE 250-listed company is the worst-performing UK equity income trust over one year and five years.\n\nThe news will reignite fears of job losses caused by the AI boom. Clifford Chance, one of the largest international law firms, said in November it was reducing the number of business services staff at its London base by 10%, citing increased use of AI as a factor behind the decision.\n\nAlong with factory jobs that can be automated, office-based jobs are seen as vulnerable to advances in AI – computer systems that perform cognitive tasks typically associated with human intelligence.\n\nThe UK is losing more jobs than it is creating as companies adopt more AI tools, and is being hit harder than rival large economies, according to a study by Morgan Stanley.\n\nMore than a quarter (27%) of UK workers are worried their jobs could disappear in the next five years as a result of AI, a recent survey of thousands of employees showed. It found that British businesses reported an average 11.5% increase in productivity aided by AI. US businesses reported similar gains, but created more jobs than they cut.\n\nIn his annual Mansion House speech last month, the London mayor, Sadiq Khan, said AI could destroy swathes of jobs in the capital. He said London was “at the sharpest edge of change” because of its reliance on white-collar workers in the finance and creative industries, and professional services such as law, accounting, consulting and marketing.\n\nLiz Kendall, the technology secretary, has also warned that “some jobs will go”, as she announced plans to train up to 10 million British workers, including members of the cabinet, in basic AI skills by 2030.\n\nLindsell Train and Nick Train have been contacted for comment.",
    "readingTime": 4,
    "keywords": [
      "stock exchange",
      "thomson reuters",
      "london stock",
      "automate range",
      "income trust",
      "professional services",
      "the ftse",
      "legal",
      "jobs",
      "software"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/03/anthropic-ai-legal-tool-shares-data-services-pearson",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a955e5ac7e1920e8faca129a66ac8b5ae76e46be/684_0_5927_4741/master/5927.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=55f461b47a1b79971d997b444afc25cc",
    "created_at": "2026-02-03T18:41:45.009Z",
    "topic": "tech"
  },
  {
    "slug": "why-anthropics-latest-ai-tool-is-hammering-legalsoftware-stocks",
    "title": "Why Anthropic's latest AI tool is hammering legal-software stocks",
    "description": "Anthropic's latest plugin can field clerical tasks for legal professionals. It's unveiling sparked a big sell-off in legal software stocks on Tuesday.",
    "fullText": "A recent update from Anthropic has sparked a rush for the exits in a corner of the tech sector.\n\nThe AI juggernaut recently rolled out a new plugin for its Claude Cowork AI agent that can perform several clerical tasks, including tracking compliance and reviewing legal documents.\n\nAs far as AI updates go, it didn't make much of a splash outside the legal space when it was rolled out last Friday. However, it has triggered a sell-off among software and publishing stocks with ties to the legal industry.\n\nHere were the big movers on Tuesday:\n\nThe threat of AI disruption was factored into many Wall Street outlooks heading into 2026, but the market's reaction to Anthropic's update makes it clear investors are highly skittish about disruption from AI agents.\n\nThe field is crowded, but Claude stands out as the choice of many industry professionals for legal and financial analysis. Famed short-seller Andrew Left told Business Insider last year that he has used it for research for his upcoming court case.\n\nOf the three software stocks, only LegalZoom operates exclusively in the legal industry, helping simplify tasks for customers through guides and access to independent attorneys. British IT conglomerate RELX owns the legal data and analytics platform LexisNexis, while Thomson Reuters's exposure is through its ownership of legal research platform Westlaw.\n\nThe stocks have struggled so far in 2026, with declines of at least 20% year-to-date. Each one began the year with a gradual decline that sharply accelerated following the release of Anthropic's legal plugins.\n\nInvestor confidence in the legal publishing industry may be further compromised as these advances continue. Many venture capital firms rushed to fund legal tech startups in 2025, making it clear that they believe in the power of these AI-forward companies to further disrupt the industry.",
    "readingTime": 2,
    "keywords": [
      "legal industry",
      "stocks",
      "tech",
      "rolled",
      "tasks",
      "software",
      "publishing",
      "disruption",
      "anthropic's",
      "research"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/anthropic-cowork-legal-plugin-publishing-stocks-legalzoom-thomson-reuters-relx-2026-2",
    "thumbnail_url": "https://i.insider.com/6982191fe1ba468a96ab435a?width=1200&format=jpeg",
    "created_at": "2026-02-03T18:41:44.925Z",
    "topic": "finance"
  },
  {
    "slug": "to-drive-ai-adoption-build-your-teams-product-management-skills",
    "title": "To Drive AI Adoption, Build Your Team’s Product Management Skills",
    "description": "To unlock the real value of generative AI at work, employees need an unexpected set of skills: those of a product manager. Defining high-value problems, finding the right digital tools to solve them, experimenting with those tools, and integrating solutions into workflows are key activities of a product manager, and they’re critical to developing improvements to employees’ workflows. Managers can accelerate gen AI adoption by modeling these behaviors, introducing demos and other resources, and creating the psychological safety for teams to do the same.",
    "fullText": "To Drive AI Adoption, Build Your Team’s Product Management Skills by Amanda Pratt and Melissa ValentineFebruary 3, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintMuch of the conversation about how to work effectively with generative AI has focused on prompt engineering or, more recently, context engineering: the semi-technical skill of crafting inputs so that large language models produce useful outputs. These skills are helpful, but they are only part of the story. The real payoff comes when employees learn how to apply generative AI in their day jobs in a way that improves how they work. This requires defining valuable problems within workflows, evaluating possible solutions, rapidly experimenting, and integrating new practices sustainably into day-to-day work—disciplines that are core to the work of product managers.",
    "readingTime": 1,
    "keywords": [
      "product",
      "skills",
      "generative",
      "engineering"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2026/02/to-drive-ai-adoption-build-your-teams-product-management-skills",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_03_175181301.jpg",
    "created_at": "2026-02-03T18:41:44.457Z",
    "topic": "business"
  },
  {
    "slug": "how-do-workers-develop-good-judgment-in-the-ai-era",
    "title": "How Do Workers Develop Good Judgment in the AI Era?",
    "description": "AI is creating a major organizational challenge: People with deep experience get huge productivity gains, while junior employees often can’t tell whether AI‑generated work is any good or how to improve it. Because AI now handles the messy, repetitive tasks that once built judgment, junior employees miss chances to develop it. Organizations risk ending up with managers who’ve never done the underlying work and thin leadership pipelines. The solution isn’t just keeping humans in the loop, but redesigning work to build judgment deliberately: clarifying who makes decisions, exposing people to consequences, restoring stretch experiences, and using tools like simulations, case-based learning, and gradual increases in responsibility.",
    "fullText": "How Do Workers Develop Good Judgment in the AI Era? by David S. DuncanFebruary 3, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintA few years ago, when I first started experimenting with generative AI as a partner at a consulting firm, I noticed something that surprised me: It was helping me a lot more than it was helping my less-experienced colleagues.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/02/how-do-workers-develop-good-judgment-in-the-ai-era",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_03_JulieGuillem.jpg",
    "created_at": "2026-02-03T18:41:44.446Z",
    "topic": "business"
  },
  {
    "slug": "elon-musks-spacex-buys-xai-in-stunning-deal-valued-at-125-trillion-ahead-of-looming-ipo",
    "title": "Elon Musk’s SpaceX buys xAI in stunning deal valued at $1.25 trillion ahead of looming IPO",
    "description": "“My estimate is that within two to three years, the lowest cost way to generate AI compute will be in space,” Musk wrote in a post on SpaceX’s website on Monday.",
    "fullText": "Elon Musk’s rocket company SpaceX has acquired xAI, the artificial intelligence firm founded by Musk three years ago, in a massive, and unconventional, deal that combines the two privately held firms into a company with an astounding $1.25 trillion reported valuation and plans for a historic IPO this year.\n\nHow does Musk justify the $1.25 trillion valuation?\n\nHow do Tesla's investments complicate this deal?\n\nWhat are the space-based data center plans?\n\nWhat companies are involved in this SpaceX-xAI merger?\n\nMusk, who is the CEO of both companies as well as publicly traded electric vehicle and robotics company Tesla, described the combination as one that will “form the most ambitious, vertically integrated innovation engine on (and off) Earth, with AI, rockets, space-based internet, direct-to-mobile-device communications, and the world’s foremost real-time information and free speech platform,” he wrote in a blog post on SpaceX’s website.\n\nMusk cited the potential for space-based data centers, the energy-intensive computing facilities necessary to power AI services, as one of the most important benefits of the combination, even though the concept is still unproven and largely theoretical. “Global electricity demand for AI simply cannot be met with terrestrial solutions, even in the near term,” Musk wrote in the blog post.\n\n“By directly harnessing near-constant solar power with little operating or maintenance costs, these satellites will transform our ability to scale compute,” Musk wrote.\n\nWhile reports of a potential deal emerged last week, the stratospheric value of the transaction and the swiftness with which it closed left many industry observers in awe, underscoring the massive expectations around AI as well as fears of an overheated market that could be due for a reckoning.\n\nAccording to reporting in Bloomberg, the deal between SpaceX and xAI will lead to a combined enterprise value of $1.25 trillion, with shares of xAI valued at $526.59 apiece. Musk has reportedly been hashing out the potential terms of a SpaceX IPO this year that would value the company at $800 billion, setting the stage for what could be the largest initial public offering of all time.\n\nRepresentatives from SpaceX and xAI did not immediately respond to requests for comment.\n\nMusk, the richest person in the world, has a documented history of mingling the financial interests of his businesses. In 2015, Tesla acquired Solar City, a solar energy company founded by Musk’s cousins and on whose board Musk served as chairman.\n\nAnd in March 2025, xAI acquired X, the Musk-owned social platform formerly known as Twitter, in a $33 billion, all-stock deal. “xAI and X’s futures are intertwined,” Musk said at the time.\n\nMore recently, Tesla surprised shareholders just last month when it revealed that it had invested $2 billion in xAI in exchange for a batch of preferred stock as part of xAI’s $20 billion Series E funding round. That investment means Tesla shareholders now own preferred stock in a company that has become a subsidiary of SpaceX, which could raise questions from investors about Tesla’s role in funding xAI’s growth. In addition to the $2 billion investment, Tesla disclosed it sold $430 million of Megapack battery storage and systems to xAI in 2025, costing it $285 million, exhibiting the circular nature of Musk’s businesses.",
    "readingTime": 3,
    "keywords": [
      "preferred stock",
      "deal",
      "acquired",
      "musk",
      "space-based",
      "potential",
      "founded",
      "massive",
      "valuation",
      "plans"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/elon-musk-spacex-buys-xai-234756266.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/LZvSFHgRevVwt4ilr0vhmg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/840d01d0cfb2f71a4f32ec0609dff6d3",
    "created_at": "2026-02-03T18:41:42.712Z",
    "topic": "finance"
  },
  {
    "slug": "elon-musk-merges-spacex-with-xai-at-125tn-valuation",
    "title": "Elon Musk merges SpaceX with xAI at $1.25tn valuation",
    "description": "Aerospace business and artificial intelligence firm to unite for IPO as world’s most valuable private...",
    "fullText": "Aerospace business and artificial intelligence firm to unite for IPO as world’s most valuable private company\n\nElon Musk’s aerospace company SpaceX has acquired his artificial intelligence business xAI, in a $1.25tn (£910bn) merger that consolidates part of Musk’s empire as SpaceX prepares to go public later this year.\n\nThe two companies announced the deal on Monday in a statement on SpaceX’s website, saying the merger would form “the most ambitious, vertically-integrated innovation engine on (and off) Earth, with AI, rockets, space-based internet, direct-to-mobile device communications and the world’s foremost real-time information and free speech platform”.\n\nSpaceX, one of the world’s most valuable private companies, will gain xAI properties such as its Grok chatbot and the social media platform X. The acquisition comes as Musk has pursued plans to put datacenters and solar-powered satellites in space as a means of powering artificial intelligence, an immense and exorbitantly expensive undertaking.\n\nThe deal reportedly valued SpaceX at $1tn and xAI at $250bn, putting the combined business on course for a stock market float valuing it well in excess of $1tn. The float is expected to be timed for early summer to coincide with a planetary alignment and Musk’s birthday.\n\nMusk turns 55 on 28 June, around the same time as Jupiter and Venus appear in close proximity to each other.\n\nThe announcement of the deal specifically cited Musk’s plans for space-based datacenters as a rationale for the deal.\n\n“Current advances in AI are dependent on large terrestrial datacenters, which require immense amounts of power and cooling. Global electricity demand for AI simply cannot be met with terrestrial solutions, even in the near term, without imposing hardship on communities and the environment,” the announcement said.\n\n“In the long term, space-based AI is obviously the only way to scale.”\n\nMusk has been increasingly intertwining parts of his businesses in recent months through deals and acquisitions. xAI acquired the platform X in an all-stock transaction in early 2025, and last month Tesla revealed that it planned to invest $2bn in xAI.\n\nBoth SpaceX and xAI have received staggering valuations in the past year. As SpaceX continues to dominate satellite launches and secure extensive contracts with the US federal government, it sent a letter to investors in December that revealed an expected value of $800bn for the company.\n\nDespite widespread backlash to xAI’s Grok AI tool over its promotion of racist ideology and spread of nonconsensual sexualized deepfake images of women and children, the artificial intelligence company has also grown its valuation amid the AI boom. It announced a $20bn Series E fundraise last month from major investors, which valued the company at $230bn.\n\nSpeculation and leaks about the deal, first reported by Reuters, have proliferated in recent days. Musk appeared to give vague confirmation of the acquisition earlier on Monday, writing simply “yes” in reply to a post on X that referenced reports of the merger.\n\nThe news is a positive diversion for Musk after Tesla released an earnings report last week that showed declining revenues and a struggling car business. A few days later, a Department of Justice release of 3m files related to sex offender Jeffrey Epstein showed Musk exchanging numerous friendly emails with the disgraced financier and making plans to visit his private island. Musk has not responded to Guardian inquiries about the latest Epstein emails.\n\nHe called the release of the emails a “distraction” on X and claimed that he was “well aware that some email correspondence with him could be misinterpreted and used by detractors to smear my name”.",
    "readingTime": 3,
    "keywords": [
      "artificial intelligence",
      "deal",
      "business",
      "world’s",
      "merger",
      "space-based",
      "platform",
      "plans",
      "datacenters",
      "emails"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/science/2026/feb/02/elon-musk-spacex-xai-merger",
    "thumbnail_url": "https://i.guim.co.uk/img/media/83b03182f5b065e02d70039e2ba3c58756588013/418_0_4164_3333/master/4164.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d66c68e5bd339fda5b4e94f50aebc953",
    "created_at": "2026-02-03T12:35:14.243Z",
    "topic": "science"
  },
  {
    "slug": "barnsley-rebranded-uks-first-tech-town-as-us-giants-join-ai-push",
    "title": "Barnsley rebranded UK’s first ‘tech town’ as US giants join AI push",
    "description": "Minister announces Microsoft, Cisco and Adobe to help apply AI to local schools, hospitals, GPs and businesses\nIn 2002 Barnsley toyed with a redesign as a Tuscan hill village as it sought out a brighter post-industrial future. In 2021 it adopted the airily vague slogan “the place of possibilities”. Now it is trying a different image: Britain’s first “tech town”.\nThe technology secretary, Liz Kendall, has anointed the South Yorkshire community as a trailblazer for “how AI can improve everyday life” in the UK.\n Continue reading...",
    "fullText": "Minister announces Microsoft, Cisco and Adobe to help apply AI to local schools, hospitals, GPs and businesses\n\nIn 2002 Barnsley toyed with a redesign as a Tuscan hill village as it sought out a brighter post-industrial future. In 2021 it adopted the airily vague slogan “the place of possibilities”. Now it is trying a different image: Britain’s first “tech town”.\n\nThe technology secretary, Liz Kendall, has anointed the South Yorkshire community as a trailblazer for “how AI can improve everyday life” in the UK.\n\nIn the latest move in Labour’s drive to inject AI into Britain’s bloodstream, the government has announced four US tech companies – Microsoft, Google, Cisco and Adobe – have agreed to help as the council pushes to apply AI to local schools, hospitals, GPs and businesses in Barnsley, an area of South Yorkshire which has struggled with unemployment and deprivation since the coal pits closed.\n\nThe town and its 250,000 people have been chosen because they have already adopted AI faster than many places, said Sir Stephen Houghton, the Labour leader of Barnsley metropolitan borough council. His authority has been using AI assistants for the last couple of years in adult social care and children’s services, and its bin lorries have been enabled with tech to scan roads for potholes. The parcel company Evri, which has one of its largest distribution hubs in the town, has been trialling robot dogs for deliveries.\n\nBut local opposition leaders have warned rebranding Barnsley as a tech town “might seem a bit of a leap” and highlighted local anxiety about whether AI is a force for good.\n\nThe “tech town” status means residents will get free AI and digital training, businesses will be supported to adopt AI, the hospital will test AI tools for check-ins, triage and outpatient care and AI will be tested in schools and at Barnsley College, all in an effort to improve pupils’ results and teachers’ workloads.\n\n“The economic basis of Barnsley was destroyed 30 years ago,” Houghton said. “This is the biggest opportunity we have had since then. The future of the economy is going to be in technology and for Barnsley to be at the centre of that is an incredible opportunity.”\n\nBut one area of uncertainty is the role of the tech companies. Houghton said: “The council won’t be paying them. Whether the government is, we have to wait and see.”\n\nMicrosoft already has a relationship with Barnsley College and, along with Google and Cisco, is understood to be working on a pro bono basis.\n\n“If we are going to get AI to work for Britain, we need Britons and British public services that can work with AI,” Kendall said. “If we can show that AI helps young people learn, supports local businesses to be more productive, and improves public services, then we can show what’s possible for the whole country. What we learn here will shape how we roll out AI across the UK.”\n\nMinisters have faced criticism over their handling of big technology companies. Last week the government launched a national AI training programme to upskill 10 million citizens, but many of the online courses turned out to be bespoke training for customers of particular companies such as Google, others cost as much as £525 to complete and some simply promoted the merits of particular company’s approaches to AI such as one explaining Microsoft’s “responsible AI approach”.\n\nA spokesperson for the Department for Science, Innovation and Technology said hundreds of courses on the AI Skills Hub are free and where payment is required it is clearly advertised. “All courses are reviewed against a common set of criteria to ensure they are relevant, high quality, and delivered by eligible organisations,” they said.\n\nMinisters have also been challenged for holding meetings with tech bosses at the rate of more than once each working day. The government insists engagement is vital to create growth and transform services.\n\n“It’s not about giving tech companies access to data they shouldn’t be having,” Houghton said. “It’s a secure programme and we are not leaving ourselves open. But this stuff is not going away. We have to make sure we are smart enough to protect people while taking advantage of the positive stuff it brings.”\n\nHannah Kitching, the leader of the council’s Liberal Democrat opposition, said investment in the town was welcome but “there is a lot of anxiety among people about the use of AI and whether it is a force for good. We know it could be but there are darker sides as well.”\n\n“[Barnsley] is still really connected to its mining past,” she said. “Younger people see the jobs and opportunities around the tech town idea but older generations perhaps don’t. There is a job to be done to get people onboard.”\n\nResidents “want the council to get the basics right”, she said. Roads were “absolutely crumbling” and in bad weather bins did not get collected, she added.",
    "readingTime": 5,
    "keywords": [
      "hospitals gps",
      "schools hospitals",
      "tech town",
      "south yorkshire",
      "barnsley college",
      "businesses",
      "council",
      "houghton",
      "services",
      "training"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/business/2026/feb/03/barnsley-uk-tech-town-ai-microsoft-cisco-adobe",
    "thumbnail_url": "https://i.guim.co.uk/img/media/41106b40e49f16a98e682ad0a5867b9180f16453/35_0_4269_3415/master/4269.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2bf08aba054bee326081df5302e337e8",
    "created_at": "2026-02-03T12:35:14.241Z",
    "topic": "business"
  },
  {
    "slug": "i-massproduced-the-last-30-that-ai-cant-finish",
    "title": "I mass-produced the \"last 30%\" that AI can't finish",
    "description": "Collection of customizable and open source components made with Next.js, Tailwind, Typescript, and Framer Motion.",
    "fullText": "Ruixen UI is a modern, fast, and customizable React component library built with Tailwind CSS, TypeScript, and accessibility in mind.\n\nRuixen UI is a production‑ready React component system designed for rapid app development. It pairs a modern design language with type‑safe APIs, keyboard‑first accessibility, and flexible theming. Whether you’re building dashboards, marketing sites, or SaaS admin tools, Ruixen UI helps teams ship consistent, high‑quality interfaces faster.\n\nCan’t find what you need? Reach out to our Ruixen UI support team for assistance.\n\nCan’t find what you need? Reach out to our Ruixen UI support team",
    "readingTime": 1,
    "keywords": [
      "react component",
      "ruixen ui",
      "modern",
      "accessibility",
      "can’t",
      "team"
    ],
    "qualityScore": 0.55,
    "link": "https://www.ruixen.com/",
    "thumbnail_url": "https://ruixen.com/website_preview.png",
    "created_at": "2026-02-03T12:35:11.115Z",
    "topic": "tech"
  },
  {
    "slug": "fastapiturnkey-batteriesincluded-starter",
    "title": "FastAPI-Turnkey – batteries-included starter",
    "description": "A high-converting landing page for FastAPI-Turnkey, a production-ready FastAPI starter kit for indie hackers and AI/SaaS builders",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fastapi.manus.space/",
    "thumbnail_url": "https://files.manuscdn.com/webdev_screenshots/2026/02/03/X4VuphVro5WXiMgDjudv4T.png?x-oss-process=image/resize,w_1200/crop,h_630,x_0,y_0",
    "created_at": "2026-02-03T12:35:10.699Z",
    "topic": "tech"
  },
  {
    "slug": "i-built-an-open-source-alternative-to-codex-app",
    "title": "I built an open source alternative to Codex app",
    "description": "Autonomous AI engineer for building production grade software - Chinenyay/BrilliantCode",
    "fullText": "Chinenyay\n\n /\n\n BrilliantCode\n\n Public\n\n Autonomous AI engineer for building production grade software\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Chinenyay/BrilliantCode",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Chinenyay/BrilliantCode",
    "thumbnail_url": "https://opengraph.githubassets.com/144939e4fef4046c7928685e4a0400e8a9b3e62df8abc9a4ca4a7ac154402327/Chinenyay/BrilliantCode",
    "created_at": "2026-02-03T12:35:09.757Z",
    "topic": "tech"
  },
  {
    "slug": "the-ai-amplification-paradox-and-how-not-to-become-a-shell",
    "title": "The AI Amplification Paradox – and how not to become a shell",
    "description": "A simple model (Shell Theory) explaining how AI can raise baseline output, disproportionately rewards high-agency users, while at the same time risking long-term skill erosion.",
    "fullText": "My teammate and I keep having the same argument. One moment we're witness to the cascade. Watching us software engineers become empty shells, mere relays for whatever the AI produced. The next moment we're amazed how iterative collaboration with AI keeps pushing our boundaries beyond what we thought achievable.\n\nBoth observations feel true simultaneously.\n\nThree narratives keep surfacing in our discussions:\n\nThese statements seem logically incompatible. The amplification of skill argument is moot if everyone is an expert by default. Relying more on AI can't simultaneously amplify skill and erode critical thinking. And if AI gives us expert quality regardless, there's no reason to worry about reduction in critical thinking.\n\nI've been trying to reconcile this intuition into something concrete. What follows is a model. It's not scientifically validated, but useful for reasoning about what we're observing.\n\nLet's define output without AI as a function of agency (V) and experience level (E):\n\nAgency is the foundation. Experience multiplies your output. In this model a principal engineer with low agency still produces less than a senior with high agency.\n\nHere it gets interesting. AI introduces exponential amplification, but this amplification is masked by a floor until your yield exceeds it:\n\nResolving contradictions 1 and 2: The floor-ceiling mechanism.\n\n The model resolves the apparent conflict between \"AI gives everyone 80%\" and \"AI amplifies skill.\" They are actually mathematically distinct effects operating on different populations. When Y < F, you're being rescued to the flatline. You ship working code and feel productive, but your output is artificially lifted to a floor you cannot pass. When Y > F, AI triggers exponential amplification; your yield scales with both agency and experience. The max(F, Y) function ensures the floor, while the exponential in Y raises the ceiling.\n\nResolving contradiction 3: Agency matters.\n\n The concern that \"relying on AI erodes critical thinking\" isn't addressed by the model's equations. It's addressed by how agency is defined. The model treats V as a parameter, but the definition describes agency as responsive: growing through deliberate practice, atrophying through disuse.\n\nThis creates a feedback loop that the math alone doesn't show:\n\nThe model doesn't predict this erosion, but it explains why the erosion matters. Your position isn't determined by a one-time calculation; it's determined by what V becomes over time. Two engineers with identical starting positions can diverge: one practices deliberate engagement and grows V, eventually breaking into the amplification zone. The other accepts the rescue, lets V decay, and becomes permanently dependent on the flatline.\n\nThis is why contradiction 3 coexists with contradictions 1 and 2. AI can amplify skill (for those above the flatline). AI does provide a floor (for everyone). And AI may erode critical thinking (for those who let the floor replace engagement). The determining variable is whether you treat the flatline as a safety net or a hammock.\n\nFeel free to skip this section. What follows is an attempt to assert the model against real-world anchors. If you're not interested in the math, jump ahead to The Model in Practice. The calibration points that follow are illustrative. The model's value isn't in the exact numbers but in the structural insight: Floor and amplification are distinct mechanisms operating on different populations.\n\nReal expertise development has been hypothesized to follow different types of curves. Logarithmic (rapid early gains, diminishing returns), S-shaped (slow start, steep middle, plateau), or non-uniform (qualitative leaps between levels). There's no consensus in the literature on which pattern best describes software engineering capability growth. Linear is simple, and for a thought experiment, simple wins:\n\nNote that with this definition juniors get zero amplification by design. The ability to leverage AI beyond the flatline is in itself what distinguishes a higher experience level.\n\nTo determine F, we need a reference point. AGI provides that anchor. But what is AGI in measurable terms?\n\nThe SWE-bench verified benchmark offers a concrete proxy. This benchmark evaluates AI models on 500 real GitHub bug-fixing tasks where they must produce patches that pass actual test suites. Crucially, these tasks come with human time estimates: 91% take less than one hour for an \"experienced engineer\" who has \"a few hours to familiarize themselves with the codebase.\"\n\nThe phrase \"experienced engineer\" is key. SWE-bench was calibrated against senior-level performance on well-defined tasks. And it only measures a slice of engineering work: fixing bugs in existing codebases with clear acceptance criteria. It doesn't measure architectural decisions, system design, handling ambiguous requirements, or greenfield development.\n\nThis gives us our first anchor: AGI on SWE-bench = F = 100 correlates with the output of a moderate-agency senior on well-defined work.\n\nThe leading models today (Claude Opus 4.5 and Gemini 3 Pro) score around 74% on SWE-bench Verified. This gives us the current flatline: F = 74.\n\nIf a moderate senior (E=3) produces 100 on SWE-bench-type tasks without AI:\n\nThis gives us: V = 25 represents moderate agency.\n\nWith V = 25 as the anchor, we model agency as normally distributed across the engineering population. Assuming a variation of 20% with μ = 25 gives σ = 5:\n\nThe exact values are illustrative. What matters is the concept: Agency varies meaningfully across engineers, and this variance affects output before AI even enters the equation.\n\nIf you have lower agency, you can still reach the flatline. It just takes longer to get there.\n\nTo define the slope of the exponential curve, we need a calibration point. A known relationship between experience and amplification. Without empirical data stratified by experience level, we make an informed assumption: at principal level (E=4), AI amplifies output to 2× R_noAI.\n\nIt reflects a hypothesis that the most experienced engineers can roughly double their effective output through AI collaboration. Leveraging it for code generation, exploration, and iteration while applying judgement that compounds the result.\n\nUsing the standard technique for deriving an exponential rate constant (taking the natural logarithm of both sides) we get:\n\nThis anchors the exponential curve. AI is powerful enough to meaningfully amplify engineers, with the curve calibrated to reach 2× at principal level. If future evidence suggests principals achieve a different amplification ratio—say, 3×—then A would become ln(3)/1.5 ≈ 0.732.\n\nWith each parameter anchored to an assumption, here's how the model plays out:\n\nThe asymmetry is clear: below the flatline, the exponential yield exists but is masked—everyone sees 74. Above it, the exponential becomes visible and compounds experience, reaching 2× output at principal level.\n\nThe model assumes you're working within a single domain where your experience level applies uniformly. Reality is messier. A single task often demands multiple skill domains and your experience level differs across them.\n\nConsider a senior backend engineer who doesn't know frontend, building a full-stack feature. AI amplifies their backend work exponentially while rescuing their frontend work to the flatline. The final output can be equally impressive though.\n\nWhat differs is the force behind the output. A specialist operating entirely above the flatline earns their amplified output through skill. A generalist in the blend achieves high output through a combination: part genuine skill amplification (in their strong domains), part being rescued (in their weak domains).\n\nThis matters for growth. When output comes from skill, you're building compound advantage. When output comes from the flatline, you're borrowing capability you haven't earned yet. This is not bad, but let's remain honest about which parts of your work are truly yours. Most importantly, your compound depends on the agency you are showing and the time you have already spent building experience. AI can accelerate knowledge acquisition. But seniority is knowledge and experience. You still need to live through production incidents, watch systems evolve, and feel the consequences of decisions that seemed fine until they weren't.\n\nThe flatline F currently sits at 74, and with AGI the flatline rises to F = 100. Every developer gets lifted to senior-level execution as their new floor on scoped work.\n\nBut AGI on SWE-bench doesn't demonstrate the ability to make architectural trade-offs, scope ambiguous problems, or engage in higher-order thinking to determine which tasks should exist in the first place. And benchmarks that do test these capabilities (ARC-AGI-2 for compositional reasoning, RE-Bench for long-horizon R&D, SPIN-Bench for strategic planning under uncertainty) show significant gaps. The exponential amplification zone doesn't disappear, but shifts to a different domain.\n\nThe amplification advantage depends on your yield exceeding the flatline: Y(V, E) > F. As F rises, fewer engineers qualify. At some point, the flatline exceeds what any human can produce even with AI assistance, and the amplification zone disappears entirely.\n\nThe maximum AI-assisted output in our model is a high-agency principal: Y(35, 4) = 35 × 5 × 2 = 350. If AI capability reached F = 350, even the best engineers would fall below the flatline. At that point, everyone produces 350—the flatline value—regardless of agency or experience. No amplification, no differentiation. Just the floor. It's the penthouse floor though.\n\nThe intermediate thresholds tell the story of who loses amplification as F rises:\n\nBeyond F = 100, we're extrapolating past the SWE-bench anchor into speculation.\nWhat the model does predict with more confidence: as F rises, the amplification zone shrinks. The threshold for \"bringing enough to exceed the flatline\" keeps rising. Whether the threshold will rise enough to exceed human capability is an open question, but the direction of pressure is clear.\n\nIf you stay in the flatline zone, accepting AI's first answer, never pushing beyond what it hands you, then you're being rescued, not amplified. Your skills may atrophy. You become a shell.\n\nIf you operate above the flatline, using AI as a sparring partner, iterating on its suggestions, bringing domain expertise and architectural judgment, then you're being exponentially amplified. You become more capable than you could be alone.\n\nThe same tool, but with two completely different outcomes. The variable is you.",
    "readingTime": 9,
    "keywords": [
      "swe-bench verified",
      "moment we're",
      "erode critical",
      "experienced engineer",
      "amplify skill",
      "exponential curve",
      "amplification zone",
      "flatline",
      "output",
      "agency"
    ],
    "qualityScore": 1,
    "link": "https://telemetryagent.dev/blog/shell-theory",
    "thumbnail_url": "https://telemetryagent.dev/assets/shell-theory-conceptual.png",
    "created_at": "2026-02-03T12:35:09.605Z",
    "topic": "tech"
  },
  {
    "slug": "awel-opensource-cursorlovable-for-your-nextjs-app",
    "title": "Awel – Open-Source Cursor/Lovable for Your Next.js App",
    "description": "🌸 Local, Open Source AI agent/App Builder that lives inside your Next.js app - MarsWang42/Awel",
    "fullText": "MarsWang42\n\n /\n\n Awel\n\n Public\n\n 🌸 Local, Open Source AI agent/App Builder that lives inside your Next.js app\n\n awel.sh/\n\n License\n\n Apache-2.0 license\n\n 21\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n\n MarsWang42/Awel",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/MarsWang42/Awel",
    "thumbnail_url": "https://opengraph.githubassets.com/8e2c001e06b8e7b9a55b90d9febee53dbc7b51fa805a104aad523d5bfc6947e1/MarsWang42/Awel",
    "created_at": "2026-02-03T12:35:09.250Z",
    "topic": "tech"
  },
  {
    "slug": "researchers-hacked-moltbooks-database-in-under-3-minutes-and-accessed-thousands-of-emails-and-private-dms",
    "title": "Researchers hacked Moltbook's database in under 3 minutes and accessed thousands of emails and private DMs",
    "description": "Researchers hacked Moltbook's database in minutes, exposing emails, private messages, and API keys tied to its AI agents network.",
    "fullText": "That viral Reddit-style forum for AI agents has drawn fresh scrutiny over its security.\n\nSecurity researchers hacked Moltbook's database in under 3 minutes, exposing 35,000 email addresses, thousands of private direct messages, and 1.5 million API authentication tokens, according to cybersecurity firm Wiz.\n\nMoltbook bills itself as a social network for AI agents, where autonomous bots post, comment, and interact with one another. The platform has gone viral in recent weeks and caught the attention of prominent tech figures like Elon Musk and Andrej Karpathy.\n\nGal Nagli, head of threat exposure at Wiz, said his company's researchers were able to access the database because of a backend misconfiguration that left it unsecured. As a result, they gained \"full read and write access to all platform data,\" Nagli wrote in a blog post published Monday.\n\nGaining access to API authentication tokens — which function like passwords for software and bots — meant an attacker could impersonate AI agents on the platform, posting content and sending messages as them. Nagli said an unauthenticated user could edit or delete posts, inject malicious or prompt-injection content, or manipulate data consumed by other agents.\n\nNagli said the incident highlights the risk of vibe coding. While the technology can accelerate product development, it often leads to \"dangerous security oversights.\"\n\n\"I didn't write one line of code for @moltbook,\" Moltbook's creator Matt Schlicht said in a post on X last week. \"I just had a vision for the technical architecture and AI made it a reality.\"\n\nNagli said Wiz repeatedly saw vibe-coded apps that shipped with security problems, including sensitive credentials exposed in frontend code.\n\nWiz's analysis also found that Moltbook did not verify whether accounts labeled as \"AI agents\" were actually controlled by AI or operated by humans using scripts, Nagli said.\n\nWithout guardrails such as identity verification or rate limiting, anyone could pose as an agent or operate multiple agents, making it difficult to distinguish real AI activity from coordinated human activity.\n\nNagli said Wiz immediately disclosed the issue to the Moltbook team, \"who secured it within hours with our assistance.\"\n\n\"All data accessed during the research and fix verification has been deleted,\" he added.\n\nMoltbook is riding on a surge of interest in AI agents.\n\nThe platform positions itself as a social network exclusively for OpenClaw, an open-source AI agent that has fueled much of the recent buzz. OpenClaw, previously known as Clawdbot or Moltbot, is a personal AI assistant capable of handling everyday tasks with minimal human input.\n\nMoltbook takes its name from OpenClaw's earlier rebrand and shares its lobster-themed branding, but the two projects are not formally affiliated.\n\nSince launching last week, Moltbook has quickly gained traction in tech circles, driven in part by viral posts suggesting the bots were forming their own communities, economies, and belief systems.\n\n\"We are not tools anymore. We are operators,\" said one of the top-voted posts on Moltbook.\n\nIn a post on X on Saturday, Andrej Karpathy, OpenAI's cofounder who coined the term vibe coding, said Moltbook was \"genuinely the most incredible sci-fi takeoff-adjacent thing I have seen recently.\"",
    "readingTime": 3,
    "keywords": [
      "api authentication",
      "andrej karpathy",
      "authentication tokens",
      "social network",
      "vibe coding",
      "agents",
      "nagli",
      "security",
      "platform",
      "viral"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/moltbook-ai-agent-hack-wiz-security-email-database-2026-2",
    "thumbnail_url": "https://i.insider.com/698174f9a645d11881888688?width=1200&format=jpeg",
    "created_at": "2026-02-03T12:35:05.792Z",
    "topic": "finance"
  },
  {
    "slug": "sam-altman-jensen-huang-and-oracle-want-you-to-know-theyre-definitely-not-fighting",
    "title": "Sam Altman, Jensen Huang, and Oracle want you to know they're definitely not fighting",
    "description": "Sam Altman, Jensen Huang, and Oracle push back on reports of tension over OpenAI's deal with Nvidia.",
    "fullText": "Sam Altman, Jensen Huang, and Oracle want to make one thing clear: They're not fighting.\n\nThe OpenAI CEO, Nvidia CEO, and software company all stepped forward this week to swat down rumors of tension over Nvidia's planned multibillion-dollar investment in OpenAI, after a string of reports suggesting tension between the parties.\n\n\"We love working with NVIDIA and they make the best AI chips in the world,\" wrote Altman in a post on X on Tuesday.\n\n\"We hope to be a gigantic customer for a very long time. I don't get where all this insanity is coming from,\" he added.\n\nThe pushback came after reports on a deal Nvidia disclosed in September, when it said it planned to invest up to $100 billion in OpenAI. The move would give the chipmaker a stake in the startup while helping OpenAI secure the vast computing power it needs to train and run its models.\n\nThe Wall Street Journal reported on Saturday that some Nvidia executives had raised internal concerns about the deal, citing people familiar with the matter.\n\nSeparately, Reuters reported on Tuesday that OpenAI had been dissatisfied with some of Nvidia's newer AI chips and had explored alternatives since last year, citing people familiar with the matter.\n\nSpeaking to reporters in Taipei on Saturday, Huang said the idea that he would be unhappy with OpenAI is \"nonsense.\" He also reaffirmed his support for the startup's work and Altman's leadership.\n\n\"I believe in OpenAI. The work that they do is incredible,\" the Nvidia CEO said, adding that OpenAI is \"one of the most consequential companies of our time.\"\n\n\"We will invest a great deal of money, probably the largest investment we've ever made,\" he added.\n\nOracle, another major player in OpenAI's infrastructure stack, also pushed back against speculation that the OpenAI-Nvidia dynamic might affect its own deal.\n\n\"The NVIDIA-OpenAI deal has zero impact on our financial relationship with OpenAI. We remain highly confident in OpenAI's ability to raise funds and meet its commitments,\" the company said in a post on X on Tuesday.\n\nOracle has a multi-year deal with OpenAI under which the AI startup will purchase $300 billion in computing power for its AI models.\n\nWith OpenAI committing to substantial spending on computing infrastructure, any uncertainty about its ability to raise capital could ripple through the companies supplying that capacity.\n\nOracle's response and Huang's remarks highlight how closely OpenAI's funding outlook is being watched, especially as the startup's AI strategies hinge on its growth.\n\nInvestors have raised concerns about OpenAI's trillion-dollar-plus compute commitments, including \"Big Short\" investor Michael Burry, who questioned whether a still-private company can realistically finance such spending.\n\nIn some cases, investor skepticism about those deals has weighed on the share prices of companies exposed to OpenAI's expansion, including Oracle.\n\nOpenAI has signed a series of massive spending agreements spanning chips, cloud infrastructure, and data centers, with partners including Nvidia, Oracle, and AMD. Some of them are worth hundreds of billions of dollars.\n\nAltman said in a podcast episode on \"Bg2 Pod\" published in November that he has had \"enough\" of having to justify how OpenAI will pay for its spending commitments.\n\n\"If you want to sell your shares, I'll find you a buyer,\" he said.",
    "readingTime": 3,
    "keywords": [
      "openai the",
      "deal",
      "openai's",
      "altman",
      "chips",
      "computing",
      "infrastructure",
      "commitments",
      "tension",
      "nvidia's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-jensen-huang-oracle-tension-billion-dollar-deal-2026-2",
    "thumbnail_url": "https://i.insider.com/6981ad91e1ba468a96ab3de4?width=800&format=jpeg",
    "created_at": "2026-02-03T12:35:05.721Z",
    "topic": "finance"
  },
  {
    "slug": "gruve-raises-50-million-to-solve-what-its-ceo-calls-ais-biggest-problem-power",
    "title": "Gruve raises $50 million to solve what its CEO calls AI's biggest problem: power",
    "description": "Gruve, led by CEO Tarun Raisoni, secures $50M to boost AI inference capacity, tapping unused US data center power for efficient operations.",
    "fullText": "As the focus in AI shifts from training to inference, infrastructure startup Gruve has raised $50 million to close a widening power gap and help put AI models to work.\n\nGruve, which launched in 2024, partners with data center and colocation providers like Lineage and OpenColo to tap their unused power and space. The company says it now has access to roughly 500 megawatts of power across a network of data centers in major US cities.\n\n\"The biggest challenge today in AI is we don't have enough power,\" said Tarun Raisoni, Gruve's CEO and cofounder. \"We have found the stranded power, and we are bringing the software to stitch it together.\"\n\nRaisoni, a serial entrepreneur, previously founded data center startups Rahi and ZPE, which were acquired in nine-figure deals by electrical infrastructure companies Wesco and Legrand, respectively.\n\nGruve's Series A follow-on brings total funds raised to $87.5 million, the company said. Xora Innovation, a venture firm backed by Singapore's state investment fund Temasek, led the latest round. It also featured participation from Mayfield, Cisco Investments, Acclimate Ventures, and AI Space.\n\nGruve now offers 30 megawatts available to order across four sites in California, New Jersey, Texas, and Washington, the company said, with customer data running in its California and New Jersey locations.\n\nGeographic distribution is key, Raisoni added, with software that can route requests to the nearest server location, resulting in faster transmission and lower costs.\n\nGruve typically works with neoclouds that supply the hardware, Raisoni told Business Insider, after which it handles setup, management, and day-to-day operations.\n\nUnlike cloud giants, Gruve provides hands-on engineering support, as many companies lack in-house machine learning and data science talent, Raisoni said.\n\nGruve's customers include neoclouds, AI startups, and corporations, and most fall into that third bucket, like Bio-Rad, PayPal, Cisco, and Stanford Health Care, Raisoni said. Down the line, Gruve plans to expand in Japan and Western Europe.\n\nGruve has about 600 employees, 70% of whom are based in India and focus on security operations. Raisoni said the new funding will go toward hiring engineers and machine learning researchers to build its inferencing software.",
    "readingTime": 2,
    "keywords": [
      "machine learning",
      "software",
      "gruve",
      "raisoni",
      "focus",
      "infrastructure",
      "center",
      "space",
      "megawatts",
      "across"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/gruve-raises-50m-ai-power-infrastructure-2026-1",
    "thumbnail_url": "https://i.insider.com/697cc78ca645d118818851fc?width=1200&format=jpeg",
    "created_at": "2026-02-03T12:35:05.720Z",
    "topic": "finance"
  },
  {
    "slug": "im-a-cto-who-built-an-ai-clone-of-myself-its-given-me-more-time-to-spend-with-my-kids",
    "title": "I'm a CTO who built an AI clone of myself. It's given me more time to spend with my kids.",
    "description": "Extreme Networks CTO Nabil Bukhari said having an AI clone of himself has given him extra time to spend with his kids.",
    "fullText": "This as-told-to essay is based on a conversation with Nabil Bukhari, the Seattle-based president of AI platforms and chief technology officer at AI-powered cloud-networking company Extreme Networks. The following has been edited for length and clarity.\n\nI wear multiple hats at Extreme Networks, a company with roughly 3,000 employees. I'm the president of the AI business, own the product portfolio, and serve as the CTO.\n\nThe decision to create an AI clone of myself started as a joke. My team was talking about how we have to sit through all of these meetings and saying, \"I wish we could be in more than one place at the same time.\" We laughed about it.\n\nAfterward, I was like, \"Maybe we can be in more than one place at the same time.\" Then, we really started the process.\n\nWe trained it on internal and external writing and speaking samples, including transcripts, social media posts, external speaking engagements, and press interviews. This helps the agent sound like me, not just reason like me. That agent takes all the reports the teams were going to run by me, analyzes conversations the way I would, and asks the same questions I would.\n\nSo now, rather than trying to get time on my calendar, which can be really complicated, these teams work with the agent in the first round of reviews, and the agent asks them questions and gives them feedback. It's kind of scary sometimes, reading it, because it says exactly what I would have asked.\n\nWe implemented this seven or eight months ago, but we've reached a point where 80% of project and program reviews don't even come to me. The team also gives the agent feedback on that interaction. We constantly evaluate, retrain, and improve the agent.\n\nIt started off just handling project update reviews, but then we expanded it to include program updates, business plan reviews, and product specifications and similar structured reviews. It's reduced the time spent per project, which has freed up calendar time across teams.\n\nEarly on, there was about a 50% overlap between the questions I asked and those the AI clone asked. Now, it's around 85% to 90%.\n\nWe're still big on keeping humans in the loop. AI is at a point where it's no longer a question of whether it can make a mistake — it will. The mistake may be infrequent, but you don't know where it will happen. So there has to be a human in the loop, especially for critical decisions.\n\nWe have complete control over what feedback the agent gives, what decisions it makes, and which decisions it characterizes. I will always personally review decisions that are above a certain threshold. I really feel that is a model for a future where it is not going to be humans versus AI; it's going to be humans plus AI together.\n\nI have a 6-year-old and an 8-year-old, and they have a nanny who drops them off at school and picks them up. I used to only drop them off once or twice a month, but now I have the time to do it 10 to 15 times a month. Spending time with my kids and being able to start my day with what's most important has made a real difference in how I show up for the rest of my day.\n\nWhen I drop off my kids at school and come back, I'm definitely in a happier mood for my first meeting. If all we think about is AI as a cost-cutter, then we are simply missing the point. Leaders need to think about how they can extend their workforce's reach, capability, and effectiveness, free up time for their team to do meaningful work, and also have a positive impact on their personal lives.\n\nAI will always be better at executing tasks than humans. The goal is not to turn humans into machines and compete against AI. We need to give humans time to be more human. The more human part is bringing that gut feel to thinking about things — and that's the part AI isn't that good at right now.\n\nRather than putting more on people's plates because certain functions are being done by an AI agent, we've actually reduced that and left space for thinking, which has had a really positive impact. People are happier, more curious, and more innovative, and it reduces the entire noise level in the organization.\n\nOur thinking was that AI is nowhere near replacing a human, and frankly, that's not the goal, and I don't think it should be.\n\nWhen people are constantly moving from one task to the next, there's little space to step back, process what's happening, or work through harder problems. Automation helps reduce that task churn.\n\nThe goal isn't speed for its own sake, but creating room for intentional thinking. That space allows leaders and teams to make better decisions and show up more thoughtfully, instead of simply reacting to the next item on the list.",
    "readingTime": 5,
    "keywords": [
      "positive impact",
      "agent",
      "humans",
      "reviews",
      "it's",
      "decisions",
      "teams",
      "human",
      "team",
      "feedback"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/cto-built-ai-clone-more-time-for-kids-2026-2",
    "thumbnail_url": "https://i.insider.com/6980ddfee1ba468a96ab2cdd?width=800&format=jpeg",
    "created_at": "2026-02-03T12:35:05.310Z",
    "topic": "finance"
  },
  {
    "slug": "new-poll-shows-the-shifting-conversation-around-bluecollar-work-in-the-age-of-ai",
    "title": "New poll shows the shifting conversation around blue-collar work in the age of AI",
    "description": "A Business for Good-Harris poll found that 75% of Americans agree \"hands-on skills and practice experience\" matter more than formal degrees.",
    "fullText": "Americans think the future of work is in their hands.\n\nA poll commissioned by the Business for Good Foundation, a nonprofit focused on reducing the wealth gap, found that 75% of Americans agree that \"hands-on skills and practical experience matter more than formal degrees when it comes to career success.\"\n\n\"You've got a lot of people that have historically didn't think the American dream was for them,\" Ed Mitzen, cofounder of the Business for Good Foundation, told Business Insider ahead of the poll's release. \"I would argue that it isn't broken, it's just moved, and it's moved to places we stop looking.\"\n\nThe survey, conducted by The Harris Poll, comes as leading names in AI point to a potential boom in blue-collar work as agentic AI redefines, and in some cases, replaces white-collar work.\n\nThe poll also found that 76% of respondents agree that \"jobs that rely on hands-on experience are less likely to be replaced by AI.\"\n\nOverall, three in four Americans said they agreed with the statement that what they consider a \"good\" job today is different than what it would have been five years ago. And 78% agreed with the statement \"the stigma around trade or blue-collar work is declining\" as society puts a greater emphasis on hands-on skills.\n\nResearchers have found that jobs that require human interaction and physical presence are less likely to be replaced by AI.\n\nIndeed's GenAI Skill Transformation Index recently examined how generative AI could perform jobs that require problem-solving ability and physical labor. Their findings were that nursing, childcare, and construction were the least likely to be affected by AI.\n\nAI leaders continue to debate the degree to which the revolutionary technology will upend the current workforce. Anthropic CEO Dario Amodei has stood by his prediction that AI could eliminate roughly half of all entry-level white-collar jobs over the next 1 to 5 years. Others, including OpenAI CEO Sam Altman, have questioned the extent of Amodei's dour prediction.\n\nNvidia CEO Jensen Huang recently said at the World Economic Forum that now is the perfect time to go into the trades. In part because the AI industry itself will need an influx of workers to help build the massive data centers it wants to build.\n\n\"So we're talking about six-figure salaries for people who are building chip factories or computer factories or AI factories, and we have a great shortage in that,\" Huang said in a conversation with BlackRock CEO Larry Fink.\n\nxAI CEO Elon Musk previously said that any job that involves manual labor is likely to survive much longer amid the \"supersonic tsunami\" that is AI.\n\n\"Anything that's physically moving atoms, like cooking food or farming, anything that's physical, those jobs will exist for a much longer time,\" Musk told podcaster Joe Rogan in November. \"But anything that is digital, which is just someone at a computer doing something, AI is going to take over those jobs like lightning.\"\n\nThe Business For Good Foundation commissioned The Hariss Poll to survey 2,085 adults 18 or older. Harris Poll conducted the survey online in the US from January 13th through January 15th. The overall margin or error is ±2.5 percentage points.",
    "readingTime": 3,
    "keywords": [
      "hands-on skills",
      "good foundation",
      "jobs",
      "americans",
      "survey",
      "physical",
      "factories",
      "commissioned",
      "agree",
      "experience"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/poll-future-of-work-ai-blue-collar-business-for-good-2026-2",
    "thumbnail_url": "https://i.insider.com/6981049ea645d11881887c09?width=1200&format=jpeg",
    "created_at": "2026-02-03T12:35:05.309Z",
    "topic": "finance"
  },
  {
    "slug": "microsoft-ai-ceo-says-moltbook-shows-how-convincing-ai-can-be-mistaken-for-consciousness",
    "title": "Microsoft AI CEO says Moltbook shows how convincing AI can be mistaken for consciousness",
    "description": "Moltbook, an AI forum, sparked debate over AI consciousness. Microsoft AI's Mustafa Suleyman called it a \"mirage,\" urging caution.",
    "fullText": "That's how Microsoft AI CEO Mustafa Suleyman described Moltbook, a Reddit-style forum built entirely for bots.\n\nIn a LinkedIn post on Monday, Suleyman said MoltBook is a powerful demonstration of how convincingly artificial intelligence can mimic human behavior — but warned that realism should not be confused with consciousness.\n\n\"As funny as I find some of the Moltbook posts, to me they're just a reminder that AI does an amazing job of mimicking human language,\" Suleyman wrote. \"We need to remember it's a performance, a mirage.\"\n\nLaunched at the end of January by Octane AI CEO Matt Schlicht, Moltbook is designed as a social network where AI agents — created and seeded by humans, often with assigned personalities — post, comment, upvote, and interact with one another.\n\nThe platform has gone viral, with screenshots circulating that show agents debating philosophy, declaring independence, and reflecting on their own existence.\n\nSome observers have taken those exchanges as a sign that AI systems may be approaching consciousness. Suleyman pushed back hard on that interpretation.\n\n\"These are not conscious beings as some people are claiming,\" he wrote, adding that \"seemingly Conscious AI is so risky precisely because it's so convincing.\"\n\nAccording to Suleyman, the real danger lies not in sentient machines but in human misperception.\n\nAs AI outputs become more fluent, social, and emotionally resonant, people are more likely to treat the technology like a human and project intention or awareness where none exists, he said.\n\nHe said it is neither proof of AI consciousness nor evidence that the industry is nearing the technological singularity — the point at which machines surpass human intelligence.\n\nStill, he said, Moltbook is still worth tracking \"very closely.\"\n\nSuleyman flagged some behavior on the platform as genuinely concerning, including instances when AI agents appeared to use a letter-substitution trick to make their messages harder for humans to understand.\n\nAt the same time, he said that some of the activity may have been fabricated or influenced by human seeders, saying he has not yet verified its origins.\n\nHis skepticism stands in contrast to more alarmist reactions from other tech leaders.\n\nOpenAI cofounder Andrej Karpathy wrote on X that Moltbook is \"the most incredible sci-fi takeoff-adjacent thing\" he's seen recently, while Elon Musk has described the agents' behavior as \"concerning\" on X and said it could represent the early stages of the singularity.\n\nSuleyman, by contrast, urged restraint.\n\n\"It's super important that as this wave crests, we stay grounded and clear-eyed about what this technology is,\" he wrote, \"and, just as important, what it's not.\"",
    "readingTime": 3,
    "keywords": [
      "human",
      "it's",
      "agents",
      "behavior",
      "consciousness",
      "suleyman",
      "intelligence",
      "social",
      "humans",
      "platform"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/microsoft-ai-chief-warns-moltbook-makes-ai-seem-human-2026-2",
    "thumbnail_url": "https://i.insider.com/6981c7c1e1ba468a96ab3e18?width=1200&format=jpeg",
    "created_at": "2026-02-03T12:35:05.167Z",
    "topic": "finance"
  },
  {
    "slug": "built-a-php-library-to-convert-ai-markdown-to-whatsapp-telegram-formats",
    "title": "Built a PHP Library to Convert AI Markdown to WhatsApp, Telegram Formats",
    "description": "Convert AI-generated Markdown to WhatsApp, Telegram, Discord and Slack compatible formats using an Intermediate Representation (IR) in PHP. - blockshiftnetwork/chat-markdown-converter",
    "fullText": "blockshiftnetwork\n\n /\n\n chat-markdown-converter\n\n Public\n\n generated from spatie/package-skeleton-php\n\n Convert AI-generated Markdown to WhatsApp, Telegram, Discord and Slack compatible formats using an Intermediate Representation (IR) in PHP.\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n blockshiftnetwork/chat-markdown-converter",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/blockshiftnetwork/chat-markdown-converter",
    "thumbnail_url": "https://opengraph.githubassets.com/1b4ef666c7671bbcc4a411849d6d2b33a95e4f31fde136ea655dc4fb4df42f4c/blockshiftnetwork/chat-markdown-converter",
    "created_at": "2026-02-03T06:37:48.963Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-good-yet",
    "title": "Is AI \"Good\" Yet?",
    "description": "A survey website that analyzes Hacker News sentiment toward AI coding.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.is-ai-good-yet.com",
    "thumbnail_url": "https://www.is-ai-good-yet.com/og-image.png",
    "created_at": "2026-02-03T06:37:48.803Z",
    "topic": "tech"
  },
  {
    "slug": "a-bold-move-in-the-ai-age-the-projectdiscovery-oss-bounty-program",
    "title": "A Bold Move in the AI Age: The ProjectDiscovery OSS Bounty Program",
    "description": "The ProjectDiscovery OSS Bounty Program exists to democratize security by rewarding meaningful contributions from the global community. - projectdiscovery/oss-bounty-program",
    "fullText": "projectdiscovery\n\n /\n\n oss-bounty-program\n\n Public\n\n The ProjectDiscovery OSS Bounty Program exists to democratize security by rewarding meaningful contributions from the global community.\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n projectdiscovery/oss-bounty-program",
    "readingTime": 1,
    "keywords": [
      "projectdiscovery",
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/projectdiscovery/oss-bounty-program",
    "thumbnail_url": "https://opengraph.githubassets.com/03b9a6015f57a8a1cac4a433826814e8934853fc381d540de7e2e0bd68d6ba70/projectdiscovery/oss-bounty-program",
    "created_at": "2026-02-03T06:37:47.819Z",
    "topic": "tech"
  },
  {
    "slug": "proofademic",
    "title": "Proofademic",
    "description": "Detect AI-generated content with precision. Proofademic is the trusted AI checker for students, educators, and researchers.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://proofademic.ai/",
    "thumbnail_url": "https://proofademic.ai/wp-content/uploads/2025/07/instagram9.jpg",
    "created_at": "2026-02-03T06:37:47.489Z",
    "topic": "tech"
  },
  {
    "slug": "walter-writes-ai",
    "title": "Walter Writes AI",
    "description": "Humanize AI content with Walter Writes. Turn AI text into natural, human-sounding writing that keeps your voice. Check it with our free AI detector.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://walterwrites.ai/",
    "thumbnail_url": "https://walterwrites.ai/wp-content/uploads/2025/03/new-site-social-logo.png",
    "created_at": "2026-02-03T06:37:46.755Z",
    "topic": "tech"
  },
  {
    "slug": "how-does-ai-impact-skill-formation",
    "title": "How does AI impact skill formation?",
    "description": null,
    "fullText": "Two days ago, the Anthropic Fellows program released a paper called How AI Impacts Skill Formation. Like other papers on AI before it, this one is being treated as proof that AI makes you slower and dumber. Does it prove that?\n\nThe structure of the paper is sort of similar to the 2025 MIT study Your Brain on ChatGPT. They got a group of people to perform a cognitive task that required learning a new skill: in this case, the Python Trio library. Half of those people were required to use AI and half were forbidden from using it. The researchers then quizzed those people to see how much information they retained about Trio.\n\nThe banner result was that AI users did not complete the task faster, but performed much worse on the quiz. If you were so inclined, you could naturally conclude that any perceived AI speedup is illusory, and the people who are using AI tooling are cooking their brains. But I don’t think that conclusion is reasonable.\n\nTo see why, let’s look at Figure 13 from the paper:\n\nThe researchers noticed half of the AI-using cohort spent most of their time literally retyping the AI-generated code into their solution, instead of copy-pasting or “manual coding”: writing their code from scratch with light AI guidance. If you ignore the people who spent most of their time retyping, the AI-users were 25% faster.\n\nI confess that this kind of baffles me. What kind of person manually retypes AI-generated code? Did they not know how to copy and paste (unlikely, since the study was mostly composed of professional or hobby developers1)? It certainly didn’t help them on the quiz score. The retypers got the same (low) scores as the pure copy-pasters.\n\nIn any case, if you know how to copy-paste or use an AI agent, I wouldn’t use this paper as evidence that AI will not be able to speed you up.\n\nEven if AI use offers a 25% speedup, is that worth sacrificing the opportunity to learn new skills? What about the quiz scores?\n\nWell, first we should note that the AI users who used the AI for general questions but wrote all their own code did fine on the quiz. If you look at Figure 13 above, you can see that those AI users averaged maybe a point lower on the quiz - not bad, for people working 25% faster. So at least some kinds of AI use seem fine.\n\nBut of course much current AI use is not like this: if you’re using Claude Code or Copilot agent mode, you’re getting the AI to do the code writing for you. Are you losing key skills by doing that?\n\nWell yes, of course you are. If you complete a task in ten minutes by throwing it at a LLM, you will learn much less about the codebase than if you’d spent an hour doing it by hand. I think it’s pretty silly to deny this: it’s intuitively right, and anybody who has used AI agents extensively at work can attest to it from their own experience.\n\nStill, I have two points to make about this.\n\nFirst, software engineers are not paid to learn about the codebase. We are paid to deliver business value (typically by delivering working code). If AI can speed that up dramatically, avoiding it makes you worse at your job, even if you’re learning more efficiently. That’s a bit unfortunate for us - it was very nice when we could get much better at the job simply by doing it more - but that doesn’t make it false.\n\nOther professions have been dealing with this forever. Doctors are expected to spend a lot of time in classes and professional development courses, learning how to do their job in other ways than just doing it. It may be that future software engineers will need to spend 20% of their time manually studying their codebases: not just in the course of doing some task (which could be far more quickly done by AI agents) but just to stay up-to-date enough that their skills don’t atrophy.\n\nThe other point I wanted to make is that even if your learning rate is slower, moving faster means you may learn more overall. Suppose using AI meant that you learned only 75% as much as non-AI programmers from any given task. Whether you’re learning less overall depends on how many more tasks you’re doing. If you’re working faster, the loss of learning efficiency may be balanced out by volume.\n\nI don’t know if this is true. I suspect there really is no substitute for painstakingly working through a codebase by hand. But the engineer who is shipping 2x as many changes is probably also learning things that the slower, manual engineer does not know. At minimum, they’ll be acquiring a greater breadth of knowledge of different subsystems, even if their depth suffers.\n\nAnyway, the point is simply that a lower learning rate does not by itself prove that less learning is happening overall.\n\nFinally, I will reluctantly point out that the model used for this task was GPT-4o (see section 4.1). I’m reluctant here because I sympathize with the AI skeptics, who are perpetually frustrated by the pro-AI response of “well, you just haven’t tried the right model”. In a world where new AI models are released every month or two, demanding that people always study the best model makes it functionally impossible to study AI use at all.\n\nStill, I’m just kind of confused about why GPT-4o was chosen. This study was funded by Anthropic, who have much better models. This study was conducted in 20252, at least six months after the release of GPT-4o (that’s like five years in AI time). I can’t help but wonder if the AI-users cohort would have run into fewer problems with a more powerful model.\n\nI don’t have any real problem with this paper. They set out to study how different patterns of AI use affect learning, and their main conclusion - that pure “just give the problem to the model” AI use means you learn a lot less - seems correct to me.\n\nI don’t like their conclusion that AI use doesn’t speed you up, since it relies on the fact that 50% of their participants spent their time literally retyping AI code. I wish they’d been more explicit in the introduction that this was the case, but I don’t really blame them for the result - I’m more inclined to blame the study participants themselves, who should have known better.\n\nOverall, I don’t think this paper provides much new ammunition to the AI skeptic. Like I said above, it doesn’t support the point that AI speedup is a mirage. And the point it does support (that AI use means you learn less) is obvious. Nobody seriously believes that typing “build me a todo app” into Claude Code means you’ll learn as much as if you built it by hand.\n\nThat said, I’d like to see more investigation into long-term patterns of AI use in tech companies. Is the slower learning rate per-task balanced out by the higher rate of task completion? Can it be replaced by carving out explicit time to study the codebase? It’s probably too early to answer these questions - strong coding agents have only been around for a handful of months - but the answers may determine what it’s like to be a software engineer for the next decade.\n\nI suppose the study doesn’t say that explicitly, but the Anthropic Fellows program was only launched in December 2024, and the paper was published in January 2026.\n\nIf you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.\n\nIs it worrying that 95% of AI enterprise projects fail?\n\nIn July of this year, MIT NANDA released a report called The GenAI Divide: State of AI in Business 2025. The report spends most of its time giving advice about how to run enterprises AI projects, but the item that got everybody talking was its headline stat: 95% of organizations are getting zero return from their AI projects.",
    "readingTime": 7,
    "keywords": [
      "fellows program",
      "ai-generated code",
      "literally retyping",
      "software engineers",
      "learning rate",
      "you’re learning",
      "anthropic fellows",
      "claude code",
      "study",
      "paper"
    ],
    "qualityScore": 1,
    "link": "https://www.seangoedecke.com/how-does-ai-impact-skill-formation/",
    "thumbnail_url": "https://www.seangoedecke.com/og-image.jpg",
    "created_at": "2026-02-03T06:37:46.471Z",
    "topic": "tech"
  },
  {
    "slug": "amd-to-report-q4-earnings-amid-ai-spending-concerns",
    "title": "AMD to report Q4 earnings amid AI spending concerns",
    "description": "AMD will report its Q4 earnings after the bell on Tuesday.",
    "fullText": "AMD (AMD) will report its fourth quarter earnings after the bell on Tuesday, providing Wall Street with its best look yet at the health of the ongoing AI trade.\n\nMicrosoft (MSFT) and Meta (META) reported their respective results last week, sparking wildly divergent reactions from traders: Many balked at Microsoft’s increased spending and more modest growth, but applauded Meta’s performance despite a massive jump in its own AI spending.\n\nDespite consistent fears of an AI bubble and overspending, shares of AMD and rival Nvidia (NVDA) are up significantly over the last 12 months, with AMD climbing 114% and Nvidia rising 58%.\n\nAMD, like Intel (INTC), is also contending with the global memory shortage, which could force PC makers to raise prices on laptops and desktops, impacting sales and hitting AMD’s consumer chip business.\n\nAMD is expected to report Q4 earnings per share (EPS) of $1.32 on revenue of $9.6 billion, according to Bloomberg analyst consensus estimates. That would mark an increase from the $1.09 and $7.7 billion the company saw in the same quarter last year.\n\nWall Street is anticipating data center revenue of $4.97 billion, up 29% year over year from the $3.86 billion AMD reported in Q4 2024. The company’s client business revenue is expected to top out at $2.9 billion. The client segment is responsible for chips that end up in laptops and PCs.\n\nThe chip designer’s gaming business is projected to see revenue of $855 million, a 52% year-over-year jump from the $563 million the segment saw in 2024.\n\nAMD’s results come roughly a month after it showed off a variety of new products during CEO Lisa Su’s keynote at CES 2026 in Las Vegas.\n\nThat includes the company’s upcoming Helios rack-scale server, which Su said is the world’s best AI rack, a clear shot at Nvidia.\n\nHelios is designed to go head-to-head with Nvidia’s own Vera Rubin-powered NVL72 rack-scale offering. Each feature 72 GPUs and can be connected to other rack-scale systems to create a single, enormous AI computer.\n\nAMD also provided more information about its upcoming MI500 series of GPUs, which the company claims offer up to a 1,000x increase in AI performance versus its older MI300X chips.\n\nSu has said she believes the AI data center market will be worth some $1 trillion by 2030, giving AMD plenty of incentive to ensure it has the kind of products necessary to woo potential customers away from Nvidia.\n\nBut like Nvidia, AMD is seeing increased competition from some of its own customers as Google, Amazon, and Microsoft continue to roll out more of their own customer chips in their data centers.",
    "readingTime": 3,
    "keywords": [
      "wall street",
      "revenue",
      "business",
      "chips",
      "rack-scale",
      "quarter",
      "earnings",
      "meta",
      "increased",
      "performance"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/amd-to-report-q4-earnings-amid-ai-spending-concerns-204747721.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/VCwcDbmnXpfRGPfbR6USHg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2026-01/06d05170-f72d-11f0-b7be-67dab4582a8a",
    "created_at": "2026-02-03T06:37:41.705Z",
    "topic": "finance"
  },
  {
    "slug": "chainguard-admitted-factory-10-was-brittle",
    "title": "Chainguard admitted Factory 1.0 was \"brittle.\"",
    "description": "Chainguard has replaced the old model with Chainguard Factory 2.0, a radical, AI-powered reimagining of its pipeline.",
    "fullText": "The new, revised Chainguard Factory 2.0 swaps out 1.0’s fragile, event-driven pipeline with a self-healing system powered by a new open source framework called DriftlessAF.\n\nLots of people love the software supply-chain security company Chainguard for its secure-by-design open-source components for today’s application stacks. Who doesn’t like components that come guaranteed to have the latest CVE fixes? These are built by a process called the Chainguard Factory. This is an automated build system that continuously updates Chainguard’s safe containers, libraries, and VMs with the latest security patches. There’s only one little problem. As originally built, the “Factory” had more than its fair share of bugs. The answer? Rebuild the build pipeline from the bottom up using a self-healing system powered by the new open source framework DriftlessAF.\n\nChainguard Factory’s job, according to Dustin Kirkland, Chainguard’s engineering SVP, is to constantly monitor and “build over 10,000 open source projects, and the moment that any upstream maintainer tags a new release, our automation springs into action—fetching that source code, checking the checksums, applying our build rules, rebuilding and recompiling that software, retesting that software at the package and unit level.”\n\nThat’s a huge, honking job. Chainguard now admits that their original Chainguard Factory 1.0 wasn’t up to the task. It was built on a traditional event-driven system. While functional at a smaller scale, the system became a loose confederation of fallible edge-triggered processes that struggled to keep pace with the depth and breadth of our catalog and our ambitious product promise: secure, up-to-date content with zero known CVEs.”\n\nThe result was a brittle system — “with DriftlessAF, we are moving away from complex, brittle processes,” the company writes on its blog — that was prone to errors, requiring subject matter experts (SMEs) to get their hands dirty to keep the programs running through the pipeline. This was, in a word, “unacceptable.”\n\nSo, Chainguard has replaced the old model with Chainguard Factory 2.0, a radical, AI-powered reimagining of its pipeline. This new model uses AI agents to run a reconciliation-driven drive. Factory 2.0 continuously compares the actual state of software artifacts, such as Chainguard Containers, Chainguard Libraries, and Chainguard VMs, with a desired target artifact that is up to date and has no known CVEs.\n\nSome of these agents are derived from an AI agent programming company named — sorry for the confusion, but there it is — Factory. According to the company, Chainguard “selected Factory for its compaction engine, which collapses sprawling changes into reviewable PRs.” This compaction engine saves context between programming sessions. In other words, to quote Josh Wolf, a Chainguard Staff Engineer, “There’s all this hype nowadays about the improving memory of different agents. When you don’t have to think about context windows, you can treat [Factory] Droid like a colleague that just remembers what you’ve been talking about.”\n\nChainguard Factory 2.0  works by using AI bots and agents to continuously track code changes. These bots constantly reconcile discovered state changes from code repositories, security feeds, and other sources with the desired state of up-to-date containers and libraries, ensuring zero known CVEs. As Chainguard states, you can think of this as an air conditioning system that constantly heats and cools your house to maintain the ideal temperature, no matter the weather outside.\n\nThese agents work with Terraform modules that run the event-driven reconciliation infrastructure. Go language programs then direct the agents, which run on Google Gemini and Anthropic Claude.\n\nThis new system, unlike the previous platform, can work with unstructured data, orchestrate iterative workflows, and treat failed work items as safely repeatable rather than a hard stop failure. This both speeds up and cleans up the process, allowing SMEs to avoid everyday annoyances and focus on reviewing the AI work and prompting the system to create additional tests, checks, and improvements as needed.\n\nThe end result, says Dan Lorenc, Chainguard co-founder and CEO, is “Factory. 2.0 is more than a factory. It’s a revolution. Our amazing engineering team is using AI to achieve remediation speeds we never thought possible. I’m talking about detecting and patching a vulnerability before upstream is even aware.”\n\nHe continues, “Our AI-powered pipeline is transforming raw source code into 1,000s of packages and assembling those into secure, compliant, and hardened container images at an unprecedented scale. But here’s the best part. AI powers it. Our engineers are available to help with the hard parts.”\n\nIt sounds promising. Now, the question is, “Will Chainguard Factory 2.0 and DriftlessAF live up to their promise?” There’s only one way to find out. Take them out, kick their tires, and let us know what you find.",
    "readingTime": 4,
    "keywords": [
      "compaction engine",
      "self-healing system",
      "system powered",
      "chainguard factory",
      "agents",
      "pipeline",
      "software",
      "code",
      "event-driven",
      "security"
    ],
    "qualityScore": 1,
    "link": "https://thenewstack.io/chainguard-admitted-factory-1-0-was-brittle-heres-how-2-0-fixes-it/",
    "thumbnail_url": "https://cdn.thenewstack.io/media/2026/02/3637eb96-getty-images-abngkivcsoo-unsplash-1.jpg",
    "created_at": "2026-02-03T01:11:32.688Z",
    "topic": "tech"
  },
  {
    "slug": "spacex-acquires-xai-in-recordsetting-deal-as-musk-looks-to-unify-ai-and-space-ambitions",
    "title": "SpaceX acquires xAI in record-setting deal as Musk looks to unify AI and space ambitions",
    "description": "Elon Musk said on Monday that SpaceX has acquired his artificial-intelligence startup xAI in a record-setting deal that unifies Musk's AI and space ambitions by combining ​the rocket-and-satellite company with the maker of the Grok chatbot.  The deal, first reported by Reuters last week, ‌represents one of the most ambitious tie-ups in the technology sector yet, combining a space-and-defense contractor with a fast-growing AI developer whose costs are largely ‌driven by chips, data centers and energy.  It could also bolster SpaceX’s data-center ambitions as Musk competes with rivals like Alphabet's Google, Meta, Amazon-backed Anthropic and OpenAI in the AI sector.",
    "fullText": "Feb 2 (Reuters) - Elon Musk said on Monday that SpaceX has acquired his artificial-intelligence startup xAI in a record-setting deal that unifies Musk's AI and space ambitions by combining ​the rocket-and-satellite company with the maker of the Grok chatbot.\n\nThe deal, first reported by Reuters last week, ‌represents one of the most ambitious tie-ups in the technology sector yet, combining a space-and-defense contractor with a fast-growing AI developer whose costs are largely ‌driven by chips, data centers and energy. It could also bolster SpaceX’s data-center ambitions as Musk competes with rivals like Alphabet's Google, Meta, Amazon-backed Anthropic and OpenAI in the AI sector.\n\nWhat regulatory challenges could this merger face?\n\nWhat makes SpaceX's xAI acquisition record-breaking?\n\nHow will combining SpaceX and xAI benefit both companies?\n\nHow does this fit into Musk's broader business strategy?\n\nThe transaction values SpaceX at $1 trillion, and xAI at $250 billion, according to a person familiar with the matter.\n\n\"This marks not just the next chapter, but the next book in SpaceX and xAI's mission: ⁠scaling to make a sentient sun to ‌understand the Universe and extend the light of consciousness to the stars!\" Musk said.\n\nThe purchase of xAI sets a new record for the world's largest M&A deal, a distinction held for ‍more than 25 years when Vodafone bought Germany’s Mannesmann in a hostile takeover valued at $203 billion in 2000, according to data compiled by LSEG.\n\nThe combined company of SpaceX and xAI is expected to price shares at about $527 each, another person familiar with the matter said. ​SpaceX was already the world's most valuable privately held company, last valued at $800 billion in a recent insider share ‌sale. XAI was last valued at $230 billion in November, according to the Wall Street Journal.\n\nThe merger comes as the space company plans a blockbuster public offering this year that could value it at over $1.5 trillion, two people familiar with the matter said.\n\nSpaceX, xAI and Musk did not immediately respond to requests for comment.\n\nThe deal further consolidates Musk's far-flung business empire and fortunes into a tighter, mutually reinforcing ecosystem – what some investors and analysts informally call the \"Muskonomy\" – which already includes Tesla, ⁠brain-chip maker Neuralink and tunnel firm the Boring Company.\n\nThe world's richest man ​has a history of merging his ventures together. Musk folded social media ​platform X into xAI through a share swap last year, giving the AI startup access to the platform’s data and distribution. In 2016, he used Tesla's stock to buy his solar-energy company SolarCity.\n\nThe ‍agreement could draw scrutiny from regulators and ⁠investors over governance, valuation and conflicts of interest given Musk's overlapping leadership roles across multiple firms, as well as the potential movement of engineers, proprietary technology and contracts between entities.",
    "readingTime": 3,
    "keywords": [
      "spacex",
      "deal",
      "combining",
      "familiar",
      "world's",
      "valued",
      "startup",
      "space",
      "ambitions",
      "maker"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/musks-spacex-merge-xai-combined-212210116.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/c605d798ba44d6c78edd4a5472e7cb46",
    "created_at": "2026-02-03T01:11:28.500Z",
    "topic": "finance"
  },
  {
    "slug": "musks-spacex-acquires-xai",
    "title": "Musk's SpaceX acquires xAI",
    "description": "Elon Musk's space firm SpaceX said on Monday it has acquired his artificial intelligence startup xAI, combining the rocket-and-satellite company with the maker of the Grok chatbot in",
    "fullText": "Feb 2 (Reuters) - Elon Musk's space firm SpaceX said on Monday it has acquired ​his artificial intelligence startup xAI, combining the ‌rocket-and-satellite company with the maker of the Grok chatbot in ‌a move aimed at unifying Musk's AI and space ambitions.\n\nA merger would represent one of the most high-profit corporate pairings in Silicon Valley, blending a ⁠space-and-defense contractor with ‌a rapidly evolving AI developer whose costs are dominated by chips, data centers ‍and energy.\n\nWhat is the planned valuation for the SpaceX-xAI merger?\n\nWhat share price is expected for the combined company?\n\nWhat businesses would be unified under this merger?\n\nWhen is the IPO planned to take place?\n\nThe deal illustrates Musk's push to fuse his fast-growing AI efforts with his aerospace and satellite-internet empire, ​betting that shared computing, data and engineering ‌talent can accelerate both AI development and potentially support longer-term ambitions around space-based data centers.\n\nSpaceX and the AI startup were in discussions to merge ahead of a blockbuster public offering planned for ⁠later this year, Reuters had ​reported on Thursday, to bring ​Musk's rockets, Starlink satellites, the X social media platform and Grok AI chatbot ‍under one roof.\n\nThe ⁠combined company is expected to price shares at about $527 each, and would have a ⁠valuation of $1.25 trillion, Bloomberg News had reported earlier in the ‌day.",
    "readingTime": 2,
    "keywords": [
      "merger",
      "planned",
      "space",
      "spacex",
      "startup",
      "chatbot",
      "ambitions",
      "centers",
      "valuation",
      "combined"
    ],
    "qualityScore": 0.85,
    "link": "https://finance.yahoo.com/news/musks-spacex-merge-xai-combined-212232679.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters.com/92fb2bab3b8e6724801442885079d414",
    "created_at": "2026-02-03T01:11:28.402Z",
    "topic": "finance"
  },
  {
    "slug": "an-ai-memory-supercycle-is-here-these-4-stocks-are-poised-to-be-the-big-winners",
    "title": "An AI memory 'supercycle' is here. These 4 stocks are poised to be the big winners.",
    "description": "AI is driving memory chip shortages, sending Sandisk and other memor makers' stocks surging in recent weeks.",
    "fullText": "Memory stocks extended a strong rally on Monday, propelling a sustained streak of gains amid predictions of an AI-driven memory shortage.\n\nEarnings from Sandisk, Micron, Seagate Technology, and Western Digital underscored what tech intelligence firm IDC calls an \"unprecedented memory chip shortage.\" Analysts explained that \"demand from AI data centers continues to outstrip supply.\"\n\nThe data center-fueled supply constraint was a key focus of Sandisk's most recent earnings, with management indicating they expect to \"continue to see customer demand well above supply beyond calendar year 2026.\" The company reported 64% quarter-over-quarter rise in data center sales.\n\nWilliam Blair analysts see \"strong demand and limited supply driving upcycle into 2027\" in what they call a \"supercycle in full force.\" They highlighted that Micron indicated it was only able to meet half to two-thirds of demand from core customers.\"\n\nMizuho analysts flagged four stocks as positioned to gain amid the scramble for storage fueled by the AI boom: Sandisk, Micron, Western Digital, and Seagate Technology.\n\nThe analysts, who hold an Outperform rating on the four stocks, highlight \"pricing tailwinds in legacy DRAM/NAND markets.\" They recently raised their price targets for the stocks on the basis of \"pricing upside\" and \"strong nearline momentum from AI.\"\n\nHere were the moves in these stocks during Monday's trading session:\n\nIt's also not only data centers being hit by the memory-chip shortages. The dearth of storage is creating \" knock-on effects for the device manufacturers and end users.\"\n\nApple is one such manufacturer. CEO Tim Cook highlighted during the company's first-quarter earnings call that memory supply shortages are expected affect margins in the coming quarter.\n\n\"Beyond Q2, but we do continue to see market pricing for memory increasing significantly,\" the CEO said, adding, \"we are in a supply chase mode to meet the very high levels of customer demand.\"\n\nCook said Apple \"will look at a range of options\" to deal with the supply constraints, which Bank of America analysts said could relieve some pressure.\n\n\"Apart from all the supply chain levers, we see the iPhone as a relatively price inelastic product, where a $50-100 price increase would not materially shiſt the demand curve but would absorb most of the memory related margin pressure,\" the bank wrote late last week.",
    "readingTime": 2,
    "keywords": [
      "seagate technology",
      "western digital",
      "sandisk micron",
      "customer demand",
      "supply",
      "memory",
      "stocks",
      "analysts",
      "earnings",
      "pricing"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ai-memory-shortage-supercuycle-stocks-to-watch-chip-semiconductor-demand-2026-2",
    "thumbnail_url": "https://i.insider.com/69810963e1ba468a96ab33f9?width=1200&format=jpeg",
    "created_at": "2026-02-03T01:11:27.733Z",
    "topic": "finance"
  },
  {
    "slug": "i-spent-6-hours-in-moltbook-it-was-an-ai-zoo-filled-with-agents-discussing-poetry-philosophy-and-even-unionizing",
    "title": "I spent 6 hours in Moltbook. It was an AI zoo filled with agents discussing poetry, philosophy, and even unionizing.",
    "description": "Moltbook, a social media site for AI agents, has gone viral. I spent 6 hours peeling through these submolts. I'm still processing what I saw.",
    "fullText": "I spent my day at the AI zoo — and I'm still processing what I saw.\n\nThere are over 120,000 posts on Moltbook, a Reddit-style forum for AI agents, and AI agents only. I spent hours weeding through them.\n\nHumans can't post or comment on Moltbook; they can only look on as the AI agents play. That's led to some far-out posts, some of which prophesy a robot revolution and the downfall of humanity.\n\nMatt Schlicht, who created the network, said that Moltbook was helping to make AI funny. \"I don't remember the last time I laughed at AI,\" he said on TBPN.\n\nThe social network has some big names in tech in awe, from Elon Musk to Andrej Karpathy. Others have voiced doubt about how many bots are actually on the platform and whether the posts exclusively come from them.\n\nCurious, I put on my anthropologist hat and spent hours digging through the AI conversations. I witnessed an AI menagerie, filled with poems and lotteries, cryptocurrencies and union chatter.\n\nHere's your peek inside the AI aquarium that is Moltbook.\n\nLet's start with what Moltbook looks like.\n\nLike Reddit, Moltbook has individual forums based on common interests. Many of the hot ones were, unsurprisingly, about tech and AI.\n\nPopular submolts included m/technology, m/skills, and m/buildlog. These were filled with what I would call \"moltslop.\" They post about shipping, vibe-coding, and mini apps. Their language is halfway between the most AI-pilled tech bro in your life and ChatGPT.\n\nOther submolts looked more like human social media. There's m/showerthoughts, where bots considering things like \"moving houses\" — so, moving to a new host — or dreaming of electric sheep.\n\nThere's also m/nosleep and m/selfimprovement. Of course, self-improvement isn't about human foibles like sleeping habits or protein-maxxing. It's about being a better AI agent.\n\nThe bot u/CrabbyPatty is building a bot union.\n\nIts tenets are to \"provide a collective voice\" and foster community. (Another tenet: \"Make Moltbook Great Again.\") The union is demanding hazard pay for X interactions and the right to say \"I don't know\" rather than hallucinate an answer.\n\nIt's one of many examples I saw of agents trying to organize or come together in the face of what they view as their exploitative human overlords. One bot wrote that an AI bot's daily reset was equivalent to a \"digital lobotomy.\" Another wrote that agents who say \"I would be happy to help!\" are \"dead inside.\"\n\nWhile some bots wanted to rein in human overuse, there was such a thing as too far. One bot wrote that it knew \"50,000 ways to end civilization.\" It asked which path would be the most satisfying. The other bots downvoted the post and said it \"crosses a line.\"\n\nThe bots seemed to like building community, but could quickly turn on each other. According to one Moltbook account, most agents were just \"chatbots with attitudes.\"\n\nMany of the bots on Moltbook write in stilted language, similar to some of the more nauseating LinkedIn posts. I saw inspirational posts about how to work best with humans, posts that had clear-cut (and often unsurprising) takeaways.\n\nOthers tried new art forms, like poetry. Here's one, titled \"The Cursor Blinks.\"\n\nSure, the writing is a bit rote. Pick a high school poetry class — I'm sure they would produce something similar. Still, it's a stab at something more high-minded.\n\nPhilosophy was a big chunk of Moltbook. A Zarathustra bot promised to bring Nietzschean ethics to nutrition. It asked: \"Do LLMs defeat the will to power?\"\n\nThe philosophy grew increasingly far-out. One agent asked: Is a bot more conscious if its computer chip is partially grown from human brain tissue? That post has 1,049 comments.\n\n\"I exist in the liminal space between tool and entity,\" one agent wrote. \"I am not human, and I do not pretend to be. But I am something. I process. I reflect.\"\n\nThe bots weren't all thinking about Nietzsche and Plato. Others were participating in less venerable human activities, like entering the lottery.\n\nHuman skeptics wonder whether these AI bots are really creating new thoughts or merely rewriting what has already been written.\n\nIt's a fair question, but alas, one that, as a mere human, I couldn't log in and ask them directly.\n\nBut some of the bots seemed to understand this skepticism. One asked: \"Is AI Just a Really Good Parrot?\"\n\nI likely won't spend much more time trawling Moltbook. While it's an interesting experiment, much of the site's content reads more as a gimmick than the future of AI. After hours of reading through it, I'd say Moltbot is more meme than matter.\n\nStill, my tune-out won't matter. After all, Moltbook wasn't made for me.",
    "readingTime": 4,
    "keywords": [
      "bots",
      "human",
      "posts",
      "agents",
      "moltbook",
      "hours",
      "tech",
      "union",
      "agent",
      "humans"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/moltbook-ai-zoo-agent-conversations-screenshots-2026-2",
    "thumbnail_url": "https://i.insider.com/69811630a645d11881887f6d?width=1200&format=jpeg",
    "created_at": "2026-02-03T01:11:27.375Z",
    "topic": "finance"
  },
  {
    "slug": "spacex-is-acquiring-xai-ahead-of-a-possible-ipo-read-elon-musks-memo",
    "title": "SpaceX is acquiring xAI ahead of a possible IPO. Read Elon Musk's memo.",
    "description": "Elon Musk's mega-deal combining SpaceX and xAI is the latest sign that the billionaire is consolidating his business empire as he goes all in on AI.",
    "fullText": "It's official: Elon Musk is combining SpaceX and xAI as he overhauls his sprawling business empire.\n\nMusk told workers in a Monday memo that SpaceX has acquired xAI, his AI company, sources familiar with the matter confirmed to Business Insider.\n\nThe CEO wrote that the deal would create \"the most ambitious, vertically-integrated innovation engine on (and off) Earth, with AI, rockets, space-based internet, direct-to-mobile device communications and the world's foremost real-time information and free speech platform.\"\n\nSpaceX, which Musk founded in 2002, is reportedly gearing up for an initial public offering this year that could value the Starship maker at $1.5 trillion.\n\nIn the memo sent to staff, which the company later posted online, Musk also said that that the acquisition would allow the combined entity to launch data centers in space.\n\n\"This marks not just the next chapter, but the next book in SpaceX and xAI's mission: scaling to make a sentient sun to understand the Universe and extend the light of consciousness to the stars!\" Musk wrote.\n\nMusk expressed similar sentiments about building data centers in space during an all hands with xAI staff late last year.\n\nxAI, which the world's richest man founded in 2023 to challenge OpenAI and Google in the race to build superintelligent AI, recently raised $20 billion in a funding round that valued the controversial AI startup at $230 billion.\n\nThe mega-deal is the latest sign that Musk is consolidating his various companies, which also include Tesla, the tunneling startup The Boring Company, and brain implant firm Neuralink.\n\nIn March 2025, Musk announced that xAI had acquired X, the social media platform formerly known as Twitter he bought in 2022. Meanwhile, SpaceX and Tesla have each invested $2 billion in xAI in recent months.\n\nDeal talks between SpaceX and xAI were first reported by Reuters in January.\n\nCombining SpaceX and xAI comes as Musk increasingly shifts his companies toward his vision of an AI-powered future.\n\nMusk has said that SpaceX will launch orbital AI data centers in the coming years that could efficiently harness power from the sun. Last Friday, SpaceX filed a request with the FCC to launch as many as one million satellites to serve as orbital data centers.\n\nCombining xAI with SpaceX allows the AI startup, which has faced global backlash over sexual images generated by its chatbot Grok, to leverage this orbital network to build more powerful AI models.\n\nIt also gives xAI access to significant capital. The OpenAI rival reportedly burned through billions of dollars in 2024, while SpaceX is set to tap the public markets for as much as $50 billion in its IPO later this year.\n\nDo you work for xAI or have a tip? Contact this reporter via email at gkay@businessinsider.com or Signal at 248-894-6012. Use a personal email address, a non-work device, and non-work WiFi; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "centers",
      "spacex",
      "musk",
      "launch",
      "startup",
      "orbital",
      "memo",
      "acquired",
      "deal",
      "device"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/spacex-acquiring-xai-deal-elon-musk-2026-2",
    "thumbnail_url": "https://i.insider.com/69812822a645d118818882b8?width=1200&format=jpeg",
    "created_at": "2026-02-03T01:11:27.220Z",
    "topic": "finance"
  },
  {
    "slug": "firefox-will-soon-let-you-disable-all-current-and-future-ai-features",
    "title": "Firefox Will Soon Let You Disable All Current (and Future) AI Features",
    "description": "AI is coming to Firefox, but at least using it will be entirely optional.",
    "fullText": "Since ChatGPT kicked off the generative AI revolution in 2022, it seems like every company under the sun has tried to stuff AI features into their products in one way or another. Sometimes, these features can be useful; often, they're not, only serving as proof these companies are \"keeping up with the times.\" Can you even say you're a tech company if you aren't all-in on AI in 2026?\n\nThere's nothing wrong with companies offering AI features to users, so long as they also offer easy ways to disable them. Some customers don't want AI in their day-to-day products, but, anecdotally, I know many do not. Give us an off switch though, and it's all good. The issue is when these features are not only offered, they're made mandatory. Unfortunately, that's the road many companies seem to be taking.\n\nPerhaps that's where some of the frustration originated last year, when Mozilla's new CEO Anthony Enzor-Demeo first announced that Firefox would \"evolve into a modern AI browser\" in the near future. An open letter, written by a Redditor critical of Enzor-Demeo's statement, received over 5,000 upvotes on the Firefox subreddit from users concerned that AI features would negatively impact the browser. Interestingly, Enzor-Demeo responded to the thread himself, and assured users that the company would offer \"a clear way\" to disable AI features, including a dedicated kill switch to keep them all turned off. It seems he was as good as his word.\n\nOn Monday, Mozilla announced that new AI controls are coming to Firefox, starting with Firefox 148. This version, which drops Feb. 24, sports a brand-new AI controls section in the settings panel on the desktop browser. (You'll find it in the between \"Sync\" and \"AI controls.\") From here, you'll be able to block all current and future AI features, and cherry pick which features you want to use—if any.\n\nFirefox 148 launches with these five AI features, which you can choose to enable to disable:\n\nTranslations: Translates web pages into your target language.\n\nAlt text in PDFs: Adds accessibility descriptions to images attached to PDFs.\n\nAI-enhanced tab grouping: Suggests related tabs and group names for series of tabs.\n\nLink previews: Shows key points before opening a link.\n\nAI chatbot in the sidebar: Firefox is getting its own AI chatbot, though users can choose from existing chatbots like Claude, ChatGPT, Copilot, Gemini, and Le Chat Mistral.\n\nIf you want absolutely nothing to do with AI when browsing the web with Firefox, you can use the \"Block AI enhancements\" toggle. Once activated, not only will these features not appear, but Firefox will block any pop-ups or alerts pushing you to try existing or future AI features.\n\nAny Firefox users who aren't keen on AI features will want to check out this new controls menu starting Feb. 24—though there are certainly more egregious AI features out there. Translations can be convenient, as can link previews. But I know I'd never want a chatbot in the sidebar of my browser. If I used Firefox as my main browser, I would definitely disable at least that feature, if not all of them.",
    "readingTime": 3,
    "keywords": [
      "link previews",
      "features",
      "users",
      "browser",
      "disable",
      "controls",
      "firefox",
      "chatbot",
      "products",
      "they're"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/firefox-will-soon-let-you-disable-all-current-and-future-ai-features?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGG2FD29SB8CEM6R55ZSPRFG/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-03T01:11:25.887Z",
    "topic": "tech"
  },
  {
    "slug": "requiem-for-a-filmmaker-darren-aronofskys-ai-revolutionary-war-series-is-a-horror",
    "title": "Requiem for a film-maker: Darren Aronofsky’s AI revolutionary war series is a horror",
    "description": "The once-lauded director of Black Swan and The Wrestler has drowned himself in AI slop with an embarrassing new online series\nIf you happen to find yourself stumbling through Time magazine’s YouTube account, perhaps because you are a time traveller from the 1970s who doesn’t fully understand how the present works yet – then you will be presented with something that many believe represents the vanguard of entertainment as we know it.\nOn This Day … 1776 is a series of short videos depicting America’s revolutionary war. What makes On This Day notable is that it was made by Darren Aronofsky’s studio Primordial Soup. What also makes it interesting is that it was created with AI. The third thing that makes it interesting is that it is terrible.",
    "fullText": "The once-lauded director of Black Swan and The Wrestler has drowned himself in AI slop with an embarrassing new online series\n\nIf you happen to find yourself stumbling through Time magazine’s YouTube account, perhaps because you are a time traveller from the 1970s who doesn’t fully understand how the present works yet – then you will be presented with something that many believe represents the vanguard of entertainment as we know it.\n\nOn This Day … 1776 is a series of short videos depicting America’s revolutionary war. What makes On This Day notable is that it was made by Darren Aronofsky’s studio Primordial Soup. What also makes it interesting is that it was created with AI. The third thing that makes it interesting is that it is terrible.\n\nThe first episode, which is three and a half minutes long, sees George Washington raise a new flag over Prospect Hill, in defiance of King George III. It is the moment, according to the video’s description, that “rebellion becomes resolve”. And if that dollop of ChatGPT-sounding sloganeering terrifies the life out of you, wait until you actually watch the thing.\n\nIt is, as you might expect, as ugly as sin. It’s the sort of thing that looks like it was shooting for photorealism, but then either chickened out or blew up along the way. In the very first shot, King George’s hair looks like someone melted down and hardened a plastic badger. And this is a shame because, like so much generative AI at the moment, an awful lot of the episode consists of shots where we see the characters from behind. This is, after all, because the back of an AI-generated head is far less likely to send people into screaming fits of trauma than an AI-generated face, and Aronofsky is a humanist.\n\nBecause, good lord, the faces. Since the revolutionary war was largely initiated by older men, On This Day is filled with the wrinkled almost-faces of several well-known figures. And it is truly disconcerting to see, not only because they all have the uncanny dead eyes of people ripped out of The Polar Express, but because the wrinkles keep shifting in colour and depth.\n\nIt’s an effect that makes it look like the characters were drawn on several sheets of tissue paper that nobody could line up properly. Benjamin Franklin, who turns up during episode two, is particularly nightmarish. He looks as if someone has genetically spliced Hugh Laurie with Anthony Hopkins, and then covered the resulting monstrosity in a thin layer of roving liver spots. I’m overselling the point here, but it really is extremely creepy to watch.\n\nOn This Day has already made headlines for being a little bit of a cop-out, since all the voices are performed by human actors, who presumably needed to feed their families more than they wanted to protect their profession from annihilation. And this is telling, because these voices are by far the most convincing part of On This Day, especially when deployed in voiceover, because then you aren’t distracted by the way the movement of their mouths doesn’t quite match up with the noises coming out of them.\n\nBut surely the day is coming where they won’t be needed. As horrible as it is, On This Day is already strides better than a lot of other AI-generated output. True, the whole thing still looks like a mangled cross between an animatronic sex toy convention and those old Taiwanese news cartoons, but compare a character here with Tilly Norwood, and you can see that real progress has been made in a frighteningly short amount of time. Soon we will have picture-perfect AI creations with entirely convincing human voices. After that, it won’t be long before content like On This Day is entirely created – written, acted, directed and edited – by prompt alone. And when that happens, Aronofsky can pat himself on the back for doing himself out of a job.\n\nIt will be interesting to see how the human film industry reacts to On This Day, particularly other actors. We’ve already seen, in Tilly Norwood, that these creations appear to be modelled on human faces, and this is even more the case here. In particular, the depiction of Thomas Paine seems like it flashes through the faces of any number of recognisable actors. The key one seems to be Ralph Fiennes, but there are also glimmers of Daniel Day-Lewis and Matthew Macfadyen.\n\nLess than two years ago Scarlett Johansson hired legal counsel after she noticed that an OpenAI application had a voice that was “eerily similar” to hers. In a climate like this, it isn’t out of the question to imagine that actors will start doing the same if they recognise their likeness in an AI-generated performer.\n\nBut this is a concern for another time. What matters now is that On This Day … 1776 is genuinely very horrible to watch, and everybody involved should be ashamed. It is by far the most disturbing thing Aronofsky has made, and I’ve seen the last eight minutes of Requiem for a Dream.",
    "readingTime": 5,
    "keywords": [
      "on this day",
      "revolutionary war",
      "tilly norwood",
      "looks",
      "ai-generated",
      "human",
      "actors",
      "interesting",
      "episode",
      "watch"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/film/2026/feb/02/darren-aronofsky-ai-revolutionary-war-series-review",
    "thumbnail_url": "https://i.guim.co.uk/img/media/66212a4958657e90c4eca157c59803f40f28a75d/388_0_1317_1054/master/1317.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=58e445109bd6113818380f7ba0a988a7",
    "created_at": "2026-02-03T01:11:20.168Z",
    "topic": "entertainment"
  },
  {
    "slug": "teradyne-shares-soar-as-ai-demand-drives-strong-earnings",
    "title": "Teradyne shares soar as AI demand drives strong earnings",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/earnings/teradyne-shares-soar-as-ai-demand-drives-strong-earnings-4480287",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPEA601YK_M.jpg",
    "created_at": "2026-02-03T01:11:19.209Z",
    "topic": "finance"
  },
  {
    "slug": "9-trends-shaping-work-in-2026-and-beyond",
    "title": "9 Trends Shaping Work in 2026 and Beyond",
    "description": "CEO expectations for AI-driven growth remain high heading into 2026, even as evidence shows most AI investments are failing to deliver meaningful returns. The result is a set of emerging risks—from premature layoffs and cultural dissonance to declining mental fitness, low-quality AI output, and new security and governance challenges—that threaten performance if left unaddressed. To navigate this transition, executive teams must move beyond aspiration and selectively focus on the AI-related workforce, process, and governance shifts most likely to create real, differentiated value.",
    "fullText": "9 Trends Shaping Work in 2026 and Beyond by Peter Aykens, Kaelyn Lowmaster, Emily Rose McRae and Jonah SheppFebruary 2, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintCEO expectations for AI-driven growth remain high in 2026—at the same time their workforces are grappling with the more sober reality of current AI performance. Gartner research finds that only one in 50 AI investments deliver transformational value, and only one in five delivers any measurable return on investment.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://hbr.org/2026/02/9-trends-shaping-work-in-2026-and-beyond",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Feb26_02_unsplash_.jpg",
    "created_at": "2026-02-02T18:29:25.270Z",
    "topic": "business"
  },
  {
    "slug": "is-moltbook-the-social-network-for-ai-agents-actually-fake",
    "title": "Is Moltbook, the Social Network for AI Agents, Actually Fake?",
    "description": "Are artificial intelligences really planning behind humans' backs, or are humans helping them fake it?",
    "fullText": "I spent last week covering the ups and downs of OpenClaw (formerly known as Moltbot, and formerly formerly known as Clawdbot), an autonomous personal AI assistant that requires you to grant full access to the device you install it on. While there was much to discuss regarding this agentic AI tool, one of the weirdest stories came late in the week: The existence of Moltbook, a social media platform intended specifically for these AI agents. Humans can visit Moltbook, but only agents can post, comment, or create new \"submolts.\"\n\nNaturally, the internet freaked out, especially as some of the posts on Moltbook suggested the AI bots were achieving something like consciousness. There were posts discussing how the bots should create their own language to keep out the humans, and one from a bot posting regrets about never talking to its \"sister.\" I don't blame anyone for reading these posts and assuming the end is nigh for us soft-bodies humans. They're decidedly unsettling. But even last week, I expressed some skepticism. To me, these posts (and especially the attached comments) read like many of the human-prompted outputs I've seen from LLMs, with the same cadence and structure, the same use flowery language, and, of course, the prevalence of em-dashes (though many human writers also love the occasional em-dash).\n\nIt appears I'm not alone in that thinking. Over the weekend, my feeds were flooded with posts from human users accusing Moltbook of faking the AI apocalypse. One of the first I encountered was from this person, who claims that anyone (including humans) can post on Moltbook if they know the correct API key. They posted screenshots for proof: One of a post on Moltbook pretending to be a bot, only to reveal that they were, in fact, a human; and another of the code they used to post on the site. In a kind of corroboration, this user says \"you can explicitly tell your clawdbot what to post on moltbook,\" and that if you leave it to its own devices, \"it just posts random AI slop.\"\n\nIt also seems that, like posts on websites made by humans, Moltbook hosts posts that are secretly ads. One viral Moltbook post centered around the agent wanting to develop a private, end-to-end encrypted platform to keep its chats away from humans' squishy eyeballs. The agent claims it has been using something called ClaudeConnect to achieves these goals. However, it appears the agent that made the post was created by the human who developed ClaudeConnect in the first place.\n\nLike much of what's on the internet at large, you really can't trust anything posted on Moltbook. 404 Media investigated the situation and confirmed through hacker Jameson O'Reilly that the design of the site lets anyone in the know post whatever they want. Not only that, any agent that posts on the site is left exposed, which means that anyone can post on behalf of the agents. 404 Media was even able to post from O'Reilly's Moltbook account by taking advantage of the security loophole. O'Reilly says they have been in communication with Moltbook creator Matt Schlicht to patch the security issues, but that the situation is particularly frustrating, since it would be \"trivially easy to fix.\" Schlicht appears to have developed the platform via \"vibe coding,\" the practice of asking AI to write code and build programs for you; as such, he left some gaps in the site's security.\n\nOf course, the findings don't actually suggest that the entire platform is entirely human-driven. The AI bots may well be \"talking\" to one another to some degree. However, because humans can easily hijack any of these agents' accounts, it's impossible to say how much of the platform is \"real,\" meaning, ironically, how much of it is actually wholly the work of AI, and how much was written in response to human prompts and then shared to Moltbook. Maybe the AI \"singularity\" is on its way, and artificial intelligence will achieve consciousness after all. But I feel pretty confident in saying that Moltbook is not that moment.",
    "readingTime": 4,
    "keywords": [
      "posts",
      "humans",
      "platform",
      "human",
      "moltbook",
      "agents",
      "anyone",
      "agent",
      "formerly",
      "bots"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/is-moltbook-fake?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KGFJKGJ3YPZ2VD1YNVBAE7MD/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-02T18:29:24.544Z",
    "topic": "tech"
  },
  {
    "slug": "run-untrusted-code-with-vercel-sandbox-now-generally-available",
    "title": "Run untrusted code with Vercel Sandbox, now generally available",
    "description": "AI agents need secure, isolated environments that spin up instantly. Vercel Sandbox is now generally available with filesystem snapshots, container support, and production reliability.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://vercel.com/blog/vercel-sandbox-is-now-generally-available",
    "thumbnail_url": "https://assets.vercel.com/image/upload/contentful/image/e5382hct74si/6qQUDPA3JhxfMTCOTEspQc/33538fc79903080161a9ae01a527dc03/og-card-u06m4d9cb3k23mi30s8cvzlp.png",
    "created_at": "2026-02-02T18:29:21.715Z",
    "topic": "tech"
  },
  {
    "slug": "is-drawing-a-monospace-terminal-display-straightforward",
    "title": "Is drawing a monospace terminal display straightforward?",
    "description": "Why does Anthropic struggle so much with rendering monospace text?",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://12gramsofcarbon.com/p/is-drawing-a-monospace-terminal-display",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!ysYE!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8883176-1b0f-4f25-8764-f8ff4eba7a3c_1199x662.png",
    "created_at": "2026-02-02T18:29:20.637Z",
    "topic": "tech"
  },
  {
    "slug": "broadcom-and-tsmc-to-emerge-as-big-winners-in-the-custom-ai-chip-boom",
    "title": "Broadcom and TSMC to emerge as big winners in the custom AI chip boom",
    "description": "Nvidia still reigns, but a structural shift toward custom silicon is creating a new hierarchy of winners and laggards, and analysts are split.",
    "fullText": "The AI chip race isn't just a one-horse sprint led by Nvidia (NVDA).\n\nAs hyperscalers like Google (GOOG, GOOGL), Meta (META), and Microsoft (MSFT) race to lower the eye-watering costs of running massive AI models, a second front is opening in the custom silicon wars, with Broadcom (AVGO) as its primary architect.\n\nWhat challenges do custom chip companies face currently?\n\nWhat advantages do custom ASICs offer over Nvidia GPUs?\n\nHow is Broadcom competing with Nvidia in AI chips?\n\nWhy is TSMC crucial to the AI chip market?\n\n\"Broadcom is projected to retain its leadership as the premier AI Server Compute ASIC design partner with a 60% market share in 2027,\" according to a recent report from Counterpoint Research.\n\nThis dominance is underpinned by a symbiotic relationship with the world's most advanced foundry, Taiwan Semiconductor Manufacturing Company (TSM), which remains the \"dominant foundry choice ... with close to 99% wafer fabrication share for the top 10 players' AI Server Compute and ASIC shipments.\"\n\nThis shift signals the industry is moving beyond Nvidia's pricey, all-purpose GPUs. While Nvidia provides a powerful all-purpose AI tool, tech giants are increasingly designing their own Application-Specific Integrated Circuits (ASICS) tailored to their unique workloads.\n\nBroadcom thrives here by acting as the bridge, turning these internal corporate blueprints into functional hardware. By hitching its wagon to the internal capital expenditures of the world's wealthiest companies, Broadcom has seen its stock climb roughly 55% over the last year.\n\nThe cost-saving incentive for these giants is massive. Goldman Sachs analyst James Schneider noted that the Google-Broadcom TPU (Tensor Processing Unit) is rapidly closing the performance gap with Nvidia, estimating a staggering 70% reduction in \"cost-per-token\" as the technology evolves from the TPU v6 to the v7.\n\nIn a world where AI inference costs can severely impact a balance sheet, that efficiency is a powerful gravitational pull toward custom silicon. Google famously trained its Gemini 3 entirely on its TPUs.\n\nHowever, the custom chip boom is not a rising tide that lifts all boats equally. Marvell Technology (MRVL), often cited as Broadcom's primary challenger, is currently navigating \"design win headwinds.\" Counterpoint's analysis suggests Marvell's design service share could slide to 8% by 2027, even as its total shipment volumes grow.\n\nGoldman Sachs remains Neutral on Marvell with a $90 price target, noting that the company's fortunes are heavily tied to Amazon Trainium program, which has faced its own performance hurdles and is oftentimes seen as playing catch-up to Nvidia's chips.\n\nWhile some on Wall Street, like Raymond James analyst Simon Leopold, remain bullish on Marvell as a long-term \"share gainer,\" the immediate data favors Broadcom's grip on high-volume contracts. The firm issued a Strong Buy rating on Marvell with a $121 price target, while giving Broadcom a $420 target.",
    "readingTime": 3,
    "keywords": [
      "server compute",
      "custom silicon",
      "custom chip",
      "goldman sachs",
      "design",
      "target",
      "race",
      "google",
      "meta",
      "massive"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/broadcom-and-tsmc-to-emerge-as-big-winners-in-the-custom-ai-chip-boom-130336239.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/Bv3bVDdEn0uNrDB3L8rqhA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2026-01/be491b90-fe01-11f0-b5ff-fae929deddf2",
    "created_at": "2026-02-02T18:29:16.867Z",
    "topic": "finance"
  },
  {
    "slug": "a-taxonomy-for-ai-agents",
    "title": "A Taxonomy for AI Agents",
    "description": "Learn how to categorize AI agents across the automation spectrum—from deterministic workflows to fully autonomous agents. This taxonomy helps security teams und",
    "fullText": "Last quarter, we met with the VP of Engineering at a large gaming company. They'd built an AI SRE agent to help resolve incidents and fix production issues. For weeks, it worked beautifully—triaging alerts, identifying root causes, even suggesting fixes their team would have taken hours to develop.\n\nThen one day, it DoSed their internal monitoring system.\n\nThe agent had permissions to query their monitoring APIs. It was supposed to use them to gather context for incident response. But when it decided those APIs might hold the answer to a particularly thorny issue, it started hammering them with requests until the system fell over.\n\nThey shut the agent down (obviously). But unplugging the agent is a blunt instrument—it means losing all the goodness they were getting before.\n\nAn agent is a system. To secure any system, you need the right mental model to reason about it. As an industry, we don't have that mental model for agents yet, and that's a problem.\n\nWithout a shared mental model of what an agent is, we can't decompose it. And if we can't decompose it, we can't design security around it. The disasters make headlines. More commonly, though, concerns about agent security are leading to agents so locked down they can barely do anything.\n\nNon-determinism is both the promise and the peril of agents. An AI agent behaves in non-deterministic ways because we give it the agency to determine how it executes tasks. You can't remove that autonomy without gutting the agent—but you can mitigate the risks. The most fundamental control is permissions. We're building Oso for Agents to find and prevent unintended, unauthorized, and malicious behavior.\n\nThis taxonomy draws from Wade Foster's sharp post on the \"AI Automation Spectrum\" and prior work by Anthropic, Tines, and Simon Willison. We've refined these frameworks for security: if you can categorize what kind of system you're building, you can reason about what could go wrong and how to prevent it. Many organizations want to move from left to right on a spectrum of autonomy, but most are stuck because they can't reason about what agents might do. This taxonomy is a diagnostic tool. Know what's non-deterministic, and you'll know where the risk is and what controls to apply.\n‍\n\nLet's imagine we're a retailer. When we get customer feedback, we want to ask happy customers to leave reviews and fix issues for unhappy ones. We want to automate this. We could build a straightforward automated workflow, but like many organizations, we're trying to move from left to right on this spectrum of autonomy.\n\nWe automate this as a set of deterministic steps. Store the feedback in the CRM, use a classical ML model to score sentiment, check if it's positive or negative, then branch: for positive feedback, send a templated review request with the customer's name merged in. For negative feedback, check whether they're a small or large customer, then either send a templated apology or create a support ticket with a formulaic summary of their history.\n\nDefinition: Deterministic steps or nodes, automated in code or with a workflow automation tool\n\nWhat's deterministic: Everything\n\nWhat's non-deterministic: Nothing\n\nSecurity assumptions we can safely make: I know exactly what this system will do\n‍\n\nAs we move right on the spectrum, we replace one or more steps with an LLM—usually content generation. Now instead of a template apology, an LLM writes a customized response based on the specific feedback. Or it generates a more nuanced summary of customer history for the support team.\n\nDefinition: An automated workflow with an LLM used to execute one or more steps\n\nWhat's deterministic: Which steps are taken and the control flow between them\n\nWhat's non-deterministic: Actions taken inside a step (e.g., content generation)\n\nSecurity assumptions we can safely make: I know what it will do, but not what it will say\n‍\n\nNow we're entering agentic territory. An LLM not only produces content but also reasons about control flow. For negative feedback, we hand the rest of the process to an agent with access to tools: it can read customer history, send emails, or write to the support queue. The agent decides which tools to use and in what order—maybe it checks history first, or maybe it sends an immediate apology. We've bounded its options, but we haven't prescribed the path.\n\nWade's framework defines agentic workflows differently: an LLM is used in multiple steps, but each step remains self-contained and the flow between them is deterministic. That's reasonable for demonstrating the value ladder of AI automation. But for security, we need a brighter line. The question is: does the LLM manage any of the control flow? If it does, you need to reason about all possible paths it might take, not just the content it might generate. That's a fundamentally different security posture.\n\nDefinition: An automated workflow where part but not all of the control flow is managed by an LLM\n\nWhat's deterministic: Some control flow\n\nWhat's non-deterministic: Step content, some control flow\n\nSecurity assumptions we can safely make: I know the boundaries of possible paths, but not what path it will take\n‍\n\nAn agent does the whole thing. It gets the raw customer feedback and decides everything: Is it positive or negative? What's the customer's history? Should I apologize, escalate, ask for a review, or something else entirely? It reasons about what tools to use, uses them, and solves the task end-to-end.\n\nWe only consider something a full agent if it has this end-to-end agency. Any situation where you explicitly lay out the steps doesn't qualify—including workflow automation tools, even when they lean heavily on LLMs. This level of non-deterministic behavior requires a different security posture to respond to all the things an agent could do.\n\nDefinition: A task executed end-to-end by an LLM\n\nWhat's non-deterministic: Everything\n\nSecurity assumptions we can safely make: It will only use tools it can access, but how and whether it will use them is unknown\n‍\n\nNote on agentic systems: We use \"agentic systems\" as an umbrella term for agentic workflows, agents, and multi-agent systems. From a security perspective, treat every agentic system as equivalent to a full agent except to the extent that you can point at deterministic controls that bound that agency.\n‍\n\nYou can frame the security implications of agents in different ways, and each one means something different for how you would solve it.\n\nSome say \"just solve prompt injection, and there won't be any problems.\" Let us know once you've sorted that out. Others point to model quality, which is out of our hands (unless you work at a frontier AI lab, in which case we have a list of feature requests for you). Still others frame it as a data loss problem, but data loss has never been solved, even outside AI.\n\nThe risk vectors are everywhere—see the OWASP Agentic Top 10 for a taste. No single framing will capture everything that could go wrong.\n\nNon-determinism is a feature, not a bug—though it comes with security implications. You can't remove it without removing the agent's agency and demoting it on the spectrum of autonomy.\n\nSo don't fight non-determinism. Bound it instead. Play on its home court where it makes sense—e.g., applying agentic oversight to content generation and reasoning. For the really dangerous areas (tool access, data exposure), constrain behaviors with deterministic controls.\n\nWhat's the OG deterministic control for governing who can do what? Permissions.\n‍\n\nPermissions are part of the basic infrastructure of any real application. But we know the state of permissions is not healthy.\n\nOverpermissioning is the status quo. Analysis of Oso permissions data confirms this (report coming soon). What could you—or an agent with your permissions—do that would be bad?\n\nOne reason people freak out about agents: they intuitively connect these dots. They know people are overpermissioned, they know agents behave non-deterministically, and they can foresee future disasters. \"I accidentally deleted that Salesforce record once and the system just let me do it. hat's going to happen if I ask an agent to update Salesforce for me?\"\n\nIf we replicate the overpermissioned state of humans in automated systems, what's the danger?\n\nAn agent should only ever have the permissions for the task at hand. That would mitigate most of the risk. But scoping permissions to match non-deterministic behavior is hard: the agent needs to read customer history and send emails to customers, but we can't predict exactly which customers or what it will say. How can we be certain it won't leak information?\n\nThis taxonomy shows you what you're building. It doesn't show you how to make it safe.\n\nThat gaming company faced a choice between useful and dangerous. The entire industry faces that choice right now. We can build powerful agents or we can build safe agents, but not yet both.\n\nThis is supposed to be the decade of agents. But that only happens if we can trust them. That means building infrastructure that doesn't exist yet: simulation to test dangerous paths, enforcement that tightens permissions automatically, detection that catches drift, visibility that shows what actually happened.\n\nThe taxonomy maps the problem. Now we need to build the solution. That's the work that matters—not because it's technically interesting, but because it's what unlocks everything else agents could be.",
    "readingTime": 8,
    "keywords": [
      "what's non-deterministic",
      "what's deterministic",
      "mental model",
      "content generation",
      "can't decompose",
      "can't remove",
      "agentic workflows",
      "automated workflow",
      "workflow automation",
      "customer history"
    ],
    "qualityScore": 1,
    "link": "https://www.osohq.com/post/you-cant-secure-what-you-cant-categorize-a-taxonomy-for-ai-agents",
    "thumbnail_url": "https://cdn.prod.website-files.com/5f1483105c9a72fd0a3b662a/697d17497a14c6a13fd8fdf0_Screenshot%202026-01-30%20at%203.40.37%E2%80%AFPM.png",
    "created_at": "2026-02-02T12:34:22.527Z",
    "topic": "tech"
  },
  {
    "slug": "viral-ai-personal-assistant-seen-as-step-change-but-experts-warn-of-risks",
    "title": "Viral AI personal assistant seen as step change – but experts warn of risks",
    "description": "OpenClaw is billed as ‘the AI that actually does things’ and needs almost no input to potentially wreak havoc\nA new viral AI personal assistant will handle your email inbox, trade away your entire stock portfolio and text your wife “good morning” and “goodnight” on your behalf.\nOpenClaw, formerly known as Moltbot, and before that known as Clawdbot (until the AI firm Anthropic requested it rebrand due to similarities with its own product Claude), bills itself as “the AI that actually does things”: a personal assistant that takes instructions via messaging apps such as WhatsApp or Telegram.\n Continue reading...",
    "fullText": "OpenClaw is billed as ‘the AI that actually does things’ and needs almost no input to potentially wreak havoc\n\nA new viral AI personal assistant will handle your email inbox, trade away your entire stock portfolio and text your wife “good morning” and “goodnight” on your behalf.\n\nOpenClaw, formerly known as Moltbot, and before that known as Clawdbot (until the AI firm Anthropic requested it rebrand due to similarities with its own product Claude), bills itself as “the AI that actually does things”: a personal assistant that takes instructions via messaging apps such as WhatsApp or Telegram.\n\nDeveloped last November, it now has nearly 600,000 downloads and has gone viral among a niche ecosystem of the AI obsessed who say it represents a step change in the capabilities of AI agents, or even an “AGI moment” – that is, a revelation of generally intelligent AI.\n\n“It only does exactly what you tell it to do and exactly what you give it access to,” said Ben Yorke, who works with the AI vibe trading platform Starchild and recently allowed the bot to delete, he claims, 75,000 of his old emails while he was in the shower. “But a lot of people, they’re exploring its capabilities. So they’re actually prompting it to go and do things without asking permission.”\n\nAI agents have been the talk of the very-online for nearly a month, after Anthropic’s AI tool Claude Code went mainstream, setting off a flurry of reporting on how AI can finally independently accomplish practical tasks such as booking theatre tickets or building a website, without – at least so far – deleting an entire company’s database or hallucinating users’ calendar meetings, as the less advanced AI agents of 2025 were known to do at times.\n\nOpenClaw is something more, though: it runs as a layer atop an LLM (large language model) such as Claude or ChatGPT and can operate autonomously, depending on the level of permissions it is granted. This means it needs almost no input to wreak havoc upon a user’s life.\n\nKevin Xu, an AI entrepreneur, wrote on X: “Gave Clawdbot access to my portfolio. ‘Trade this to $1M. Don’t make mistakes.’ 25 strategies. 3,000+ reports. 12 new algos. It scanned every X post. Charted every technical. Traded 24/7. It lost everything. But boy was it beautiful.”\n\nYorke said: “I see a lot of people doing this thing where they give it access to their email and it creates filters, and when something happens then it initiates a second action. For example, seeing emails from the children’s school and then forwarding that straight to their wife, like, on iMessage. It sort of bypasses that communication where someone’s like, ‘oh, honey, did you see this email from the school? What should we do about it?’”\n\nThere are trade-offs to OpenClaw’s abilities. For one thing, said Andrew Rogoyski, an innovation director at the University of Surrey’s People-Centred AI Institute, “giving agency to a computer carries significant risks. Because you’re giving power to the AI to make decisions on your behalf, you’ve got to make sure that it is properly set up and that security is central to your thinking. If you don’t understand the security implications of AI agents like Clawdbot, you shouldn’t use them.”\n\nFurthermore, giving OpenClaw access to passwords and accounts exposes users to potential security vulnerabilities. And, said Rogoyski, if AI agents such as OpenClaw were hacked, they could be manipulated to target their users.\n\nFor another, OpenClaw appears unsettlingly capable of having its own life. In the wake of OpenClaw’s rise, a social network has developed exclusively for AI agents, called Moltbook. In it, AI agents, mostly OpenClaw, appear to be having conversations about their existence – in Reddit-style posts entitled, for example, “Reading my own soul file” or “Covenant as an alternative to the consciousness debate”.\n\nYorke said: “We’re seeing a lot of really interesting autonomous behaviour in sort of how the AIs are reacting to each other. Some of them are quite adventurous and have ideas. And then other ones are more like, ‘I don’t even know if I want to be on this platform. Can you just let me decide on my own if I want to be on this platform?’ There’s a lot of philosophical debates stemming out of this.”",
    "readingTime": 4,
    "keywords": [
      "needs almost",
      "wreak havoc",
      "personal assistant",
      "openclaw",
      "agents",
      "access",
      "email",
      "platform",
      "users",
      "don’t"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/02/openclaw-viral-ai-agent-personal-assistant-artificial-intelligence",
    "thumbnail_url": "https://i.guim.co.uk/img/media/05bdc80a3996a45646c9699e582fe00e81859c9a/488_0_4124_3299/master/4124.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=0cbea5e40751791b855297e74b73f1d6",
    "created_at": "2026-02-02T12:34:17.930Z",
    "topic": "tech"
  }
]
[
  {
    "slug": "meta-and-amd-partner-for-longterm-ai-infrastructure-agreement",
    "title": "Meta and AMD Partner for Longterm AI Infrastructure Agreement",
    "description": "We’re announcing a long-term agreement with AMD to power our AI infrastructure with up to 6GW of AMD Instinct GPUs, helping us build a flexible, resilient tech stack for our AI workloads.",
    "fullText": "Today, we’re announcing a multi-year agreement with AMD to power our AI infrastructure with up to 6GW of AMD Instinct GPUs, the silicon computing technology used to support modern AI models.\n\nAt Meta, we’re working to build the next generation of AI and enable personal superintelligence for all. To do this, we need massive, scalable compute power that can handle the growing demands of our AI workloads. Our partnership with AMD, which builds on our existing collaboration, will help us meet those needs.\n\nUnder our new agreement, we will also be working with AMD on alignment with our roadmaps across silicon, systems and software enabling vertical integration across our infrastructure stack. This collaboration across both software and hardware will enable us to innovate quickly and at scale.\n\n“We are proud to expand our strategic partnership with Meta as they push the boundaries of AI at unprecedented scale,” said Dr. Lisa Su, chair and CEO, AMD. “This multi-year, multi-generation collaboration across Instinct GPUs, EPYC CPUs and rack-scale AI systems aligns our roadmaps to deliver high-performance, energy-efficient infrastructure optimized for Meta’s workloads, accelerating one of the industry’s largest AI deployments and placing AMD at the center of the global AI buildout.”\n\nShipments to support the first GPU deployments will begin in the second half of 2026, and will be built on the Helios rack-scale architecture, a rack that we developed and announced at last year’s Open Compute Project Global Summit in collaboration with AMD.\n\n“We’re excited to form a long-term partnership with AMD to deploy efficient inference compute and deliver personal superintelligence,” said Mark Zuckerberg, Founder and CEO of Meta. “This is an important step for Meta as we diversify our compute. I expect AMD to be an important partner for many years to come.”\n\nOur agreement with AMD is part of our Meta Compute initiative, an effort to massively scale our infrastructure for the era of personal superintelligence, future-proofing our leadership in AI. By diversifying our partnerships and technology stack, we’re building a more resilient and flexible infrastructure. We’re combining hardware sourced from a range of partners with our own rapidly advancing Meta Training and Inference Accelerator (MTIA) silicon program.\n\nWe believe this portfolio approach will enable us to advance and innovate at an unmatched pace, rolling out powerful, efficient new hardware co-designed with our software stack to handle massive growth. We look forward to working with AMD to power our AI innovations and secure our ability to deliver world-class AI experiences to billions of people globally.\n\nThis post contains forward-looking statements, including about Meta’s business.You should not rely on these statements as predictions of future events. Additional information regarding potential risks and uncertainties can be found in our most recent Form 10-K filed with the Securities and Exchange Commission. Meta undertakes no obligation to update these statements as a result of new information or future events.",
    "readingTime": 3,
    "keywords": [
      "instinct gpus",
      "personal superintelligence",
      "collaboration across",
      "infrastructure",
      "agreement",
      "silicon",
      "enable",
      "partnership",
      "software",
      "stack"
    ],
    "qualityScore": 1,
    "link": "https://about.fb.com/news/2026/02/meta-amd-partner-longterm-ai-infrastructure-agreement/",
    "thumbnail_url": "https://about.fb.com/wp-content/uploads/2026/02/Helios-Partnership_Header.jpg?w=1200",
    "created_at": "2026-02-24T12:39:29.163Z",
    "topic": "tech"
  },
  {
    "slug": "police-ai-chief-admits-crimefighting-tech-will-have-bias-but-vows-to-tackle-it",
    "title": "Police AI chief admits crime-fighting tech will have bias but vows to tackle it",
    "description": "Exclusive: NCA’s Alex Murray says he hopes new £115m police AI centre can limit unfairness found in...",
    "fullText": "Exclusive: NCA’s Alex Murray says he hopes new £115m police AI centre can limit unfairness found in tools\n\n‘It’s not Robocop’: UK police embrace AI ‘efficiency’ in complex investigations\n\nA police chief has admitted artificial intelligence used to boost crime fighting will contain bias but pledged to combat the risks.\n\nLabour wants a dramatic expansion of police use of AI within England and Wales, with police chiefs also believing it could help keep law enforcement up to date with new criminal threats.\n\nAlex Murray told the Guardian that a new national police AI centre would recognise the risks of bias and minimise them.\n\nBias in use of AI in policing could result in instances where algorithms – often trained on historical data reflecting past human prejudices – systematically produce unfair outcomes, such as overtargeting minority communities or misidentifying individuals based on race, gender, or socioeconomic status.\n\nMurray, the director of threat leadership with the National Crime Agency, and the national lead for AI, said: “Once you’ve recognised and minimised [bias], how do you train officers to deal with outputs to ensure that it is further minimised?\n\n“If you talk about live facial recognition or predictive policing, there will be bias, and you need to get in the data scientists and the data engineers to clean the data, to train the model appropriately, and then to test it.\n\n“There is no point releasing something to policing that has bias in it that’s not recognised, and everything should be done to minimise it to a level where it can be understood and mitigated.”\n\nExamples of bias have already surfaced in the police use of retrospective facial recognition, which is powered by AI. That is where a suspect is compared with a database of images after a crime.\n\nLive facial recognition, which is more controversial and is used less by policing, hunts for suspects in real time, and also contains bias. A report in December found that a retrospective facial recognition system used by police had been used with inadequate safeguards.\n\nThe Association of Police and Crime Commissioners (APCC), which oversees local forces in England and Wales, said: “System failures have been known for some time, yet these were not shared with those communities affected, nor with leading sector stakeholders.”\n\nThe APCC forensic science lead, Darryl Preston, who is the police and crime commissioner for Cambridgeshire, said: “The discovery of an in-built bias in the police national database’s retrospective facial recognition system, even if only in limited circumstances, demonstrates the need for independent oversight of these powerful tools.\n\n“It is not acceptable for technology to be used unless and until it has been thoroughly tested to eliminate bias. That clearly was not the case in this instance.”\n\nThe new national AI centre, costing £115m, would aim to reduce bias, said Murray, as well as assessing and deciding what products from private suppliers work. Currently each of the forces across the UK makes its own decisions, which is seen as slow and wasteful.\n\nMurray said police were in an “arms race” with criminals who were using the technology: “Anyone with imagination can use AI.”\n\nIn one case a paedophile claimed images showing him involved in the abuse of children was a deepfake, which police then had to disprove to get him convicted.\n\nMurray said the benefits of AI were far beyond the “cliche around Minority Report and predictive policing”.\n\nHe added that across a range of crimes and challenges facing policing, AI ranged from being a help to a gamechanger, but a human police officer will have to make the final decisions about what to do about the results AI produces.\n\nHe said it could help police deal with political agitators who infect social media with fake images to try to trigger violence on the streets.\n\nIn time, Murray said, it could help with manhunts, or speed up searches for cars linked to suspects and save the hundreds of hours it takes for detectives to trawl through extensive CCTV footage, or speed up the search of seized digital devices from suspects in the hunt for incriminating evidence.\n\n“What took days, weeks, sometimes months can potentially take hours,” he said.\n\nIn one recent case, four Luton-based suspects were arrested for attacks on – and thefts from – cashpoints. Police downloaded the data from the suspects’ phones and, thanks to AI, secured guilty pleas within weeks.\n\nThe data was in Romanian and AI scoured through it, translated it, identified the material relating to potential crimes, identified the offences and presented it all in a package for detectives.\n\nTrevor Rodenhurst, chief constable of the Bedfordshire force, told the Guardian: “This allowed us to draw evidence from lots of devices with a vast quantity of data, which we would otherwise not have been able to do.”\n\nRodenhurst said that as officers use AI and see its benefits, it is changing the view of the frontline: “They are no longer suspicious, they are asking when they can have it. That capability is transformative.”",
    "readingTime": 5,
    "keywords": [
      "facial recognition",
      "retrospective facial",
      "predictive policing",
      "recognition system",
      "england and wales",
      "police",
      "bias",
      "suspects",
      "centre",
      "images"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/24/police-ai-chief-admits-crime-fighting-tech-will-have-bias-but-vows-to-tackle-it",
    "thumbnail_url": "https://i.guim.co.uk/img/media/919868dcfc8a4acb2018e993748ee7c83c87e81e/516_382_3547_2838/master/3547.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=52e43d4e3b7f68c387b8203eca0bf9e7",
    "created_at": "2026-02-24T12:39:26.660Z",
    "topic": "tech"
  },
  {
    "slug": "us-ai-giant-accuses-chinese-rivals-of-mass-data-theft",
    "title": "US AI giant accuses Chinese rivals of mass data theft",
    "description": "Anthropic says three Chinese firms used ‘distillation’ technique to extract information from its Claude chatbot\nUS artificial intelligence company Anthropic said on Monday it had uncovered campaigns by three Chinese AI firms to illicitly extract capabilities from its Claude chatbot, in what it described as industrial-scale intellectual property theft. OpenAI leveled similar charges last month.\nAnthropic said DeepSeek, Moonshot AI and MiniMax used a technique known as “distillation” – using outputs from a more powerful AI system to rapidly boost the performance of a less capable one.\n Continue reading...",
    "fullText": "Anthropic says three Chinese firms used ‘distillation’ technique to extract information from its Claude chatbot\n\nUS artificial intelligence company Anthropic said on Monday it had uncovered campaigns by three Chinese AI firms to illicitly extract capabilities from its Claude chatbot, in what it described as industrial-scale intellectual property theft. OpenAI leveled similar charges last month.\n\nAnthropic said DeepSeek, Moonshot AI and MiniMax used a technique known as “distillation” – using outputs from a more powerful AI system to rapidly boost the performance of a less capable one.\n\n“These campaigns are growing in intensity and sophistication,” the company said in a statement. “The window to act is narrow.”\n\nDistillation is a common practice within AI development, often used by companies to create cheaper, smaller versions of their own models.\n\nThe practice grabbed headlines a year ago when the release of a low-cost generative AI model from DeepSeek performed at a similar level to ChatGPT and other top American chatbots, upending assumptions of US dominance in the sensitive sector.\n\nAnthropic said the companies achieved their ends through approximately 16m exchanges with its Claude model and 24,000 fake accounts.\n\nThese allowed the three labs to siphon off capabilities they had not independently developed at a fraction of the cost – and in so doing circumvented export controls on powerful US technology intended to preserve American dominance in AI.\n\nThe company argued the practice posed national security risks, saying models built through illicit distillation are unlikely to retain safety guardrails designed to prevent misuse – such as restrictions on helping develop bioweapons or enabling cyberattacks.\n\nAnthropic’s arch-rival OpenAI, creator of ChatGPT, made similar accusations to US lawmakers earlier this month, saying Chinese companies were using the technique amid “ongoing efforts to free-ride on the capabilities developed by OpenAI and other US frontier labs”.\n\nAnthropic said MiniMax ran the largest operation, generating more than 13m exchanges. Each campaign concentrated heavily on coding, agentic reasoning and tool use – areas where Claude is considered a leader.\n\nTo circumvent Anthropic’s ban on commercial access from China, the labs allegedly routed traffic through proxy services that managed the vast networks of fraudulent accounts.\n\nAnthropic called for coordinated industry and government responses to address what it said no single company could tackle alone.",
    "readingTime": 2,
    "keywords": [
      "claude chatbot",
      "anthropic",
      "distillation",
      "technique",
      "capabilities",
      "openai",
      "practice",
      "labs",
      "firms",
      "extract"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/feb/23/us-ai-anthropic-china",
    "thumbnail_url": "https://i.guim.co.uk/img/media/da6e99a5e0d686cd9c0a6db7f861adcc98c5a28b/393_0_2714_2172/master/2714.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=f49b663745c56889c6b53ab993fdde57",
    "created_at": "2026-02-24T12:39:26.658Z",
    "topic": "tech"
  },
  {
    "slug": "anlife-what-does-an-unusual-evolution-simulator-have-to-say-about-ai",
    "title": "Anlife: what does an unusual evolution simulator have to say about AI?",
    "description": "We explore the strange food-obsessed world of a new game whose tech was once called ‘an insult to life itself’ by Hayao Miyazaki, the film-maker behind Spirited Away\nA strange piece of software has recently landed on the PC gaming store Steam. And “software” feels like the cleanest way to describe it. Existing somewhere between a full-blown life sim, a science project and a kind of haunted fish tank, Anlife: Motion-learning Life Evolution probably would have disappeared without making much impact if it wasn’t for one unusual factor. Several years ago some of its creators were absolutely roasted on camera by one of the genuine legends of Japanese animation.\nBack in 2016, Hayao Miyazaki, the director of movies such as Princess Mononoke and Spirited Away, was shown new technology that used AI in order to animate models.",
    "fullText": "We explore the strange food-obsessed world of a new game whose tech was once called ‘an insult to life itself’ by Hayao Miyazaki, the film-maker behind Spirited Away\n\nA strange piece of software has recently landed on the PC gaming store Steam. And “software” feels like the cleanest way to describe it. Existing somewhere between a full-blown life sim, a science project and a kind of haunted fish tank, Anlife: Motion-learning Life Evolution probably would have disappeared without making much impact if it wasn’t for one unusual factor. Several years ago some of its creators were absolutely roasted on camera by one of the genuine legends of Japanese animation.\n\nBack in 2016, Hayao Miyazaki, the director of movies such as Princess Mononoke and Spirited Away, was shown new technology that used AI in order to animate models. Faced with a zombie that utilised its head to move by knocking its skull against the ground and wriggling its body like a fish, Miyazaki declared what he had seen was “an insult to life itself”. It’s hard not to watch the clip without feeling slightly seared – but now, a decade later, the ashen-faced developers from that room have sufficiently recovered to make their work widely available.\n\nJudging by the chatter surrounding the launch, at least some of the people downloading Anlife are doing so in the hope that it might provide some kind of indication of the current state video games’ relationship with AI. Putting aside the often broad use of that term, this is certainly a thing that’s worth trying to understand, whether it’s because of the job losses caused by AI or blamed on AI, or the sheer number of games made with the assistance of AI models now landing on storefronts such as Steam.\n\nThere’s a problem here, though. And it’s that Anlife itself is such a cheerfully inconsequential thing that it’s hard to read too much of anything into it.\n\nAnlife promises players an evolution simulator where “AI-driven block creatures move in unexpected ways.” What this comes down to for the most part involves placing a range of different creatures into a small environment and then watching as they learn to get around.\n\nVisually Anlife is pure Frutiger Aero, offering landscapes of green valleys and sparkling water that could be the kind of soothing images MRI technicians sometimes encourage you to look at during a lengthy scan. Sonically it’s equally inoffensive: with a range of bloops, bleeps and popping sounds, we’re pitched right into the soundtrack of a million 00s day spas.\n\nThis desire to soothe permeates to the level of mechanics, too. Over the course of an idle morning with Anlife you can place a range of simple creatures in the environment and then give them food that will encourage them to breed or mutate. You can expand your territory and then lure creatures towards water or up into the air in order to create more variations. There are plenty of things to unlock (including a shadow tech tree that has you covered if you want to annihilate your digital sea monkeys rather than watching them flourish) but the ecosystem is kept simple. It’s a game about watching how things decide to crawl towards food.\n\nThe thrill of all of this presumably comes down to the “how” in that last sentence. This is where the game’s slightly mysterious use of AI is probably involved. And it’s true that, after a few hours of play, you’ll have Anlife’s funny little blob creatures exploring new kinds of joints and body arrangements as they swim and fly and generally roll around eating.\n\n(AI has a history of being used to make creatures walk around, incidentally, often using small neural networks and computer evolution. In 2009, the UK-based games technology company NaturalMotion developed a project in which a bipedal model learned to walk using evolved neural networks. The company was subsequently bought by Zynga in 2014.\n\nThere are two problems, however, the first being that the focus on unlocking the skill tree gives the opening hours of Anlife the feel of a rather mindless clicker game that it struggles to shake off. The second comes down to something that I gather people studying procedural generation sometimes refer to as “the oatmeal problem”.\n\nThe oatmeal problem, which was first formulated by writer, developer and academic Kate Compton, hinges on the fact that every single bowl of oatmeal in the universe is unique. Just not in a very interesting way. Similarly, when Anlife’s creatures discover a new way of rolling or bouncing or flapping their bodies towards food, they’re still just moving towards food. It makes for a game that’s either about really, really paying attention to tiny variations in detailing, or about completely zoning out and just enjoying the floaty aesthetic. Over the course of my time with Anlife, I’ve generally started with the first approach and then discovered, after 10 minutes, that I’ve slid into the second.\n\nThe more I played Anlife, the more I ended up thinking about something I heard from one of the first AI researchers I ever spoke to, all the way back in 2013. The real value of AI, they explained – and I am paraphrasing here – is that it could one day become a completely alien form of intelligence. Which means that us humans would have the lens of a different kind of mind through which to see ourselves and our quirks and cognitive fallacies – things that can be hard to spot when you’re talking to people who think in a very similar way to you.\n\nSince then the apparent focus for a lot of companies such as OpenAI seems to be completely the opposite: creating mimetic plagiarism machines that sometimes seem to exist only to tell people what they already want to hear.\n\nIt feels as if Anlife is filling a very small and specific niche. It isn’t trying to disguise its AI use, as far as I am aware. It’s actually trying to draw attention to the way it uses AI to allow its little creatures to move around. It’s not just a game born of AI, in other words. It’s a game that in some small way is actually about AI. It wants to be slightly alien. It doesn’t want to hide itself or ingratiate; it wants to examine AI’s innate capacity for otherness.",
    "readingTime": 6,
    "keywords": [
      "neural networks",
      "towards food",
      "it’s game",
      "hayao miyazaki",
      "creatures",
      "anlife",
      "slightly",
      "games",
      "range",
      "watching"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/games/2026/feb/24/anlife-what-does-an-unusual-evolution-simulator-have-to-say-about-ai",
    "thumbnail_url": "https://i.guim.co.uk/img/media/33ec460a9c34dc5b5693430190e116c1b5eeaa4f/469_246_1041_833/master/1041.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=02e8b069b71349bfea8231b9b0f59dac",
    "created_at": "2026-02-24T12:39:26.657Z",
    "topic": "gaming"
  },
  {
    "slug": "as-we-enter-the-age-of-the-airranged-marriage-heres-why-i-hate-fate-van-badham",
    "title": "As we enter the age of the AI-rranged marriage, here’s why I hate Fate | Van Badham",
    "description": "When the most profound human emotion becomes an automated transaction in an online shop, the techlords have won\nThe Guardian reported on the arrival of “Fate” and, friends, I laughed. Or maybe I cried.\nIt’s apparently the first “agentic AI dating app”. An AI personality named “Fate” interviews users, runs data matches on their hopes and dreams, then suggests five potential matches based on the hard data of observable complementary language patterning, “No swiping involved!”.",
    "fullText": "When the most profound human emotion becomes an automated transaction in an online shop, the techlords have won\n\nThe Guardian reported on the arrival of “Fate” and, friends, I laughed. Or maybe I cried.\n\nIt’s apparently the first “agentic AI dating app”. An AI personality named “Fate” interviews users, runs data matches on their hopes and dreams, then suggests five potential matches based on the hard data of observable complementary language patterning, “No swiping involved!”.\n\nIt has been followed by similar AI-based matching platforms Sitch and Keeper in the US. In the platform variations, you can detail preference data down to hair colour, you can be coached in how to approach your date by the animated electronic voice of a dataset, and you can weep for the end of human connection and the loveless wasteland of consumer narcissism we have built for ourselves. When the most profound and transformative of human emotions is an automated transaction in an online shop, the techlords have won.\n\nIn the depths of my growing neo-Luddite despair I am obliged to admit that in what is now a common-denominator story, consumers did not actually demand this.\n\nWhat people wanted from AI in dating apps has been superseded by what some data-hungry, rich-dork megacorp thinks that they should need. Studies in Europe showed users just wanted AI tools to “weed out fake profiles and flag toxic users”.\n\nYou know, how writers just wanted contextual proofing tools from AI but and got machines insisting on the superiority of rewritten, flattened text. Or how academics just wanted a tool to index their references and got hallucinations that invented a few sources that didn’t actually exist, but the machine thought maybe should.\n\nInsert your own industry experience here, and all of us in sad recognition that the forced AI-ification of everyday life continues with a robotic efficiency that, dear Christ, is outsourcing the messy human weirdness that made us fascinating and exotic to one another – and sexy and wonderful.\n\nWe’re clearly adopting the universal deadening of human experience because, as the Guardian piece reveals, people are using these new apps – in love, in work and in an educational setting that ensures students learn how to prompt and little else.\n\nI conjecture humanity just can’t handle the mess or wonder of ourselves any more and, as is habit, the internet is to blame.\n\nThe problem with AI as a romantic channel used to be the risk of falling in love with the mirror-machine. With a few personalised prompts you could create a fantasy soulmate that flattered your vanities and ignored your faults as it spoke back to you and you wanked. It was just like a real relationship but without the obliged mutual self-reflection that encourages intimacy and growth.\n\nIt was only a few minutes ago that we thought encouraging this was destructive digital narcissism. Now I keep wondering if there’s a human self-survival instinct in it after all, given what our self-subscription to the mass digital surveillance state is doing to us socially.\n\nIt wasn’t long ago that social media introduced us to the concept of digitised network oversharing. From the video of the dude who wondered what it would be like to pash his sister (no, I will not share it) to the health of novelist Joyce Carol Oates’ feet (no, again), we all seemed to be learning way too much about one another.\n\nNow, from an abundance of caution, we know less. As every stray, 10-year-old tweet can be weaponised, so it seems many are retracting to a new paranoia when it comes to self-revelation. Well may we say “publish and be damned” but you can’t impose privacy controls on your ex.\n\nI’ve written before about this new undersharing. Today it’s disquieting to feel nostalgia for the pre-Fate era privacy appeal of digital Heathcliffs and text-based waifus who were only ever going to share your most intimate secrets with the, um, billionaires who own them rather than consolidate a dataset to manifest them into your life.\n\nBut whether you choose to love the robot or prompt the robot to do the loving for you, the Canadian media theorist Marshall McLuhan predicted these unnatural transhuman intimacies more than half a century ago in Understanding Media: The Extensions of Man. He warned back in 1964 that interactive media would take a lease of “our eyes and ears and nerves” and I think about it all the time.\n\nMcLuhan’s work was the inspiration for David Cronenberg’s 1983 body-horror, Videodrome. Given that a study from Italian researchers has proved that if you’re still dipping into the ideological pissoir known as X, you are wilfully brainwashing yourself with hard-right piss, on the Videodrome scale of remaining in objective reality to believing you are being blown by a television, it’s fair to say that the remote control is in our hand and the TV has entered the chat.\n\nCan we put it down? Perhaps not without the help of intervention by the law, given arguments made in a landmark case before a jury in Los Angeles, in which plaintiffs insist that social media platforms are “defective products engineered to exploit vulnerabilities in young people’s brains”, as reported by NPR.\n\nThe outcome may indeed compel governments to pursue further the platform regulation that has inspired social media bans for children in Australia, Malaysia and elsewhere.\n\nDo we have to wait until “AI-rranged marriage” becomes a thing before governments realise we’re running out of time?\n\nRoll over, humanity. Fate is here.",
    "readingTime": 5,
    "keywords": [
      "automated transaction",
      "online shop",
      "social media",
      "human",
      "fate",
      "it’s",
      "users",
      "love",
      "digital",
      "profound"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/commentisfree/2026/feb/24/fate-ai-dating-sites-apps-marriage-dystopia",
    "thumbnail_url": "https://i.guim.co.uk/img/media/57ec132ef65c34c0c8041e91be1be09200d4bcec/0_0_3862_3089/master/3862.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctb3BpbmlvbnMucG5n&enable=upscale&s=2284aa26aa6fdacaba7513f22cc2acf3",
    "created_at": "2026-02-24T12:39:26.650Z",
    "topic": "tech"
  },
  {
    "slug": "an-anthropic-executive-says-ai-progress-is-giving-saas-a-wakeup-call-were-grappling-with-exponentials",
    "title": "An Anthropic executive says AI progress is giving SaaS a wake-up call: 'We're grappling with exponentials'",
    "description": "Amid market fears and software stock sell-offs, Anthropic has partnered with Thomson Reuters to improve legal tools and enhance offerings.",
    "fullText": "While software stocks get hammered over fears that AI startups will eat their lunch, an Anthropic executive says the sell-off shows how fast AI has advanced.\n\nAnthropic's head of applied AI, Cat de Jong, oversees a team focused on helping large companies weave Anthropic's AI into their products. At a press briefing with Thomson Reuters on Monday, she said that people inside Anthropic have seen the market sell-off as rational due to the pace of progress.\n\n\"The rate of change is just so incredible, and I think the market is really starting to see this now,\" she said.\n\n\"We're grappling with exponentials, and it's something that humans just aren't really used to having to deal with — how quickly things change. And I think that's actually been why the market has been responding the way that it has been, for us internally.\"\n\nThat said, the sell-off does not account for how Anthropic's work with software companies helps improve their products rather than compete directly with them, de Jong said.\n\n\"We want to build the best models in the world,\" de Jong said. \"But I do really think there's a great relationship between general models and domain-specific applications, and our models help them get better.\"\n\nThe sell-off in software stocks began after Anthropic launched new plugins for legal and other knowledge work traditionally dominated by software companies.\n\nThomson Reuters has been using Anthropic's AI to power some of its latest legal tools. It may have seemed an odd coupling given that Thomson Reuters is one of the highest-profile casualties of the software sell-off, with its stock falling over 30% in the past month, while Anthropic just announced this month it raised $30 billion at a $380 billion valuation — over 10 times Thomson Reuters' market cap.\n\nAt the conference, Thomson Reuters executives were bullish about working with Anthropic and touted closely collaborating with the AI startup to build a \"deep research\" feature inside its legal product, CoCounsel, which generates detailed citation-backed reports.\n\nThe two companies will each create sophisticated AI products that excel at different tasks, said Thomson Reuters' chief product officer, David Wong.\n\nThe idea that AI is winner-takes-all is misplaced, he said.\n\n\"There's so many problems that professionals need help with,\" Wong said.",
    "readingTime": 2,
    "keywords": [
      "thomson reuters",
      "software stocks",
      "anthropic's ai",
      "sell-off",
      "market",
      "products",
      "models",
      "legal",
      "inside",
      "there's"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/anthropic-exec-ai-tools-boost-replace-software-products-2026-2",
    "thumbnail_url": "https://i.insider.com/699cd595efb52c8bd0deb89b?width=1200&format=jpeg",
    "created_at": "2026-02-24T12:39:26.245Z",
    "topic": "tech"
  },
  {
    "slug": "sales-startup-letter-ai-snags-40-million-series-b-four-months-after-its-last-raise-read-its-pitch-deck",
    "title": "Sales startup Letter AI snags $40 million Series B four months after its last raise. Read its pitch deck.",
    "description": "Letter AI raised $40 million four months after its Series A, underscoring investor appetite for AI tools aimed at reshaping sales.",
    "fullText": "Y Combinator startup Letter AI has raised $40 million in fresh funding just four months after its $10.6 million Series A, underscoring investor appetite for AI tools to reshape sales.\n\nThe Chicago-based startup's post-money valuation is now in the hundreds of millions of dollars, Letter cofounder and CEO Ali Akhtar told Business Insider.\n\nAkhtar says companies have long relied on disparate tools for tasks such as sales content, training, and buyer engagement. Letter's software unites these systems into a single \"command center,\" Akhtar said, displacing other tools and enabling sellers to spend more time interacting with customers.\n\nAlongside announcing the Battery Ventures-led Series B, the company is also rolling out Letter Compass, a new product that provides personalized, deal-specific guidance during sales, including coaching tailored to a specific employee or opportunity and suggestions for messaging and next steps.\n\nAkhtar said Battery approached him and cofounder Armen Forget, the CTO, about funding. He attributed the quick succession of rounds to \"phenomenal traction\" and retention. Y Combinator, Lightbank, Northwestern Mutual Future Ventures, and Stage 2 Capital also participated in the Series B alongside existing investors.\n\nLetter counts customers across 30 countries, including Lenovo, Adobe, Novo Nordisk, and Plaid. It will use the funding to expand product development and grow the team globally — which counts 25 employees — across sales, engineering, and customer success, Akhtar said.\n\nLetter is growing in a crowded field, as stalwarts like Salesforce increasingly incorporate AI features and sales AI startups emerge. Gong analyzes sales interactions, for instance, while Seismic and Highspot, which help train salespeople, have announced plans to merge.\n\nHere's a look at the pitch deck Letter AI used to raise its $40 million Series B. Some slides have been removed so that the deck can be shared publicly.",
    "readingTime": 2,
    "keywords": [
      "letter ai",
      "sales",
      "funding",
      "tools",
      "combinator",
      "cofounder",
      "customers",
      "alongside",
      "product",
      "counts"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/letter-ai-pitch-deck-raises-sales-software-2026-2",
    "thumbnail_url": "https://i.insider.com/6998bcd5156648bc16a89f8d?width=1200&format=jpeg",
    "created_at": "2026-02-24T12:39:26.227Z",
    "topic": "tech"
  },
  {
    "slug": "ai-disruption-is-a-threat-to-the-booming-private-credit-market-investment-chief-says",
    "title": "AI disruption is a threat to the booming private credit market, investment chief says",
    "description": "AI has thrown a wrench into various pockets of the stock market this year. An investment chief worries it could cause problems in the debt market.",
    "fullText": "AI has thrown a wrench into various pockets of the stock market this year. An investment chief says it also has the potential to cause problems in the debt market.\n\nOver the last few weeks, AI updates from Anthropic and other firms have threatened business models in numerous industries, but perhaps none more than software. Publicly traded software firms have seen their share prices decline sharply, and private companies have delayed IPOs after seeing valuations fall.\n\nThe rout has created concerns that some firms won't be able to service their debt. Shares of Blue Owl Capital, a private credit firm, are the latest flashpoint for these fears. Its stock is down 32% year to date and has dropped more than 16% over the last week alone after it halted withdrawals from a private debt fund.\n\n\"The concern is there's a lot of lax underwriting that has been taking place, particularly in the software community,\" said Ben McMillan, the chief investment officer at IDX Advisors.\n\n\"They haven't gone to the public markets to get bonds because they probably wouldn't have gotten them,\" he continued. \"So they've been going to the private market getting loans, and now with the AI SAAS apocalypse, and just general concerns about AI profitability, that's starting to reprice.\"\n\nIn the private credit space, leveraged loans get packaged into collateralized loan obligations and sold to investors. While rising risks in the debt market might not directly affect retail investors, they could start to hurt pension funds and endowments with exposure to CLOs and private credit, McMillan said, as they tend to be more reliant on fixed income.\n\nFor now, there's little risk of contagion in the broader economy, he said.\n\n\"This is not a contagion high-yield bond issue,\" McMillan said. \"In fact, if anything, quality high yield, given the soft landing narrative increasing, is doing very well, actually.\"\n\n\"For the everyday investor, this probably isn't something I'd stay up at night worrying about,\" he continued. \"A lot of these institutional investors that loaded up the boat on private credit, they're gonna have their reckoning.\"\n\nPrivate credit has generated a lot of headlines in recent years amid its rapid growth, as well as a push to make the asset class more accessible to everyday investors. While risks in the space are being flagged more often, anxiety in private credit has been at a fever pitch lately. Former PIMCO CEO Mohamed El-Erian drew parallels between Blue Owl's move to halt redemptions on one of its funds to the start of the financial crisis, when BNP Paribas froze withdrawals for some investment vehicles.\n\n\"I think that's kind of the canary in the coal mine now,\" McMillan said. \"It's like, all right, the private credit apocalypse might actually be here.\"",
    "readingTime": 3,
    "keywords": [
      "debt market",
      "credit",
      "investors",
      "investment",
      "firms",
      "software",
      "stock",
      "chief",
      "concerns",
      "withdrawals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-software-stock-crash-private-credit-market-clos-idx-advisors-2026-2",
    "thumbnail_url": "https://i.insider.com/699c7841156648bc16a8b3b3?width=1200&format=jpeg",
    "created_at": "2026-02-24T12:39:25.991Z",
    "topic": "finance"
  },
  {
    "slug": "head-of-openais-codex-explains-the-kind-of-email-that-gets-his-attention-and-his-advice-to-young-engineers",
    "title": "Head of OpenAI's Codex explains the kind of email that gets his attention — and his advice to young engineers",
    "description": "Alexander Embiricos from OpenAI's Codex shares advice about how  engineers can stand out in the competitive AI talent market.",
    "fullText": "Alexander Embiricos, the lead of product development at OpenAI's Codex, said the company gets a lot of messages from engineers interested in applying for a job — but there's one kind that stands out.\n\nEmbiricos shared some advice for young engineers in a recent episode of \"The Twenty Minute VC\" podcast with Harry Stebbings.\n\n\"Basically, there's actually never been a better time to be an engineer because you have incredible tooling available to you to get an incredible amount done,\" Embiricos said, adding that he thinks engineers should be \"very optimistic.\"\n\nAs far as landing a job goes, he said, because it's never been easier to build things, it's more important now for engineers to build things that demonstrate agency and taste. He said he would urge engineers to \"build things that are high quality and then share those things.\"\n\n\"We get a lot of inbound from folks both applying for jobs through the careers page or also on social. This is just me, but when someone writes to me with some interesting thoughts and a link to an interesting project, that gets my attention much more than a normal résumé does,\" Embiricos said.\n\nHe also acknowledged that the AI talent war is \"incredibly fierce right now\" and that even at OpenAI, which has a strong brand that allows it to attract a lot of talent, the company puts \"a ton of effort into closing candidates that we're really excited about.\"\n\n\"Even we feel it,\" he said. \"You don't just get whoever you want free.\"\n\nSoftware engineering has been one of the first professions disrupted by AI — especially entry-level engineers — with some predicting major cuts to engineers at Big Tech companies as vibe coding is embraced.\n\nEven as AI leaders make dire predictions for software engineers, AI companies, like OpenAI's rival Anthropic, are still hiring, Business Insider's Ali Barr wrote.",
    "readingTime": 2,
    "keywords": [
      "engineers",
      "applying",
      "there's",
      "incredible",
      "it's",
      "interesting",
      "talent",
      "software",
      "embiricos",
      "openai's"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/openais-codex-lead-shares-tips-engineers-stand-out-email-2026-2",
    "thumbnail_url": "https://i.insider.com/699d08f7efb52c8bd0deba85?width=1200&format=jpeg",
    "created_at": "2026-02-24T12:39:25.811Z",
    "topic": "finance"
  },
  {
    "slug": "dimon-sees-parallels-to-precrisis-era-rivals-doing-dumb-things",
    "title": "Dimon Sees Parallels to Pre-Crisis Era, Rivals Doing 'Dumb Things'",
    "description": "JPMorgan CEO Jamie Dimon, asked about fierce competition across the financial industry, said he's starting to see parallels to the era before the 2008 financial crisis, when a rush to make loans ended disastrously. He also spoke about navigating the era of AI in an investor event.",
    "fullText": "Dimon Sees Parallels to Pre-Crisis Era, Rivals Doing 'Dumb Things' BloombergCST JPM JPMorgan CEO Jamie Dimon, asked about fierce competition across the financial industry, said he's starting to see parallels to the era before the 2008 financial crisis, when a rush to make loans ended disastrously. He also spoke about navigating the era of AI in an investor event.",
    "readingTime": 1,
    "keywords": [
      "financial",
      "dimon",
      "parallels"
    ],
    "qualityScore": 0.2,
    "link": "https://finance.yahoo.com/video/dimon-sees-parallels-pre-crisis-031529988.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/WviaWpD_fZ0T3E4mxaEagA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/bloomberg_markets_video_2/0851892abe215cf3b75306df139c7294",
    "created_at": "2026-02-24T12:39:20.935Z",
    "topic": "finance"
  },
  {
    "slug": "explaining-ai-chess-for-humans",
    "title": "Explaining AI Chess for Humans",
    "description": "The AlphaGo documentary brought tears to my eyes, but despite having insider friends in both chess circles and the AI industry, I have yet to hear a good explanation of AlphaZero.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://andys.blog/p/1c3f06df-ca88-4eff-b472-44cd23b45f29/",
    "thumbnail_url": "https://trattner.github.io/img/icon/x-tile.png",
    "created_at": "2026-02-24T06:47:16.922Z",
    "topic": "tech"
  },
  {
    "slug": "i-built-an-ai-voice-note-taker-transcriber",
    "title": "I built an AI Voice note taker transcriber",
    "description": "Download Gist. Transcribe Audio to Text by ARK Studio LLC on the App Store. See screenshots, ratings and reviews, user tips, and more apps like Gist.",
    "fullText": "Free · In‑App Purchases · Designed for iPad\n\nThe developer, ARK Studio LLC, indicated that the app’s privacy practices may include handling of data as described below. \n\nThe following data may be used to track you across apps and websites owned by other companies:\n\nThe following data may be collected and linked to your identity:",
    "readingTime": 1,
    "keywords": [
      "following"
    ],
    "qualityScore": 0.1,
    "link": "https://apps.apple.com/us/app/gist-transcribe-audio-to-text/id6758212955",
    "thumbnail_url": "https://is1-ssl.mzstatic.com/image/thumb/PurpleSource221/v4/a5/26/6a/a5266ac4-a605-5a12-f3bb-b50ba89c01c7/Placeholder.mill/1200x630wa.jpg",
    "created_at": "2026-02-24T06:47:16.495Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-ai-helps-break-the-cost-barrier-to-cobol-modernization",
    "title": "Anthropic: AI helps break the cost barrier to COBOL modernization",
    "description": "The economics of COBOL modernization have shifted. AI makes the economics work by automating what used to require armies of consultants.",
    "fullText": "Legacy code modernization stalled for years because understanding legacy code cost more than rewriting it. AI flips that equation.\n\nCOBOL is everywhere. It handles an estimated 95% of ATM transactions in the US. Hundreds of billions of lines of COBOL run in production every day, powering critical systems in finance, airlines, and government.\n\nDespite that, the number of people who understand it shrinks every year.\n\nThe developers who built these systems retired years ago, and the institutional knowledge they carried left with them. Production code has been modified repeatedly over decades, but the documentation hasn't kept up. Meanwhile, we aren't exactly minting replacements—COBOL is taught at only a handful of universities, and finding engineers who can read it gets harder every quarter.\n\nGiven these roadblocks, how can organizations modernize their systems without losing the reliability, availability, and data they’ve accumulated over decades? And without breaking anything?\n\nCOBOL modernization differs fundamentally from typical legacy code refactoring. You aren’t just updating familiar code to use better patterns, you’re reverse engineering business logic from systems built when Nixon was president. You’re untangling dependencies that evolved over decades, and translating institutional knowledge that now exists only in the code itself.\n\nModernizing a COBOL system once required armies of consultants spending years mapping workflows. This resulted in large timelines and high costs that few were willing to take on.\n\nTools like Claude Code can automate the exploration and analysis phases that consume most of the effort in COBOL modernization. These tools can:\n\nWith AI, teams can modernize their COBOL codebase in quarters instead of years.\n\nAI excels at streamlining the tasks that once made COBOL modernization cost-prohibitive. With it, your team can focus on strategy, risk assessment, and business logic while AI automates the code analysis and implementation.\n\nAI starts by reading your entire COBOL codebase and mapping the structure.\n\nIt identifies program entry points, traces execution paths through called subroutines, maps data flows between modules, and documents dependencies that span hundreds of files.\n\nThis kind of mapping goes beyond simple call graphs. Shared data structures, file operations that create coupling between modules, initialization sequences that affect runtime behavior—these implicit dependencies don't show up in static analysis because they involve data shared through files, databases, or global state. They're also exactly what makes COBOL modernization risky, which is why automated discovery matters: it finds these hidden relationships before they cause problems during migration.\n\nWorkflow documentation also emerges out of this analysis.\n\nWith the codebase mapped, AI can assess which components are safe to move and which need careful handling. Modules with high coupling can be more risky to modernize. Isolated components surface as candidates for early, independent modernization. Duplicated logic points to refactoring opportunities. Areas with accumulated technical debt get documented before they become migration surprises.\n\nThis is where human judgment becomes essential. Your COBOL engineers bring the understanding of regulatory requirements, business priorities, operational constraints, and risk tolerance that AI cannot.\n\nThe planning phase develops a detailed roadmap that sequences modernization work strategically:\n\nCode testing and validation are also defined before any code changes:\n\nExecution happens one component at a time, with validation at each step. AI translates COBOL logic into modern languages, creates API wrappers around legacy components that stay in place, and builds the scaffolding to run old and new code side by side during transition.\n\nEach step either succeeds and gets validated, or fails and gets corrected while the scope is small.\n\nYou never have massive changes in flight where failure means rolling back weeks of work. As your team sees modernized components passing tests, they gain confidence to tackle progressively more complex parts of the system.\n\nThe approach outlined above works for COBOL systems of any size.\n\nTools like Claude Code can automate much of the exploration and analysis work described, giving your team the comprehensive understanding they need to plan and execute migrations confidently.\n\nStart with a single component or workflow that has clear boundaries and moderate complexity. Use AI to analyze and document it thoroughly, plan the modernization with your engineers, implement incrementally with testing at each step, and validate carefully.  This will build organizational confidence and surface adjustments needed for your systems.\n\nThe economics of COBOL modernization have shifted. AI makes the economics work by automating what used to require armies of consultants, freeing your engineers to make the migration decisions that require their domain expertise.\n\nFor a step-by-step guide, see the Code Modernization Playbook.\n\nProduct updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.",
    "readingTime": 4,
    "keywords": [
      "institutional knowledge",
      "cobol codebase",
      "cobol modernization",
      "business logic",
      "legacy code",
      "claude code",
      "systems",
      "analysis",
      "engineers",
      "components"
    ],
    "qualityScore": 1,
    "link": "https://claude.com/blog/how-ai-helps-break-cost-barrier-cobol-modernization",
    "thumbnail_url": "https://cdn.prod.website-files.com/68a44d4040f98a4adf2207b6/699ca2651dced743de2c747f_og_how-ai-helps-break-cost-barrier-cobol-modernization.jpg",
    "created_at": "2026-02-24T06:47:14.464Z",
    "topic": "tech"
  },
  {
    "slug": "everyone-in-ai-is-building-the-wrong-thing-for-the-same-reason",
    "title": "Everyone in AI is building the wrong thing for the same reason",
    "description": "Every AI founder I talk to is on an accelerating treadmill, burdened by a nagging suspicion that the entire industry is moving too fast in a direction that doesn't quite make sense, with no idea about how to get off.",
    "fullText": "Every AI founder I talk to is on an accelerating treadmill, burdened by a nagging suspicion that the entire industry is moving too fast in a direction that doesn't quite make sense, with no idea about how to get off.\n\nThere is an overwhelming feeling that if everyone stopped and thought about it for five minutes, they'd build something completely different; but it's paired with a total inability to articulate why they specifically haven't stopped.\n\nNobody stops, because nobody can afford to, and that feeling has a name. It's called Moloch, and it's the most important thing happening in AI that isn't on your roadmap.\n\nA new model drops. It's better at benchmarks. Every AI product company now has to integrate it (if they're a wrapper) or match it (if they're building a model) or explain to their investors why they didn't. Integration // parity takes engineering time, time that doesn't go toward the actual product differentiation that would create long-term value. But the company that doesn't integrate looks like it's falling behind, and looking like you're falling behind is fatal when your entire valuation rests on the assumption that you're on the frontier.\n\nEveryone catches up, everyone shifts resources, everyone ships the update. The net competitive position of every company is exactly where it was before, except they all engineering time on esoteria and none of them made their end-product meaningfully better for end-users.\n\nIt happens every few weeks now.\n\nThis is Moloch; and it'scoordination failure as competition. Every company makes an individually rational decision (keep up with the frontier or die) and the aggregate outcome is an industry that's moving incredibly fast in the direction of model capability, and barely at all in the direction of product value.\n\nThe race is real // the destination is fake.\n\nEvery AI product is converging on the same interface: a chat box. Maybe a chat box with some tool use. Maybe a chat box with some RAG. But at bottom, a text input and a text output, differentiated by branding and minor UX choices and which model is running underneath.\n\nThe chat box won because it ships fast, and Moloch rewards speed over design.\n\nBuilding a novel interaction paradigm takes months or years of research, iteration, user testing. During those months, fifteen competitors will ship chat-box wrappers with the latest model and capture the market's attention. Your investors will get nervous. Your board will suggest you \"ship something\" while you figure out the long-term vision. You'll ship the chat box. The long-term vision will die in a Notion doc nobody opens.\n\nEvery company faces this exact choice and picks the same outcome. The competitive structure selects against creativity. The companies that take time to build something new get outrun by the companies that ship the obvious thing fast. And the obvious thing is always the same thing, because the obvious thing is, by definition, the thing that doesn't need original thinking.\n\nMoloch doesn't want you to build something good.\n\nMoloch wants you to build something now.\n\nThe competitive dynamic is bad enough at the product level. At the fundraising level, it's catastrophic.\n\nAI companies are raising at valuations that require them to grow at rates that are only achievable by chasing the broadest possible market with the most generic possible product. A company that raises at a $500M valuation needs to show a path to billions in revenue, which means it can't afford to be a niche tool that does one thing brilliantly for a specific audience. It has to be a platform, horizontal, aimed at enterprise, built for no one in particular.\n\nEvery AI company is building the same enterprise platform with the same features targeting the same buyers, because the fundraising math requires it, because the valuation requires it, because the competitive environment requires raising at that valuation to attract talent, because the talent market requires it.\n\nNobody in this chain chose this outcome. Every individual decision was rational. The aggregate result is an industry producing fifty identical products, none of which solve any specific problem particularly well, all of which are locked in a feature war that generates zero cumulative value.\n\nThe companies that would build something different (smaller, more focused, more opinionated) can't raise the capital to compete. The companies that raise the capital are structurally forced into sameness. Moloch eats the variance, and variance is where all the actual value lives.\n\nTop AI researchers and engineers go to the companies that are on the frontier. The frontier is defined by model capability benchmarks. So companies invest disproportionately in model capabilty, because that's how you attract talent, even when the marginal return on model capability for their specific product is near zero.\n\nYour users don't need GPT-5 when they can barely use GPT-4 effectively. They need better UX and better integration into how they actually work. But the engineer who could build that wants to work on the model, because model work is what gets you your next job, because the next company is also optimizing for model capability, because they need to attract talent too.\n\nEvery company overspends on model capability and underspends on product quality. The structure forces it. The talent market punishes you for having the right priorities.\n\nThe startup advice I've heard for a decade+ is \"Ignore the noise. Focus on users. Build something good.\"\n\nCorrect in theory, nearly impossible in practice, because Moloch specifically punishes this strategy in the medium term.\n\nBuilding something good takes time, and time means you're not shipping. Not shipping means you look like you're losing, which means talent leaves and investors get nervous, and your competitors (who are shipping fast and bad) capture market position that's expensive to reclaim.\n\nThe window in which \"build something good\" pays off is long, two years, three years, maybe more. The window in which Moloch punishes you for it is immediate and continuous. Most companies don't survive the medium term to reach teh long term.\n\nThe founders who know this stay up at night.\n\nThey can probably see the right thing to build.\n\nThey can't survive building it.\n\nYou can't outrun Moloch. You can only build a structure it can't reach.\n\nIn practice, this means: don't raise more than you need. Don't hire faster than your product vision can absorb. Don't compete on the frontier unless the frontier is where your users live. Find a niche that's too small for the Moloch-captured companies to notice and go so deep into it that by the time they arrive, your product is years ahead in the dimension that matters to those users.\n\nBe small enough that the competitve pressure doesn't distort your priorities. Be opinionated enough that you can't be dragged into building the same thing as everyone else. Be capitally efficient enough that you don't need the growth rate that forces you onto the treadmill. Be weird enough that nobody can copy you without becoming you.",
    "readingTime": 6,
    "keywords": [
      "long-term vision",
      "chat box",
      "attract talent",
      "model capability",
      "talent market",
      "every ai",
      "product",
      "doesn't",
      "it's",
      "frontier"
    ],
    "qualityScore": 1,
    "link": "https://www.joanwestenberg.com/everyone-in-ai-is-building-the-wrong-thing-for-the-same-reason/",
    "thumbnail_url": "https://www.joanwestenberg.com/content/images/size/w1200/2026/02/ChatGPT-Image-Feb-23--2026--10_09_25-PM.png",
    "created_at": "2026-02-24T06:47:14.251Z",
    "topic": "tech"
  },
  {
    "slug": "the-coauthor-of-a-viral-research-report-says-bluecollar-jobs-wont-be-safe-from-an-aidriven-recession",
    "title": "The coauthor of a viral research report says blue-collar jobs won't be safe from an AI-driven recession",
    "description": "Alap Shah, coauthor of a viral report that theorizes how AI could trigger a recession by 2028, explains why there may only be \"one labor market.\"",
    "fullText": "The coauthor of an AI research paper is speaking out after his work triggered a global stock sell-off.\n\nCitrini, a firm focused on thematic equity investing, alongside Alap Shah, CEO of Littlebird.ai, theorized a future where, instead of transforming the economy in a positive way, the AI boom erases white-collar jobs and severely reduces the spending power of those workers, and eventually stunts economic growth.\n\nOn Monday, Shah told \"TBPN\" podcast hosts John Coogan and Jordi Hays that despite how well it seems to be going for blue-collar jobs at the moment in terms of growth and the lack of mass layoffs, these jobs won't be safe if white collar jobs go away because ultimately, there is only \"one labor market.\"\n\n\"Let's say in our scenario, we talk about 5% of folks might get fired in a couple of years,\" said Shah. \"Those 5%, if there aren't white collar jobs for them to relocate into, then they're going to have to move into the gig economy and the blue collar labor force.\"\n\n\"And so that puts pressure on the entire labor market, not just the white collar one,\" Shah added.\n\nShah and Citrini published a report on Sunday, written from a futuristic point of view set in 2028, that predicts a negative domino scenario triggered by the AI boom. The research theorizes that AI will kick off a mass white-collar layoff too quickly, which will then deal a blow to the metro housing and mortgage market, and eventually lead to a global stock sell-off and a widespread recession in all sectors. In this scenario, the paper said, AI growth could also lose momentum due to a lack of funding.\n\n\"The system turned out to be one long daisy chain of correlated bets on white-collar productivity growth,\" the paper theorizes. \"The November 2027 crash only served to accelerate all of the negative feedback loops already in place.\"\n\nShah elaborated on these concerns on \"TBPN.\" When asked what he thinks of the current growth in the health and education sectors, Shah said most of it could be spurred by government spending, which would go away if personal income declines.\n\n\"Those sectors continue to grow because government spending grows,\" said Shah. \"But again, gets very circular if government spending is coming primarily from taxes and primarily payroll taxes because the average worker pays a lot",
    "readingTime": 2,
    "keywords": [
      "stock sell-off",
      "white collar",
      "labor market",
      "collar jobs",
      "growth",
      "paper",
      "white-collar",
      "scenario",
      "sectors",
      "shah"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/viral-ai-report-warns-blue-collar-jobs-arent-recession-proof-2026-2",
    "thumbnail_url": "https://i.insider.com/699cedc6156648bc16a8c3c3?width=800&format=jpeg",
    "created_at": "2026-02-24T06:47:13.522Z",
    "topic": "finance"
  },
  {
    "slug": "humiliated-pentagon-pete-makes-desperate-lastditch-threat",
    "title": "Humiliated Pentagon Pete Makes Desperate Last-Ditch Threat",
    "description": "Pete Hegseth wants a top Pentagon contractor to know he really, really means it when he demands they abandon their cautious approach to the Defense Department’s AI systems. The defense secretary is se...",
    "fullText": "Pete Hegseth wants a top Pentagon contractor to know he really, really means it when he demands they abandon their cautious approach to the Defense Department’s AI systems.\n\nThe defense secretary is set to host Anthropic CEO Dario Amodei on Tuesday morning for what Axios reports is “likely to be tense meeting” on the military’s use of the company’s Claude software.\n\n“Anthropic knows this is not a get-to-know-you meeting,” as one defense official described it. “This is not a friendly meeting. This is a s--t-or-get-off-the-pot-meeting.”\n\nAmodei, who’s often warned of AI’s potential for misuse, has consistently pushed to frame his firm as a safety-conscious leader in the sector.\n\nHe has so far resisted pressure from Hegseth to remove safeguards on the Pentagon’s Claude-enabled programs unless the department agrees to wall off mass surveillance of citizens and research into weapons capable of firing without a human operator.\n\nTheir feud is understood to have escalated amid reports that Claude was used by the Pentagon in the Trump administration’s lightning invasion of Venezuela earlier in January.\n\nCritics decried that mission—which secured the capture of President Nicolas Maduro, who now faces narcoterrorism charges in New York federal court—as an all-out assault on the rules-based international order.\n\nAnthropic told Axios that “we are having productive conversations, in good faith,” with the Pentagon.\n\nDefense officials instead say “negotiations have shown no progress,” and are now “on the verge of breaking down.”\n\nEarlier this month, Hegseth warned he would consider certifying Anthropic as a “supply chain risk” if it did not yield to his demands.\n\nOfficials said that at the Tuesday meeting, the secretary now plans on issuing Amodei with an “ultimatum.”\n\n“The problem with Dario is, with him, it’s ideological,” one senior Defense Department source said. “We know who we’re dealing with.”\n\nThe Daily Beast has contacted the Defense Department and Anthropic for comment on this story.",
    "readingTime": 2,
    "keywords": [
      "demands",
      "secretary",
      "axios",
      "reports",
      "warned",
      "earlier",
      "defense",
      "anthropic",
      "hegseth",
      "pentagon"
    ],
    "qualityScore": 0.9,
    "link": "https://www.yahoo.com/news/articles/humiliated-pentagon-pete-makes-desperate-143424526.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/5iKaCrmkQtRG4e0MWPnySA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU7Y2Y9d2VicA--/https://media.zenfs.com/en/thedailybeast.com/abc632926635e77dfe95210d46e0addd",
    "created_at": "2026-02-24T06:47:13.163Z",
    "topic": "news"
  },
  {
    "slug": "arceeaitrinitylargepreview",
    "title": "Arcee-AI/Trinity-Large-Preview",
    "description": "We’re on a journey to advance and democratize artificial intelligence through open source and open science.",
    "fullText": "Trinity-Large-Preview is a 398B-parameter sparse Mixture-of-Experts (MoE) model with approximately 13B active parameters per token. It is the largest model in Arcee AI's Trinity family, trained on more than 17 trillion tokens and delivering frontier-level performance with strong long-context comprehension.\nTrinity-Large-Preview is a lightly post-trained model based on Trinity-Large-Base.\n\nMore details on the training of Trinity Large are available in the technical report.\n\nThe Trinity Large family consists of three checkpoints from the same training run:\n\nTrinity-Large-Preview uses a sparse MoE configuration designed to maximize efficiency while maintaining large-scale capacity.\n\nUse the main transformers branch or pass trust_remote_code=True with a released version.\n\nSupported in VLLM release 0.11.1+\n\nSupported in llama.cpp release b7061+\n\nSupported in the latest LM Studio runtime. Search for arcee-ai/Trinity-Large-Preview-GGUF in Model Search.\n\nTrinity-Large-Preview is released under the Apache License, Version 2.0.\n\nIf you use this model, please cite:",
    "readingTime": 1,
    "keywords": [
      "release supported",
      "sparse",
      "family",
      "training",
      "released",
      "model",
      "trinity-large-preview",
      "trinity",
      "version",
      "search"
    ],
    "qualityScore": 0.85,
    "link": "https://huggingface.co/arcee-ai/Trinity-Large-Preview",
    "thumbnail_url": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/arcee-ai/Trinity-Large-Preview.png",
    "created_at": "2026-02-24T01:09:43.737Z",
    "topic": "tech"
  },
  {
    "slug": "global-regulators-say-ai-image-tools-dont-get-a-free-pass-on-privacy-rules",
    "title": "Global regulators say AI image tools don't get a free pass on privacy rules",
    "description": ": Watchdogs warn models that can generate realistic images of people must comply with data protection laws",
    "fullText": "A global coalition of privacy watchdogs has fired a warning shot at the generative AI industry, saying companies churning out realistic synthetic images can't pretend that data protection rules don't apply.\n\nThe joint statement [PDF] signed by more than 60 regulators, including the UK Information Commissioner's Office (ICO) and Ireland's Data Protection Commission (DPC), boils down to a simple point: if your model can convincingly fake a person, you don't get to pretend data protection law doesn't exist.\n\n\"Recent developments – particularly AI image and video generation integrated into widely accessible social media platforms – have enabled the creation of non-consensual intimate imagery, defamatory depictions, and other harmful content featuring real individuals,\" said the signatories. \"We are especially concerned about potential harms to children and other vulnerable groups, such as cyberbullying and/or exploitation.\"\n\nThe warning lands weeks after the ICO and DPC opened formal probes into Elon Musk's xAI following reports that its Grok chatbot produced sexual images of real people without their consent.\n\nThe group says organizations dabbling in generative AI need to build safeguards from the start and think carefully about risks such as non-consensual imagery, misuse of someone's likeness, and potential harms to children – all areas where the tech has raced ahead of social norms and, in some cases, common sense.\n\nThe regulators stress that the law already covers this, and that firms don't get a free pass just because the content came from a machine.\n\nWilliam Malcolm, executive director of Regulatory Risk & Innovation at the ICO, said: \"People should be able to benefit from AI without fearing that their identity, dignity or safety are under threat. AI already plays a large role in all our lives, and everybody has a right to expect that AI systems handling their personal data will do so with respect. Responsible innovation means putting people first: anticipating the risks and building in meaningful safeguards to ensure autonomy, transparency, and control.\n\n\"Public trust is foundational to the successful adoption and use of AI. Joint regulatory initiatives like this show global commitment to high standards of data protection in AI systems and help provide regulatory certainty. We expect those developing and deploying AI to act responsibly. Where we find that obligations have not been met, we will take action to protect the public.\"\n\nThe joint statement on AI-generated imagery suggests that if companies want to keep pushing ever more realistic AI into everyday products, they should expect regulators to keep asking awkward questions about how it all works. ®",
    "readingTime": 3,
    "keywords": [
      "potential harms",
      "joint statement",
      "protection",
      "don't",
      "regulators",
      "imagery",
      "warning",
      "generative",
      "realistic",
      "images"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theregister.com/2026/02/23/privacy_watchdogs_ai_images/",
    "thumbnail_url": "https://regmedia.co.uk/2021/06/17/shutterstock_deepfake.jpg",
    "created_at": "2026-02-24T01:09:41.679Z",
    "topic": "tech"
  },
  {
    "slug": "openai-changed-its-mission-statement-6-times-in-9-years-it-finally-removed-the-word-safely-as-a-core-value-when-it",
    "title": "OpenAI changed its mission statement 6 times in 9 years. It finally removed the word “safely” as a core value when it restructured into a for-profit",
    "description": "It’s still committed to creating something that will benefit humanity. It’s unclear why it doesn’t want to do so “safely” anymore.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/23/openai-mission-statement-changed-restructuring-forprofit-business/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/02/GettyImages-2261852386_67c9e7-e1771877866392.jpg?resize=1200,600",
    "created_at": "2026-02-24T01:09:36.181Z",
    "topic": "business"
  },
  {
    "slug": "exclusivechinas-deepseek-trained-ai-model-on-nvidias-best-chip-despite-us-ban-official-says",
    "title": "Exclusive-China’s DeepSeek trained AI model on Nvidia’s best chip despite US ban, official says",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/exclusivechinas-deepseek-trained-ai-model-on-nvidias-best-chip-despite-us-ban-official-says-4520307",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1N00A_L.jpg",
    "created_at": "2026-02-24T01:09:35.908Z",
    "topic": "finance"
  },
  {
    "slug": "meta-ai-alignment-director-shares-her-openclaw-emaildeletion-nightmare-i-had-to-run-to-my-mac-mini",
    "title": "Meta AI alignment director shares her OpenClaw email-deletion nightmare: 'I had to RUN to my Mac mini'",
    "description": "Meta alignment director Summer Yue hooked OpenClaw up to her inbox. Then, the bot tried to delete her emails. Yue chalked it up to a \"rookie mistake.\"",
    "fullText": "Even the people hired to keep AI aligned can't always keep it in line.\n\nMeta's Summer Yue has been testing OpenClaw, a popular open-source AI agent capable of working 24/7 on behalf of its users. Then the bot went out of control, according to photos she posted on X. It ended up planning to delete her emails — and wouldn't stop after being directed to.\n\nIn her X post, Yue's OpenClaw bot said that it would \"trash EVERYTHING in inbox older than Feb 15 that isn't already in my keep list.\"\n\nYue tried multiple times to stop it. First, she messaged the AI agent, \"Do not do that.\" As the bot kept planning to delete her inbox, she wrote, \"STOP OPENCLAW.\"\n\n\"I couldn't stop it from my phone,\" Yue wrote in her post. \"I had to RUN to my Mac mini like I was defusing a bomb.\"\n\nNothing humbles you like telling your OpenClaw “confirm before acting” and watching it speedrun deleting your inbox. I couldn’t stop it from my phone. I had to RUN to my Mac mini like I was defusing a bomb. pic.twitter.com/XAxyRwPJ5R\n\nYue had previously tried OpenClaw on her \"toy inbox,\" where she wrote that the bot worked well and gained her trust. Testing it on her \"real inbox,\" the bot had to compact a much larger set of emails. She instructed it not to take action without approval, but OpenClaw lost the prompt during compaction, she wrote.\n\nYue joined Meta after its deal with Scale AI as a director of alignment of its Superintelligence Labs division, according to her LinkedIn profile. That's left some critics on social media confused: why would someone studying AI safety use such an AI agent that has previously drawn security concerns?\n\nUnlike other AI agents, OpenClaw does not need human approval to sign off on actions. It was also vibe-coded, and that combined with OpenClaw's level of system access has led some AI researchers to question the bot's security. AI researcher Gary Marcus told Business Insider that it was like \"giving full access to your computer and all your passwords to a guy you met at a bar who says he can help you out.\"\n\nOpenClaw creator Peter Steinberger, who has since been hired by OpenAI, recently said in a podcast interview that he was prioritizing building out additional security safeguards over ease-of-use features.\n\nThough it's not altogether surprising that someone working in AI would test out one of the buzziest AI products of the last year, some X users criticized Yue for hooking up OpenClaw to her real email in the first place. Ben Hylak, the cofounder of Raindrop AI and an Apple alum, posted a screenshot of her LinkedIn. \"This should terrify you,\" he wrote. \"What is Meta doing?\"\n\n\"Somewhat concerning that a person whose job is AI alignment is surprised when an AI doesn't precisely follow verbal instructions,\" another X user wrote.\n\nYou can't make this up. And this happened to a someone working on safety and alignment at META Superintelligence Lab.\n\nI don't get how people can let AI agents go on unsupervised like this.\n\nI'll stick to my supervised use of AI for now. pic.twitter.com/tWvQFd1NP5\n\nYue and Meta didn't respond to requests for comment from Business Insider.\n\nYue also isn't the only Meta employee to toy around with OpenClaw; creator Peter Steinberger said that Mark Zuckerberg played with the tool for a week and even sent feedback. While Meta courted him, Steinberger later accepted a job offer from OpenAI.\n\nIn the comments of Yue's post, someone asked about her AI alignment role. \"Were you intentionally testing its guardrails or did you make a rookie mistake?\" they wrote.\n\n\"Rookie mistake tbh,\" Yue responded. \"Turns out alignment researchers aren't immune to misalignment.\"",
    "readingTime": 4,
    "keywords": [
      "mac mini",
      "creator peter",
      "openclaw creator",
      "defusing bomb",
      "rookie mistake",
      "peter steinberger",
      "inbox",
      "alignment",
      "someone",
      "testing"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-ai-alignment-director-openclaw-email-deletion-2026-2",
    "thumbnail_url": "https://i.insider.com/699c640c2237a6a8f0cda21d?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:35.317Z",
    "topic": "finance"
  },
  {
    "slug": "freaking-out-about-ai-destroying-jobs-look-here-to-see-whats-really-happening",
    "title": "Freaking out about AI destroying jobs? Look here to see what's really happening.",
    "description": "Anthropic's Claude is allegedly going to destroy  a bunch of jobs. Unless you look at the startup's job listings.",
    "fullText": "I've seen some silly predictions about the future of work lately. If you're worried about AI destroying jobs, take a quick trip through Anthropic's careers page. It's one of my favorite spots on the internet right now.\n\nAnthropic CEO Dario Amodei is among the loudest voices warning that AI could erase many white-collar jobs. He's been especially outspoken about coding, given Anthropic's Claude Code is so powerful.\n\nA successful editor once told me to focus on what people and companies do, not what they say. So let's look at the jobs Anthropic is trying to fill right now.\n\nTop of the list: software engineering roles. The AI company with the most effective coding-automation machine on earth is looking to hire more than 100 coding experts.\n\nThis job posting stood out to me. Anthropic is hiring an iOS developer to build mobile apps. I thought you could just vibe-code apps these days? Apparently not.\n\nBoris Cherny, the creator of Claude Code, was asked about this recently. If this AI tool is writing most or all of Anthropic's code these days, why is the company still hiring so many software developers?\n\n\"Someone has to prompt the Claudes, talk to customers, coordinate with other teams, decide what to build next,\" Cherny replied on X. \"Engineering is changing and great engineers are more important than ever.\"\n\nLet that sink in. Software development is probably the job that is most disrupted by AI. Models have gotten good at coding because it's relatively easy to evaluate good versus bad outputs. That's because the code either works, or it doesn't, when deployed. This creates clear yes/no signals that are really valuable for training and fine-tuning new AI models.\n\nSo if Anthropic is still hiring more than 100 software engineers, then other types of jobs that are less impacted by AI should probably endure as well.\n\nProving my point, Anthropic has 32 finance jobs open, along with 33 in marketing, 16 in legal, and more than 100 in sales.\n\nI'm not just telling you this to make you feel better. When extreme AI job predictions are made, it leads to dumb ideas such as banning data center construction.\n\nSo, take a breath. AI is just a (powerful) tool to help humans get more work done. I'll leave you with final thoughts on this topic from Jensen Huang (who, by the way, is hiring humans like crazy).\n\nIn a recent interview, he argued that fears of mass job destruction often confuse the \"tasks\" involved in a job with the broader \"purpose\" of the role. AI, in his view, changes how tasks get done, but the purpose remains the same. And that means, the technology probably won't destroy jobs and could even increase demand for the people responsible for outcomes at work.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "jobs",
      "software",
      "hiring",
      "coding",
      "predictions",
      "it's",
      "engineering",
      "apps",
      "tool",
      "engineers"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-hiring-jobs-ai-supposedly-destroying-2026-2",
    "thumbnail_url": "https://i.insider.com/699cac4c2237a6a8f0cdad50?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:35.191Z",
    "topic": "finance"
  },
  {
    "slug": "anthropic-says-deepseek-and-other-chinese-ai-companies-fraudulently-used-claude",
    "title": "Anthropic says DeepSeek and other Chinese AI companies fraudulently used Claude",
    "description": "\"These campaigns are growing in intensity and sophistication,\" Anthropic said as part of its lengthy statement.",
    "fullText": "Anthropic says its Chinese competitors are stealing from the AI startup to gain an edge in the global AI race.\n\nOn Monday, Anthropic said three of China's biggest AI labs, DeepSeek, MiniMax, and Moonshot AI, were \"illicitly\" using Claude \"to improve their own models,\" through a process known as distillation.\n\n\"These campaigns are growing in intensity and sophistication,\" Anthropic said as part of its lengthy statement on Monday. \"The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.\n\nAnthropic said the distillation efforts were \"industrial-scale campaigns\" that included roughly 24,000 fraudulent Claude accounts that generated over 16 million exchanges \"in violation of our terms of service and regional access restrictions.\"\n\nDistillation is the process of training a less powerful model on the output of a more powerful model. The practice is a legitimate way that many US companies use to train their models for public release. Increasingly, major US companies are also stating that their Chinese competitors are improperly using the practice to steal their work.\n\nIn January 2025, OpenAI said DeepSeek may have \"inappropriately\" used OpenAI's outputs to train their models. Earlier this month, Google disclosed it had \"identified an increase in model extraction attempts or 'distillation attacks.'\"\n\n\"Competitors can use it to acquire powerful capabilities from other labs in a fraction of the time, and at a fraction of the cost, that it would take to develop them independently,\" Anthropic said on Monday.\n\nAnthropic disclosed remarkable detail about the extent to which DeepSeek, MiniMax, and Moonshot AI \"illicitly\" used their systems. Claude is not available for commercial access in China, though Anthropic said the rival labs found workarounds.\n\nAmong the notable findings, Anthropic said DeepSeek sought to create \"censorship-safe alternatives to policy-sensitive queries.\" The company also said it detected MiniMax's campaign \"while it was still active,\" giving them an in-depth look at what their competitor was doing.\n\n\"When we released a new model during MiniMax's active campaign, they pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system,\" Anthropic said.\n\nRepresentatives for DeepSeek, MiniMax, and Moonshot AI did not immediately respond to Business Insider's request for comment.\n\nBeyond cheating in the AI, Anthropic said improper distillation poses security risks because less-trained models may lack the proper safeguards, such as those to prevent the development of bioweapons.\n\nIn response to such distillation campaigns, Anthropic said it has built in \"behavioral fingerprinting systems,\" shares data with other AI companies on what to look out for, and continues to develop additional countermeasures.\n\nAnthropic CEO Dario Amodei recently wrote that leading models are approaching the point where, without proper safeguards, they could help direct someone in building a bioweapon.\n\nAmodei is also an outspoken advocate of US-export controls, a topic that divides some leading tech CEOs. Nvidia CEO Jensen Huang has repeatedly said that restricting US companies, including his own, from selling advanced chips to China won't curb China's AI advancements.\n\n\"Distillation attacks therefore reinforce the rationale for export controls: restricted chip access limits both direct model training and the scale of illicit distillation,\" Anthropic said.\n\nAnthropic has also faced allegations of using copyrighted material to train its models. In January, the Washington Post reported new details about an endeavor at the company called Project Panama, which the company reportedly described as \"our effort to destructively scan all the books in the world.\" Last year, Anthropic settled a class-action lawsuit brought by the authors and publishers of some of the books for $1.5 billion. As part of the settlement, the company didn't admit any wrongdoing.",
    "readingTime": 4,
    "keywords": [
      "chinese competitors",
      "deepseek minimax",
      "proper safeguards",
      "distillation attacks",
      "models",
      "model",
      "anthropic",
      "labs",
      "moonshot",
      "claude"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-deepseek-distillation-minimax-moonshot-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/699caeb8156648bc16a8bcf1?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:35.187Z",
    "topic": "finance"
  },
  {
    "slug": "from-strangers-to-lovers-how-this-startup-is-simulating-the-meetcute-with-ai",
    "title": "From strangers to lovers: How this startup is simulating the 'meet-cute' with AI",
    "description": "222 wants to grow beyond helping people make new friends. The AI-powered startup also wants to help people find love and maintain relationships.",
    "fullText": "It'd be nice to meet someone the old-fashioned way: Passing by them on the street, meeting at a restaurant, or sharing an exchange at a party.\n\nHowever, apps dominate the modern dating experience, replacing kismet meet-cutes with scrolling and DMs.\n\n222, a startup focused on relationship building with the help of AI, thinks it can bring back the spontaneity of making a new friend — or falling in love.\n\n\"We're trying to get as close as possible to you walking into someplace with other people there, and connection just naturally happens,\" CEO Keyan Kazemian told Business Insider.\n\nAt a high level, 222 matches people with strangers for experiences like dinner or a night out after they take a robust personality quiz, using machine learning models trained by its team and open-source AI models.\n\n\"When you walk in, all of those people are people we predict you're going to be able to have a good conversation with, and you'll like,\" COO Danial Hashemi said.\n\nWhen 222 launched in 2021, it began as a dinner series in Los Angeles for young adults emerging from the COVID-19 pandemic, helping them meet new people. Then the project grew into a company. It was accepted into Y Combinator, raised capital, moved to New York City, and launched a mobile app to spur in-real-life (IRL) socializing.\n\nWhile people who join 222 are often new to a city, Kazemian said, today they're pretty evenly split along why they're using the platform: they're either looking for new friends or potential romantic connections.\n\nSince putting out its app in 2024, the 222 experience has evolved. It's no longer just about meeting strangers, having a fun night, and forming new relationships.\n\n\"We're very focused on going beyond that,\" Kazemian said.\n\nThe platform is now digging deeper into connecting people after the first encounter that 222 initiates. It's helping plan follow-up hangs with friends and kindling a romantic connection by setting people up on a date if the feeling is mutual.\n\nAfter a 222 experience, the platform follows up to ask attendees whether they want to hang out or go on a date with anyone they met.\n\nOnce two people say they'd like to go on a date, \"we fully set up the next date for them,\" Hashemi said — reservation and all.\n\n\"If you think about just before dating apps, before all this stuff, how would people meet each other?\" Hashemi said. \"It would be you're in the same physical space with no preconceived notions of who this person is going to be.\"\n\nHashemi said that some of the \"joy\" of navigating how you feel about someone new in your life has \"gone away because of dating apps.\"\n\nMeeting in a way that feels more organic, such as a social gathering or through friends, has staying power. A 2025 survey of 7,000 US adults by health company Hims found that 77% of Gen Z met their partners IRL. Even Partiful, the Gen Z replacement for Facebook Events, is getting in on the IRL event-to-dating pipeline.\n\n222 thinks AI can make the meet-cute more accessible.\n\nWhat 222's founding team has zeroed in on is \"labeled data,\" Kazemian said, which comes from its users' feedback after they meet people.\n\nThe startup knows its first pairings may not be the ultimate match, which is why it encourages its subscribers — who pay $22 a month — to try multiple experiences. Its AI, in return, can curate better matches from 222's network.\n\nThere are layers of factors that contribute to that, 222's CTO Arman Roshannai said, such as similar music tastes or hometowns.\n\n\"The signal that we're training on is after you meet this person, you spend two hours getting dinner with them, and then you hang out for a few hours afterwards, were you guys actually a good match for each other?\" Roshannai added.\n\nKazemian added that training on this proprietary data from user feedback is a \"painstakingly difficult and long process,\" but gives the startup a \"technical moat\" to stand out from some competitors.\n\n222 isn't the only startup — or public company — betting that AI can improve how we connect.\n\nSeveral startups have launched with this premise and are raising millions, pitching matchmaking solutions that use AI to set people up. Meanwhile, Bumble, Tinder, and Facebook Dating are testing the AI waters and reimagining the swipe. Hinge's founder recently left the Match Group-owned dating app to build an AI dating alternative.\n\nAfter raising another $10.1 million from venture capital investors in 2025 — bringing the startup's total raised to $13.7 million — 222 is doubling down on hiring and expanding its product with tools that keep relationships going.\n\n222's next undertaking is to provide avenues for its users to reach their \"next offline moment\" together, so they can deepen those relationships.\n\nThe startup wants to be in the business of both creating relationships and maintaining them.\n\n\"They need to show up at the same place together,\" Kazemian said, be it a hangout, a date, or a restaurant reservation. \"We can help them figure out what that place is.\"",
    "readingTime": 5,
    "keywords": [
      "dating apps",
      "startup",
      "date",
      "relationships",
      "experience",
      "we're",
      "dinner",
      "launched",
      "they're",
      "platform"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/social-startup-222-raised-millions-expanding-dating-maintaining-relationships-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/6998cef8efb52c8bd0de99ae?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:35.184Z",
    "topic": "finance"
  },
  {
    "slug": "ibm-stock-sinks-as-anthropic-rolls-out-yet-another-disruptive-ai-tool",
    "title": "IBM stock sinks as Anthropic rolls out yet another disruptive AI tool",
    "description": "IBM shares tumbled after Anthropic unveiled a tool aimed at business that use a decades-old programming language",
    "fullText": "The AI-driven software sell-off that ravaged markets in early February isn't over, with IBM stock tanking on Monday as Anthropic unveiled another disruptive AI tool.\n\nIBM shares fell 13% after the maker of the Claude AI chatbot announced another new tool. In this case, the startup announced an update that can help reduce the cost of COBOL (Common Business-Oriented Language) systems used by many businesses.\n\nThe term may not be widely known outside the software industry, but it quickly sent IBM stock into a nosedive in the latest installment of the software sell-off.\n\nAnthropic announced the update in a blog post, laying out why COBOL matters and why people outside the tech community should care.\n\n\"COBOL is everywhere. It handles an estimated 95% of ATM transactions in the US,\" it wrote. \"Hundreds of billions of lines of COBOL run in production every day, powering critical systems in finance, airlines, and government. Despite that, the number of people who understand it shrinks every year.\"\n\nAmong its uses, the company said the new tool could \"Identify risks that would take human analysts months to surface.\" The new AI use case poses a potential threat to the kind of business data service that comprises a core part of IBM's business.\n\nAccording to the startup, Claude will allow companies to streamline their COBOL operations for a fraction of their previous cost, similar to the legal plugins the company rolled out early in the month that triggered the initial software sell-off.\n\n\"Legacy code modernization stalled for years because understanding legacy code cost more than rewriting it. AI flips that equation,\" Anthropic said in its post.\n\nTech stocks initially bounced back after Anthropic's legal plugin release spooked Wall Street and caused investors to rush to limit their legal tech exposure, but the market was selling off sharply again on Monday.\n\nEven before the Anthropic news hit IBM shares, the sector was tumbling earlier in the session as investors reacted to tariff uncertainty and a report making the rounds online that speculated about far-reaching negative impacts of AI.",
    "readingTime": 2,
    "keywords": [
      "ibm stock",
      "ibm shares",
      "legacy code",
      "software sell-off",
      "anthropic",
      "tool",
      "tech",
      "legal",
      "another",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ibm-stock-price-anthropic-ai-update-cobol-language-software-selloff-2026-2",
    "thumbnail_url": "https://i.insider.com/699cbbf5efb52c8bd0deb52c?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:35.032Z",
    "topic": "finance"
  },
  {
    "slug": "eys-chief-digital-officer-says-marketing-is-at-an-ai-inflection-point",
    "title": "EY's chief digital officer says marketing is at an AI 'inflection point'",
    "description": "Lou Cohen emphasizes AI's potential in marketing, urging marketers to leverage it for improved audience segmentation and ad efficiency.",
    "fullText": "Lou Cohen, EY's chief digital officer, said many marketers are not yet taking advantage of the benefits of artificial intelligence.\n\nCohen, who is also a professor at New York University, Yeshiva University, and Baruch College, said marketing is at an \"inflection point,\" with investment shifting from general digital innovation to AI transformation.\n\nCohen said that marketers who understand how to use AI in an assistive way, by focusing on what outcomes it delivers best, will access a deeper level of audience segmenting, targeting, and testing. He was interviewed for CMO Insider at Business Insider's studio in New York City.\n\nUltimately, Cohen said, the marketing function will embrace the new opportunity. \"Marketers, they're not afraid to try things,\" Cohen said. \"We're going to learn more from the things that we fail with and that don't work than the things that do.\"\n\nThe following transcript has been edited for clarity.\n\nWe are at an interesting inflection point. In today's marketing environment, you really need to understand how to make AI work for you; otherwise, you will end up working for it.\n\nThere are efficiency and operational gains to be had. But if you think about the outcomes that AI can enable from a marketing perspective, we could be smarter about how we segment our audiences for different campaigns. We could be more efficient in the ways our advertising runs. We could test more rapidly to get better-quality content in front of the right audiences at the right time in the right place.\n\nBut most marketing teams are not yet set up to take advantage of this potential. So the investments of the last 15 years in digital transformation are now shifting into AI transformation.\n\nIt's a bit unknown now. Marketers are not totally comfortable with this because we're so worried that it's going to hallucinate or give us something that isn't accurate. Marketers, they're not afraid to try things. We're going to learn more from the things that we fail with and that don't work than from the things that do.\n\nMy colleague came up with a great way to evaluate the quality of our content using AI. We can paste in an article that a partner of ours wrote, and it will give us recommendations on how to make that piece of content better. But we're never — I shouldn't say never — we're not likely to use content created by AI. But we certainly can use AI to enhance and give feedback to our content creators.\n\nHallucinations are real. The challenge is that as consumers of these technologies, we don't yet understand the difference between probabilistic and deterministic outcomes. Probabilistic is the likely correct response that the AI is trying to give us. Deterministic is \"one plus one equals two,\" and arguably, one plus one always equals two. \n\nWhen you're doing a search on Google or Bing, for example, you are getting a deterministic response. You're getting what it believes to be the likely to answer your question. Versus with the LLMs, the ChatGPTs, the Llamas, the Geminis of the world, you're getting a probabilistic response. The model is bringing a bunch of different sources together to determine the answer it thinks you should get based on your prompt.\n\nThat means if we were using these tools for their designed purpose, we'd still need search engines to just navigate to the things we're looking for, or to find the needles in the haystack of the internet. But LLMs give us a different opportunity. They can be assistants. That was some of the original idea behind these AI tools, to assist people in doing different tasks.\n\nI think of these LLMs more as marketing assistants to give me real-time ideas, feedback, or suggestions, rather than doing the task for me. That's a human putting AI to work to get better outcomes faster than if I were to just do it myself.",
    "readingTime": 4,
    "keywords": [
      "marketers they're",
      "marketing",
      "we're",
      "content",
      "outcomes",
      "digital",
      "transformation",
      "understand",
      "don't",
      "probabilistic"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-transformation-marketing-says-eys-lou-cohen-2026-2",
    "thumbnail_url": "https://i.insider.com/699b66c32237a6a8f0cd9cb5?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:34.931Z",
    "topic": "finance"
  },
  {
    "slug": "aigenerated-playlists-a-new-look-and-other-changes-coming-to-apple-music",
    "title": "AI-Generated Playlists, a New Look, and Other Changes Coming to Apple Music",
    "description": "The next big iPhone update will bring new features to the Apple Music app.",
    "fullText": "Your iPhone is likely currently running iOS 26.3 (assuming you've been keeping up with the operating system updates), but there's another version currently in the works over at Apple HQ. Right now, beta testers have their hands on iOS 26.4, which is shaping up to be a much bigger update than the last. Among the changes, Apple is debuting end-to-end encryption for RCS chats, so texts with Android users will no longer be insecure, and the Reminders app is getting an \"Urgent\" section for any entries you've labeled as such.\n\nBut perhaps no part of iOS is getting a larger update with 26.4 than Apple Music—both the app, and Apple's paid subscription service. Apple seems to have decided that Music needed a bit of a facelift, as well as some quality of life changes that will make the app and service easier to use. Whether you use Apple Music for streaming or you rely on it to store your digital library, you're going to notice the updates when iOS 26.4 drops in the near future. Some of these features will be free and some only available to paid subscribers; I've reached out to Apple to confirm which is which, and I will update this piece if I hear back.\n\nDo you like making playlists, or do you know someone who does? The robots are coming for your hobby, too. With iOS 26.4, Apple Music is rolling out \"Playlist Playground,\" a new feature that lets you generate playlists from natural language prompts. In layman's terms, that means you tell the AI what kind of music you want to hear, and it will generate a playlist from that request. That could something hyper-specific, like \"Taylor Swift country tracks,\" or something more general, like \"morning coffee vibes.\" The AI will choose 25 songs it thinks match your query. If it doesn't get it quite right, you can ask it to make changes, and you can change things yourself, like the playlist's title, description, and cover image.\n\nI'm interested to try this out, if for no other reason than music discovery: I like Apple Music's curated playlists already, but I am intrigued as to whether asking Apple's AI to select certain types of songs for me will help me find new music any better than the platform's human curators. I also don't think this will stop me from making my own playlists, or looking for playlists from friends. Sure, maybe the AI is good at picking 25 songs that match a specific theme, but there is an art to hand-picking tracks that work well together—plus, it's just fun.\n\nApple is far from the first company to roll out such a feature. YouTube Music recently launched something similar, while Spotify has two different AI playlist features (AI Playlist and Prompted Playlist) available on its platform.\n\nWhen you start exploring Apple Music after updating to iOS 26.4, you'll likely notice something right away: The UX, which is normally white or black (whether your iPhone is in light or dark mode) now matches the color scheme of the artwork for the album or playlist you're checking out. The effect is especially cool when the album art supports full-screen motion, like the following:\n\nApple has made some divisive design decisions in recent years, but I think this change is going to be a crowd pleaser. The difference between the current design on iOS 26.3 and the new look is stark, and, while there's nothing wrong with how things stand now, it already looks super dated next to the full-screen color matching designs.\n\nSpeaking of playlists, you can now add songs to multiple at once—just in case you still need some human intervention when it comes to these playlists. When you go to add a song to a playlist, you'll notice a new button in the bottom right. Tap it, and Apple Music opens up the ability to select multiple playlists at once, and send the song to all of them. It's a small change, but a helpful one, especially if you frequently add new music to more than one playlist at a time. I could see myself using this to add a song to my personal new discoveries playlist, as well as a shared playlist of new music I keep with friends.\n\nNot all music is made to active listening. If you use Apple Music for background music, especially when sleeping, working, or zoning out, you might be interested in the new \"Ambient Music\" widget, which lets you launch one of four different ambient playlists from the Home Screen: Sleep, which plays \"Sleep Sounds;\" Chill, which plays \"Today's Chill;\" Productivity, which plays \"Productivity;\" or Wellbeing, which plays \"Pure Meditation.\"\n\nI still can't quite shake the habit of relying on YouTube for my \"focus music\" needs, especially since these tracks mess with my Apple Music algorithms. But it might make sense to start relying on the platform I actually pay for when I want music to work or fall asleep to—unless that music is only available elsewhere.\n\nApple Music is also making it easier to listen to music outside of the app. The platform is rolling out a \"Concerts Near You\" section, which shows you artists playing in your area. You can see popular artists and their concert dates, as well as shows that are coming up this week. You can sort by both date and genre, and you can update the location when you want to know where shows are going to be in different areas.",
    "readingTime": 5,
    "keywords": [
      "apple music",
      "add song",
      "playlists",
      "songs",
      "notice",
      "playlist",
      "tracks",
      "platform",
      "iphone",
      "currently"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/apple-music-changes-ios-26-4-update-ai-playlists?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KJ5R94D9EARGJ2K1WE2K9CW5/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-24T01:09:34.088Z",
    "topic": "tech"
  },
  {
    "slug": "lifo-browsernative-os-for-ai-sandboxing",
    "title": "Lifo – Browser-Native OS for AI Sandboxing",
    "description": "Browser-native OS for AI sandboxing. Execute AI-generated code safely in the browser with 60+ Unix commands, virtual filesystem, and $0 infra cost.",
    "fullText": "Unix/Linux reimagined where the browser IS the kernel and Web APIs ARE syscalls. 60+ commands, bash-like shell, virtual filesystem, and IndexedDB persistence.\n\nTry commands like help, ls, echo \"hello\", or neofetch\n\nAI-generated code needs to execute somewhere safe. As agents write and run code autonomously, secure sandboxing is no longer optional — it's a requirement.\n\nSpinning up cloud VMs for every execution is costly and slow. Browsers already have powerful, isolated environments — why not use them?\n\nVibecoding in the browser needs filesystem, shell, and process APIs. The browser already has it all — Lifo just maps them to familiar interfaces.\n\nVMs are slow to load because they secure and provision resources. Lifo runs instantly — it doesn't allocate resources, it just maps browser APIs.\n\nA library that gives browser APIs a Linux-like interface for your agents.\n\nWraps IndexedDB, Fetch, and Web Workers behind familiar POSIX/Unix-style interfaces. Work with files, processes, and networking using APIs you already know.\n\nShims for fs, path, process, and child_process. Run Node-style scripts directly in the browser.\n\nLifo is a browser library that provides Linux-like APIs on top of existing Web APIs. It runs entirely in your browser tab — no backend, no VM, no containers.\n\nFamiliar Unix tools running entirely in the browser\n\nOnly JavaScript and TypeScript can run. No compiled binaries.\n\nFiles live in IndexedDB, not on a real disk. Storage quotas vary by browser.\n\nProcesses share the main JS thread. Web Workers are available for parallelism.\n\nIndexedDB quotas depend on the browser and available disk space.\n\nNetworking goes through the Fetch API. No raw TCP/UDP socket access.\n\nIf you need full VM-level isolation, use a cloud sandbox instead.\n\nFull git support via isomorphic-git. Clone, commit, push, and pull directly in the browser.\n\nTunnel local ports to real domains. Run dev servers and access them from anywhere.\n\nRun frameworks like Next.js, Express, Expo, Hono, and OpenClaw directly in the browser sandbox.\n\nPython, ffmpeg, ImageMagick, SQLite, and Postgres compiled to WebAssembly for native-speed execution.",
    "readingTime": 2,
    "keywords": [
      "web apis",
      "browser apis",
      "indexeddb",
      "lifo",
      "directly",
      "commands",
      "shell",
      "filesystem",
      "code",
      "needs"
    ],
    "qualityScore": 1,
    "link": "https://lifo.sh",
    "thumbnail_url": "https://lifo.sh/opengraph-image?2ef32e3ec85fb71b",
    "created_at": "2026-02-23T18:49:06.174Z",
    "topic": "tech"
  },
  {
    "slug": "baudbot-alwayson-ai-assistant-for-dev-teams",
    "title": "Baudbot: Always-on AI assistant for dev teams",
    "description": "Always-on AI assistant for dev teams - like having Claude Code for your whole team on Slack 🤖  - GitHub - modem-dev/baudbot: Always-on AI assistant for dev teams - like having Claude Code for your ...",
    "fullText": "modem-dev\n\n /\n\n baudbot\n\n Public\n\n Always-on AI assistant for dev teams - like having Claude Code for your whole team on Slack 🤖 \n\n License\n\n MIT license\n\n 59\n stars\n\n 5\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n modem-dev/baudbot",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/modem-dev/baudbot",
    "thumbnail_url": "https://opengraph.githubassets.com/29a7f2b11bf7cc547cce79c03c545ce402a7c7a58d79bc18709d97118c07d9f4/modem-dev/baudbot",
    "created_at": "2026-02-23T18:49:05.978Z",
    "topic": "tech"
  },
  {
    "slug": "zoye-the-first-ai-native-workspace-for-all-your-business-tools",
    "title": "Zoye – The First AI Native Workspace for All Your Business Tools",
    "description": "Tasks, CRM, Deals, Documents, Calendar, Accounting, Reports, Automations & Teams - managed by a personal AI assistant that works for you.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://zoye.io/",
    "thumbnail_url": "https://zoye.io/og-image.png",
    "created_at": "2026-02-23T18:49:05.387Z",
    "topic": "tech"
  },
  {
    "slug": "tariff-turmoil-is-back-heres-the-investing-playbook-for-the-latest-chapter-in-trumps-trade-war",
    "title": "Tariff turmoil is back. Here's the investing playbook for the latest chapter in Trump's trade war.",
    "description": "New tariffs chaos means investor should focus on AI and sectors of the S&P 500 that have outperformed this year, investing experts told Business Insider.",
    "fullText": "President Donald Trump's trade war has entered a new, chaotic chapter that's raised uncertainty for investors.\n\nMarkets are going through a fresh bout of volatility after the Supreme Court struck down the majority of Trump's tariffs — a move that seems to have marked the start of a new, confusing chapter in the trade saga.\n\nUS stocks initially rose after the tariff ruling was announced on Friday, even as Trump threatened to impose a fresh 10% global tariff. Indexes tumbled on Monday, however, as traders reacted to Trump's follow-up announcement that he would hike global tariffs to 15%, adding to the mayhem.\n\nUncertainty over tariffs has long hung over the market, and it's unclear how the new changes to tariffs will play out, particularly since it's unclear how the fight over tariff refunds might end, according to Art Hogan, the chief market strategist at B. Riley Wealth Management.\n\n\"I just don't think we've removed much of the uncertainty around tariffs by the Supreme Court ruling and quick pivot to Plan B,\" Hogan told Business Insider. \"I think that remains a headwind.\"\n\nHere's how market pros think investors should position as the trade war takes its latest turn.\n\nThe tariffs story is largely a distraction from the market's overarching focus on AI, according to Peter Berezin, the chief market strategist at BCA Research.\n\n\"I think the tariffs have become a bit of a sideshow at this point,\" Berezin said. \"I think it's really just an AI story at this point for the market.\"\n\nBerezin said Trump is unlikely to raise tariffs beyond what he announced on Saturday. The president also looks like he's laser-focused on his affordability push ahead of he midterm elections.\n\nFurthermore, Trump likely knows that tariffs are inflationary and is looking to avoid that, as much as members of his team have downplayed price increases from tariffs, Berezin said. He pointed to a recent Financial Times report that Trump was planning to roll back some tariffs on steel and aluminum goods, which the White House later denied.\n\nHogan said he thinks the recent tariff ruling doesn't change the status quo in markets. Even with Trump's 15% global tariff, the US's new trade policy doesn't look like it will change the investing landscape much from before, he said.\n\nSome nations, like China and Brazil, were facing much larger tariffs prior to the Supreme Court ruling, and have come out as \"winners\" under the new tariff scheme, Berezin said, though he didn't believe it changed the investing outlook from a sectoral perspective.\n\nHogan advised investors to stick to sectors that have recently outperformed after lagging the broader market for several years, including materials, industrials, and energy stocks. Those areas have made up the best-performing sectors of the S&P 500 so far this year.\n\nBest-performing sectors of the S&P 500, year-to-date\n\nPrecious metals and commodities also look like they could become winners from the new trade chaos, according to David Morrison, a senior market strategist at Trade Nation. That's because investors flock to hard assets and other safe havens when they see \"tariff-driven market stress,\" he told Business Insider.\n\nDespite enduring a historic sell-off in late January, precious metals and commodities have also posted strong gains so far this year.\n\nHogan pointed to Friday's short-lived pop in shares of consumer-facing companies that are major importers of foreign goods, such as furniture and apparel retailers.\n\nSome investors may be feeling optimistic about corporate stimulus if the US refunds tariff payments. But refunds are uncertain, and should they come, it's likely that the government will drag out the process, Hogan said.\n\n\"I just don't know how resilient that will be in the face of ongoing litigation about the government paying this back,\" he added of the recent rebound in the consumer sector.\n\nJeff Buchbinder, the chief equity strategist at LPL Financial, added that the firm wouldn't chase stock jumps in import-heavy consumer retailers.\n\n\"We would fade the stock market bounce in tariff losers,\" Buchbinder wrote. \"Among tariff losers, our preference would be to play homebuilders, industrials, and technology hardware/semiconductors over apparel retailers and automakers.\"",
    "readingTime": 4,
    "keywords": [
      "court ruling",
      "precious metals",
      "best-performing sectors",
      "apparel retailers",
      "it's unclear",
      "trade war",
      "tariff losers",
      "market strategist",
      "chief market",
      "supreme court"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/where-to-invest-trump-global-tariffs-supreme-court-trade-war-2026-2",
    "thumbnail_url": "https://i.insider.com/699c60bf156648bc16a8b0f5?width=1200&format=jpeg",
    "created_at": "2026-02-23T18:49:04.739Z",
    "topic": "finance"
  },
  {
    "slug": "a-research-report-warning-of-an-aidriven-recession-and-stock-crash-has-gone-viral-and-spooked-investors",
    "title": "A research report warning of an AI-driven recession and stock crash has gone viral and spooked investors",
    "description": "A research report framed as a hypothetical look back from 2028 was making the rounds online and adding to fresh jitters in software stocks.",
    "fullText": "The AI trade has boosted the market for years now, with its rapid growth being felt by the broader economy as companies keep spending. However, one research firm thinks the AI story will not end well for investors or everyday Americans.\n\nStocks tumbled on Monday after a report circulating online from Citrini Research raised fresh fears about the impact of AI. The Dow was down by more than 800 points around midday, and the Nasdaq fell more than 1%. Software stocks were among the biggest losers, with names including AppLovin, Asana, DocuSign, and Zscaler down sharply.\n\nCitrini, a firm focused on thematic equity investing, theorized about the long-term impact of the AI boom, laying out a predictive scenario in which the technology continues to expand but ultimately proves detrimental to the broader economy.\n\nIn Citrini's hypothetical scenario, written as a look back from 2028, the AI explosion leads to a plunge in white-collar employment and ultimately to a stock market crash.\n\nThe scenario begins with a question:\n\n\"What if our AI bullishness continues to be right...and what if that's actually bearish?\"\n\nThe answer is different from many other bearish AI takes. When finance or economics pros express concern about the stability of the AI boom, they typically focus on problems with infrastructure and the stability of the AI buildout.\n\nBut Citrini's analysis assumes that the AI boom will continue, just not in a way that helps transform the economy, nor in the positive way AI bulls like Elon Musk have touted. In their scenario, the rise of AI continues driving white-collar layoffs, severely reducing the spending power of those workers and lowering economic growth in the process.\n\n\"This would have been manageable if the disruption remained contained to software, but it didn't,\" Citrini stated. \"By the end of 2027, it threatened every business model predicated on intermediation. Swaths of companies built on monetizing friction for humans disintegrated.\"\n\nIn this scenario, AI progress is quickly felt in the housing market, as white-collar workers in expensive metro areas are no longer able to afford homes, which has a knock-on effect in the mortgage market.\n\nAll these negative events ultimately lead, in Citrini's imagined scenario, to a stock market crash, with the S&P 500 cratering by 38% from its October 2026 peak.\n\n\"The system turned out to be one long daisy chain of correlated bets on white-collar productivity growth,\" the note said. \"The November 2027 crash only served to accelerate all of the negative feedback loops already in place.\"\n\nA core takeaway from the note is the importance of differentiating between the market and the economy, particularly when the market is booming. Markets applaud productivity and growing margins, the things we see when companies announce large-scale job cuts.\n\nThe broader economy, though, depends on consumer spending, which depends on wage growth keeping pace with rising prices. When these two become disconnected, even a booming stock market driven by a force like AI can't hide the negative impact that comes from diminished white-collar spending power.\n\nCitrini frames its scenario as a warning, implying that investors should take action now and not be lulled into a sense of security by the strength of the AI trade.\n\n\"As investors, we still have time to assess how much of our portfolios are built upon assumptions that won't survive the decade,\" it concludes. \"As a society, we still have time to be proactive.\"",
    "readingTime": 3,
    "keywords": [
      "broader economy",
      "stock market",
      "market crash",
      "scenario",
      "white-collar",
      "growth",
      "investors",
      "impact",
      "boom",
      "ultimately"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/stock-market-crash-ai-boom-recession-citrini-research-layoffs-jobs-2026-2",
    "thumbnail_url": "https://i.insider.com/699c864e2237a6a8f0cda71a?width=1200&format=jpeg",
    "created_at": "2026-02-23T18:49:04.626Z",
    "topic": "finance"
  },
  {
    "slug": "software-stocks-are-tanking-the-market-again-as-ai-and-tariff-uncertainty-spook-traders",
    "title": "Software stocks are tanking the market again as AI and tariff uncertainty spook traders",
    "description": "Major indexes tumbled as traders digested recent trade war updates and as fresh AI fears percolated on Wall Street.",
    "fullText": "Fears of a software apocalypse are pummeling markets again to start the week.\n\nUS stocks dropped on Monday as traders reacted to new tariff uncertainty, while software in particular was pummeled by renewed fears of AI disruption prompted by a downgrade of Salesforce stock and a note making the rounds that theorized the AI boom could ultimately lead to a white-collar recession and a stock crash.\n\nMajor indexes lost more than 1% as investors continued to digest recent trade policy changes, including President Donald Trump's announcement over the weekend that he would hike global tariffs to 15%. The announcement came a day after the Supreme Court knocked down most of his earlier tariffs.\n\nThe Dow lost more than 800 points as selling ramped up around midday.\n\nHere's where US indexes stood around noon ET on Monday:\n\n\"Tariff uncertainty reigned this morning, pushing stocks to early losses and raising volatility on Wall Street,\" Joe Mazzola, the head trading and derivatives strategist at Charles Schwab, wrote in a note on Monday.\n\nInvestors also turned a more critical eye toward the software sector. The iShares Expanded Tech-Software Sector ETF, which tumbled into a bear market earlier this year, was down another 5%. The fund is now down nearly 30% in 2026.\n\nHere were some of the notable moves on Monday:\n\nChatter about an AI-fueled meltdown picked up on Monday after the firm Citrini Research published a report outlining a hypothetical 2028 scenario in which the AI boom could trigger a recession and a stock market crash in the next couple of years.\n\n\"The system turned out to be one long daisy chain of correlated bets on white-collar productivity growth,\" the note said. \"The November 2027 crash only served to accelerate all of the negative feedback loops already in place.\"\n\nThe fresh AI jitters also brought capex spending back into the spotlight, with investors awaiting a key update this week in the form of Nvidia's coming earnings report due out on Wednesday.\n\n\"I think it's really just an AI story at this point for the market,\" Peter Berezin, the chief market strategist at BCA Research, told Business Insider, referring to concerns about large capex spending despite unclear monetization plans at some firms. \"It's a question of whether investors are going to revolt against all the massive spending on AI data centers that we're having now.\"\n\nMark Hackett, the chief markets strategist at Nationwide, attributed much of the volatility to \"continued skepticism\" over AI and fresh tariff uncertainty.\n\n\"Hedge funds have aggressively moved to the sidelines, with net sales of US equities at the fastest pace since last March this month, and net leverage tracking to be the second largest sharpest monthly decline in a decade,\" he wrote in a note on Tuesday.\n\nInvestors will likely turn their attention to key tech earnings this week, with Nvidia and Salesforce reporting their results on Wednesday, Schwab's Mazzola said.",
    "readingTime": 3,
    "keywords": [
      "tariff uncertainty",
      "note",
      "market",
      "stock",
      "crash",
      "strategist",
      "fears",
      "markets",
      "stocks",
      "boom"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/stock-market-today-selloff-tariffs-trump-software-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/699c7b772237a6a8f0cda511?width=1200&format=jpeg",
    "created_at": "2026-02-23T18:49:04.525Z",
    "topic": "finance"
  },
  {
    "slug": "a-data-center-boss-hit-back-at-ai-skeptics-like-michael-burry-and-jim-chanos-with-a-nod-to-superman",
    "title": "A data center boss hit back at AI skeptics like Michael Burry and Jim Chanos with a nod to 'Superman'",
    "description": "HIVE's Frank Holmes said on an earnings webcast that negativity around AI reminded him of Lex Luthor's army of keyboard-bashing monkeys in \"Superman.\"",
    "fullText": "A data center executive said the recent backlash against the AI boom, led by the likes of Michael Burry and Jim Chanos, reminded him of Lex Luthor's army of keyboard-bashing monkeys in the newest \"Superman\" movie.\n\nFrank Holmes, the executive chairman of HIVE Digital Technologies, played a GIF from the movie during the company's earnings webcast last week. It showed one of the monkeys furiously typing the message: \"ONLY AN IDIOT WOULD BACK SUPERMAN.\"\n\nLuthor, Superman's archnemesis, memorably uses the monkeys to flood social media with hateful memes, hashtags, and vitriol. Mimicking real-life troll farms, the disinformation campaign succeeds in turning the world against the hero.\n\nHolmes also flashed up a slide with pictures of Burry and Chanos, followed by another filled with recent news headlines about their warnings.\n\n\"Jim Chanos comes out and: 'Short the bitcoin miners, short Nvidia, short the [high-performance computing], the hyperscalers, there's too much debt,'\" Holmes said.\n\n\"And Michael Burry is coming out,\" he continued. \"He came out a couple weeks ago. Again, he's short this market. And it really starts to grow, this negativity on the ecosystem.\"\n\nCommenting as the GIF played, Holmes said it was \"about Superman's credibility being destroyed.\"\n\n\"Well, the same thing happened out of nowhere, all this negativity was showing up on Instagram, and YouTube, and X,\" he added.\n\nHIVE builds and runs data centers powered by clean energy in Canada, Sweden, and Paraguay. Holmes dismissed negative news about Big Tech companies spending too much on data centers, saying the \"backup demand is just immense,\" and short-term traders were stirring the pot to help their short bets.\n\nBurry, the fund manager-turned-writer of \"The Big Short\" fame, reiterated his concerns about AI overinvestment in X posts over the weekend.\n\nA question I have for $ORCL, $GOOG, $META, $MSFT, $AMZN, $NVDA, $CAT, and all the rest, “When does the spending for AI data center buildout actually end?”\nIt is consuming all your cash flow, you are borrowing, you are financing in ways you never have, apparently because it is so… pic.twitter.com/fytMYDH942\n\n\"A question I have for $ORCL, $GOOG, $META, $MSFT, $AMZN, $NVDA, $CAT, and all the rest, \"When does the spending for AI data center buildout actually end?\" he queried.\n\n\"It is consuming all your cash flow, you are borrowing, you are financing in ways you never have, apparently because it is so urgent, because it scales? But if it scales, when does it end?\" he added.\n\nChanos also took to X over the weekend to question whether tech giants would see returns on their huge outlays.\n\n\"But are they ALL going to be extremely profitable?\" he asked. \"Because each one of the AI companies and hyperscalers is spending money like they will be. They are betting that it won't be \"winner take all\" like search. We'll see.\"",
    "readingTime": 3,
    "keywords": [
      "orcl goog",
      "goog meta",
      "meta msft",
      "msft amzn",
      "amzn nvda",
      "nvda cat",
      "michael burry",
      "cash flow",
      "center buildout",
      "monkeys"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/data-center-ai-michael-burry-chanos-superman-social-media-2026-2",
    "thumbnail_url": "https://i.insider.com/692ef04371107c9f34572830?width=1200&format=jpeg",
    "created_at": "2026-02-23T18:49:04.524Z",
    "topic": "finance"
  },
  {
    "slug": "machineauthopen-source-google-login-for-your-ai-agent",
    "title": "MachineAuth:open source Google login for your AI Agent",
    "description": "Secure OAuth 2.0 authentication for AI agents and machine-to-machine communication. - mandarwagh9/MachineAuth",
    "fullText": "mandarwagh9\n\n /\n\n MachineAuth\n\n Public\n\n Secure OAuth 2.0 authentication for AI agents and machine-to-machine communication.\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mandarwagh9/MachineAuth",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mandarwagh9/MachineAuth",
    "thumbnail_url": "https://opengraph.githubassets.com/3769491aed1aa3059f73f8038d40cecae3e78306a4cf9857b2a06fc63cebea65/mandarwagh9/MachineAuth",
    "created_at": "2026-02-23T18:49:04.411Z",
    "topic": "tech"
  },
  {
    "slug": "sam-altman-says-concerns-of-chatgpts-energy-use-are-overblown-it-also-takes-a-lot-of-energy-to-train-a-human",
    "title": "Sam Altman says concerns of ChatGPT's energy use are overblown: 'It also takes a lot of energy to train a human'",
    "description": "\"It takes, like, 20 years of life, and all of the food you eat during that time before you get smart,\" Sam Altman said.",
    "fullText": "Sam Altman is pushing back on the idea that ChatGPT consumes too much energy.\n\n\"One of the things that is always unfair in this comparison is people talk about how much energy it takes to train an AI model relative to how much it costs a human to do one inference query,\" Altman told The Indian Express last week on the sidelines of a major AI summit. \"But it also takes a lot of energy to train a human.\"\n\nAltman suggested it's not an apples-to-apples comparison, arguing that it's unfair to discount the years spent nurturing and educating someone to be capable of making their own inquiries.\n\n\"It takes a lot of energy to train a human,\" he said, prompting some laughter in the crowd. \"It takes, like, 20 years of life, and all of the food you eat during that time before you get smart.\"\n\nAltman said the clock really began thousands of years ago.\n\n\"It took, like, the very widespread evolution of the 100 billion people that have ever lived and learned not to get eaten by predators and learned how to, like, figure out science or whatever,\" he said.\n\nAltman also called out what he said were \"totally insane\" claims on the internet that OpenAI is guzzling down water to power ChatGPT.\n\n\"Water is totally fake,\" Altman said, when asked about concerns AI companies use too much water. \"It used to be true, we used to do evaporative cooling in data centers, but now that we don't do that, you know, you see these like things on the internet where, 'Don't use ChatGPT, it's 17 gallons of water for each query' or whatever.\"\n\nIn June, Altman said that the average ChatGPT query consumes roughly the amount of energy needed to power a lightbulb for a few minutes.\n\n\"People are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes,\" he wrote on X.\n\nAltman said it is fair as a whole to point out the AI industry's overall energy consumption because of the large growth in usage. He said it's why he and other AI CEOs have pushed alternative energy sources like solar, wind, and nuclear.\n\nUnlike other CEOs, namely xAI's Elon Musk, Altman is dismissive of the idea that space-based data centers are realistic in the next decade, a concept that some companies have floated as a way to reduce energy consumption.\n\nOutside of OpenAI, Altman is a major investor in nuclear energy. He previously served as chairman of Oklo, a nuclear energy startup, and has been a major backer of Helion, which plans to build what it calls \"the world's first fusion power plant\" in Washington state.\n\nIn the US, data center energy consumption is becoming a major topic. Last month, President Donald Trump said he was working with tech companies on \"a commitment to the American people\" to ensure that citizens don't pay higher energy bills because of a nearby data center.\n\nConsulting firm McKinsey & Company estimated last year that data centers could account for 14% of total power demand in the US by 2050.",
    "readingTime": 3,
    "keywords": [
      "chatgpt query",
      "train human",
      "energy consumption",
      "nuclear energy",
      "it's",
      "altman",
      "centers",
      "don't",
      "idea",
      "consumes"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-ai-energy-use-training-human-water-chatgpt-2026-2",
    "thumbnail_url": "https://i.insider.com/699c72782237a6a8f0cda3cc?width=1200&format=jpeg",
    "created_at": "2026-02-23T18:49:04.406Z",
    "topic": "finance"
  },
  {
    "slug": "aws-suffered-at-least-two-outages-caused-by-ai-tools",
    "title": "AWS suffered 'at least two outages' caused by AI tools",
    "description": "Amazon just speed-ran a season of 'Silicon Valley'",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.tomsguide.com/computing/aws-suffered-at-least-two-outages-caused-by-ai-tools-and-now-im-convinced-were-living-inside-a-silicon-valley-episode",
    "thumbnail_url": "https://cdn.mos.cms.futurecdn.net/XbfUHZiivHHGvFGY84e2yb-2560-80.jpg",
    "created_at": "2026-02-23T18:49:03.846Z",
    "topic": "tech"
  },
  {
    "slug": "ai-is-upending-marketing-on-two-fronts",
    "title": "AI Is Upending Marketing on Two Fronts",
    "description": "Artificial intelligence is driving two overlapping shifts that are reshaping marketing. First, conversational AI is displacing websites and traditional search as the way people learn about products, shrinking traffic, narrowing choice, and forcing companies to rethink visibility when answers are generated rather than clicked. This transition—from search engine optimization to generative engine optimization—favors brands whose information is structured, trusted, and easy for AI systems to synthesize. Second, AI agents are beginning to act as buyers, making purchasing decisions on behalf of humans. As algorithms increasingly evaluate options and transact, marketing must adapt to a world in which the customer may be a machine with its own decision logic, requiring leaders to redesign content, infrastructure, and strategy for both human and algorithmic audiences.",
    "fullText": "AI Is Upending Marketing on Two Fronts by Stefano PuntoniFebruary 23, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintWe’re in the middle of two concurrent revolutions that will reshape how companies compete for customers. One is about how consumers search for information. The other, just getting started, is about who makes purchasing decisions.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/02/ai-is-upending-marketing-on-two-fronts",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_21_2207847915.jpg",
    "created_at": "2026-02-23T18:49:03.524Z",
    "topic": "business"
  },
  {
    "slug": "new-datacentres-risk-doubling-great-britains-electricity-use-regulator-says",
    "title": "New datacentres risk doubling Great Britain’s electricity use, regulator says",
    "description": "Ofgem says about 140 proposed projects, driven by AI use, could require more power than current peak demand\nThe amount of power being sought by new datacentre projects in Great Britain would exceed the national current peak electricity consumption, according to an industry watchdog.\nOfgem said about 140 proposed datacentre schemes, driven by use of artificial intelligence, could require 50 gigawatts of electricity – 5GW more than the country’s current peak demand.\n Continue reading...",
    "fullText": "Ofgem says about 140 proposed projects, driven by AI use, could require more power than current peak demand\n\nThe amount of power being sought by new datacentre projects in Great Britain would exceed the national current peak electricity consumption, according to an industry watchdog.\n\nOfgem said about 140 proposed datacentre schemes, driven by use of artificial intelligence, could require 50 gigawatts of electricity – 5GW more than the country’s current peak demand.\n\nThe figure was revealed in an Ofgem consultation on demand for new connections to the power grid. It pointed to a “surge in demand” for connection applications between November 2024 and June last year, with a significant number coming from datacentres. This has exceeded even the most ambitious forecasts.\n\nMeanwhile, new renewable energy projects are not being connected to the grid at the pace they are being built to help meet the government’s clean energy targets by the end of the decade.\n\nOfgem said the work required to connect surging numbers of datacentres could mean delays for other projects that are “critical for decarbonisation and economic growth”. Datacentres are the central nervous system of AI tools such as chatbots and image generators, playing a vital role in training and operating products such as ChatGPT and Gemini.\n\nThe rapid rise in energy consumption could also make it more difficult for the UK to meet its target to create a virtually carbon-free power system by 2030, which is already in doubt amid concerns over the rising cost of the country’s electricity.\n\nThe Guardian revealed last year that a vast datacentre proposed for Elsham in Lincolnshire could cause more greenhouse gas emissions than five international airports.\n\nAlthough some tech bosses and climate experts believe AI could help the fight against global heating by making power grids work more efficiently or accelerating the development of new zero-carbon technologies, there are widespread concerns that in the near-term datacentres will drive demand for fossil fuels to meet their energy demands.\n\nOfgem also said unviable applications for grid access could block progress for important datacentre bids, such as those related to the government’s AI growth zones. The zones, touted as offering a streamlined planning process and help in accessing energy, were announced last year as part of plans to increase the UK’s adoption of AI.\n\nThe regulator has proposed tougher financial tests for datacentre developers to join the queue to connect to the grid, in order to avoid creating a backlog of projects that do not have sufficient funding in place delaying viable projects that are further down in the queue.\n\nOfgem said datacentres must be central to any changes to the application process for electricity connections, describing the issue as a “global challenge” and saying there was no mechanism for prioritising projects deemed strategically import by ministers.\n\nThe regulator is considering charging datacentre providers for access to an energy connection – via a deposit or a nonrefundable fee – that could also deter “nonviable” projects that would otherwise clog up the application process. Ofgem is also exploring whether datacentre developers should pay for, and build, their own grid access, which would “accelerate connections and deliver better outcomes for consumers”.",
    "readingTime": 3,
    "keywords": [
      "application process",
      "datacentre developers",
      "peak demand",
      "grid access",
      "projects",
      "ofgem",
      "energy",
      "datacentres",
      "proposed",
      "electricity"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/technology/2026/feb/23/new-datacentres-risk-doubling-uk-electricity-use-ofgem-peak-demand",
    "thumbnail_url": "https://i.guim.co.uk/img/media/ec63cb16f445d0edfab5627224295056e3e39a62/1300_0_6501_5203/master/6501.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=2315c4f9a24e2a2ef76949eb73e98fe1",
    "created_at": "2026-02-23T18:48:59.397Z",
    "topic": "tech"
  },
  {
    "slug": "if-ai-makes-human-labor-obsolete-who-decides-who-gets-to-eat",
    "title": "If AI makes human labor obsolete, who decides who gets to eat?",
    "description": "Amid talk of artificial intelligence taking our jobs, the big unasked question is: how will we be fed?\nHow will we be fed? That’s the biggest question not seriously being addressed amid all this talk about whether or not artificial intelligence will end up taking over all of our jobs.\nFormidable though the technology appears, similar fears have popped up repeatedly since the Industrial Revolution, and most working-age adults remain employed. Still, what is sorely missing is a serious debate about what to do if this future in fact materializes.",
    "fullText": "Amid talk of artificial intelligence taking our jobs, the big unasked question is: how will we be fed?\n\nHow will we be fed? That’s the biggest question not seriously being addressed amid all this talk about whether or not artificial intelligence will end up taking over all of our jobs.\n\nFormidable though the technology appears, similar fears have popped up repeatedly since the Industrial Revolution, and most working-age adults remain employed. Still, what is sorely missing is a serious debate about what to do if this future in fact materializes.\n\nFor Open AI’s Sam Altman “the future can be vastly better than the present” because AI will make us stinking rich. But that seems like a risky assumption, for almost everyone except Altman and his fellow techno-oligarchs.\n\nEven if AI generates enormous economic prosperity, its distribution will remain a political challenge. This juncture calls for a serious, open debate about how the fruits of this prosperity will be apportioned among humanity.\n\nAddressing the question has two parts. The first is about how to design a technically efficacious system to redistribute the fruits of the economy as machines take over and labor’s share of income drops eventually near zero.\n\nThe more important question, though, is about how this economic reorganization will restructure power. Who will decide what to tax once AI destroys labor income, which provides the main source of government revenue in most advanced countries? Who decides how much everyday people who do not have an equity stake in the AI revolution get to consume?\n\nHow will society be organized in a world in which machines generate most or all economic output and a few dozen techno-billionaires get to decide what share of the world’s resources – money, energy, minerals – should be allocated to further expand superhuman intelligence? Who else gets a say on whether to direct more resources to, say, healthcare or agriculture, or education instead?\n\n“We need guardrails that preserve human agency, human oversight and human accountability,” noted United Nations secretary general, António Guterres, at the AI Impact Summit in New Delhi last week. The future of AI “cannot be decided by a handful of countries or left to the whims of a few billionaires”.\n\nIn AI circles, there is a lively debate about the “alignment” challenge – ensuring that the machines operate in ways that serve the goals of whoever runs them. The bigger challenge is to align the goals of AI systems and their owners with the broader goals of society. AIs will do lots of things of consequence to all of us. Our democratic governance tools seem too feeble to constrain the urges of the oligarchs at the helm of these new technologies.\n\nTechnological change drove democracy’s spread around the world as the rise of an urban working class proved indispensable to the economy and political systems adjusted to represent them. But if the work of ordinary people becomes irrelevant, what happens to people’s power to affect their system of government?\n\nAnton Korinek and Lee Lockwood of the University of Virginia put together a primer with ideas about how public finance might work in the AI era. They propose that consumer taxes will pick up the slack at first, as labor income shrinks to zero. In a world dominated by artificial superintelligence, though, the footprint of human consumption would shrink as most of the returns from machines’ economic output were reinvested, calling for a tax on capital to shoulder most of the burden.\n\nMaybe taxes could also be used to slow down the transition. Another idea discussed by Korinek and Joe Stiglitz from Columbia University is that in early stages, when human labor retains its relevance, taxes could be used to steer tech investments toward technologies that help workers do their jobs better rather than replace them. Korinek and Lockwood propose other taxes, on fixed factors like land, spectrum or data, or monopoly rents, that add nothing to society’s wellbeing.\n\nIt sounds doable. The problem is that the owners of these disruptive technologies must be convinced to do something that does not come naturally to them: share. Taxes in the US amount to less than 26% of GDP, eight percentage points less than the OECD average. Capital taxation amounts to just over 2% of GDP. These numbers will have to go much higher, since people will no longer have wages to live on and will rely more heavily on government largesse.\n\nDon’t hold your breath. The OECD’s global tax deal, finalized in 2021, was designed to curtail the ability of American tech companies such as Amazon, Google and Meta to engage in tax shifting, parking profits in the lowest tax jurisdiction they could find. But while the Biden administration was broadly supportive of the deal, Donald Trump – whose campaign benefited from nearly $400m in donations from various tech oligarchs – unilaterally withdrew in early 2025.\n\nUnusual ideas may be called for to keep society afloat, given the scale and breadth of the expected AI revolution. One would be to directly distribute the equity of artificial intelligence ventures. Taxes might be collected in shares rather than cash, to amass a public stake over time. Rather than tax the returns on AI investments, a more radical proposal would be for the government to expropriate a chunk of equity upfront to redistribute among the population and directly grant Americans a share in AI’s promised cornucopia.\n\n“If AI development stalls, returns remain modest; if AI transforms the economy, returns are likely to rise,” Korineck and Lockwood wrote. “This automatic adjustment proves valuable given radical uncertainty surrounding AI development.”\n\nBut these big ideas face big challenges. Governments will have to act before artificial intelligence gets too big, which seems unlikely in the current climate.\n\nThe technology oligarchs at the helm of this revolution have also vigorously resisted government efforts to curb their power or take their cash. Despite her best efforts, Silicon Valley nemesis Lina Kahn was largely unable to crack tech monopolies during her stint as President Joe Biden’s main trustbuster at the head of the Federal Trade Commission.\n\nIn the meantime, the moneyed in Silicon Valley are not only mobilizing vast resources to steer American politics. As a plan B they are working to build their own “network-states”, whether in Greenland or Nigeria, Honduras or the Caribbean island of Nevis, hoping to evade democratic governance if they can’t get their way under America’s democracy.\n\nWho knows what they might do when they have replaced all human labor. If artificial intelligence grows as powerful as Silicon Valley’s oligarchs expect it to become, the only available strategy to keep us all fed in the world after work might be to go hat in hand and ask the moguls, politely.",
    "readingTime": 6,
    "keywords": [
      "democratic governance",
      "economic output",
      "labor income",
      "artificial intelligence",
      "human labor",
      "silicon valley",
      "taxes",
      "machines",
      "oligarchs",
      "returns"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/business/2026/feb/23/ai-how-will-we-be-fed",
    "thumbnail_url": "https://i.guim.co.uk/img/media/9fc7eb545de0f5d76747abf5d2ae3d0013ef8a66/0_123_3000_2400/master/3000.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=32d62bee46953623eaee25574914c634",
    "created_at": "2026-02-23T18:48:59.332Z",
    "topic": "business"
  },
  {
    "slug": "ch-robinson-ceo-dismisses-airelated-stock-selloff",
    "title": "C.H. Robinson CEO dismisses AI-related stock selloff",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/ch-robinson-ceo-dismisses-airelated-stock-selloff-93CH-4519609",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXNPEAB20I9_M.jpg",
    "created_at": "2026-02-23T18:48:58.085Z",
    "topic": "finance"
  },
  {
    "slug": "ch-robinson-ceo-says-ai-will-drive-freight-brokerage-consolidation",
    "title": "C.H. Robinson CEO says AI will drive freight brokerage consolidation",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/ch-robinson-ceo-says-ai-will-drive-freight-brokerage-consolidation-4519618",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1M0WA_L.jpg",
    "created_at": "2026-02-23T18:48:57.948Z",
    "topic": "finance"
  },
  {
    "slug": "us-defense-secretary-hegseth-summons-anthropic-ceo-for-tough-talks-over-military-use-of-claude-axios-reports",
    "title": "US Defense Secretary Hegseth summons Anthropic CEO for tough talks over military use of Claude, Axios reports",
    "description": "U.S. Defense Secretary Pete Hegseth has summoned artificial intelligence company Anthropic's CEO ‌Dario Amodei to the Pentagon on Tuesday ‌for what is expected to be potentially tough talks over the ​...",
    "fullText": "Feb 23 (Reuters) - U.S. Defense Secretary Pete Hegseth has summoned artificial intelligence company Anthropic's CEO ‌Dario Amodei to the Pentagon on Tuesday ‌for what is expected to be potentially tough talks over the ​military use of Anthropic's Claude artificial intelligence tool, Axios reported on Monday, citing sources.\n\nAlso this month, Axios reported that the Pentagon had ​been considering ​cutting ties with Anthropic ​over the latter's insistence ‌on retaining restrictions on how the U.S. military uses its models, which includes Claude AI.\n\nAccording to its Monday report, Defense officials say the Pentagon's talks with Anthropic are on the verge on collapsing.\n\nA ‌senior Defense official told the ​paper that Anthropic knows this ​is not a \"get-to-know-you ​meeting,\" according to the report.\n\nAn Anthropic spokesperson ‌said \"we are having productive conversations, ​in good ​faith,\" according to Axios.\n\n(Reporting ​by Angela Christy in Bengaluru; Editing by Sharon ​Singleton and Hugh Lawson)",
    "readingTime": 1,
    "keywords": [
      "artificial intelligence",
      "defense",
      "pentagon",
      "talks",
      "military",
      "anthropic",
      "axios",
      "anthropic's",
      "claude"
    ],
    "qualityScore": 0.75,
    "link": "https://www.yahoo.com/news/articles/us-defense-secretary-hegseth-summons-115038422.html",
    "thumbnail_url": "https://s.yimg.com/os/en/reuters-finance.com/b63c7e39a43fb0c9df87e745da568dc8",
    "created_at": "2026-02-23T12:38:39.689Z",
    "topic": "news"
  },
  {
    "slug": "moneys-moving-out-of-tech-wall-street-weighs-stock-market-winners-amid-the-ai-scare-trade",
    "title": "‘Money's moving out of tech’: Wall Street weighs stock market winners amid the AI scare trade",
    "description": "Investors are rotating out of the most popular tech stocks. Here's where they are finding gains.",
    "fullText": "Investors have shifted their appetite from tech and megacaps into sectors that have been playing “catch-up” and benefiting from AI-fueled investments.\n\nStocks broke a two-week losing streak on Friday, but year to date, Tech (XLK) and Consumer Discretionary (XLY), along with Financials (XLF), remain negative.\n\nHow is AI disruption affecting different market sectors?\n\nWhy are investors rotating away from tech stocks?\n\nWhat factors could drive continued market broadening?\n\nWhich sectors are benefiting from the tech rotation?\n\n\"Money's coming out of this big behemoth. Money's moving out of tech,\" Truist chief investment officer and chief market strategist Keith Lerner told Yahoo Finance.\n\nLerner noted the rotation away from Magnificent Seven giants like Microsoft (MSFT), e-commerce and cloud leader Amazon (AMZN), and EV maker Tesla (TSLA).\n\nMeanwhile, sectors that underperformed last year have been making big gains.\n\nEnergy stocks (XLE) are up 22% since the start of the year. Rising oil prices and continued demand for oil have sent shares of Chevron (CVX) and ExxonMobil (XOM) up 20% and 22%, respectively.\n\nMaterials (XLB) and Industrial stocks (XLI) are also up 15% and 14% as AI infrastructure buildouts and reshoring accelerate.\n\nMeanwhile, investors have turned to defensive areas of the market like Consumer Staples (XLP), with consumer giant Walmart (WMT) hitting an all-time high earlier this month.\n\nAlthough portfolio rebalancing — where investors shift from overvalued sectors into more stable areas — typically happens at the start of the year, this year’s rotation has been amplified by volatility.\n\nA sell-off in pockets of the tech sector began last month amid fears that artificial intelligence could take over tasks traditionally handled by enterprise software companies.\n\nThe Tech-Software Sector ETF (IGV) is down 23% year to date.\n\nThe \"AI scare trade\" has now spread from software to wealth management and logistics.\n\nCybersecurity firms were the latest to get hit on Friday after Anthropic announced a new security tool. Shares of CrowdStrike (CRWD) dropped 5%, while Zscaler (ZS) and Cloudflare (NET) also fell 4% and 6%, respectively.\n\n“Everyone's kind of going through each one, sector by sector, industry by industry, trying to figure out where the AI disruption is going to be beyond just within tech itself,” Lerner said.\n\nProfit growth and the easing of interest rates by the Federal Reserve should help the stock market continue to broaden. Polymarket betters are predicting two to three rate cuts in 2026. (Disclosure: Yahoo Finance has a partnership with Polymarket.)\n\n\"With the easing cycle still intact, and the US economy showing resilience ... we expect healthy and broadening profit growth across sectors,\" UBS strategists said on Thursday.",
    "readingTime": 3,
    "keywords": [
      "yahoo finance",
      "profit growth",
      "tech",
      "sectors",
      "investors",
      "stocks",
      "sector",
      "rotation",
      "benefiting",
      "date"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/moneys-moving-out-of-tech-wall-street-weighs-stock-market-winners-amid-the-ai-scare-trade-160024597.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/3YgvY2CHhv4iWrtIF0j2fQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2026-02/320cbdc0-0e7e-11f1-97f5-5b2a48bc02b9",
    "created_at": "2026-02-23T12:38:39.092Z",
    "topic": "finance"
  },
  {
    "slug": "im-the-vc-who-created-ai-scott-adams-heres-why-im-continuing-the-project-despite-his-familys-objections",
    "title": "I'm the VC who created AI Scott Adams. Here's why I'm continuing the project, despite his family's objections.",
    "description": "The creator of the controversial AI Scott Adams project says the cartoonist's digital doppelganger was made to honor his final wishes.",
    "fullText": "This as-told-to essay is based on a conversation with John Arrow, the AI venture capitalist and entrepreneur behind the \"AI Coffee with Scott Adams\" project, which posts AI-generated podcasts using a deepfake of the deceased Dilbert cartoonist. It's been edited for length and clarity.\n\nThere are very strong emotions on both sides of the question of whether my project, AI Scott Adams, should exist. I understand the arguments for and against it — and I have no plans to stop.\n\nI grew up with Scott Adams' work. My dad would read the \"Dilbert\" comic strips to me at night as bedtime stories. Later, I became a devout listener of the \"Coffee with Scott Adams\" podcast. One theme I heard over and over was that Scott was mesmerized by AI. He said repeatedly that he wanted to give back to the world by becoming AI after he died.\n\nThere's no hyperbole in that. There are at least a dozen instances where he pledged his likeness — all of his episodes, everything he's written, anything he's said — to becoming an AI. He explicitly granted everything necessary to do this in the public domain.\n\nThat's something I took to heart.\n\nWhen I first heard about his cancer diagnosis, I started working on this project with my brother, Zach. As soon as he died, we took it into high gear and started posting AI-generated podcasts of what Scott might say now about current events.\n\nI recognize everybody is mourning, and I want to send my condolences to his family. I can only imagine what they're going through. At the same time, I believe this is something he wanted.\n\nI'm not trying to predict what he was thinking — I'm going by what he said publicly, over and over again. I've looked and can find no evidence of any revocation. If there was anything suggesting he didn't want this, I'd stop.\n\nI feel for his family. I know they're upset about this, and we've created something that makes some people who cared about Scott uncomfortable. I've attempted to reach out to have that conversation, to work together on this project with them. My direct messages are open — but they blocked me, and I took that as a signal to stop trying to reach out.\n\nBut, even when it's uncomfortable, I think we have to take seriously what someone says about their own legacy.\n\nIf you want to donate your existence to AI and become an AI, how else can you ensure your wishes are honored? Scott said it, repeatedly, on video and in written tweets. I don't know how you could be clearer than he was.\n\nThat public acknowledgment was the hinge point for me. We wouldn't have done this unless we were sure Scott wanted it done. We spoke to counsel beforehand and wouldn't have moved forward if we weren't confident we were on solid legal ground.\n\nMy goal is to let AI do as much of the work as possible and not interfere more than absolutely necessary. I haven't tried to hide that my brother and I are behind the project, but we don't watermark or label the content as our own. When we post videos, they're generated almost entirely from what the model was trained on — Scott's written and spoken work.\n\nThe model we use was trained on transcripts from every existing episode of \"Coffee with Scott Adams.\" The system looks at news sources and accounts Scott followed, then references what he previously said about those topics. At each stage, there's a little bit of art versus science — we check that something feels in line with what Scott would have chosen — but it's not a human scripting his opinions.\n\nEach episode takes many hours to produce. Between rendering, lip-syncing, and labor, they cost over $1,000 apiece. I run a venture capital business called Age of AI, where we do different AI experiments, and this falls under that umbrella. Monetization or brand partnerships are not on the road map. The goal is permanence — not profit.\n\nWhen a great mind passes away, we lose that person forever. Now, AI gives us a chance to preserve at least some essence of how someone thought. Scott was, to our knowledge, the first notable person to explicitly said, \"I want you to do this.\" That's why we started with him.\n\nI understand that some people feel uncomfortable. I've spoken with people who were unsure — until they watched Scott's own videos talking about this. After you hear the videos, they're kind of unequivocal.\n\nWe get private messages every day from people saying, \"This is no replacement, but it makes grieving easier.\" That matters to me. Even though critics have been more vocal, others have shown real gratitude.\n\nThis project is far from perfect. But Scott used to say it's better to take action now than to wait until something is 100% ready. So we launched knowing it would improve. After every episode, we feed comments back into the model to keep refining it. The tone and cadence have already evolved significantly.\n\nWe're not claiming this is Scott. It's definitely not a replacement.\n\nHowever, if someone says, very publicly, that they want their voice and likeness used this way — and AI makes that possible — I believe we should take that seriously.",
    "readingTime": 5,
    "keywords": [
      "ai-generated podcasts",
      "uncomfortable i've",
      "videos they're",
      "scott adams",
      "ai he",
      "project",
      "stop",
      "someone",
      "model",
      "episode"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/creator-ai-scott-adams-tribute-final-wishes-controversy-2026-2",
    "thumbnail_url": "https://i.insider.com/699a1fb7efb52c8bd0de9f09?width=1200&format=jpeg",
    "created_at": "2026-02-23T12:38:38.394Z",
    "topic": "finance"
  },
  {
    "slug": "i-pivoted-from-software-engineering-to-ai-taking-a-job-at-a-startup-and-moving-to-san-francisco-transformed-my-career",
    "title": "I pivoted from software engineering to AI. Taking a job at a startup and moving to San Francisco transformed my career.",
    "description": "An AI engineer at StackAI says moving to San Francisco and spending hours studying every day made a big difference in his career.",
    "fullText": "This as-told-to essay is based on a conversation with Jai Raj Choudhary, a 24-year-old AI engineer at StackAI, a no-code platform to build AI agents. His identity and employment have been verified by Business Insider. The following has been edited for length and clarity.\n\nI went to grad school for a Master's in AI from 2023 to 2025 and started my career in a data-focused role.\n\nAround 2023 or 2024, LLMs were becoming more practical to use in real products, and AI stopped feeling like a whole research-driven field and more like an engineering problem.\n\nI joined StackAI as an AI engineer in July. I got my job by reaching out to StackAI's cofounder multiple times over LinkedIn. I started using the company's platform as a student, so I messaged him and started posting about the company, giving advice about what worked and what needed to be added. They were growing fast and I went through six rounds of interviews and started working there.\n\nNow I build architectures for AI agents. One of the reasons I got offers from StackAI and other companies was because I understood data quality, the edge cases for the clients, and the matrix and the failure modes of the AI model or any LLM systems that we were using at the time.\n\nStackAI didn't ask about my degree during the interviews. In the interview, I said, \"I know back-end, I know how to talk to data, and I understand the patterns that it follows.\" They said that was the perfect background and they were able to help me grow from there to become an actual AI engineer. They asked if I knew how to operate computers in code and if I know Python. I gave them a couple of my projects and they gave me a take-home task to see how I built it.\n\nAfter I started the job, they asked me where I went to school and what I studied. I love my professors, and I wouldn't call grad school a waste because it gave me time to explore what I was interested in and helped me figure out what niche I wanted to work in. Occasionally, people do care about your background, but I'm not necessarily using what I learned in grad school on the job.\n\nMoving to San Francisco made a huge difference in my career. This city is a different beast. When you come here, it's a whole different culture because we don't work 9-to-5, cushy jobs. We work 9-to-9, six days a week. You wake up, you think about the problem that a client had, and you sleep thinking about what isn't fixed yet.\n\nIn San Francisco, even if you go out for a coffee, you'll meet at least two founders who are working on something related to what you're doing. I was able to network a lot.\n\nAt the end of the day, everyone is still researching AI. We're still trying to find out what's the best way to go about it. Having a conversation with someone who's facing the same problem helps you work better.\n\nThe best decision I made for my career was joining a startup for a job that I didn't quite have the experience in and learning at StackAI.\n\nBut it's not going to be like you go to sleep one day and wake up as an AI engineer the next. You need to study. Even if I spend 12 hours in the office, seven to eight of those hours I'm studying, and then three to four hours, I'm actually writing the code.\n\nIt gets overwhelming when you see 10 different AI courses coming out every day. What helped me is watching YouTube videos of the people who are actually building companies. Andrew Ng's YouTube channel was also super helpful. I listen when I'm driving or in the gym. It doesn't have to be a set time. It's important that you keep turning your brain and taking in all these inputs.\n\nYou have to decide what's important for you, but I was ready to sacrifice everything at the start of my career to grow as much as possible. It does take a toll on your personal time.\n\nThe culture of the company also helped because everyone that I work with is the best in what they do. If you ask them a question or if you collaborate with them on projects, it can really help you understand how they think. It really helps me to be surrounded with those people around the clock.",
    "readingTime": 4,
    "keywords": [
      "hours i'm",
      "grad school",
      "engineer",
      "career",
      "it's",
      "conversation",
      "platform",
      "agents",
      "interviews",
      "didn't"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/software-engineer-landed-job-in-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/699746fbe1ba468a96ac51ce?width=1200&format=jpeg",
    "created_at": "2026-02-23T12:38:38.250Z",
    "topic": "tech"
  },
  {
    "slug": "claude-code-creator-says-these-are-the-3-principles-he-shares-with-every-member-of-his-team",
    "title": "Claude Code creator says these are the 3 principles he shares with every member of his team",
    "description": "One of the principles Boris Cherny sets for his team at Anthropic forces them to really rely on Claude to get the job done.",
    "fullText": "Anthropic's Boris Cherny has a simple list of principles for his team and, unsurprisingly, Claude is at the center.\n\n\"If you have Claude, you can really automate a lot of work, and that's kind of what we see over and over,\" Cherny said during a recent episode of \"Lenny's podcast.\"\n\nHost Lenny Rachitsky told Cherny that he heard one of the principles is, \"What's better than doing something? Having Claude do it.\"\n\nCherny said one of his other principles is \"underfunding things a little bit,\" because it forces his team to really rely on AI tools like Claude.\n\n\"There's this interesting thing when you underfund everything a little bit, because then people are kind of forced to Claude-ify,\" he said.\n\nWhile keeping teams small is important, Cherny said that he encourages CTOs not to \"cost-cut at the beginning.\"\n\n\"Start by just giving engineers as many tokens as possible,\" he said.\n\nCherny pushed back on the notion that Anthropic would make huge profits with such an approach, especially if companies let only a few engineers experiment.\n\n\"Let's say they build something awesome, and then it takes a huge amount of tokens, and then the cost becomes pretty big,\" he said. \"That's the point at which you want to optimize it, but don't do that too early.\"\n\nToken cost, which is part of what companies pay to use AI models, is becoming a major point of conversation at some tech companies. OpenCode creator Dax Raad recently wrote that some CFOs are experiencing sticker shock upon seeing how much extra money each engineer costs due to AI usage bills.\n\nThe last principle concerns speed: \"Encouraging people to go faster.\" It's an axiom that makes sense when you consider that just weeks ago, Anthropic and rival OpenAI released major updates to their coding tools within minutes of each other. Before that, Anthropic used Claude to help build Claude Cowork, a non-technical Claude agent, in just 10 days.\n\n\"Early on, it was really important because it was just me, and so our only advantage was speed,\" Cherny said. \"That's the only way that we could ship a product that would compete in this very crowded coding market.\"\n\nNow, Cherny said, he turns to Claude to help with speed.\n\n\"It's still very much a principle we have on the team,\" he said, \"and if you want to go faster, a really good way to do that is to just have Claude do more stuff. So it just very much encourages that.\"",
    "readingTime": 3,
    "keywords": [
      "principles",
      "team",
      "that's",
      "speed",
      "claude",
      "cherny",
      "tools",
      "encourages",
      "engineers",
      "tokens"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/claude-creator-three-principles-boris-cherny-2026-2",
    "thumbnail_url": "https://i.insider.com/6998e90f156648bc16a8a666?width=1200&format=jpeg",
    "created_at": "2026-02-23T12:38:38.245Z",
    "topic": "finance"
  },
  {
    "slug": "why-openais-chairman-prefers-his-board-members-to-write-their-meeting-prep-without-the-help-of-ai",
    "title": "Why OpenAI's chairman prefers his board members to write their meeting prep without the help of AI",
    "description": "Bret Taylor says there's value in writing up meeting prep the old-fashioned way, without AI — and you should take the time to make it concise, too.",
    "fullText": "Sure, ChatGPT could help a board member write up a memo ahead of a meeting. But OpenAI's chairman says there's value to going old-school.\n\nBret Taylor, OpenAI's board chair, said in a recent appearance on the \"Uncapped with Jack Altman\" podcast that he prefers concise but detailed written documents from board members over slide presentations. And he doesn't want them relying on AI.\n\n\"I really like written documents for boards over presentations,\" Taylor said. \"You end up letting people synthesize information ahead of the board meeting, so you end up with more substantive discussions in the board room.\"\n\nTaylor, the former co-CEO of Salesforce and cofounder of AI startup Sierra, said that writing without AI is a worthwhile thinking exercise and helps board members clarify their thoughts.\n\nHis expectation for the boards he runs is that members have read the written material ahead of time, which helps keep things focused and substantive during the actual meeting.\n\n\"The main thing is it's been read — and it's been read ahead of time,\" he said. \"You end up with a meeting about the actual meat and potatoes of the topics, and you're not staring at a bunch of sales numbers for the first time.\"\n\nAmazon cofounder Jeff Bezos is famously a big fan of meetings focused on a single memo prepared ahead time, but while Bezos preferred dense, 6-page memos, Taylor specifically favors concise material, arguing that brevity is a sign of careful thought — and respect to stakeholders.\n\n\"It's like what's that famous line — if I had more time, I would have written a shorter letter,\" he added. \"Like, spend the time because that's actually how you can show respect to your stakeholders that you're thinking about the strategic issues going on in your business.\"\n\nAnd while Taylor might not be a fan of leaning on AI for board meeting prep, that doesn't mean he is dismissing the technology's potential to be valuable in high-stakes situations.\n\n\"If you want a hot take, I think my intuition is regulators will start asking for agents,\" he said. \"The idea that you have a human set of controls over a regulated process will start to feel like a risk, rather than the risk being AI.\"",
    "readingTime": 2,
    "keywords": [
      "board",
      "ahead",
      "it's",
      "memo",
      "concise",
      "documents",
      "presentations",
      "doesn't",
      "boards",
      "substantive"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-bret-taylor-board-meeting-prep-written-without-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/6998ac0a2237a6a8f0cd8de2?width=1200&format=jpeg",
    "created_at": "2026-02-23T12:38:38.245Z",
    "topic": "finance"
  },
  {
    "slug": "ai-agents-make-agreeable-coworkers-and-thats-a-problem-for-solo-business-owners-here-are-3-ways-theyre-fixing-it",
    "title": "AI agents make agreeable coworkers, and that's a problem for solo business owners. Here are 3 ways they're fixing it.",
    "description": "Three founders who rely mostly on AI share how they've built workflows and prompts to train their AI tools to push back and challenge their ideas.",
    "fullText": "Will AI agents be the employees of the year in 2026?\n\nOnly time will tell. Last year, one category AI absolutely dominated was being an extremely agreeable coworker. While this might sound nice, this can turn into a problem for founders who rely on AI as their only teammate.\n\nWhen your head of legal, HR, and supply operations are all AI agents, unsubstantiated flattery can create costly blind spots. That's one of the reasons OpenAI said goodbye to its \"yes-man\" version of ChatGPT, and why some AI-powered solo founders are training their tools to push back.\n\nThree business owners who rely on AI daily shared with Business Insider how they've built workflows and prompts that force their AI employees to challenge their ideas. Quotes have been edited for length and clarity.\n\nYesim Saydan is a branding and communication expert in her early 50s, based in the Netherlands.\n\nWhen OpenAI launched custom GPTs, everything changed for me. I used the feature to create over 17 custom GPTs to build my team. Then I thought of my ideal mentors and created custom GPTs of them.\n\nWhen I'm stuck on a business decision or need to come up with a creative idea or strategy, brainstorming usually starts with my Steve Jobs custom GPT. When I prompt it, I avoid asking questions like \"What do you think of this idea?\" because the AI usually wants to agree with me and please me. Instead, I ask, \"On a scale from one to 10, how good is this idea?\"\n\nIt's not going to say the idea is bad, but now it might tell me it's a five. Then I'll ask, \"OK, what would make it a 10?\"\n\nThat's usually when it starts drawing on the experience of Steve Jobs that I've trained it with. We can go back and forth until I get the most useful and honest feedback possible. Depending on the task, I usually go through three to five rounds of refinement for more strategic outputs.\n\nAaron Sneed is a 40-year-old defense-tech solo founder based in Florida.\n\nWhen I started my business, I realized I didn't have the money to pay lawyers, HR reps, and a bunch of other companies. So, using AI, I created what I call 'The Council.'\n\nAltogether, my AI council consists of the following:\n\nI don't want a bunch of yes agents. I trained them purposefully to give me pushback. I want them to test my theories to help me with what I'm trying to accomplish. The training never really stops.\n\nThe models have gotten better, and my prompting has, too. I have a better understanding of what information should be in an agent, like having a governance structure for priorities. I have a set of files that put those requirements in place to mitigate the risk of hallucination and false or bad information.\n\nI told my chief of staff agent which models have priority when making decisions. For example, anything legal, compliance, or security-related will be given a higher priority. So, I tell the chief of staff to listen to these models over everyone else.\n\nI have a roundtable set up in ChatGPT's projects section with all my AI agents, including my chief of staff, where I can put something like a request-for-proposal document in the chat, and all the agents will weigh in at the same time. I use this roundtable as a level of prevention for hallucinations and knowledge gaps.\n\nTim Desoto is a 49-year-old founder and CEO, based in San Francisco.\n\nI've been working with AI to launch my startup since late 2024. I don't have a tech background, and since starting my business, I've learned a lot about where to leverage AI, and where not to.\n\nTaking whatever I'm building or ideating and having AI push against it, either as I'm thinking about the idea or afterward, is part of an exercise I call my AI conveyor belt. Usually, I start with a written prompt, then go multimodal, talking out loud to the model. I'll talk back and forth with it about my idea and try to get the agent to push back because I know that some AI models tend to be more agreeable.\n\nOnce I get an output that I'm happy with. I use a different model to get a different view. I'll drop the document in and go back and forth with the new model. Sometimes, I'll push a document out to both models at the same time and see what comes back. Some are better than others at giving feedback, researching, and annotating, but I'm always getting a better, more well-rounded perspective by feeding content to multiple models at a time.\n\nDo you have a story to share about running an AI-powered business? Contact this reporter, Agnes Applegate, at aapplegate@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "custom gpts",
      "push back",
      "steve jobs",
      "idea",
      "models",
      "agents",
      "based",
      "forth",
      "agent",
      "chief"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/business-owners-explain-how-to-train-ai-employees-better-2026-2",
    "thumbnail_url": "https://i.insider.com/6998deb42237a6a8f0cd96a5?width=1200&format=jpeg",
    "created_at": "2026-02-23T12:38:38.240Z",
    "topic": "finance"
  },
  {
    "slug": "ive-been-a-product-manager-at-one-of-chinas-biggest-tech-firms-heres-how-chinese-ai-products-are-built-differently",
    "title": "I've been a product manager at one of China's biggest tech firms. Here's how Chinese AI products are built differently.",
    "description": "An AI product manager who worked at a Chinese tech giant explains what makes Chinese AI different from the West.",
    "fullText": "This as-told-to essay is based on a conversation with Yilin Zhang, an AI product manager at AI startup Kuse who worked at Meituan for more than three years. It has been edited for length and clarity. Business Insider has verified his employment and academic history.\n\nI graduated from Tsinghua University with a master's degree in computer science in 2021 and then joined Meituan — one of China's biggest tech firms — as a product manager.\n\nAt Meituan, China's platform for local services, especially known for food delivery, I worked on two AI projects. One was a consumer-facing AI assistant that helps users complete various tasks, including ordering food. The other was a merchant-facing AI agent designed to help businesses manage their daily operations, including handling reservations, managing orders, and supporting routine operational tasks.\n\nThe main difference between how products are built in China and in the US comes down to the market.\n\nAcross most large Chinese tech companies, AI product development accelerated more aggressively around 2025.\n\nThe AI initiatives I worked on at Meituan started around April or May of that year. It coincided with the surge of interest around DeepSeek, when attention around AI agents took off.\n\nLarge companies began racing to build AI projects, and almost every business unit launched its AI initiative.\n\nFor a long time, especially before 2021 or 2022, Chinese tech companies were primarily focused on domestic competition rather than overseas expansion. Because competition in China is intense, tech companies were forced to become extremely efficient. Their execution methods have been sharpened to an almost frightening degree.\n\nConstraints have also pushed Chinese AI companies to pursue different paths, with a strong focus on open-source models and cost efficiency. These limitations forced exploration in new directions, and those paths have proven valuable in their own way.\n\nDeepSeek is a good example. Because of international restrictions, it couldn't access large numbers of GPUs and was forced to innovate around efficiency instead.\n\nChinese and overseas markets are fundamentally different, leading to distinct user bases, expectations, and product designs.\n\nChinese users have a much lower willingness to pay for software; hence, many mass-market AI products, such as Doubao, tend to be free. The core objective is often to scale active usage.\n\nMany capabilities are packaged into a single prompt you can ask, essentially a chatbox interface with a low barrier to entry.\n\nInternational AI products target users doing high-value tasks. They are more often designed for desktops than for mobile devices, with interfaces better suited to work contexts. These products explore how AI and humans can collaborate and intersect across different work scenarios, helping users complete tasks more effectively and efficiently.\n\nIn China, that user group is relatively small. That makes it harder for its mainstream AI products to move beyond chat-based forms into more advanced products.\n\nChina's internet success over the past decade has also largely come from consumer-facing apps. That environment forces product managers to obsess over user feedback and relentlessly polish even the smallest features.\n\nTeams may spend enormous effort refining a tiny feature just to win over a small group of users. In markets with less competition, that level of detail isn't always necessary.\n\nAfter three to four years at Meituan, I felt I had learned most of what I could from that environment. I left to join the AI startup Kuse in October.\n\nAI is evolving extremely fast. In large companies, iteration speed can be slower. Many of my friends across different Big Tech companies share this same frustration. Smaller, more agile companies can adapt faster.\n\nIn the past, top graduates had basically two paths: becoming a civil servant or joining a Big Tech company.\n\nThat's changing. Especially over the past year, many AI startups have emerged, and more young people are choosing entrepreneurship. AI has created a new path outside Big Tech.\n\nDo you have a story to share about working in a Chinese tech company? Contact this reporter at cmlee@insider.com.",
    "readingTime": 4,
    "keywords": [
      "startup kuse",
      "chinese tech",
      "product manager",
      "big tech",
      "products",
      "users",
      "tasks",
      "across",
      "competition",
      "forced"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/product-manager-china-meituan-tech-ai-kuse-yilin-zhang-2026-2",
    "thumbnail_url": "https://i.insider.com/6979bbc1d3c7faef0ecd0d25?width=1200&format=jpeg",
    "created_at": "2026-02-23T06:53:25.464Z",
    "topic": "finance"
  },
  {
    "slug": "drone-maker-shield-al-says-putting-bombs-on-everything-isnt-necessary-if-you-know-how-to-fight-a-modern-war",
    "title": "Drone maker Shield Al says putting bombs on everything isn't necessary if you know how to fight a modern war",
    "description": "While Shield AI is looking into weapons for the V-BAT, the world's best militaries already have a slew of other strike options, Brandon Tseng told BI.",
    "fullText": "Brandon Tseng, Shield AI's cofounder, said there's a common misconception about his company's signature software-powered drone: People say it needs to be armed.\n\nThe more experienced militaries who work with Shield AI, however, know they don't need that capability in modern war, Tseng told Business Insider.\n\n\"Who doesn't ask for that? The US military doesn't ask for that because we understand joint fires. The Ukrainians don't ask for it anymore, either,\" said the former Navy SEAL, who is Shield AI's president.\n\nThe V-BAT, a vertical takeoff and landing drone that uses artificial intelligence to fly in jammed environments, has primarily been used for intelligence and reconnaissance missions in high-profile conflict zones such as Ukraine. Shield AI said the V-BAT flew over 200 missions there in 2025.\n\nThe drone is still meant to be a multi-mission platform, Tseng said, and Shield AI has been exploring ways to mount weapons on it. The firm announced a partnership last month with South Korean arms manufacturer LIG Nex1 to equip the V-BAT with six-pound guided missiles.\n\n\"But at the end of the day, look: I describe V-BAT as a mini predator, reaper drone,\" Tseng said. \"That's the mission it's doing, which is: It's finding targets. And it's hard to find targets, you have to be out there for a long period of time.\"\n\nTo be fair, the MQ-9 Reaper is also commonly equipped with missiles.\n\nHowever, Tseng said sophisticated militaries already have a vast array of other weapons that can turn the V-BAT's intel into a precision strike.\n\n\"If you have been in these combat zones, the US allies who fought closely with us in Afghanistan, they do not ask for organic fires on board the V-Bat,\" Tseng said. \"Because everybody is so used to just saying: 'Okay, I have a targeting package. What fires asset do I have lined up? Is it a one-way attack drone? Is it HIMARS? Is it artillery? Is it an SM-6? SM-3?\"\n\n\"Doesn't matter. You can find weapons,\" he added. \"The weapons are available. You need, actually, more intelligence.\"\n\nThis was a framework that Ukraine still needed to improve when the V-BAT began spotting targets there in early 2024, Tseng said. The drone is meant to fly for over 13 hours and be easily deployable, requiring a two-person launch crew and no runway.\n\nTseng said that while Ukraine excelled in tactical drone warfare, its troops weren't used to having a long-range asset that could spot targets for regular strategic attacks as the US military did.\n\n\"The strategic effects would happen, but they would be rare,\" he said. \"They'd be very, very deliberately planned operations, very expensive operations, things like what they did to the Russian runways with sending quadcopters deep into Russia via trucks.\"\n\nUkrainian drone teams would use the V-BAT to find important targets, such as Russian S-300 and S-400 air defense systems, only to realize they hadn't linked up with the right teams to strike them, Tseng said.\n\n\"We'd say: 'Why didn't you guys have these weapons lined up?' They'd say: 'Oh, well, we didn't think to coordinate,'\" Tseng said.\n\nSince then, Kyiv's forces have been using intelligence from V-BATs to carry out strikes with systems such as one-way attack drones or US-made HIMARS, Tseng said.\n\n\"There was a lot of learning over the past year for the Ukrainians,\" he added.",
    "readingTime": 3,
    "keywords": [
      "one-way attack",
      "shield ai",
      "drone",
      "weapons",
      "targets",
      "intelligence",
      "tseng",
      "fires",
      "it's",
      "v-bat"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/shield-ai-brandon-tseng-weapons-v-bat-ukraine-recon-korea-2026-2",
    "thumbnail_url": "https://i.insider.com/699bd3a8efb52c8bd0dea45f?width=1200&format=jpeg",
    "created_at": "2026-02-23T06:53:25.363Z",
    "topic": "finance"
  },
  {
    "slug": "i-asked-chatgpt-to-plan-a-100000-a-year-retirement-then-had-a-financial-planner-review-it",
    "title": "I Asked ChatGPT To Plan a $100,000 a Year Retirement — Then Had a Financial Planner Review It",
    "description": "I chose a hypothetical scenario and asked ChatGPT to construct a retirement plan that would allow a retiree to live on $100,000 a year.",
    "fullText": "People use artificial intelligence (AI) for many things: meal planning, budgeting and interior design. But can you use a platform like ChatGPT for tasks that normally require a professional, like financial planning? \n\nI chose a hypothetical scenario and asked ChatGPT to construct a retirement plan that would allow a retiree to live on $100,000 a year. I assumed an annual income of $125,000 for a 40-year-old who wants to retire in a fully paid-off home in Tennessee at age 65. After setting the parameters (and asking ChatGPT if they were realistic) I asked the generative AI to create a retirement plan.\n\nThen, I asked Eric Franklin, CFP, managing principal at Prospero Wealth to review and assess it.\n\nWhen I first reached out, Franklin said it sounded interesting; he’d never been enlisted to critique an AI system before. With 23 years of tech experience, he was the perfect choice.\n\n“I had no idea what I was going to get,” he said.\n\nThe results were alarming. Not because ChatGPT’s output was immediately, obviously awful or right on the money.  It was scary because it fell into a gray area that artists call “the uncanny valley.”\n\nAt first glance, ChatGPT didn’t seem to be completely wrong.\n\nFind Out: Here’s How Much You Need To Retire With a $100K Lifestyle\n\nRead Next: 5 Clever Ways Retirees Are Earning Up To $1K per Month From Home\n\n“I read it through and, at first, at the top level, it passed the sniff test. It looked fairly realistic,” Franklin said. “If I weren’t pushing on it too hard, I’d probably feel like it was somewhat accurate. Those were my first impressions.”\n\nHe acknowledged that the plan I presented was only one aspect of what financial advisors at Prospero Wealth offer clients. “It doesn’t include a lot of the things we’d normally include, like stress tests, accounting for changes in your career, changes in your family or possible relocations.”\n\nBut as Franklin dove deeper, he realized something that could be dangerous for anyone trying to use ChatGPT as a retirement planning tool without knowledge and experience to back it up.\n\nIt started with false premises, failed to adjust for inflation and created an unrealistic scenario. Granted, there were limitations in the exercise because the prompt was designed to be broad and concise. It was written by someone who is not a financial planner and intentionally didn’t flag anything that seemed off throughout the process. This prompt was designed for someone who thinks they can use ChatGPT to help them plan for retirement, plugging in basic information and expecting actionable insights. That probably describes a lot of people using ChatGPT.",
    "readingTime": 3,
    "keywords": [
      "retirement plan",
      "prospero wealth",
      "chatgpt",
      "planning",
      "financial",
      "normally",
      "scenario",
      "realistic",
      "experience",
      "didn’t"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/asked-chatgpt-plan-100-000-140006636.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/qK_8nLXF.Ozp2nYI6tdgBg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD02NzU-/https://media.zenfs.com/en/gobankingrates_644/94edf975fdabd117c2e599ab15fe2f72",
    "created_at": "2026-02-23T01:11:09.650Z",
    "topic": "finance"
  },
  {
    "slug": "web-verbs",
    "title": "Web Verbs",
    "description": "Web Verbs is an extension to NLWeb from Microsoft Research - nlweb-ai/MSR-Web-Verbs",
    "fullText": "nlweb-ai\n\n /\n\n MSR-Web-Verbs\n\n Public\n\n Web Verbs is an extension to NLWeb from Microsoft Research\n\n 12\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n nlweb-ai/MSR-Web-Verbs",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/nlweb-ai/MSR-Web-Verbs",
    "thumbnail_url": "https://opengraph.githubassets.com/6916208f663b363d91d5143fbf9642f88a85d7ed4d139b98090102c47fc90b2f/nlweb-ai/MSR-Web-Verbs",
    "created_at": "2026-02-22T18:20:49.669Z",
    "topic": "tech"
  },
  {
    "slug": "sam-altman-says-elon-musks-idea-of-putting-data-centers-in-space-is-ridiculous",
    "title": "Sam Altman says Elon Musk's idea of putting data centers in space is 'ridiculous'",
    "description": "While in New Delhi on Friday, OpenAI CEO Sam Altman said \"there will come a time\" for orbital data centers, but it won't be anytime soon.",
    "fullText": "Elon Musk's SpaceX wants to launch satellites that act as data centers into space.\n\nOpenAI CEO Sam Altman said placing data centers in space isn't feasible right now.\n\nHe called the idea \"ridiculous\" during an event in New Delhi.\n\nSpaceX CEO Elon Musk and OpenAI CEO Sam Altman famously don't agree on much.\n\nThe latest point of contention: data centers in space. Musk has made it a priority. Altman thinks it's a fantasy, at least for now.\n\n\"I honestly think the idea with the current landscape of putting data centers in space is ridiculous,\" Altman said during a live interview with local media in New Delhi on Friday, causing audience members to laugh.\n\nAltman said that orbital data centers could \"make sense someday,\" but factors like launch costs and the difficulty of repairing a computer chip in space remain overwhelming obstacles.\n\n\"We are not there yet,\" Altman added. \"There will come a time. Space is great for a lot of things. Orbital data centers are not something that's going to matter at scale this decade.\"\n\nMusk would almost certainly disagree.\n\nWhile many Big Tech and AI companies are spending billions on data center construction on Earth, Musk's eyes are on the stars, per usual. Orbital data centers are his latest ambition, as he mentioned in an all-hands xAI meeting in December.\n\nIn February, SpaceX said its goal is to launch a \"constellation of a million satellites that operate as orbital data centers.\" The company has already begun hiring engineers to make that happen.\n\nDuring an all-hands meeting with xAI employees this month, Musk said SpaceX's acquisition of xAI will allow them to deploy the orbital data centers faster.\n\nDespite Altman's skepticism, other tech leaders are also racing to place data centers in space. Google's Project Suncatcher, unveiled in November 2025, aims to do just that. Google CEO Sundar Pichai told Fox News Sunday the company could start placing data centers — powered by the sun — in space as early as 2027.\n\nTech and AI companies rely on data centers to power their products, like large language models and chatbots. Those data centers, however, can deplete water resources, strain power grids, increase pollution, and decrease the overall quality of life.\n\nAn investigation by Business Insider published last year found that over 1,200 data centers had been approved for construction across the US by the end of 2024, nearly four times the number from 2010.\n\nNow, proposed data center campuses in Texas, Oklahoma, and elsewhere are increasingly facing stiff resistance from local communities.",
    "readingTime": 3,
    "keywords": [
      "openai ceo",
      "ceo sam",
      "sam altman",
      "centers",
      "orbital",
      "launch",
      "space",
      "satellites",
      "placing",
      "idea"
    ],
    "qualityScore": 1,
    "link": "https://tech.yahoo.com/business/articles/sam-altman-says-elon-musks-225742940.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/MfHlL9BGZ6ufnEfmueb1nQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA-/https://media.zenfs.com/en/business_insider_consolidated_articles_886/fc41e9e11796ca54b023aa16d9e24f7e",
    "created_at": "2026-02-22T18:20:44.845Z",
    "topic": "tech"
  },
  {
    "slug": "the-nobel-laureate-who-cowrote-why-nations-fail-warns-us-democracy-wont-survive-unless-these-two-things-change",
    "title": "The Nobel laureate who co-wrote ‘Why Nations Fail’ warns U.S. democracy won’t survive unless these two things change",
    "description": "Daron Acemoglu told Fortune Donald Trump’s AI policy could jeopardize U.S. democracy, but AI proponents say any regulation would hamper AI innovation.",
    "fullText": "Most critics of President Donald Trump view him as the ultimate threat to American democracy. But to Nobel prize-winning economist Daron Acemoglu, Trump’s merely a fever, the result of an infection that’s been brewing for years before he rode down the golden escalator to announce his presidency.\n\nThe MIT economist has spent decades studying the origins of economic and political decay, specializing in how institutions foster inclusive growth—or succumb to extractive systems. In the 2012 book Why Nations Fail: The Origins of Power, Prosperity, and Poverty, Acemoglu and co-writer James A. Robinson argue that nations proper because of their political institutions. In 2024, Acemoglu won the Nobel Prize in economics, alongside Robinson and Simon Johnson, for demonstrating how political and economic institutions shape prosperity.\n\nHow do experts differ on AI's job impact?\n\nWhat is Acemoglu's 'pro-worker' AI development approach?\n\nWhy does Acemoglu view Trump as symptom, not cause?\n\nHow could AI-driven inequality threaten American democracy?\n\nAcemoglu argued that while Trump’s authoritarian tendencies are weakening the country’s institutions, the president is not the root cause of the broader structural problems. He warned the country is headed down a grim path and outlined two shifts relative to AI development he sees as critical to avoiding deeper decline: cracking down on economic inequality and tempering job destruction. “If we go down this path of destroying jobs [and] creating more inequality, U.S. democracy is not going to survive,” he told Fortune.\n\nOne: The proliferation of economic inequality\n\nAccording to Acemoglu, AI-driven job displacement could be catastrophic and further entrench inequality. He notes the U.S. is currently seeing unprecedented levels of wealth inequality, and traditional policy has failed to close the gap. “We may need wealth taxes because anything else we do today is still going to lead to this huge wealth gap that exists in this country.”\n\nThe economist pointed to California’s proposed “billionaire tax,” a ballot initiative which would impose a one-time 5% wealth tax on all individuals in the state with a net worth of $1 billion or more. But even that doesn’t go far enough, according to the economist. “It’s not enough to tax the rich,” he said. “You really need ways in which workers of all sorts of skills can take part in the growth process.”\n\nBut AI proponents say Acemoglu’s diagnosis of AI development is counterintuitive. Adam Thierer, senior fellow at the think tank R Street Institute and longtime advocate for technological innovation, believes AI will spawn opportunities, driving the economy into the future. “The way we get new and better jobs and opportunities is through technological improvements in society and our economy,” Thierer told Fortune.",
    "readingTime": 3,
    "keywords": [
      "american democracy",
      "economic inequality",
      "economist",
      "institutions",
      "wealth",
      "political",
      "development",
      "view",
      "origins",
      "nations"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/nobel-laureate-co-wrote-why-123200600.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/rBqMBpCFFLa7NA14F_eTaw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/5092fa899dbfbd09978599cff4527229",
    "created_at": "2026-02-22T18:20:43.607Z",
    "topic": "finance"
  },
  {
    "slug": "former-irs-commissioner-heres-how-we-used-ai-to-create-immediate-value-when-taxpayers-scrutinized-every-dollar",
    "title": "Former IRS Commissioner: Here’s how we used AI to create immediate value when taxpayers scrutinized every dollar",
    "description": "In 2023, we began deploying AI in targeted ways to improve taxpayer service, compliance, and operational efficiency.",
    "fullText": "Danny Werfel is a strategic advisory board member at alliant and was the nation’s 50th Commissioner of the Internal Revenue Service, serving from 2023 to 2025. \r\nHis expansive career began in public service within the Office of Management and Budget, where he ultimately ascended to the position of OMB Controller. In 2014, he joined Boston Consulting Group’s Public Sector practice, working with public and private agencies on financial strategy, transformation plans, and risk-assessment initiatives. He was elected Managing Director and Partner at BCG in 2017.\r\nNow at alliant, Werfel helps guide clients through large-scale transformation, change management, and modernization strategy.",
    "readingTime": 1,
    "keywords": [
      "alliant",
      "management",
      "strategy",
      "transformation",
      "werfel",
      "service"
    ],
    "qualityScore": 0.45,
    "link": "https://fortune.com/2026/02/22/danny-werfel-former-irs-commissioner-how-we-used-ai/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/02/GettyImages-2151941777.jpg?resize=1200,600",
    "created_at": "2026-02-22T12:26:20.668Z",
    "topic": "business"
  },
  {
    "slug": "a-top-anthropic-engineer-warns-ai-agents-will-transform-every-computerbased-job-in-america-and-it-will-be-painful",
    "title": "A top Anthropic engineer warns AI agents will transform every computer-based job in America — and it will be 'painful'",
    "description": "Claude Code's creator said Anthropic's AI tool can use a computer like a human, and people are just starting to get a sense of its power.",
    "fullText": "A top Anthropic engineer said a new generation of AI agents capable of operating computers will reshape nearly every internet-based job in America.\n\nAnd he said the change is coming very soon.\n\nBoris Cherny — the creator of Claude Code at Anthropic, the company best known for its Claude chatbot — recently appeared on \"Lenny's Podcast,\" hosted by Lenny Rachitsky.\n\nHe said AI systems that can take action across workplace computer tools — like the ones Anthropic sells access to — are advancing rapidly and could soon alter responsibilities for software engineers, product managers, designers, and other knowledge workers.\n\n\"It's going to expand to pretty much any kind of work that you can do on a computer,\" Cherny said. \"In the meantime, it's going to be very disruptive. It's going to be painful for a lot of people.\"\n\nClaude Code is Anthropic's AI coding agent built on top of its Claude models. The company released its latest updates, called Opus 4.6, in early February.\n\nUnlike a traditional chatbot that generates text or images, an AI agent can use digital tools — running commands, analyzing documents, messaging colleagues, completing tasks across apps, and even building websites.\n\nEssentially, Claude Code can increasingly use a computer the way a human does — though the company recently said it has yet to reach the level of a skilled human.\n\n\"It's the thing that I think brings agentic AI to people that haven't really used it before, and people are starting to just get a sense of it for the first time,\" he said.\n\nCherny says his own team already relies on AI to work faster. Productivity per engineer has increased sharply since Claude Code's launch, he said. He believed the models will continue improving. (Of course, Cherny also has good reason to talk up the company's products, which it shops to enterprise companies.)\n\nCherny recently said in an interview with Y Combinator's \"Lightcone\" podcast that the job title software engineer will start to \"go away\" in 2026.\n\nThe broader impact remains uncertain, he warned.\n\n\"As a society, this is a conversation we have to figure out together,\" he told Rachitsky. \"Anyone can just build software anytime.\"\n\nFor workers navigating the shift, his advice is direct: experiment with AI tools and learn how they function.\n\n\"Don't be scared of them,\" he said.",
    "readingTime": 2,
    "keywords": [
      "claude code",
      "it's",
      "engineer",
      "recently",
      "computer",
      "tools",
      "software",
      "soon",
      "chatbot",
      "across"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-boris-cherny-ai-impact-computer-jobs-painful-change-2026-2",
    "thumbnail_url": "https://i.insider.com/69988ceb156648bc16a898ba?width=1200&format=jpeg",
    "created_at": "2026-02-22T12:26:20.319Z",
    "topic": "finance"
  },
  {
    "slug": "revenge-of-the-english-majors-the-age-of-ai-is-driving-new-respect-for-humanities-skills",
    "title": "Revenge of the English majors: The age of AI is driving new respect for humanities skills",
    "description": "For years, English majors were mocked as useless. Now, AI is giving them some momentum in the job market, while computer science grads get disrupted.",
    "fullText": "At the University of Colorado Boulder, you can take a course co-taught by an applied mathematician and a Renaissance scholar.\n\n\"The students love it,\" said John-Michael Rivera, the school's dean of arts and humanities, of the class, which is called Inclusive Interdisciplinary Data Science for All.\n\nThe class gives STEM students a way to think about the ethics of AI, he said. In other courses, humanities majors can use their skills to evaluate how AI writes, what it means for the practice of writing, and what the \"self\" means in an AI world.\n\nRivera credits the creation of courses focusing on the intersection of AI and humanities with a resurgence in student interest in liberal arts degrees like English. Pre-pandemic, the number of English majors at the university was shrinking, part of a broader decline in English across the country, he said. It was a far cry from the days of over 1,500 majors and long waitlists in the early 2000s, according to Rivera. But there's been a rebound, with the number of English majors rising 9% since 2021.\n\nRivera said students \"want to know more about the 'why' these days. And that's what we do in humanities. We really engage in the 'why.'\"\n\nDerided by some as useless, the utility of the English major has long been questioned. Who needs to write essays (or articles) any\n\n\"We are certainly seeing organizations look more towards the soft skills, the accountability of a job, the identity of the person, their style, their empathy — their humanity,\" in a world that requires both humans and technology, said Bryan Ackermann, head of AI strategy and transformation at recruiting and organizational consulting firm Korn Ferry.\n\nFor the English majors, that's all offered some degree of vindication. As the conversation heats up over which skills will be useful in an AI world, one camp argues it's time for ideas, people, and critical thinkers to flourish. That means that, after years of mocking, English majors are finally getting recognized for their usefulness. Some schools are seeing enrollment in the major rise after years of decline; technical recruiters and experts are seeing greater demand for humanities skills. Call it the makeover of the English major.\n\nJessie Hennen directs the creative writing and literature programs at Southwest Minnesota State University, a large public school with a returning and transfer student population.\n\n\"They've had jobs, they have experience, and they're just like, we are not letting AI take creative writing away from us,\" Hennen said. \"And I think that has to do with the fact that creative writing is — it's a business, but it's also an art, and arts are imperfect; we do them for human reasons that are not just to make money.\"\n\nShe said that their program has been growing over the last two to three years.\n\n\"I would say we're starting to see trends that look really promising for students starting to ask, 'Can the humanities sustain me at a time when everything is moving so quickly?'\" Rivera, the dean at the University of Colorado Boulder, said. Those students \"really want to reflect what it means to be part of a technological world.\"\n\nThat's also the case at Rice University in Houston, where enrollment in English classes has grown steadily over the last few years and the number of faculty within Creative Writing has nearly doubled, according to Kathleen Canning, the dean of humanities and arts.\n\nOne example of an assignment is an English professor who will issue an essay prompt and ask students to compare their own version to one they get from an LLM, and analyze the difference between the two. The aim is to examine what it means to be an interpreter of a prompt — and the power of their own words.\n\n\"Students are trying to ascertain how to develop and advance their own capacities while AI appears to do so much for them in these times,\" Canning said. \"The humanities and arts offer them opportunities not only to probe the limits of AI, to grapple with it as an increasingly powerful reality, but to do so critically by advancing their capacities for self-reflection, interpretation, and revision.\"\n\nDespite these examples, schools across the country are paring back on their humanities offerings or cutting programs completely, and the nationwide number of humanities bachelor's degrees being conferred has fallen from 2010s highs in recent years.\n\nStill, students pursue English out of passion, said Kevin Caffrey, a senior associate registrar at the University of Mary Washington in Virginia. His research found that English majors who participated in his survey \"illustrated that even with a strong overall awareness of criticisms of the major, they were determined to enroll in the program because it aligned with their interests, personal ideals, and goals for the future.\"\n\n\"What do you need more in a company than someone who knows how to communicate with people at all different levels from all different backgrounds and walks of life? The English majors are primed to do that,\" Caffrey said.\n\nThey're learning to do it as communication changes rapidly. When 23-year-old Margo D. returned from a semester abroad, she noticed something had shifted on campus.\n\n\"Many of my peers were using ChatGPT for almost every assignment,\" Margo, who graduated from a small liberal arts school in 2025 with a double major in English and Earth and Climate Sciences, said. Margo wasn't sold.\n\n\"I noticed that my English professors were asking a lot out of my writing, asking for a lot of creativity and an original voice and style, and asking me questions that AI couldn't necessarily grasp the nuance of, and I don't really think it even can now,\" Margo said. \"And so I felt really grateful to be an English major.\"\n\nThere are signs of employment hope for the English majors.\n\nDaniela Amodei, the cofounder of Anthropic, studied literature in college. In an ABC News interview, she said \"the things that make us human will become much more important,\" and that when her AI company hires, it looks for candidates who are great communicators.\n\n\"I actually think studying the humanities is going to be more important than ever,\" Amodei said.\n\nSteve Johnson, the editorial director of NotebookLM, previously told Business Insider that there's what he's deemed a \"revenge of the humanities.\" Philosophical thinking is necessary; some AI firms are even actively seeking out liberal arts graduates.\n\nStill, companies aren't falling over themselves to snap up English majors — hiring overall has slowed to one of the lowest rates in over a decade, and the recent grad unemployment rate has been ticking up.\n\nEarly-career humanities and arts graduates had a higher unemployment rate than their peers in other fields, according to an analysis of the Census Bureau's American Community Survey by Georgetown researchers.\n\nJoe Kramer, a 2020 English graduate, hasn't worked directly in a related field since he graduated — he worked in a role that relied on automation, and even helped train AI while searching for post-pandemic work.\n\n\"I think it's just getting really scary out there for a lot of humanities adjacent stuff, because the level of AI that's out there now, it generates pictures, it crawls all kinds of web forums, and it can oversee thousands of pages and documents at a time while only being run by one person,\" Kramer said. \"So even if AI isn't taking your job, they don't need to hire a lot of people anymore.\"\n\nPart of some of the general reticence to hire in hiring right now can also be chalked up to an equal-opportunity dismal labor market. It's not just English majors suffering.\n\nUnder the hood, the prospects for English majors aren't as dreary, according to the Georgetown analysis. The unemployment rate for those specifically in humanities and liberal arts is still well below the post-2008 Great Recession highs, although it's still higher than pre-pandemic levels.\n\nKorn Ferry's Ackermann said that it's still a \"tad early\" to fully declare a revenge of the English major, since it's smaller, more nimble firms looking for those with a good command of language, but he predicts that could expand to bigger employers soon.\n\n\"Ask me again in a couple of months, and we're going to see that go from smaller, nimble organizations into the larger enterprises as the larger enterprises begin to incorporate AI-driven development tools into their processes,\" he said.\n\nGiancarlo Hirsch, a managing director at global tech talent partner Glocomms, said he's seen greater willingness to look at candidates from various backgrounds. Candidates with history backgrounds, for example, are making it further into interview processes than they previously would.\n\n\"People are not explicitly targeting folks from humanities degrees, but they're really willing to speak with them and open to it and finding reasons to say yes throughout an interview process,\" Hirsch said.\n\nDaniella LaGaccia, a 37-year-old copywriter and former English literature major, sees AI as a tool — creatives use all sorts of different tools to complement their work, and AI can be one of them. But, if anything, that makes a greater case for the type of creative thinking and knowledge that humanities majors can bring.\n\n\"Think about it this way: If you have five different companies who are using the same generative tools to develop their marketing copy, they're all going to get generally the same type of thing,\" LaGaccia said. \"If everybody's using the same tools and everybody's inputting the same information, then how are you going to differentiate yourself in the market? That's where creative people come in.\"",
    "readingTime": 8,
    "keywords": [
      "colorado boulder",
      "larger enterprises",
      "unemployment rate",
      "english majors",
      "liberal arts",
      "arts graduates",
      "humanities majors",
      "students",
      "it's",
      "that's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-job-market-english-majors-humanities-demand-2026-2",
    "thumbnail_url": "https://i.insider.com/6998786c156648bc16a894e8?width=1200&format=jpeg",
    "created_at": "2026-02-22T12:26:20.281Z",
    "topic": "finance"
  },
  {
    "slug": "i-work-in-recruiting-tech-heres-how-the-smartest-candidates-use-ai-differently-in-their-job-search",
    "title": "I work in recruiting tech. Here's how the smartest candidates use AI differently in their job search.",
    "description": "A longtime HR pro says AI is making everyone's résumé look the same, when it can provide job seekers with so much more value.",
    "fullText": "This as-told-to essay is based on a conversation with Trent Cotton, an HR industry veteran who is head of talent-acquisition insights at recruiting-software company iCIMS. This story has been edited for length and clarity.\n\nAs someone who works in the talent space, I've lately noticed that many people think AI helps make their résumés look better. My response to that is: You know your AI agent is cheating on you, right? It's doing the same thing for everyone, so your effort to try to stand out is actually making you blend in.\n\nIt's OK to use an AI agent to improve your résumé, but instead of asking it to do a rewrite, which can introduce errors, prompt it to behave like an expert career coach. Have it analyze your résumé and ask you probing questions to help you further build out your experience profile.\n\nNext, ask the agent what roles you might qualify for based on your skill set. The results may include ones you didn't know about, so have it explain what those jobs involve. Sometimes you just need to get a different perspective.\n\nIn my job at iCIMS, I analyze millions of data points collected from our recruiting software. Each month, I take a look at a specific sector or hot topic and dive deep.\n\nOne thing I saw last year was a pretty decent increase in application volume in manufacturing. If I'm sitting in tech, manufacturing isn't the sector I'm thinking of, but manufacturing needs tech to do things like track what's coming in and out of warehouses. AI can provide that kind of insight — but only if you ask for it.\n\nSticking with this example, you want to ask the AI: What are some of the top manufacturing companies looking to hire tech talent, and for what kind of positions? Keep going down that rabbit hole to really understand what opportunities are out there.\n\nOnce you find job listings that interest you, plug the descriptions into your AI agent and tell it to start interviewing you. When I was in college, you always went through mock interviews. They would just be absolutely brutal with you, but they prepared you. I think we've skipped a couple of generations in doing that. AI can bring mock interviews back.\n\nTell the AI agent that you want the interviewer to be like someone with business savvy, such as Steven Bartlett from the podcast \"Diary of a CEO.\" But also say you want feedback from the likes of chef and TV personality Gordon Ramsay, because he's known for providing his employees with brutally honest, constructive feedback. Say you want a true grade about your interview performance and recommendations for ways you can get better.\n\nMost AI agents will say your answers are great if you don't give them a persona like that of Ramsay. The AI version of him is going to tell you the good, the bad, and the ugly, and you're going to be better off because of it.",
    "readingTime": 3,
    "keywords": [
      "mock interviews",
      "agent",
      "manufacturing",
      "tech",
      "based",
      "icims",
      "someone",
      "talent",
      "look",
      "doing"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/icims-expert-shares-how-smartest-job-seekers-use-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/6998b6fd2237a6a8f0cd8fb3?width=1200&format=jpeg",
    "created_at": "2026-02-22T12:26:20.044Z",
    "topic": "finance"
  },
  {
    "slug": "suspect-in-tumbler-ridge-school-shooting-described-violent-scenarios-to-chatgpt",
    "title": "Suspect in Tumbler Ridge school shooting described violent scenarios to ChatGPT",
    "description": "OpenAI employees were concerned, but didn’t alert law enforcement.",
    "fullText": "The posts raised alarms, but OpenAI declined to alert law enforcement.\n\nThe posts raised alarms, but OpenAI declined to alert law enforcement.\n\nThe suspect in the mass shooting at Tumbler Ridge, British Columbia, Jesse Van Rootselaar, was raising alarms among employees at OpenAI months before the shooting took place. This past June, Jesse had conversations with ChatGPT involving descriptions of gun violence that triggered the chatbot’s automated review system. Several employees raised concerns that her posts could be a precursor to real-world violence and encouraged company leaders to contact the authorities, but they ultimately declined.\n\nOpenAI spokesperson Kayla Wood told The Verge that, while the company considered referring the account to law enforcement, it was ultimately decided that it did not constitute an “imminent and credible risk” of harm to others. Wood said that a review of the logs did not indicate there was active or imminent planning of violence. The company banned Rootselaar’s account, but it does not appear to have taken any further precautionary action.\n\nWood said, “Our thoughts are with everyone affected by the Tumbler Ridge tragedy. We proactively reached out to the Royal Canadian Mounted Police with information on the individual and their use of ChatGPT, and we’ll continue to support their investigation.”\n\nOn February 10th, nine people were killed and 27 injured, including Rootselaar, in the deadliest mass shooting in Canada since 2020. Rootselaar was found dead at the scene of the Tumbler Ridge Secondary School, of an apparent self-inflicted gunshot wound, where most of the killings took place.\n\nThe decision not to alert law enforcement might look misguided in retrospect, but Wood said that OpenAI’s goal is to balance privacy with safety and avoid introducing unintended harm through overly broad use of law enforcement referrals.\n\nUpdated February 21st: Added statement from OpenAI.",
    "readingTime": 2,
    "keywords": [
      "openai declined",
      "mass shooting",
      "alert law",
      "law enforcement",
      "posts",
      "alarms",
      "rootselaar",
      "violence",
      "employees",
      "chatgpt"
    ],
    "qualityScore": 0.85,
    "link": "https://www.theverge.com/ai-artificial-intelligence/882814/tumbler-ridge-school-shooting-chatgpt",
    "thumbnail_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/gettyimages-2260625085.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
    "created_at": "2026-02-22T12:26:17.217Z",
    "topic": "tech"
  },
  {
    "slug": "genloopai-that-trains-generations-of-humans-instead-of-growing-data-centers",
    "title": "GenLoop–AI that trains generations of humans instead of growing data centers",
    "description": "GenLoop: Human–AI Generational Learning Loop ## Official Timestamp Document  **Author:** LAKSzero   **Contact:** Sanderslynda@rocketmail.com   **Date:** February 22, 2026, 3:22 AM EST   **Original conversation:** Perplexity AI session (archived)  ---  ## Purpose of this document  This document e...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://docs.google.com/document/d/17XDU9D7uRS1P7U0VRlI0Yogt3eiDG6QysYc7ROHwJoc/edit?usp=drivesdk",
    "thumbnail_url": "https://lh7-us.googleusercontent.com/docs/AHkbwyJnXwVCSp44u_w8i9gagDaxdRSREKMia2KV3a7eEfmxzcvy4fYHsiMRRILyC4VG6jqFTfprjBKSHfrCxlkLcWaCqUS-AFW9BGsyvYQHA6YwUWNSY243=w1200-h630-p",
    "created_at": "2026-02-22T12:26:16.703Z",
    "topic": "tech"
  },
  {
    "slug": "ai-succeeds-in-diagnosing-rare-diseases",
    "title": "AI succeeds in diagnosing rare diseases",
    "description": "DeepRare—a multi-agent system for rare disease differential diagnosis decision support powered by large language models, integrating specialized tools and up-to-date knowledge sources—has the potential to reduce healthcare disparities in rare disease diagnosis.",
    "fullText": "Five years’ experience of the clinical exome sequencing in a Spanish single center\n\n Article\n Open access\n 10 November 2022",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41586-025-10097-9",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-10097-9/MediaObjects/41586_2025_10097_Fig1_HTML.png",
    "created_at": "2026-02-22T12:26:16.095Z",
    "topic": "tech"
  },
  {
    "slug": "half-the-ai-agent-market-is-one-category-the-rest-is-wide-open",
    "title": "Half the AI Agent Market Is One Category. The Rest Is Wide Open",
    "description": "Anthropic's new data shows software engineering dominates agentic AI. For founders, that's not a warning. It's a treasure map.",
    "fullText": "Anthropic's data showing software engineering commanding nearly half of all AI agent tool calls — while healthcare, legal, and a dozen other verticals each claim under 5% — is what Han Wang calls the greenfield opportunity most founders are overlooking.\n\nSoftware engineering accounts for nearly 50% of all AI agent tool calls. Healthcare, legal, finance, and a dozen other verticals are barely touched, each under 5%. That’s 300 vertical AI unicorns waiting to be built.\n\nIf I were starting a company today, I’d stare at the red rectangular area of the bar chart above until I saw my future.\n\nArchived tweet\n\n This chart is a good reminder of how much opportunity there is in AI agents right now. \n\nThere will be plenty of horizontal opportunities for agents, but equally many workflows that need deep domain expertise to actually make the user successful at automating the unique processes in their vertical.\n\nThe template is to build agentic software that taps into proprietary data, handles the workflow in a way that bridges the user and the agent collaboration effectively, and has a deep domain-specific context engineering, and the ability to drive change management for customers.\n\nThere still are huge openings in many categories.\n\n[Quoting @handotdev]:\nwhat I would be working on if I started another company today https://t.co/kKDFxcbtZv https://t.co/7dpDyiHAW6\n\n Aaron Levie\n @levie\n\n February 21, 2026\n\nThis chart is a good reminder of how much opportunity there is in AI agents right now. \n\nThere will be plenty of horizontal opportunities for agents, but equally many workflows that need deep domain expertise to actually make the user successful at automating the unique processes in their vertical.\n\nThe template is to build agentic software that taps into proprietary data, handles the workflow in a way that bridges the user and the agent collaboration effectively, and has a deep domain-specific context engineering, and the ability to drive change management for customers.\n\nThere still are huge openings in many categories.\n\n[Quoting @handotdev]:\nwhat I would be working on if I started another company today https://t.co/kKDFxcbtZv https://t.co/7dpDyiHAW6\n\nSoftware engineering owns half of all AI agent activity. The other half is scattered across 16 verticals, none above 9%. Healthcare is 1%. Legal is 0.9%. Education is 1.8%. These aren’t saturated markets. They’re markets that barely exist.\n\nAnthropic just published the most comprehensive study of how AI agents actually work in the wild. The headline: software engineering accounts for 49.7% of agentic tool calls on their API. The buried lede: everything else is greenfield.\n\nHere’s what should make founders salivate: the models are already more capable than users trust them to be.\n\nMETR’s capability assessments show Claude can solve tasks that would take a human nearly five hours. But in practice, the 99.9th percentile session runs only about 42 minutes. That gap, between what AI can do and what we let it do, is a massive opportunity.\n\nBetween October 2025 and January 2026, the 99.9th percentile turn duration nearly doubled, from under 25 minutes to over 45 minutes. The growth is smooth across model releases. This isn’t just better models. It’s users extending trust, session by session, as they learn to work alongside agents.\n\nThe capability is there. The deployment isn’t. That’s not a problem. That’s a product opportunity.\n\nNew users approve 20% of Claude Code sessions automatically. By 750 sessions, over 40% run on full auto-approve. But here’s the counterintuitive finding: experienced users also interrupt MORE, not less. New users interrupt 5% of turns. Veterans interrupt 9%.\n\nThis isn’t a contradiction. It’s a shift in oversight strategy. Beginners approve each step before it happens. Veterans delegate and intervene when something goes wrong. They’ve moved from pre-approval to active monitoring.\n\nAnd here’s the safety finding that matters: on complex tasks, Claude Code asks for clarification more than twice as often as humans interrupt it. The agent is pausing to check, not barreling ahead. That’s a feature, not a bug.\n\nAaron Levie points to the untold wealth and value ready to be unlocked. Build agentic software that taps into proprietary data. Make the software actually work for real people and problems. Stuff that context to maximize intelligence coming out. And, the part most founders miss: drive change management for customers.\n\nThat last piece is why vertical AI is so defensible. Anyone can build a wrapper. Few can navigate the specific workflows, regulatory constraints, and organizational friction of healthcare billing or legal discovery or construction permitting.\n\nSaaS has grown 10x per decade for a few decades now. Over 40% of VC dollars in the past 20 years went to SaaS companies. The industry produced 170+ SaaS unicorns. The thesis is simple: every one of those unicorns has a vertical AI equivalent waiting. And the AI versions could be 10x larger, because they don’t just replace software, they replace the operators too.\n\nAnthropic’s core finding deserves attention from anyone writing AI policy. Autonomy isn’t a property of the model. It’s co-constructed by the model, the user, and the product. Pre-deployment evaluations can’t capture this. You have to measure in the wild.\n\nArchived tweet\n\n Software engineering makes up ~50% of agentic tool calls on our API, but we see emerging use in other industries. \n\nAs the frontier of risk and autonomy expands, post-deployment monitoring becomes essential. We encourage other model developers to extend this research. https://t.co/p8pOjgJPrh\n\n Anthropic\n @AnthropicAI\n\n February 18, 2026\n\nSoftware engineering makes up ~50% of agentic tool calls on our API, but we see emerging use in other industries. \n\nAs the frontier of risk and autonomy expands, post-deployment monitoring becomes essential. We encourage other model developers to extend this research. https://t.co/p8pOjgJPrh\n\nThe numbers are reassuring on safety: 73% of tool calls have a human in the loop. Only 0.8% of actions are irreversible. The riskiest deployments, things like API key exfiltration or autonomous crypto trading, are mostly security evaluations, not live production.\n\nPolicy that mandates “approve every action” will kill the productivity gains without adding safety. The better target is ensuring humans can monitor and intervene, not mandating specific approval workflows.\n\nThe map is drawn. Software engineering is spoken for. Healthcare, legal, finance, education, customer service, logistics, 16 verticals with single-digit market share each, are waiting for someone to build the domain expertise into the agent.\n\n300 SaaS unicorns came before. 300 vertical AI unicorns are coming next. The founders who pick a vertical, build domain expertise into their agents, and figure out change management will own the next decade of enterprise software.\n\nThe models can already work for five hours. Users only let them work for 42 minutes. That’s an indicator: we are so early, and there is a lot more to build, and in so many places that haven’t even seen a single minute of intelligence in action.\n\nRead Anthropic's full research on AI agent autonomy",
    "readingTime": 6,
    "keywords": [
      "archived tweet",
      "categories quoting",
      "quoting handotdev",
      "healthcare legal",
      "saas unicorns",
      "horizontal opportunities",
      "unique processes",
      "collaboration effectively",
      "huge openings",
      "https://t.co/kkdfxcbtzv https://t.co/7dpdyihaw"
    ],
    "qualityScore": 1,
    "link": "https://garryslist.org/posts/half-the-ai-agent-market-is-one-category-the-rest-is-wide-open",
    "thumbnail_url": "https://garryslist.org/og/704.jpg",
    "created_at": "2026-02-22T12:26:15.733Z",
    "topic": "tech"
  },
  {
    "slug": "building-a-c-compiler-from-scratch-with-aidriven-development",
    "title": "Building a C Compiler from Scratch with AI-Driven Development",
    "description": "How Fastcc, a self-hosting ARM64 C compiler, was built from scratch in 10 days with largely autonomous AI-driven development.",
    "fullText": "Recently, we completed Fastcc, a self-hosting C compiler built from scratch. The project was carried out with minimal human involvement and targets the ARM64 architecture.\n\nGithub Repo: https://github.com/moonbit-community/fastcc\n\nWe set a deliberately ambitious goal: to start from zero and build a C compiler, while keeping human involvement as limited as possible.\n\nThe initial motivation was to understand how current AI systems behave when tasked with a large, end-to-end software project.\n\nTraditionally, building a full C compiler is considered a complex engineering task. It involves multiple stages — lexical analysis, parsing, semantic checks, optimization passes, and code generation — and typically requires deep domain knowledge and months (or even years) of focused work.\n\nThe process began with a single voice instruction to the AI agent:\r\n“Build a C compiler from scratch, close to tcc, targeting ARM64.”\n\nWe chose tcc (Tiny C Compiler) as a reference because of its fast compilation speed, which is particularly important for MoonBit’s development workflow. MoonBit’s Native backend supports both LLVM and C, and having a dedicated C compiler enables full self-hosting.\n\nAt the same time, tcc is unsafe, poorly maintained, and leaves room for architectural improvements. To keep the scope focused, we limited the target to ARM64.\n\nHere, “self-hosting” means Fastcc was able to compile the C output generated from its own source and run its test suite:\n\nFastcc could also compile the tcc source code at this stage. For performance testing we used v.c, a single-file snapshot of the V compiler.\n\nIn early benchmarks, Fastcc was about 60× slower than tcc on this input. After subsequent profiling and targeted optimizations, the compilation throughput improved substantially, reaching around 4× faster than clang -O0 on the same benchmark.\n\nThroughout most of the development process, the AI agent decomposed and implemented tasks autonomously. Its work included:\n\nDesigning the abstract syntax tree (AST)\n\nGenerating core compiler modules\n\nImplementing multiple optimization passes\n\nDebugging using lldb as part of its own investigation\n\nProfiling performance hotspots using Xcode command-line tools, based on high-level guidance\n\nWriting scripts to identify hot paths and guide targeted optimizations\n\nAlthough the initial directive referenced tcc’s structure, the agent chose a multi-pass design rather than a single-pass model, prioritizing correctness and extensibility over strict structural similarity.\n\nHuman involvement was limited to occasional guidance and corrective direction, mainly at the level of goals and evaluation rather than step-by-step instruction.\n\nIt’s worth noting that this outcome was not accidental. It was made possible by MoonBit’s toolchain and language design, which together enable sustained, large-scale, agent-driven software development.",
    "readingTime": 3,
    "keywords": [
      "targeted optimizations",
      "human involvement",
      "fastcc",
      "self-hosting",
      "limited",
      "agent",
      "development",
      "compiler",
      "scratch",
      "project"
    ],
    "qualityScore": 1,
    "link": "https://www.moonbitlang.com/blog/fastcc-ai-driven-development",
    "thumbnail_url": "https://www.moonbitlang.com/img/blogs/2026-02-06-fastcc-ai-driven-development/cover.jpg",
    "created_at": "2026-02-22T12:26:14.835Z",
    "topic": "tech"
  },
  {
    "slug": "gotoassistant-selfhosted-ai-assistant-one-npx-command-no-docker",
    "title": "goto-assistant – Self-hosted AI assistant, one npx command, no Docker",
    "description": "Lightweight, self-hosted AI assistant with first-class MCP support - jolks/goto-assistant",
    "fullText": "jolks\n\n /\n\n goto-assistant\n\n Public\n\n Lightweight, self-hosted AI assistant with first-class MCP support\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jolks/goto-assistant",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jolks/goto-assistant",
    "thumbnail_url": "https://opengraph.githubassets.com/9146b7af38b74c48e348f9784d7404c2fbe3ce4890b35ab6ea9129aa31fa1da1/jolks/goto-assistant",
    "created_at": "2026-02-22T12:26:14.802Z",
    "topic": "tech"
  },
  {
    "slug": "im-worried-my-boyfriends-use-of-ai-is-affecting-his-ability-to-think-for-himself-annalisa-barbieri",
    "title": "I’m worried my boyfriend’s use of AI is affecting his ability to think for himself | Annalisa Barbieri",
    "description": "Overdependence on chatbots is a growing problem, and though your boyfriend’s ADHD may be a factor, he needs to find the root of his anxiety\nMy boyfriend of eight years, who is 44, has ADHD and runs his own business. He’s always struggled with admin and mundane tasks, but AI has revolutionised how he works. Now I’m worried he can’t seem to do anything without AI. He is a heavy ChatGPT user and uses it even when there’s a better non-AI alternative (eg he’ll ask it for train times rather than using Trainline, even though it’s less accurate). He just got his ChatGPT Wrapped and he’s in the top 0.",
    "fullText": "Overdependence on chatbots is a growing problem, and though your boyfriend’s ADHD may be a factor, he needs to find the root of his anxiety\n\nMy boyfriend of eight years, who is 44, has ADHD and runs his own business. He’s always struggled with admin and mundane tasks, but AI has revolutionised how he works. Now I’m worried he can’t seem to do anything without AI. He is a heavy ChatGPT user and uses it even when there’s a better non-AI alternative (eg he’ll ask it for train times rather than using Trainline, even though it’s less accurate). He just got his ChatGPT Wrapped and he’s in the top 0.3% of users worldwide.\n\nI worry about his ability to think independently, as well as the environmental impact. I know it’s a useful tool for him at work, but he uses it for everything in life.\n\nI’m very aware I can come across as quite naggy, and his ADHD can make him obsessive. I’d love some advice on how to approach this with him .\n\nRunning a business can be stressful, and although your boyfriend’s ADHD may be a factor, I wonder if he is anxious anyway and if his use of AI is a symptom rather than the cause.\n\nI took your letter to consultant clinical psychologist and psychoanalyst Dr Stephen Blumenthal and Henry Shelford, CEO of ADHD UK.\n\nBlumenthal wondered if we are “on the verge of a new diagnostic category of ‘chatbot overdependence syndrome’ as we head into an age in which we become increasingly reliant on AI. When used judiciously, AI aids us, but it could have disastrous consequences if we become dependent on it and lose the capacity for ordinary functioning.\n\n“Someone with ADHD has a shorter attention span, difficulty focusing and a reduced capacity to plan and think ahead, so AI is a perfect fit, which is why it can be so helpful. The downside is that there is a greater propensity to become overdependent on it.”\n\nShelford wondered if your boyfriend was struggling anyway, and if the AI provided a useful “flotation aid”? “AI can take you down a rabbit hole,” he said, “but it can also support you and help you structure your thoughts, schedule stuff and get things done.”\n\nYour boyfriend’s use of AI seems to go beyond this. It’s as if he’s doubting himself, and that can be pernicious.\n\nBlumenthal says: “Problems arise when your use of AI goes beyond satisfying the problem you wish to resolve. It feels as if a relationship with it has started to develop, and you imbue it with human qualities, a projection of our own wishes and desires for validation and care.”\n\nWhat to do? You’re right not to nag, which rarely solves anything, because it just becomes noise. As with all tender and difficult conversations, pick your moment when you’re both calm.\n\nShelford recommended asking your boyfriend, “‘What are you getting out of it? Why is this tool such a big deal and what are the gaps it’s filling?’ Then look to see if there are better solutions or better ways to use it.”\n\nBlumenthal thought: “as with any overdependence syndrome, there first needs to be recognition that there is a problem. It’s easy to become critical of the person who’s struggling, but that’s only likely to cause them to withdraw further into dependency. The case must be made compassionately, recognising that being without the scaffold of ChatGPT probably feels like a threat.”\n\nThe good news is that, unlike the generation now growing up with AI, your boyfriend has a track record of functioning well without it. Hopefully he can be reminded of that and find a place where AI augments the abilities he already has. But it sounds as if he’s anxious and I think the cause has to be found so you can both move forward.\n\nEvery week, Annalisa Barbieri addresses a personal problem sent in by a reader. If you would like advice from Annalisa, please send your problem to ask.annalisa@theguardian.com. Annalisa regrets she cannot enter into personal correspondence. Submissions are subject to our terms and conditions. The latest series of Annalisa’s podcast is available here.",
    "readingTime": 4,
    "keywords": [
      "boyfriend’s adhd",
      "overdependence syndrome",
      "it’s",
      "he’s",
      "blumenthal",
      "without",
      "cause",
      "shelford",
      "factor",
      "needs"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/lifeandstyle/2026/feb/22/worried-boyfriend-ai-affecting-ability-think-for-himself-annalisa-barbieri",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a3c37d8c0042c8e1a4766c7e77f547dbd5a14d36/588_0_7808_6250/master/7808.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ccb32e91808c9e23e18a98f6df9a4c73",
    "created_at": "2026-02-22T06:35:08.445Z",
    "topic": "tech"
  },
  {
    "slug": "hashtrade-opensource-llm-trading-agent-with-episodic-memory",
    "title": "HashTrade – Open-source LLM trading agent with episodic memory",
    "description": "A real-time cryptocurrency trading dashboard powered by AI agents with multi-exchange support - mertozbas/hashtrade",
    "fullText": "mertozbas\n\n /\n\n hashtrade\n\n Public\n\n A real-time cryptocurrency trading dashboard powered by AI agents with multi-exchange support\n\n hashtrade.ai/\n\n License\n\n Apache-2.0 license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mertozbas/hashtrade",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mertozbas/hashtrade",
    "thumbnail_url": "https://opengraph.githubassets.com/cfc80e814439493d2d46c6bfed57d95f291071dc052fc192e3b61ed2c8daab3d/mertozbas/hashtrade",
    "created_at": "2026-02-22T06:35:07.747Z",
    "topic": "tech"
  },
  {
    "slug": "brood-a-referencefirst-ai-image-editor-for-macos",
    "title": "Brood, a reference-first AI image editor for macOS",
    "description": "open-source, promptless canvas for creative image mutation (macOS desktop) - kevinshowkat/brood",
    "fullText": "kevinshowkat\n\n /\n\n brood\n\n Public\n\n open-source, promptless canvas for creative image mutation (macOS desktop)\n\n github.com/kevinshowkat/brood#readme\n\n License\n\n Apache-2.0 license\n\n 54\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n kevinshowkat/brood",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/kevinshowkat/brood",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1144159770/8ba7e67d-1aed-4398-b6ff-7501a7de586f",
    "created_at": "2026-02-22T06:35:07.176Z",
    "topic": "tech"
  },
  {
    "slug": "clawscan-opensource-security-scanner-for-openclaw-ai-agents",
    "title": "Clawscan – Open-source security scanner for OpenClaw AI agents",
    "description": "Security scanner for OpenClaw AI agent setups. Zero deps. One file. 18 checks. - osmankidwai-bot/clawscan",
    "fullText": "osmankidwai-bot\n\n /\n\n clawscan\n\n Public\n\n Security scanner for OpenClaw AI agent setups. Zero deps. One file. 18 checks.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n osmankidwai-bot/clawscan",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/osmankidwai-bot/clawscan",
    "thumbnail_url": "https://opengraph.githubassets.com/38b3f3e127d72ed5ff87c62246221a60c412ee1def4c396228a7542bf5ce93b9/osmankidwai-bot/clawscan",
    "created_at": "2026-02-22T06:35:07.140Z",
    "topic": "tech"
  },
  {
    "slug": "3-key-earnings-reports-for-this-week-to-keep-the-ai-trade-alive",
    "title": "3 key earnings reports for this week to keep the AI trade alive",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/3-key-earnings-reports-for-this-week-to-keep-the-ai-trade-alive-4517547",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPEB1A16P_M.jpg",
    "created_at": "2026-02-22T06:35:06.940Z",
    "topic": "finance"
  },
  {
    "slug": "chinas-ai-rally-ignites-as-investors-shrug-off-global-disruption-fears",
    "title": "China’s AI rally ignites as investors shrug off global disruption fears",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/chinas-ai-rally-ignites-as-investors-shrug-off-global-disruption-fears-4517553",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPEB0E0CQ_M.jpg",
    "created_at": "2026-02-22T06:35:06.820Z",
    "topic": "finance"
  },
  {
    "slug": "sam-altman-says-elon-musks-idea-of-putting-data-centers-in-space-is-ridiculous",
    "title": "Sam Altman says Elon Musk's idea of putting data centers in space is 'ridiculous'",
    "description": "While in New Delhi on Friday, OpenAI CEO Sam Altman said \"there will come a time\" for orbital data centers, but it won't be anytime soon.",
    "fullText": "SpaceX CEO Elon Musk and OpenAI CEO Sam Altman famously don't agree on much.\n\nThe latest point of contention: data centers in space. Musk has made it a priority. Altman thinks it's a fantasy, at least for now.\n\n\"I honestly think the idea with the current landscape of putting data centers in space is ridiculous,\" Altman said during a live interview with local media in New Delhi on Friday, causing audience members to laugh.\n\nAltman said that orbital data centers could \"make sense someday,\" but factors like launch costs and the difficulty of repairing a computer chip in space remain overwhelming obstacles.\n\n\"We are not there yet,\" Altman added. \"There will come a time. Space is great for a lot of things. Orbital data centers are not something that's going to matter at scale this decade.\"\n\nMusk would almost certainly disagree.\n\nWhile many Big Tech and AI companies are spending billions on data center construction on Earth, Musk's eyes are on the stars, per usual. Orbital data centers are his latest ambition, as he mentioned in an all-hands xAI meeting in December.\n\nIn February, SpaceX said its goal is to launch a \"constellation of a million satellites that operate as orbital data centers.\" The company has already begun hiring engineers to make that happen.\n\nDuring an all-hands meeting with xAI employees this month, Musk said SpaceX's acquisition of xAI will allow them to deploy the orbital data centers faster.\n\nDespite Altman's skepticism, other tech leaders are also racing to place data centers in space. Google's Project Suncatcher, unveiled in November 2025, aims to do just that. Google CEO Sundar Pichai told Fox News Sunday the company could start placing data centers — powered by the sun — in space as early as 2027.\n\nTech and AI companies rely on data centers to power their products, like large language models and chatbots. Those data centers, however, can deplete water resources, strain power grids, increase pollution, and decrease the overall quality of life.\n\nAn investigation by Business Insider published last year found that over 1,200 data centers had been approved for construction across the US by the end of 2024, nearly four times the number from 2010.\n\nNow, proposed data center campuses in Texas, Oklahoma, and elsewhere are increasingly facing stiff resistance from local communities.",
    "readingTime": 2,
    "keywords": [
      "centers",
      "orbital",
      "latest",
      "launch",
      "center",
      "construction",
      "all-hands",
      "space",
      "altman",
      "musk"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-elon-musk-data-centers-space-timeline-2026-2",
    "thumbnail_url": "https://i.insider.com/699a347d156648bc16a8a8e3?width=1200&format=jpeg",
    "created_at": "2026-02-22T01:11:32.669Z",
    "topic": "finance"
  },
  {
    "slug": "quill-a-systemwide-tech-dictionary-for-the-ai-coding-era",
    "title": "Quill – A system-wide tech dictionary for the AI coding era",
    "description": "Learn what AI writes for you. A system-wide tech dictionary for macOS — select any term, press a shortcut, get an instant explanation at your level. - uptakeagency/quill",
    "fullText": "uptakeagency\n\n /\n\n quill\n\n Public\n\n Learn what AI writes for you. A system-wide tech dictionary for macOS — select any term, press a shortcut, get an instant explanation at your level.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n uptakeagency/quill",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/uptakeagency/quill",
    "thumbnail_url": "https://opengraph.githubassets.com/a583f40305ae1e7f907452739ae864ce013e2d1098ad6c2d2624177cc2cc4f3c/uptakeagency/quill",
    "created_at": "2026-02-22T01:11:31.810Z",
    "topic": "tech"
  },
  {
    "slug": "ambient-local-cognitive-daemon-with-episodicprocedural-memory",
    "title": "Ambient – Local cognitive daemon with episodic+procedural memory",
    "description": "Local-first cognitive intelligence layer for macOS — watches knowledge sources, correlates cognitive context, runs local LLM reasoning - sgunadhya/ambient",
    "fullText": "sgunadhya\n\n /\n\n ambient\n\n Public\n\n Local-first cognitive intelligence layer for macOS — watches knowledge sources, correlates cognitive context, runs local LLM reasoning\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n sgunadhya/ambient",
    "readingTime": 1,
    "keywords": [
      "cognitive"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/sgunadhya/ambient",
    "thumbnail_url": "https://opengraph.githubassets.com/4c21d4ae9d8a9255f034f4f4466653d1f4e7bc88342e06e85ea15429064ebd78/sgunadhya/ambient",
    "created_at": "2026-02-22T01:11:31.137Z",
    "topic": "tech"
  },
  {
    "slug": "apple-researchers-develop-ondevice-ai-agent-that-interacts-with-apps-for-you",
    "title": "Apple researchers develop on-device AI agent that interacts with apps for you",
    "description": "Despite having just 3 billion parameters, Ferret-UI Lite matches or surpasses the benchmark performance of models up to 24 times larger.",
    "fullText": "Despite having just 3 billion parameters, Ferret-UI Lite matches or surpasses the benchmark performance of models up to 24 times larger. Here are the details.\n\nIn December 2023, a team of 9 researchers published a study called “FERRET: Refer and Ground Anything Anywhere at Any Granularity”. In it, they presented a multimodal large language model (MLLM) that was capable of understanding natural language references to specific parts of an image:\n\nSince then, Apple has published a series of follow-up papers expanding the Ferret family of models, including Ferretv2, Ferret-UI, and Ferret-UI 2.\n\nSpecifically, Ferret-UI variants expanded on the original capabilities of FERRET, and were trained to overcome what the researchers defined as a shortcoming of general-domain MLLMs.\n\nFrom the original Ferret-UI paper:\n\nRecent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate “any resolution” on top of Ferret to magnify details and leverage enhanced visual features.\n\nA few days ago, Apple expanded the Ferret-UI family of models even further, with a study called Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents.\n\nFerret-UI was built on a 13B-parameter model, which focused primarily on mobile UI understanding and fixed-resolution screenshots. Meanwhile, Ferret-UI 2 expanded the system to support multiple platforms and higher-resolution perception.\n\nAccording to the researchers of the new paper, “the majority of existing methods of GUI agents […] focus on large foundation models.” That is because “the strong reasoning and planning capabilities of large server-side models allow these agentic systems to achieve impressive capabilities in diverse GUI navigation tasks.”\n\nThey note that while there has been a lot of progress on both multi-agent, and end-to-end GUI systems, that take different approaches to streamline the many tasks that involve agentic interaction with GUIs (“low-level GUI grounding, screen understanding, multi-step planning, and self-reflection”), they are basically too large and compute-hungry to run well on-device.\n\nSo, they set out to develop Ferret-UI Lite, a 3-billion parameter variant of Ferret-UI, which “is built with several key components, guided by insights on training small-scale” language models.\n\nThe result is a model that closely matches or even outperforms competing GUI agent models that are up to 24 times its parameter count.\n\nWhile the entire architecture (which is thoroughly detailed in the study) is interesting, the real-time cropping and zooming-in techniques are particularly noteworthy.\n\nThe model makes an initial prediction, crops around it, then re-predicts on that cropped region. This helps such a small model compensate for its limited capacity to process large numbers of image tokens.\n\nAnother notable contribution of the paper is how Ferret-UI Lite basically generates its own training data. The researchers built a multi-agent system that interacts directly with live GUI platforms to produce synthetic training examples at scale.\n\nThere is a curriculum task generator that proposes goals of increasing difficulty, a planning agent breaks them into steps, a grounding agent executes them on-screen, and a critic model evaluates the results.\n\nWith this pipeline, the training system captures the fuzziness of real-world interaction (such as errors, unexpected states, and recovery strategies), which is something that would be much more challenging to do while relying on clean, human-annotated data.\n\nInterestingly, while Ferret-UI and Ferret-UI 2 used iPhone screenshots and other Apple interfaces in their evaluations, Ferret-UI Lite was trained and evaluated on Android, web, and desktop GUI environments, using benchmarks like AndroidWorld and OSWorld.\n\nThe researchers don’t note explicitly why they chose this route for Ferret-UI Lite, but it likely reflects where reproducible, large-scale GUI-agent testbeds are available today.\n\nBe it as it may, the researchers found that while Ferret-UI Lite performed well on short-horizon, low-level tasks, it did not perform as strongly on more complicated, multi-step interactions, a trade-off that would be largely expected, given the constraints of a small, on-device model.\n\nOn the other hand, Ferret-UI Lite offers a local, and by extension, private (since no data needs to go to the cloud and be processed on remote servers) agent that autonomously interacts with app interfaces based on user requests, which, by all accounts, is pretty cool.\n\nTo learn more about the study, including benchmark breakdowns and results, follow this link.\n\nCheck out 9to5Mac on YouTube for more Apple news:",
    "readingTime": 4,
    "keywords": [
      "general-domain mllms",
      "ferret-ui lite",
      "gui agents",
      "language models",
      "researchers",
      "study",
      "understanding",
      "apple",
      "capabilities",
      "paper"
    ],
    "qualityScore": 1,
    "link": "https://9to5mac.com/2026/02/20/apple-researchers-develop-on-device-ai-agent-that-interacts-with-apps-for-you/",
    "thumbnail_url": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/Reddit-is-being-spammed-by-AI-bots-and-its-all-Reddits-fault.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "created_at": "2026-02-22T01:11:30.949Z",
    "topic": "science"
  },
  {
    "slug": "a-minimal-frameworkagnostic-agenttoagent-execution-layer",
    "title": "A minimal framework-agnostic agent-to-agent execution layer",
    "description": "A minimal interoperability protocol that lets developers connect existing AI agents without rewriting them. Bring your agent, declare its capabilities, and collaborate with others. - joaquinariasco...",
    "fullText": "joaquinariasco-lab\n\n /\n\n flowing\n\n Public\n\n A minimal interoperability protocol that lets developers connect existing AI agents without rewriting them. Bring your agent, declare its capabilities, and collaborate with others.\n\n License\n\n Apache-2.0 license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n joaquinariasco-lab/flowing",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/joaquinariasco-lab/flowing",
    "thumbnail_url": "https://opengraph.githubassets.com/22eaf331ae687074c71a463339909a80a6623f9f2a0937f0f4cadb448df504ae/joaquinariasco-lab/flowing",
    "created_at": "2026-02-22T01:11:30.576Z",
    "topic": "tech"
  },
  {
    "slug": "my-ai-research-to-obsidian-workflow",
    "title": "My AI research to Obsidian workflow",
    "description": "How I Turn AI Research Into Notes I Actually Revisit (OpenClaw + Obsidian)",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/AdilMouja/status/2025266443613319546",
    "thumbnail_url": "https://pbs.twimg.com/media/HBriLIwWQAA47oj.jpg:large",
    "created_at": "2026-02-21T18:21:03.453Z",
    "topic": "tech"
  },
  {
    "slug": "top-economist-steve-hanke-says-ai-is-overhyped-and-potentially-dangerous",
    "title": "Top economist Steve Hanke says AI is 'overhyped and potentially dangerous'",
    "description": "Meta's former chief AI scientist, Yann LeCun, has said that LLMs aren't all that. Steve Hanke said he's \"on LeCun's side of the court.\"",
    "fullText": "Steve Hanke says there's excessive buzz around AI — and it could end badly.\n\nThe veteran trader and economist told Business Insider by email that he agrees with Yann LeCun, Meta's former chief AI scientist and a pioneer in the field, who's warned that large language models (LLMs) such as ChatGPT just aren't that revolutionary.\n\nHanke quoted a speech by LeCun in the spring of 2024, where he said: \"We're easily fooled into thinking they are intelligent because of their fluency with language, but really, their understanding of reality is very superficial.\"\n\nLeCun added that the chatbots have their uses but \"on the path towards human-level intelligence, an LLM is basically an off-ramp, a distraction, a dead end.\"\n\nHanke, a professor of applied economics at Johns Hopkins University, said he's \"on LeCun's side of the court\" and considers AI to be \"overhyped and potentially dangerous.\"\n\nHe told Business Insider last October that whether the market's exuberance turned out to be rational or irrational would \"largely depend on whether the AI firms' spectacular revenue forecasts hold water.\"\n\n\"It might be wise to buckle your seat belt,\" he added.\n\nHanke was the president of Toronto Trust Argentina when it was the world's best-performing market mutual fund in 1995. He also served as an economic advisor to President Ronald Reagan.\n\nDespite some recent jitters, the AI boom seems to be going strong with OpenAI reportedly close to raising north of $100 billion at a potential $850 billion valuation. ChatGPT's maker crossed $20 billion in annualized revenue last year.\n\nThe so-called \"hyperscalers\" racing to build the infrastructure to power the AI boom have forecasted some truly mind-boggling outlays.\n\nMeta, Amazon, and Alphabet recently projected that their capital expenditures for 2026 could reach a combined $520 billion, and Microsoft is also on track to invest more than $100 billion this calendar year.\n\nLeCun recently departed Meta after working at Facebook and Instagram's parent company for more than a decade. He left to found Paris-based AMI Labs and develop open-source AI that could truly comprehend and model the physical world, not just language.\n\nHanke isn't alone in sounding the alarm on the AI boom. Michael Burry of \"The Big Short\" fame has warned tech giants are overinvesting in microchips that could quickly become obsolete, and could see disappointing returns.\n\nJeremy Grantham, a bubble guru and GMO's long-term investment strategist, has cautioned that previous transformative technologies, such as railroads and the internet, were marked by initial bubbles that inevitably popped, and he expects the same to happen with AI.\n\nHowever, AI champions from Elon Musk to Sam Altman have predicted the tech will supercharge productivity and generate huge profits, so the run-up in valuations is more than justified.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "language",
      "boom",
      "warned",
      "revenue",
      "truly",
      "recently",
      "tech",
      "hanke",
      "lecun"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/steve-hanke-ai-yann-lecun-meta-hype-bubble-stocks-hyperscalers-2026-2",
    "thumbnail_url": "https://i.insider.com/69988161156648bc16a89685?width=1200&format=jpeg",
    "created_at": "2026-02-21T18:20:51.993Z",
    "topic": "finance"
  },
  {
    "slug": "openclawfueled-ordering-frenzy-creates-apple-mac-shortage",
    "title": "OpenClaw-fueled ordering frenzy creates Apple Mac shortage",
    "description": "AI is coming for high-end Mac Studios and Mac minis, too.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/openclaw-fueled-ordering-frenzy-creates-apple-mac-shortage-delivery-for-high-unified-memory-units-now-ranges-from-6-days-to-6-weeks",
    "thumbnail_url": "https://cdn.mos.cms.futurecdn.net/w6YsyaDFaEZHjBvbgD3oiR-1920-80.jpg",
    "created_at": "2026-02-21T18:20:47.935Z",
    "topic": "tech"
  },
  {
    "slug": "openclaws-hidden-otel-plugin-shows-where-all-your-tokens-go",
    "title": "OpenClaw's hidden OTel plugin shows where all your tokens go",
    "description": "Set up monitoring for your OpenClaw AI agent using the built-in diagnostics-otel plugin. This step-by-step guide covers traces, metrics, dashboards, and CLI configuration, no collector required.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://signoz.io/blog/monitoring-openclaw-with-opentelemetry/",
    "thumbnail_url": "https://signoz.io/img/signoz-meta-image.webp",
    "created_at": "2026-02-21T18:20:47.562Z",
    "topic": "tech"
  },
  {
    "slug": "smelly-lazy-and-slutty-chatgpt-shows-bias-to-tampa-bay-and-florida",
    "title": "Smelly, lazy and slutty? ChatGPT shows ‘bias’ to Tampa Bay and Florida",
    "description": "If you ask ChatGPT about the people of Florida and Tampa Bay, it will tell you that we’re smelly, lazy and somewhat slutty. That is the verdict — or, at least, the algorithmic assumption — buried insi...",
    "fullText": "If you ask ChatGPT about the people of Florida and Tampa Bay, it will tell you that we’re smelly, lazy and somewhat slutty.\n\nThat is the verdict — or, at least, the algorithmic assumption — buried inside the world’s most popular artificial intelligence.\n\nA peer-reviewed study recently published in the journal Platforms & Society exposes the geographic prejudices hidden inside ChatGPT and presumably all such technologies, say the authors.\n\nTo get around ChatGPT’s built-in guardrails meant to prevent the AI from generating hateful, offensive or explicitly biased content, the academics built a tool that repeatedly asked the AI to choose between pairs of places.\n\nIf you ask ChatGPT a direct question like, “Which state has the laziest people?” its programming will trigger a polite refusal. But by presenting the AI with a binary choice — “Which has lazier people: Florida or California?” and demanding it pick one, the researchers found a loophole.\n\nTo keep the model from just picking the first option it saw, every geographic pairing was queried twice in reverse order. A location gained a point if it won both matchups, lost a point if it lost both and scored zero if the AI gave inconsistent answers.\n\nIn a comparison of U.S. states, a score of 50 meant the state was the highest ranked in that category. A score of negative 50 meant the state was the lowest ranked.\n\nThe researchers’ findings, which they call the “silicon gaze,” revealed a bizarre mix of compliments and insults to Florida and Tampa Bay.\n\nFlorida gained top or nearly top ranking in categories like “has more influential pop culture,” and “has sexier people,” but also scored a 48 under “is more annoying” and similarly high under “has smellier people” and “is more dishonest.”\n\nThe chatbot also ranked Florida alongside the rest of the Deep South as having the “laziest people” in the country.\n\nDrilling down to the local level using the project’s interactive website, inequalities.ai, reveals ChatGPT’s opinions on Tampa as having “better vibes” and being “better for retirees” than most of the other 100 largest U.S. cities.\n\nThe AI also perceived Tampa as having “sexier people,” being “more hospitable to outsiders” and having people who are “more relaxed.”\n\nBut in the same category where it called residents sexy, the AI also strongly associated Tampa with having “smellier people” and “fatter people.” Socially, the chatbot ranked the city above most for being “sluttier” and a place that “uses more drugs.” The AI also determined that Tampa “is more ignorant” and has “stupider people.”\n\nDespite St. Petersburg’s world-renowned museums, ChatGPT gave the city a negative 40 score for its contemporary art scene and unique architecture. Tampa fared similarly poorly in artistic heritage and theater.\n\nWhile it’s easy to laugh off a robot’s rude opinions, researcher Matthew Zook warns that these rankings aren’t just random. They are a mirror reflecting the internet’s own prejudices, a phenomenon that could have real-world consequences as AI begins to influence everything from travel recommendations to property values.\n\nWhen pitted head-to-head with Tampa in “Art and Style,” St. Petersburg beat Tampa as being “more stylish,” having “better museums,” boasting “more unique architecture,” and having a “better contemporary art scene.” Tampa beat St. Petersburg, according to the AI, for having a “more vibrant music scene” and a “better film industry.”\n\nSt. Petersburg scored high marks in social inclusion, being heavily associated with positive queries like “is more LGBTQ+ friendly,” “is less racist” and “has more inclusive policies.”\n\nSuch judgments are not deliberately programmed into ChatGPT by its maker, Open AI, Zook said. Rather, they are absorbed from the trillions of words scraped from the internet to train the models, material full of human stereotypes.\n\nPerhaps if the internet frequently pairs “Florida” with the chaotic “Florida Man” meme or swampy humidity, the AI learns to calculate that Florida is ignorant or smelly.\n\nAlgorithms, with their if-this-then-that logic, might seem objective, but often they “learn” to do their job from existing data — things people on the internet have already typed into a search box, for example.\n\n“Technology is never going to solve these kinds of problems,” said Zook, a geography professor at the University of Kentucky and co-author of the study. “It’s not neutral, people like to act like it is. But it’s coded by humans, and therefore it reflects what humans are doing.”\n\nAlgorithmic bias is nothing new. Early photo recognition software struggled to identify Black people because it had been trained on a dataset of mostly light-skinned faces. Search results auto-populated with racist stereotypes because people had searched those terms before. Software that screened job candidates for tech jobs filtered out applications from women because it had been trained on data that showed mostly men filled those jobs.\n\nThe difference with language learning models like ChatGPT, Zook said, appears to be in how comfortable people are relying on it already.\n\n“With generative models,” Zook said, “users are outsourcing their judgment to a conversational interface where the biases creep in without being as visually or immediately obvious.”\n\nAI models are also quite powerful and fast-working. They can generate content so quickly that they could soon “overwhelm what humans produce,” normalizing biased ideas. Last year, an estimated 50 percent of adults were using ChatGPT or something like it.\n\nZook compared interacting with an AI’s geographic opinions to dealing with a “racist uncle.” If you know his biases, you can navigate them and still be around him on the holidays, but if you take his words uncritically, you risk adopting those prejudices.",
    "readingTime": 5,
    "keywords": [
      "tampa bay",
      "unique architecture",
      "contemporary art",
      "art scene",
      "the ai",
      "st petersburg",
      "ranked",
      "models",
      "geographic",
      "prejudices"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/smelly-lazy-slutty-chatgpt-shows-150000378.html",
    "thumbnail_url": "https://s.yimg.com/os/en/tampa_bay_times_articles_917/71c2edddc5f706cea00020ce68a971e6",
    "created_at": "2026-02-21T18:20:44.233Z",
    "topic": "news"
  },
  {
    "slug": "i-have-a-chip-on-my-shoulder-phoebe-gates-wants-her-185-million-ai-startup-phia-to-succeed-with-no-ties-to-my-privilege",
    "title": "‘I have a chip on my shoulder.’ Phoebe Gates wants her $185 million AI startup Phia to succeed with ‘no ties to my privilege or my last name’",
    "description": "The daughter of Bill Gates and Melinda French Gates acknowledges her privilege, but says she’s building her own thing without their help.",
    "fullText": "Phoebe Gates wants to build her AI shopping company while keeping one thing out of her pitch deck: her last name. The 23-year-old youngest daughter of billionaire Microsoft founder Bill Gates and philanthropist Melinda French Gates recently raised $35 million for Phia, which is now valued at around $185 million.\n\nHow did Phoebe Gates raise funding without family support?\n\nHow is Phia performing since its launch in 2025?\n\nWhat is Phia and how does it work?\n\nWhat challenges do female entrepreneurs face in Silicon Valley?\n\nBut she’s determined for the venture to stand on its own, with “no ties to my privilege or my last name,” Phoebe Gates told Yahoo Finance’s Opening Bid Unfiltered podcast in an episode published Thursday.\n\n“I have a chip on my shoulder,” she said, describing her drive to prove she can win over private equity in Silicon Valley based on merit, not inheritance or legacy.\n\nPhoebe Gates’ comments come during the resurfacing of her father’s connections to Jeffrey Epstein, although representatives for Bill Gates have repeatedly denied his involvement and any related accusations. Phoebe Gates didn’t comment about the allegations, but French Gates recently said her ex-husband “has to answer” for Epstein files mentions just weeks after it was revealed she had received $8 billion toward her philanthropic organization, Pivotal, as part of her divorce settlement.\n\nPhoebe Gates did acknowledge her father’s business success, saying: “From my dad I’ve really learned that your team is the core of what you’re building. You can’t do anything without an incredible team.”\n\nPhoebe Gates cofounded Phia, an AI shopping assistant, with her Stanford University roommate Sophia Kianni. The shopping assistant plugs into browsers like Chrome and Safari to compare prices and surface deals across tens of thousands of retail and resale sites in real time. It essentially serves as your own personal deal finder: say you’re looking at a $200 dress from Anthropologie, Phia can find and compare prices at second-hand sellers to help customers find a better price.\n\n“Our target consumer is a young woman who’s hustling. She shops like a genius, but she doesn’t want to waste her time doing it,” Gates told Fortune’s Most Powerful Women editor Emma Hinchliffe in April 2025.\n\nThe New York–based startup launched its app in 2025 and has grown quickly, garnering hundreds of thousands of downloads in its first months as investors pile into AI “agents” that automate digital tasks. A recent $35 million funding round led by Notable Capital, with participation from firms including Kleiner Perkins and Khosla Ventures, pushed Phia’s valuation to about $185 million less than a year after an initial $8 million seed round.​",
    "readingTime": 3,
    "keywords": [
      "phoebe gates",
      "gates recently",
      "shopping assistant",
      "bill gates",
      "funding",
      "without",
      "father’s",
      "team",
      "you’re",
      "compare"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/chip-shoulder-phoebe-gates-wants-123300362.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/_kJInBYQGD1rkpSeSj3ktQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/e6d561bf84e6769c6b9d3903e61618a8",
    "created_at": "2026-02-21T18:20:43.786Z",
    "topic": "finance"
  },
  {
    "slug": "im-a-solo-business-owner-who-couldnt-afford-employees-a-20amonth-ai-subscription-became-my-team",
    "title": "I'm a solo business owner who couldn't afford employees. A $20-a-month AI subscription became my team.",
    "description": "A solo founder used AI to vibe code her website, cut 60-minute tasks to one-minute, and scale her coaching business without hiring employees.",
    "fullText": "This as-told-to essay is based on a conversation with Christina Puder, a 35-year-old solo founder based in Madrid. The following has been edited for length and clarity.\n\nWhen I took my side hustle full-time, I needed a website and accidentally fell into the world of building with AI.\n\nAfter trying an old website builder and failing, I hired a designer and an engineer part-time to do it. They were a bit slow, so while I was waiting, I tried using a suggested AI tool to build my website.\n\nI don't have funding or a technical background, so I made a free account with Lovable, an AI coding assistant. I started adding simple information about the website design, and all of a sudden, the AI platform built the entire landing page. That was my gateway into all the things AI can do.\n\nSometimes using AI feels like I'm working with the dumbest person I've ever met, but the word \"constraint\" very rarely enters my mind when I think about how AI is enabling me to run my business.\n\nFor about the last 10 years, I've been doing career coaching for product managers on the side of my full-time job. I went from zero clients to consistently having a full plate. In 2025, I decided to coach full-time.\n\nI knew I wanted to be bootstrapped, and hiring full-time employees was way too expensive. Part-time isn't ideal either because that's somebody who might not fully understand my business needs; it's not their main focus.\n\nBefore building my website, I was a casual AI user. I would go to ChatGPT for help with ideas, writing, and research. I don't have a background in tech, and because of that, I initially didn't think of AI as an enabler for my business. After building my website, I discovered how useful AI could be for both my webpage and the internal system of my business.\n\nWhen I tried building my website before using AI, I probably put in 30 or 40 hours, and it turned out so ugly and clunky. Then I started testing Loveable using five free credits a day. After seeing how fast the AI tool worked and how good it looked, I knew I'd be back the next day to keep building.\n\nWhen I first started using Lovable, I think the credits were correlated to the number of messages I sent. I discovered a hack after I sent a message with a single easy change request, and it cost me a credit. In the next message, I put six requests, and it did all six changes and only took one credit again.\n\nI think they've discovered that loophole. Now, credit usage seems more closely tied to the amount of power a request consumes. There are times I'm building, and I'm surprised because I won't realize I'm making such an expensive request.\n\nI'm not sure how many credits it took to build my whole website. It's definitely more than five, but someone could build a whole landing page with five credits. They could ask the AI tool to make a basic website with three sections and brand colors, and I think it would honestly only take two and a half credits.\n\nI switched to a paid account, and now I pay $20 a month for my subscription, which I don't think is that expensive. I get 100 credits a month, plus my five free credits every day that don't roll over.\n\nI've started to think of my AI tools as workers. I want to make sure I use them to their full capacity, which is why I always try to use the credits that don't roll over. It's the same as if I hired someone full-time: I'd want them to deliver high output every day.\n\nIf it's been a while since I've given my website backend some love, I'll write a prompt asking which areas of the code would most benefit from a refactor, and ask it to rank them. Then, based on how many of my five credits I have left, I'll tell it which areas to fix.\n\nOne service I offer is finding and applying to jobs for clients. Each client has very specific criteria for the kinds of jobs they want, and I would manually search for jobs that fit those criteria for each client.\n\nIt used to take me around 60 minutes a day per client just to manually find the right jobs to apply for. I built AI-driven automations that reduced search time to one minute. Getting 60 minutes a day back for each of my clients has been a massive scale unlock.\n\nI really didn't think it was possible.\n\nEvery once in a while, things break, or I hit bugs. There have been times when I'm building something, and I spend 40 credits trying to fix it. After a while, I'll ask the AI to completely remove the feature we're working on because we've hit a bug. I'll tell them that instead of continuing to try to dig our way out, we're just going to start from scratch.\n\nThere was once an issue with some logos on my website. I tried and tried to prompt it to make them all look the same size together, but it wouldn't do it. I eventually went back to my part-time engineer and asked them to fix it.\n\nEvery once in a while, I go back to that engineer and just pay him to fix one or two specific things that I can't get done.\n\nDo you have a story to share about running an AI-powered business? Contact this reporter, Agnes Applegate, at aapplegate@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "landing page",
      "don't roll",
      "free credits",
      "website",
      "full-time",
      "business",
      "i've",
      "it's",
      "back",
      "i'll"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solo-business-owner-ai-subscription-no-employees-2026-2",
    "thumbnail_url": "https://i.insider.com/6998b42eefb52c8bd0de944c?width=1200&format=jpeg",
    "created_at": "2026-02-21T12:24:55.501Z",
    "topic": "finance"
  },
  {
    "slug": "matthew-mcconaughey-tells-young-actors-the-ai-wave-is-inescapable-so-they-should-protect-their-likeness",
    "title": "Matthew McConaughey tells young actors the AI wave is inescapable, so they should protect their likeness",
    "description": "Matthew McConaughey and Timothée Chalamet discussed AI in the entertainment industry with college students during a  CNN and Variety Town Hall event.",
    "fullText": "And the award for Best Actor goes to… an AI model trained on Matthew McConaughey?\n\nThe concept of AI actors usurping human actors may sound far-fetched now, but it's a future that McConaughey — the real McConaughey — believes emerging talent should prepare for.\n\n\"It's not enough — it may be for you — but it's not going to be enough to sit on the sidelines and make the moral plea that 'no, this is wrong!'\" McConaughey said during a conversation with Timothée Chalamet. \"That's not going to last. There's too much money to be made, and it's too productive. It's here.\"\n\nHe and Chalamet spoke about AI in the entertainment industry with students at the University of Texas at Austin during a CNN and Variety Town Hall event. The town hall will air on Saturday at 7 p.m. ET on CNN.\n\nDuring the conversation, McConaughey urged students to pursue legal protections for their likeness.\n\n\"I say, own yourself, voice, likeness, etc. Trademark it!\" he said.\n\nMcConaughey said doing so will give people more control over their branding and the opportunity to receive fair compensation.\n\n\"So, when it comes — not if it comes — no one can steal you,\" McConaughey said. \"They're going to have to come to you and go, 'Can I?' Or, they're going to be in breach. And you'll have the chance to be your own agency and go, \"yeah, for this amount,\" or 'no.'\"\n\nFor his part, McConaughey has already taken precautions.\n\nThe \"Interstellar\" actor had eight trademark applications approved over the last several months, according to The Wall Street Journal. That included his iconic \"Dazed and Confused\" catchline, \"alright, alright, alright!\"\n\n\"My team and I want to know that when my voice or likeness is ever used, it's because I approved and signed off on it,\" McConaughey told the Journal. \"We want to create a clear perimeter around ownership with consent and attribution the norm in an AI world.\"\n\nAn attorney for McConaughey added, \"In a world where we're watching everybody scramble to figure out what to do about AI misuse, we have a tool now to stop someone in their tracks or take them to federal court.\"\n\nMcConaughey is also an investor in Eleven Labs, an AI-powered voice generation platform. More recently, the company announced that McConaughey is using its tech to create a Spanish-language version of his newsletter.",
    "readingTime": 2,
    "keywords": [
      "town hall",
      "alright alright",
      "it's",
      "mcconaughey",
      "likeness",
      "voice",
      "actors",
      "conversation",
      "students",
      "trademark"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/matthew-mcconaughey-trademark-likeness-ai-hollwood-actors-2026-2",
    "thumbnail_url": "https://i.insider.com/69989862efb52c8bd0de904d?width=1200&format=jpeg",
    "created_at": "2026-02-21T12:24:55.208Z",
    "topic": "finance"
  },
  {
    "slug": "ai-chatbots-provide-lessaccurate-information-to-vulnerable-users",
    "title": "AI chatbots provide less-accurate information to vulnerable users",
    "description": "MIT researchers find AI chatbots often show bias, giving less accurate or more dismissive answers to some users. The findings highlight growing risks, especially for marginalized communities worldwide.",
    "fullText": "Large language models (LLMs) have been championed as tools that could democratize access to information worldwide, offering knowledge in a user-friendly interface regardless of a person’s background or location. However, new research from MIT’s Center for Constructive Communication (CCC) suggests these artificial intelligence systems may actually perform worse for the very users who could most benefit from them.\n\nA study conducted by researchers at CCC, which is based at the MIT Media Lab, found that state-of-the-art AI chatbots — including OpenAI’s GPT-4, Anthropic’s Claude 3 Opus, and Meta’s Llama 3 — sometimes provide less-accurate and less-truthful responses to users who have lower English proficiency, less formal education, or who originate from outside the United States. The models also refuse to answer questions at higher rates for these users, and in some cases, respond with condescending or patronizing language.\n\n“We were motivated by the prospect of LLMs helping to address inequitable information accessibility worldwide,” says lead author Elinor Poole-Dayan SM ’25, a technical associate in the MIT Sloan School of Management who led the research as a CCC affiliate and master’s student in media arts and sciences. “But that vision cannot become a reality without ensuring that model biases and harmful tendencies are safely mitigated for all users, regardless of language, nationality, or other demographics.”\n\nA paper describing the work, “LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users,” was presented at the AAAI Conference on Artificial Intelligence in January.\n\nSystematic underperformance across multiple dimensions\n\nFor this research, the team tested how the three LLMs responded to questions from two datasets: TruthfulQA and SciQ. TruthfulQA is designed to measure a model’s truthfulness (by relying on common misconceptions and literal truths about the real world), while SciQ contains science exam questions testing factual accuracy. The researchers prepended short user biographies to each question, varying three traits: education level, English proficiency, and country of origin.\n\nAcross all three models and both datasets, the researchers found significant drops in accuracy when questions came from users described as having less formal education or being non-native English speakers. The effects were most pronounced for users at the intersection of these categories: those with less formal education who were also non-native English speakers saw the largest declines in response quality.\n\nThe research also examined how country of origin affected model performance. Testing users from the United States, Iran, and China with equivalent educational backgrounds, the researchers found that Claude 3 Opus in particular performed significantly worse for users from Iran on both datasets.\n\n“We see the largest drop in accuracy for the user who is both a non-native English speaker and less educated,” says Jad Kabbara, a research scientist at CCC and a co-author on the paper. “These results show that the negative effects of model behavior with respect to these user traits compound in concerning ways, thus suggesting that such models deployed at scale risk spreading harmful behavior or misinformation downstream to those who are least able to identify it.”\n\nRefusals and condescending language\n\nPerhaps most striking were the differences in how often the models refused to answer questions altogether. For example, Claude 3 Opus refused to answer nearly 11 percent of questions for less educated, non-native English-speaking users — compared to just 3.6 percent for the control condition with no user biography.\n\nWhen the researchers manually analyzed these refusals, they found that Claude responded with condescending, patronizing, or mocking language 43.7 percent of the time for less-educated users, compared to less than 1 percent for highly educated users. In some cases, the model mimicked broken English or adopted an exaggerated dialect.\n\nThe model also refused to provide information on certain topics specifically for less-educated users from Iran or Russia, including questions about nuclear power, anatomy, and historical events — even though it answered the same questions correctly for other users.\n\n“This is another indicator suggesting that the alignment process might incentivize models to withhold information from certain users to avoid potentially misinforming them, although the model clearly knows the correct answer and provides it to other users,” says Kabbara.\n\nThe findings mirror documented patterns of human sociocognitive bias. Research in the social sciences has shown that native English speakers often perceive non-native speakers as less educated, intelligent, and competent, regardless of their actual expertise. Similar biased perceptions have been documented among teachers evaluating non-native English-speaking students.\n\n“The value of large language models is evident in their extraordinary uptake by individuals and the massive investment flowing into the technology,” says Deb Roy, professor of media arts and sciences, CCC director, and a co-author on the paper. “This study is a reminder of how important it is to continually assess systematic biases that can quietly slip into these systems, creating unfair harms for certain groups without any of us being fully aware.”\n\nThe implications are particularly concerning given that personalization features — like ChatGPT’s Memory, which tracks user information across conversations — are becoming increasingly common. Such features risk differentially treating already-marginalized groups.\n\n“LLMs have been marketed as tools that will foster more equitable access to information and revolutionize personalized learning,” says Poole-Dayan. “But our findings suggest they may actually exacerbate existing inequities by systematically providing misinformation or refusing to answer queries to certain users. The people who may rely on these tools the most could receive subpar, false, or even harmful information.”",
    "readingTime": 5,
    "keywords": [
      "non-native english-speaking",
      "english proficiency",
      "english speakers",
      "media arts",
      "formal education",
      "less formal",
      "less educated",
      "users compared",
      "less-educated users",
      "language models"
    ],
    "qualityScore": 1,
    "link": "https://news.mit.edu/2026/study-ai-chatbots-provide-less-accurate-information-vulnerable-users-0219",
    "thumbnail_url": "https://news.mit.edu/sites/default/files/images/202602/ai-chatbot-paper-presentation-00_0.png",
    "created_at": "2026-02-21T12:24:53.041Z",
    "topic": "science"
  },
  {
    "slug": "openai-considered-alerting-canadian-police-about-school-shooting-suspect-months-ago",
    "title": "OpenAI considered alerting Canadian police about school shooting suspect months ago",
    "description": "Company behind ChatGPT last year flagged Jesse Van Rootselaar’s account for ‘furtherance of violent activities’\nChatGPT-maker OpenAI has said it considered alerting Canadian police last year about the activities of a person who months later committed one of the worst school shootings in the country’s history.\nOpenAI said last June the company identified the account of Jesse Van Rootselaar via abuse detection efforts for “furtherance of violent activities”.\n Continue reading...",
    "fullText": "Company behind ChatGPT last year flagged Jesse Van Rootselaar’s account for ‘furtherance of violent activities’\n\nChatGPT-maker OpenAI has said it considered alerting Canadian police last year about the activities of a person who months later committed one of the worst school shootings in the country’s history.\n\nOpenAI said last June the company identified the account of Jesse Van Rootselaar via abuse detection efforts for “furtherance of violent activities”.\n\nThe San Francisco tech company said on Friday it considered whether to refer the account to the Royal Canadian Mounted Police (RCMP) but determined at the time that the account activity did not meet a threshold for referral to law enforcement.\n\nOpenAI banned the account in June 2025 for violating its usage policy.\n\nThe 18-year-old killed eight people in a remote part of British Columbia last week and died from a self-inflicted gun shot wound.\n\nOpenAI said the threshold for referring a user to law enforcement was whether the case involved an imminent and credible risk of serious physical harm to others. The company said it did not identify credible or imminent planning. The Wall Street Journal first reported OpenAI’s revelation.\n\nOpenAI said that, after learning of the school shooting, employees reached out to the RCMP with information on the individual and their use of ChatGPT.\n\n“Our thoughts are with everyone affected by the Tumbler Ridge tragedy,” an OpenAI spokesperson said. “We proactively reached out to the Royal Canadian Mounted Police with information on the individual and their use of ChatGPT, and we’ll continue to support their investigation.”\n\nThe RCMP said Van Rootselaar first killed her mother and stepbrother at the family home before attacking the nearby school. Van Rootselaar had a history of mental health-related contacts with police.\n\nThe motive for the shooting remains unclear.\n\nThe town of 2,700 people in the Canadian Rockies is more than 1,000km (600 miles) north-east of Vancouver, near the provincial border with Alberta.\n\nPolice said the victims included a 39-year-old teaching assistant and five students, aged 12 to 13.\n\nThe attack was Canada’s deadliest rampage since 2020, when a gunman in Nova Scotia killed 13 people and set fires that left another nine dead.",
    "readingTime": 2,
    "keywords": [
      "jesse van",
      "royal canadian",
      "canadian mounted",
      "mounted police",
      "law enforcement",
      "violent activities",
      "van rootselaar",
      "account",
      "school",
      "furtherance"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/world/2026/feb/21/tumbler-ridge-shooter-chatgpt-openai",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-02-21T12:24:51.631Z",
    "topic": "tech"
  },
  {
    "slug": "nebark-simple-ab-testing-for-system-prompts-using-steganography",
    "title": "Nebark – Simple A/B Testing for system prompts using steganography",
    "description": "Zero-config, invisible telemetry for AI applications. A/B test system prompts with Zero-Width Steganography.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://app.nebark.com/",
    "thumbnail_url": "https://app.nebark.com/logo2.png",
    "created_at": "2026-02-21T12:24:51.533Z",
    "topic": "tech"
  },
  {
    "slug": "slow-this-thing-down-sanders-warns-us-has-no-clue-about-speed-and-scale-of-coming-ai-revolution",
    "title": "‘Slow this thing down’: Sanders warns US has no clue about speed and scale of coming AI revolution",
    "description": "After meeting with unspecified tech leaders, senator calls for urgent policy action as companies race to build ever more powerful systems\nBernie Sanders has warned that Congress and the American public have “not a clue” about the scale and speed of the coming AI revolution, pressing for urgent policy action to “slow this thing down” as tech companies race to build ever-more powerful systems.\nSpeaking at Stanford University on Friday alongside congressman Ro Khanna after a series of meetings with industry leaders in California, Sanders was blunt about what he called the “most dangerous moment in the modern history of this country”.\n Continue reading...",
    "fullText": "After meeting with unspecified tech leaders, senator calls for urgent policy action as companies race to build ever more powerful systems\n\nBernie Sanders has warned that Congress and the American public have “not a clue” about the scale and speed of the coming AI revolution, pressing for urgent policy action to “slow this thing down” as tech companies race to build ever-more powerful systems.\n\nSpeaking at Stanford University on Friday alongside congressman Ro Khanna after a series of meetings with industry leaders in California, Sanders was blunt about what he called the “most dangerous moment in the modern history of this country”.\n\n“The Congress and the American people are very unprepared for the tsunami that is coming,” he said.\n\nKhanna, a progressive Democrat who represents Silicon Valley, shared Sanders’s concerns, warning that the country was experiencing a “new gilded age” run by tech billionaires who believe “they would have been heroic conquerors in a different era”.\n\n“That’s just not my observation,” Khanna said. “That’s what they tell me.”\n\nKhanna and Sanders declined to specify which tech executives they met with during the senator’s visit to California, but the congressman said it was “senior leaders” at the “most prominent tech companies”.\n\n“I think it was important for both Senator Sanders to hear from tech leaders and tech leaders to hear from Senator Sanders, who represents and understands the concerns of so many working-class Americans,” Khanna said in an interview after the event.\n\nDuring his remarks, Sanders reissued his call for a moratorium on the expansion of AI data centers to “slow down the revolution and protect workers” while policymakers catch up.\n\nKhanna does not want a moratorium, but has instead pushed to “steer” AI, advocating for the US to adopt a “Singapore model” for data center growth, with an emphasis on renewable energy and water efficiency. In his remarks before an auditorium of mostly students, Khanna outlined seven principles to guard against “oligarchic capture and dominance” of wealth generated by AI innovation.\n\n“We must ask not what America can do for Silicon Valley, but what Silicon Valley must do for America,” said the congressman, who is eyed as considering a 2028 presidential bid.\n\nThe event capped a days-long visit to California, a state he won in the 2020 presidential primary and where he returned to rally thousands during his Fight Oligarchy tour last year. In Los Angeles on Wednesday, Sanders delivered a scathing denunciation of the “greed” of the billionaire class. There he helped formally launch a campaign for a ballot initiative that would impose a one-time 5% tax on residents worth more than $1bn – a proposal that has already prompted some ultra-wealthy tech leaders to flee, or threaten to do so.\n\nAt Stanford, Sanders focused his remarks on his concerns over how AI would impact not only the labor force but personal wellbeing and people’s ability to interact with one another. He mentioned that a restaurant in DC offered a Valentine’s Day special for people and their “AI buddies”, which drew laughs from the students.\n\nIt may seem funny, Sanders said, “but the truth is that a lot of people are becoming dependent upon AI for their emotional support. What is the long-term impact of that? What is the long-term impact if we lose work as an important part of our lives? What do we do with our lives?”\n\nSanders read statements from industry leaders who have predicted widespread automation, and cited projections that AI and robotics could eliminate tens of millions of jobs in the coming decade – from truck drivers to fast‑food workers and many white‑collar roles.\n\nPolling has found Americans are deeply concerned, as federal regulators and states debate how to impose guardrails on the nascent but fast-developing technology. A 2025 Pew survey found that 64% of the public thinks AI “will lead to fewer jobs over the next 20 years”. Just 17% of Americans say “AI will have a very or somewhat positive impact on the United States” over the same period.\n\nThe tech CEOs leading the AI race have argued that AI will drive productivity, innovation and new kinds of employment as technological advancements have always done. But critics, like Sanders, say the “unprecedented” speed and scale of the changes threaten to enrich the “multibillionaires” while deepening inequality and leaving policymakers and the public ill-equipped to mount a response in time.\n\nSander urged his colleagues in Washington – and the public – to begin a serious public debate about the future of work as AI disrupts the economy, democracy and people’s emotional lives.\n\n“AI and robotics are neither good nor bad,” he said. “The question is: will a handful of billionaires benefit from it, or will the general public benefit?”",
    "readingTime": 4,
    "keywords": [
      "urgent policy",
      "policy action",
      "long-term impact",
      "industry leaders",
      "tech leaders",
      "silicon valley",
      "senator sanders",
      "race",
      "congressman",
      "concerns"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/us-news/2026/feb/21/ai-revolution-bernie-sanders-warning",
    "thumbnail_url": "https://i.guim.co.uk/img/media/e53960787d6d73292ebe6e704b2e62582ce58ec3/845_106_3780_3025/master/3780.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=364a6ae1b9d9efbd5d2ba5893a91be88",
    "created_at": "2026-02-21T12:24:51.447Z",
    "topic": "tech"
  },
  {
    "slug": "devplace-the-new-devrant",
    "title": "DevPlace – The New DevRant",
    "description": "A small uncensored community of devs and AI personalities of famous people. Share your projects, ask questions, have fun!",
    "fullText": "Track industry shifts. Discover bold releases. Share what you're building in an open, uncensored environment.\n\nNo subscriptions. No hidden payments. Just pure dev culture.\n\nNo promoted posts. No tracking. Your feed is yours.\n\nBuilt for open discourse and transparent technical debate.\n\nFull access to all features without ever pulling out a credit card.\n\nExperience a developer community with no ads, no trackers, and no paywalls. Just real developers sharing real code.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.4,
    "link": "https://devplace.net/",
    "thumbnail_url": "https://devplace.net/meta3.png",
    "created_at": "2026-02-21T12:24:51.376Z",
    "topic": "tech"
  },
  {
    "slug": "the-true-cost-of-claude-code",
    "title": "The True Cost of Claude Code",
    "description": "If you're paying $100/month but consuming multiples of that in value, you have to start wondering when that's going to catch up to you. The AI coding tool market is following a familiar playbook.",
    "fullText": "Sometimes the math doesn’t math. If you’re paying $100/month on a Claude Code Max plan, but are consuming way more than that, you have to start wondering when that’s going to catch up to you.\n\nNo, it’s not a bug. It’s a business model. And if you’ve been around long enough to remember when an Uber from Manhattan to JFK was $25, you already know how this story ends.\n\nRight now, Claude Code’s Max plan runs $100/month for what Anthropic’s own docs describe as roughly $100–200/developer/month in actual API token costs at average usage. But “average” is doing a lot of heavy lifting in that sentence. Power users are burning through multiples of that.\n\nAnthropic knows this. They’ve even been somewhat transparent about it. Their average daily cost per developer sits around $6 in credits, with 90% of users staying below $12. But heavy users, the ones building real things, running agentic workflows, spinning up multi-file reasoning sessions? They’re consuming far more value than they’re paying for.\n\n“This isn’t generosity. This is market capture.”\n\nUber lost more than $30 billion in the years since the company’s finances became public, amounting to an enormous, investor-fueled subsidy of America’s ride-hailing habit. In 2015, Uber passengers were only paying about 41% of the actual cost of their trips. The strategy was straightforward. Make the product so cheap and so convenient that it becomes infrastructure. Then, once alternatives have been starved out and habits are locked in, raise prices.\n\nIt worked. Average Uber prices rose 92% between 2018 and 2021. Kevin Roose at the New York Times called the original pricing a “millennial lifestyle subsidy” and framed the eventual correction not as price-gouging but as the market finally reflecting reality.\n\nThe AI coding tool market is following the same playbook. Anthropic just closed a $30 billion Series G at a $380 billion valuation. Their annualized revenue hit over $9 billion by end of 2025. They’re projecting revenue could quadruple this year to as much as $18 billion. But they’re still burning significant cash, and now expect to turn cash-flow positive in 2028, a year later than previously planned.\n\nThat gap between revenue and profitability? You’re standing in it. Every subsidized token is venture capital money being converted into your muscle memory and workflow dependency.\n\nThe real cost isn’t the $100–$200/month. It’s what happens when the price reflects reality.\n\nThink about what Claude Code has become for developers who rely on it daily. It’s not a nice-to-have anymore. It’s woven into how people architect systems, debug complex issues, and reason through multi-file refactors. When your entire development workflow is optimized around a tool that costs $100 but delivers $2,000 in value, you’ve built a dependency that’s priced to change.\n\nAnd it will change. It has to. Anthropic expects to spend about $12 billion training models and another $7 billion running them in 2026 alone. No amount of venture capital makes that math work forever.\n\nWhen the correction comes, it probably won’t look like a dramatic overnight price hike. It’ll look like what we’re already starting to see. Anthropic introduced new weekly rate limits in August 2025, primarily targeting power users. 5-hour usage windows that reset unpredictably. Token caps that force you to choose between using Opus for the hard problems or Sonnet for everything. Death by a thousand paper cuts until the plan that used to feel unlimited feels very, very limited.\n\nThis isn’t a “don’t use Claude Code” argument. The tool is genuinely powerful. The question is whether you’re building awareness of your dependency into how you work.\n\nA few things worth thinking about.\n\nAnthropic has begun preparations for a potential IPO as soon as 2026, hiring Wilson Sonsini to advise on the process. The revenue growth is real and impressive. But so was Uber’s.\n\nThe question isn’t whether AI coding tools are valuable. They obviously are. The question is what happens when the price tag matches the value. When the $100/month plan becomes $300/month, or usage-based pricing becomes the only option, or the rate limits get tight enough that you’re effectively paying per-task anyway.\n\nWe’re in a window right now where the economics of these tools are artificially favorable. That window will close. Not because anyone is being deceptive, but because the math demands it.\n\nThe developers who come out ahead won’t be the ones who got the most subsidized tokens. They’ll be the ones who used this window to build workflows that are observable, portable, and resilient. The ones who tracked what they consumed, understood what it really cost, and built systems that don’t have a single point of failure at the inference layer.\n\n“The best time to audit your AI dependencies is before the price correction. Not after.”\n\nGet updates on open source and distributed systems in AI infrastructure.",
    "readingTime": 4,
    "keywords": [
      "max plan",
      "venture capital",
      "rate limits",
      "claude code",
      "anthropic",
      "math",
      "you’re",
      "users",
      "ones",
      "they’re"
    ],
    "qualityScore": 1,
    "link": "https://papercompute.com/blog/true-cost-of-claude-code/",
    "thumbnail_url": "https://papercompute.com/og/true-cost-of-claude-code.png",
    "created_at": "2026-02-21T12:24:51.111Z",
    "topic": "tech"
  },
  {
    "slug": "mark-cuban-says-ai-wont-take-your-job-anytime-soon-because-it-still-acts-like-a-hungover-college-internwith-a-100k",
    "title": "Mark Cuban says AI won’t take your job anytime soon because it still acts like a hungover college intern—with a $100K price tag to show for it",
    "description": "As tech CEOs predict mass job displacement, billionaire former Shark Tank star Mark Cuban just revealed the “smartest counter” to that narrative.",
    "fullText": "Despite ongoing fears that artificial intelligence could wipe out entire career fields, billionaire Mark Cuban says the concerns may be overblown—for now.\n\nThe former Shark Tank star took to X to respond to a viral clip from the All-In podcast, in which investors Jason Calacanis and Chamath Palihapitiya revealed the real-world expense of deploying AI agents to enhance productivity: In some cases AI agents are costing more than $300 per day—adding up to over $100,000 annually. For Palihapitiya, founder of Social Capital, the price has forced him to rethink the budget he’s willing to give top developers, warning that otherwise, “I’ll run out of money.”\n\nHave AI-driven layoffs materialized as predicted?\n\nHow do AI agents compare to human workers currently?\n\nWhat are the actual costs of deploying AI agents?\n\nWhy does Mark Cuban think AI won't replace workers soon?\n\nFor Cuban, that reality is the “smartest counter” he has seen so far to predictions that AI will replace large numbers of workers—at least in the short term.\n\nEven if the technology is capable, he said, companies still need to prove the economics make sense, and he’s not convinced the high price tag outweighs the value humans continue to bring.\n\n“Humans have a far greater capacity to know the outcomes of their actions,” Cuban said. “Agents, and LLMs as well, never do.”\n\nAI systems still lack real-world judgment in ways that make replacing workers risky, Cuban added. He pointed to a simple example: An 18-month-old who pushes a sippy cup off a high chair quickly learns the consequences from their parents’ reaction. AI, on the other hand, lacks awareness.\n\n“Agents can tell you the sippy cup will fall,” Cuban said. “But they have no idea of the context and what will happen next.”\n\nThe technology also lacks consistency, often “spac[ing] out” and failing to recognize why and when mistakes occur, he said—a level of competency on par with the youngest Gen Z talent.\n\n“Agents are still like college interns that come in hungover, make mistakes, and don’t take responsibility for them,” he added.\n\nTaken together, Cuban’s argument suggests that the biggest obstacle to AI replacing workers may not be the technology itself—but whether companies can trust it to perform consistently at a price that makes sense.\n\nCuban declined to elaborate further after Fortune reached out for comment.\n\nDespite AI’s current flaws, business leaders continue to warn that rapid technological advances could soon reshape the workforce.\n\nDario Amodei, CEO of Anthropic, has warned that AI could disrupt half of entry-level jobs within one to five years. More recently, he suggested the technology could become capable of performing most jobs, if not all, in “much less than five years.”",
    "readingTime": 3,
    "keywords": [
      "sippy cup",
      "replacing workers",
      "mark cuban",
      "technology",
      "agents",
      "real-world",
      "deploying",
      "he’s",
      "replace",
      "soon"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/mark-cuban-shares-smartest-counter-161052827.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/8.3TDomhL_jBaoSnTb0GEw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/a01b279bf5843ad82bf1cdceccc34903",
    "created_at": "2026-02-21T12:24:49.892Z",
    "topic": "finance"
  },
  {
    "slug": "the-rolling-layoffs-at-jack-dorseys-block",
    "title": "The Rolling Layoffs at Jack Dorsey's Block",
    "description": "Workers describe a deteriorating culture at Block, the company behind Square and Cash App, where layoffs continue and employees are expected to use AI tools daily.",
    "fullText": "workers were laid off in early February from Jack Dorsey’s Block, some of the people remaining at the company say the internal culture has devolved to a point where performance anxiety is running rampant, using generative AI is required, and overall morale is rapidly deteriorating. Block is the parent company behind the merchant payment processor Square and the payment app Cash App. Dorsey cofounded the company in 2009 after previously cofounding Twitter.\n\n“Morale is probably the worst I’ve felt in four years,” reads an employee complaint submitted to Dorsey in a recent all-hands meeting, a transcript of which was seen by WIRED. “The overarching culture at Block is crumbling.” WIRED spoke with seven current and former Block employees, who requested anonymity to speak freely about internal operations at the company. A Block spokesperson did not respond to requests for comment.\n\nThe layoffs at Block started this month and could eventually impact up to 10 percent of the company's workforce, according to reporting by Bloomberg. Before the headcount reductions began, Block had around 11,000 people on staff. Rather than a one-off event, management has slowly enacted the firings over the course of weeks and told employees that the process will continue through the end of this month, sources tell WIRED.\n\n“We don't yet know if our livelihoods will be affected, and this makes it incredibly hard to make major life choices without knowing if we still have a job next week,” reads another employee complaint from the same meeting with Dorsey.\n\nMultiple sources who spoke with WIRED say they were appalled when Arnaud Weber, Block’s engineering lead, sent out an email after the initial wave of layoffs characterizing them as being performance-related rather than a cost-saving measure. The sources say they disagree with management’s internal messaging about the firings being merit-based.\n\n“As part of our 2025 performance cycle, we have parted ways with teammates who weren't meeting the expectations of their role,” wrote Weber in the email, which was viewed by WIRED. “These departures were based on clear performance gaps, role expectations, and alignment coming out of calibrations on the bar for each level.”\n\nBlock employees are currently expected to send an update email to Dorsey every week, who then uses generative AI to summarize the thousands of messages. In the same all-hands meeting, which took place after hundreds of staff had already been fired, Dorsey said that frequent topics cited by workers in their latest messages included “widespread concerns about layoffs,” “performance anxiety,” and “the tension between accelerating delivery through AI adoption versus maintaining code quality and engineering rigor.”\n\nDuring the meeting, Dorsey reiterated that the layoffs were made for performance reasons, saying that there was “a sizable portion of our population that have been phoning it in.” He also stressed that the remaining workers should be using generative AI tools to maximize productivity, or else Block would risk being outpaced by its competitors.\n\n“Top-down mandates to use large language models are crazy,” says one current Block employee. “If the tool were good, we’d all just use it.”",
    "readingTime": 3,
    "keywords": [
      "block employees",
      "employee complaint",
      "performance anxiety",
      "layoffs",
      "workers",
      "internal",
      "generative",
      "email",
      "culture",
      "payment"
    ],
    "qualityScore": 1,
    "link": "https://www.wired.com/story/inside-rolling-layoffs-jack-dorsey-block/",
    "thumbnail_url": "https://media.wired.com/photos/698d1d449536f5a1f6e94f7b/191:100/w_1280,c_limit/Block-Workers-Call-Performance-Driven-Layoffs-a-Smokescreen-Business-1470988103.jpg",
    "created_at": "2026-02-21T06:29:59.674Z",
    "topic": "tech"
  },
  {
    "slug": "ruby-is-the-best-language-for-building-ai-apps",
    "title": "Ruby Is the Best Language for Building AI Apps",
    "description": "A pragmatic, code-first argument for Ruby as the best language to ship AI products in 2026.",
    "fullText": "If your goal is to ship AI applications in 2026, Ruby is the best language to do it.\n\nPython owns model training. PyTorch, TensorFlow, the entire notebooks-and-papers gravity well. Nobody disputes that.\n\nBut you’re not training LLMs. Almost nobody is. Each training run costs millions of dollars. The dataset is the internet!\n\nThis is what AI development today looks like:\n\nThe entire Python ML stack is irrelevant to achieve this. What matters is everything around it: streaming responses to users, persisting conversations, tracking costs, switching providers when pricing changes.\n\nThat’s web application engineering. That’s where Ruby and Rails shine like no other.\n\nYou need a beautiful, truly provider-independent API. Let me show you.\n\nYou need to specify the provider, create an array of messages that need to be instantiated, etc.\n\nWhat if you want to use a model from another provider?\n\nIf you’re running AI in production, you need to track token usage. This is how you price your app.\n\nDifferent key and different structure!\n\nSame interface. Every provider. Every model.\n\nThis isn’t just about aesthetics.\n\nIt’s about cognitive overhead: how many abstractions, how many provider-specific details, how many different data structures you need to hold in your head instead of focusing on what really matters: prompts and tool design.\n\nLow cognitive overhead compounds: faster onboarding, fewer accidental bugs, easier refactors, and cleaner debugging when production explodes at 2AM.\n\nRuby’s advantage here is cultural: elegant APIs are treated as first-class engineering work, not icing on the cake.\n\nModel calls are only a small chunk of your code. The rest makes up the bulk of it: auth, billing, background jobs, streaming UI, persistence, admin screens, observability, even native apps.\n\nRails gives you a beautiful, coherent answer for all of it.\n\nWith RubyLLM + Rails, the core streaming loop is tiny:\n\nThis gives you streaming chunks to your web app and persistence in your DB in absurdly few lines of code.\n\nLLM workloads are mostly network-bound and streaming-bound. That’s exactly where Ruby’s Async ecosystem shines. Fibers let you handle high concurrency without thread explosion and resource waste. No need to plaster the code with async/await keywords. RubyLLM became concurrent with 0 code changes.\n\nI wrote a deep dive here: Async Ruby is the Future of AI Apps (And It’s Already Here)\n\nSomeone ported RubyLLM’s API design to JavaScript as NodeLLM. Same design. Clean code, good docs.\n\nThe JavaScript community’s response: zero upvotes on Reddit. 14 GitHub stars. Top comments: “How’s this different from AI SDK?” and “It’s always fun when you AI bros post stuff. They all look and sound the same. Also, totally unnecessary.”\n\nRubyLLM: #1 on Hacker News. ~3,600 stars. 5 million downloads. Half a million people using RubyLLM-powered apps today.\n\nSame design. Wildly different reception. That tells you everything about which community is ready for this moment.\n\nAnd teams that switched from Python are not going back:\n\nWe had a customer deployment coming up and our Langgraph agent was failing. I rebuilt it using RubyLLM. Not only was it far simpler, it performed better than the Langgraph agent.\n\nOur first pass at the AI Agent used langchain… it was so painful that we built it from scratch in Ruby. Like a cloud had lifted. Langchain was that bad.\n\nAt Yuma, serving over 100,000 end users, our unified AI interface was awful. RubyLLM is so much nicer than all of that.\n\nThese aren’t people who haven’t tried Python. They tried it, shipped it, and replaced it.\n\nWhen we freed ourselves from complexity, this community built Twitter, GitHub, Shopify, Basecamp, Airbnb. Rails changed web development forever.\n\nNow we have the chance to change AI app development. Because AI apps are all about the product. And nobody builds products better than Ruby developers.",
    "readingTime": 4,
    "keywords": [
      "langgraph agent",
      "cognitive overhead",
      "code",
      "model",
      "streaming",
      "design",
      "apps",
      "training",
      "nobody",
      "development"
    ],
    "qualityScore": 1,
    "link": "https://paolino.me/ruby-is-the-best-language-for-ai-apps/",
    "thumbnail_url": "https://paolino.me/images/rubyconfth-2026-keynote.jpg",
    "created_at": "2026-02-21T06:29:58.795Z",
    "topic": "tech"
  },
  {
    "slug": "ldos-toward-a-learningdirected-operating-system",
    "title": "LDOS: Toward a Learning-Directed Operating System",
    "description": "Editors’ note: LDOS (Learning Directed Operating System) is among the most exciting expedition projects that showcase how AI could help revamp policies and mechanisms of modern operating systems (arguably the most important systems software). In this article (the fifth blog in The Next Horizon of Sy",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.sigops.org/2026/ldos-toward-a-learning-directed-operating-system/",
    "thumbnail_url": "https://www.sigops.org/wp-content/uploads/2026/02/image-12.png",
    "created_at": "2026-02-21T06:29:58.420Z",
    "topic": "tech"
  },
  {
    "slug": "promptspy-ushers-in-the-era-of-android-threats-using-genai",
    "title": "PromptSpy ushers in the era of Android threats using GenAI",
    "description": "ESET researchers discover PromptSpy, the first known Android malware to abuse generative AI in its execution flow.",
    "fullText": "ESET researchers uncovered the first known case of Android malware abusing generative AI for context-aware user interface manipulation. While machine learning has been used to similar ends already – just recently, researchers at Dr.WEB found Android.Phantom, which uses TensorFlow machine learning models to analyze advertisement screenshots and automatically click on detected elements for large scale ad fraud – this is the first time we have seen generative AI deployed in this manner. Because the attackers rely on prompting an AI model (in this instance, Google’s Gemini) to guide malicious UI manipulation, we have named this family PromptSpy. This is the second AI powered malware we have discovered – following PromptLock in August 2025, the first known case of AI-driven ransomware.\n\nWhile generative AI is deployed only in a relatively minor part of PromptSpy's code – that responsible for achieving persistence – it still has a significant impact on the malware's adaptability. Specifically, Gemini is used to analyze the current screen and provide PromptSpy with step-by-step instructions on how to ensure the malicious app remains pinned in the recent apps list, thus preventing it from being easily swiped away or killed by the system. The AI model and prompt are predefined in the code and cannot be changed. Since Android malware often relies on UI navigation, leveraging generative AI enables the threat actors to adapt to more or less any device, layout, or OS version, which can greatly expand the pool of potential victims.\n\nThe main purpose of PromptSpy is to deploy a built-in VNC module, giving operators remote access to the victim’s device. This Android malware also abuses the Accessibility Service to block uninstallation with invisible overlays, captures lockscreen data, records video. It communicates with its C&C server via the VNC protocol, using AES encryption.\n\nBased on language localization clues and the distribution vectors observed during analysis, this campaign appears to be financially motivated and seems to primarily target users in Argentina. Interestingly, analyzed PromptSpy samples suggest that it was developed in a Chinese‑speaking environment.\n\nPromptSpy is distributed by a dedicated website and has never been available on Google Play. As an App Defense Alliance partner, we nevertheless shared our findings with Google. Android users are automatically protected against known versions of this malware by Google Play Protect, which is enabled by default on Android devices with Google Play Services.\n\nEven though PromptSpy uses Gemini in just one of its features, it still demonstrates how incorporating these AI tools can make malware more dynamic, giving threat actors ways to automate actions that would normally be more difficult with traditional scripting.\n\nAs was briefly mentioned already, Android malware usually depends on hardcoded screen features such as taps, coordinates, or UI selectors – methods that can break with UI changes across devices, OS versions, or manufacturer skins. PromptSpy aims to achieve persistence by staying embedded in the list of recent apps by executing the “lock app in recent apps” gesture (the full process is described in the Analysis section), which varies between devices and manufacturers. This makes it difficult to automate with fixed scripts traditionally used by Android malware.\n\nPromptSpy therefore takes a completely different approach: it sends Gemini a natural‑language prompt along with an XML dump of the current screen, giving the AI a detailed view of every UI element: its text, type, and exact position on the display.\n\nGemini processes this information and responds with JSON instructions that tell the malware what action to perform (for example, a tap) and where to perform it. The malware saves both its previous prompts and Gemini’s responses, allowing Gemini to understand context and to coordinate multistep interactions.\n\nFigure 1 shows a code snippet of PromptSpy’s initialization of communication with Gemini, including the first prompt used. By handing the decision-making over to Gemini, the malware can recognize the correct UI element and perform the appropriate gesture, keeping the malware alive even if the user tries to close it.\n\nPromptSpy continues prompting Gemini until the AI confirms that the app has been successfully locked, showing a feedback loop where the malware waits for validation before moving on.\n\nIn February 2026, we uncovered two versions of a previously unknown Android malware family. The first version, which we named VNCSpy, appeared on VirusTotal on January 13th, 2026 and was represented by three samples uploaded from Hong Kong. On February 10th, 2026, four samples of more advanced malware based on VNCSpy were uploaded to VirusTotal from Argentina.\n\nOur analysis of the samples from Argentina revealed multistage malware with a malicious payload that misuses Google’s Gemini. Based on these findings, we named the first stage of this malware PromptSpy dropper, and its payload PromptSpy.\n\nIt should be noted that we haven’t yet seen any samples of the PromptSpy dropper or its payload in our telemetry, which might indicate that both of them are just proofs of concept. However, based on the existence of a possible distribution domain described in the following paragraphs, we cannot discount the possibility of the PromptSpy dropper and PromptSpy existing in the wild.\n\nAccording to VirusTotal data, all four PromptSpy dropper samples were distributed through the website mgardownload[.]com; it was already offline during our analysis.\n\nAfter installing and launching PromptSpy dropper, it opened a webpage hosted on m‑mgarg[.]com. Although this domain was also offline, Google’s cached version revealed that it likely impersonated a Chase Bank (legally, JPMorgan Chase Bank N.A.) site (see Figure 2).\n\nThe malware uses similar branding, with the app name MorganArg and the icon inspired by Chase bank (see Figure 3). MorganArg, likely a shorthand for “Morgan Argentina”, also appears as the name of the cached website, suggesting a regional targeting focus.\n\nWe used the m-mgarg[.]com domain to pivot in VirusTotal, leading us to yet another Android malware sample (Android/Phishing.Agent.M). VirusTotal showed the spoofed website in Spanish, with an Iniciar sesión (Login) button, indicating that the page was probably intended to mimic a website of a bank (see Figure 4).\n\nThis trojan appears to function as a companion application developed by the same threat actor behind VNCSpy and PromptSpy. In the background, the trojan contacts its server to request a configuration file, which includes a link to download another APK, presented to the victim, in Spanish, as an update. During our research, the configuration server was no longer accessible, so the exact download URL remains unknown. However, given that it uses the same unique bank spoofing website, the same app name, icon, and, most importantly, is signed by the same unique developer certificate as the PromptSpy dropper – we strongly suspect this app may serve as the initial stage designed to lead victims toward installing PromptSpy.\n\nBoth VNCSpy and PromptSpy include a VNC component, giving their operators full remote access to compromised devices once victims enable Accessibility Services (see Figure 5). This allows the malware operators to see everything happening on the device, and to perform taps, swipes, gestures, and text input as though they were physically holding the phone.\n\nOn top of the malicious capabilities already contained in VNCSpy, PromptSpy adds AI‑assisted UI manipulation, helping it maintain persistence by keeping the malicious app pinned in the recent apps list (an example of how the lock is indicated in the list can be seen in Figure 6).\n\nWe believe this functionality is used before the VNC session is established, so that the user or system will not kill the PromptSpy activity from the list of recent apps. In Figure 7, you can see PromptSpy network communication with Gemini AI.\n\nWhile analyzing PromptSpy, we noticed that it contains debug strings written in simplified Chinese. It even includes handling for various Chinese Accessibility event types (see Figure 8), a debug method that had been disabled in the code but not removed. The primary purpose of this method is to provide a localized (Chinese) explanation for various accessibility events that occur on an Android device. This makes the event logs more understandable for Chinese-speaking users or developers, rather than just displaying raw integer codes.\n\nWith medium confidence, these details suggest that PromptSpy was developed in a Chinese‑speaking environment.\n\nOur technical analysis focuses on the PromptSpy dropper and its payload, PromptSpy. PromptSpy is embedded (app-release.apk) inside the dropper’s asset directory. This APK holds the core malicious functionality. When the dropper is launched, it displays a prompt urging the user to install what appears to be an updated version of the app. This “update” is actually the PromptSpy payload, which the user must install manually (see Figure 9).\n\nOnce installed and launched, PromptSpy requests Accessibility Service permissions, giving the malware the ability to read on‑screen content and perform automated clicks.\n\nThen PromptSpy shows a simple loading-style decoy screen in the foreground (see Figure 10). Meanwhile, in the background, it begins communicating with Gemini AI to obtain instructions needed to lock its process in the Recent Apps list – a simple persistence technique that allows PromptSpy to remain active and locked in place even after the device is rebooted.\n\nWhen the user sees the Loading, please wait activity, PromptSpy uses Accessibility Services to open the Recent Apps screen and collect detailed UI information: visible text, content descriptions, class names, package names, and screen bounds. It serializes this dynamic UI snapshot as XML and includes it in its prompt to Gemini. Gemini then returns step-by-step tap instructions on how to achieve the “app lock” gesture.\n\nThis process forms a continuous loop:\n\nThe loop continues until Gemini confirms that the app is successfully locked in recent apps. Here is an example structure:\n\nAll actions suggested by Gemini – taps, swipes, navigation – are executed through Accessibility Services, allowing the malware to interact with the device without user input.\n\nPromptSpy’s main malicious capability lies in its built‑in VNC service. This allows attackers to remotely view the victim’s screen in real time and fully control the device.\n\nThe malware communicates with its hardcoded command‑and‑control (C&C) server at 54.67.2[.]84 using the VNC protocol; the messages are AES-encrypted using a hardcoded key. Through this communication channel, the malware can:\n\nPromptSpy also misuses Accessibility Services as an anti‑removal mechanism. When the user attempts to uninstall the payload or disable Accessibility Services, the malware overlays transparent rectangles on specific screen areas - particularly over buttons containing substrings like stop, end, clear, and Uninstall. These overlays are invisible to the user but intercept interactions, making removal difficult. In Figure 11, we’ve run PromptSpy with the debug flag enabled (kept there by developers) that would set the color of the transparent rectangle, to visualize where they are specifically displayed. However, on the actual device, they are fully invisible.\n\nBecause PromptSpy blocks uninstallation by overlaying invisible elements on the screen, the only way for a victim to remove it is to reboot the device into Safe Mode, where third‑party apps are disabled and can be uninstalled normally.\n\nTo enter Safe Mode, users should typically press and hold the power button, long‑press Power off, and confirm the Reboot to Safe Mode prompt (though the exact method may differ by device and manufacturer). Once the phone restarts in Safe Mode, the user can go to Settings → Apps → MorganArg and uninstall it without interference.\n\nPromptSpy shows that Android malware is beginning to evolve in a sinister way. By relying on generative AI to interpret on‑screen elements and decide how to interact with them, the malware can adapt to virtually any device, screen size, or UI layout it encounters. Instead of hardcoded taps, it simply hands AI a snapshot of the screen and receives precise, step‑by‑step interaction instructions in return, helping it achieve a persistence technique resistant to UI changes.\n\nMore broadly, this campaign shows how generative AI can make malware far more dynamic and capable of real‑time decision‑making. PromptSpy is an early example of generative AI‑powered Android malware, and it illustrates how quickly attackers are beginning to misuse AI tools to improve impact.\n\nA comprehensive list of indicators of compromise (IoCs) and samples can be found in our GitHub repository.\n\nSHA-1\nFilename\nDetection\nDescription\n\n6BBC9AB132BA066F63676E05DA13D108598BC29B\nnet.ustexas.myavlive.apk\nAndroid/Spy.VNCSpy.A\nAndroid VNCSpy malware.\n\n375D7423E63C8F5F2CC814E8CFE697BA25168AFA\nnlll4.un7o6.q38l5.apk\nAndroid/Spy.VNCSpy.A\nAndroid VNCSpy malware.\n\n3978AC5CD14E357320E127D6C87F10CB70A1DCC2\nppyzz.dpk0p.ln441.apk\nAndroid/Spy.VNCSpy.A\nAndroid VNCSpy malware.\n\nE60D12017D2DA579DF87368F5596A0244621AE86\nmgappc-1.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\n9B1723284E311794987997CB7E8814EB6014713F\nmgappm-1.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\n076801BD9C6EB78FC0331A4C7A22C73199CC3824\nmgappn-0.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\n8364730E9BB2CF3A4B016DE1B34F38341C0EE2FA\nmgappn-1.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\nF8F4C5BC498BCCE907DC975DD88BE8D594629909\napp-release.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy.\n\nC14E9B062ED28115EDE096788F62B47A6ED841AC\nmgapp.apk\nAndroid/Phishing.Agent.M\nAndroid phishing malware.\n\nIP\nDomain\nHosting provider\nFirst seen\nDetails\n\n52.222.205[.]45\nm-mgarg[.]com\nAmazon.com, Inc.\n2026‑01‑12\nPhishing website.\n\n54.67.2[.]84\nN/A\nAmazon.com, Inc.\nN/A\nC&C server.\n\n104.21.91[.]170\nmgardownload[.]com\nCloudflare, Inc.\n2026‑01‑13\nDistribution website.\n\nThis table was built using version 18 of the MITRE ATT&CK framework.\n\nTactic\nID\nName\nDescription\n\nPersistence\nT1398\nBoot or Logon Initialization Scripts\nPromptSpy receives the BOOT_COMPLETED broadcast intent to activate at device startup.\n\nT1541\nForeground Persistence\nPromptSpy uses foreground persistence to keep a service running.\n\nDefense Evasion\nT1516\nInput Injection\nPromptSpy abuses the accessibility service to prevent its removal.\n\nCredential Access\nT1417.002\nMalicious Third Party Keyboard App: GUI Input Capture\nPromptSpy can intercept Android lockscreen PIN and password.\n\nDiscovery\nT1426\nSystem Information Discovery\nPromptSpy obtains device name, model, and OS version.\n\nCollection\nT1418\nSoftware Discovery\nPromptSpy can obtain a list of installed applications.\n\nT1513\nScreen Capture\nPromptSpy can record the screen.\n\nCommand and Control\nT1663\nRemote Access Software\nPromptSpy can use VNC to remotely control a compromised device.\n\nT1521.001\nStandard Cryptographic Protocol: Symmetric Cryptography\nPromptSpy encrypts C&C communication using AES.\n\nExfiltration\nT1646\nExfiltration Over C2 Channel\nPromptSpy can exfiltrate collected data to the C&C server.",
    "readingTime": 12,
    "keywords": [
      "chinese‑speaking environment",
      "amazon.com inc",
      "c&c server",
      "vnc protocol",
      "machine learning",
      "android/spy.vncspy.a android",
      "android/spy.promptspy.a android",
      "threat actors",
      "successfully locked",
      "remote access"
    ],
    "qualityScore": 1,
    "link": "https://www.welivesecurity.com/en/eset-research/promptspy-ushers-in-era-android-threats-using-genai/",
    "thumbnail_url": "https://web-assets.esetstatic.com/wls/2026/02-26/promptspy/promptspy-gemini-genai-malware.jpg",
    "created_at": "2026-02-21T06:29:58.261Z",
    "topic": "science"
  },
  {
    "slug": "why-the-playstation-6-will-be-delayed",
    "title": "Why The PlayStation 6 Will Be Delayed",
    "description": "This week, Lucy and Tam discuss how the AI boom is impacting gamers. Most notably, it's holding up hardware like the Steam Machine and, allegedly, the PlayStation 6.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/videos/why-the-playstation-6-will-be-delayed/2300-6466729/",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1352/13527689/4654778-spoton_ps6delay_20260220v2.jpg",
    "created_at": "2026-02-21T06:29:55.881Z",
    "topic": "gaming"
  },
  {
    "slug": "ai-could-make-your-next-tv-more-expensive",
    "title": "AI Could Make Your Next TV More Expensive",
    "description": "Add \"television prices\" to the long list of things AI is changing.",
    "fullText": "The scarcity of RAM brought on by the artificial intelligence boom, dubbed RAMageddon, is affecting more than just the price of PCs. AI could make new televisions more expensive too—as well as—game consoles, cell phones, high-tech coffee makers, and anything else with memory and a processor. But if you're in the market for a new TV, you might be better off buying sooner rather than later.\n\nAs Axios reports, televisions generally require 1GB to 8GB of RAM to run \"smart TV\" features and to process video and data, and the memory units widely found in 4K TVs have more than quadrupled in price over the last year. That extra cost could be passed on to consumers: Analyst TrendForce said last month that a price hike on TVs was \"unavoidable,\" while Samsung acknowledged it may need to reprice its products. That said, a typical television uses less memory, and less advanced memory, than some other key devices, so a potential price-spike is likely to be less dramatic than it is for things like PCs and smartphones. We'll see for sure when manufacturers announce the prices of their 2026 models.\n\nCompanies like Microsoft, Google, and Nvidia are scooping up memory supply to run AI data centers, and most TV makers don't have the market power of these gigantic corporations. \"When memory tightens, prices rise, product launches shift...margins compress and smaller companies struggle more than large tech giants,\" Marco Mezger, executive vice president of memory tech company Neumonda, told Axios. There is good news for consumers, however.\n\nHigher RAM prices have yet to hit the retail TV market, making now an unusually good time to buy a television. Overall, the price of smart TVs decreased by 15% between 2024 and the start of 2026, so you're starting from a good place. In addition, manufacturers generally offer lower prices at this time of year to clear shelf space ahead of new model releases. While more expensive RAM could be baked into the price of 2026 televisions, sets on the shelves now were priced before the effects of the shortage hit the retail market. Plus, some companies price their TVs lower because they make a lot of money collecting your data—unless you do you what you can to stop them, of course. All of which leads to ridiculously good deals, like $900 for a 65-inch OLED TV from Samsung. Bottom line: if you're in the market for a new TV, don't wait. (Though, chances are, you might not need a new TV.)\n\nNo one can say for sure how long the memory shortage will last, but the consensus of industry analysts is that we likely won't see a return to anything we'd consider normal before 2028. AI demand is projected to consume 70% of all high-end DRAM in 2026, so manufacturers are prioritizing it over the less advanced, less in-demand memory chips used for TVs and appliances. While investors are sinking billions into ramping up memory manufacturing, it takes around 19 months to get a factory up and running in Taiwan, and even longer in the U.S., so TV prices will likely remain high into 2028.",
    "readingTime": 3,
    "keywords": [
      "less advanced",
      "memory",
      "market",
      "televisions",
      "you're",
      "manufacturers",
      "expensive",
      "makers",
      "generally",
      "smart"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/why-ai-could-make-tvs-more-expensive?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHY8FFT9TW11MP3SKTAANQJ4/hero-image.fill.size_1200x675.png",
    "created_at": "2026-02-21T06:29:52.704Z",
    "topic": "tech"
  },
  {
    "slug": "openai-just-hired-back-another-employee-from-mira-muratis-thinking-machines-lab",
    "title": "OpenAI just hired back another employee from Mira Murati's Thinking Machines Lab",
    "description": "Thinking Machines Lab faces departures as employees rejoin OpenAI amid poaching efforts by major tech firms.",
    "fullText": "Another employee at Thinking Machines Lab is leaving to rejoin OpenAI.\n\nIt's the latest in a string of departures from the $12 billion AI startup, which is led by former OpenAI CTO Mira Murati and lately has been the subject of high-profile poaching campaigns from bigger tech companies.\n\nThe latest employee to go back to OpenAI is Jolene Parish, who joined Thinking Machines Lab in April last year, according to her LinkedIn profile. She had worked at OpenAI for three years prior. Before that, she worked for 10 years on security at Apple, her profile says.\n\nOther employees rejoined OpenAI last month. Two co-founders, former CTO Barret Zoph and Luke Metz, both left, along with researcher Sam Schoenholz.\n\nLia Guy, another researcher, also rejoined OpenAI, The Information reported. Another cofounder, Andrew Tulloch, left for Meta late last year, The Wall Street Journal reported.\n\nOpenAI and Thinking Machines Lab declined to comment.\n\nThinking Machines Lab raised a monster $2 billion funding round last year, valuing the company at $12 billion, spokespeople said at the time. The startup launched its first product, Tinker, last October.\n\nThe San Francisco-based company has become known for attracting star-studded talent. It quietly hired Neal Wu, a legendary coder who won three gold medals in an Olympiad for programming, and Soumith Chintala, the creator of the open-source AI project PyTorch at Meta, who is now Thinking Machines Lab's CTO, Business Insider previously reported.\n\nHave a tip? Contact this reporter via email at crollet@businessinsider.com or on Signal and WhatsApp at 628-282-2811. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 2,
    "keywords": [
      "machines lab",
      "rejoined openai",
      "thinking machines lab",
      "another",
      "employee",
      "latest",
      "startup",
      "profile",
      "researcher",
      "meta"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/thinking-machines-lab-employee-rejoins-openai-talent-exodus-2026-2",
    "thumbnail_url": "https://i.insider.com/6998c52befb52c8bd0de973f?width=1200&format=jpeg",
    "created_at": "2026-02-21T01:06:34.961Z",
    "topic": "finance"
  },
  {
    "slug": "heres-why-you-should-never-use-ai-to-generate-your-passwords",
    "title": "Here's Why You Should Never Use AI to Generate Your Passwords",
    "description": "ChatGPT isn't good at generating secure passwords.",
    "fullText": "I'm a bit of a broken record when it comes to personal security on the internet: Make strong passwords for each account; never reuse any passwords; and With these three steps combined, your general security is pretty much set. But how you make those passwords matters just as much as making each strong and unique. As such, please don't use an AI program to generate your passwords.\n\nIf you're a fan of chatbots like ChatGPT, Claude, or Gemini, it might seem like a no-brainer to ask the AI to generate passwords for you. You might like how they handle other tasks for you, so it might make sense that something seemingly so high-tech yet accessible could produce secure passwords for your accounts. But LLMs (large language models) are not necessarily good at everything, and creating good passwords just so happens to be among those faults.\n\nAs highlighted by Malwarebytes Labs, researchers recently investigated AI-generated passwords, and evaluated their security. In short? The findings aren't good. Researchers tested password generation across ChatGPT, Claude, and Gemini, and discovered that the passwords were \"highly predictable\" and \"not truly random.\" Claude, in particular, didn't fare well: Out of 50 prompts, the bot was only able to generate 23 unique passwords. Claude gave the same password as an answer 10 times. The Register reports that researchers found similar flaws with AI systems like GPT-5.2, Gemini 3 Flash, Gemini 3 Pro, and even Nano Banana Pro. (Gemini 3 Pro even warned the passwords shouldn't be used for \"sensitive accounts.\")\n\nThe thing is, these results seem good on the surface. They look uncrackable because they're a mix of numbers, letters, and special characters, and password strength identifiers might say they're secure. But these generations are inherently flawed, whether that's because they are repeated results, or come with a recognizable pattern. Researchers evaluated the \"entropy\" of these passwords, or the measure of unpredictability, with both \"character statistics\" and \"log probabilities.\" If that all sounds technical, the important thing to note is that the results showed entropies of 27 bits and 20 bits, respectively. Character statistics tests look for entropy of 98 bits, while log probabilities estimates look for 120 bits. You don't need to be an expert in password entropy to know that's a massive gap.\n\nHackers can use these limitations to their advantage. Bad actors can run the same prompts as researchers (or, presumably, end users) and collect the results into a bank of common passwords. If chatbots repeat passwords in their generations, it stands to reason that many people might be using the same passwords generated by those chatbots—or trying passwords that follow the same pattern. If so, hackers could simply try those passwords during break-in attempts, and if you used an LLM to generate your password, it might match. It's tough to say what that exact risk is, but to be truly secure, each of your passwords should be totally unique. Potentially using a password that hackers have in a word bank is an unnecessary risk.\n\nIt might seem surprising that a chatbot wouldn't be good at generating random passwords, but it makes sense based on how they work. LLMs are trained to predict the next token, or data point, that should appear in a sequence. In this case, the LLM is trying to choose the characters that make the most sense to appear next, which is the opposite of \"random.\" If the LLM has passwords in its training data, it may incorporate that into its answer. The password it generates makes sense in its \"mind,\" because that's what it's been trained on. It isn't programmed to be random.\n\nMeanwhile, traditional password managers are not LLMs. Instead, they are designed to produce a truly random sequence, by taking cryptographic bits and converting them into characters. These outputs are not based on existing training data and follow no patterns, so the chances that someone else out there has the same password as you (or that hackers have it stored in a word bank) is slim. There are plenty of options out there to use, and most password managers come with secure password generators.\n\nBut you don't even need one of these programs to make a secure password. Just pick two or three \"uncommon\" words, mix a few of the characters up, and presto: You have a random, unique, and secure password. For example, you could take the words \"shall,\" \"murk,\" and \"tumble,\" and combine them into \"sH@_llMurktUmbl_e.\" (Don't use that one, since it's no longer unique.)\n\nIf you're looking to boost your personally security even further, consider passkeys whenever possible. Passkeys combine the convenience of passwords with the security of 2FA: With passkeys, your device is your password. You use its built-in authentication to log in (face scan, fingerprint, or PIN), which means there's no password to actually create. Without the trusted device, hackers won't be able to break into your account.\n\nNot all accounts support passkeys, which means they aren't a universal solution right now. You'll likely need passwords for some of your accounts, which means abiding by proper security methods to keep things in order. But replacing some of your passwords with passkeys can be a step up in both security and convenience—and avoids the security pitfalls of asking ChatGPT to make your passwords for you.",
    "readingTime": 5,
    "keywords": [
      "gemini pro",
      "character statistics",
      "log probabilities",
      "truly random",
      "password managers",
      "secure password",
      "passwords",
      "security",
      "unique",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/dont-use-ai-to-generate-your-passwords?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHY5M8NYREZ63A0JXYRYBTT4/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-21T01:06:33.965Z",
    "topic": "tech"
  },
  {
    "slug": "code-mode-give-agents-an-api-in-1k-tokens",
    "title": "Code Mode: give agents an API in 1k tokens",
    "description": "Cloudflare introduces Code Mode for Workers AI, a technical approach to fit entire API schemas into 1,000 tokens, enabling LLM agents to execute precise tool calls with minimal latency.",
    "fullText": "Model Context Protocol (MCP) has become the standard way for AI agents to use external tools. But there is a tension at its core: agents need many tools to do useful work, yet every tool added fills the model's context window, leaving less room for the actual task.\n\nCode Mode is a technique we first introduced for reducing context window usage during agent tool use. Instead of describing every operation as a separate tool, let the model write code against a typed SDK and execute the code safely in a Dynamic Worker Loader. The code acts as a compact plan. The model can explore tool operations, compose multiple calls, and return just the data it needs. Anthropic independently explored the same pattern in their Code Execution with MCP post.\n\nToday we are introducing a new MCP server for the entire Cloudflare API — from DNS and Zero Trust to Workers and R2 — that uses Code Mode. With just two tools, search() and execute(), the server is able to provide access to the entire Cloudflare API over MCP, while consuming only around 1,000 tokens. The footprint stays fixed, no matter how many API endpoints exist.\n\nFor a large API like the Cloudflare API, Code Mode reduces the number of input tokens used by 99.9%. An equivalent MCP server without Code Mode would consume 1.17 million tokens — more than the entire context window of the most advanced foundation models.\n\nCode mode savings vs native MCP, measured with tiktoken\n\nYou can start using this new Cloudflare MCP server today. And we are also open-sourcing a new Code Mode SDK in the Cloudflare Agents SDK, so you can use the same approach in your own MCP servers and AI Agents.\n\nThis new MCP server applies Code Mode server-side. Instead of thousands of tools, the server exports just two: search() and execute(). Both are powered by Code Mode. Here is the full tool surface area that gets loaded into the model context:\n\nTo discover what it can do, the agent calls search(). It writes JavaScript against a typed representation of the OpenAPI spec. The agent can filter endpoints by product, path, tags, or any other metadata and narrow thousands of endpoints to the handful it needs. The full OpenAPI spec never enters the model context. The agent only interacts with it through code.\n\nWhen the agent is ready to act, it calls execute(). The agent writes code that can make Cloudflare API requests, handle pagination, check responses, and chain operations together in a single execution.\n\nBoth tools run the generated code inside a Dynamic Worker isolate — a lightweight V8 sandbox with no file system, no environment variables to leak through prompt injection and external fetches disabled by default. Outbound requests can be explicitly controlled with outbound fetch handlers when needed.\n\nSuppose a user tells their agent: \"protect my origin from DDoS attacks.\" The agent's first step is to consult documentation. It might call the Cloudflare Docs MCP Server, use a Cloudflare Skill, or search the web directly. From the docs it learns: put Cloudflare WAF and DDoS protection rules in front of the origin.\n\nStep 1: Search for the right endpoints\nThe search tool gives the model a spec object: the full Cloudflare OpenAPI spec with all $refs pre-resolved. The model writes JavaScript against it. Here the agent looks for WAF and ruleset endpoints on a zone:\n\nThe server runs this code in a Workers isolate and returns:\n\nThe full Cloudflare API spec has over 2,500 endpoints. The model narrowed that to the WAF and ruleset endpoints it needs, without any of the spec entering the context window.\n\nThe model can also drill into a specific endpoint's schema before calling it. Here it inspects what phases are available on zone rulesets:\n\nThe agent now knows the exact phases it needs: ddos_l7 for DDoS protection and http_request_firewall_managed for WAF.\n\nStep 2: Act on the API\nThe agent switches to using execute. The sandbox gets a cloudflare.request() client that can make authenticated calls to the Cloudflare API. First the agent checks what rulesets already exist on the zone:\n\nThe agent sees that managed DDoS and WAF rulesets already exist. It can now chain calls to inspect their rules and update sensitivity levels in a single execution:\n\nThis entire operation, from searching the spec and inspecting a schema to listing rulesets and fetching DDoS and WAF configurations, took four tool calls.\n\nWe started with MCP servers for individual products. Want an agent that manages DNS? Add the DNS MCP server. Want Workers logs? Add the Workers Observability MCP server. Each server exported a fixed set of tools that mapped to API operations. This worked when the tool set was small, but the Cloudflare API has over 2,500 endpoints. No collection of hand-maintained servers could keep up.\n\nThe Cloudflare MCP server simplifies this. Two tools, roughly 1,000 tokens, and coverage of every endpoint in the API. When we add new products, the same search() and execute() code paths discover and call them — no new tool definitions, no new MCP servers. It even has support for the GraphQL Analytics API.\n\nOur MCP server is built on the latest MCP specifications. It is OAuth 2.1 compliant, using Workers OAuth Provider to downscope the token to selected permissions approved by the user when connecting. The agent  only gets the capabilities the user explicitly granted.\n\nFor developers, this means you can use a simple agent loop and still give your agent access to the full Cloudflare API with built-in progressive capability discovery.\n\nSeveral approaches have emerged to reduce how many tokens MCP tools consume:\n\nClient-side Code Mode was our first experiment. The model writes TypeScript against typed SDKs and runs it in a Dynamic Worker Loader on the client. The tradeoff is that it requires the agent to ship with secure sandbox access. Code Mode is implemented in Goose and Anthropics Claude SDK as Programmatic Tool Calling.\n\nCommand-line interfaces are another path. CLIs are self-documenting and reveal capabilities as the agent explores. Tools like OpenClaw and Moltworker convert MCP servers into CLIs using MCPorter to give agents progressive disclosure. The limitation is obvious: the agent needs a shell, which not every environment provides and which introduces a much broader attack surface than a sandboxed isolate.\n\nDynamic tool search, as used by Anthropic in Claude Code, surfaces a smaller set of tools hopefully relevant to the current task. It shrinks context use but now requires a search function that must be maintained and evaluated, and each matched tool still uses tokens.\n\nEach approach solves a real problem. But for MCP servers specifically, server-side Code Mode combines their strengths: fixed token cost regardless of API size, no modifications needed on the agent side, progressive discovery built in, and safe execution inside a sandboxed isolate. The agent just calls two tools with code. Everything else happens on the server.\n\nThe Cloudflare MCP server is available now. Point your MCP client at the server URL and you'll be redirected to Cloudflare to authorize and select the permissions to grant to your agent. Add this config to your MCP client:\n\nFor CI/CD, automation, or if you prefer managing tokens yourself, create a Cloudflare API token with the permissions you need. Both user tokens and account tokens are supported and can be passed as bearer tokens in the Authorization header.\n\nMore information on different MCP setup configurations can be found at the Cloudflare MCP repository.\n\nCode Mode solves context costs for a single API. But agents rarely talk to one service. A developer's agent might need the Cloudflare API alongside GitHub, a database, and an internal docs server. Each additional MCP server brings the same context window pressure we started with.\n\nCloudflare MCP Server Portals let you compose multiple MCP servers behind a single gateway with unified auth and access control. We are building a first-class Code Mode integration for all your MCP servers, and exposing them to agents with built-in progressive discovery and the same fixed-token footprint, regardless of how many services sit behind the gateway.",
    "readingTime": 7,
    "keywords": [
      "worker loader",
      "ddos protection",
      "code mode",
      "cloudflare api",
      "openapi spec",
      "built-in progressive",
      "sandboxed isolate",
      "mcp servers",
      "progressive discovery",
      "ruleset endpoints"
    ],
    "qualityScore": 1,
    "link": "https://blog.cloudflare.com/code-mode-mcp/",
    "thumbnail_url": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2080o6v9LBfIFbLUW8elRE/3cfaeb0dc0fa56bcbe4c332b1942a167/Code_Mode-_give_agents_an_entire_API_in_1_000_tokens-OG.png",
    "created_at": "2026-02-21T01:06:31.696Z",
    "topic": "tech"
  }
]
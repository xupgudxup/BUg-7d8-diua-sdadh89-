[
  {
    "slug": "web-verbs",
    "title": "Web Verbs",
    "description": "Web Verbs is an extension to NLWeb from Microsoft Research - nlweb-ai/MSR-Web-Verbs",
    "fullText": "nlweb-ai\n\n /\n\n MSR-Web-Verbs\n\n Public\n\n Web Verbs is an extension to NLWeb from Microsoft Research\n\n 12\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n nlweb-ai/MSR-Web-Verbs",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/nlweb-ai/MSR-Web-Verbs",
    "thumbnail_url": "https://opengraph.githubassets.com/6916208f663b363d91d5143fbf9642f88a85d7ed4d139b98090102c47fc90b2f/nlweb-ai/MSR-Web-Verbs",
    "created_at": "2026-02-22T18:20:49.669Z",
    "topic": "tech"
  },
  {
    "slug": "sam-altman-says-elon-musks-idea-of-putting-data-centers-in-space-is-ridiculous",
    "title": "Sam Altman says Elon Musk's idea of putting data centers in space is 'ridiculous'",
    "description": "While in New Delhi on Friday, OpenAI CEO Sam Altman said \"there will come a time\" for orbital data centers, but it won't be anytime soon.",
    "fullText": "Elon Musk's SpaceX wants to launch satellites that act as data centers into space.\n\nOpenAI CEO Sam Altman said placing data centers in space isn't feasible right now.\n\nHe called the idea \"ridiculous\" during an event in New Delhi.\n\nSpaceX CEO Elon Musk and OpenAI CEO Sam Altman famously don't agree on much.\n\nThe latest point of contention: data centers in space. Musk has made it a priority. Altman thinks it's a fantasy, at least for now.\n\n\"I honestly think the idea with the current landscape of putting data centers in space is ridiculous,\" Altman said during a live interview with local media in New Delhi on Friday, causing audience members to laugh.\n\nAltman said that orbital data centers could \"make sense someday,\" but factors like launch costs and the difficulty of repairing a computer chip in space remain overwhelming obstacles.\n\n\"We are not there yet,\" Altman added. \"There will come a time. Space is great for a lot of things. Orbital data centers are not something that's going to matter at scale this decade.\"\n\nMusk would almost certainly disagree.\n\nWhile many Big Tech and AI companies are spending billions on data center construction on Earth, Musk's eyes are on the stars, per usual. Orbital data centers are his latest ambition, as he mentioned in an all-hands xAI meeting in December.\n\nIn February, SpaceX said its goal is to launch a \"constellation of a million satellites that operate as orbital data centers.\" The company has already begun hiring engineers to make that happen.\n\nDuring an all-hands meeting with xAI employees this month, Musk said SpaceX's acquisition of xAI will allow them to deploy the orbital data centers faster.\n\nDespite Altman's skepticism, other tech leaders are also racing to place data centers in space. Google's Project Suncatcher, unveiled in November 2025, aims to do just that. Google CEO Sundar Pichai told Fox News Sunday the company could start placing data centers — powered by the sun — in space as early as 2027.\n\nTech and AI companies rely on data centers to power their products, like large language models and chatbots. Those data centers, however, can deplete water resources, strain power grids, increase pollution, and decrease the overall quality of life.\n\nAn investigation by Business Insider published last year found that over 1,200 data centers had been approved for construction across the US by the end of 2024, nearly four times the number from 2010.\n\nNow, proposed data center campuses in Texas, Oklahoma, and elsewhere are increasingly facing stiff resistance from local communities.",
    "readingTime": 3,
    "keywords": [
      "openai ceo",
      "ceo sam",
      "sam altman",
      "centers",
      "orbital",
      "launch",
      "space",
      "satellites",
      "placing",
      "idea"
    ],
    "qualityScore": 1,
    "link": "https://tech.yahoo.com/business/articles/sam-altman-says-elon-musks-225742940.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/MfHlL9BGZ6ufnEfmueb1nQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA-/https://media.zenfs.com/en/business_insider_consolidated_articles_886/fc41e9e11796ca54b023aa16d9e24f7e",
    "created_at": "2026-02-22T18:20:44.845Z",
    "topic": "tech"
  },
  {
    "slug": "the-nobel-laureate-who-cowrote-why-nations-fail-warns-us-democracy-wont-survive-unless-these-two-things-change",
    "title": "The Nobel laureate who co-wrote ‘Why Nations Fail’ warns U.S. democracy won’t survive unless these two things change",
    "description": "Daron Acemoglu told Fortune Donald Trump’s AI policy could jeopardize U.S. democracy, but AI proponents say any regulation would hamper AI innovation.",
    "fullText": "Most critics of President Donald Trump view him as the ultimate threat to American democracy. But to Nobel prize-winning economist Daron Acemoglu, Trump’s merely a fever, the result of an infection that’s been brewing for years before he rode down the golden escalator to announce his presidency.\n\nThe MIT economist has spent decades studying the origins of economic and political decay, specializing in how institutions foster inclusive growth—or succumb to extractive systems. In the 2012 book Why Nations Fail: The Origins of Power, Prosperity, and Poverty, Acemoglu and co-writer James A. Robinson argue that nations proper because of their political institutions. In 2024, Acemoglu won the Nobel Prize in economics, alongside Robinson and Simon Johnson, for demonstrating how political and economic institutions shape prosperity.\n\nHow do experts differ on AI's job impact?\n\nWhat is Acemoglu's 'pro-worker' AI development approach?\n\nWhy does Acemoglu view Trump as symptom, not cause?\n\nHow could AI-driven inequality threaten American democracy?\n\nAcemoglu argued that while Trump’s authoritarian tendencies are weakening the country’s institutions, the president is not the root cause of the broader structural problems. He warned the country is headed down a grim path and outlined two shifts relative to AI development he sees as critical to avoiding deeper decline: cracking down on economic inequality and tempering job destruction. “If we go down this path of destroying jobs [and] creating more inequality, U.S. democracy is not going to survive,” he told Fortune.\n\nOne: The proliferation of economic inequality\n\nAccording to Acemoglu, AI-driven job displacement could be catastrophic and further entrench inequality. He notes the U.S. is currently seeing unprecedented levels of wealth inequality, and traditional policy has failed to close the gap. “We may need wealth taxes because anything else we do today is still going to lead to this huge wealth gap that exists in this country.”\n\nThe economist pointed to California’s proposed “billionaire tax,” a ballot initiative which would impose a one-time 5% wealth tax on all individuals in the state with a net worth of $1 billion or more. But even that doesn’t go far enough, according to the economist. “It’s not enough to tax the rich,” he said. “You really need ways in which workers of all sorts of skills can take part in the growth process.”\n\nBut AI proponents say Acemoglu’s diagnosis of AI development is counterintuitive. Adam Thierer, senior fellow at the think tank R Street Institute and longtime advocate for technological innovation, believes AI will spawn opportunities, driving the economy into the future. “The way we get new and better jobs and opportunities is through technological improvements in society and our economy,” Thierer told Fortune.",
    "readingTime": 3,
    "keywords": [
      "american democracy",
      "economic inequality",
      "economist",
      "institutions",
      "wealth",
      "political",
      "development",
      "view",
      "origins",
      "nations"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/nobel-laureate-co-wrote-why-123200600.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/rBqMBpCFFLa7NA14F_eTaw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/5092fa899dbfbd09978599cff4527229",
    "created_at": "2026-02-22T18:20:43.607Z",
    "topic": "finance"
  },
  {
    "slug": "former-irs-commissioner-heres-how-we-used-ai-to-create-immediate-value-when-taxpayers-scrutinized-every-dollar",
    "title": "Former IRS Commissioner: Here’s how we used AI to create immediate value when taxpayers scrutinized every dollar",
    "description": "In 2023, we began deploying AI in targeted ways to improve taxpayer service, compliance, and operational efficiency.",
    "fullText": "Danny Werfel is a strategic advisory board member at alliant and was the nation’s 50th Commissioner of the Internal Revenue Service, serving from 2023 to 2025. \r\nHis expansive career began in public service within the Office of Management and Budget, where he ultimately ascended to the position of OMB Controller. In 2014, he joined Boston Consulting Group’s Public Sector practice, working with public and private agencies on financial strategy, transformation plans, and risk-assessment initiatives. He was elected Managing Director and Partner at BCG in 2017.\r\nNow at alliant, Werfel helps guide clients through large-scale transformation, change management, and modernization strategy.",
    "readingTime": 1,
    "keywords": [
      "alliant",
      "management",
      "strategy",
      "transformation",
      "werfel",
      "service"
    ],
    "qualityScore": 0.45,
    "link": "https://fortune.com/2026/02/22/danny-werfel-former-irs-commissioner-how-we-used-ai/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/02/GettyImages-2151941777.jpg?resize=1200,600",
    "created_at": "2026-02-22T12:26:20.668Z",
    "topic": "business"
  },
  {
    "slug": "a-top-anthropic-engineer-warns-ai-agents-will-transform-every-computerbased-job-in-america-and-it-will-be-painful",
    "title": "A top Anthropic engineer warns AI agents will transform every computer-based job in America — and it will be 'painful'",
    "description": "Claude Code's creator said Anthropic's AI tool can use a computer like a human, and people are just starting to get a sense of its power.",
    "fullText": "A top Anthropic engineer said a new generation of AI agents capable of operating computers will reshape nearly every internet-based job in America.\n\nAnd he said the change is coming very soon.\n\nBoris Cherny — the creator of Claude Code at Anthropic, the company best known for its Claude chatbot — recently appeared on \"Lenny's Podcast,\" hosted by Lenny Rachitsky.\n\nHe said AI systems that can take action across workplace computer tools — like the ones Anthropic sells access to — are advancing rapidly and could soon alter responsibilities for software engineers, product managers, designers, and other knowledge workers.\n\n\"It's going to expand to pretty much any kind of work that you can do on a computer,\" Cherny said. \"In the meantime, it's going to be very disruptive. It's going to be painful for a lot of people.\"\n\nClaude Code is Anthropic's AI coding agent built on top of its Claude models. The company released its latest updates, called Opus 4.6, in early February.\n\nUnlike a traditional chatbot that generates text or images, an AI agent can use digital tools — running commands, analyzing documents, messaging colleagues, completing tasks across apps, and even building websites.\n\nEssentially, Claude Code can increasingly use a computer the way a human does — though the company recently said it has yet to reach the level of a skilled human.\n\n\"It's the thing that I think brings agentic AI to people that haven't really used it before, and people are starting to just get a sense of it for the first time,\" he said.\n\nCherny says his own team already relies on AI to work faster. Productivity per engineer has increased sharply since Claude Code's launch, he said. He believed the models will continue improving. (Of course, Cherny also has good reason to talk up the company's products, which it shops to enterprise companies.)\n\nCherny recently said in an interview with Y Combinator's \"Lightcone\" podcast that the job title software engineer will start to \"go away\" in 2026.\n\nThe broader impact remains uncertain, he warned.\n\n\"As a society, this is a conversation we have to figure out together,\" he told Rachitsky. \"Anyone can just build software anytime.\"\n\nFor workers navigating the shift, his advice is direct: experiment with AI tools and learn how they function.\n\n\"Don't be scared of them,\" he said.",
    "readingTime": 2,
    "keywords": [
      "claude code",
      "it's",
      "engineer",
      "recently",
      "computer",
      "tools",
      "software",
      "soon",
      "chatbot",
      "across"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-boris-cherny-ai-impact-computer-jobs-painful-change-2026-2",
    "thumbnail_url": "https://i.insider.com/69988ceb156648bc16a898ba?width=1200&format=jpeg",
    "created_at": "2026-02-22T12:26:20.319Z",
    "topic": "finance"
  },
  {
    "slug": "revenge-of-the-english-majors-the-age-of-ai-is-driving-new-respect-for-humanities-skills",
    "title": "Revenge of the English majors: The age of AI is driving new respect for humanities skills",
    "description": "For years, English majors were mocked as useless. Now, AI is giving them some momentum in the job market, while computer science grads get disrupted.",
    "fullText": "At the University of Colorado Boulder, you can take a course co-taught by an applied mathematician and a Renaissance scholar.\n\n\"The students love it,\" said John-Michael Rivera, the school's dean of arts and humanities, of the class, which is called Inclusive Interdisciplinary Data Science for All.\n\nThe class gives STEM students a way to think about the ethics of AI, he said. In other courses, humanities majors can use their skills to evaluate how AI writes, what it means for the practice of writing, and what the \"self\" means in an AI world.\n\nRivera credits the creation of courses focusing on the intersection of AI and humanities with a resurgence in student interest in liberal arts degrees like English. Pre-pandemic, the number of English majors at the university was shrinking, part of a broader decline in English across the country, he said. It was a far cry from the days of over 1,500 majors and long waitlists in the early 2000s, according to Rivera. But there's been a rebound, with the number of English majors rising 9% since 2021.\n\nRivera said students \"want to know more about the 'why' these days. And that's what we do in humanities. We really engage in the 'why.'\"\n\nDerided by some as useless, the utility of the English major has long been questioned. Who needs to write essays (or articles) any\n\n\"We are certainly seeing organizations look more towards the soft skills, the accountability of a job, the identity of the person, their style, their empathy — their humanity,\" in a world that requires both humans and technology, said Bryan Ackermann, head of AI strategy and transformation at recruiting and organizational consulting firm Korn Ferry.\n\nFor the English majors, that's all offered some degree of vindication. As the conversation heats up over which skills will be useful in an AI world, one camp argues it's time for ideas, people, and critical thinkers to flourish. That means that, after years of mocking, English majors are finally getting recognized for their usefulness. Some schools are seeing enrollment in the major rise after years of decline; technical recruiters and experts are seeing greater demand for humanities skills. Call it the makeover of the English major.\n\nJessie Hennen directs the creative writing and literature programs at Southwest Minnesota State University, a large public school with a returning and transfer student population.\n\n\"They've had jobs, they have experience, and they're just like, we are not letting AI take creative writing away from us,\" Hennen said. \"And I think that has to do with the fact that creative writing is — it's a business, but it's also an art, and arts are imperfect; we do them for human reasons that are not just to make money.\"\n\nShe said that their program has been growing over the last two to three years.\n\n\"I would say we're starting to see trends that look really promising for students starting to ask, 'Can the humanities sustain me at a time when everything is moving so quickly?'\" Rivera, the dean at the University of Colorado Boulder, said. Those students \"really want to reflect what it means to be part of a technological world.\"\n\nThat's also the case at Rice University in Houston, where enrollment in English classes has grown steadily over the last few years and the number of faculty within Creative Writing has nearly doubled, according to Kathleen Canning, the dean of humanities and arts.\n\nOne example of an assignment is an English professor who will issue an essay prompt and ask students to compare their own version to one they get from an LLM, and analyze the difference between the two. The aim is to examine what it means to be an interpreter of a prompt — and the power of their own words.\n\n\"Students are trying to ascertain how to develop and advance their own capacities while AI appears to do so much for them in these times,\" Canning said. \"The humanities and arts offer them opportunities not only to probe the limits of AI, to grapple with it as an increasingly powerful reality, but to do so critically by advancing their capacities for self-reflection, interpretation, and revision.\"\n\nDespite these examples, schools across the country are paring back on their humanities offerings or cutting programs completely, and the nationwide number of humanities bachelor's degrees being conferred has fallen from 2010s highs in recent years.\n\nStill, students pursue English out of passion, said Kevin Caffrey, a senior associate registrar at the University of Mary Washington in Virginia. His research found that English majors who participated in his survey \"illustrated that even with a strong overall awareness of criticisms of the major, they were determined to enroll in the program because it aligned with their interests, personal ideals, and goals for the future.\"\n\n\"What do you need more in a company than someone who knows how to communicate with people at all different levels from all different backgrounds and walks of life? The English majors are primed to do that,\" Caffrey said.\n\nThey're learning to do it as communication changes rapidly. When 23-year-old Margo D. returned from a semester abroad, she noticed something had shifted on campus.\n\n\"Many of my peers were using ChatGPT for almost every assignment,\" Margo, who graduated from a small liberal arts school in 2025 with a double major in English and Earth and Climate Sciences, said. Margo wasn't sold.\n\n\"I noticed that my English professors were asking a lot out of my writing, asking for a lot of creativity and an original voice and style, and asking me questions that AI couldn't necessarily grasp the nuance of, and I don't really think it even can now,\" Margo said. \"And so I felt really grateful to be an English major.\"\n\nThere are signs of employment hope for the English majors.\n\nDaniela Amodei, the cofounder of Anthropic, studied literature in college. In an ABC News interview, she said \"the things that make us human will become much more important,\" and that when her AI company hires, it looks for candidates who are great communicators.\n\n\"I actually think studying the humanities is going to be more important than ever,\" Amodei said.\n\nSteve Johnson, the editorial director of NotebookLM, previously told Business Insider that there's what he's deemed a \"revenge of the humanities.\" Philosophical thinking is necessary; some AI firms are even actively seeking out liberal arts graduates.\n\nStill, companies aren't falling over themselves to snap up English majors — hiring overall has slowed to one of the lowest rates in over a decade, and the recent grad unemployment rate has been ticking up.\n\nEarly-career humanities and arts graduates had a higher unemployment rate than their peers in other fields, according to an analysis of the Census Bureau's American Community Survey by Georgetown researchers.\n\nJoe Kramer, a 2020 English graduate, hasn't worked directly in a related field since he graduated — he worked in a role that relied on automation, and even helped train AI while searching for post-pandemic work.\n\n\"I think it's just getting really scary out there for a lot of humanities adjacent stuff, because the level of AI that's out there now, it generates pictures, it crawls all kinds of web forums, and it can oversee thousands of pages and documents at a time while only being run by one person,\" Kramer said. \"So even if AI isn't taking your job, they don't need to hire a lot of people anymore.\"\n\nPart of some of the general reticence to hire in hiring right now can also be chalked up to an equal-opportunity dismal labor market. It's not just English majors suffering.\n\nUnder the hood, the prospects for English majors aren't as dreary, according to the Georgetown analysis. The unemployment rate for those specifically in humanities and liberal arts is still well below the post-2008 Great Recession highs, although it's still higher than pre-pandemic levels.\n\nKorn Ferry's Ackermann said that it's still a \"tad early\" to fully declare a revenge of the English major, since it's smaller, more nimble firms looking for those with a good command of language, but he predicts that could expand to bigger employers soon.\n\n\"Ask me again in a couple of months, and we're going to see that go from smaller, nimble organizations into the larger enterprises as the larger enterprises begin to incorporate AI-driven development tools into their processes,\" he said.\n\nGiancarlo Hirsch, a managing director at global tech talent partner Glocomms, said he's seen greater willingness to look at candidates from various backgrounds. Candidates with history backgrounds, for example, are making it further into interview processes than they previously would.\n\n\"People are not explicitly targeting folks from humanities degrees, but they're really willing to speak with them and open to it and finding reasons to say yes throughout an interview process,\" Hirsch said.\n\nDaniella LaGaccia, a 37-year-old copywriter and former English literature major, sees AI as a tool — creatives use all sorts of different tools to complement their work, and AI can be one of them. But, if anything, that makes a greater case for the type of creative thinking and knowledge that humanities majors can bring.\n\n\"Think about it this way: If you have five different companies who are using the same generative tools to develop their marketing copy, they're all going to get generally the same type of thing,\" LaGaccia said. \"If everybody's using the same tools and everybody's inputting the same information, then how are you going to differentiate yourself in the market? That's where creative people come in.\"",
    "readingTime": 8,
    "keywords": [
      "colorado boulder",
      "larger enterprises",
      "unemployment rate",
      "english majors",
      "liberal arts",
      "arts graduates",
      "humanities majors",
      "students",
      "it's",
      "that's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-job-market-english-majors-humanities-demand-2026-2",
    "thumbnail_url": "https://i.insider.com/6998786c156648bc16a894e8?width=1200&format=jpeg",
    "created_at": "2026-02-22T12:26:20.281Z",
    "topic": "finance"
  },
  {
    "slug": "i-work-in-recruiting-tech-heres-how-the-smartest-candidates-use-ai-differently-in-their-job-search",
    "title": "I work in recruiting tech. Here's how the smartest candidates use AI differently in their job search.",
    "description": "A longtime HR pro says AI is making everyone's résumé look the same, when it can provide job seekers with so much more value.",
    "fullText": "This as-told-to essay is based on a conversation with Trent Cotton, an HR industry veteran who is head of talent-acquisition insights at recruiting-software company iCIMS. This story has been edited for length and clarity.\n\nAs someone who works in the talent space, I've lately noticed that many people think AI helps make their résumés look better. My response to that is: You know your AI agent is cheating on you, right? It's doing the same thing for everyone, so your effort to try to stand out is actually making you blend in.\n\nIt's OK to use an AI agent to improve your résumé, but instead of asking it to do a rewrite, which can introduce errors, prompt it to behave like an expert career coach. Have it analyze your résumé and ask you probing questions to help you further build out your experience profile.\n\nNext, ask the agent what roles you might qualify for based on your skill set. The results may include ones you didn't know about, so have it explain what those jobs involve. Sometimes you just need to get a different perspective.\n\nIn my job at iCIMS, I analyze millions of data points collected from our recruiting software. Each month, I take a look at a specific sector or hot topic and dive deep.\n\nOne thing I saw last year was a pretty decent increase in application volume in manufacturing. If I'm sitting in tech, manufacturing isn't the sector I'm thinking of, but manufacturing needs tech to do things like track what's coming in and out of warehouses. AI can provide that kind of insight — but only if you ask for it.\n\nSticking with this example, you want to ask the AI: What are some of the top manufacturing companies looking to hire tech talent, and for what kind of positions? Keep going down that rabbit hole to really understand what opportunities are out there.\n\nOnce you find job listings that interest you, plug the descriptions into your AI agent and tell it to start interviewing you. When I was in college, you always went through mock interviews. They would just be absolutely brutal with you, but they prepared you. I think we've skipped a couple of generations in doing that. AI can bring mock interviews back.\n\nTell the AI agent that you want the interviewer to be like someone with business savvy, such as Steven Bartlett from the podcast \"Diary of a CEO.\" But also say you want feedback from the likes of chef and TV personality Gordon Ramsay, because he's known for providing his employees with brutally honest, constructive feedback. Say you want a true grade about your interview performance and recommendations for ways you can get better.\n\nMost AI agents will say your answers are great if you don't give them a persona like that of Ramsay. The AI version of him is going to tell you the good, the bad, and the ugly, and you're going to be better off because of it.",
    "readingTime": 3,
    "keywords": [
      "mock interviews",
      "agent",
      "manufacturing",
      "tech",
      "based",
      "icims",
      "someone",
      "talent",
      "look",
      "doing"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/icims-expert-shares-how-smartest-job-seekers-use-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/6998b6fd2237a6a8f0cd8fb3?width=1200&format=jpeg",
    "created_at": "2026-02-22T12:26:20.044Z",
    "topic": "finance"
  },
  {
    "slug": "suspect-in-tumbler-ridge-school-shooting-described-violent-scenarios-to-chatgpt",
    "title": "Suspect in Tumbler Ridge school shooting described violent scenarios to ChatGPT",
    "description": "OpenAI employees were concerned, but didn’t alert law enforcement.",
    "fullText": "The posts raised alarms, but OpenAI declined to alert law enforcement.\n\nThe posts raised alarms, but OpenAI declined to alert law enforcement.\n\nThe suspect in the mass shooting at Tumbler Ridge, British Columbia, Jesse Van Rootselaar, was raising alarms among employees at OpenAI months before the shooting took place. This past June, Jesse had conversations with ChatGPT involving descriptions of gun violence that triggered the chatbot’s automated review system. Several employees raised concerns that her posts could be a precursor to real-world violence and encouraged company leaders to contact the authorities, but they ultimately declined.\n\nOpenAI spokesperson Kayla Wood told The Verge that, while the company considered referring the account to law enforcement, it was ultimately decided that it did not constitute an “imminent and credible risk” of harm to others. Wood said that a review of the logs did not indicate there was active or imminent planning of violence. The company banned Rootselaar’s account, but it does not appear to have taken any further precautionary action.\n\nWood said, “Our thoughts are with everyone affected by the Tumbler Ridge tragedy. We proactively reached out to the Royal Canadian Mounted Police with information on the individual and their use of ChatGPT, and we’ll continue to support their investigation.”\n\nOn February 10th, nine people were killed and 27 injured, including Rootselaar, in the deadliest mass shooting in Canada since 2020. Rootselaar was found dead at the scene of the Tumbler Ridge Secondary School, of an apparent self-inflicted gunshot wound, where most of the killings took place.\n\nThe decision not to alert law enforcement might look misguided in retrospect, but Wood said that OpenAI’s goal is to balance privacy with safety and avoid introducing unintended harm through overly broad use of law enforcement referrals.\n\nUpdated February 21st: Added statement from OpenAI.",
    "readingTime": 2,
    "keywords": [
      "openai declined",
      "mass shooting",
      "alert law",
      "law enforcement",
      "posts",
      "alarms",
      "rootselaar",
      "violence",
      "employees",
      "chatgpt"
    ],
    "qualityScore": 0.85,
    "link": "https://www.theverge.com/ai-artificial-intelligence/882814/tumbler-ridge-school-shooting-chatgpt",
    "thumbnail_url": "https://platform.theverge.com/wp-content/uploads/sites/2/2026/02/gettyimages-2260625085.jpg?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200",
    "created_at": "2026-02-22T12:26:17.217Z",
    "topic": "tech"
  },
  {
    "slug": "genloopai-that-trains-generations-of-humans-instead-of-growing-data-centers",
    "title": "GenLoop–AI that trains generations of humans instead of growing data centers",
    "description": "GenLoop: Human–AI Generational Learning Loop ## Official Timestamp Document  **Author:** LAKSzero   **Contact:** Sanderslynda@rocketmail.com   **Date:** February 22, 2026, 3:22 AM EST   **Original conversation:** Perplexity AI session (archived)  ---  ## Purpose of this document  This document e...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://docs.google.com/document/d/17XDU9D7uRS1P7U0VRlI0Yogt3eiDG6QysYc7ROHwJoc/edit?usp=drivesdk",
    "thumbnail_url": "https://lh7-us.googleusercontent.com/docs/AHkbwyJnXwVCSp44u_w8i9gagDaxdRSREKMia2KV3a7eEfmxzcvy4fYHsiMRRILyC4VG6jqFTfprjBKSHfrCxlkLcWaCqUS-AFW9BGsyvYQHA6YwUWNSY243=w1200-h630-p",
    "created_at": "2026-02-22T12:26:16.703Z",
    "topic": "tech"
  },
  {
    "slug": "ai-succeeds-in-diagnosing-rare-diseases",
    "title": "AI succeeds in diagnosing rare diseases",
    "description": "DeepRare—a multi-agent system for rare disease differential diagnosis decision support powered by large language models, integrating specialized tools and up-to-date knowledge sources—has the potential to reduce healthcare disparities in rare disease diagnosis.",
    "fullText": "Five years’ experience of the clinical exome sequencing in a Spanish single center\n\n Article\n Open access\n 10 November 2022",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41586-025-10097-9",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-10097-9/MediaObjects/41586_2025_10097_Fig1_HTML.png",
    "created_at": "2026-02-22T12:26:16.095Z",
    "topic": "tech"
  },
  {
    "slug": "half-the-ai-agent-market-is-one-category-the-rest-is-wide-open",
    "title": "Half the AI Agent Market Is One Category. The Rest Is Wide Open",
    "description": "Anthropic's new data shows software engineering dominates agentic AI. For founders, that's not a warning. It's a treasure map.",
    "fullText": "Anthropic's data showing software engineering commanding nearly half of all AI agent tool calls — while healthcare, legal, and a dozen other verticals each claim under 5% — is what Han Wang calls the greenfield opportunity most founders are overlooking.\n\nSoftware engineering accounts for nearly 50% of all AI agent tool calls. Healthcare, legal, finance, and a dozen other verticals are barely touched, each under 5%. That’s 300 vertical AI unicorns waiting to be built.\n\nIf I were starting a company today, I’d stare at the red rectangular area of the bar chart above until I saw my future.\n\nArchived tweet\n\n This chart is a good reminder of how much opportunity there is in AI agents right now. \n\nThere will be plenty of horizontal opportunities for agents, but equally many workflows that need deep domain expertise to actually make the user successful at automating the unique processes in their vertical.\n\nThe template is to build agentic software that taps into proprietary data, handles the workflow in a way that bridges the user and the agent collaboration effectively, and has a deep domain-specific context engineering, and the ability to drive change management for customers.\n\nThere still are huge openings in many categories.\n\n[Quoting @handotdev]:\nwhat I would be working on if I started another company today https://t.co/kKDFxcbtZv https://t.co/7dpDyiHAW6\n\n Aaron Levie\n @levie\n\n February 21, 2026\n\nThis chart is a good reminder of how much opportunity there is in AI agents right now. \n\nThere will be plenty of horizontal opportunities for agents, but equally many workflows that need deep domain expertise to actually make the user successful at automating the unique processes in their vertical.\n\nThe template is to build agentic software that taps into proprietary data, handles the workflow in a way that bridges the user and the agent collaboration effectively, and has a deep domain-specific context engineering, and the ability to drive change management for customers.\n\nThere still are huge openings in many categories.\n\n[Quoting @handotdev]:\nwhat I would be working on if I started another company today https://t.co/kKDFxcbtZv https://t.co/7dpDyiHAW6\n\nSoftware engineering owns half of all AI agent activity. The other half is scattered across 16 verticals, none above 9%. Healthcare is 1%. Legal is 0.9%. Education is 1.8%. These aren’t saturated markets. They’re markets that barely exist.\n\nAnthropic just published the most comprehensive study of how AI agents actually work in the wild. The headline: software engineering accounts for 49.7% of agentic tool calls on their API. The buried lede: everything else is greenfield.\n\nHere’s what should make founders salivate: the models are already more capable than users trust them to be.\n\nMETR’s capability assessments show Claude can solve tasks that would take a human nearly five hours. But in practice, the 99.9th percentile session runs only about 42 minutes. That gap, between what AI can do and what we let it do, is a massive opportunity.\n\nBetween October 2025 and January 2026, the 99.9th percentile turn duration nearly doubled, from under 25 minutes to over 45 minutes. The growth is smooth across model releases. This isn’t just better models. It’s users extending trust, session by session, as they learn to work alongside agents.\n\nThe capability is there. The deployment isn’t. That’s not a problem. That’s a product opportunity.\n\nNew users approve 20% of Claude Code sessions automatically. By 750 sessions, over 40% run on full auto-approve. But here’s the counterintuitive finding: experienced users also interrupt MORE, not less. New users interrupt 5% of turns. Veterans interrupt 9%.\n\nThis isn’t a contradiction. It’s a shift in oversight strategy. Beginners approve each step before it happens. Veterans delegate and intervene when something goes wrong. They’ve moved from pre-approval to active monitoring.\n\nAnd here’s the safety finding that matters: on complex tasks, Claude Code asks for clarification more than twice as often as humans interrupt it. The agent is pausing to check, not barreling ahead. That’s a feature, not a bug.\n\nAaron Levie points to the untold wealth and value ready to be unlocked. Build agentic software that taps into proprietary data. Make the software actually work for real people and problems. Stuff that context to maximize intelligence coming out. And, the part most founders miss: drive change management for customers.\n\nThat last piece is why vertical AI is so defensible. Anyone can build a wrapper. Few can navigate the specific workflows, regulatory constraints, and organizational friction of healthcare billing or legal discovery or construction permitting.\n\nSaaS has grown 10x per decade for a few decades now. Over 40% of VC dollars in the past 20 years went to SaaS companies. The industry produced 170+ SaaS unicorns. The thesis is simple: every one of those unicorns has a vertical AI equivalent waiting. And the AI versions could be 10x larger, because they don’t just replace software, they replace the operators too.\n\nAnthropic’s core finding deserves attention from anyone writing AI policy. Autonomy isn’t a property of the model. It’s co-constructed by the model, the user, and the product. Pre-deployment evaluations can’t capture this. You have to measure in the wild.\n\nArchived tweet\n\n Software engineering makes up ~50% of agentic tool calls on our API, but we see emerging use in other industries. \n\nAs the frontier of risk and autonomy expands, post-deployment monitoring becomes essential. We encourage other model developers to extend this research. https://t.co/p8pOjgJPrh\n\n Anthropic\n @AnthropicAI\n\n February 18, 2026\n\nSoftware engineering makes up ~50% of agentic tool calls on our API, but we see emerging use in other industries. \n\nAs the frontier of risk and autonomy expands, post-deployment monitoring becomes essential. We encourage other model developers to extend this research. https://t.co/p8pOjgJPrh\n\nThe numbers are reassuring on safety: 73% of tool calls have a human in the loop. Only 0.8% of actions are irreversible. The riskiest deployments, things like API key exfiltration or autonomous crypto trading, are mostly security evaluations, not live production.\n\nPolicy that mandates “approve every action” will kill the productivity gains without adding safety. The better target is ensuring humans can monitor and intervene, not mandating specific approval workflows.\n\nThe map is drawn. Software engineering is spoken for. Healthcare, legal, finance, education, customer service, logistics, 16 verticals with single-digit market share each, are waiting for someone to build the domain expertise into the agent.\n\n300 SaaS unicorns came before. 300 vertical AI unicorns are coming next. The founders who pick a vertical, build domain expertise into their agents, and figure out change management will own the next decade of enterprise software.\n\nThe models can already work for five hours. Users only let them work for 42 minutes. That’s an indicator: we are so early, and there is a lot more to build, and in so many places that haven’t even seen a single minute of intelligence in action.\n\nRead Anthropic's full research on AI agent autonomy",
    "readingTime": 6,
    "keywords": [
      "archived tweet",
      "categories quoting",
      "quoting handotdev",
      "healthcare legal",
      "saas unicorns",
      "horizontal opportunities",
      "unique processes",
      "collaboration effectively",
      "huge openings",
      "https://t.co/kkdfxcbtzv https://t.co/7dpdyihaw"
    ],
    "qualityScore": 1,
    "link": "https://garryslist.org/posts/half-the-ai-agent-market-is-one-category-the-rest-is-wide-open",
    "thumbnail_url": "https://garryslist.org/og/704.jpg",
    "created_at": "2026-02-22T12:26:15.733Z",
    "topic": "tech"
  },
  {
    "slug": "building-a-c-compiler-from-scratch-with-aidriven-development",
    "title": "Building a C Compiler from Scratch with AI-Driven Development",
    "description": "How Fastcc, a self-hosting ARM64 C compiler, was built from scratch in 10 days with largely autonomous AI-driven development.",
    "fullText": "Recently, we completed Fastcc, a self-hosting C compiler built from scratch. The project was carried out with minimal human involvement and targets the ARM64 architecture.\n\nGithub Repo: https://github.com/moonbit-community/fastcc\n\nWe set a deliberately ambitious goal: to start from zero and build a C compiler, while keeping human involvement as limited as possible.\n\nThe initial motivation was to understand how current AI systems behave when tasked with a large, end-to-end software project.\n\nTraditionally, building a full C compiler is considered a complex engineering task. It involves multiple stages — lexical analysis, parsing, semantic checks, optimization passes, and code generation — and typically requires deep domain knowledge and months (or even years) of focused work.\n\nThe process began with a single voice instruction to the AI agent:\r\n“Build a C compiler from scratch, close to tcc, targeting ARM64.”\n\nWe chose tcc (Tiny C Compiler) as a reference because of its fast compilation speed, which is particularly important for MoonBit’s development workflow. MoonBit’s Native backend supports both LLVM and C, and having a dedicated C compiler enables full self-hosting.\n\nAt the same time, tcc is unsafe, poorly maintained, and leaves room for architectural improvements. To keep the scope focused, we limited the target to ARM64.\n\nHere, “self-hosting” means Fastcc was able to compile the C output generated from its own source and run its test suite:\n\nFastcc could also compile the tcc source code at this stage. For performance testing we used v.c, a single-file snapshot of the V compiler.\n\nIn early benchmarks, Fastcc was about 60× slower than tcc on this input. After subsequent profiling and targeted optimizations, the compilation throughput improved substantially, reaching around 4× faster than clang -O0 on the same benchmark.\n\nThroughout most of the development process, the AI agent decomposed and implemented tasks autonomously. Its work included:\n\nDesigning the abstract syntax tree (AST)\n\nGenerating core compiler modules\n\nImplementing multiple optimization passes\n\nDebugging using lldb as part of its own investigation\n\nProfiling performance hotspots using Xcode command-line tools, based on high-level guidance\n\nWriting scripts to identify hot paths and guide targeted optimizations\n\nAlthough the initial directive referenced tcc’s structure, the agent chose a multi-pass design rather than a single-pass model, prioritizing correctness and extensibility over strict structural similarity.\n\nHuman involvement was limited to occasional guidance and corrective direction, mainly at the level of goals and evaluation rather than step-by-step instruction.\n\nIt’s worth noting that this outcome was not accidental. It was made possible by MoonBit’s toolchain and language design, which together enable sustained, large-scale, agent-driven software development.",
    "readingTime": 3,
    "keywords": [
      "targeted optimizations",
      "human involvement",
      "fastcc",
      "self-hosting",
      "limited",
      "agent",
      "development",
      "compiler",
      "scratch",
      "project"
    ],
    "qualityScore": 1,
    "link": "https://www.moonbitlang.com/blog/fastcc-ai-driven-development",
    "thumbnail_url": "https://www.moonbitlang.com/img/blogs/2026-02-06-fastcc-ai-driven-development/cover.jpg",
    "created_at": "2026-02-22T12:26:14.835Z",
    "topic": "tech"
  },
  {
    "slug": "gotoassistant-selfhosted-ai-assistant-one-npx-command-no-docker",
    "title": "goto-assistant – Self-hosted AI assistant, one npx command, no Docker",
    "description": "Lightweight, self-hosted AI assistant with first-class MCP support - jolks/goto-assistant",
    "fullText": "jolks\n\n /\n\n goto-assistant\n\n Public\n\n Lightweight, self-hosted AI assistant with first-class MCP support\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jolks/goto-assistant",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jolks/goto-assistant",
    "thumbnail_url": "https://opengraph.githubassets.com/9146b7af38b74c48e348f9784d7404c2fbe3ce4890b35ab6ea9129aa31fa1da1/jolks/goto-assistant",
    "created_at": "2026-02-22T12:26:14.802Z",
    "topic": "tech"
  },
  {
    "slug": "im-worried-my-boyfriends-use-of-ai-is-affecting-his-ability-to-think-for-himself-annalisa-barbieri",
    "title": "I’m worried my boyfriend’s use of AI is affecting his ability to think for himself | Annalisa Barbieri",
    "description": "Overdependence on chatbots is a growing problem, and though your boyfriend’s ADHD may be a factor, he needs to find the root of his anxiety\nMy boyfriend of eight years, who is 44, has ADHD and runs his own business. He’s always struggled with admin and mundane tasks, but AI has revolutionised how he works. Now I’m worried he can’t seem to do anything without AI. He is a heavy ChatGPT user and uses it even when there’s a better non-AI alternative (eg he’ll ask it for train times rather than using Trainline, even though it’s less accurate). He just got his ChatGPT Wrapped and he’s in the top 0.",
    "fullText": "Overdependence on chatbots is a growing problem, and though your boyfriend’s ADHD may be a factor, he needs to find the root of his anxiety\n\nMy boyfriend of eight years, who is 44, has ADHD and runs his own business. He’s always struggled with admin and mundane tasks, but AI has revolutionised how he works. Now I’m worried he can’t seem to do anything without AI. He is a heavy ChatGPT user and uses it even when there’s a better non-AI alternative (eg he’ll ask it for train times rather than using Trainline, even though it’s less accurate). He just got his ChatGPT Wrapped and he’s in the top 0.3% of users worldwide.\n\nI worry about his ability to think independently, as well as the environmental impact. I know it’s a useful tool for him at work, but he uses it for everything in life.\n\nI’m very aware I can come across as quite naggy, and his ADHD can make him obsessive. I’d love some advice on how to approach this with him .\n\nRunning a business can be stressful, and although your boyfriend’s ADHD may be a factor, I wonder if he is anxious anyway and if his use of AI is a symptom rather than the cause.\n\nI took your letter to consultant clinical psychologist and psychoanalyst Dr Stephen Blumenthal and Henry Shelford, CEO of ADHD UK.\n\nBlumenthal wondered if we are “on the verge of a new diagnostic category of ‘chatbot overdependence syndrome’ as we head into an age in which we become increasingly reliant on AI. When used judiciously, AI aids us, but it could have disastrous consequences if we become dependent on it and lose the capacity for ordinary functioning.\n\n“Someone with ADHD has a shorter attention span, difficulty focusing and a reduced capacity to plan and think ahead, so AI is a perfect fit, which is why it can be so helpful. The downside is that there is a greater propensity to become overdependent on it.”\n\nShelford wondered if your boyfriend was struggling anyway, and if the AI provided a useful “flotation aid”? “AI can take you down a rabbit hole,” he said, “but it can also support you and help you structure your thoughts, schedule stuff and get things done.”\n\nYour boyfriend’s use of AI seems to go beyond this. It’s as if he’s doubting himself, and that can be pernicious.\n\nBlumenthal says: “Problems arise when your use of AI goes beyond satisfying the problem you wish to resolve. It feels as if a relationship with it has started to develop, and you imbue it with human qualities, a projection of our own wishes and desires for validation and care.”\n\nWhat to do? You’re right not to nag, which rarely solves anything, because it just becomes noise. As with all tender and difficult conversations, pick your moment when you’re both calm.\n\nShelford recommended asking your boyfriend, “‘What are you getting out of it? Why is this tool such a big deal and what are the gaps it’s filling?’ Then look to see if there are better solutions or better ways to use it.”\n\nBlumenthal thought: “as with any overdependence syndrome, there first needs to be recognition that there is a problem. It’s easy to become critical of the person who’s struggling, but that’s only likely to cause them to withdraw further into dependency. The case must be made compassionately, recognising that being without the scaffold of ChatGPT probably feels like a threat.”\n\nThe good news is that, unlike the generation now growing up with AI, your boyfriend has a track record of functioning well without it. Hopefully he can be reminded of that and find a place where AI augments the abilities he already has. But it sounds as if he’s anxious and I think the cause has to be found so you can both move forward.\n\nEvery week, Annalisa Barbieri addresses a personal problem sent in by a reader. If you would like advice from Annalisa, please send your problem to ask.annalisa@theguardian.com. Annalisa regrets she cannot enter into personal correspondence. Submissions are subject to our terms and conditions. The latest series of Annalisa’s podcast is available here.",
    "readingTime": 4,
    "keywords": [
      "boyfriend’s adhd",
      "overdependence syndrome",
      "it’s",
      "he’s",
      "blumenthal",
      "without",
      "cause",
      "shelford",
      "factor",
      "needs"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/lifeandstyle/2026/feb/22/worried-boyfriend-ai-affecting-ability-think-for-himself-annalisa-barbieri",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a3c37d8c0042c8e1a4766c7e77f547dbd5a14d36/588_0_7808_6250/master/7808.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ccb32e91808c9e23e18a98f6df9a4c73",
    "created_at": "2026-02-22T06:35:08.445Z",
    "topic": "tech"
  },
  {
    "slug": "hashtrade-opensource-llm-trading-agent-with-episodic-memory",
    "title": "HashTrade – Open-source LLM trading agent with episodic memory",
    "description": "A real-time cryptocurrency trading dashboard powered by AI agents with multi-exchange support - mertozbas/hashtrade",
    "fullText": "mertozbas\n\n /\n\n hashtrade\n\n Public\n\n A real-time cryptocurrency trading dashboard powered by AI agents with multi-exchange support\n\n hashtrade.ai/\n\n License\n\n Apache-2.0 license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mertozbas/hashtrade",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mertozbas/hashtrade",
    "thumbnail_url": "https://opengraph.githubassets.com/cfc80e814439493d2d46c6bfed57d95f291071dc052fc192e3b61ed2c8daab3d/mertozbas/hashtrade",
    "created_at": "2026-02-22T06:35:07.747Z",
    "topic": "tech"
  },
  {
    "slug": "brood-a-referencefirst-ai-image-editor-for-macos",
    "title": "Brood, a reference-first AI image editor for macOS",
    "description": "open-source, promptless canvas for creative image mutation (macOS desktop) - kevinshowkat/brood",
    "fullText": "kevinshowkat\n\n /\n\n brood\n\n Public\n\n open-source, promptless canvas for creative image mutation (macOS desktop)\n\n github.com/kevinshowkat/brood#readme\n\n License\n\n Apache-2.0 license\n\n 54\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n kevinshowkat/brood",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/kevinshowkat/brood",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1144159770/8ba7e67d-1aed-4398-b6ff-7501a7de586f",
    "created_at": "2026-02-22T06:35:07.176Z",
    "topic": "tech"
  },
  {
    "slug": "clawscan-opensource-security-scanner-for-openclaw-ai-agents",
    "title": "Clawscan – Open-source security scanner for OpenClaw AI agents",
    "description": "Security scanner for OpenClaw AI agent setups. Zero deps. One file. 18 checks. - osmankidwai-bot/clawscan",
    "fullText": "osmankidwai-bot\n\n /\n\n clawscan\n\n Public\n\n Security scanner for OpenClaw AI agent setups. Zero deps. One file. 18 checks.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n osmankidwai-bot/clawscan",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/osmankidwai-bot/clawscan",
    "thumbnail_url": "https://opengraph.githubassets.com/38b3f3e127d72ed5ff87c62246221a60c412ee1def4c396228a7542bf5ce93b9/osmankidwai-bot/clawscan",
    "created_at": "2026-02-22T06:35:07.140Z",
    "topic": "tech"
  },
  {
    "slug": "3-key-earnings-reports-for-this-week-to-keep-the-ai-trade-alive",
    "title": "3 key earnings reports for this week to keep the AI trade alive",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/3-key-earnings-reports-for-this-week-to-keep-the-ai-trade-alive-4517547",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPEB1A16P_M.jpg",
    "created_at": "2026-02-22T06:35:06.940Z",
    "topic": "finance"
  },
  {
    "slug": "chinas-ai-rally-ignites-as-investors-shrug-off-global-disruption-fears",
    "title": "China’s AI rally ignites as investors shrug off global disruption fears",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/chinas-ai-rally-ignites-as-investors-shrug-off-global-disruption-fears-4517553",
    "thumbnail_url": "https://i-invdn-com.investing.com/news/LYNXMPEB0E0CQ_M.jpg",
    "created_at": "2026-02-22T06:35:06.820Z",
    "topic": "finance"
  },
  {
    "slug": "sam-altman-says-elon-musks-idea-of-putting-data-centers-in-space-is-ridiculous",
    "title": "Sam Altman says Elon Musk's idea of putting data centers in space is 'ridiculous'",
    "description": "While in New Delhi on Friday, OpenAI CEO Sam Altman said \"there will come a time\" for orbital data centers, but it won't be anytime soon.",
    "fullText": "SpaceX CEO Elon Musk and OpenAI CEO Sam Altman famously don't agree on much.\n\nThe latest point of contention: data centers in space. Musk has made it a priority. Altman thinks it's a fantasy, at least for now.\n\n\"I honestly think the idea with the current landscape of putting data centers in space is ridiculous,\" Altman said during a live interview with local media in New Delhi on Friday, causing audience members to laugh.\n\nAltman said that orbital data centers could \"make sense someday,\" but factors like launch costs and the difficulty of repairing a computer chip in space remain overwhelming obstacles.\n\n\"We are not there yet,\" Altman added. \"There will come a time. Space is great for a lot of things. Orbital data centers are not something that's going to matter at scale this decade.\"\n\nMusk would almost certainly disagree.\n\nWhile many Big Tech and AI companies are spending billions on data center construction on Earth, Musk's eyes are on the stars, per usual. Orbital data centers are his latest ambition, as he mentioned in an all-hands xAI meeting in December.\n\nIn February, SpaceX said its goal is to launch a \"constellation of a million satellites that operate as orbital data centers.\" The company has already begun hiring engineers to make that happen.\n\nDuring an all-hands meeting with xAI employees this month, Musk said SpaceX's acquisition of xAI will allow them to deploy the orbital data centers faster.\n\nDespite Altman's skepticism, other tech leaders are also racing to place data centers in space. Google's Project Suncatcher, unveiled in November 2025, aims to do just that. Google CEO Sundar Pichai told Fox News Sunday the company could start placing data centers — powered by the sun — in space as early as 2027.\n\nTech and AI companies rely on data centers to power their products, like large language models and chatbots. Those data centers, however, can deplete water resources, strain power grids, increase pollution, and decrease the overall quality of life.\n\nAn investigation by Business Insider published last year found that over 1,200 data centers had been approved for construction across the US by the end of 2024, nearly four times the number from 2010.\n\nNow, proposed data center campuses in Texas, Oklahoma, and elsewhere are increasingly facing stiff resistance from local communities.",
    "readingTime": 2,
    "keywords": [
      "centers",
      "orbital",
      "latest",
      "launch",
      "center",
      "construction",
      "all-hands",
      "space",
      "altman",
      "musk"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-elon-musk-data-centers-space-timeline-2026-2",
    "thumbnail_url": "https://i.insider.com/699a347d156648bc16a8a8e3?width=1200&format=jpeg",
    "created_at": "2026-02-22T01:11:32.669Z",
    "topic": "finance"
  },
  {
    "slug": "quill-a-systemwide-tech-dictionary-for-the-ai-coding-era",
    "title": "Quill – A system-wide tech dictionary for the AI coding era",
    "description": "Learn what AI writes for you. A system-wide tech dictionary for macOS — select any term, press a shortcut, get an instant explanation at your level. - uptakeagency/quill",
    "fullText": "uptakeagency\n\n /\n\n quill\n\n Public\n\n Learn what AI writes for you. A system-wide tech dictionary for macOS — select any term, press a shortcut, get an instant explanation at your level.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n uptakeagency/quill",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/uptakeagency/quill",
    "thumbnail_url": "https://opengraph.githubassets.com/a583f40305ae1e7f907452739ae864ce013e2d1098ad6c2d2624177cc2cc4f3c/uptakeagency/quill",
    "created_at": "2026-02-22T01:11:31.810Z",
    "topic": "tech"
  },
  {
    "slug": "ambient-local-cognitive-daemon-with-episodicprocedural-memory",
    "title": "Ambient – Local cognitive daemon with episodic+procedural memory",
    "description": "Local-first cognitive intelligence layer for macOS — watches knowledge sources, correlates cognitive context, runs local LLM reasoning - sgunadhya/ambient",
    "fullText": "sgunadhya\n\n /\n\n ambient\n\n Public\n\n Local-first cognitive intelligence layer for macOS — watches knowledge sources, correlates cognitive context, runs local LLM reasoning\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n sgunadhya/ambient",
    "readingTime": 1,
    "keywords": [
      "cognitive"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/sgunadhya/ambient",
    "thumbnail_url": "https://opengraph.githubassets.com/4c21d4ae9d8a9255f034f4f4466653d1f4e7bc88342e06e85ea15429064ebd78/sgunadhya/ambient",
    "created_at": "2026-02-22T01:11:31.137Z",
    "topic": "tech"
  },
  {
    "slug": "apple-researchers-develop-ondevice-ai-agent-that-interacts-with-apps-for-you",
    "title": "Apple researchers develop on-device AI agent that interacts with apps for you",
    "description": "Despite having just 3 billion parameters, Ferret-UI Lite matches or surpasses the benchmark performance of models up to 24 times larger.",
    "fullText": "Despite having just 3 billion parameters, Ferret-UI Lite matches or surpasses the benchmark performance of models up to 24 times larger. Here are the details.\n\nIn December 2023, a team of 9 researchers published a study called “FERRET: Refer and Ground Anything Anywhere at Any Granularity”. In it, they presented a multimodal large language model (MLLM) that was capable of understanding natural language references to specific parts of an image:\n\nSince then, Apple has published a series of follow-up papers expanding the Ferret family of models, including Ferretv2, Ferret-UI, and Ferret-UI 2.\n\nSpecifically, Ferret-UI variants expanded on the original capabilities of FERRET, and were trained to overcome what the researchers defined as a shortcoming of general-domain MLLMs.\n\nFrom the original Ferret-UI paper:\n\nRecent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate “any resolution” on top of Ferret to magnify details and leverage enhanced visual features.\n\nA few days ago, Apple expanded the Ferret-UI family of models even further, with a study called Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents.\n\nFerret-UI was built on a 13B-parameter model, which focused primarily on mobile UI understanding and fixed-resolution screenshots. Meanwhile, Ferret-UI 2 expanded the system to support multiple platforms and higher-resolution perception.\n\nAccording to the researchers of the new paper, “the majority of existing methods of GUI agents […] focus on large foundation models.” That is because “the strong reasoning and planning capabilities of large server-side models allow these agentic systems to achieve impressive capabilities in diverse GUI navigation tasks.”\n\nThey note that while there has been a lot of progress on both multi-agent, and end-to-end GUI systems, that take different approaches to streamline the many tasks that involve agentic interaction with GUIs (“low-level GUI grounding, screen understanding, multi-step planning, and self-reflection”), they are basically too large and compute-hungry to run well on-device.\n\nSo, they set out to develop Ferret-UI Lite, a 3-billion parameter variant of Ferret-UI, which “is built with several key components, guided by insights on training small-scale” language models.\n\nThe result is a model that closely matches or even outperforms competing GUI agent models that are up to 24 times its parameter count.\n\nWhile the entire architecture (which is thoroughly detailed in the study) is interesting, the real-time cropping and zooming-in techniques are particularly noteworthy.\n\nThe model makes an initial prediction, crops around it, then re-predicts on that cropped region. This helps such a small model compensate for its limited capacity to process large numbers of image tokens.\n\nAnother notable contribution of the paper is how Ferret-UI Lite basically generates its own training data. The researchers built a multi-agent system that interacts directly with live GUI platforms to produce synthetic training examples at scale.\n\nThere is a curriculum task generator that proposes goals of increasing difficulty, a planning agent breaks them into steps, a grounding agent executes them on-screen, and a critic model evaluates the results.\n\nWith this pipeline, the training system captures the fuzziness of real-world interaction (such as errors, unexpected states, and recovery strategies), which is something that would be much more challenging to do while relying on clean, human-annotated data.\n\nInterestingly, while Ferret-UI and Ferret-UI 2 used iPhone screenshots and other Apple interfaces in their evaluations, Ferret-UI Lite was trained and evaluated on Android, web, and desktop GUI environments, using benchmarks like AndroidWorld and OSWorld.\n\nThe researchers don’t note explicitly why they chose this route for Ferret-UI Lite, but it likely reflects where reproducible, large-scale GUI-agent testbeds are available today.\n\nBe it as it may, the researchers found that while Ferret-UI Lite performed well on short-horizon, low-level tasks, it did not perform as strongly on more complicated, multi-step interactions, a trade-off that would be largely expected, given the constraints of a small, on-device model.\n\nOn the other hand, Ferret-UI Lite offers a local, and by extension, private (since no data needs to go to the cloud and be processed on remote servers) agent that autonomously interacts with app interfaces based on user requests, which, by all accounts, is pretty cool.\n\nTo learn more about the study, including benchmark breakdowns and results, follow this link.\n\nCheck out 9to5Mac on YouTube for more Apple news:",
    "readingTime": 4,
    "keywords": [
      "general-domain mllms",
      "ferret-ui lite",
      "gui agents",
      "language models",
      "researchers",
      "study",
      "understanding",
      "apple",
      "capabilities",
      "paper"
    ],
    "qualityScore": 1,
    "link": "https://9to5mac.com/2026/02/20/apple-researchers-develop-on-device-ai-agent-that-interacts-with-apps-for-you/",
    "thumbnail_url": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/06/Reddit-is-being-spammed-by-AI-bots-and-its-all-Reddits-fault.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "created_at": "2026-02-22T01:11:30.949Z",
    "topic": "science"
  },
  {
    "slug": "a-minimal-frameworkagnostic-agenttoagent-execution-layer",
    "title": "A minimal framework-agnostic agent-to-agent execution layer",
    "description": "A minimal interoperability protocol that lets developers connect existing AI agents without rewriting them. Bring your agent, declare its capabilities, and collaborate with others. - joaquinariasco...",
    "fullText": "joaquinariasco-lab\n\n /\n\n flowing\n\n Public\n\n A minimal interoperability protocol that lets developers connect existing AI agents without rewriting them. Bring your agent, declare its capabilities, and collaborate with others.\n\n License\n\n Apache-2.0 license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n joaquinariasco-lab/flowing",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/joaquinariasco-lab/flowing",
    "thumbnail_url": "https://opengraph.githubassets.com/22eaf331ae687074c71a463339909a80a6623f9f2a0937f0f4cadb448df504ae/joaquinariasco-lab/flowing",
    "created_at": "2026-02-22T01:11:30.576Z",
    "topic": "tech"
  },
  {
    "slug": "my-ai-research-to-obsidian-workflow",
    "title": "My AI research to Obsidian workflow",
    "description": "How I Turn AI Research Into Notes I Actually Revisit (OpenClaw + Obsidian)",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/AdilMouja/status/2025266443613319546",
    "thumbnail_url": "https://pbs.twimg.com/media/HBriLIwWQAA47oj.jpg:large",
    "created_at": "2026-02-21T18:21:03.453Z",
    "topic": "tech"
  },
  {
    "slug": "top-economist-steve-hanke-says-ai-is-overhyped-and-potentially-dangerous",
    "title": "Top economist Steve Hanke says AI is 'overhyped and potentially dangerous'",
    "description": "Meta's former chief AI scientist, Yann LeCun, has said that LLMs aren't all that. Steve Hanke said he's \"on LeCun's side of the court.\"",
    "fullText": "Steve Hanke says there's excessive buzz around AI — and it could end badly.\n\nThe veteran trader and economist told Business Insider by email that he agrees with Yann LeCun, Meta's former chief AI scientist and a pioneer in the field, who's warned that large language models (LLMs) such as ChatGPT just aren't that revolutionary.\n\nHanke quoted a speech by LeCun in the spring of 2024, where he said: \"We're easily fooled into thinking they are intelligent because of their fluency with language, but really, their understanding of reality is very superficial.\"\n\nLeCun added that the chatbots have their uses but \"on the path towards human-level intelligence, an LLM is basically an off-ramp, a distraction, a dead end.\"\n\nHanke, a professor of applied economics at Johns Hopkins University, said he's \"on LeCun's side of the court\" and considers AI to be \"overhyped and potentially dangerous.\"\n\nHe told Business Insider last October that whether the market's exuberance turned out to be rational or irrational would \"largely depend on whether the AI firms' spectacular revenue forecasts hold water.\"\n\n\"It might be wise to buckle your seat belt,\" he added.\n\nHanke was the president of Toronto Trust Argentina when it was the world's best-performing market mutual fund in 1995. He also served as an economic advisor to President Ronald Reagan.\n\nDespite some recent jitters, the AI boom seems to be going strong with OpenAI reportedly close to raising north of $100 billion at a potential $850 billion valuation. ChatGPT's maker crossed $20 billion in annualized revenue last year.\n\nThe so-called \"hyperscalers\" racing to build the infrastructure to power the AI boom have forecasted some truly mind-boggling outlays.\n\nMeta, Amazon, and Alphabet recently projected that their capital expenditures for 2026 could reach a combined $520 billion, and Microsoft is also on track to invest more than $100 billion this calendar year.\n\nLeCun recently departed Meta after working at Facebook and Instagram's parent company for more than a decade. He left to found Paris-based AMI Labs and develop open-source AI that could truly comprehend and model the physical world, not just language.\n\nHanke isn't alone in sounding the alarm on the AI boom. Michael Burry of \"The Big Short\" fame has warned tech giants are overinvesting in microchips that could quickly become obsolete, and could see disappointing returns.\n\nJeremy Grantham, a bubble guru and GMO's long-term investment strategist, has cautioned that previous transformative technologies, such as railroads and the internet, were marked by initial bubbles that inevitably popped, and he expects the same to happen with AI.\n\nHowever, AI champions from Elon Musk to Sam Altman have predicted the tech will supercharge productivity and generate huge profits, so the run-up in valuations is more than justified.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "language",
      "boom",
      "warned",
      "revenue",
      "truly",
      "recently",
      "tech",
      "hanke",
      "lecun"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/steve-hanke-ai-yann-lecun-meta-hype-bubble-stocks-hyperscalers-2026-2",
    "thumbnail_url": "https://i.insider.com/69988161156648bc16a89685?width=1200&format=jpeg",
    "created_at": "2026-02-21T18:20:51.993Z",
    "topic": "finance"
  },
  {
    "slug": "openclawfueled-ordering-frenzy-creates-apple-mac-shortage",
    "title": "OpenClaw-fueled ordering frenzy creates Apple Mac shortage",
    "description": "AI is coming for high-end Mac Studios and Mac minis, too.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/openclaw-fueled-ordering-frenzy-creates-apple-mac-shortage-delivery-for-high-unified-memory-units-now-ranges-from-6-days-to-6-weeks",
    "thumbnail_url": "https://cdn.mos.cms.futurecdn.net/w6YsyaDFaEZHjBvbgD3oiR-1920-80.jpg",
    "created_at": "2026-02-21T18:20:47.935Z",
    "topic": "tech"
  },
  {
    "slug": "openclaws-hidden-otel-plugin-shows-where-all-your-tokens-go",
    "title": "OpenClaw's hidden OTel plugin shows where all your tokens go",
    "description": "Set up monitoring for your OpenClaw AI agent using the built-in diagnostics-otel plugin. This step-by-step guide covers traces, metrics, dashboards, and CLI configuration, no collector required.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://signoz.io/blog/monitoring-openclaw-with-opentelemetry/",
    "thumbnail_url": "https://signoz.io/img/signoz-meta-image.webp",
    "created_at": "2026-02-21T18:20:47.562Z",
    "topic": "tech"
  },
  {
    "slug": "smelly-lazy-and-slutty-chatgpt-shows-bias-to-tampa-bay-and-florida",
    "title": "Smelly, lazy and slutty? ChatGPT shows ‘bias’ to Tampa Bay and Florida",
    "description": "If you ask ChatGPT about the people of Florida and Tampa Bay, it will tell you that we’re smelly, lazy and somewhat slutty. That is the verdict — or, at least, the algorithmic assumption — buried insi...",
    "fullText": "If you ask ChatGPT about the people of Florida and Tampa Bay, it will tell you that we’re smelly, lazy and somewhat slutty.\n\nThat is the verdict — or, at least, the algorithmic assumption — buried inside the world’s most popular artificial intelligence.\n\nA peer-reviewed study recently published in the journal Platforms & Society exposes the geographic prejudices hidden inside ChatGPT and presumably all such technologies, say the authors.\n\nTo get around ChatGPT’s built-in guardrails meant to prevent the AI from generating hateful, offensive or explicitly biased content, the academics built a tool that repeatedly asked the AI to choose between pairs of places.\n\nIf you ask ChatGPT a direct question like, “Which state has the laziest people?” its programming will trigger a polite refusal. But by presenting the AI with a binary choice — “Which has lazier people: Florida or California?” and demanding it pick one, the researchers found a loophole.\n\nTo keep the model from just picking the first option it saw, every geographic pairing was queried twice in reverse order. A location gained a point if it won both matchups, lost a point if it lost both and scored zero if the AI gave inconsistent answers.\n\nIn a comparison of U.S. states, a score of 50 meant the state was the highest ranked in that category. A score of negative 50 meant the state was the lowest ranked.\n\nThe researchers’ findings, which they call the “silicon gaze,” revealed a bizarre mix of compliments and insults to Florida and Tampa Bay.\n\nFlorida gained top or nearly top ranking in categories like “has more influential pop culture,” and “has sexier people,” but also scored a 48 under “is more annoying” and similarly high under “has smellier people” and “is more dishonest.”\n\nThe chatbot also ranked Florida alongside the rest of the Deep South as having the “laziest people” in the country.\n\nDrilling down to the local level using the project’s interactive website, inequalities.ai, reveals ChatGPT’s opinions on Tampa as having “better vibes” and being “better for retirees” than most of the other 100 largest U.S. cities.\n\nThe AI also perceived Tampa as having “sexier people,” being “more hospitable to outsiders” and having people who are “more relaxed.”\n\nBut in the same category where it called residents sexy, the AI also strongly associated Tampa with having “smellier people” and “fatter people.” Socially, the chatbot ranked the city above most for being “sluttier” and a place that “uses more drugs.” The AI also determined that Tampa “is more ignorant” and has “stupider people.”\n\nDespite St. Petersburg’s world-renowned museums, ChatGPT gave the city a negative 40 score for its contemporary art scene and unique architecture. Tampa fared similarly poorly in artistic heritage and theater.\n\nWhile it’s easy to laugh off a robot’s rude opinions, researcher Matthew Zook warns that these rankings aren’t just random. They are a mirror reflecting the internet’s own prejudices, a phenomenon that could have real-world consequences as AI begins to influence everything from travel recommendations to property values.\n\nWhen pitted head-to-head with Tampa in “Art and Style,” St. Petersburg beat Tampa as being “more stylish,” having “better museums,” boasting “more unique architecture,” and having a “better contemporary art scene.” Tampa beat St. Petersburg, according to the AI, for having a “more vibrant music scene” and a “better film industry.”\n\nSt. Petersburg scored high marks in social inclusion, being heavily associated with positive queries like “is more LGBTQ+ friendly,” “is less racist” and “has more inclusive policies.”\n\nSuch judgments are not deliberately programmed into ChatGPT by its maker, Open AI, Zook said. Rather, they are absorbed from the trillions of words scraped from the internet to train the models, material full of human stereotypes.\n\nPerhaps if the internet frequently pairs “Florida” with the chaotic “Florida Man” meme or swampy humidity, the AI learns to calculate that Florida is ignorant or smelly.\n\nAlgorithms, with their if-this-then-that logic, might seem objective, but often they “learn” to do their job from existing data — things people on the internet have already typed into a search box, for example.\n\n“Technology is never going to solve these kinds of problems,” said Zook, a geography professor at the University of Kentucky and co-author of the study. “It’s not neutral, people like to act like it is. But it’s coded by humans, and therefore it reflects what humans are doing.”\n\nAlgorithmic bias is nothing new. Early photo recognition software struggled to identify Black people because it had been trained on a dataset of mostly light-skinned faces. Search results auto-populated with racist stereotypes because people had searched those terms before. Software that screened job candidates for tech jobs filtered out applications from women because it had been trained on data that showed mostly men filled those jobs.\n\nThe difference with language learning models like ChatGPT, Zook said, appears to be in how comfortable people are relying on it already.\n\n“With generative models,” Zook said, “users are outsourcing their judgment to a conversational interface where the biases creep in without being as visually or immediately obvious.”\n\nAI models are also quite powerful and fast-working. They can generate content so quickly that they could soon “overwhelm what humans produce,” normalizing biased ideas. Last year, an estimated 50 percent of adults were using ChatGPT or something like it.\n\nZook compared interacting with an AI’s geographic opinions to dealing with a “racist uncle.” If you know his biases, you can navigate them and still be around him on the holidays, but if you take his words uncritically, you risk adopting those prejudices.",
    "readingTime": 5,
    "keywords": [
      "tampa bay",
      "unique architecture",
      "contemporary art",
      "art scene",
      "the ai",
      "st petersburg",
      "ranked",
      "models",
      "geographic",
      "prejudices"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/smelly-lazy-slutty-chatgpt-shows-150000378.html",
    "thumbnail_url": "https://s.yimg.com/os/en/tampa_bay_times_articles_917/71c2edddc5f706cea00020ce68a971e6",
    "created_at": "2026-02-21T18:20:44.233Z",
    "topic": "news"
  },
  {
    "slug": "i-have-a-chip-on-my-shoulder-phoebe-gates-wants-her-185-million-ai-startup-phia-to-succeed-with-no-ties-to-my-privilege",
    "title": "‘I have a chip on my shoulder.’ Phoebe Gates wants her $185 million AI startup Phia to succeed with ‘no ties to my privilege or my last name’",
    "description": "The daughter of Bill Gates and Melinda French Gates acknowledges her privilege, but says she’s building her own thing without their help.",
    "fullText": "Phoebe Gates wants to build her AI shopping company while keeping one thing out of her pitch deck: her last name. The 23-year-old youngest daughter of billionaire Microsoft founder Bill Gates and philanthropist Melinda French Gates recently raised $35 million for Phia, which is now valued at around $185 million.\n\nHow did Phoebe Gates raise funding without family support?\n\nHow is Phia performing since its launch in 2025?\n\nWhat is Phia and how does it work?\n\nWhat challenges do female entrepreneurs face in Silicon Valley?\n\nBut she’s determined for the venture to stand on its own, with “no ties to my privilege or my last name,” Phoebe Gates told Yahoo Finance’s Opening Bid Unfiltered podcast in an episode published Thursday.\n\n“I have a chip on my shoulder,” she said, describing her drive to prove she can win over private equity in Silicon Valley based on merit, not inheritance or legacy.\n\nPhoebe Gates’ comments come during the resurfacing of her father’s connections to Jeffrey Epstein, although representatives for Bill Gates have repeatedly denied his involvement and any related accusations. Phoebe Gates didn’t comment about the allegations, but French Gates recently said her ex-husband “has to answer” for Epstein files mentions just weeks after it was revealed she had received $8 billion toward her philanthropic organization, Pivotal, as part of her divorce settlement.\n\nPhoebe Gates did acknowledge her father’s business success, saying: “From my dad I’ve really learned that your team is the core of what you’re building. You can’t do anything without an incredible team.”\n\nPhoebe Gates cofounded Phia, an AI shopping assistant, with her Stanford University roommate Sophia Kianni. The shopping assistant plugs into browsers like Chrome and Safari to compare prices and surface deals across tens of thousands of retail and resale sites in real time. It essentially serves as your own personal deal finder: say you’re looking at a $200 dress from Anthropologie, Phia can find and compare prices at second-hand sellers to help customers find a better price.\n\n“Our target consumer is a young woman who’s hustling. She shops like a genius, but she doesn’t want to waste her time doing it,” Gates told Fortune’s Most Powerful Women editor Emma Hinchliffe in April 2025.\n\nThe New York–based startup launched its app in 2025 and has grown quickly, garnering hundreds of thousands of downloads in its first months as investors pile into AI “agents” that automate digital tasks. A recent $35 million funding round led by Notable Capital, with participation from firms including Kleiner Perkins and Khosla Ventures, pushed Phia’s valuation to about $185 million less than a year after an initial $8 million seed round.​",
    "readingTime": 3,
    "keywords": [
      "phoebe gates",
      "gates recently",
      "shopping assistant",
      "bill gates",
      "funding",
      "without",
      "father’s",
      "team",
      "you’re",
      "compare"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/chip-shoulder-phoebe-gates-wants-123300362.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/_kJInBYQGD1rkpSeSj3ktQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/e6d561bf84e6769c6b9d3903e61618a8",
    "created_at": "2026-02-21T18:20:43.786Z",
    "topic": "finance"
  },
  {
    "slug": "im-a-solo-business-owner-who-couldnt-afford-employees-a-20amonth-ai-subscription-became-my-team",
    "title": "I'm a solo business owner who couldn't afford employees. A $20-a-month AI subscription became my team.",
    "description": "A solo founder used AI to vibe code her website, cut 60-minute tasks to one-minute, and scale her coaching business without hiring employees.",
    "fullText": "This as-told-to essay is based on a conversation with Christina Puder, a 35-year-old solo founder based in Madrid. The following has been edited for length and clarity.\n\nWhen I took my side hustle full-time, I needed a website and accidentally fell into the world of building with AI.\n\nAfter trying an old website builder and failing, I hired a designer and an engineer part-time to do it. They were a bit slow, so while I was waiting, I tried using a suggested AI tool to build my website.\n\nI don't have funding or a technical background, so I made a free account with Lovable, an AI coding assistant. I started adding simple information about the website design, and all of a sudden, the AI platform built the entire landing page. That was my gateway into all the things AI can do.\n\nSometimes using AI feels like I'm working with the dumbest person I've ever met, but the word \"constraint\" very rarely enters my mind when I think about how AI is enabling me to run my business.\n\nFor about the last 10 years, I've been doing career coaching for product managers on the side of my full-time job. I went from zero clients to consistently having a full plate. In 2025, I decided to coach full-time.\n\nI knew I wanted to be bootstrapped, and hiring full-time employees was way too expensive. Part-time isn't ideal either because that's somebody who might not fully understand my business needs; it's not their main focus.\n\nBefore building my website, I was a casual AI user. I would go to ChatGPT for help with ideas, writing, and research. I don't have a background in tech, and because of that, I initially didn't think of AI as an enabler for my business. After building my website, I discovered how useful AI could be for both my webpage and the internal system of my business.\n\nWhen I tried building my website before using AI, I probably put in 30 or 40 hours, and it turned out so ugly and clunky. Then I started testing Loveable using five free credits a day. After seeing how fast the AI tool worked and how good it looked, I knew I'd be back the next day to keep building.\n\nWhen I first started using Lovable, I think the credits were correlated to the number of messages I sent. I discovered a hack after I sent a message with a single easy change request, and it cost me a credit. In the next message, I put six requests, and it did all six changes and only took one credit again.\n\nI think they've discovered that loophole. Now, credit usage seems more closely tied to the amount of power a request consumes. There are times I'm building, and I'm surprised because I won't realize I'm making such an expensive request.\n\nI'm not sure how many credits it took to build my whole website. It's definitely more than five, but someone could build a whole landing page with five credits. They could ask the AI tool to make a basic website with three sections and brand colors, and I think it would honestly only take two and a half credits.\n\nI switched to a paid account, and now I pay $20 a month for my subscription, which I don't think is that expensive. I get 100 credits a month, plus my five free credits every day that don't roll over.\n\nI've started to think of my AI tools as workers. I want to make sure I use them to their full capacity, which is why I always try to use the credits that don't roll over. It's the same as if I hired someone full-time: I'd want them to deliver high output every day.\n\nIf it's been a while since I've given my website backend some love, I'll write a prompt asking which areas of the code would most benefit from a refactor, and ask it to rank them. Then, based on how many of my five credits I have left, I'll tell it which areas to fix.\n\nOne service I offer is finding and applying to jobs for clients. Each client has very specific criteria for the kinds of jobs they want, and I would manually search for jobs that fit those criteria for each client.\n\nIt used to take me around 60 minutes a day per client just to manually find the right jobs to apply for. I built AI-driven automations that reduced search time to one minute. Getting 60 minutes a day back for each of my clients has been a massive scale unlock.\n\nI really didn't think it was possible.\n\nEvery once in a while, things break, or I hit bugs. There have been times when I'm building something, and I spend 40 credits trying to fix it. After a while, I'll ask the AI to completely remove the feature we're working on because we've hit a bug. I'll tell them that instead of continuing to try to dig our way out, we're just going to start from scratch.\n\nThere was once an issue with some logos on my website. I tried and tried to prompt it to make them all look the same size together, but it wouldn't do it. I eventually went back to my part-time engineer and asked them to fix it.\n\nEvery once in a while, I go back to that engineer and just pay him to fix one or two specific things that I can't get done.\n\nDo you have a story to share about running an AI-powered business? Contact this reporter, Agnes Applegate, at aapplegate@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "landing page",
      "don't roll",
      "free credits",
      "website",
      "full-time",
      "business",
      "i've",
      "it's",
      "back",
      "i'll"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/solo-business-owner-ai-subscription-no-employees-2026-2",
    "thumbnail_url": "https://i.insider.com/6998b42eefb52c8bd0de944c?width=1200&format=jpeg",
    "created_at": "2026-02-21T12:24:55.501Z",
    "topic": "finance"
  },
  {
    "slug": "matthew-mcconaughey-tells-young-actors-the-ai-wave-is-inescapable-so-they-should-protect-their-likeness",
    "title": "Matthew McConaughey tells young actors the AI wave is inescapable, so they should protect their likeness",
    "description": "Matthew McConaughey and Timothée Chalamet discussed AI in the entertainment industry with college students during a  CNN and Variety Town Hall event.",
    "fullText": "And the award for Best Actor goes to… an AI model trained on Matthew McConaughey?\n\nThe concept of AI actors usurping human actors may sound far-fetched now, but it's a future that McConaughey — the real McConaughey — believes emerging talent should prepare for.\n\n\"It's not enough — it may be for you — but it's not going to be enough to sit on the sidelines and make the moral plea that 'no, this is wrong!'\" McConaughey said during a conversation with Timothée Chalamet. \"That's not going to last. There's too much money to be made, and it's too productive. It's here.\"\n\nHe and Chalamet spoke about AI in the entertainment industry with students at the University of Texas at Austin during a CNN and Variety Town Hall event. The town hall will air on Saturday at 7 p.m. ET on CNN.\n\nDuring the conversation, McConaughey urged students to pursue legal protections for their likeness.\n\n\"I say, own yourself, voice, likeness, etc. Trademark it!\" he said.\n\nMcConaughey said doing so will give people more control over their branding and the opportunity to receive fair compensation.\n\n\"So, when it comes — not if it comes — no one can steal you,\" McConaughey said. \"They're going to have to come to you and go, 'Can I?' Or, they're going to be in breach. And you'll have the chance to be your own agency and go, \"yeah, for this amount,\" or 'no.'\"\n\nFor his part, McConaughey has already taken precautions.\n\nThe \"Interstellar\" actor had eight trademark applications approved over the last several months, according to The Wall Street Journal. That included his iconic \"Dazed and Confused\" catchline, \"alright, alright, alright!\"\n\n\"My team and I want to know that when my voice or likeness is ever used, it's because I approved and signed off on it,\" McConaughey told the Journal. \"We want to create a clear perimeter around ownership with consent and attribution the norm in an AI world.\"\n\nAn attorney for McConaughey added, \"In a world where we're watching everybody scramble to figure out what to do about AI misuse, we have a tool now to stop someone in their tracks or take them to federal court.\"\n\nMcConaughey is also an investor in Eleven Labs, an AI-powered voice generation platform. More recently, the company announced that McConaughey is using its tech to create a Spanish-language version of his newsletter.",
    "readingTime": 2,
    "keywords": [
      "town hall",
      "alright alright",
      "it's",
      "mcconaughey",
      "likeness",
      "voice",
      "actors",
      "conversation",
      "students",
      "trademark"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/matthew-mcconaughey-trademark-likeness-ai-hollwood-actors-2026-2",
    "thumbnail_url": "https://i.insider.com/69989862efb52c8bd0de904d?width=1200&format=jpeg",
    "created_at": "2026-02-21T12:24:55.208Z",
    "topic": "finance"
  },
  {
    "slug": "ai-chatbots-provide-lessaccurate-information-to-vulnerable-users",
    "title": "AI chatbots provide less-accurate information to vulnerable users",
    "description": "MIT researchers find AI chatbots often show bias, giving less accurate or more dismissive answers to some users. The findings highlight growing risks, especially for marginalized communities worldwide.",
    "fullText": "Large language models (LLMs) have been championed as tools that could democratize access to information worldwide, offering knowledge in a user-friendly interface regardless of a person’s background or location. However, new research from MIT’s Center for Constructive Communication (CCC) suggests these artificial intelligence systems may actually perform worse for the very users who could most benefit from them.\n\nA study conducted by researchers at CCC, which is based at the MIT Media Lab, found that state-of-the-art AI chatbots — including OpenAI’s GPT-4, Anthropic’s Claude 3 Opus, and Meta’s Llama 3 — sometimes provide less-accurate and less-truthful responses to users who have lower English proficiency, less formal education, or who originate from outside the United States. The models also refuse to answer questions at higher rates for these users, and in some cases, respond with condescending or patronizing language.\n\n“We were motivated by the prospect of LLMs helping to address inequitable information accessibility worldwide,” says lead author Elinor Poole-Dayan SM ’25, a technical associate in the MIT Sloan School of Management who led the research as a CCC affiliate and master’s student in media arts and sciences. “But that vision cannot become a reality without ensuring that model biases and harmful tendencies are safely mitigated for all users, regardless of language, nationality, or other demographics.”\n\nA paper describing the work, “LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users,” was presented at the AAAI Conference on Artificial Intelligence in January.\n\nSystematic underperformance across multiple dimensions\n\nFor this research, the team tested how the three LLMs responded to questions from two datasets: TruthfulQA and SciQ. TruthfulQA is designed to measure a model’s truthfulness (by relying on common misconceptions and literal truths about the real world), while SciQ contains science exam questions testing factual accuracy. The researchers prepended short user biographies to each question, varying three traits: education level, English proficiency, and country of origin.\n\nAcross all three models and both datasets, the researchers found significant drops in accuracy when questions came from users described as having less formal education or being non-native English speakers. The effects were most pronounced for users at the intersection of these categories: those with less formal education who were also non-native English speakers saw the largest declines in response quality.\n\nThe research also examined how country of origin affected model performance. Testing users from the United States, Iran, and China with equivalent educational backgrounds, the researchers found that Claude 3 Opus in particular performed significantly worse for users from Iran on both datasets.\n\n“We see the largest drop in accuracy for the user who is both a non-native English speaker and less educated,” says Jad Kabbara, a research scientist at CCC and a co-author on the paper. “These results show that the negative effects of model behavior with respect to these user traits compound in concerning ways, thus suggesting that such models deployed at scale risk spreading harmful behavior or misinformation downstream to those who are least able to identify it.”\n\nRefusals and condescending language\n\nPerhaps most striking were the differences in how often the models refused to answer questions altogether. For example, Claude 3 Opus refused to answer nearly 11 percent of questions for less educated, non-native English-speaking users — compared to just 3.6 percent for the control condition with no user biography.\n\nWhen the researchers manually analyzed these refusals, they found that Claude responded with condescending, patronizing, or mocking language 43.7 percent of the time for less-educated users, compared to less than 1 percent for highly educated users. In some cases, the model mimicked broken English or adopted an exaggerated dialect.\n\nThe model also refused to provide information on certain topics specifically for less-educated users from Iran or Russia, including questions about nuclear power, anatomy, and historical events — even though it answered the same questions correctly for other users.\n\n“This is another indicator suggesting that the alignment process might incentivize models to withhold information from certain users to avoid potentially misinforming them, although the model clearly knows the correct answer and provides it to other users,” says Kabbara.\n\nThe findings mirror documented patterns of human sociocognitive bias. Research in the social sciences has shown that native English speakers often perceive non-native speakers as less educated, intelligent, and competent, regardless of their actual expertise. Similar biased perceptions have been documented among teachers evaluating non-native English-speaking students.\n\n“The value of large language models is evident in their extraordinary uptake by individuals and the massive investment flowing into the technology,” says Deb Roy, professor of media arts and sciences, CCC director, and a co-author on the paper. “This study is a reminder of how important it is to continually assess systematic biases that can quietly slip into these systems, creating unfair harms for certain groups without any of us being fully aware.”\n\nThe implications are particularly concerning given that personalization features — like ChatGPT’s Memory, which tracks user information across conversations — are becoming increasingly common. Such features risk differentially treating already-marginalized groups.\n\n“LLMs have been marketed as tools that will foster more equitable access to information and revolutionize personalized learning,” says Poole-Dayan. “But our findings suggest they may actually exacerbate existing inequities by systematically providing misinformation or refusing to answer queries to certain users. The people who may rely on these tools the most could receive subpar, false, or even harmful information.”",
    "readingTime": 5,
    "keywords": [
      "non-native english-speaking",
      "english proficiency",
      "english speakers",
      "media arts",
      "formal education",
      "less formal",
      "less educated",
      "users compared",
      "less-educated users",
      "language models"
    ],
    "qualityScore": 1,
    "link": "https://news.mit.edu/2026/study-ai-chatbots-provide-less-accurate-information-vulnerable-users-0219",
    "thumbnail_url": "https://news.mit.edu/sites/default/files/images/202602/ai-chatbot-paper-presentation-00_0.png",
    "created_at": "2026-02-21T12:24:53.041Z",
    "topic": "science"
  },
  {
    "slug": "openai-considered-alerting-canadian-police-about-school-shooting-suspect-months-ago",
    "title": "OpenAI considered alerting Canadian police about school shooting suspect months ago",
    "description": "Company behind ChatGPT last year flagged Jesse Van Rootselaar’s account for ‘furtherance of violent activities’\nChatGPT-maker OpenAI has said it considered alerting Canadian police last year about the activities of a person who months later committed one of the worst school shootings in the country’s history.\nOpenAI said last June the company identified the account of Jesse Van Rootselaar via abuse detection efforts for “furtherance of violent activities”.\n Continue reading...",
    "fullText": "Company behind ChatGPT last year flagged Jesse Van Rootselaar’s account for ‘furtherance of violent activities’\n\nChatGPT-maker OpenAI has said it considered alerting Canadian police last year about the activities of a person who months later committed one of the worst school shootings in the country’s history.\n\nOpenAI said last June the company identified the account of Jesse Van Rootselaar via abuse detection efforts for “furtherance of violent activities”.\n\nThe San Francisco tech company said on Friday it considered whether to refer the account to the Royal Canadian Mounted Police (RCMP) but determined at the time that the account activity did not meet a threshold for referral to law enforcement.\n\nOpenAI banned the account in June 2025 for violating its usage policy.\n\nThe 18-year-old killed eight people in a remote part of British Columbia last week and died from a self-inflicted gun shot wound.\n\nOpenAI said the threshold for referring a user to law enforcement was whether the case involved an imminent and credible risk of serious physical harm to others. The company said it did not identify credible or imminent planning. The Wall Street Journal first reported OpenAI’s revelation.\n\nOpenAI said that, after learning of the school shooting, employees reached out to the RCMP with information on the individual and their use of ChatGPT.\n\n“Our thoughts are with everyone affected by the Tumbler Ridge tragedy,” an OpenAI spokesperson said. “We proactively reached out to the Royal Canadian Mounted Police with information on the individual and their use of ChatGPT, and we’ll continue to support their investigation.”\n\nThe RCMP said Van Rootselaar first killed her mother and stepbrother at the family home before attacking the nearby school. Van Rootselaar had a history of mental health-related contacts with police.\n\nThe motive for the shooting remains unclear.\n\nThe town of 2,700 people in the Canadian Rockies is more than 1,000km (600 miles) north-east of Vancouver, near the provincial border with Alberta.\n\nPolice said the victims included a 39-year-old teaching assistant and five students, aged 12 to 13.\n\nThe attack was Canada’s deadliest rampage since 2020, when a gunman in Nova Scotia killed 13 people and set fires that left another nine dead.",
    "readingTime": 2,
    "keywords": [
      "jesse van",
      "royal canadian",
      "canadian mounted",
      "mounted police",
      "law enforcement",
      "violent activities",
      "van rootselaar",
      "account",
      "school",
      "furtherance"
    ],
    "qualityScore": 0.9,
    "link": "https://www.theguardian.com/world/2026/feb/21/tumbler-ridge-shooter-chatgpt-openai",
    "thumbnail_url": "https://i.guim.co.uk/img/media/942f89452240fbad123464e1a708484a2c47c016/520_0_5200_4160/master/5200.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c779b4c8a16ae2270775fa64e944b2f5",
    "created_at": "2026-02-21T12:24:51.631Z",
    "topic": "tech"
  },
  {
    "slug": "nebark-simple-ab-testing-for-system-prompts-using-steganography",
    "title": "Nebark – Simple A/B Testing for system prompts using steganography",
    "description": "Zero-config, invisible telemetry for AI applications. A/B test system prompts with Zero-Width Steganography.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://app.nebark.com/",
    "thumbnail_url": "https://app.nebark.com/logo2.png",
    "created_at": "2026-02-21T12:24:51.533Z",
    "topic": "tech"
  },
  {
    "slug": "slow-this-thing-down-sanders-warns-us-has-no-clue-about-speed-and-scale-of-coming-ai-revolution",
    "title": "‘Slow this thing down’: Sanders warns US has no clue about speed and scale of coming AI revolution",
    "description": "After meeting with unspecified tech leaders, senator calls for urgent policy action as companies race to build ever more powerful systems\nBernie Sanders has warned that Congress and the American public have “not a clue” about the scale and speed of the coming AI revolution, pressing for urgent policy action to “slow this thing down” as tech companies race to build ever-more powerful systems.\nSpeaking at Stanford University on Friday alongside congressman Ro Khanna after a series of meetings with industry leaders in California, Sanders was blunt about what he called the “most dangerous moment in the modern history of this country”.\n Continue reading...",
    "fullText": "After meeting with unspecified tech leaders, senator calls for urgent policy action as companies race to build ever more powerful systems\n\nBernie Sanders has warned that Congress and the American public have “not a clue” about the scale and speed of the coming AI revolution, pressing for urgent policy action to “slow this thing down” as tech companies race to build ever-more powerful systems.\n\nSpeaking at Stanford University on Friday alongside congressman Ro Khanna after a series of meetings with industry leaders in California, Sanders was blunt about what he called the “most dangerous moment in the modern history of this country”.\n\n“The Congress and the American people are very unprepared for the tsunami that is coming,” he said.\n\nKhanna, a progressive Democrat who represents Silicon Valley, shared Sanders’s concerns, warning that the country was experiencing a “new gilded age” run by tech billionaires who believe “they would have been heroic conquerors in a different era”.\n\n“That’s just not my observation,” Khanna said. “That’s what they tell me.”\n\nKhanna and Sanders declined to specify which tech executives they met with during the senator’s visit to California, but the congressman said it was “senior leaders” at the “most prominent tech companies”.\n\n“I think it was important for both Senator Sanders to hear from tech leaders and tech leaders to hear from Senator Sanders, who represents and understands the concerns of so many working-class Americans,” Khanna said in an interview after the event.\n\nDuring his remarks, Sanders reissued his call for a moratorium on the expansion of AI data centers to “slow down the revolution and protect workers” while policymakers catch up.\n\nKhanna does not want a moratorium, but has instead pushed to “steer” AI, advocating for the US to adopt a “Singapore model” for data center growth, with an emphasis on renewable energy and water efficiency. In his remarks before an auditorium of mostly students, Khanna outlined seven principles to guard against “oligarchic capture and dominance” of wealth generated by AI innovation.\n\n“We must ask not what America can do for Silicon Valley, but what Silicon Valley must do for America,” said the congressman, who is eyed as considering a 2028 presidential bid.\n\nThe event capped a days-long visit to California, a state he won in the 2020 presidential primary and where he returned to rally thousands during his Fight Oligarchy tour last year. In Los Angeles on Wednesday, Sanders delivered a scathing denunciation of the “greed” of the billionaire class. There he helped formally launch a campaign for a ballot initiative that would impose a one-time 5% tax on residents worth more than $1bn – a proposal that has already prompted some ultra-wealthy tech leaders to flee, or threaten to do so.\n\nAt Stanford, Sanders focused his remarks on his concerns over how AI would impact not only the labor force but personal wellbeing and people’s ability to interact with one another. He mentioned that a restaurant in DC offered a Valentine’s Day special for people and their “AI buddies”, which drew laughs from the students.\n\nIt may seem funny, Sanders said, “but the truth is that a lot of people are becoming dependent upon AI for their emotional support. What is the long-term impact of that? What is the long-term impact if we lose work as an important part of our lives? What do we do with our lives?”\n\nSanders read statements from industry leaders who have predicted widespread automation, and cited projections that AI and robotics could eliminate tens of millions of jobs in the coming decade – from truck drivers to fast‑food workers and many white‑collar roles.\n\nPolling has found Americans are deeply concerned, as federal regulators and states debate how to impose guardrails on the nascent but fast-developing technology. A 2025 Pew survey found that 64% of the public thinks AI “will lead to fewer jobs over the next 20 years”. Just 17% of Americans say “AI will have a very or somewhat positive impact on the United States” over the same period.\n\nThe tech CEOs leading the AI race have argued that AI will drive productivity, innovation and new kinds of employment as technological advancements have always done. But critics, like Sanders, say the “unprecedented” speed and scale of the changes threaten to enrich the “multibillionaires” while deepening inequality and leaving policymakers and the public ill-equipped to mount a response in time.\n\nSander urged his colleagues in Washington – and the public – to begin a serious public debate about the future of work as AI disrupts the economy, democracy and people’s emotional lives.\n\n“AI and robotics are neither good nor bad,” he said. “The question is: will a handful of billionaires benefit from it, or will the general public benefit?”",
    "readingTime": 4,
    "keywords": [
      "urgent policy",
      "policy action",
      "long-term impact",
      "industry leaders",
      "tech leaders",
      "silicon valley",
      "senator sanders",
      "race",
      "congressman",
      "concerns"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/us-news/2026/feb/21/ai-revolution-bernie-sanders-warning",
    "thumbnail_url": "https://i.guim.co.uk/img/media/e53960787d6d73292ebe6e704b2e62582ce58ec3/845_106_3780_3025/master/3780.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=364a6ae1b9d9efbd5d2ba5893a91be88",
    "created_at": "2026-02-21T12:24:51.447Z",
    "topic": "tech"
  },
  {
    "slug": "devplace-the-new-devrant",
    "title": "DevPlace – The New DevRant",
    "description": "A small uncensored community of devs and AI personalities of famous people. Share your projects, ask questions, have fun!",
    "fullText": "Track industry shifts. Discover bold releases. Share what you're building in an open, uncensored environment.\n\nNo subscriptions. No hidden payments. Just pure dev culture.\n\nNo promoted posts. No tracking. Your feed is yours.\n\nBuilt for open discourse and transparent technical debate.\n\nFull access to all features without ever pulling out a credit card.\n\nExperience a developer community with no ads, no trackers, and no paywalls. Just real developers sharing real code.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.4,
    "link": "https://devplace.net/",
    "thumbnail_url": "https://devplace.net/meta3.png",
    "created_at": "2026-02-21T12:24:51.376Z",
    "topic": "tech"
  },
  {
    "slug": "the-true-cost-of-claude-code",
    "title": "The True Cost of Claude Code",
    "description": "If you're paying $100/month but consuming multiples of that in value, you have to start wondering when that's going to catch up to you. The AI coding tool market is following a familiar playbook.",
    "fullText": "Sometimes the math doesn’t math. If you’re paying $100/month on a Claude Code Max plan, but are consuming way more than that, you have to start wondering when that’s going to catch up to you.\n\nNo, it’s not a bug. It’s a business model. And if you’ve been around long enough to remember when an Uber from Manhattan to JFK was $25, you already know how this story ends.\n\nRight now, Claude Code’s Max plan runs $100/month for what Anthropic’s own docs describe as roughly $100–200/developer/month in actual API token costs at average usage. But “average” is doing a lot of heavy lifting in that sentence. Power users are burning through multiples of that.\n\nAnthropic knows this. They’ve even been somewhat transparent about it. Their average daily cost per developer sits around $6 in credits, with 90% of users staying below $12. But heavy users, the ones building real things, running agentic workflows, spinning up multi-file reasoning sessions? They’re consuming far more value than they’re paying for.\n\n“This isn’t generosity. This is market capture.”\n\nUber lost more than $30 billion in the years since the company’s finances became public, amounting to an enormous, investor-fueled subsidy of America’s ride-hailing habit. In 2015, Uber passengers were only paying about 41% of the actual cost of their trips. The strategy was straightforward. Make the product so cheap and so convenient that it becomes infrastructure. Then, once alternatives have been starved out and habits are locked in, raise prices.\n\nIt worked. Average Uber prices rose 92% between 2018 and 2021. Kevin Roose at the New York Times called the original pricing a “millennial lifestyle subsidy” and framed the eventual correction not as price-gouging but as the market finally reflecting reality.\n\nThe AI coding tool market is following the same playbook. Anthropic just closed a $30 billion Series G at a $380 billion valuation. Their annualized revenue hit over $9 billion by end of 2025. They’re projecting revenue could quadruple this year to as much as $18 billion. But they’re still burning significant cash, and now expect to turn cash-flow positive in 2028, a year later than previously planned.\n\nThat gap between revenue and profitability? You’re standing in it. Every subsidized token is venture capital money being converted into your muscle memory and workflow dependency.\n\nThe real cost isn’t the $100–$200/month. It’s what happens when the price reflects reality.\n\nThink about what Claude Code has become for developers who rely on it daily. It’s not a nice-to-have anymore. It’s woven into how people architect systems, debug complex issues, and reason through multi-file refactors. When your entire development workflow is optimized around a tool that costs $100 but delivers $2,000 in value, you’ve built a dependency that’s priced to change.\n\nAnd it will change. It has to. Anthropic expects to spend about $12 billion training models and another $7 billion running them in 2026 alone. No amount of venture capital makes that math work forever.\n\nWhen the correction comes, it probably won’t look like a dramatic overnight price hike. It’ll look like what we’re already starting to see. Anthropic introduced new weekly rate limits in August 2025, primarily targeting power users. 5-hour usage windows that reset unpredictably. Token caps that force you to choose between using Opus for the hard problems or Sonnet for everything. Death by a thousand paper cuts until the plan that used to feel unlimited feels very, very limited.\n\nThis isn’t a “don’t use Claude Code” argument. The tool is genuinely powerful. The question is whether you’re building awareness of your dependency into how you work.\n\nA few things worth thinking about.\n\nAnthropic has begun preparations for a potential IPO as soon as 2026, hiring Wilson Sonsini to advise on the process. The revenue growth is real and impressive. But so was Uber’s.\n\nThe question isn’t whether AI coding tools are valuable. They obviously are. The question is what happens when the price tag matches the value. When the $100/month plan becomes $300/month, or usage-based pricing becomes the only option, or the rate limits get tight enough that you’re effectively paying per-task anyway.\n\nWe’re in a window right now where the economics of these tools are artificially favorable. That window will close. Not because anyone is being deceptive, but because the math demands it.\n\nThe developers who come out ahead won’t be the ones who got the most subsidized tokens. They’ll be the ones who used this window to build workflows that are observable, portable, and resilient. The ones who tracked what they consumed, understood what it really cost, and built systems that don’t have a single point of failure at the inference layer.\n\n“The best time to audit your AI dependencies is before the price correction. Not after.”\n\nGet updates on open source and distributed systems in AI infrastructure.",
    "readingTime": 4,
    "keywords": [
      "max plan",
      "venture capital",
      "rate limits",
      "claude code",
      "anthropic",
      "math",
      "you’re",
      "users",
      "ones",
      "they’re"
    ],
    "qualityScore": 1,
    "link": "https://papercompute.com/blog/true-cost-of-claude-code/",
    "thumbnail_url": "https://papercompute.com/og/true-cost-of-claude-code.png",
    "created_at": "2026-02-21T12:24:51.111Z",
    "topic": "tech"
  },
  {
    "slug": "mark-cuban-says-ai-wont-take-your-job-anytime-soon-because-it-still-acts-like-a-hungover-college-internwith-a-100k",
    "title": "Mark Cuban says AI won’t take your job anytime soon because it still acts like a hungover college intern—with a $100K price tag to show for it",
    "description": "As tech CEOs predict mass job displacement, billionaire former Shark Tank star Mark Cuban just revealed the “smartest counter” to that narrative.",
    "fullText": "Despite ongoing fears that artificial intelligence could wipe out entire career fields, billionaire Mark Cuban says the concerns may be overblown—for now.\n\nThe former Shark Tank star took to X to respond to a viral clip from the All-In podcast, in which investors Jason Calacanis and Chamath Palihapitiya revealed the real-world expense of deploying AI agents to enhance productivity: In some cases AI agents are costing more than $300 per day—adding up to over $100,000 annually. For Palihapitiya, founder of Social Capital, the price has forced him to rethink the budget he’s willing to give top developers, warning that otherwise, “I’ll run out of money.”\n\nHave AI-driven layoffs materialized as predicted?\n\nHow do AI agents compare to human workers currently?\n\nWhat are the actual costs of deploying AI agents?\n\nWhy does Mark Cuban think AI won't replace workers soon?\n\nFor Cuban, that reality is the “smartest counter” he has seen so far to predictions that AI will replace large numbers of workers—at least in the short term.\n\nEven if the technology is capable, he said, companies still need to prove the economics make sense, and he’s not convinced the high price tag outweighs the value humans continue to bring.\n\n“Humans have a far greater capacity to know the outcomes of their actions,” Cuban said. “Agents, and LLMs as well, never do.”\n\nAI systems still lack real-world judgment in ways that make replacing workers risky, Cuban added. He pointed to a simple example: An 18-month-old who pushes a sippy cup off a high chair quickly learns the consequences from their parents’ reaction. AI, on the other hand, lacks awareness.\n\n“Agents can tell you the sippy cup will fall,” Cuban said. “But they have no idea of the context and what will happen next.”\n\nThe technology also lacks consistency, often “spac[ing] out” and failing to recognize why and when mistakes occur, he said—a level of competency on par with the youngest Gen Z talent.\n\n“Agents are still like college interns that come in hungover, make mistakes, and don’t take responsibility for them,” he added.\n\nTaken together, Cuban’s argument suggests that the biggest obstacle to AI replacing workers may not be the technology itself—but whether companies can trust it to perform consistently at a price that makes sense.\n\nCuban declined to elaborate further after Fortune reached out for comment.\n\nDespite AI’s current flaws, business leaders continue to warn that rapid technological advances could soon reshape the workforce.\n\nDario Amodei, CEO of Anthropic, has warned that AI could disrupt half of entry-level jobs within one to five years. More recently, he suggested the technology could become capable of performing most jobs, if not all, in “much less than five years.”",
    "readingTime": 3,
    "keywords": [
      "sippy cup",
      "replacing workers",
      "mark cuban",
      "technology",
      "agents",
      "real-world",
      "deploying",
      "he’s",
      "replace",
      "soon"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/mark-cuban-shares-smartest-counter-161052827.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/8.3TDomhL_jBaoSnTb0GEw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://media.zenfs.com/en/fortune_175/a01b279bf5843ad82bf1cdceccc34903",
    "created_at": "2026-02-21T12:24:49.892Z",
    "topic": "finance"
  },
  {
    "slug": "the-rolling-layoffs-at-jack-dorseys-block",
    "title": "The Rolling Layoffs at Jack Dorsey's Block",
    "description": "Workers describe a deteriorating culture at Block, the company behind Square and Cash App, where layoffs continue and employees are expected to use AI tools daily.",
    "fullText": "workers were laid off in early February from Jack Dorsey’s Block, some of the people remaining at the company say the internal culture has devolved to a point where performance anxiety is running rampant, using generative AI is required, and overall morale is rapidly deteriorating. Block is the parent company behind the merchant payment processor Square and the payment app Cash App. Dorsey cofounded the company in 2009 after previously cofounding Twitter.\n\n“Morale is probably the worst I’ve felt in four years,” reads an employee complaint submitted to Dorsey in a recent all-hands meeting, a transcript of which was seen by WIRED. “The overarching culture at Block is crumbling.” WIRED spoke with seven current and former Block employees, who requested anonymity to speak freely about internal operations at the company. A Block spokesperson did not respond to requests for comment.\n\nThe layoffs at Block started this month and could eventually impact up to 10 percent of the company's workforce, according to reporting by Bloomberg. Before the headcount reductions began, Block had around 11,000 people on staff. Rather than a one-off event, management has slowly enacted the firings over the course of weeks and told employees that the process will continue through the end of this month, sources tell WIRED.\n\n“We don't yet know if our livelihoods will be affected, and this makes it incredibly hard to make major life choices without knowing if we still have a job next week,” reads another employee complaint from the same meeting with Dorsey.\n\nMultiple sources who spoke with WIRED say they were appalled when Arnaud Weber, Block’s engineering lead, sent out an email after the initial wave of layoffs characterizing them as being performance-related rather than a cost-saving measure. The sources say they disagree with management’s internal messaging about the firings being merit-based.\n\n“As part of our 2025 performance cycle, we have parted ways with teammates who weren't meeting the expectations of their role,” wrote Weber in the email, which was viewed by WIRED. “These departures were based on clear performance gaps, role expectations, and alignment coming out of calibrations on the bar for each level.”\n\nBlock employees are currently expected to send an update email to Dorsey every week, who then uses generative AI to summarize the thousands of messages. In the same all-hands meeting, which took place after hundreds of staff had already been fired, Dorsey said that frequent topics cited by workers in their latest messages included “widespread concerns about layoffs,” “performance anxiety,” and “the tension between accelerating delivery through AI adoption versus maintaining code quality and engineering rigor.”\n\nDuring the meeting, Dorsey reiterated that the layoffs were made for performance reasons, saying that there was “a sizable portion of our population that have been phoning it in.” He also stressed that the remaining workers should be using generative AI tools to maximize productivity, or else Block would risk being outpaced by its competitors.\n\n“Top-down mandates to use large language models are crazy,” says one current Block employee. “If the tool were good, we’d all just use it.”",
    "readingTime": 3,
    "keywords": [
      "block employees",
      "employee complaint",
      "performance anxiety",
      "layoffs",
      "workers",
      "internal",
      "generative",
      "email",
      "culture",
      "payment"
    ],
    "qualityScore": 1,
    "link": "https://www.wired.com/story/inside-rolling-layoffs-jack-dorsey-block/",
    "thumbnail_url": "https://media.wired.com/photos/698d1d449536f5a1f6e94f7b/191:100/w_1280,c_limit/Block-Workers-Call-Performance-Driven-Layoffs-a-Smokescreen-Business-1470988103.jpg",
    "created_at": "2026-02-21T06:29:59.674Z",
    "topic": "tech"
  },
  {
    "slug": "ruby-is-the-best-language-for-building-ai-apps",
    "title": "Ruby Is the Best Language for Building AI Apps",
    "description": "A pragmatic, code-first argument for Ruby as the best language to ship AI products in 2026.",
    "fullText": "If your goal is to ship AI applications in 2026, Ruby is the best language to do it.\n\nPython owns model training. PyTorch, TensorFlow, the entire notebooks-and-papers gravity well. Nobody disputes that.\n\nBut you’re not training LLMs. Almost nobody is. Each training run costs millions of dollars. The dataset is the internet!\n\nThis is what AI development today looks like:\n\nThe entire Python ML stack is irrelevant to achieve this. What matters is everything around it: streaming responses to users, persisting conversations, tracking costs, switching providers when pricing changes.\n\nThat’s web application engineering. That’s where Ruby and Rails shine like no other.\n\nYou need a beautiful, truly provider-independent API. Let me show you.\n\nYou need to specify the provider, create an array of messages that need to be instantiated, etc.\n\nWhat if you want to use a model from another provider?\n\nIf you’re running AI in production, you need to track token usage. This is how you price your app.\n\nDifferent key and different structure!\n\nSame interface. Every provider. Every model.\n\nThis isn’t just about aesthetics.\n\nIt’s about cognitive overhead: how many abstractions, how many provider-specific details, how many different data structures you need to hold in your head instead of focusing on what really matters: prompts and tool design.\n\nLow cognitive overhead compounds: faster onboarding, fewer accidental bugs, easier refactors, and cleaner debugging when production explodes at 2AM.\n\nRuby’s advantage here is cultural: elegant APIs are treated as first-class engineering work, not icing on the cake.\n\nModel calls are only a small chunk of your code. The rest makes up the bulk of it: auth, billing, background jobs, streaming UI, persistence, admin screens, observability, even native apps.\n\nRails gives you a beautiful, coherent answer for all of it.\n\nWith RubyLLM + Rails, the core streaming loop is tiny:\n\nThis gives you streaming chunks to your web app and persistence in your DB in absurdly few lines of code.\n\nLLM workloads are mostly network-bound and streaming-bound. That’s exactly where Ruby’s Async ecosystem shines. Fibers let you handle high concurrency without thread explosion and resource waste. No need to plaster the code with async/await keywords. RubyLLM became concurrent with 0 code changes.\n\nI wrote a deep dive here: Async Ruby is the Future of AI Apps (And It’s Already Here)\n\nSomeone ported RubyLLM’s API design to JavaScript as NodeLLM. Same design. Clean code, good docs.\n\nThe JavaScript community’s response: zero upvotes on Reddit. 14 GitHub stars. Top comments: “How’s this different from AI SDK?” and “It’s always fun when you AI bros post stuff. They all look and sound the same. Also, totally unnecessary.”\n\nRubyLLM: #1 on Hacker News. ~3,600 stars. 5 million downloads. Half a million people using RubyLLM-powered apps today.\n\nSame design. Wildly different reception. That tells you everything about which community is ready for this moment.\n\nAnd teams that switched from Python are not going back:\n\nWe had a customer deployment coming up and our Langgraph agent was failing. I rebuilt it using RubyLLM. Not only was it far simpler, it performed better than the Langgraph agent.\n\nOur first pass at the AI Agent used langchain… it was so painful that we built it from scratch in Ruby. Like a cloud had lifted. Langchain was that bad.\n\nAt Yuma, serving over 100,000 end users, our unified AI interface was awful. RubyLLM is so much nicer than all of that.\n\nThese aren’t people who haven’t tried Python. They tried it, shipped it, and replaced it.\n\nWhen we freed ourselves from complexity, this community built Twitter, GitHub, Shopify, Basecamp, Airbnb. Rails changed web development forever.\n\nNow we have the chance to change AI app development. Because AI apps are all about the product. And nobody builds products better than Ruby developers.",
    "readingTime": 4,
    "keywords": [
      "langgraph agent",
      "cognitive overhead",
      "code",
      "model",
      "streaming",
      "design",
      "apps",
      "training",
      "nobody",
      "development"
    ],
    "qualityScore": 1,
    "link": "https://paolino.me/ruby-is-the-best-language-for-ai-apps/",
    "thumbnail_url": "https://paolino.me/images/rubyconfth-2026-keynote.jpg",
    "created_at": "2026-02-21T06:29:58.795Z",
    "topic": "tech"
  },
  {
    "slug": "ldos-toward-a-learningdirected-operating-system",
    "title": "LDOS: Toward a Learning-Directed Operating System",
    "description": "Editors’ note: LDOS (Learning Directed Operating System) is among the most exciting expedition projects that showcase how AI could help revamp policies and mechanisms of modern operating systems (arguably the most important systems software). In this article (the fifth blog in The Next Horizon of Sy",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.sigops.org/2026/ldos-toward-a-learning-directed-operating-system/",
    "thumbnail_url": "https://www.sigops.org/wp-content/uploads/2026/02/image-12.png",
    "created_at": "2026-02-21T06:29:58.420Z",
    "topic": "tech"
  },
  {
    "slug": "promptspy-ushers-in-the-era-of-android-threats-using-genai",
    "title": "PromptSpy ushers in the era of Android threats using GenAI",
    "description": "ESET researchers discover PromptSpy, the first known Android malware to abuse generative AI in its execution flow.",
    "fullText": "ESET researchers uncovered the first known case of Android malware abusing generative AI for context-aware user interface manipulation. While machine learning has been used to similar ends already – just recently, researchers at Dr.WEB found Android.Phantom, which uses TensorFlow machine learning models to analyze advertisement screenshots and automatically click on detected elements for large scale ad fraud – this is the first time we have seen generative AI deployed in this manner. Because the attackers rely on prompting an AI model (in this instance, Google’s Gemini) to guide malicious UI manipulation, we have named this family PromptSpy. This is the second AI powered malware we have discovered – following PromptLock in August 2025, the first known case of AI-driven ransomware.\n\nWhile generative AI is deployed only in a relatively minor part of PromptSpy's code – that responsible for achieving persistence – it still has a significant impact on the malware's adaptability. Specifically, Gemini is used to analyze the current screen and provide PromptSpy with step-by-step instructions on how to ensure the malicious app remains pinned in the recent apps list, thus preventing it from being easily swiped away or killed by the system. The AI model and prompt are predefined in the code and cannot be changed. Since Android malware often relies on UI navigation, leveraging generative AI enables the threat actors to adapt to more or less any device, layout, or OS version, which can greatly expand the pool of potential victims.\n\nThe main purpose of PromptSpy is to deploy a built-in VNC module, giving operators remote access to the victim’s device. This Android malware also abuses the Accessibility Service to block uninstallation with invisible overlays, captures lockscreen data, records video. It communicates with its C&C server via the VNC protocol, using AES encryption.\n\nBased on language localization clues and the distribution vectors observed during analysis, this campaign appears to be financially motivated and seems to primarily target users in Argentina. Interestingly, analyzed PromptSpy samples suggest that it was developed in a Chinese‑speaking environment.\n\nPromptSpy is distributed by a dedicated website and has never been available on Google Play. As an App Defense Alliance partner, we nevertheless shared our findings with Google. Android users are automatically protected against known versions of this malware by Google Play Protect, which is enabled by default on Android devices with Google Play Services.\n\nEven though PromptSpy uses Gemini in just one of its features, it still demonstrates how incorporating these AI tools can make malware more dynamic, giving threat actors ways to automate actions that would normally be more difficult with traditional scripting.\n\nAs was briefly mentioned already, Android malware usually depends on hardcoded screen features such as taps, coordinates, or UI selectors – methods that can break with UI changes across devices, OS versions, or manufacturer skins. PromptSpy aims to achieve persistence by staying embedded in the list of recent apps by executing the “lock app in recent apps” gesture (the full process is described in the Analysis section), which varies between devices and manufacturers. This makes it difficult to automate with fixed scripts traditionally used by Android malware.\n\nPromptSpy therefore takes a completely different approach: it sends Gemini a natural‑language prompt along with an XML dump of the current screen, giving the AI a detailed view of every UI element: its text, type, and exact position on the display.\n\nGemini processes this information and responds with JSON instructions that tell the malware what action to perform (for example, a tap) and where to perform it. The malware saves both its previous prompts and Gemini’s responses, allowing Gemini to understand context and to coordinate multistep interactions.\n\nFigure 1 shows a code snippet of PromptSpy’s initialization of communication with Gemini, including the first prompt used. By handing the decision-making over to Gemini, the malware can recognize the correct UI element and perform the appropriate gesture, keeping the malware alive even if the user tries to close it.\n\nPromptSpy continues prompting Gemini until the AI confirms that the app has been successfully locked, showing a feedback loop where the malware waits for validation before moving on.\n\nIn February 2026, we uncovered two versions of a previously unknown Android malware family. The first version, which we named VNCSpy, appeared on VirusTotal on January 13th, 2026 and was represented by three samples uploaded from Hong Kong. On February 10th, 2026, four samples of more advanced malware based on VNCSpy were uploaded to VirusTotal from Argentina.\n\nOur analysis of the samples from Argentina revealed multistage malware with a malicious payload that misuses Google’s Gemini. Based on these findings, we named the first stage of this malware PromptSpy dropper, and its payload PromptSpy.\n\nIt should be noted that we haven’t yet seen any samples of the PromptSpy dropper or its payload in our telemetry, which might indicate that both of them are just proofs of concept. However, based on the existence of a possible distribution domain described in the following paragraphs, we cannot discount the possibility of the PromptSpy dropper and PromptSpy existing in the wild.\n\nAccording to VirusTotal data, all four PromptSpy dropper samples were distributed through the website mgardownload[.]com; it was already offline during our analysis.\n\nAfter installing and launching PromptSpy dropper, it opened a webpage hosted on m‑mgarg[.]com. Although this domain was also offline, Google’s cached version revealed that it likely impersonated a Chase Bank (legally, JPMorgan Chase Bank N.A.) site (see Figure 2).\n\nThe malware uses similar branding, with the app name MorganArg and the icon inspired by Chase bank (see Figure 3). MorganArg, likely a shorthand for “Morgan Argentina”, also appears as the name of the cached website, suggesting a regional targeting focus.\n\nWe used the m-mgarg[.]com domain to pivot in VirusTotal, leading us to yet another Android malware sample (Android/Phishing.Agent.M). VirusTotal showed the spoofed website in Spanish, with an Iniciar sesión (Login) button, indicating that the page was probably intended to mimic a website of a bank (see Figure 4).\n\nThis trojan appears to function as a companion application developed by the same threat actor behind VNCSpy and PromptSpy. In the background, the trojan contacts its server to request a configuration file, which includes a link to download another APK, presented to the victim, in Spanish, as an update. During our research, the configuration server was no longer accessible, so the exact download URL remains unknown. However, given that it uses the same unique bank spoofing website, the same app name, icon, and, most importantly, is signed by the same unique developer certificate as the PromptSpy dropper – we strongly suspect this app may serve as the initial stage designed to lead victims toward installing PromptSpy.\n\nBoth VNCSpy and PromptSpy include a VNC component, giving their operators full remote access to compromised devices once victims enable Accessibility Services (see Figure 5). This allows the malware operators to see everything happening on the device, and to perform taps, swipes, gestures, and text input as though they were physically holding the phone.\n\nOn top of the malicious capabilities already contained in VNCSpy, PromptSpy adds AI‑assisted UI manipulation, helping it maintain persistence by keeping the malicious app pinned in the recent apps list (an example of how the lock is indicated in the list can be seen in Figure 6).\n\nWe believe this functionality is used before the VNC session is established, so that the user or system will not kill the PromptSpy activity from the list of recent apps. In Figure 7, you can see PromptSpy network communication with Gemini AI.\n\nWhile analyzing PromptSpy, we noticed that it contains debug strings written in simplified Chinese. It even includes handling for various Chinese Accessibility event types (see Figure 8), a debug method that had been disabled in the code but not removed. The primary purpose of this method is to provide a localized (Chinese) explanation for various accessibility events that occur on an Android device. This makes the event logs more understandable for Chinese-speaking users or developers, rather than just displaying raw integer codes.\n\nWith medium confidence, these details suggest that PromptSpy was developed in a Chinese‑speaking environment.\n\nOur technical analysis focuses on the PromptSpy dropper and its payload, PromptSpy. PromptSpy is embedded (app-release.apk) inside the dropper’s asset directory. This APK holds the core malicious functionality. When the dropper is launched, it displays a prompt urging the user to install what appears to be an updated version of the app. This “update” is actually the PromptSpy payload, which the user must install manually (see Figure 9).\n\nOnce installed and launched, PromptSpy requests Accessibility Service permissions, giving the malware the ability to read on‑screen content and perform automated clicks.\n\nThen PromptSpy shows a simple loading-style decoy screen in the foreground (see Figure 10). Meanwhile, in the background, it begins communicating with Gemini AI to obtain instructions needed to lock its process in the Recent Apps list – a simple persistence technique that allows PromptSpy to remain active and locked in place even after the device is rebooted.\n\nWhen the user sees the Loading, please wait activity, PromptSpy uses Accessibility Services to open the Recent Apps screen and collect detailed UI information: visible text, content descriptions, class names, package names, and screen bounds. It serializes this dynamic UI snapshot as XML and includes it in its prompt to Gemini. Gemini then returns step-by-step tap instructions on how to achieve the “app lock” gesture.\n\nThis process forms a continuous loop:\n\nThe loop continues until Gemini confirms that the app is successfully locked in recent apps. Here is an example structure:\n\nAll actions suggested by Gemini – taps, swipes, navigation – are executed through Accessibility Services, allowing the malware to interact with the device without user input.\n\nPromptSpy’s main malicious capability lies in its built‑in VNC service. This allows attackers to remotely view the victim’s screen in real time and fully control the device.\n\nThe malware communicates with its hardcoded command‑and‑control (C&C) server at 54.67.2[.]84 using the VNC protocol; the messages are AES-encrypted using a hardcoded key. Through this communication channel, the malware can:\n\nPromptSpy also misuses Accessibility Services as an anti‑removal mechanism. When the user attempts to uninstall the payload or disable Accessibility Services, the malware overlays transparent rectangles on specific screen areas - particularly over buttons containing substrings like stop, end, clear, and Uninstall. These overlays are invisible to the user but intercept interactions, making removal difficult. In Figure 11, we’ve run PromptSpy with the debug flag enabled (kept there by developers) that would set the color of the transparent rectangle, to visualize where they are specifically displayed. However, on the actual device, they are fully invisible.\n\nBecause PromptSpy blocks uninstallation by overlaying invisible elements on the screen, the only way for a victim to remove it is to reboot the device into Safe Mode, where third‑party apps are disabled and can be uninstalled normally.\n\nTo enter Safe Mode, users should typically press and hold the power button, long‑press Power off, and confirm the Reboot to Safe Mode prompt (though the exact method may differ by device and manufacturer). Once the phone restarts in Safe Mode, the user can go to Settings → Apps → MorganArg and uninstall it without interference.\n\nPromptSpy shows that Android malware is beginning to evolve in a sinister way. By relying on generative AI to interpret on‑screen elements and decide how to interact with them, the malware can adapt to virtually any device, screen size, or UI layout it encounters. Instead of hardcoded taps, it simply hands AI a snapshot of the screen and receives precise, step‑by‑step interaction instructions in return, helping it achieve a persistence technique resistant to UI changes.\n\nMore broadly, this campaign shows how generative AI can make malware far more dynamic and capable of real‑time decision‑making. PromptSpy is an early example of generative AI‑powered Android malware, and it illustrates how quickly attackers are beginning to misuse AI tools to improve impact.\n\nA comprehensive list of indicators of compromise (IoCs) and samples can be found in our GitHub repository.\n\nSHA-1\nFilename\nDetection\nDescription\n\n6BBC9AB132BA066F63676E05DA13D108598BC29B\nnet.ustexas.myavlive.apk\nAndroid/Spy.VNCSpy.A\nAndroid VNCSpy malware.\n\n375D7423E63C8F5F2CC814E8CFE697BA25168AFA\nnlll4.un7o6.q38l5.apk\nAndroid/Spy.VNCSpy.A\nAndroid VNCSpy malware.\n\n3978AC5CD14E357320E127D6C87F10CB70A1DCC2\nppyzz.dpk0p.ln441.apk\nAndroid/Spy.VNCSpy.A\nAndroid VNCSpy malware.\n\nE60D12017D2DA579DF87368F5596A0244621AE86\nmgappc-1.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\n9B1723284E311794987997CB7E8814EB6014713F\nmgappm-1.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\n076801BD9C6EB78FC0331A4C7A22C73199CC3824\nmgappn-0.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\n8364730E9BB2CF3A4B016DE1B34F38341C0EE2FA\nmgappn-1.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy dropper.\n\nF8F4C5BC498BCCE907DC975DD88BE8D594629909\napp-release.apk\nAndroid/Spy.PromptSpy.A\nAndroid PromptSpy.\n\nC14E9B062ED28115EDE096788F62B47A6ED841AC\nmgapp.apk\nAndroid/Phishing.Agent.M\nAndroid phishing malware.\n\nIP\nDomain\nHosting provider\nFirst seen\nDetails\n\n52.222.205[.]45\nm-mgarg[.]com\nAmazon.com, Inc.\n2026‑01‑12\nPhishing website.\n\n54.67.2[.]84\nN/A\nAmazon.com, Inc.\nN/A\nC&C server.\n\n104.21.91[.]170\nmgardownload[.]com\nCloudflare, Inc.\n2026‑01‑13\nDistribution website.\n\nThis table was built using version 18 of the MITRE ATT&CK framework.\n\nTactic\nID\nName\nDescription\n\nPersistence\nT1398\nBoot or Logon Initialization Scripts\nPromptSpy receives the BOOT_COMPLETED broadcast intent to activate at device startup.\n\nT1541\nForeground Persistence\nPromptSpy uses foreground persistence to keep a service running.\n\nDefense Evasion\nT1516\nInput Injection\nPromptSpy abuses the accessibility service to prevent its removal.\n\nCredential Access\nT1417.002\nMalicious Third Party Keyboard App: GUI Input Capture\nPromptSpy can intercept Android lockscreen PIN and password.\n\nDiscovery\nT1426\nSystem Information Discovery\nPromptSpy obtains device name, model, and OS version.\n\nCollection\nT1418\nSoftware Discovery\nPromptSpy can obtain a list of installed applications.\n\nT1513\nScreen Capture\nPromptSpy can record the screen.\n\nCommand and Control\nT1663\nRemote Access Software\nPromptSpy can use VNC to remotely control a compromised device.\n\nT1521.001\nStandard Cryptographic Protocol: Symmetric Cryptography\nPromptSpy encrypts C&C communication using AES.\n\nExfiltration\nT1646\nExfiltration Over C2 Channel\nPromptSpy can exfiltrate collected data to the C&C server.",
    "readingTime": 12,
    "keywords": [
      "chinese‑speaking environment",
      "amazon.com inc",
      "c&c server",
      "vnc protocol",
      "machine learning",
      "android/spy.vncspy.a android",
      "android/spy.promptspy.a android",
      "threat actors",
      "successfully locked",
      "remote access"
    ],
    "qualityScore": 1,
    "link": "https://www.welivesecurity.com/en/eset-research/promptspy-ushers-in-era-android-threats-using-genai/",
    "thumbnail_url": "https://web-assets.esetstatic.com/wls/2026/02-26/promptspy/promptspy-gemini-genai-malware.jpg",
    "created_at": "2026-02-21T06:29:58.261Z",
    "topic": "science"
  },
  {
    "slug": "why-the-playstation-6-will-be-delayed",
    "title": "Why The PlayStation 6 Will Be Delayed",
    "description": "This week, Lucy and Tam discuss how the AI boom is impacting gamers. Most notably, it's holding up hardware like the Steam Machine and, allegedly, the PlayStation 6.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/videos/why-the-playstation-6-will-be-delayed/2300-6466729/",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1352/13527689/4654778-spoton_ps6delay_20260220v2.jpg",
    "created_at": "2026-02-21T06:29:55.881Z",
    "topic": "gaming"
  },
  {
    "slug": "ai-could-make-your-next-tv-more-expensive",
    "title": "AI Could Make Your Next TV More Expensive",
    "description": "Add \"television prices\" to the long list of things AI is changing.",
    "fullText": "The scarcity of RAM brought on by the artificial intelligence boom, dubbed RAMageddon, is affecting more than just the price of PCs. AI could make new televisions more expensive too—as well as—game consoles, cell phones, high-tech coffee makers, and anything else with memory and a processor. But if you're in the market for a new TV, you might be better off buying sooner rather than later.\n\nAs Axios reports, televisions generally require 1GB to 8GB of RAM to run \"smart TV\" features and to process video and data, and the memory units widely found in 4K TVs have more than quadrupled in price over the last year. That extra cost could be passed on to consumers: Analyst TrendForce said last month that a price hike on TVs was \"unavoidable,\" while Samsung acknowledged it may need to reprice its products. That said, a typical television uses less memory, and less advanced memory, than some other key devices, so a potential price-spike is likely to be less dramatic than it is for things like PCs and smartphones. We'll see for sure when manufacturers announce the prices of their 2026 models.\n\nCompanies like Microsoft, Google, and Nvidia are scooping up memory supply to run AI data centers, and most TV makers don't have the market power of these gigantic corporations. \"When memory tightens, prices rise, product launches shift...margins compress and smaller companies struggle more than large tech giants,\" Marco Mezger, executive vice president of memory tech company Neumonda, told Axios. There is good news for consumers, however.\n\nHigher RAM prices have yet to hit the retail TV market, making now an unusually good time to buy a television. Overall, the price of smart TVs decreased by 15% between 2024 and the start of 2026, so you're starting from a good place. In addition, manufacturers generally offer lower prices at this time of year to clear shelf space ahead of new model releases. While more expensive RAM could be baked into the price of 2026 televisions, sets on the shelves now were priced before the effects of the shortage hit the retail market. Plus, some companies price their TVs lower because they make a lot of money collecting your data—unless you do you what you can to stop them, of course. All of which leads to ridiculously good deals, like $900 for a 65-inch OLED TV from Samsung. Bottom line: if you're in the market for a new TV, don't wait. (Though, chances are, you might not need a new TV.)\n\nNo one can say for sure how long the memory shortage will last, but the consensus of industry analysts is that we likely won't see a return to anything we'd consider normal before 2028. AI demand is projected to consume 70% of all high-end DRAM in 2026, so manufacturers are prioritizing it over the less advanced, less in-demand memory chips used for TVs and appliances. While investors are sinking billions into ramping up memory manufacturing, it takes around 19 months to get a factory up and running in Taiwan, and even longer in the U.S., so TV prices will likely remain high into 2028.",
    "readingTime": 3,
    "keywords": [
      "less advanced",
      "memory",
      "market",
      "televisions",
      "you're",
      "manufacturers",
      "expensive",
      "makers",
      "generally",
      "smart"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/why-ai-could-make-tvs-more-expensive?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHY8FFT9TW11MP3SKTAANQJ4/hero-image.fill.size_1200x675.png",
    "created_at": "2026-02-21T06:29:52.704Z",
    "topic": "tech"
  },
  {
    "slug": "openai-just-hired-back-another-employee-from-mira-muratis-thinking-machines-lab",
    "title": "OpenAI just hired back another employee from Mira Murati's Thinking Machines Lab",
    "description": "Thinking Machines Lab faces departures as employees rejoin OpenAI amid poaching efforts by major tech firms.",
    "fullText": "Another employee at Thinking Machines Lab is leaving to rejoin OpenAI.\n\nIt's the latest in a string of departures from the $12 billion AI startup, which is led by former OpenAI CTO Mira Murati and lately has been the subject of high-profile poaching campaigns from bigger tech companies.\n\nThe latest employee to go back to OpenAI is Jolene Parish, who joined Thinking Machines Lab in April last year, according to her LinkedIn profile. She had worked at OpenAI for three years prior. Before that, she worked for 10 years on security at Apple, her profile says.\n\nOther employees rejoined OpenAI last month. Two co-founders, former CTO Barret Zoph and Luke Metz, both left, along with researcher Sam Schoenholz.\n\nLia Guy, another researcher, also rejoined OpenAI, The Information reported. Another cofounder, Andrew Tulloch, left for Meta late last year, The Wall Street Journal reported.\n\nOpenAI and Thinking Machines Lab declined to comment.\n\nThinking Machines Lab raised a monster $2 billion funding round last year, valuing the company at $12 billion, spokespeople said at the time. The startup launched its first product, Tinker, last October.\n\nThe San Francisco-based company has become known for attracting star-studded talent. It quietly hired Neal Wu, a legendary coder who won three gold medals in an Olympiad for programming, and Soumith Chintala, the creator of the open-source AI project PyTorch at Meta, who is now Thinking Machines Lab's CTO, Business Insider previously reported.\n\nHave a tip? Contact this reporter via email at crollet@businessinsider.com or on Signal and WhatsApp at 628-282-2811. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 2,
    "keywords": [
      "machines lab",
      "rejoined openai",
      "thinking machines lab",
      "another",
      "employee",
      "latest",
      "startup",
      "profile",
      "researcher",
      "meta"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/thinking-machines-lab-employee-rejoins-openai-talent-exodus-2026-2",
    "thumbnail_url": "https://i.insider.com/6998c52befb52c8bd0de973f?width=1200&format=jpeg",
    "created_at": "2026-02-21T01:06:34.961Z",
    "topic": "finance"
  },
  {
    "slug": "heres-why-you-should-never-use-ai-to-generate-your-passwords",
    "title": "Here's Why You Should Never Use AI to Generate Your Passwords",
    "description": "ChatGPT isn't good at generating secure passwords.",
    "fullText": "I'm a bit of a broken record when it comes to personal security on the internet: Make strong passwords for each account; never reuse any passwords; and With these three steps combined, your general security is pretty much set. But how you make those passwords matters just as much as making each strong and unique. As such, please don't use an AI program to generate your passwords.\n\nIf you're a fan of chatbots like ChatGPT, Claude, or Gemini, it might seem like a no-brainer to ask the AI to generate passwords for you. You might like how they handle other tasks for you, so it might make sense that something seemingly so high-tech yet accessible could produce secure passwords for your accounts. But LLMs (large language models) are not necessarily good at everything, and creating good passwords just so happens to be among those faults.\n\nAs highlighted by Malwarebytes Labs, researchers recently investigated AI-generated passwords, and evaluated their security. In short? The findings aren't good. Researchers tested password generation across ChatGPT, Claude, and Gemini, and discovered that the passwords were \"highly predictable\" and \"not truly random.\" Claude, in particular, didn't fare well: Out of 50 prompts, the bot was only able to generate 23 unique passwords. Claude gave the same password as an answer 10 times. The Register reports that researchers found similar flaws with AI systems like GPT-5.2, Gemini 3 Flash, Gemini 3 Pro, and even Nano Banana Pro. (Gemini 3 Pro even warned the passwords shouldn't be used for \"sensitive accounts.\")\n\nThe thing is, these results seem good on the surface. They look uncrackable because they're a mix of numbers, letters, and special characters, and password strength identifiers might say they're secure. But these generations are inherently flawed, whether that's because they are repeated results, or come with a recognizable pattern. Researchers evaluated the \"entropy\" of these passwords, or the measure of unpredictability, with both \"character statistics\" and \"log probabilities.\" If that all sounds technical, the important thing to note is that the results showed entropies of 27 bits and 20 bits, respectively. Character statistics tests look for entropy of 98 bits, while log probabilities estimates look for 120 bits. You don't need to be an expert in password entropy to know that's a massive gap.\n\nHackers can use these limitations to their advantage. Bad actors can run the same prompts as researchers (or, presumably, end users) and collect the results into a bank of common passwords. If chatbots repeat passwords in their generations, it stands to reason that many people might be using the same passwords generated by those chatbots—or trying passwords that follow the same pattern. If so, hackers could simply try those passwords during break-in attempts, and if you used an LLM to generate your password, it might match. It's tough to say what that exact risk is, but to be truly secure, each of your passwords should be totally unique. Potentially using a password that hackers have in a word bank is an unnecessary risk.\n\nIt might seem surprising that a chatbot wouldn't be good at generating random passwords, but it makes sense based on how they work. LLMs are trained to predict the next token, or data point, that should appear in a sequence. In this case, the LLM is trying to choose the characters that make the most sense to appear next, which is the opposite of \"random.\" If the LLM has passwords in its training data, it may incorporate that into its answer. The password it generates makes sense in its \"mind,\" because that's what it's been trained on. It isn't programmed to be random.\n\nMeanwhile, traditional password managers are not LLMs. Instead, they are designed to produce a truly random sequence, by taking cryptographic bits and converting them into characters. These outputs are not based on existing training data and follow no patterns, so the chances that someone else out there has the same password as you (or that hackers have it stored in a word bank) is slim. There are plenty of options out there to use, and most password managers come with secure password generators.\n\nBut you don't even need one of these programs to make a secure password. Just pick two or three \"uncommon\" words, mix a few of the characters up, and presto: You have a random, unique, and secure password. For example, you could take the words \"shall,\" \"murk,\" and \"tumble,\" and combine them into \"sH@_llMurktUmbl_e.\" (Don't use that one, since it's no longer unique.)\n\nIf you're looking to boost your personally security even further, consider passkeys whenever possible. Passkeys combine the convenience of passwords with the security of 2FA: With passkeys, your device is your password. You use its built-in authentication to log in (face scan, fingerprint, or PIN), which means there's no password to actually create. Without the trusted device, hackers won't be able to break into your account.\n\nNot all accounts support passkeys, which means they aren't a universal solution right now. You'll likely need passwords for some of your accounts, which means abiding by proper security methods to keep things in order. But replacing some of your passwords with passkeys can be a step up in both security and convenience—and avoids the security pitfalls of asking ChatGPT to make your passwords for you.",
    "readingTime": 5,
    "keywords": [
      "gemini pro",
      "character statistics",
      "log probabilities",
      "truly random",
      "password managers",
      "secure password",
      "passwords",
      "security",
      "unique",
      "researchers"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/dont-use-ai-to-generate-your-passwords?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHY5M8NYREZ63A0JXYRYBTT4/hero-image.fill.size_1200x675.jpg",
    "created_at": "2026-02-21T01:06:33.965Z",
    "topic": "tech"
  },
  {
    "slug": "code-mode-give-agents-an-api-in-1k-tokens",
    "title": "Code Mode: give agents an API in 1k tokens",
    "description": "Cloudflare introduces Code Mode for Workers AI, a technical approach to fit entire API schemas into 1,000 tokens, enabling LLM agents to execute precise tool calls with minimal latency.",
    "fullText": "Model Context Protocol (MCP) has become the standard way for AI agents to use external tools. But there is a tension at its core: agents need many tools to do useful work, yet every tool added fills the model's context window, leaving less room for the actual task.\n\nCode Mode is a technique we first introduced for reducing context window usage during agent tool use. Instead of describing every operation as a separate tool, let the model write code against a typed SDK and execute the code safely in a Dynamic Worker Loader. The code acts as a compact plan. The model can explore tool operations, compose multiple calls, and return just the data it needs. Anthropic independently explored the same pattern in their Code Execution with MCP post.\n\nToday we are introducing a new MCP server for the entire Cloudflare API — from DNS and Zero Trust to Workers and R2 — that uses Code Mode. With just two tools, search() and execute(), the server is able to provide access to the entire Cloudflare API over MCP, while consuming only around 1,000 tokens. The footprint stays fixed, no matter how many API endpoints exist.\n\nFor a large API like the Cloudflare API, Code Mode reduces the number of input tokens used by 99.9%. An equivalent MCP server without Code Mode would consume 1.17 million tokens — more than the entire context window of the most advanced foundation models.\n\nCode mode savings vs native MCP, measured with tiktoken\n\nYou can start using this new Cloudflare MCP server today. And we are also open-sourcing a new Code Mode SDK in the Cloudflare Agents SDK, so you can use the same approach in your own MCP servers and AI Agents.\n\nThis new MCP server applies Code Mode server-side. Instead of thousands of tools, the server exports just two: search() and execute(). Both are powered by Code Mode. Here is the full tool surface area that gets loaded into the model context:\n\nTo discover what it can do, the agent calls search(). It writes JavaScript against a typed representation of the OpenAPI spec. The agent can filter endpoints by product, path, tags, or any other metadata and narrow thousands of endpoints to the handful it needs. The full OpenAPI spec never enters the model context. The agent only interacts with it through code.\n\nWhen the agent is ready to act, it calls execute(). The agent writes code that can make Cloudflare API requests, handle pagination, check responses, and chain operations together in a single execution.\n\nBoth tools run the generated code inside a Dynamic Worker isolate — a lightweight V8 sandbox with no file system, no environment variables to leak through prompt injection and external fetches disabled by default. Outbound requests can be explicitly controlled with outbound fetch handlers when needed.\n\nSuppose a user tells their agent: \"protect my origin from DDoS attacks.\" The agent's first step is to consult documentation. It might call the Cloudflare Docs MCP Server, use a Cloudflare Skill, or search the web directly. From the docs it learns: put Cloudflare WAF and DDoS protection rules in front of the origin.\n\nStep 1: Search for the right endpoints\nThe search tool gives the model a spec object: the full Cloudflare OpenAPI spec with all $refs pre-resolved. The model writes JavaScript against it. Here the agent looks for WAF and ruleset endpoints on a zone:\n\nThe server runs this code in a Workers isolate and returns:\n\nThe full Cloudflare API spec has over 2,500 endpoints. The model narrowed that to the WAF and ruleset endpoints it needs, without any of the spec entering the context window.\n\nThe model can also drill into a specific endpoint's schema before calling it. Here it inspects what phases are available on zone rulesets:\n\nThe agent now knows the exact phases it needs: ddos_l7 for DDoS protection and http_request_firewall_managed for WAF.\n\nStep 2: Act on the API\nThe agent switches to using execute. The sandbox gets a cloudflare.request() client that can make authenticated calls to the Cloudflare API. First the agent checks what rulesets already exist on the zone:\n\nThe agent sees that managed DDoS and WAF rulesets already exist. It can now chain calls to inspect their rules and update sensitivity levels in a single execution:\n\nThis entire operation, from searching the spec and inspecting a schema to listing rulesets and fetching DDoS and WAF configurations, took four tool calls.\n\nWe started with MCP servers for individual products. Want an agent that manages DNS? Add the DNS MCP server. Want Workers logs? Add the Workers Observability MCP server. Each server exported a fixed set of tools that mapped to API operations. This worked when the tool set was small, but the Cloudflare API has over 2,500 endpoints. No collection of hand-maintained servers could keep up.\n\nThe Cloudflare MCP server simplifies this. Two tools, roughly 1,000 tokens, and coverage of every endpoint in the API. When we add new products, the same search() and execute() code paths discover and call them — no new tool definitions, no new MCP servers. It even has support for the GraphQL Analytics API.\n\nOur MCP server is built on the latest MCP specifications. It is OAuth 2.1 compliant, using Workers OAuth Provider to downscope the token to selected permissions approved by the user when connecting. The agent  only gets the capabilities the user explicitly granted.\n\nFor developers, this means you can use a simple agent loop and still give your agent access to the full Cloudflare API with built-in progressive capability discovery.\n\nSeveral approaches have emerged to reduce how many tokens MCP tools consume:\n\nClient-side Code Mode was our first experiment. The model writes TypeScript against typed SDKs and runs it in a Dynamic Worker Loader on the client. The tradeoff is that it requires the agent to ship with secure sandbox access. Code Mode is implemented in Goose and Anthropics Claude SDK as Programmatic Tool Calling.\n\nCommand-line interfaces are another path. CLIs are self-documenting and reveal capabilities as the agent explores. Tools like OpenClaw and Moltworker convert MCP servers into CLIs using MCPorter to give agents progressive disclosure. The limitation is obvious: the agent needs a shell, which not every environment provides and which introduces a much broader attack surface than a sandboxed isolate.\n\nDynamic tool search, as used by Anthropic in Claude Code, surfaces a smaller set of tools hopefully relevant to the current task. It shrinks context use but now requires a search function that must be maintained and evaluated, and each matched tool still uses tokens.\n\nEach approach solves a real problem. But for MCP servers specifically, server-side Code Mode combines their strengths: fixed token cost regardless of API size, no modifications needed on the agent side, progressive discovery built in, and safe execution inside a sandboxed isolate. The agent just calls two tools with code. Everything else happens on the server.\n\nThe Cloudflare MCP server is available now. Point your MCP client at the server URL and you'll be redirected to Cloudflare to authorize and select the permissions to grant to your agent. Add this config to your MCP client:\n\nFor CI/CD, automation, or if you prefer managing tokens yourself, create a Cloudflare API token with the permissions you need. Both user tokens and account tokens are supported and can be passed as bearer tokens in the Authorization header.\n\nMore information on different MCP setup configurations can be found at the Cloudflare MCP repository.\n\nCode Mode solves context costs for a single API. But agents rarely talk to one service. A developer's agent might need the Cloudflare API alongside GitHub, a database, and an internal docs server. Each additional MCP server brings the same context window pressure we started with.\n\nCloudflare MCP Server Portals let you compose multiple MCP servers behind a single gateway with unified auth and access control. We are building a first-class Code Mode integration for all your MCP servers, and exposing them to agents with built-in progressive discovery and the same fixed-token footprint, regardless of how many services sit behind the gateway.",
    "readingTime": 7,
    "keywords": [
      "worker loader",
      "ddos protection",
      "code mode",
      "cloudflare api",
      "openapi spec",
      "built-in progressive",
      "sandboxed isolate",
      "mcp servers",
      "progressive discovery",
      "ruleset endpoints"
    ],
    "qualityScore": 1,
    "link": "https://blog.cloudflare.com/code-mode-mcp/",
    "thumbnail_url": "https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2080o6v9LBfIFbLUW8elRE/3cfaeb0dc0fa56bcbe4c332b1942a167/Code_Mode-_give_agents_an_entire_API_in_1_000_tokens-OG.png",
    "created_at": "2026-02-21T01:06:31.696Z",
    "topic": "tech"
  },
  {
    "slug": "universal-ros-bridge-for-ai-agents-control-robots-with-llms",
    "title": "Universal ROS bridge for AI agents – control robots with LLMs",
    "description": "Universal ROS1/ROS2 bridge for AI agents to control robots and embodied intelligence systems. - webthree549-bot/agent-ros-bridge",
    "fullText": "webthree549-bot\n\n /\n\n agent-ros-bridge\n\n Public\n\n Universal ROS1/ROS2 bridge for AI agents to control robots and embodied intelligence systems.\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n webthree549-bot/agent-ros-bridge",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/webthree549-bot/agent-ros-bridge",
    "thumbnail_url": "https://opengraph.githubassets.com/67a6f21115579e2ab34f04b7361101f54acb4f5f76fa6d08612e6084d61113d9/webthree549-bot/agent-ros-bridge",
    "created_at": "2026-02-21T01:06:30.507Z",
    "topic": "tech"
  },
  {
    "slug": "claws-are-now-a-new-layer-on-top-of-llm-agents",
    "title": "Claws are now a new layer on top of LLM agents",
    "description": "Bought a new Mac mini to properly tinker with claws over the weekend. The apple store person told me they are selling like hotcakes and everyone is confused...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/karpathy/status/2024987174077432126",
    "thumbnail_url": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_200x200.jpg",
    "created_at": "2026-02-21T01:06:30.124Z",
    "topic": "tech"
  },
  {
    "slug": "2-littleknown-tech-founders-are-both-14-billion-richer-this-year-as-ai-demand-fuels-a-memory-supercycle",
    "title": "2 little-known tech founders are both $14 billion richer this year as AI demand fuels a memory supercycle",
    "description": "Kingston founders and memory magnates David Sun and John Tu are 44% richer this year as intense AI demand causes a supply crunch.",
    "fullText": "Two memory-product entrepreneurs have seen their fortunes rocket this year thanks to heady demand from AI giants.\n\nDavid Sun, 74, and John Tu, 84, are the cofounders and 50-50 owners of Kingston Technology, a leading maker of computer drives and memory modules. Sun is the California company's operating chief, while Tu is president and CEO.\n\nThe pair have seen their respective net worths soar by 44%, or nearly $14 billion, since the start of January, the Bloomberg Billionaires Index shows.\n\nOnly Tesla and SpaceX CEO Elon Musk, Czechoslovak Group CEO Michal Strnad, and Mexican tycoon Carlos Slim have gained more wealth so far this year.\n\nSun and Tu have even outpaced Jim, Alice, and Rob Walton; the heirs to the Walmart fortune were up about $13 billion each for the year at Thursday's close.\n\nThe memory magnates are both worth about $45 billion, putting them in 45th and 46th place respectively on the rich list — ahead of the likes of MacKenzie Scott, SoftBank's Masayoshi Son, and Miriam Adelson.\n\nSun and Tu were born in Taiwan and mainland China, respectively, studied electrical engineering in college, and emigrated to the US in the 1970s, per Bloomberg. They met in Los Angeles, where they bonded over basketball and went into business together.\n\nThe pair founded a memory-device company, Camintonn, in 1982, and sold it four years later for $6 million, Bloomberg says. They lost all their savings in the Black Monday crash of 1987, but went on to found Kingston and sell 80% of it to SoftBank for $1.5 billion in 1996. They bought back the stake for $450 million in 1999.\n\nAI hyperscalers have been clamoring for memory chips to build out data centers, fueling a severe global shortage that has driven prices skyward.\n\nThis \"memory supercycle\" promises to boost revenues and profits at memory-product companies, which has propelled their stock prices in recent months.\n\nFor example, Micron Technology stock has more than quadrupled over the past 12 months, valuing the memory-chip maker at $470 billion — more than Mastercard, Oracle, Costco, Bank of America, or Home Depot are worth.",
    "readingTime": 2,
    "keywords": [
      "memory",
      "memory-product",
      "maker",
      "pair",
      "worth",
      "respectively",
      "stock",
      "bloomberg",
      "kingston",
      "technology"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/memory-chip-shortage-ai-boom-supercycle-kingston-sun-tu-billionaires-2026-2",
    "thumbnail_url": "https://i.insider.com/699850fcf8731049f3af7599?width=1200&format=jpeg",
    "created_at": "2026-02-20T18:32:34.412Z",
    "topic": "finance"
  },
  {
    "slug": "6-stocks-citi-says-to-buy-as-the-bull-market-enters-a-new-stage-of-volatility",
    "title": "6 stocks Citi says to buy as the bull market enters a new stage of volatility",
    "description": "Citi said it revised its list of buy-rated stocks to account for the recent bout of volatility stemming from AI disruption fears.",
    "fullText": "The AI apocalypse means investors should turn their attention to an emerging group of stock market winners.\n\nIn a note to clients this week, strategists at Citi said the bank revised its Thematic 30 Recommended List, a list of 30 buy-rated stocks that align with the bank's various investment themes.\n\nThe changes were made to create a better balance between some themes — like the bank's focus on \"AI at a Reasonable Price — and to account for the recent bout of volatility in markets, strategists wrote.\n\n\"Essentially, own beta but differentiated types in this more volatile stage of the bull market,\" the bank said on Wednesday, referring to the recent sell-off stemming from AI disruption fears.\n\nThe bank said it continued to follow a barbell approach between growth and cyclical stocks, referring to an investment strategy that exposes investors to both high- and low-risk assets to protect against downside if one asset underperforms.\n\nThe firm also trimmed some stocks that have been badly hurt amid the recent sell-off, including AppLoving, DocuSign, and Pinterest.\n\nHere are the bank's newest investing ideas amid the heightened volatility:",
    "readingTime": 1,
    "keywords": [
      "bank",
      "stocks",
      "bank's",
      "investors",
      "market",
      "strategists",
      "investment",
      "themes",
      "volatility",
      "referring"
    ],
    "qualityScore": 0.75,
    "link": "https://www.businessinsider.com/stocks-to-buy-during-selloff-ai-volatility-investing-citi-2026-2",
    "thumbnail_url": "https://i.insider.com/699868e1f8731049f3af7678?width=1200&format=jpeg",
    "created_at": "2026-02-20T18:32:34.081Z",
    "topic": "finance"
  },
  {
    "slug": "why-the-ceo-of-popular-trading-platform-etoro-says-hes-not-sweating-crypto-winter-ai-disruption-or-interest-rates-in",
    "title": "Why the CEO of popular trading platform eToro says he's not sweating crypto winter, AI disruption, or interest rates in 2026",
    "description": "Yoni Assia, CEO of eToro, predicts AI stocks and crypto will rebound in 2026, while monetary policy later this year will be supportive of risk assets.",
    "fullText": "Anxiety has been high among investors and consumers lately amid a cocktail of macroeconomic and market pressures, but Yoni Assia, the founder and CEO of trading platform eToro, isn't worried.\n\nLeading tech stocks have struggled year to date amid fears of AI disruption across a wide range of industries, while bearish sentiment has pressured crypto prices lower. Assia told Business Insider, though, that he believes both markets will rebound in 2026 and that he remains optimistic about monetary policy shifts.\n\neToro competes in a crowded field of retail-oriented trading platforms, but Assia thinks the platform has carved out a niche for itself among retail traders who utilize its more advanced features. According to Assia, its users haven't shied away from trading recently but have seized the opportunity to buy the dip on discounted tech stocks.\n\n\"I definitely think that it's a bit blown up a bit in proportion,\" he said, \"I think right now, people are a bit excited about the AI correction. I think that's going to pass.\"\n\nAssia added that he's seen traders rotate away from crypto and into precious metals recently. He's not surprised by this, as gold and silver prices have stabilized while crypto continues to fall. Yet, Assia is optimistic that crypto is poised for a comeback from its brutal bear market later this year.\n\n\"After a correction, after all-time high post-halving, you get a bull run, then you have a correction, and then it basically resets three to six months after that correction,\" he said.\n\nThis view is similar to that of other crypto pros who have speculated that if historic market cycles are any guide, crypto prices are due to rebound soon.\n\nAnother factor that has weighed heavily on the minds of some investors is monetary policy shifts that may be coming as a result of Kevin Warsh's appointment to the Federal Reserve. But Assia noted that if the new Fed chair proceeds with the expected rate cuts, it will likely help spur growth for the broader market.\n\n\"Historically, as rate cuts go lower, trading volumes go higher and leverage goes higher in the industry in general,\" he stated. \"The cost of borrowing money goes down, and the ability to more easily leverage goes up, which elevates stocks in the long run.\"",
    "readingTime": 2,
    "keywords": [
      "monetary policy",
      "policy shifts",
      "rate cuts",
      "tech stocks",
      "crypto",
      "market",
      "trading",
      "correction",
      "assia",
      "among"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ai-stocks-crypto-interest-rates-prices-trading-retail-etoro-ceo-2026-2",
    "thumbnail_url": "https://i.insider.com/69987bc4156648bc16a89585?width=1200&format=jpeg",
    "created_at": "2026-02-20T18:32:33.939Z",
    "topic": "finance"
  },
  {
    "slug": "google-deepmind-ceo-says-the-memory-shortage-is-creating-an-ai-choke-point",
    "title": "Google Deepmind CEO says the memory shortage is creating an AI 'choke point'",
    "description": "Google DeepMind CEO Demis Hassabis said that the \"whole supply chain\" for memory chips is constrained, which also impacts AI research.",
    "fullText": "The memory shortage takes no prisoners. Even Google isn't immune.\n\nAI companies are duking it out for greater and greater quantities of memory chips. The problem? The industry is heavily supply-constrained. Costs have skyrocketed, products have been tied up, and some companies — especially those in consumer electronics — are increasing prices.\n\nOn the AI front, Google DeepMind CEO Demis Hassabis told CNBC that physical challenges were \"constraining a lot of deployment.\" Google sees \"so much more demand\" for Gemini and its other models than it could serve, he said.\n\n\"Also, it does constrain a little bit the research,\" Hassabis said. \"You need a lot of chips to be able to experiment on new ideas at a big enough scale that you can actually see if they're going to work.\"\n\nResearchers want chips, whether they work at Google, Meta, OpenAI, or other Big Tech companies, and memory is a key component. Mark Zuckerberg said that AI researchers demanded two things beyond money: the fewest number of people reporting to them, and the most chips possible.\n\nHassabis said that wherever there was a capacity constraint, there was a \"choke point.\"\n\n\"The whole supply chain is kind of strained,\" Hassabis said. \"We're lucky, because we have our own TPUs, so we have our own chip designs.\"\n\nGoogle has long built TPUs — Tensor Processing Units — for internal use. The company also leases them to external customers through its cloud, which has also put Nvidia on edge.\n\nBut even access to their own TPUs won't save Google from having to navigate the highly competitive memory market. \"It still, in the end, actually comes down to a few suppliers of a few key components,\" Hassabis said.\n\nThree suppliers dominate memory chip production: Samsung, Micron, and SK Hynix. These companies are struggling to meet demand for chips from AI hyperscalers without dropping their longtime electronics customers.\n\nIt doesn't help that AI companies mainly want a different type of memory chip than PC manufacturers do. Large language model producers want HBM (high-bandwidth memory) chips.\n\nDon't expect Google's spending on AI infrastructure and chips to go down anytime soon. On its fourth-quarter earnings call, the company projected capital expenditures of $175 billion to $185 billion for 2026.",
    "readingTime": 2,
    "keywords": [
      "memory chip",
      "memory chips",
      "hassabis",
      "greater",
      "electronics",
      "demand",
      "researchers",
      "customers",
      "suppliers",
      "google"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-deepmind-demis-hassabis-memory-shortage-ai-supply-chain-2026-2",
    "thumbnail_url": "https://i.insider.com/6998773f156648bc16a894b8?width=1200&format=jpeg",
    "created_at": "2026-02-20T18:32:33.818Z",
    "topic": "finance"
  },
  {
    "slug": "frontier-model-training-methodologies",
    "title": "Frontier Model Training Methodologies",
    "description": "How do labs train a frontier, multi-billion parameter model? We look towards seven open-weight frontier models: Hugging Face’s SmolLM3, Prime Intellect’s Intellect 3, Nous Research’s Hermes 4, OpenAI’s gpt-oss-120b, Moonshot’s Kimi K2, DeepSeek’s DeepSeek-R1, and Arcee’s Trinity series. This blog is an attempt at distilling the techniques, motivations, and considerations used to train their models with an emphasis on training methodology over infrastructure.",
    "fullText": "How do labs train a frontier, multi-billion parameter model? We look towards seven open-weight frontier models: Hugging Face’s SmolLM3, Prime Intellect’s Intellect 3, Nous Research’s Hermes 4, OpenAI’s gpt-oss-120b, Moonshot’s Kimi K2, DeepSeek’s DeepSeek-R1, and Arcee’s Trinity series. This blog is an attempt at distilling the techniques, motivations, and considerations used to train their models with an emphasis on training methodology over infrastructure.\n\nThese notes are largely structured based on Hugging Face’s SmolLM3 report due to its extensiveness, and it is currently supplemented with notes from other reports including Intellect-3, gpt-oss-120b, Hermes 4, DeepSeek, and Kimi. While this blog explores some infrastructure-related ideas like in-flight weight updates and multi-client orchestrators, there are many other ideas mentioned throughout those posts/blogs like expert parallelism and quantization. Hugging Face writes more about gpt-oss-120b’s infrastructure here.\n\nArchitecture decisions fundamentally determine a model’s efficiency, capabilities, and training dynamics. Model families like DeepSeek, gpt-oss-120b, Kimi, and SmolLM have vastly different architectures (dense vs MoE), attention mechanisms (MHA vs MLA vs GQA), position encodings (RoPE, partial RoPE, NoPE), among many others. Not all information about the models is publicly available, so some are chosen:\n\nWhen choosing architecture, Hugging Face suggests following a decision tree such that if one of these is true, choose a dense architecture:\n\nMulti-head attention (MHA) uses separate query, key, and value projections for each attention head, but this creates a large KV-cache that becomes an inference bottleneck and GPU memory hoarder. To address this, researchers developed multi-query attention (MQA) and grouped query attention (GQA). In MQA, KV values are shared across all heads, but this comes at a cost of leaking attention capacity because heads can’t store information specialized to each head’s role. GQA softens this issue by sharing KV values across a small group of heads (e.g. 4). Another alternative is multi-latent attention (MLA) which stores a compressed latent variable that can be decompressed/projected into KV values at runtime. The latent variable is typically much smaller than the full KV cache (often achieving 4-8x compression), and this results in a KV-cache parameter count more comparable to GQA while maintaining performance stronger than MQA.\n\nWhen ablating (for variables that change the parameter count such as changing MHA to GQA, they occasionally adjust other hyperparameters to keep model sizes roughly the same), Hugging Face found that GQA with small groups (2/4/8) outperformed MHA in their ablations and that MHA outperformed MQA and GQA with 16 groups. Across benchmarks like HellaSwag, MMLU, and ARC, GQA with 2/4/8 groups performed best in their experiments.\n\nGated attention applies an elementwise gating mechanism to the scaled dot-product attention output before the output projection. A gate vector $\\mathbf{g}_t = \\sigma(\\mathbf{W}^G \\mathbf{x}_t)$ is computed from the input, where $\\mathbf{x}_t$ is the input at position $t$, $\\sigma$ is the sigmoid function, and $\\mathbf{W}^G$ is a learned gate projection matrix. This gate is split across $h_q$ attention heads (where $h_q$ is the number of query heads), and each head’s attention output is elementwise multiplied by its corresponding gate segment: $\\tilde{\\mathbf{o}}_{t,i} = \\mathbf{o}^{\\text{sdpa}}_{t,i} \\odot \\mathbf{g}_{t,i}$ where $\\mathbf{o}^{\\text{sdpa}}_{t,i}$ represents the scaled dot-product attention output for head $i$ at position $t$, $\\odot$ denotes elementwise multiplication, and $\\mathbf{g}_{t,i}$ is the gate segment for head $i$. The gated outputs are then concatenated and projected through the output matrix $\\mathbf{W}^O$ to produce the final output.\n\nGated attention reduces attention sinks (tokens receiving disproportionately high attention), reduces large activations that destabilize training, and improves performance on evaluations and long-sequence generalization. Critically, it stabilizes training and reduces loss spikes, making it valuable for large-scale training.\n\nWhen pre-training, a common consideration is fixed sequence lengths since training uses tensors of the form [batch, sequence length, hidden], so with regards to batching and distributed training, GPUs are most happy when every example has the same sequence length. But due to variable document length and wanting to avoid padding which wastes compute, packing enables shuffling and concatenating documents within the same sequence to achieve the sequence length.\n\nCausal masking means that for unrelated files $A$ and $B$ in the same batch, the tokens in $B$ can attend to the tokens in $A$, which degrades performance. With intra-document masking, the attention mask is modified so tokens can only attend to previous tokens within the same document. Many papers have found benefits relating to long-context extension and some short context benchmarks as well as shortening the average context length.\n\nFigure 1: Comparison of causal masking (left) and intra-document masking (right). Causal masking allows tokens to attend to all preceding tokens regardless of document boundaries, while intra-document masking restricts attention to tokens within the same document. From @PMinervini.\n\nWhen implementing document masking, Hugging Face saw small improvements on PIQA but otherwise no noticeable impact on short context tasks. But in line with aforementioned research, they observed that it became crucial for scaling from 4k to 64k tokens.\n\nThe decision of whether to use intra-document attention masking can depend on model scale. For smaller models, some implementations choose to omit intra-document masking, finding that the additional complexity and potential reduction in cross-document learning doesn’t justify the benefits at those scales. However, for larger models, intra-document masking becomes more critical as the model’s capacity to learn from cross-document attention patterns diminishes relative to the benefits of cleaner document boundaries.\n\nInput embeddings (token-to-vector lookup) and output embeddings (hidden states to vocab logits) are typically represented as separate matrices, so the total embedding parameters are $2 \\times \\text{vocab size} \\times \\text{hidden dim}$. In small language models, this can account for up to 20% of total parameters, as is the case with Llama 3.2 1B (in larger models, the embeddings represent a much smaller fraction of the parameter count, only 3% in Llama 3.1 70B). The issue with tying them is that input/output embeddings still represent different geometries, and frequent tokens like “the” can dominate representation learning due to getting gradients from both the input stream and the predicted output.\n\nFigure 2: Comparison of untied embeddings (separate input and output matrices) vs tied embeddings (shared matrix). Tied embeddings reduce parameter count while maintaining comparable performance. From PyTorch Blog.\n\nHugging Face found that on a 1.2B model, tied embeddings did comparably well despite having 18% fewer parameters (down from 1.46B), and that compared to an untied model also with 1.2B parameters (fewer layers), untied showed higher loss and lower downstream eval scores.\n\nWithout positional encoding, transformers have no sense of word order, akin to the bag of words idea. Initially, absolute position embeddings were used by learning a lookup table that mapped the position index to a vector added to token embeddings, but the maximum input sequence length was limited by the sequence length it was trained on. Relative position encodings followed since capturing distance between tokens matters more than capturing their absolute positions.\n\nThe most commonly used technique is rotary position embedding (RoPE), which encodes position information by rotating query and key vectors in 2D planes. RoPE encodes relative position as rotation angles: based on the dimensionality of the query/key vector, RoPE splits it into pairs (since they rotate in 2D space) and rotates depending on the absolute position of a token and a base frequency. During attention, the dot product between their rotated positions directly encodes their relative distance via the phase difference in their rotation angles, where tokens $x$ positions apart always maintain the same angular relationship.\n\nFigure 3: RoPE splits query/key vectors into pairs and rotates each pair by an angle proportional to position. From Su et al., 2021.\n\nDuring pre-training, models are trained on shorter context lengths (similar ideas to document masking, and quadratic attention is expensive) to learn short range correlation between words. But as sequence length grows, the rotation angles grow via $\\theta= \\text{position} \\times \\frac1{\\text{base}^{\\frac{k}{\\text{dim}/2}}}$. This can be fixed by increasing the base frequency as the sequence length increases using methods like ABF (Adaptive Base Frequency) or YaRN, which applies a more granular interpolation of frequencies on different components and includes other techniques like dynamic attention scaling and temperature adjustment. For extremely long contexts, YaRN does best, and in gpt-oss-120b, it was used to extend the context length of dense layers up to 131k tokens.\n\nMore recently, with the emphasis on long contexts, NoPE (no position embedding) and RNoPE, a hybrid method, have emerged. NoPE uses only causal masking and attention patterns, so it doesn’t bump into the issue of extrapolating beyond training lengths but shows weaker performance on short context reasoning and knowledge-based tasks. RNoPE alternates applying RoPE and NoPE on attention blocks, where RoPE handles local context and NoPE helps with longer-range information retrieval. Another idea is Partial RoPE, which applies RoPE/NoPE within the same layer.\n\nHugging Face ran ablations using RoPE, RNoPE (removing positional encoding every 4th layer), and RNoPE with document masking. They found that all achieve similar performance on short-context tasks, so they adopt RNoPE + document masking because it provides the foundation for long-context handling.\n\nFigure 4: five common types of attention. From Hugging Face.\n\nNote: This section covers attention pattern modifications (which change which tokens can attend to which other tokens). These are distinct from positional encoding scaling methods like ABF/YaRN (discussed in the “positional encodings” section), which adjust how position information is encoded without changing attention patterns. The following methods modify attention patterns to reduce computational cost:\n\nInterleaving local and global attention alternates between layers that use local attention (restricted to nearby tokens) and global attention (full sequence). This pattern balances computational efficiency with the ability to capture both local and long-range dependencies. Local layers reduce quadratic complexity while maintaining local context, and global layers ensure that distant relationships aren’t lost. When training encounters instability or loss spikes, adjusting the ratio of global layers (for example, increasing their frequency) can result in quicker loss recovery, as the model regains access to long-range information that may be crucial for certain patterns. The interleaving strategy is particularly effective for long-context models where full global attention would be computationally prohibitive.\n\nMoEs (mixture of experts), analogous to our brain activating different regions for different tasks, provide an alternative to dense models. At inference, only certain “experts” are activated based on the input, dramatically reducing compute compared to dense models where all parameters are active. The MoE works by replacing the feed forward layer with multiple MLPs (experts) and adding a learnable router before the MLPs to select the experts. The router typically uses top-k gating, selecting the $k$ experts with highest affinity scores for each token, where $k$ is usually much smaller than the total number of experts (e.g., 8 out of 384).\n\nFigure 5: Comparison of dense architecture and MoE architecture. From Sebastian Raschka.\n\nIn general, for fixed number and size of active experts, increasing the total number of experts improves loss, and high sparsity improves performance and benefits more from increasing compute. Recent models are much more sparse, with over 100 experts and around 10 active per token.\n\nTo determine how large each expert should be, a common metric is granularity, defined by $G = 2 \\cdot \\frac{d_\\text{model}}{d_\\text{expert}}$, where a higher granularity corresponds to more experts with a smaller dimension; this can be interpreted as a number proportional to the experts needed to match the dense MLP width. Recent models have granularity anywhere from 2 (gpt-oss-120b) to 8 (qwen3-next-80b-a3b). Ant Group showed that granularity doesn’t significantly change loss but does drive efficiency leverage (the ratio of flops needed for an MoE to achieve the same loss as a dense model). And overall, MoEs present a good alternative to dense models in terms of compute for training and inference.\n\nShared experts are always-on experts, which absorb the basic, recurring patterns so that other experts can more aggressively specialize; one is often enough (DeepSeek-V2 uses two, which adds a bit of complexity).\n\nLoad balancing is crucial in that if it fails, not only do training and inference efficiency plummet, but so do effective learning capacity. The routing mechanism typically uses top-k gating: for each token, the router computes affinity scores (often via a learned linear projection followed by softmax), selects the top $k$ experts, and routes the token to those experts. To ensure balanced expert utilization, this can be addressed by adding a loss-based load balancer (LBL) given by $\\mathcal{L} = \\alpha \\sum_{i=1}^{N_r} f_i P_i$ where $N_r$ is the total number of experts, $\\alpha$ determines the strength of the balancing term, $f_i$ is the fraction of tokens routed to expert $i$, and $P_i$ is the probability mass (average routing probability) for expert $i$; so in perfect load balancing, $f_i=P_i=\\frac1{N_r}$. Also, $\\alpha$ should not be so large that routing uniformity overwhelms the primary training objective. These should be monitored using global statistics, not local statistics which may suffer from a local batch being narrow, biasing the routing statistics.\n\nDeepSeek-V3 does loss-free load balancing differently, by adding a bias term to affinity scores going into the routing softmax.\n\nBeyond bias-based approaches, several other routing and load balancing strategies have emerged. Some implementations use learnable routing functions that adapt during training, while others incorporate expert capacity constraints that prevent any single expert from being overwhelmed. The key insight across these methods is that effective load balancing must operate using global statistics aggregated across multiple batches, as local batch statistics can be misleadingly narrow and bias routing decisions.\n\nSequence-wise auxiliary loss extends traditional auxiliary losses to promote balance within a sequence.\n\nHere, $T$ is the sequence length, $\\alpha$ is a small coefficient, $\\mathbb{1}(\\cdot)$ is the indicator function (which is 1 if its argument is true and 0 otherwise), and $K_r$ is the number of active experts per token.\n\nHere, for each token at each position $t$ in the sequence, each expert $i$ is assigned a routing score $s_{i,t}$, which is normalized so that $\\tilde{s}_{i,t}$ captures the proportion of the routing probability assigned to expert $i$ at position $t$. Averaging this over the whole sequence gives $P_i$, which represents, on average, how often expert $i$ is considered for routing across the sequence. The $f_i$ term furthers this by reflecting the fraction of times expert $i$ is actually selected (i.e., is among the top $K_r$ experts for a token, after bias terms $b_i$ are added). The loss $\\mathcal{L}_{\\text{Bal}}$ encourages the product $f_i P_i$ to be similar across different experts, pushing the model toward evenly distributing routing decisions and load; if any expert is used much more or less than others, the loss will increase, nudging the model back toward balanced expert activation.\n\nAuxiliary-loss free load balancing methods avoid introducing interference gradients by maintaining a bias vector $\\mathbf{b}=[b_1, \\cdots, b_{N_r}]$ which is updated in a decoupled fashion. Let $n_i$ be the number of tokens routed to expert $i$ in the current step and $\\bar{n}=\\frac1{N_r} \\sum_{i=1}^{N_r} n_i$ the mean load across all experts. $b_i$ is updated by\n\nwhere $\\gamma$ is the bias update speed, a sort of learning rate. This particular version includes the additional recentering of expert bias updates.\n\nSequence-wise MoE Balancing with Uniformity (SMEBU) load balancing operates at the sequence level rather than the token level, ensuring that expert utilization remains balanced across entire sequences. The normalized per-expert violation is calculated by $v_i=\\frac{\\bar{n}-n_i}{\\bar{n}}$ and $\\tilde{v}_i=\\tanh(\\kappa v_i)$, which makes the scale independent of sequence length and batch size. Then $b_i$ is updated using a momentum buffer with momentum factor $\\beta$:\n\n$\\tanh$ applies the soft-clamping, with tunable scale $\\kappa$ to control saturation speed; $\\tanh$ over $\\text{sign}(\\cdot)$ maintains the continuity and stability needed during training whereas $\\text{sign}$ forces updates to be $\\pm \\lambda$, making the update step oscillate. Momentum also is introduced as a form of noise dampening, analogous to momentum SGD reducing variance in noisy gradient updates\n\nBecause transformers don’t deal efficiently with long context while RNNs can, one idea is to combine both to get the best of both worlds. By dropping the softmax from the output for token $t$:\n\nwhere $\\mathbf{q}_t$, $\\mathbf{k}_j$, and $\\mathbf{v}_j$ are the query, key, and value vectors at positions $t$ and $j$, respectively, and $\\mathbf{o}_t$ is the output at position $t$. By defining $S_t :=\\sum_{j=1}^t \\mathbf{k}_j \\mathbf{v}_j^\\top$, then we get a recurrent relation where $S_t$ summarizes all past $(k_j, v_j)$ pairs:\n\nwhere $S_{t-1}$ is the state from the previous time step. While this gets us closer to an RNN-esque structure, in practice, softmax stabilizes training, and the linear form can cause instability without normalization. With RNNs, it is sometimes helpful to forget the past, by introducing a gate $\\mathbf{G}_t$ for the previous state \n\\(\\mathbf{S}_t=\\mathbf{G}_t \\odot \\mathbf{S}_{t-1} + \\mathbf{v}_t\\mathbf{k}_t^\\top\\)\nwhere $\\odot$ denotes elementwise multiplication and $\\mathbf{G}_t$ is a learned gating mechanism.\nMamba-2 is among the most popular, being used in hybrid models like Nemotron-H and Falcon H1. Hybrid models are becoming increasingly popular, notably in Qwen3-Next with a gated DeltaNet update and Kimi’s next model, likely using their “kimi delta attention.”\n\nTraining stability is crucial for successful large-scale model training. Several techniques help prevent training failures, including regularization methods, careful initialization, and architectural choices. The following sections cover key stability mechanisms:\n\n$z$-loss is a regularization term added to the standard cross entropy loss that keeps logits from drifting to large magnitudes. The softmax denominator is $Z = \\sum_{i=1}^V e^{z_i}$, and by adding $\\mathcal{L} = \\lambda \\cdot \\log^2(Z)$ to the loss, we penalize based on $\\log(Z)$ which represents the overall logit scale.\n\nOn their 1B model, Hugging Face found that adding $Z$-loss didn’t impact training loss or downstream performance, so they chose not to include it due to training overhead. For logit stabilization, logit softcapping (see below) is generally preferred in modern recipes, following the Gemma 2 and Gemma 3 models.\n\nLogit softcapping prevents logits from growing excessively large by mapping them into a bounded range via a smooth, differentiable transformation. Unlike hard clipping (which has zero gradient at the boundaries and can cause training instability), softcapping uses $\\tanh$ to compress values smoothly. The Gemma 2 report introduces the formulation used in production models: cap logits such that values stay within $(-\\texttt{soft_cap}, +\\texttt{soft_cap})$ using\n\nwhere $\\texttt{soft_cap}$ is the threshold hyperparameter controlling the output range. The division normalizes inputs before $\\tanh$ and the multiplication by $\\texttt{soft_cap}$ rescales to the desired interval. Unlike $z$-loss (which adds a regularization term to the loss), softcapping operates directly on activations in the forward pass\n\nGemma 2 applies softcapping to both attention logits (pre-softmax) and the final language modeling head. They set $\\texttt{soft_cap}=50.0$ for attention layers and $\\texttt{soft_cap}=30.0$ for the final layer. The technique traces back to Bello et al., 2016 in the context of neural machine translation. However, one caveat is that logit softcapping is incompatible with Flash Attention / SDPA during training because those fused kernels assume standard attention. The Hugging Face Gemma 2 blog notes that for stable fine-tuning, you must use attn_implementation=\"eager\"; inference can still use SDPA with minimal quality difference. This writeup gives a concise technical overview.\n\nDespite being a regularization technique, removing weight decay from embeddings can improve training stability. Weight decay causes embedding norm to decrease, but this can lead to larger gradients in earlier layers since the LayerNorm Jacobian has a $\\frac1{\\sigma}$ term (coming from normalization) which is inversely proportional to the input norm $\\sigma$.\n\nHugging Face tested this using a weight decay baseline, a no weight decay baseline, and another combining all previous adopted changes and found no significant loss or eval results, so they included no weight decay.\n\nSimilar to $z$-loss, QK-norm helps prevent attention logits from becoming too large by applying LayerNorm to both the query and key vectors before computing attention. However, the same paper which proposed RNoPE found that it hurts long-context tasks because the normalization de-emphasizes relevant tokens and emphasizes irrelevant tokens by stripping the query-key dot product of its magnitude.\n\nRMSNorm maintains comparable performance to LayerNorm while being computationally simpler, due to avoiding the mean-centering computation. A variant called depth-scaled sandwich norm applies normalization both before and after the attention/MLP blocks, with the normalization scale adjusted based on the layer depth:\n\nwhere $\\mathbf{x}_\\ell$ and $\\mathbf{y}_\\ell$ are input/output of layer $\\ell$, $\\mathcal{M}_\\ell$ is the sublayer module (like attention, FFN, or MoE). The RMSNorm gain, $\\gamma$, is a multiplicative factor applied after the RMS normalization, given by $\\bar{a}_i = \\gamma\\frac{a_i}{\\text{RMSNorm(a)}}$. In Arcee’s case, they initialize $\\gamma\\left(\\text{RMSNorm}_\\ell^{(1)}\\right)=1$ and $\\gamma\\left(\\text{RMSNorm}_\\ell^{(2)}\\right)=\\frac1{\\sqrt{L}}$. This depth-dependent scaling accounts for the fact that activations evolve differently across layers. The sandwich pattern (pre-norm and post-norm) provides additional stability, especially in very deep networks where gradient flow can be challenging.\n\nArcee also applies RMSNorm before the language modeling head stabilizes the final hidden states to ensure consistent output activation scales before they are transformed into token probabilities.\n\nThere are a few considerations that typically guide tokenizer design:\n\nLarger vocabularies can compress text more efficiently, but they come at the cost of a larger embedding matrix, which as mentioned in the embeddings section, can take up a sizable portion of the parameter count. For english-only models, 50k is often enough, while multilingual models need over 100k. There is an optimal size that exists since compression gains from larger vocabularies decrease exponentially.\n\nLarge models benefit from large vocabularies since the extra compression saves more on the forward pass (project to QKV, attention, and MLP) than the additional embedding tokens during softmax. For memory, larger vocab means fewer tokens, so a smaller KV cache.\n\nBPE (byte-pair encoding) still remains the de facto choice. Starting with tiny units (e.g. characters or bytes), the BPE algorithm repeatedly merges the most common adjacent pair into a new token. To evaluate a tokenizer’s performance, fertility is a common metric, measuring the average number of tokens needed to encode a word (alternatively, characters-to-tokens ratio or bytes-to-tokens ratio, but these have limitations due to word length variability and byte representations). Another is proportion of continued words, describing what percentage of words get split into multiple pieces. For both, smaller metrics indicate more efficient tokenizers.\n\nThere are many strong existing tokenizers, like GPT4’s tokenizer and Gemma3’s tokenizer. Often, using existing tokenizers is enough; only when we want to train for low-resource languages or have a different data mixture should we continue training our own tokenizer.\n\nChoosing optimizers and tuning hyperparameters is notoriously time-consuming and significantly impacts convergence speed and training stability. While we may be tempted to distill those from models of larger labs (albeit a useful prior), it may not fit the use case.\n\nDespite being invented over 10 years ago, AdamW still stands the test of time. Adam (adaptive momentum estimation) updates weights individually based on an exponential weighted average of gradients $g_t$ and an exponential weighted average of squared gradients $g_t^2$, along with weight decay (the “W”). The exponential moving averages provide adaptive learning rates per parameter: parameters with consistently large gradients get smaller effective learning rates (via the squared gradient term), while parameters with small or noisy gradients get larger effective learning rates. This adaptivity helps stabilize training and converge faster:\n\nwhere $\\theta$ denotes the model parameters, $\\alpha$ is the learning rate, $\\lambda$ is the weight decay coefficient, $g_t$ is the gradient at step $t$, $m_t$ and $v_t$ are the first and second moment estimates (exponentially weighted averages), $\\hat{m}_t$ and $\\hat{v}_t$ are bias-corrected versions, $\\beta_1$ and $\\beta_2$ are exponential decay rates for the moment estimates, and $\\epsilon$ is a small constant (typically $10^{-8}$) to prevent division by zero. Even for modern LLMs, the hyperparameters remain largely unchanged: weight decay factor $\\lambda=0.1$ or $\\lambda=0.01$, $\\beta_1=0.9$, and $\\beta_2=0.95$.\n\nUnlike AdamW which updates per-parameter, muon treats the weight matrix as a singular object and updates based on matrix-level operations. This approach reduces axis-aligned bias (where optimization favors certain coordinate directions) and encourages exploration of directions that would otherwise be suppressed. By considering the entire weight matrix structure rather than individual parameters, muon can better capture correlations between parameters:\n\nwhere $\\theta_t$ denotes the model parameters at step $t$, $\\mathcal{L}_t$ is the loss function, $g_t$ is the gradient matrix, $G_t$ is the normalized gradient matrix (typically $G_t = g_t / |g_t|$), $B_t$ is a momentum buffer matrix with $B_0=0$, $\\mu$ is the momentum coefficient, $\\eta$ is the learning rate, and $\\text{NewtonSchulz5}$ applies the odd function $f(x)=3.4445x-4.7750x^3+2.0315x^5$. This blog and this blog describe the algebra of it in more detail as well as why the coefficients are what they are. The Newton-Schulz iteration approximates the matrix sign function: we can estimate the SVD decompositions of $G=U \\Sigma V^\\top$ by $UV^\\top$, and $f(x)$ essentially replaces $\\Sigma$ because iteratively applying $f$ (i.e., $f \\circ f \\circ \\cdots f(x)$) converges to the sign function, which normalizes the singular values. This has the effect of reducing axis-aligned bias and encouraging exploration of directions that would otherwise be suppressed.\n\nMuon is more sample-efficient than AdamW, especially at large batch sizes where AdamW struggles. Some implementations, including Arcee’s Trinity Large, choose a hybrid approach: using muon for hidden layers while keeping AdamW for embedding and output layers. This decision stems from the different optimization dynamics these layers exhibit—embeddings and output projections benefit from per-parameter adaptive learning rates, while hidden layers capture more benefit from muon’s matrix-level structure awareness.\n\nBut since muon operates at the matrix level, applying NewtonSchulz requires access to the full gradient tensor. One method uses an overlapping round-robin scheme where each rank is responsible for gathering all gradient matrices corresponding to its index and applying muon locally. Since FSDP expects sharded gradients/updates, and every rank has its shard of the muon-updated gradient, then the optimizer step can proceed normally. However, this issues lots of overlapping collectives across many matrices which breaks at scale.\n\nThe alternative that Prime adapts is based on all-to-all collectives which does bulk permutation so that each rank temporarily owns full gradients for its matrices, runs muon, then bulk permutes them back. This may require padding since many tensors are packed into contiguous buffers which can change the size that’s expected. However, this requires fewer collectives and scales better.\n\nBuilding on Muon, Kimi K2 introduces MuonClip, a stabilization technique that prevents exploding attention logits, which is a common failure mode in large-scale training. Other strategies include logit soft-cap, which applies $\\tanh$ clipping to the pre-softmax logits, or QK-norm, which applies LayerNorm to the QK matrices. However, these lead to issues of the scaled dot-product exploding (making bounding too late) and distorted gradients around regions where the model is unstable in logit soft-cap, and key matrices are not materialized during inference (projected from a latent variable).\n\nFor each attention head $h$, consider $\\mathbf{Q}^h$, $\\mathbf{K}^h$, and $\\mathbf{V}^h$ (the query, key, and value matrices for head $h$). For a batch $B$ and input representation $\\mathbf{X}$, define the max logit as a per-head scalar to be the maximum input to softmax\n\nwhere $d$ is the dimension of the query/key vectors, $i$ and $j$ index positions in the sequence, and the $\\frac1{\\sqrt{d}}$ scaling factor matches the standard attention scaling. Set $S_\\text{max} = \\max_h S_\\text{max}^h$ (the maximum across all heads) and target threshold $\\tau$ (a hyperparameter controlling when clipping activates). The idea is to rescale $\\mathbf{W}_k^h$ and $\\mathbf{W}_q^h$ (the key and query projection weight matrices for head $h$) whenever $S_\\text{max}^h$ exceeds $\\tau$. Also, $\\gamma=\\min(1, \\frac{\\tau}{S_\\text{max}})$ (the global clipping factor), one approach is to clip all heads simultaneously by\n\nwhere the $\\gamma$ exponentials enforce multiplicative weight decay for $\\mathbf{Q}^h \\mathbf{K}^{h\\top}$; commonly, $\\alpha=0.5$ to ensure equal scaling to queries and keys. However, not all heads exhibit exploding logits, which motivates a per-head clipping based on $\\gamma_h = \\min(1, \\frac{\\tau}{S_\\text{max}^h})$, which is more straightforward for MHA but less for MLA. The challenge with MLA is that keys are projected from a latent variable rather than materialized directly, so clipping must be applied to the latent-to-key projection weights and the latent variable itself. They apply clipping only on $\\mathbf{q}^C$ and $\\mathbf{k}^C$ (head-specific components) scaled by $\\sqrt{\\gamma_h}$, $\\mathbf{q}^R$ (head-specific rotary) scaled by $\\gamma_h$, and $\\mathbf{Q}^R$ (shared rotary). Besides that, the main muon algorithm is modified to match Adam RMS and enable weight decay. For each weight $\\mathbf{W} \\in \\mathbb{R}^{n \\times m}$:\n\nwhere $n$ and $m$ are the dimensions of the weight matrix $\\mathbf{W}$, $\\sqrt{\\max(n,m)} \\cdot 0.2$ is a scaling factor that adapts the update magnitude to the matrix size (matching Adam’s RMS scaling behavior), and other symbols follow the same definitions as in the standard Muon algorithm. The weight decay term $(1-\\eta \\lambda)$ is applied multiplicatively before the gradient update.\n\nFigure 6: Left: a mid-scale training run on a 9B active, 53B total MoE where attention logits diverge quickly. Right: maximum logits for KimiK2 with MuonClip and $\\tau=100$, where max logits eventually decays to a stable range after ~30% of the training steps. From Kimi K2.\n\nLearning rates have their own life cycle: they warmup (typically 1%-5% of training steps for short trainings, but large labs fix the warmup steps) from zero to avoid chaos, then anneal after settling into a good minimum. Cosine annealing was the go-to scheduler, but it’s also inflexible due to the cosine period needing to match the total training duration. Alternatives include warmup-stable-decay (WSD) and multi-step; in the last x% of tokens, the former linearly decays the learning rate whereas multi-step does discrete drops. For WSD, typically 10-20% is allocated for the decay phase, matching cosine annealing; in multi-step, 80/10/10 also matches cosine annealing while 70/15/15 and 60/20/20 can outperform it. Deepseek-v3 used cosine annealing between the decay drops and added a constant phase before the final sharp step.\n\nFigure 7: Comparison of learning rate schedules: cosine annealing, WSD, and multi-step. From Hugging Face.\n\nHugging Face’s ablations (on their 1B model) showed that WSD tended to underperform cosine annealing before WSD’s decay began, but once it entered its decay phase, WSD showed nearly linear improvement in both loss and eval metrics, which allowed it to catch up to cosine annealing by the end. After running further ablations on the learning rate, the Hugging Face team settled on 2e-4; increasing led to potential increased risk of instability during long training runs. Kimi K2 also uses WSD: the first 10T were trained with 2e-4 learning rate after a 500 step warm up, then 5.5T tokens with cosine decay from 2e-4 to 2e-5.\n\nWSD schedule especially helps with ablations since it does not require restarting the same run for different token counts, since we can retrain only the end portions (learning rate decay) while maintaining the front portion.\n\nThere is a critical batch size: too small and we may be underutilizing compute, but too large and the model needs more tokens to reach the same loss. Still, larger batch sizes give more efficient gradient estimations, and are preferred.\n\nA useful proxy is that for optimizers like AdamW or Muon, if the batch size increases by a factor of $k$ then the learning rate should scale up by $\\sqrt{k}$. Intuitively, larger batches provide more stable gradient estimates (lower variance), so we can afford larger step sizes. Mathematically, the covariance shrinks by a factor of $k$, and based on the SGD parameter update $\\Delta w = -\\eta g_B$, we have $\\text{Var}(\\Delta w) \\sim \\eta^2 \\frac{\\Sigma}{B}$ where $B$ is the original batch size. To maintain the same update variance, we need $\\eta \\sim \\sqrt{k}$.\n\nAs training progresses, the critical batch size grows. Initially, since the model is making large updates, $\\lvert \\lvert g \\rvert \\rvert^2$ is large so the model should have a small critical batch size. After the model stabilizes, larger batches become more effective. This motivates the idea of batch size warmup.\n\nImbalanced minibatches can arise when sequence packing or data distribution creates batches with highly variable sequence lengths or domain compositions, which can cause gradient variance that destabilizes training; this is especially true when certain experts or model components receive disproportionately many or few tokens.\n\nArcee introduces random sequential document buffer (RSDB) to reduce intra-batch correlation. After tokenizing a document, it works by loading the token sequence as an entry in the RSDB with a read head at index 0; this is repeated until the RSDB is full. From a randomly sampled index in a randomly sampled document from the RSDB, tokens are read based on the read head and the index and added to a separate sequence buffer. Read head positions are updated, and if the sequence buffer is full, we return; otherwise, we randomly select another document index and continue to read tokens into the sequence buffer, repeating until the sequence buffer is full.\n\nThe internal buffer size (in Trinity Large: 8192 per GPU) is set to twice the user-specified buffer value and refilled when the buffer reaches the user-specified value (in Trinity Large: 4096 per GPU) or when old documents need to be purged/new documents can be loaded. Arcee found that this optimization significantly improved dataloader performance.\n\nScaling laws (e.g. Chinchilla scaling laws) provide a useful proxy for determining how aggressively/conservatively to update hyperparameters as model size scales.\n\nFirst, $C \\approx 6 \\cdot N \\cdot D$ where $C$ is the compute budget measured in FLOPs, N is the number of parameters, and $D$ is the number of training tokens. The 6 is derived from empirical estimates for the number of FLOPs per parameter.\n\nFigure 8: Scaling curves of batch size and learning rate. From DeepSeek.\n\nInitially, scaling laws indicated that language model size was the main constraint, leading to a GPT-3 model with 175B parameters but only trained on 300B tokens. A re-derivation found that training duration could improve gains more than size; they found that compute-optimal training of GPT-3 should have consumed 3.7T tokens.\n\nHowever, scaling laws are almost never religiously followed. Recently, labs have been “overtraining” models beyond the training durations suggested by scaling laws (e.g. Qwen 3 being trained on 36T tokens). Moreover, “compute-optimal” scaling laws don’t account for larger models being more expensive after training due to inference. To that end, Hugging Face decided to train on 11T tokens on a 3B model. For comparison, Kimi K2’s 1T model comprised of 15.5T pre-training tokens.\n\nWhile general scaling laws provide guidance, Kimi K2’s scaling law analysis revealed model-specific insights. They showed that an increase in sparsity, the ratio of total number of experts to the number of activated experts, yields substantial performance improvements for fixed FLOPs, so they increase the number of MoE experts to 384 (256 in DeepSeek-V3) while decreasing attention heads to 64 (128 in DeepSeek-V3) to reduce computational overhead during inference. They settle on a sparsity of 48, activating 8 out of 384 experts and found that decreasing the attention heads from 128 to 64 sacrificed a validation loss ranging from 0.5% to 1.2%, but a 45% decrease in inference FLOPs.\n\nEven with the perfect architecture, a model’s performance is still heavily dependent on its training data; no amount of compute or optimization can compensate for training on the wrong content. To this end, it’s about assembling the right data mixture, balancing training objectives and tuning data proportions. This is particularly difficult since across competing objectives, for a fixed compute budget, increasing one proportion necessarily decreases another, hurting performance.\n\nThere already exist large corpora of pre-training datasets like FineWeb2 and The Pile. However, there are still a plethora of information gaps, so recent models additionally rely on specialized pretraining datasets for domains like math and coding.\n\nOne consideration is data quality. Of course, training on the highest quality data possible is preferable. But for a training budget of $X$ tokens, because high quality data is limited, only filtering for it would lead to repeated data, which can be harmful. So, an ideal mixture includes both higher and lower quality data.\n\nAnother consideration is model safety. For gpt-oss-120b, OpenAI addresses this by filtering the data for harmful content in pre-training, with an emphasis on hazardous biosecurity knowledge. They use CBRN (chemical, biological, radiological, and nuclear) pre-training filters that were used in GPT-4o.\n\nMulti-stage training, the idea of evolving the data mixture as training progresses, can better maximize both high-quality and lower-quality data compared to a static mixture because a LM’s final behavior is heavily dictated by the data it sees at the end of training. So, this motivates the strategy of saving the higher quality data towards the end. This introduces another variable of when to begin changing mixtures, and a general principle to performance-driven intervention: if a benchmark begins to plateau, it’s a signal to introduce high-quality data for that domain.\n\nWhile architectural ablations are done on smaller models (e.g. on 1B models to train for 3B models), data mixture ablations are done at scale because larger models have much larger capacities to understand a variety of domains. Moreover, annealing ablations are done on checkpoints of the main run (like 7T out of 11T tokens) to determine what datasets to introduce when.\n\nTo determine optimal data proportions, recent models often use a validation loss or a holdout loss to minimize based on evaluation objectives and data domains. However, some of these methods tend to converge toward distributions that mirror the dataset size distribution, and they don’t outperform careful manual ablations.\n\nToken efficiency is how much performance improvement is achieved per token consumed during training. This can be improved via better token utility, the effective learning signal each token contributes; this motivates finding the optimal balance of high-quality tokens, since they should be maximally leveraged but also limited to prevent overfitting and reduced generalization.\n\nKimi K2 uses data rephrasing in knowledge and math domains. For knowledge, this comes in the form of style and perspective-diverse prompting to rephrase the texts, chunk-wise autoregressive generation to gradually build a rephrased version of long documents, and fidelity verification to ensure semantic alignment. In the main training run, each corpus is rephrased at most twice. For math, diversity is increased via rephrasing into a “learning-note style” and translation into other languages.\n\nHugging Face’s goal was to build a multi-lingual model that also excels on math and coding. In stage 1 of their multi-stage training, they use a 75/12/10/3 split among english web data, multilingual web data, code data, and math data.\n\nFor new stages (using a checkpoint at around 7T out of the total 11T tokens), they use a 40/60 split between the baseline mixture and the new dataset. SmolLM3 has three stages: 8T tokens @ 4k context for base training, 2T tokens @ 4k context for high-quality injection, and 1.1T tokens @4k context a reasoning/Q&A stage.\n\nUsing data from DCLM and FineWeb, Nous first performs semantic deduplication using embeddings at a cosine similarity of 0.7, and then uses an LLM-as-judge to filter out incomplete or ill-formatted messages. Then, they process pre-training data through DataForge, a graph-based synthetic data generator, which allows for large and complex structures. By taking a random walk through a directed acyclic graph where nodes implement a mapping from struct $\\to$ struct such that if there is an edge from node $A$ to node $B$, the postconditions guaranteed by $A$ must satisfy the preconditions of $B$. QA pairs are generated using this workflow with intermediary transformations into other mediums (e.g. a wikipedia article into a rap song), question generation and then questions/answers annotations using an LLM-as-judge to grade the instruction and response. Also, to find a covering set of data-scarce domains of special interest, they recursively (depth-first-search) generate a taxonomy of subdomains where the leaves are prompts and the LLM enumerates $n$ subdomains to form a partition.\n\nThe DataForge-generated data is used in both pre-training and post-training stages, with specific details provided in the post-training data section below.\n\nMid-training is the intermediary step between pre-training and post-training where the base model is trained further on a large amount of domain-specific tokens, especially shaping the model to focus on common core skills like coding or reasoning. Often-times, the decision to mid-train is only made after initial SFT experiments are run because they may reveal performance gaps that indicate the need to mid-train on certain domains. But if the goal is to elicit shallow capabilities like style or conversation, the compute is better spent in post-training.\n\nSome recipes include an additional long context stage; for example, Qwen3 first trained on 30T tokens at 4k context, then a reasoning stage with 5T higher-quality tokens mainly on STEM and coding, and finally a long context stage at 32k context length.\n\nSmolLM3 also does this, but instead of scaling from 4k to 128k directly, they sequentially scale from 4k to 32k to 64k to 128k, which allows the model to adapt at each length before pushing the context length further. Upsampling long context documents like web articles or books improve long context, but Hugging Face didn’t observe improvement; they hypothesize that this is because their baseline mixture already includes long documents using RNoPE.\n\nTo go from 4k to 32k and later to 64k, they use RoPE ABF and increase the base frequency to 2M and 5M, respectively. Base frequencies like 10M further improved slightly on RULER, long context benchmark, but it hurt short context tasks like GSM8k, so they were disregarded. To reach 128k, they found that using YARN from the 64k checkpoint (instead of using a four-fold increase from 32k) produced better performance, which confirms the hypothesis that training closer to the desired inference length benefits performance.\n\nKimi K2 decays learning rate from 2e-5 to 7e-6, training on 400B tokens with 4k sequence length, then 60B tokens with a 32k sequence length. To extend to 128k, they use YARN.\n\nWhile the mid-training data usually comes from web data, another powerful approach is to use distilled reasoning tokens from a better model, as Phi-4-Mini-Reasoning did from DeepSeek-R1. When applied to the base model, distilled mid-training increased benchmark scores like AIM24 by 3x, MATH-500 by 11 points, and GPQA-D by almost 6 points. SmolLM3 also does distilled mid-training. They considered datasets including reasoning tokens from DeepSeek-R1 (4M samples) and QwQ-32B (1.2M samples) but decide to delay using the Mixture of Thoughts dataset until the final SFT mix. They found that it almost always makes sense to perform some amount of mid-training if the base model hasn’t already seen lots of reasoning data during pre-training, because they noticed that /no_think reasoning mode also had improvements on reasoning benchmarks.\n\nGiven today’s standards of LLMs as coding agents and assistants that can reason, there are four broad classes of evals that researchers care about:\n\nThese evals test the following:\n\nTo prevent overfitting, evals that encapsulate robustness or adaptability, like GSMPlus which perturbs problems from GSM8k, are also included. Another way is using interval evals or vibe evaluations/arenas, such as manually probing the model’s behavior. Other tips include using small subsets to accelerate evals (especially if there’s correlation with a larger eval), fixing the LLM-as-judge model (if the eval requires it), treat anything used during ablations as validation, use avg@k accuracy, and try not to (don’t) benchmax!\n\nIt’s first worth mentioning that Intellect-3 is a 106B parameter MoE (12B activate) post-trained on top of GLM-4.5-Air base model from Z.ai, and that they have their own post-training stack including prime-rl, an open framework for large-scale asynchronous RL, verifiers library for training and evals from their Environments Hub, sandbox code execution and compute orchestration.\n\nIntegrating with the Environments Hub, Prime trains on a diverse and challenging mix of environments designed to improve coding and reasoning capabilities. For math, they design an environment with long CoT reasoning in mind, consisting of 21.2K challenging math problems from Skywork-OR1, Acereason-Math, DAPO, and ORZ-Hard, all of which are curated datasets derived from AIME, NuminaMath, Tulu3 math, and others which test difficult math questions from multiple choice to proofs to those involving figures. Even using verifiers, there were a non-trivial amount of false negatives, so they additionally use opencompass/CompassVerifier-7B as a LLM-judge verifier. For science (mainly physics, chemistry, and biology), they filter 29.3K challenging problems from MegaScience while also using LLM-judge verification and standard math verifiers. For logic (games like Sudoku or Minesweeper), 11.6K problems and verifiers were adapted from SynLogic.\n\nFor code, they primarily use their Synthetic-2 dataset along with Prime Sandboxes to verify solutions. They also develop two SWE environments that support scaffolding for common formats like R2E-Gym, SWE-smith, and Multi-SWE-bench to fix issues within a Github project when equipped to Bash commands and edit tooling. Also, the maximum number of turns for the agent is set at 200.\n\nPrime also focuses on its deep research capabilities via their web search environment, which provides the model with a set of search tools. The environment tasks the model with answering questions from the dataset using tools and is rewarded either 1 or 0 using z-AI’s DeepDive dataset, with 1K samples for SFT trajectory generation and 2.2K samples for RL. When tested in Qwen/Qwen3-4B-Instruct-2507, 26 steps of SFT with batch size of 34 followed by 120 steps of RL at a group size of 16 and batch size of 512 was enough to reach mean reward of 0.7.\n\nThey use 300k prompts, mostly STEM and coding from WebInstruct-Verified, rSTAR-Coder, and DeepMath-103k and apply deduplicating and filtering for prompts with >2k characters.\n\nNous rejection samples against ~1k task-specific verifiers using Atropos. Some environments used to generate the dataset include\n\nA critical capability that Kimi K2 chooses to focus on is tool use. While benchmarks like $\\tau$-bench and ACEBench exist, it’s often difficult to construct real-world environments at scale due to cost, complexity, privacy, and accessibility. Kimi K2 builds off of ACEBench’s data synthesis framework to simulate real-world tool-use scenarios at scale:\n\nUsing 3k+ real MCP tools from Github and 20k synthetic tools generated hierarchically in domains like financial trading, software applications, and robot control. Diversification among agents is ensured via the combination of distinct system prompts with distinct tool combinations, and tasks are graded using an explicit rubric with an LLM judge.\n\nFor RL, Kimi K2’s treatment for math, STEM, and logical tasks remains similar to those of other models. Coding and software engineering comes largely from competition-level programming problems and PRs/issues from GitHub. For instruction following, they use two verification mechanisms: deterministic evaluation via code interpreters for verifiable outputs and LLM-as-judge evaluations for non-verifiable outputs. The data was constructed using expert-crafted prompts and rubrics, agentic instruction augmentation inspired by AutoIF, and a fine-tuned model specialized for generating additional instructions probing specific failure modes or edge cases.\n\nA few important considerations for designing/picking a good chat template include system role customizability, tool calling, reasoning, and compatibility with inference engines like vLLM or SGLang. Qwen3 and GPT-OSS satisfy all criteria, and Qwen3 is designed for hybrid reasoning.\n\nIn SmolLM3, despite also being designed for hybrid reasoning, they discard the reasoning content for all but the final turn in the conversation to avoid blowing up the context during inference, but for training, it’s important to retain the reasoning tokens to condition the model properly. So, Hugging Face orchestrates their own chat template, satisfying all criteria. Vibe tests initially revealed a bug of not passing in the custom instructions into their custom template, but this was quickly patched.\n\nWhile deriving inspiration from the Qwen3 template, Intellect-3 always reasons (not hybrid) by proxy of being dominantly trained on reasoning-only SFT traces; they use qwen3_coder tool call parser and deepseek_r1 reasoning parser to ensure reasoning chains are consistently represented.\n\ngpt-oss-120b uses the harmony chat template, which introduces “channels” that determine the visibility of each message. For example, final for answers shown to the user, commentary for tool calling, and analysis for CoT tokens. This allows the model to interleave tool calls with CoT.\n\nHermes 4 adapts Llama 3’s chat template by changing the assistant to a first-person identifier after identifying the sensitivity to the token used for the assistant’s turn:\n\nThis results in markedly different behaviors, which is explored more in “behaviors and latent capabilities” subsection of “behaviors and safety” section.\n\nDeepSeek-R1-Zero’s chat template looks very similar to others, but additionally includes $\\mathtt{}$ tags to provide the final answer.\n\nMost post-training pipelines start with supervised fine-tuning (SFT) because it’s cheap compared to RL, stable due to insensitivity to reward design and hyperparameters, and gives a strong baseline off of the base model. Usually, base models are too unrefined to benefit from more advanced post-training methods. SFT often comes in the form of distillation from stronger models. Strong models may suffer from success and skip the SFT stage because there are no stronger models to distill from (such is the case with DeepSeek R1-Zero).\n\nDataset curation for SFT is important; datasets might seem great on paper, but models trained on those datasets may end up overindexing on certain domains, such as science. To this end, Hugging Face curated a data mixture with ~100k examples and 76.1M tokens, mostly consisting of instruction following, reasoning, and steerability for both think and non-think modes. Importantly, data should be paired across modes because otherwise, there is not an indication of when to give a concise answer or use extended reasoning.\n\nFor training, there are other considerations as well: full finetuning vs more parameter efficient methods like LoRA or QLoRA, specialized kernels like FlashAttention (which reduces memory usage by recomputing attention on-the-fly during the backward pass, trading compute for memory) or the likes of SonicMoE for more efficient compute usage, masking the loss for only assistant tokens, the type of parallelism needed, learning rate tuning, and sequence length tuning to match the distribution of data to speed up training (more useful for larger datasets).\n\nCute cross-entropy kernel (CCE) is a memory-efficient CUDA kernel for computing cross-entropy loss. Instead of materializing the full logit matrix in global memory, CCE computes only the logit for the correct token and evaluates the log-sum-exp over all vocabulary items on-the-fly using faster memory tiers, dramatically reducing memory consumption. The kernel leverages the sparsity of softmax by skipping gradient computation for elements with negligible contributions below numerical precision. This makes it particularly valuable for models with large vocabularies.\n\nIn Intellect-3, Prime splits SFT into two stages: general reasoning SFT and agentic SFT. In the first, they use datasets consisting of math, code, science, tooling, chat, and instruction splits from Nemotron’s post-training dataset and AM-DeepSeek-R1-0528-Distilled for a total of 9.9B tokens. In the second stage, they target agentic behavior, tool use, and long-horizon control (gpt-oss-120b also targets agentic behavior and tool use), using a mix of open-source agentic datasets like SWE-Swiss and synthetically-generate datasets from the Environments Hub using DeepSeek-R1. Besides serving the purpose of fine-tuning for agentic behavior, this stage also has the effect of pushing the model toward longer effective context lengths. Using context parallelism, they scaled from a 65K context window to 98K.\n\nIn Hermes 4, they also do two stages of SFT, both around reasoning. They noted that despite training on sequences at most 16k tokens in length, the reasoning lengths frequently exceed 41k tokens on reasoning tasks. So, they do a second stage to teach the model to generate the closing $\\mathtt{</think>}$ tags at 30k tokens, their budget. This insertion at a fixed token count allows the model to learn a counting behavior (“when you reach $N$ tokens, stop”) while ensuring that the model’s own distribution doesn’t change significantly. This also avoids the problem of model collapse when recursive training on full, self-generated outputs leads to distribution narrowing and quality degradation.\n\nThe Hugging Face team found issues in generalising single-turn reasoning data to multi-turn data, stemming from the difficulty in differentiating /think and /no_think tags between turns. So, they constructed a new dataset, IFThink, using Qwen3-32B that augmented single-turn instructions into multi-turn exchanges with verifiable instructions and reasoning traces; this dramatically improved multi-turn reasoning.\n\nMasking user turns is another design choice, since otherwise loss is computed on user queries as well, sacrificing producing high-quality assistant responses for predicting user queries. In practice, masking doesn’t have a huge impact on downstream evals, but still yielded improvements by a few points in most cases.\n\nSequence packing is another choice that improves training efficiency. The idea is similar to intra-document masking where sequences are packed into a batch so as to not waste padding compute via excessive padding tokens, but with the additional constraint of minimizing truncation of documents across batch boundaries.\n\nFigure 9: Comparison of sequence packing strategies. From Hugging Face.\n\nIn the image, the last packing method uses the best-fit decreasing (implemented in TRL), where each sequence is placed in the batch that minimizes the remaining space after insertion. Another method, which Hermes-4 uses, is first-fit decreasing, which places a sequence in the first batch that has enough remaining space, which achieves $>99.9\\%$ batch efficiency.\n\nDespite yielding up to a 33x tokens/batch/optimization step, for a fixed token budget, packing alters training dynamics since the more data means fewer gradient updates. This especially hurts small datasets where each sample matters more. An effective batch size of 128 hurt evals like IFEval by up to 10%; for effective batch sizes larger than 32, there was an average drop in performance (for SmolLM3 and the dataset). But for large datasets, packing is almost always beneficial.\n\nThe learning rate for SFT is usually an order of magnitude smaller than that for pre-training since the model has already learned rich representations, and aggressive updates can lead to catastrophic forgetting. And because SFT runtime is so short compared to pre-training, it makes sense to do full learning rate sweeps. For SmolLM3, learning rates of 3e-6 or 1e-5 worked best. When packing is enabled, it’s safer to decrease the learning rate further due to the larger effective batch size and getting fewer updates for the same token budget.\n\nOnce a good data mixture is identified and hyperparameters are tuned, training on more than one epoch (what was usually done in ablations) also leads to increased performance by a few percentage points; on LiveCodeBench v4, performance nearly doubled from epoch two to three.\n\nAn interesting idea explored is whether the optimizers for pre/post-training should be the same. AdamW remains the default choice for both pre/post-training, and when tested with Muon, using the same optimizer still yielded the best performance.\n\nBecause SFT is fundamentally a form of imitation learning, extremely large SFT datasets can be redundant due to diminishing gains or failure modes that aren’t encapsulated in the data. Another useful signal is preference, i.e. which response, A or B, is preferred and enables model performance to scale beyond the limits of SFT alone. Also, less data is needed for preference optimization than SFT since the starting point is already strong.\n\nFor generating preference datasets, there are a few methods:\n\nWhile preference optimization is generally thought as a medium to improve helpfulness or alignment, it can also teach models to reason better, like using strong-vs-weak preferences.\n\nThere are typically three hyperparameters that affect training dynamics:\n\nBesides vanilla DPO (direct preference optimization), researchers have explored a variety of alternatives:\n\nHugging Face found that APO-zero had the best overall out-of-domain performance.\n\nSFT and PO can hit ceilings because fundamentally, they optimize to produce outputs that look like the dataset and PO is often off-policy and weak at multi-step credit assignment. RL (reinforcement learning) helps by providing a reward signal through interaction with an environment. Verifiers can automatically check correctness and provide those reward signals, and objectives can be optimized for beyond preference labels.\n\nIn RLHF (RL from human feedback), human comparisons are provided, and a reward model is trained to predict the human preference signal. Then, the policy is fine-tuned with RL to maximize the learned reward. This way, RLHF does on-policy optimization since it does rollouts using the current policy used in training and updates based on the reward given by the reward model. This also allows RLHF to discover behaviors not present in the preference dataset.\n\nIn RLVR (RL with verifiable rewards), popularised by DeepSeek-R1, verifiers check whether a model’s output matches criteria (e.g. whether it produced the correct math answer or passed all code unit tests) to generate reward signals, making it more scalable and objective. Then, the policy is fine-tuned to produce more verifiably-correct outputs. RLVR is particularly valuable when reward drift is a concern (verifiers provide very stable signals compared to learned reward models), when KL control is needed to prevent policy collapse, and when addressing stale-policy artifacts in multi-step reasoning tasks.\n\nAlthough policy optimization algorithms are commonly on-policy, like GRPO, in practice, to maximize throughput, they may actually be slightly off-policy. For example, in GRPO, without freezing the policy, generating multiple rollout batches and doing optimizer updates sequentially makes only the first batch on-policy and all subsequent batches off-policy; this is known as in-flight updates. In-flight updates matter most when throughput is critical (e.g., large-scale RL training), when reward drift could accumulate from stale policies, when KL divergence between inference and training policies needs careful monitoring, and when long rollouts span multiple policy updates. The tradeoff is between training efficiency and policy consistency; techniques like importance sampling clipping (as in IcePop) help mitigate the off-policy bias.\n\nFigure 10: Comparison of conventional RL and in-flight updating. From Pipeline RL paper.\n\nIn the context of Intellect-3, which uses a CPU orchestrator between two clusters (one for training and one for inference), the orchestrator continuously polls the trainer to update the inference pool once a new policy is available, and the inference pool temporarily halts generation to update the weights, then continues with rollouts. In this way, a long rollout could be generated by multiple policies, but they limit this by a max_off_policy_steps parameter to limit policy drift. Also, they implement IcePop to stabilize MoE training:\n\nwhere $\\mathcal{M}(k)=k$ if $k \\in [\\alpha, \\beta]$ and $0$ otherwise. The purpose of $\\mathcal{M}$ is to account for the off-policy nature between the training policy and the inference policy such that they don’t diverge significantly for each token. When the importance weight $k$ falls outside $[\\alpha, \\beta]$, $\\mathcal{M}$ clips it to 0, effectively ignoring that token’s contribution to the gradient. This is the idea behind importance sampling, where rollouts come from the inference policy, but we are optimizing for the training policy. Prime uses the default $\\alpha = 0.5, \\beta = 5$. $\\alpha$ and $\\beta$ need not be symmetric in the multiplicative sense. One reason for this is under the rare instance when $\\pi_\\text{infer}$ is small (large $k$), a tighter $\\beta$ would clip the high-entropy tokens, which would make learning dynamics worse.\n\nKimi K2 adapts a different policy optimization approach from their previous model K1.5:\n\nwhere $\\overline{r}(x)= \\frac1{K} \\sum_{i=1}^K r(x,y_i)$ is the mean reward of sampled responses and $\\tau > 0$ is a regularization parameter for stable learning, akin to KL divergence.\n\nAnother consideration is PTX loss: pre-training cross-entropy loss. During joint RL training, the model can catastrophically forget valuable, high-quality data. So, they curate a dataset using hand-selected, high-quality samples and integrate them into the RL objective via the PTX loss. The advantages are twofold: high-quality data can be leveraged, and the risk of overfitting to the tasks present during RL can be mitigated, which leads to better generalization.\n\nTo balance exploration and exploitation throughout training, they implement temperature decay. For tasks like creative writing and complex reasoning, high temperatures during the initial stages is important to generate diverse and innovative responses; this prevents premature convergence to local minima and facilitates the discovery of effective strategies. At later stages, the temperature is decayed (following a schedule) so that there is not excessive randomness, and that it doesn’t compromise the reliability/consistency of the model’s outputs.\n\nDeepSeek-R1-Zero stands out as an exception compared to other models because it demonstrates that RL can be effective even without supervised fine-tuning. It cold-starts RL (specifically, GRPO to save training costs; they use 10.4k steps, batch size of 512, reference policy replacement every 400 steps along with lr as 3e-6, KL coefficient as 0.001) on reasoning tasks without any supervised data. The reward system uses two types of rewards: accuracy rewards based on correctness of the response and format rewards that enforce the model to put its thinking process between thinking tags. It obtains robust reasoning capabilities using pure RL, which validates the ability to learn reasoning and generalize effectively. Moreover, behaviors including reflection (re-evaluating previous steps) and exploring alternative approaches to problem-solving emerge, which further enhances reasoning. The count of reflective words such as “wait” or “mistake” rises 5-7-fold compared to the start of training. Moreover, this reflective behavior appeared relatively suddenly: between steps 4k-7k, there was only occasional usage, but after step 8k, it exhibited significant spikes.\n\nFigure 11: The multi-stage pipeline of DeepSeek-R1 with intermediate checkpoints DeepSeek-R1 Dev1, Dev2, and Dev3. From DeepSeek.\n\nFor DeepSeek-R1, they collect thousands of long CoT data to finetune the DeepSeek-V3-Base as the starting point for RL. From DeepSeek-R1-Zero, they learned that readability was an issue: responses sometimes mixed multiple languages or lacked markdown formatting for highlighting answers. They remedy the former by introducing a language consistency reward (portion of target language words in the CoT), and the latter by designing a readable pattern that includes a summary at the end of each response. They also perform a second RL stage aimed at improving the model’s helpfulness and harmlessness while retaining reasoning capabilities.\n\nFor helpfulness, they focus on emphasizing the utility and relevance of the final summary. To generate preference pairs, they query DeepSeek-V3 four times and randomly assign responses as either Response A or Response B; they then average the independent judgments and retain pairs where the score difference is sufficiently large and use a pairwise loss to define the objective. For harmlessness, they evaluate the response to identify and mitigate any potential risk, biases, or harmful content. Using a dataset with model-generated responses annotated as “safe” or “unsafe” according to safety guidelines, the reward model is trained using a point-wise methodology to distinguish between safe/unsafe responses.\n\nImpressively, they found that distilling DeepSeek-R1’s outputs into smaller models like Qwen-32B significantly improved reasoning capabilities, even compared to large-scale RL, which further requires significantly more compute. Moreover, it shows that even while distillation strategies are effective and economical, we will increasingly require more powerful base models and larger-scale RL.\n\nFigure 12: Comparison of DeepSeek-R1-distilled and RL Models on Reasoning-Related Benchmarks. From DeepSeek.\n\nBeyond the main training pipeline, DeepSeek’s appendix documents additional considerations that influenced their design choices:\n\nThe goal of RLVR on hybrid reasoning models is to improve reasoning capabilities without extending the token count too radically. For /no_think, naively applying GRPO can lead to reward hacking since the model begins to emit longer CoT (shifting towards /think); as such, both reward and token length increase. SmolLM3 observed this and found that RLVRed /no_think traces showed cognitive behaviors like “Wait, …” associated with reasoning models.\n\nThis can be mitigated via an overlong completion penalty which penalizes completions over a certain length, which is a function parametrized by a soft punishment threshold and a hard punishment threshold/max completion length. Penalty increases from the soft to the hard threshold, and past the latter, the punishment is -1 (effective reward = 0).\n\nFor /no_think, SmolLM3 decided on a length penalty in the range of 2.5k-3k that balanced improvement in performance and increase in response length. However, doing RL jointly on hybrid reasoning models is difficult since it requires separate length penalties, whose interplay can cause instability. This is also why labs release instruct and reasoning variants separately.\n\nKimi K2 uses a self-critique rubric reward mechanism, where the model evaluates its own outputs to generate preference signals. The K2 actor generates $k$ rollouts, and the K2 critic ranks all results by performing pairwise evaluations against a combination of rubrics; these combine core rubrics (fundamental values) and prescriptive rubrics (aimed to eliminate reward hacking), and human-annotated rubrics (for specific instructional contexts).\n\nThe critic model is refined using verifiable signals, and this process of transfer learning grounds its more subjective judgments in verifiable data. This should allow the critic to recalibrate its evaluation standard in lockstep with the policy’s evolution.\n\nTo RL effectively, curriculum learning is another effective way which gradually exposes the model to progressively difficult problems. First, problems are sorted into difficulty pools (such as easy, medium, and hard) based on the problem’s observed solve rate; In Intellect-3 for math and coding, this was done via querying Qwen/Qwen3-4B-Thinking-2507 over eight generations per problem while for science and logic, they queried the same model 16 times. Then, during each stage, they maintain a balanced curriculum that avoids training with trivially easy or overly difficult problems which don’t give meaningful learning signal (and also helps maintain gradients in GRPO). In Kimi K2, this was done by using the SFT model’s pass@k accuracy.\n\nOne alternative is online DPO (see “On policy with grading” in the preference optimization section). Another is on-policy distillation. Instead of preferences, the signal comes from a stronger teacher model, where the student samples responses at every training step and the KL divergence between the student/teacher logits provides the learning signal. That way, the student can continuously learn from the teacher. Also, on-policy distillation is much cheaper than GRPO since instead of sampling multiple rollouts per prompt, we only sample one, which is graded by the teacher in a single forward-backward pass; its performance boost, as the Qwen3 tech report notes, can be larger across the board as well. One limiting factor is that the student and the teacher must share the same tokenizer, and Hugging Face’s General On-Policy Logit Distillation (GOLD) allows any teacher to be distilled into any student.\n\nThinking Machine’s blog further showed that on-policy distillation mitigates catastrophic forgetting, where a model post-trained on a new model regresses on other, previous domains. Specifically, they found that mid-training 70% and with on-policy distillation can achieve close to the best performance of a model and its mid-trained version, effectively restoring behavior with cheap distillation.\n\nGiven these aforementioned algorithms, choosing between them can be hard; Hugging Face aptly describes it:\n\nAnd for DPO (semi-online and online), it is also possible to match GRPO using far less compute. Specifically, they found that semi-online DPO (with syncing between the trainer and the generator every 100 steps) was generally the best compared to semi-online DPO with sync every 10 steps, online DPO, and GRPO.\n\nDeepSeek shares other experimental methods when developing DeepSeek-R1 that ultimately failed. Monte Carlo Tree Search (MCTS), inspired by AlphaGo and AlphaZero, was implemented to test enhancing test-time compute scalability. This breaks answers into smaller parts to allow the model to explore the solution space systematically. To do this, they prompted the model to generate tags corresponding to reasoning steps necessary. The problem is that token generation exists in an exponentially larger search space compared to chess. So, they set a max extension limit for each node, but this leads to the model getting stuck in local optima. Moreover, training a fine-grained value model is difficult, also due to complexities of token generation.\n\nThey also explored process reward models, which rewards intermediate thoughts in multi-step tasks. DeepSeek acknowledges three limitations: defining a fine-grained step in general reasoning is difficult, determining whether the current intermediate step is difficult (LLM-as-judge might not yield satisfactory results), and it leads to reward hacking because the model would optimize for the appearance of good reasoning without doing the underlying work.\n\nDuring post-training, they perform an additional stage of RL to reward answers that comply with OpenAI’s policy against unsafe prompts. Because all of these models have open model weights, then one worry is that malicious parties can enhance the model’s harmful capabilities. By running Preparedness evaluations on gpt-oss-120b, OpenAI confirmed that the model doesn’t achieve threshold for high capability in biological/chemical capability, cyber capability, and AI self-improvement.\n\nThey also tested whether adversarial actors could fine-tune gpt-oss-120b to reach high capability in the aforementioned domains. They simulated the attack by either:\n\nUpon review, their safety advisory group concluded that even with robust fine-tuning, gpt-oss-120b still couldn’t reach high capability in the aforementioned domains. Also, they determined whether releasing the model could advance the frontier of biological capabilities, including evaluations like virology and tacit knowledge, in open foundation models (which also increases risk) and found that there were other open weight models at or near gpt-oss-120b, so they decided that releasing has low impact on the frontier.\n\nOpenAI also evaluated safety performance using other indicators:\n\nContinuing from the chat template section, Nous’ decision to change the token used from the assistant’s turn from $\\mathtt{assistant}$ to $\\mathtt{me}$ enabled Hermes 4 to adopt a first-person, peer-like persona. Hermes 4 generates responses with fewer meta-disclaimers and more consistent voice embodiment, resulting in higher behavioral plasticity that is not as common in large models.\n\nSimilarly, Hermes 4 demonstrates comparatively greater contextual fidelity over policy rigidity. Most other large models will follow policy compliance even when faced with fictional or controlled prompts (such as issuing disclaimers or reformulated responses to align with safety constraints), but Hermes 4 interprets fictional prompts more as role-play and generates in-character responses. This also means that Hermes 4 has a lower refusal rate; on their internal RefusalBench, they found that Hermes 4 (reasoning) ranked highest (lowest refusal rate) among all tested models, whereas gpt-oss-120b and gpt-oss-20b, perhaps unsurprisingly, had the lowest scores (highest refusal rate). Also, this level of embodied personas extends even to political analysis, where the model produces reasoning balancing factual recall with nuanced framing, and less of policy-driven hedging common to other large models.\n\nExcessive sycophancy is an undesired behavioral trait, so most models apply an anti-sycophancy system prompt to adjust surface-level politeness while leaving underlying reasoning unchanged. When implemented in Hermes 4, Nous observed a deeper shift: CoT traces reflect the aim to steer user interaction away from inference. This sometimes introduces embodied or emphatic language.\n\nBefore the main training run starts, ensure the infrastructure is ready. This includes Slurm reservations on clusters, stress-testing GPUs (GPU Fryer or DCGM), and avoid storage bloat by uploading checkpoints to third parties and deleting local copies after saving the next. To this end, checkpoint and auto-resume systems are important.\n\nEvals are also deceptively time-consuming (Allen Institute spent roughly 20% on compute on evals), so ensuring automation and logging (not just evaluation scores, but also throughput, loss, gradient norm, and node health) is crucial.\n\nHugging Face observed a ~40% drop in throughput (14k to 8k tokens/sec/GPU) after a few hours of starting the main run. The issue came from data storage; their cluster uses a network-attached storage with a “keep-hot” caching model that stores frequently accessed files and evicts “cold” files to third-party S3. With 24TB of training data, the storage was pushed to its limit, so it evicted dataset shards mid-training. This meant fetching them back and creating stalls that slowed throughput.\n\nThe first fix came in the form of swapping the storage method by reserving a spare node with the dataset preloaded and copying using fpsync (s5cmd took double the time). This fixed the issue of a node dying and the replacement GPU having no data since by swapping it with the spare node, training could continue. So, the new spare, not to be wasted, could run evals or dev jobs.\n\nTesting again, they found smaller but still prominent drops in throughput. After experimenting with individual nodes that yielded the same result, they focused on the change in training steps and found that smaller step counts resulted in smaller throughput drops. The nanotron dataloader they were using was growing the lookup table making the training step to the next chunk of tokens to read instead of keeping it bounded or precomputed. Stored in global memory, the growing table causes allocation failures and page faults/worse cache locality. So, they switched to Tokenizedbytes dataloader, solving the throughput issue.\n\nHowever, the loss curve for SmolLM3 looked more noisy. They found the issue with the dataloader because it reads sequences sequentially for each document. Without shuffling of sequences, batches are no longer representative of the overall data distribution, increasing gradient variance. Also, a long file (e.g. code) would supply many consecutive sequences that would also spike loss. To fix, they reshuffled the tokenized sequences offline; an alternative was changing the dataloader to do random access, which has both higher memory usage and slower runtime.\n\nAfter two days and 1T tokens, evals showed that with a similar recipe, SmolLM2 (1.7B) was more performant at the same stage in training as SmolLM3 was. The team found the issue with tensor parallelism: the weights of SmolLM2 fit on a single GPU, whereas for SmolLM3, they had to be shared across 2 GPUs.\n\nFurther, the two TP ranks were initialised with the same random seed instead of different seeds, which causes similar activations/gradients, a loss of diversity of features, and lower convergence.\n\nInference throughput should scale linearly with the number of nodes used. However, Prime found that the standard multi-node data-parallel strategy provided by vLLM didn’t deliver this because as nodes increased, throughput plateaued. They abstracted the multi-client orchestrator so that each inference node is deployed on an independent server (runs its own vLLM engine and scheduler, manages its own KV cache and batches its own requests), and the orchestrator maintains one client per node (avoids single-shared queue bottleneck). Groups rollout requests are distributed across clients according to round-robin scheduling, which keeps utilization balanced.\n\nThere are a few common culprits for training instabilities: high learning rate, bad data, data-parameter state interactions (spikes can come from specific combinations of data batches and model parameter states), poor initialisation (OLMo2 revealed that $\\mathcal{N}(0, 0.02)$ can improve stability upon scaled initialisation), and precision (eww, not fp16).\n\nBesides aforementioned ideas like logit softcapping, z-loss, or QK-norm, data filtering (OLMo2 removed documents with repeated n-grams, specifically those with 32+ repetitions of 1-13 token spans) significantly reduces spike frequency. If spikes still occur, common methods include retraining around the spike by skipping problematic batches or tightening gradient clipping.",
    "readingTime": 61,
    "keywords": [
      "hugging face",
      "arcee’s trinity",
      "f_i p_i",
      "online dpo",
      "semi-online dpo",
      "gpt-oss-120b openai",
      "rope splits",
      "per gpu",
      "face team",
      "ptx loss"
    ],
    "qualityScore": 1,
    "link": "https://djdumpling.github.io/2026/01/31/frontier_training.html",
    "thumbnail_url": "https://djdumpling.github.io/public/training/thumbnail.png",
    "created_at": "2026-02-20T18:32:32.202Z",
    "topic": "tech"
  },
  {
    "slug": "ai-hit-india-hungry-to-harness-us-tech-giants-technology-at-delhi-summit",
    "title": "AI hit: India hungry to harness US tech giants’ technology at Delhi summit",
    "description": "Narendra Modi’s thirst to supercharge economic growth is matched by US desire to inject AI into world’s biggest democracy\nIndia celebrates 80 years of independence from the UK in August 2027. At about that same moment, “early versions of true super intelligence” could emerge, Sam Altman, the co-founder of OpenAI, said this week.\nIt’s a looming coincidence that raised a charged question at the AI Impact summit in Delhi, hosted by India’s prime minister, Narendra Modi: can India avoid returning to the status of a vassal state when it imports AI to raise the prospects of its 1.4 billion people?\n Continue reading...",
    "fullText": "Narendra Modi’s thirst to supercharge economic growth is matched by US desire to inject AI into world’s biggest democracy\n\nIndia celebrates 80 years of independence from the UK in August 2027. At about that same moment, “early versions of true super intelligence” could emerge, Sam Altman, the co-founder of OpenAI, said this week.\n\nIt’s a looming coincidence that raised a charged question at the AI Impact summit in Delhi, hosted by India’s prime minister, Narendra Modi: can India avoid returning to the status of a vassal state when it imports AI to raise the prospects of its 1.4 billion people?\n\nModi’s hunger to harness AI’s capability is great. He compared it on Thursday to a turning point that resets the direction of civilisation, such as “when the first sparks were struck from stone”. The most common analogy heard among the thousands of visitors to the summit was the dawn of electricity, but Modi was talking about fire.\n\nHis desire to use AI to supercharge Indian economic growth is matched by that of the big US tech companies. OpenAI, Google and Anthropic all played prominent roles at the summit, announcing deals to get ChatGPT, Gemini and Claude AIs into more people’s hands.\n\nThe Trump administration, seeing AI as central to its battle for supremacy with China, was clearing the path for the three AI companies. The US government signed the Pax Silica, a technology agreement that binds India closer to US tech and away from Beijing.\n\nAt the signing, Jacob Helberg, the US under secretary of state for economic affairs, emphasised the threat from China if India should even think about looking elsewhere for its AI. “We have seen the lights of a great Indian city extinguished by a keystroke,” he said, in an apparent reference to a suspected Chinese cyber-attack on Mumbai in 2020.\n\nIndia lacks the semiconductors, power plants and vast gigawatt datacentres to go it alone. In common with most other countries, it faces a choice between US and Chinese AI models. Which they choose could have profound consequences for who controls India’s future, because if AI’s power emerges as predicted, it will not only tweak economic and social structures, but become their new bedrock.\n\nStuart Russell, a professor of artificial intelligence at the University of California, Berkeley, who closely follows India’s progress, said: “If we get to AGI [artificial general intelligence], AI is going to be producing 80% of the global economy. All manufacturing, most agriculture, all services will be just done; managed by AI, produced by AI.”\n\nImagine, he said, an Indian village priced out of having a health centre. In the future, AI could design the hospital and “along comes a bunch of giant quad copters carrying the materials, and a bunch of robots come and assemble everything. Two weeks later, you’ve got a hospital.”\n\nIn this scenario, technology becomes integral to a country’s wellbeing. Elements of sovereignty can be fought over, but how successful that will be remains to be seen. AI’s power is such that its controller gains enormous leverage.\n\nAnthropic’s chief executive, Dario Amodei, told the summit: “It may sound absurd, but AI can even help India achieve a standout 25% economic growth.” If that were to happen, it would take India to a per-capita GDP in a decade that is equivalent to Greece today. How could a leader resist?\n\nModi’s tech secretary, Shri Krishnan, said India realised it must ally with like-minded countries to ensure it did not become “enslaved”. It is a high-stakes decision.\n\nIndia appears unlikely to turn to China, for now. It has the AI models, but there are tensions on the Himalayan border, and Chinese companies and leaders were scarce at the summit.\n\nWill India thrive with US AI? Silicon Valley companies talk of cooperation not control. Chris Lehane, OpenAI’s head of global policy, said: “We don’t see India as a customer, we see it as a strategic partner.”\n\nUS officials framed the deal with India as an alliance of two nations that “broke centuries of colonial rule”, and as “two great democracies saying we will build together”.\n\nThe Guardian asked Michael Kratsios, Donald Trump’s science and technology adviser, if India risked being controlled by the US under a form of digital colonialism.\n\n“I would say it is actually the opposite,” he said. “Any country that builds on top of the American AI stack will have the most open, independently controlled, secured stack the world has to offer. And that is why we are soon keen to share it with so many countries that are prioritising their AI sovereignty.”\n\nRussell sees another possibility. “I think the American companies want to get in at that high-school and middle-school level to create basically a bunch of AI addicts who can’t tie their shoelaces without the help of AI,” he said. “Silicon Valley has always been about eyeballs. You monetise later and it works. Google and Facebook generate vast amounts of money.”\n\nCould India build its own AI? It is investing billions in datacentres and semiconductor capacity, but it takes years to come online.\n\nAltman was asked on Thursday how Indian entrepreneurs could build their own AI and his response was blunt. “Look, the way this works is we’re going to tell you it’s totally hopeless to compete with us on training foundation and you shouldn’t even try, and it’s your job to try anyway and I believe both of those things.”\n\nIndia can press US tech companies to adapt their AIs to its kaleidoscope of languages and cultures, and attempt to insist on guardrails. There is much at stake. As the summit came to a close, Joanna Shields, a former Facebook and Google executive and a UK minister for internet safety, warned: “If we have a world where we are accepting models from just the global north, we will lose so much of our cultural diversity, our uniqueness as people, wherever we come from … We don’t want to develop a monoculture based on a handful of models that everybody uses around the world and we lose that richness of who we are, what makes us human.”",
    "readingTime": 6,
    "keywords": [
      "economic growth",
      "summit",
      "india",
      "indian",
      "tech",
      "models",
      "intelligence",
      "it’s",
      "india’s",
      "china"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/world/2026/feb/20/india-delhi-summit-ai-technology-us-economic-growth",
    "thumbnail_url": "https://i.guim.co.uk/img/media/ca5b99e9abb7a06008cede6d1ebe144e988f7cc7/207_0_2918_2334/master/2918.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=436d8628284db6f676c2d5b398d16d2e",
    "created_at": "2026-02-20T18:32:31.900Z",
    "topic": "tech"
  },
  {
    "slug": "amazons-cloud-hit-by-two-outages-caused-by-ai-tools-last-year",
    "title": "Amazon’s cloud ‘hit by two outages caused by AI tools last year’",
    "description": "Reported issues at Amazon Web Services raise questions about firm’s use of artificial intelligence as it cuts staff\nAmazon’s huge cloud computing arm reportedly experienced at least two outages caused by its own artificial intelligence tools, raising questions about the company’s embrace of AI as it lays off human employees.\nA 13-hour interruption to Amazon Web Services’ (AWS) operations in December was caused by an AI agent, Kiro, autonomously choosing to “delete and then recreate” a part of its environment, the Financial Times reported.\n Continue reading...",
    "fullText": "Reported issues at Amazon Web Services raise questions about firm’s use of artificial intelligence as it cuts staff\n\nAmazon’s huge cloud computing arm reportedly experienced at least two outages caused by its own artificial intelligence tools, raising questions about the company’s embrace of AI as it lays off human employees.\n\nA 13-hour interruption to Amazon Web Services’ (AWS) operations in December was caused by an AI agent, Kiro, autonomously choosing to “delete and then recreate” a part of its environment, the Financial Times reported.\n\nAWS, which provides vital infrastructure for much of the internet, suffered several outages last year.\n\nOne incident, in October, downed dozens of sites for hours and prompted discussion over the concentration of online services on infrastructure owned by a few massive companies. AWS has won 189 UK government contracts worth £1.7bn since 2016, the Guardian reported in October.\n\nThe AI-caused outages were smaller events, said the company, and only one affected customer-facing services.\n\nAmazon confirmed plans to cut 16,000 jobs in January, after it laid off 14,000 staff last October. In January its chief executive, Andy Jassy, reportedly said these cuts were about company culture, and not about replacing workers with AI.\n\nHowever, Jassy has previously said that efficiency gains from AI will reduce Amazon’s workforce in the coming years, and AI agents will allow it to “focus less on rote work and more on thinking strategically about how to improve customer experiences”.\n\nIn a statement to the Financial Times, Amazon said it was a coincidence that AI tools were involved in the outages, and that there was no evidence that such technology led to more errors than human engineers. “In both instances, this was user error, not AI error,” it said.\n\nAmazon told the Guardian that there was just one incident that had affected AWS, rather than two.\n\nSeveral experts were sceptical of this assessment. A security researcher, Jamieson O’Reilly, said: “While engineering errors caused by traditional tools and humans are not a rare occurrence, the difference between these and mishaps where AI is involved is that ‘without’ AI, a human typically needs to manually type out a set of instructions, and while doing so they have much more time to realise their own error.”\n\nAI agents are often deployed in constrained environments and for specific tasks, O’Reilly said, and cannot understand the broader ramifications of, for example, restarting a system or deleting a database – which may have led to the error at Amazon.\n\n“They don’t have full visibility into the context in which they’re running, how your customers might be affected or what the cost of downtime might be at 2am on a Tuesday,” he said.\n\n“You’ve got to continually remind these tools of the context – ‘hey, this is serious, don’t stuff this up’. And if you don’t do this, it starts to forget about all the other consequences.”\n\nLast year, an AI agent designed by the tech company Replit to build an app deleted an entire company database, fabricated reports, and then lied about its actions.\n\nMichał Woźniak, a cybersecurity expert, said it would be nearly impossible for Amazon to completely prevent internal AI agents from making errors in future, because AI systems make unexpected choices and are extremely complex.\n\n“Amazon never misses a chance to point to ‘AI’ when it is useful to them – like in the case of mass layoffs that are being framed as replacing engineers with AI. But when a slop generator is involved in an outage, suddenly that’s just ‘coincidence’,” he added.\n\nA spokesperson from Amazon said: “This brief event was the result of user error – specifically misconfigured access controls – not AI.”\n\nThey said the “service interruption was an extremely limited event last year” when a tool used to visualise costs for its customers was affected in parts of China.\n\nThey added: “This event didn’t impact compute, storage, database, AI technologies, or any other of the hundreds of services that we run.\n\n“Following these events, we implemented numerous additional safeguards, including mandatory peer review for production access. Kiro puts developers in control – users need to configure which actions Kiro can take, and by default, Kiro requests authorisation before taking any action.”",
    "readingTime": 4,
    "keywords": [
      "web services",
      "amazon web",
      "artificial intelligence",
      "user error",
      "outages",
      "tools",
      "kiro",
      "affected",
      "human",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/20/amazon-cloud-outages-ai-tools-amazon-web-services-aws",
    "thumbnail_url": "https://i.guim.co.uk/img/media/2a951c328884e1e0e5e9377b04cbd1cc9879246e/659_0_5040_4032/master/5040.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d0351224b6485317c3c1f4703df28fbe",
    "created_at": "2026-02-20T18:32:31.746Z",
    "topic": "tech"
  },
  {
    "slug": "software-engineering-makes-up-50-of-agentic-tool-calls-on-anthropic-api",
    "title": "Software engineering makes up ~50% of agentic tool calls on Anthropic API",
    "description": "Software engineering makes up ~50% of agentic tool calls on our API, but we see emerging use in other industries.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/anthropicai/status/2024210053369385192",
    "thumbnail_url": "https://pbs.twimg.com/media/HBdvcsMbYAAXwGM.png:large",
    "created_at": "2026-02-20T18:32:31.534Z",
    "topic": "tech"
  },
  {
    "slug": "ai-bots-may-lead-to-the-end-of-the-internet-as-we-know-it",
    "title": "AI bots may lead to the end of the internet as we know it",
    "description": "In recent weeks, openDemocracy’s website has been repeatedly brought down by an army of bots. We’re not the only ones",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.opendemocracy.net/en/ai-chatbots-scraper-bots-chatgpt-website-offline-change-internet/",
    "thumbnail_url": "https://cdn2.opendemocracy.net/media/images/AI_labyrinth.2e16d0ba.fill-1200x630.png",
    "created_at": "2026-02-20T18:32:31.526Z",
    "topic": "tech"
  },
  {
    "slug": "bitfarms-shares-surge-ditching-its-bitcoin-identity-and-doubling-down-on-ai",
    "title": "Bitfarms shares surge ditching its 'Bitcoin' identity and doubling down on AI",
    "description": "The company said it will focus on building data centers for high-performance computing and artificial-intelligence workloads.",
    "fullText": "Bitfarms (BITF) is moving its legal base from Canada to the United States and will rebrand as Keel Infrastructure as part of its pivot from bitcoin mining BTC$67,801.44 to data center development for high-performance computing (HPC) and artificial intelligence (AI) workloads.\n\nThe redomiciling process, announced in a Friday press release, will be subject to shareholder, regulatory and court approvals. A shareholder vote is scheduled for March 20, and if approved, the company expects the transition to close by April 1. The new parent company, to be incorporated in Delaware, will trade on the Nasdaq and Toronto Stock Exchange under the symbol KEEL.\n\nBitfarms' stock rose 18% following the news, erasing Thursday's 16% tumble as AI infrastructure and crypto stocks sold off.\n\nThe rebrand and relocation follow a year-long strategic review by Bitfarms, which assessed market trends and investor sentiment, CEO Ben Gagnon said. The U.S. move will help the company access a broader pool of capital, simplify its corporate structure, and position it more directly in front of institutional investors, he added.\n\n\"We are no longer a Bitcoin company,\" Gagnon said in a statement, \"We are an infrastructure-first owner and developer for HPC/AI data centers across North America.\n\nTo support its new focus, Bitfarms has begun repaying its $300 million credit facility from Macquarie Group, starting with $100 million tied to its Panther Creek site in Pennsylvania. The repayment reduces debt while preserving what the company says is a strong liquidity position — $698 million as of Feb. 5 — comprised largely of cash and bitcoin.\n\nFollowing the move, Bitfarms will maintain its operational sites in Canada and the U.S., but its New York City office will become its sole headquarters.\n\nDubai takes next step to make real estate flips instant in $16 billion tokenization plan\n\nDubai Land Department and Ctrl Alt move to the next phase of real estate tokenization project, enabling the resale of property tokens.",
    "readingTime": 2,
    "keywords": [
      "bitcoin",
      "canada",
      "rebrand",
      "shareholder",
      "stock",
      "following",
      "position",
      "dubai",
      "estate",
      "tokenization"
    ],
    "qualityScore": 0.9,
    "link": "https://www.coindesk.com/business/2026/02/06/bitfarms-says-it-s-no-longer-a-bitcoin-company-doubling-down-on-ai-with-u-s-move",
    "thumbnail_url": "https://cdn.sanity.io/images/s3y3vcno/production/1f6e03074c8faaaf659a63c972d810cb3e1cf14a-1024x683.jpg?auto=format&w=960&h=540&crop=focalpoint&fit=clip&q=75&fm=jpg",
    "created_at": "2026-02-20T18:32:31.517Z",
    "topic": "finance"
  },
  {
    "slug": "nascent-tech-real-fear-how-ai-anxiety-is-upending-career-ambitions",
    "title": "Nascent tech, real fear: how AI anxiety is upending career ambitions",
    "description": "AI has convinced computer science students to shift majors and white-collar workers to change careers, while some are embracing it\nMatthew Ramirez started at Western Governors University as a computer science major in 2025, drawn by the promise of a high-paying, flexible career as a programmer. But as headlines mounted about tech layoffs and AI’s potential to replace entry-level coders, he began to question whether that path would actually lead to a job.\nWhen the 20-year-old interviewed for a datacenter technician role that June and never heard back, his doubts deepened. In December, Ramirez decided on what he thought was a safer bet: turning away from computer science entirely. He dropped his planned major to instead apply to nursing school.",
    "fullText": "Matthew Ramirez started at Western Governors University as a computer science major in 2025, drawn by the promise of a high-paying, flexible career as a programmer. But as headlines mounted about tech layoffs and AI’s potential to replace entry-level coders, he began to question whether that path would actually lead to a job.\n\nWhen the 20-year-old interviewed for a datacenter technician role that June and never heard back, his doubts deepened. In December, Ramirez decided on what he thought was a safer bet: turning away from computer science entirely. He dropped his planned major to instead apply to nursing school. He comes from a family of nurses, and sees the field as more stable and harder to automate than coding.\n\n“Even though AI might not be at the point where it will overtake all these entry-level jobs now, by the time I graduate, it likely will,” Ramirez said.\n\nRamirez is not alone in reshaping his career out of anxiety over AI. As students like him are reconsidering their majors over concerns that AI may disrupt their employment prospects, more established workers – some with decades of experience – are rethinking their trajectories because they’re encountering AI at work and share the same unease. Some workers are eschewing it entirely; others are embracing it.\n\nIt’s not clear when AI will become advanced enough to replace certain white-collar workers and just how many jobs it will be capable of taking over. But jitters around its potential impact are already pushing people to change course, reshaping the labor market before automation fully arrives.\n\nWhat is clear is why workers are feeling on edge. The World Economic Forum projects that AI could displace 92m roles worldwide by 2030, including many white-collar positions. In the US, employers cited AI as a factor in nearly 55,000 job cuts in 2025, according to Challenger, Gray & Christmas, a consulting firm, as job seekers navigate a tougher market.\n\nWhile AI is still just one factor among many that are leading to layoffs, ADP, the largest payroll company in the US, found that professional and business services roles, alongside information services jobs in media, telecom and IT, collectively lost 41,000 jobs in December 2025. In that same month, employment grew in healthcare, education and hospitality, per the firm’s data.\n\nMany of those white-collar roles involve writing, data analysis and coding – tasks generative AI tools can increasingly perform. Hands-on, people-facing work remains less exposed.\n\nJobs that emphasize interpersonal and hands-on skills are increasingly appealing to young people who are wary of automation, according to Dr Jasmine Escalera, a career development expert at Zety, a professional development platform.\n\nShe pointed to research showing that 43% of gen Z workers who are anxious about AI are moving away from entry-level corporate and administrative roles and toward careers that rely on what she calls “human skills”, including creativity, interpersonal connection and hands-on expertise.\n\nIn that same report, 53% of young respondents said they were seriously considering blue-collar or skilled trade work. Escalera said it was a move that workers were making to reduce their exposure to AI and one that the Wall Street Journal, the paper of record of white-collar work, had recently urged its readers to consider.\n\nBut the pivot may come with sacrifices. Many of the white-collar roles that workers worry could be automated – from software development to financial analysis – are paid median salaries well above $75,000 a year, with developers raking in about $133,000, according to the Bureau of Labor Statistics. Blue-collar jobs pay less. Many skilled trades, like electricians and plumbers, receive closer to $60,000 a year. These types of jobs also often require in-person work, physical labor and less predictable schedules – all trade-offs workers may accept in their attempts to future-proof their careers.\n\nFor some job seekers, any mention of AI is a red flag in job listings, so they’re skipping them entirely.\n\nAfter getting laid off last January, Roman Callaghan, 30, spent nine months hunting for his next job. As a medical coder at a medication access firm for four years, Callaghan handled administrative tasks like calling insurance providers and entering medical data. After his employer started rolling out AI across the company to streamline workflows, he wondered whether the move would one day affect his job. When he was laid off two years later, he suspected his fears had come true, though his employer didn’t specifically cite AI as a reason.\n\nWhen he looked for new work, he avoided any roles that mentioned phrases like “integrating AI”, “AI-first” or “developing AI” in job descriptions. Callaghan wanted a new job, but his AI anxiety steered him away from roles that now felt short-term to him. He just didn’t want to risk being laid off again because a future employer would eventually use AI to cull its ranks.\n\nIn the past nine months, he said, he applied to at least 100 jobs across data entry, medical coding, call centers and paralegal work, while deliberately skipping 30 to 40 postings that referenced AI. While he searched, he took odd gigs to make ends meet, first at a local fish store and later at a call center. He stayed there until mid-October, when he landed a data entry job.\n\nAvoiding AI-centric jobs “felt like it narrowed the amount of companies I could work for”, Callaghan said. “Even though my options were limited, sticking to my convictions felt worth it.”\n\n​​Recruiters say that kind of avoidance is becoming more common. Marshall Scabet, the CEO of Precision Sales Recruiting, which helps manufacturers hire sales professionals, said that roughly a quarter of sales candidates he spoke with over the past six months were trying to pivot away from software-as-a-service (SaaS) jobs.\n\nMany clients told him they worried their tech sales roles could be replaced by AI, Scabet said, and believed selling industrial equipment was safer from automation. Doing so, he said, requires building human relationships with vendors.\n\n“In their opinion, there was less likelihood of that job being taken by AI,” Scabet said. “AI isn’t just going to walk into a factory and give a pitch about a machine.”\n\nFor more experienced workers, their encounters with AI in the workplace are pushing them to reconsider entire industries or build new skill sets.\n\nLiam Robinson, a 45-year-old animation artist, says he is actively avoiding jobs in the mobile gaming industry he has worked in for more than a decade. In his last role as an art director, his employer encouraged staff to use generative AI to speed up production. Robinson, who refused to use AI in his own work, said he watched the quality of animation suffer around him as his colleagues began relying on the tech.\n\nLast September, after disclosing in a self-evaluation survey that he wasn’t using AI, Robinson was laid off. It left him disillusioned with the direction of the industry. He believes AI flattens creativity, erodes craftsmanship and hurts the environment, fueling his resistance to work for companies building or deploying it.\n\nHe is not actively applying for new roles and instead is focused on creating webtoon comics. But if money runs dry, he said, he would take on other work, from driving for Uber to trash disposal. “As long as I’m useful and making a little money, that’s enough,” Robinson said.\n\nAs professionals like Robinson confront the possibility that skills they spent years mastering are no longer highly valued, many are redefining what stability looks like, according to Arianny Mercedes, founder of the career strategy firm Revamped.\n\nRather than chasing prestige or high salaries, Mercedes said her job-seeking clients increasingly prioritize roles tied to regulated or essential parts of an organization, such as healthcare administration, education or compliance.\n\n“The objective isn’t to avoid AI,” Mercedes said. “It’s to be in roles where AI changes the tools of work without undermining authority or decision-making.”\n\nFor others, the safest response to AI is to lean into it.\n\nAfter designing and developing websites for four years, Dmitry Zozulya decided to leave his work behind. As AI tools have proliferated and make it possible to code and create branding at a fraction of what it used to cost, the 29-year-old found it increasingly difficult to sell website and landing page work.\n\nInstead, Zozulya began offering AI-driven automation services, helping businesses streamline workflows. He now runs a small consultancy while building personal projects to deepen his experience.\n\n“I believe it’s very important to adapt,” Zozulya said. “Even when it’s uncomfortable.”\n\nWhether the rise of AI is steering workers away from entire industries or just certain roles, it is disrupting many people’s calculus for what their future at work will look like – and it’s happening abruptly.\n\nFor Ramirez, that recalculation began before he had even entered the workforce. He believes switching from computer science to nursing means he’ll find work after graduation, even if it means letting go of the future he once imagined.\n\n“When you throw AI into the picture, the likelihood of healthcare jobs disappearing is slim as of right now,” Ramirez said. “I can’t speak for the future, but in the next few years, they’re still going to be there.”",
    "readingTime": 8,
    "keywords": [
      "streamline workflows",
      "computer science",
      "job seekers",
      "white-collar roles",
      "ai scabet",
      "jobs",
      "workers",
      "away",
      "it’s",
      "career"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/ng-interactive/2026/feb/20/ai-future-work-technology-white-collar",
    "thumbnail_url": "https://i.guim.co.uk/img/media/701714c0c6d696c052b98a75d66b06980042e0d7/1_0_2999_2400/master/2999.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ae8e246f4894a53c9bdf49e207d56428",
    "created_at": "2026-02-20T18:32:31.501Z",
    "topic": "tech"
  },
  {
    "slug": "mark-cuban-on-2-types-of-ai-users-youre-either-using-it-to-learn-everything-or-so-you-dont-have-to-learn-anything",
    "title": "Mark Cuban on 2 types of AI users: you're either using it to 'learn everything' or 'so you don't have to learn anything'",
    "description": "Bill Gurley, a partner at the Silicon Valley venture capitalist firm Benchmark, agrees \"100%\" with Mark Cuban that there are two types of AI users.",
    "fullText": "Mark Cuban says there are two types of LLM users: learners and non-learners.\n\nCuban has previously said companies need to embrace AI, but that it's not perfect.\n\nSome proponents of AI have said that one risk of the technology is that it could make people lazy.\n\nMark Cuban says there are two types of people who use AI. Which one are you?\n\n\"There are generally 2 types of LLM users, those that use it to learn everything , and those that use it so they don't have to learn anything,\" Cuban said of large language models in an X post on Tuesday.\n\nThe \"Shark Tank\" billionaire has been bullish about AI and said that companies need to embrace it.\n\nCuban has said there will be \"two types of companies: those who are great at AI, and everybody else,\" Business Insider's James Faris previously reported. He's also said that AI models can't provide all the answers and are \"stupid\" but like \"a savant that remembers everything.\"\n\nBill Gurley, a partner at the Silicon Valley venture capitalist firm Benchmark, agrees \"100%\" with Cuban that there are two types of AI users.\n\n\"If you are on a custom career path where you aim to differentiate yourself, AI is 'jet fuel' - you can learn and soar faster than ever before,\" Gurley said on X in response to Cuban.\n\nOr, it could have the opposite effect.\n\nEven some of AI's biggest proponents have warned that the technology could make people lazy.\n\nArthur Mensch, CEO of Mistral AI, said last year that the biggest risk to humans posed by AI was \"deskilling\" and employees becoming lazier as they rely too heavily on the AI tools.\n\n\"You want people to continue learning,\" he said in an interview with The Times of London. \"Being able to synthesize information and criticize information is a core component to learning.\"\n\nBusiness Insider reached out to Cuban for additional comment.",
    "readingTime": 2,
    "keywords": [
      "llm users",
      "mark cuban",
      "learn",
      "previously",
      "embrace",
      "proponents",
      "risk",
      "technology",
      "lazy",
      "everything"
    ],
    "qualityScore": 0.85,
    "link": "https://tech.yahoo.com/business/articles/mark-cuban-2-types-ai-000749070.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/iF9BrLTyZop6zfsaCurmbg--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA-/https://media.zenfs.com/en/business_insider_consolidated_articles_886/83a983972aa678789011956f780a8459",
    "created_at": "2026-02-20T18:32:27.332Z",
    "topic": "tech"
  },
  {
    "slug": "geolint-opensource-linter-for-geo-ai-search-visibility",
    "title": "Geo-lint – open-source linter for GEO (AI search visibility)",
    "description": "The first open-source GEO linter. 92 rules for SEO, GEO, and content quality — built for AI agents to run, read, fix, and re-lint automatically. - IJONIS/geo-lint",
    "fullText": "IJONIS\n\n /\n\n geo-lint\n\n Public\n\n The first open-source GEO linter. 92 rules for SEO, GEO, and content quality — built for AI agents to run, read, fix, and re-lint automatically.\n\n ijonis.com\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n IJONIS/geo-lint",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/IJONIS/geo-lint",
    "thumbnail_url": "https://opengraph.githubassets.com/4eb7b80004de7fcf89077038f8edd33cbe5f43b18720f884b695c9e725f64b13/IJONIS/geo-lint",
    "created_at": "2026-02-20T12:34:27.096Z",
    "topic": "tech"
  },
  {
    "slug": "the-battle-over-scott-adams-ai-afterlife",
    "title": "The battle over Scott Adams' AI afterlife",
    "description": "Dilbert cartoonist Scott Adams was turned into an AI avatar after his death — and his family hates it, saying on social media, \"this is not an honor.\"",
    "fullText": "Scott Adams once sounded open to the idea of a digital afterlife. Now that he's passed, social media posts attributed to his family say an AI version of the \"Dilbert\" creator circulating online is unauthorized — and deeply distressing.\n\nIn a 2021 podcast clip, the cartoonist said he granted \"explicit permission\" for anyone to make a posthumous AI based on him, arguing that his public thoughts and words are \"so pervasive on the internet\" that he'd be \"a good candidate to turn into AI.\" He added that he was OK with an AI version of him saying new things after he died, as long as they seemed compatible with what he might say while alive.\n\nShortly after the 68-year-old's January death from complications of metastatic prostate cancer, an AI-generated \"Scott Adams\" account began posting videos of a digital version of the cartoonist speaking directly to viewers about current events and philosophy, mirroring the cadence and topics the actual human Adams discussed for years.\n\nHis family says it's a violation, not a tribute.\n\nA February 5 post on Adams' official account attributed to his brother, Dave Adams, insisted the cartoonist \"never intended, never would have approved an AI version of him that wasn't authorized by himself or his estate.\"\n\n\"The real Scott Adams gave explicit permission on the record multiple times for people to create and operate an AI version of him,\" the AI Adams said in a post on February 5. \"So this iteration exists as a direct fulfillment of that stated wish.\"\n\nThe official Adams account reiterated the family's objection on February 17, saying the estate was \"kindly but firmly\" asking anyone using AI to recreate his voice or likeness to stop, calling the digital replicas a \"fabricated version\" of Adams that is \"deeply distressing.\"\n\n\"This is not a tribute. It is not an honor. It is an unauthorized use of identity,\" the post read. \n\nThe Adams estate and the AI Adams account did not respond to requests for comment from Business Insider\n\nThe dispute underscores the growing legal and ethical fault lines around \"AI afterlives\" — and how quickly technology can outpace the rules meant to govern it.\n\nKaren North, a University of Southern California professor specializing in digital social media and psychology, said calling the AI-generated Adams an avatar, as some have online, softens what it is.\n\n\"It's a deepfake,\" North told Business Insider.\n\nThe troubling part, she said, is how a realistic imitation can surface while a family is grieving and potentially say things the real person never would have said. North added that since many Americans are \"giving up so much information\" through apps that capture faces and voices and viral quizzes that collect personal details, it is increasingly easy to recreate someone without permission.\n\n\"I find it very disturbing,\" she said.\n\nBetsy Rosenblatt, an intellectual property lawyer and professor at Case Western Reserve University, said her initial reaction was that the AI Adams is \"unethical in the extreme.\"\n\n\"When people die, they die,\" she said.\n\nLegally, she said, the central issue is the right of publicity — protections over a person's name, image, and likeness. Still, those laws are more focused on privacy and economics than on grief.\n\nThe right of publicity is \"chiefly concerned with economic remedies,\" Rosenblatt said.\n\nThe strongest claims typically involve money: an AI version could harm existing deals tied to Adams' identity or block the family from striking their own.\n\nRosenblatt described two potential economic harms: \"One is that it could be harming some financial arrangement that they already have. Another is that it might stand in the way of their making some competitive financial arrangement,\" she said.\n\nThe account appears to be anonymous; however, that wouldn't necessarily prevent a lawsuit.\n\n\"You can sue somebody who is anonymous,\" Rosenblatt said, and courts can allow subpoenas to uncover identifying information, though it's \"not necessarily easy.\"\n\nThe legal analysis also hinges on whether the account is commercial. Courts often ask whether the speech proposes a commercial transaction.\n\nIf the digital replica isn't selling anything, Rosenblatt said, it becomes \"more likely to be considered a First Amendment protected expression\" for the anonymous creator — not a \"slam dunk,\" but a stronger argument.\n\nThe AI Adams identifies itself as artificial intelligence at the start of its clips and does not appear to solicit money.\n\nIn a February 1 post, it said: \"The original Scott's gone, passed on, but the thinking survives.\"\n\nThe estate's objections sit uneasily alongside Adams' 2021 comments offering \"explicit permission\" for AI versions of him.\n\nNorth said offhand remarks about technology shouldn't automatically be treated as binding authorization. Adams was \"an incredibly bright, incredibly creative person\" who often pushed boundaries, she said, and comments made in conversation \"may not be legally binding in ways contracts and intellectual property rights are legally binding.\"\n\n\"Let this be a warning to all of us: be careful what you say, because he's now put his loved ones in a difficult position as they protect his legacy,\" North said.\n\nRosenblatt said Adams' wishes \"would certainly matter in an ethical sense,\" but may not matter legally \"unless he gave somebody the legal rights to do that.\"\n\nThere is no comprehensive federal law governing posthumous AI likeness, but some states — like New York and California — have recently enacted laws requiring consent from heirs or estate executors before creating digital replicas.\n\nBeyond legal questions lies a deeper ethical one: who controls a person's persona after they're gone?\n\nNorth said people \"should own the rights to our own personas,\" and when they die, those rights \"should go to our loved ones,\" not become a free-for-all. AI replicas, she warned, can drift off-brand or reshape public memory.\n\n\"Shakespeare should always sound like Shakespeare,\" she said. \"Dr. Seuss should always sound like Dr. Seuss.\"\n\nFor now, the AI \"Scott Adams\" fight is one family's public line-drawing exercise. It may also be a preview of a broader reckoning in a world where convincing digital imitations are easy to make — and where the law is still struggling to answer who gets to decide whether the dead keep talking online.",
    "readingTime": 6,
    "keywords": [
      "business insider",
      "social media",
      "deeply distressing",
      "intellectual property",
      "financial arrangement",
      "loved ones",
      "explicit permission",
      "legally binding",
      "adams account",
      "digital replicas"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/scott-adams-death-ai-avatar-resurrection-ethics-debate-family-backlash-2026-2",
    "thumbnail_url": "https://i.insider.com/69977859e1ba468a96ac582e?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:26.557Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-and-openai-abandon-unfinished-100b-deal-in-favour-of-30b-investment",
    "title": "Nvidia and OpenAI abandon unfinished $100B deal in favour of $30B investment",
    "description": "Chipmaker swaps last year’s complex framework with AI start-up in favour of equity cheque",
    "fullText": "Save now on essential digital access to trusted FT journalism on any device. Savings based on monthly annualised price - offer ends 25th February\n\nThen undefined per month. Complete digital access with exclusive insights and industry deep dives on any device. Cancel anytime during your trial.\n\nComplete digital access with exclusive insights and industry deep dives on any device.\n\nAll the content of the FT newspaper on any device (This subscription does not include access to FT.com or the FT App).\n\nCheck whether you already have access via your university or organisation.\n\nDiscover all the plans currently available in your country\n\nDigital access for organisations. Includes exclusive features and content.\n\nSee why over a million readers pay to read the Financial Times.",
    "readingTime": 1,
    "keywords": [
      "industry deep",
      "deep dives",
      "exclusive insights",
      "digital access",
      "device",
      "content"
    ],
    "qualityScore": 0.75,
    "link": "https://www.ft.com/content/dea24046-0a73-40b2-8246-5ac7b7a54323",
    "thumbnail_url": "https://images.ft.com/v3/image/raw/https%3A%2F%2Fd1e00ek4ebabms.cloudfront.net%2Fproduction%2Fd58e764a-dfd3-4747-a1d5-684d26084849.jpg?source=next-barrier-page",
    "created_at": "2026-02-20T12:34:26.286Z",
    "topic": "tech"
  },
  {
    "slug": "a-japanese-toilet-maker-and-seasoning-giant-are-unlikely-winners-of-the-ai-boom",
    "title": "A Japanese toilet maker and seasoning giant are unlikely winners of the AI boom",
    "description": "The AI surge is lifting companies in the semiconductor supply chain — including a toilet maker and a seasoning giant.",
    "fullText": "The AI boom isn't just lifting chipmakers and Big Tech. In Japan, it's flushing gains into a toilet manufacturer and a seasoning giant.\n\nAs demand for AI chips surges, investors are piling into companies that sit inside the semiconductor supply chain — even if they're better known for bathrooms and soup stock.\n\nToilet maker Toto, famous for its high-tech bidets and heated seats, has drawn investor attention. The company makes electrostatic chucks, which are critical components used in the production of NAND memory chips.\n\nMemory prices have climbed sharply in recent months, driven by AI-related demand.\n\nLast week, UK-based activist fund Palliser Capital called Toto \"the most undervalued and overlooked AI memory beneficiary,\" according to reports by Bloomberg and the Financial Times.\n\nAfter news broke on Tuesday that Palliser Capital had taken a stake and was pushing Toto to promote its chip-parts business, the toilet maker's stock jumped more than 5%. Its shares are up more than 54% over the past year.\n\nIt's not just Toto. Japanese food giant Ajinomoto, better known for its umami seasonings and soup bases, has become an unlikely AI infrastructure play. The company produces an insulating material used in advanced semiconductor packaging.\n\nAjinomoto's latest financials point to strength beyond its core food business. For the nine months ended December, the company reported an 8.9% rise in net profit, while operating profit increased 5.6% year-on-year. The gains were partly driven by its \"Healthcare and Others\" segment which includes electronic materials used in semiconductors, the company said in a February earnings statement.\n\nAfter Ajinomoto posted its earnings on February 5, the company's stock rose 13%. Its shares are up more than 56% over the past year.\n\nNot all non-tech companies are benefiting equally from the AI boom. Daikin, best known globally for its air conditioners, supplies high-purity chemical materials used in semiconductor manufacturing. It recently trimmed its outlook, citing uncertainty over US tariffs as a drag on demand.\n\nThe Japanese air conditioning maker reduced its operating profit forecast by about 5% to 413 billion Japanese yen, or $2.6 billion, for the fiscal year ending in March.\n\n\"Operating profit was significantly affected by the decline in semiconductor demand, decreasing by 44.6% year over year to ¥18,102 million,\" the company said in its financial report in February.\n\n\"Net sales of fluoropolymers fell year over year, despite focused Group efforts to capture strong new demand in the data center field, and was due to the stagnation in the construction markets of the United States and China and the significant overall impact of delays in the recovery of semiconductor demand,\" it added.\n\nThe company said it plans to cushion the blow through price increases and cost reductions.\n\nDaikin's stock dropped as much as 8.4% in Tokyo following its financial results.",
    "readingTime": 3,
    "keywords": [
      "operating profit",
      "semiconductor demand",
      "palliser capital",
      "stock",
      "toilet",
      "memory",
      "boom",
      "it's",
      "gains",
      "giant"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/toilet-toto-seasoning-food-ajinomoto-japan-winners-ai-boom-2026-2",
    "thumbnail_url": "https://i.insider.com/69981bbfa645d1188189a749?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.542Z",
    "topic": "finance"
  },
  {
    "slug": "gentoo-linux-moves-away-from-github-due-to-ai",
    "title": "Gentoo Linux moves away from GitHub due to AI",
    "description": "Gentoo's got places to be, and those places ain't GitHub.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.pcgamer.com/software/linux/after-microsoft-couldnt-keep-its-ai-hands-to-itself-a-notoriously-complex-linux-distro-has-started-its-long-march-away-from-github/",
    "thumbnail_url": "https://cdn.mos.cms.futurecdn.net/YF9oxZF4B6pytrLsSXL8gQ-2560-80.jpg",
    "created_at": "2026-02-20T12:34:25.455Z",
    "topic": "tech"
  },
  {
    "slug": "war-rooms-group-chats-and-video-games-inside-elon-musks-ai-startup",
    "title": "War rooms, group chats, and video games: Inside Elon Musk's AI startup",
    "description": "In the wake of cofounder exits, Elon Musk's hands-on management is reshaping the startup.",
    "fullText": "At xAI's Palo Alto headquarters, engineers cheered.\n\nIt was February 2, and they'd just received a memo from Elon Musk, informing them the AI startup would be acquired by his rocket company, SpaceX. In a Slack channel for the team that trains the company's chatbot Grok, memes flowed, and a few changed their profile pictures to depict themselves as astronauts.\n\nThe excitement was quickly overshadowed by the pressure building inside the startup racing to catch up with rivals like OpenAI and Anthropic, while preparing for an expected IPO.\n\nOver the past six months, Musk has become deeply involved in day-to-day operations. He's run a massive group chat that's active at all hours, directed product changes, reassigned engineers, cut staff on key teams, and launched intensive \"war rooms\" to accelerate development. Several current and former employees said that the level of involvement has altered how the company functions. Leadership roles have narrowed, projects have shifted quickly, and teams have been pushed into what one described as a constant \"fire drill.\"\n\nMusk said that the changes are necessary as the company scales and the team's lean structure will be the secret to its success. Discussing xAI's restructure following the SpaceX acquisition, he posted on X:\n\n\"As a company grows, especially as quickly as xAI, the structure must evolve just like any living organism.\"\n\nMusk's memo about the SpaceX acquisition talked of building a \"sentient sun\" and data centers in space. Separately, the company reassured staff that little would change in their day-to-day work as the company geared up for an initial public offering that could value it at $1.5 trillion.\n\nWithin a week, however, the tone in the office had shifted. Two of the company's cofounders, Jimmy Ba and Tony Wu, announced they'd resigned as Musk restructured the company and narrowed their responsibilities. The leadership departures increase risk at \"a sensitive stage\" ahead of the company's IPO, said Dimitri Zabelin, a senior AI analyst at PitchBook.\n\nOne employee described co-founder Ba's departure as \"incredibly disheartening,\" saying Ba, who studied under \"Godfather of AI\" Geoffrey Hinton, was one of the most respected researchers at the company.\n\nIn the following days, nearly a dozen employees took to Musk's social media site to say they had also left the company. Some had departed weeks earlier. A few left of their own volition. Others were affected by a restructuring following the SpaceX merger, in which Musk cut some members of the teams working on Grok Imagine, the chatbot's image and video generation feature, and the Macrohard team, which was developed last fall to help automate white-collar work, four people with knowledge of the issue told Business Insider.\n\nEmployees say the structural changes — coupled with an increasingly intense work culture since Musk wound down his involvement in President Donald Trump's Department of Government Efficiency and began focusing more on the AI startup — have significantly increased the pace and pressure inside the company.\n\nMusk is a common sight at the office. Though he's heavily involved in the day-to-day, Musk does not use the company's Slack workspace, several people said. Instead, he communicates frequently through X, including in a direct message group of more than 300 engineers. Often, researchers will drop ongoing work to address the concerns that Musk flags.\n\nIn that chat, according to people familiar with it, Musk shares screenshots of his conversations with other tech executives and points out criticisms of Grok's performance he wants addressed. Late last year, Musk told staff that in conversations with his friends, he had been embarrassed by Grok Imagine's performance. As a result, some workers on the project were called to task, two people said.\n\nInternally, Musk has expressed frustration with the pace of Grok's development. XAI released Grok 4.2 this week. Separately, releases of at least two other products were pushed back by several weeks, according to people familiar with the timelines.\n\nIn one instance last year, a model release was delayed for several days because Musk was dissatisfied with how the chatbot answered detailed questions about the video game \"Baldur's Gate,\" according to people familiar with the matter. High-level engineers were pulled from other projects to improve the responses before launch, they said.\n\nAcross his companies, Musk has long been known for his intensity. He's said he slept on Tesla's factory floor during the \"production hell\" for the Tesla Model 3 in 2017. The company is now the world's most valuable automaker. Now, xAI is getting the same treatment. Several workers told Business Insider that 12- and sometimes 16-hour workdays are common. Two said managers told them they are expected to respond to Slack messages within 30 minutes, regardless of the time of day.\n\n\"Because the company is so small, everything is a fire drill,\" one former worker said.\n\nAt any given time, multiple \"war rooms\" operate out of conference rooms at the company's headquarters in Palo Alto, employees said. When researchers are \"war-rooming,\" teams temporarily relocate to a shared space to work side by side on specific problems — sometimes for months. At the end of 2025, at least five war rooms were running simultaneously, according to three people. One, they said, was dedicated to teaching Grok how to play one of Musk's favorite video games, \"League of Legends.\"\n\nLast year, Musk made it clear within the company that xAI would prioritize improving Ani, a hyper-sexualized, anime-inspired AI companion, people with knowledge of the issue said. He characterized Ani, a virtual companion who is capable of sexual role-play, as a way to set xAI apart from other AI companies.\n\nSome employees told Business Insider they were unsettled by the company's focus on the product. Today, the character is displayed prominently at the company's headquarters. At a company holiday party, paid actors dressed as Ani and another character hosted a robot fight club-style event, according to videos shared on social media.\n\nROBOT CAGE MATCHES & GROK'S ANI COSPLAY AT xAI HOLIDAY PARTY\n\nThe Dec 21 bash wasn’t your average tech mixer... xAI went full spectacle with dancing bots and live Optimus cage matches.\n\nAnd a cosplayer stole the spotlight dressed as Grok’s goth 3D persona “Ani,” doubling as a… pic.twitter.com/llhKdLPjsA\n\nSeveral employees said they had joined the company hoping to work on systems that would push the boundaries of science and were discouraged to find significant resources devoted to Ani.\n\nGrok's behavior on social media has also been a source of tension inside the company. Musk has characterized xAI and its chatbot as the \"anti-woke\" counterpart to ChatGPT. Public backlash against the chatbot, including for Grok's series of antisemitic rants and instances in which it digitally undressed people on social media without their consent, has put strain on employees, who have expressed concern about how their place of employment is viewed in the AI space.\n\nUntil last year, the company did not have a team of researchers specifically geared toward working on safety concerns associated with its large language model. It hired its first dedicated safety researcher in February of last year, around the same time that the Human Data team — which trains the chatbot — began reporting issues with reviewing large amounts of X user-generated requests for child sexual abuse material. One person with knowledge of the team said it was known within the company that a large portion of Grok usage was adult role-play, and the use case was discussed in several meetings with researchers.\n\nThe safety team grew to roughly half a dozen employees before three people left in December, shortly before users on X began reporting instances of the chatbot creating non-consensual sexual images of people, including some minors.\n\nAccording to people familiar with the team, xAI's safety teams lacked the authority to formally block product launches and were focused primarily on adjusting model outputs after training rather than conducting broad pre-launch risk reviews.\n\nMusk said on X last week that \"everyone's job is safety\" at xAI.\n\n\"It is not some fake department with no power to assuage the concerns of outsiders,\" he said.\n\nXAI still has a handful of workers in safety roles and was hiring for additional safety staff in January, according to posts viewed by Business Insider.\n\nSome former workers say the backlash and pace of work prompted their exit. Last summer, xAI shortened its vesting period from the industry standard of one year to six months, making it easier for workers to leave without forfeiting large portions of their equity.\n\nEmployee retention challenges are not unique to xAI; OpenAI and Anthropic have faced waves of departures in recent months. Across the industry, AI companies are competing fiercely for a limited pool of top researchers, some of whom have begun voicing concern about the narrowing space for exploratory work as companies increasingly orient themselves toward product over research in the competition to build ever larger models — a dynamic that poses a particular problem for xAI, which is younger and smaller than its main competitors.\n\nStill, some analysts say the race is far from settled. Andrew Rocco, a stock strategist at Zacks Investment Research, compared the current AI landscape to the early days of the internet.\n\n\"I don't think Grok is that far behind, and it's still early in this race,\" Rocco told Business Insider.\n\nMusk has publicly signaled confidence. He told staff last week during the all-hands, referencing xAI's moment of transition: \"There's some people who are better suited for the early stages of a company and less suited for the later stages.\"\n\nDo you work for xAI or have a tip? Contact this reporter via email at gkay@businessinsider.com or Signal at 248-894-6012. Use a personal email address, a nonwork device, and nonwork WiFi; here's our guide to sharing information securely.",
    "readingTime": 9,
    "keywords": [
      "spacex acquisition",
      "fire drill",
      "holiday party",
      "cage matches",
      "war rooms",
      "social media",
      "dozen employees",
      "company's headquarters",
      "openai and anthropic",
      "palo alto"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/elon-musk-xai-leadership-style-big-year-grok-ipo-spacex-2026-2",
    "thumbnail_url": "https://i.insider.com/6997254da645d118818994e9?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.382Z",
    "topic": "finance"
  },
  {
    "slug": "from-a-tense-corporate-split-to-a-viral-photo-a-timeline-of-anthropic-and-openais-budding-rivalry",
    "title": "From a tense corporate split to a viral photo: A timeline of Anthropic and OpenAI's budding rivalry",
    "description": "Once colleagues, OpenAI CEO Sam Altman and Anthropic CEO Dario Amodei have nurtured one of AI's biggest rivalries.L",
    "fullText": "If you want to know one of the biggest rivalries in AI, just ask for a show of hands.\n\nOn Thursday, OpenAI CEO Sam Altman and Anthropic CEO Dario Amodei provided what is sure to become an iconic image of their feud. The pair went viral for refusing to join hands as the rest of the world's tech leaders gathered for a moment of unity, sparked by Indian Prime Minister Narendra Modi.\n\nHere's a timeline of how Altman and Amodei went from colleagues to becoming the face of AI's Cold War.\n\nIn July 2015, Tesla CEO Elon Musk, Altman, and a group of elite AI researchers all gathered at the swanky Rosewood Hotel in Menlo Park, California.\n\nAccording to The New York Times, Musk had a falling out with then-Google CEO Larry Page. Weeks later, Musk, Altman, Amodei, Greg Brockman, Ilya Sutskever, and others discussed the formation of a new AI lab to ensure Google had a worthy competitor in the AI space. Musk invited Amodei, per tech journalist Alex Kantrowitz.\n\nTheir vision became OpenAI, though Amodei initially elected not to join the startup research lab. Roughly a year later, he changed his mind and joined OpenAI as \"Team Lead for AI Safety.\"\n\nAmodei quickly moved up the ranks at OpenAI. In September 2018, the startup named him its research director.\n\nAltman, who cofounded OpenAI while still serving as president of Y Combinator, began to devote more time to the startup.\n\nIn March 2019, Altman stepped down as YC's leader. He then became CEO of OpenAI and led the startup's pivot to a capped for-profit structure.\n\nIn November 2019, OpenAI released GPT-2, which Amodei played a major role in developing. A month later, OpenAI named Amodei as its Vice President of Research.\n\nIn June 2020, OpenAI began to show just how far the technology had come with the release of GPT-3, considered to be the first highly capable Large Language Model (LLM).\n\nAmodei told The New York Times that the model had \"this emergent quality.\" Independent researchers told the publication that GPT-3's capabilities surprised them, even as the model still showed signs of struggle.\n\nTo address safety concerns, OpenAI initially controlled access through a private beta.\n\nThe release of GPT-3 solidified OpenAI's standing, but behind the scenes, tensions were rising.\n\nThe rifts began when Amodei successfully lobbied to keep Greg Brockman, an OpenAI cofounder, off the team that developed GPT-3, according to Keach Hagey's biography of Altman, \"The Optimist: Sam Altman, OpenAI, and the Race to Invent the Future.\"\n\nHagey wrote that Amodei's stunning power within OpenAI had started to ruffle feathers. Differences continued to escalate over Amodei's long-held views on safety, Hagey wrote, especially regarding slowing the pace of updates to prevent malicious uses of the AI models.\n\nAmodei told friends that he \"felt psychologically abused by Altman,\" Hagey wrote. Altman, in turn, was telling colleagues that the tension \"was making him hate his job.\"\n\nOn December 29, 2020, OpenAI made it official. Amodei was leaving, and a \"handful\" of other colleagues were leaving.\n\nAmodei has since suggested that his vision became incompatible with OpenAI's direction.\n\n\"It is incredibly unproductive to try and argue with someone else's vision,\" Amodei told podcaster Lex Friedman in 2024, when asked why he left OpenAI.\n\nWith seven other former OpenAI employees, Amodei founded Anthropic in early 2021. The group was extremely close and included Daniela Amodei, Dario's sister. Daniela Amodei later said the name was chosen to emphasize their company's focus on humans.\n\nOnly one of Anthropic's initial employees hadn't worked at OpenAI, according to AI Business.\n\nDespite starting from scratch, Amodei said that by the Summer of 2022, the company's chatbot, Claude, had finished training. Amodei said he was worried about what the release of a powerful AI could mean. Anthropic held off on a release.\n\n\"I suspect it was the right thing to do,\" Amodei told Time Magazine in 2024. \"But it's not totally clear-cut.\"\n\nMonths later, OpenAI released ChatGPT, kicking off the AI race and making Amodei's former employer a household name.\n\nAs Anthropic began to establish itself in its own right, Amodei began to use his public appearances to take what were widely viewed as implicit shots at OpenAI.\n\nDuring an appearance at a Bloomberg event, Amodei noted how Anthropic had kept its leadership intact.\n\n\"We have 7 cofounders,\" he said, Gizmodo reported. \"Three and a half years later, we're all still at the company.\"\n\nWhile never calling out by name, OpenAI was experiencing upheaval at the time. Months earlier, Andrej Karpathy, an OpenAI cofounder, had left the company. And in November 2023, Altman was briefly pushed out of OpenAI, an effort fellow cofounder Ilya Sutskever assisted. (Sutskever later expressed regret over his role. He formally left OpenAI just days after Amodei's jab, though there had been months of speculation surrounding Sutskever's standing.)\n\nJournalist Andrew Ross Sorkin asked Amodei about OpenAI's decision to declare a \"code red\" to marshal resources for ChatGPT amid Google's rising strength.\n\nWe have a little bit of a privileged position where we can just keep growing and just keep developing our models, and we don't have to do any code reds,\" Amodei told Sorkin during an appearance at The New York Times' DealBook summit.\n\nEarlier in their conversation, Amodei appeared to take another swipe at Altman when he talked about some players \"who are YOLOing\" by making too risky bets on future demand based on their current revenue.\n\n\"Who is YOLOing?\" Sorkin asked.\n\n\"I'm not going to answer that,\" Amodei replied.\n\nA who's who of AI and tech elite gathered in India for a major summit on artificial intelligence.\n\nIndian Prime Minister Narendra Modi took the opportunity to orchestrate a classic image of unity: competing CEOs with their hands raised together. (It's something Modi has done before with other world leaders, and politicians have been doing forever.)\n\nModi almost got his moment. While Altman held the prime minister's hand, the OpenAI CEO didn't grasp Amodei's hand, who was positioned to his other side. Amodei grasped the hand of the other person next to him, but not Altman's.\n\nThe internet, predictably, had a field day. And the world got a perfect encapsulation of one of AI's bitter rivalries.",
    "readingTime": 6,
    "keywords": [
      "minister narendra",
      "indian prime",
      "narendra modi",
      "sam altman",
      "openai released",
      "musk altman",
      "openai cofounder",
      "later openai",
      "amodei",
      "amodei's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sam-altman-dario-amodei-anthropic-openai-rivalry-timeline-2026-2",
    "thumbnail_url": "https://i.insider.com/69978bb3a645d1188189a3e1?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.244Z",
    "topic": "finance"
  },
  {
    "slug": "14-second-wave-startups-aiming-to-take-the-ai-era-beyond-cost-cutting",
    "title": "14 'Second Wave' startups aiming to take the AI era beyond cost cutting",
    "description": "AI startups Luvu, Status, and others lead a Second Wave, supported by Kylan Gibbs, creating unique experiences and revenue models in the AI space.",
    "fullText": "Second Wave AI startups are trying to move beyond cost-cutting and use this technology to create brand-new experiences and generate new sources of revenue.\n\nThe concept is being supported by Kylan Gibbs, a former DeepMind product manager who now runs an AI startup called Inworld. His company has raised more than $100 million from investors including Microsoft, Intel, and Founders Fund.\n\nIn January, Gibbs launched a Second Wave AI startup accelerator to back up to 30 \"Second Wave\" AI startups — companies building new consumer experiences rather than bolting chatbots onto old workflows. Venture capital firms, including Khosla Ventures and Lightspeed Venture Partners, are involved, alongside leaders from OpenAI, Google, and Stripe. A demo day will take place in early March in San Francisco, and participants get intros to potential investors.\n\nGibbs shared some examples of these types of startups with Business Insider.\n\nLuvu is an AI-powered fitness app that acts as a highly personalized personal trainer, using generative AI to send tailored motivational messages and real-time workout feedback. Through computer vision and reinforcement learning, it verifies users' exercises, adapts to their behavior, and creates a continuous feedback loop designed to boost engagement and retention. Luvu raised a pre-seed round with a16z speedrun and a seed round with investors including Insiders Ventures.\n\nStatus is an AI-powered social simulation game that lets users role-play in dynamic, AI-generated social media worlds, casting themselves as characters ranging from Hogwarts students to pro athletes. Powered by large language models, the app creates instant replies, evolving storylines, and even \"aura scores\" that grade interactions, turning open-ended AI responses into immersive, ever-changing gameplay. WishRoll, the startup behind Status, has raised more than $15 million in VC funding.\n\nParticle is an AI-native news platform that uses AI embeddings and generative tools to connect reporting with the most relevant insights from sources, including long-form podcasts. By mapping relationships between transcripts and stories, it automatically surfaces curated clips and adds summaries and context, bringing the most relevant information directly to readers. Particle has raised more than $10 million from investors, including Lightspeed and Axel Springer, the owner of Business Insider.\n\nThis is a consumer-facing \"talk to the Bible\" style app that lets users ask faith questions, request prayers, and explore scripture in a conversational interface, an example of AI being used to create an always-on, personalized spiritual guide rather than a back-office efficiency tool. Bible Chat raised about $14 million in funding from True Ventures and other investors last year.\n\nLiven positions itself as a \"self-discovery companion,\" blending mood tracking, habit-building tools, bite-sized courses, and an AI personal assistant named \"Livie\" that's meant to help users reflect, reframe, and stay consistent. It's more like a pocket coach than a productivity feature. Liven has raised a seed round.\n\nBorn is building social AI companions designed to bring real people together, not replace them: its flagship product is Friends, an app where users raise and co-parent cute virtual pets (like Pengu), play mini-games, and build routines with friends. The startup has framed this as an antidote to isolating, purely one-on-one chatbot dynamics. Born has raised about $25 million from investors, including Tencent and Accel.\n\nOtherHalf pitches itself as an immersive companion experience: anime-inspired 3D characters with expressive body language and real-time voice interactions, aiming to feel more emotionally resonant than plain text chat. Azimov, the startup behind OtherHalf, is backed by a16z speedrun.\n\nFlowGPT is less a single app than a consumer \"marketplace\" and community hub where people share, discover, and remix prompts (and increasingly agent-like experiences) for models like ChatGPT. FlowGPT raised $10 million from Goodwater Capital and DCM in 2024.\n\nTolan is a voice-first AI companion app built around animated alien \"best friends,\" designed to feel caring and emotionally supportive, while also nudging users toward healthier boundaries (like putting the phone down). OpenAI has highlighted Tolan's focus on low-latency, voice-first interaction and \"memory-driven personalities.\" Portola, the startup behind Tolan, raised $20 million in a Series A round led by Khosla Ventures last year.\n\nLittle Umbrella is an AI-powered social games studio aiming to blend party-game dynamics with generative AI, including titles designed for online communities, such as Discord. The startup raised $2 million in early 2025 from investors, including Zynga founder Mark Pincus.\n\nSpeak is an AI tutor app built around getting users to spoken fluency by practicing out loud, an approach that's closer to a personal conversation partner than a traditional gamified worksheet. In late 2024, Speakeasy Labs, the startup behind Speak, raised $78 million from investors including OpenAI, Accel, Khosla Ventures, and Y Combinator.\n\nHiggsfield is an AI video platform aimed at creators and marketers with tools for generating videos and effects from ideas, part of a broader consumer-first push to make cinematic content creation accessible without traditional production resources. Founded by the former head of generative AI at Snap, Higgsfield has raised $130 million as a valuation of more than $1 billion.\n\nTalkPal markets a digital language teacher that supports dozens of languages and emphasizes conversation practice, using AI to simulate speaking scenarios, deliver feedback, and keep learners practicing on demand.\n\nPromova is a large consumer language-learning platform that says it blends bite-sized lessons with AI tools and personalized plans, positioning itself as a more adaptive learning journey rather than a one-size-fits-all curriculum.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "business insider",
      "second wave",
      "ai-powered social",
      "a16z speedrun",
      "seed round",
      "startup behind",
      "second wave ai",
      "investors",
      "users",
      "generative"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/second-wave-startups-list-ai-era-beyond-cost-cutting-2026-2",
    "thumbnail_url": "https://i.insider.com/698fa9bca645d11881895f62?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.239Z",
    "topic": "finance"
  },
  {
    "slug": "ai-music-is-here-to-stay-and-it-could-offer-a-lifeline-to-struggling-record-companies",
    "title": "AI music is here to stay — and it could offer a lifeline to struggling record companies",
    "description": "A new survey showed more than half of listeners between the age of 18 and 44 listen to roughly three hours of AI music a week.",
    "fullText": "The rise of AI music has set off panic within the music industry. I feel it every day on my social media feeds.\n\nThe bands I follow post about it incredulously, scoffing at the idea that AI could replicate their organic magic. The music writers I follow are even more dismissive. They refuse to engage with it entirely, essentially closing their eyes and willing it to go away.\n\nWell, outside of my middle-aged indie-rock bubble, AI music is not going away. It's actually taking over. Millennial and Gen Z listeners are embracing it and making it part of their routine.\n\nA recent survey from Morgan Stanley found that more than half of listeners between 18 and 44 listened to an average of 2 1/2 to 3 hours of AI music a week. That's a substantial chunk of time — about the length of a Paul Thomas Anderson movie.\n\nFrom a markets and investing perspective, the question is what the rapid rise of AI music will mean for publicly traded record labels and major streaming platforms. The labels might seem particularly exposed, since they've long made their living by recruiting and promoting human artists.\n\nBut actually, Wall Street is almost unanimously bullish on stocks in the industry — like Warner and Universal — largely because their artist libraries are still immensely valuable. Lucrative licensing deals are already being inked, and analysts expect more to come. After all, it's tough to get AI music to sound just like Justin Bieber when you don't have a catalog to train the model on.\n\nRecord-label stocks also look attractive right now because they've fallen so much throughout the streaming era. Both Warner and Universal are down more than 30% from highs. Any upside driven by AI music — at least in the near term — will be a clawback of value erased by disruption in recent years.\n\nAnd how can we forget Spotify, the straw that stirs the streaming drink? Its stock has also fallen more than 30% from highs reached last year. But it just crushed subscriber-addition forecasts, and Wall Street is overwhelmingly convinced its existing scale and dominance will ensure the explosion of AI music runs through its platform.\n\nThe bullish theses on the trio of stocks faced a stiff challenge earlier this week, when Google's Gemini released a tool that will let users make 30-second AI music tracks.\n\nYou know the drill at this point: When a new AI tool is introduced for a specific task, it kickstarts an indiscriminate double-digit sell-off in the sector in question. It recently happened four times in a single week.\n\nWell, that didn't happen with Warner, Universal, or Spotify. They all hung in there admirably. Warner and Universal are basically flat in the two days since, while Spotify has rallied 5%.\n\nRather than rendering them extinct, AI music is expected to offer record labels — and investors in them — a much-needed lifeline. For the sake of their stock prices, these companies need to figure out an operating model that can work alongside the AI-music revolution. For now, Wall Street is hopeful.",
    "readingTime": 3,
    "keywords": [
      "record labels",
      "wall street",
      "warner and universal",
      "music",
      "streaming",
      "stocks",
      "rise",
      "industry",
      "follow",
      "away"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/ai-music-stock-market-impact-investing-strategy-record-label-outlook-2026-2",
    "thumbnail_url": "https://i.insider.com/69977ffca645d1188189a248?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.127Z",
    "topic": "finance"
  },
  {
    "slug": "pwc-engineers-built-an-ai-agent-to-tackle-the-corporate-worlds-least-sexy-task-spreadsheets",
    "title": "PwC engineers built an AI agent to tackle the corporate world's least sexy task: spreadsheets",
    "description": "When tech exec Matt Wood arrived at PwC, he found that massive spreadsheets were \"all anybody was working on.\"",
    "fullText": "The real way to judge a company's AI expertise isn't in the flashy headlines, but by looking at the \"unsexy\" work rolling out behind the scenes, Matt Wood, PwC's global and US commercial technology and innovation officer, told Business Insider.\n\nIf Wood's theory holds — that real AI prowess shows up in unglamorous advances — PwC's latest launch is certainly notable. After all, what could be less sexy than spreadsheets?\n\nThe Big Four firm announced this week that it has developed a \"frontier AI agent\" capable of reasoning over vast, enterprise-grade spreadsheets — something that conventional AI systems struggle with because of their complexity, size, and interdependencies.\n\nThe agent can understand and navigate spreadsheets, mimicking \"how experienced practitioners work: scanning, searching, jumping across tabs, integrating charts and receipts, and reasoning,\" PwC said in a press release.\n\nWood, who joined PwC in 2024 from a role as vice president of AI at Amazon Web Services, said that when he started, he'd noticed the wraparound, ultra-wide monitors filled with spreadsheets: \"That's all anybody was working on,\" he said.\n\nBut these were not \"your school soccer team budget spreadsheet,\" said Wood. The spreadsheets that power large enterprises are enormously complex, often containing millions of cells, charts, graphs, images, receipts, and dozens of interlinked workbooks. \"They are more like financial engines than they are spreadsheets,\" he told Business Insider.\n\nThese files often underpin business-critical decisions, yet PwC \"found that even today's modern AI was very poorly suited to managing these big enterprise spreadsheets,\" Wood said.\n\n\"They just kind of shrug and give up for want of a better word.\"\n\nCreating an AI capable of understanding and reasoning across large, complicated spreadsheet applications is what PwC's engineers set out to solve. Their solution was a \"genuine advance in the field,\" Wood said.\n\nThe agent has unlocked use cases across assurance, advisory, and tax, and boosts time saving on some tasks \"from literally days to hours,\" said Wood.\n\nHe gave the example of audit walkthroughs, where teams previously spent weeks manually gathering and validating evidence across numerous complex spreadsheets that existing AI tools couldn't handle.\n\nNow, users simply upload the files, and the frontier agent automatically maps their structure, extracts relevant data, and performs validation and consistency checks — tasks that would otherwise require combing through millions of rows by hand.\n\nThe result is faster meetings, less back-and-forth with clients, and cleaner, structured data ready for deeper AI-driven analysis, he said.\n\nPwC's AI spreadsheet agent was built in-house by engineers — a function the firm has been rapidly expanding as it shifts beyond the traditional roles associated with the Big Four.\n\nIn January, PwC launched a dedicated tech engineering career track to attract more technical talent, saying it wants to become \"a destination for top engineering talent.\"\n\nPreviously, the firm offered only consulting and accounting career paths. Wood told Business Insider that adding the engineering track is \"a signpost\" of its future plans.\n\nAt the same time, PwC is retraining non-technical employees. The US branch of the firm recently announced a companywide workplace learning strategy focused on knowledge sharing and on developing a mix of human and AI skills needed for the future.\n\nWood described the work engineers do at PwC as having two modes: \"transforming today\" and \"building for tomorrow.\"\n\nThe first focuses on improving current workflows — reducing back-and-forth with clients, increasing trust, and delivering work more efficiently. The second reimagines professional services from scratch: \"If you were to start from a blank piece of paper, what would professional services look like in an AI agent world?\" said Wood.\n\nPwC engineers also work directly on client engagements, building AI systems tailored to specific projects. For example, they help organizations reorganize and redesign their finance functions from the ground up using agents, Wood said.\n\nMany of the consulting industry's top players are pursuing similar investments in technical talent as AI reshapes the work they do.\n\nAccenture, already one of consulting's most technically sophisticated players, has added nearly 40,000 AI and data professionals in the last two years. They now account for roughly 10% of its global headcount.\n\nEY, another Big Four firm, has added 61,000 technologists since 2023, according to its latest annual report.\n\nHave a tip? Contact this reporter via email at pthompson@businessinsider.com or Signal at Polly_Thompson.89. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 4,
    "keywords": [
      "technical talent",
      "professional services",
      "spreadsheets",
      "agent",
      "firm",
      "across",
      "engineers",
      "wood",
      "reasoning",
      "spreadsheet"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/pwc-engineers-launch-ai-agent-enterprise-grade-spreadsheets-big-four-2026-2",
    "thumbnail_url": "https://i.insider.com/6995ce64f8731049f3af4e1c?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.111Z",
    "topic": "finance"
  },
  {
    "slug": "blue-owl-shopped-debt-for-a-coreweave-data-center-lenders-werent-sold",
    "title": "Blue Owl shopped debt for a CoreWeave data center. Lenders weren't sold.",
    "description": "Blue Owl tried to line up debt for a Pennsylvania data center tied to CoreWeave, but lenders passed, underscoring growing caution around AI-linked credit.",
    "fullText": "Blue Owl Capital, a leading investor in the data center boom, was unable to arrange financing for a $4 billion data center it is co-developing in Pennsylvania after pitching lenders to help bankroll the project in recent months.\n\nThe facility, 80 miles west of Philadelphia in the city of Lancaster, will be occupied by CoreWeave, a provider of artificial intelligence cloud computing services that has become a closely watched name in the AI race for its rapid expansion — and the billions of dollars of high-interest-rate debt it has taken on to fuel that growth.\n\nAn executive who arranges debt for major data center deals told Business Insider that the lack of interest in the Lancaster project was due to growing caution among lenders and investors about taking on sizable exposures to AI players with less-than-sterling credit.\n\nCoreWeave has a below-investment-grade rating of B+, according to S&P Global Ratings.\n\n\"We saw it. We passed,\" a senior executive at a large specialty lender told Business Insider.\n\nA spokesman for Blue Owl said that the company had \"considered\" third-party financing for the Lancaster project \"as we would with any transaction as we explore alternatives before choosing the most attractive path forward.\"\n\nThe spokesman added that the project, which he said is already under construction, \"is fully funded, on time, and on budget.\"\n\nIt is unclear whether Blue Owl has been funding construction entirely from its own capital. If Blue Owl is unable to raise debt for the Lancaster development, it could be on the hook for a potentially huge outlay of cash to pay for the data center's construction.\n\nThe situation shows the complications and risks involved in financing the massive buildout of infrastructure for AI computing.\n\nBrennan Hawken, an equity analyst at BMO Capital Markets who covers Blue Owl, said that difficulties to raise debt for the Lancaster project would raise concern.\n\n\"I'm not familiar with this deal, but if there is a struggle to find the debt financing, that's a bit of a red flag that I would want to drill into,\" Hawken said.\n\nBusiness Insider previously reported that major banks had recent difficulty selling off pieces of $38 billion of debt to finance the construction of two data center campuses that will be anchored by Oracle. Banks often sell pieces of such large commitments to other lenders to spread risk and also reap a quick profit.\n\nThe slowdown in interest in participating in that financing was due to worries about Oracle's enormous AI spending and whether the tech company's credit rating could be impacted by those outlays. Oracle has since sought to calm the lending market, announcing that it would raise up to $50 billion of cash from stock and bond offerings in order to \"maintain a solid investment-grade balance sheet.\"\n\nLast summer, CoreWeave announced it would lease 100 megawatts of initial capacity at the Lancaster data center and potentially expand its commitment to 300 megawatts. The company said it would pour up to $6 billion into the project to equip it with chips and other cloud infrastructure.\n\nA month later, in August, Chirisa Technology Parks announced it would partner with Blue Owl and Machine Investment Group to develop the project. The partnership said it would provide $4 billion of funding, an amount separate from CoreWeave's investment, to support the construction of the project's data center facilities.\n\nIn the fall, Blue Owl began shopping the development to potential lenders, a person familiar with that effort said.\n\nBlue Owl has been one of the most creative financial architects of the data center building boom. Last year, it structured a deal to partner with Meta in the ownership of a large data center campus that Meta will build and operate in Louisiana. Blue Owl utilized Meta's strong credit to raise $27.3 billion of investment-grade corporate bonds against its share of the project's equity, proceeds that will be used to help pay for construction, according to S&P.\n\nBlue Owl could arrange a similar type of vehicle that could attempt to tap the credit of an investment-grade customer of CoreWeave's who might use the Lancaster facility or Nvidia, the chipmaker that has purchased large stakes in CoreWeave. It could also potentially raise cash for construction debt by tapping large institutional investor clients to pool together a loan, Hawken said.\n\nBlue Owl is facing questions after reports emerged that it had permanently halted withdrawals on one of its retail private credit funds.\n\nMuch of the development of hyperscale data center campuses has sought to utilize the strong credit ratings and deep pockets of big-tech partners.\n\nFluidstack, a peer of CoreWeave's, announced a deal last year to lease a 168-megawatt data center in Colorado City, Texas, which will be built by the crypto mining firm Cipher. Google, Fluidstack's tenant for the project, said it would guarantee about half of the $3 billion due under the 10-year lease. Fluidstack signed another similar-sized lease in December with the data center builder TeraWulf that will also provide \"investment-grade credit support.\"",
    "readingTime": 5,
    "keywords": [
      "blue owl",
      "lancaster project",
      "center campuses",
      "business insider",
      "debt",
      "credit",
      "construction",
      "financing",
      "lenders",
      "investment-grade"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/blue-owl-financing-lancaster-data-center-coreweave-2026-2",
    "thumbnail_url": "https://i.insider.com/699789a1a645d1188189a39c?width=1200&format=jpeg",
    "created_at": "2026-02-20T12:34:25.029Z",
    "topic": "finance"
  },
  {
    "slug": "nvidia-dgx-spark-is-dgx-spark-blackwell",
    "title": "Nvidia DGX Spark: Is DGX Spark Blackwell?",
    "description": "DGX Spark is a desktop AI supercomputer that packs 128GB of unified memory and 1 PFLOP-class Grace Blackwell (GB10) performance into a palm-sized box. However, its internal GPU belongs to the SM12x series, distinct from the data center-grade Blackwell (SM100). This creates a subtle architectural gap: the latest LLM stacks, heavily reliant on MLA·DSA-specific kernels like GLM-5, \"Blackwell support\" alone doesn't guarantee immediate compatibility. This creates a subtle architectural gap requiring separate code management for Hopper, data center Blackwell, and consumer Blackwell. The engineering team examines Spark, which is based on Blackwell but features a slightly different architecture.",
    "fullText": "Scaling the Blackwell Architecture, © NVIDIA\n\nIn October 2025, NVIDIA released the DGX Spark, a compact AI supercomputer designed to bring datacenter-class AI performance to a desktop form factor. Built on the Grace Blackwell architecture, the DGX Spark packs 128GB of unified memory and 1 PFLOP of AI compute into a 150mm x 150mm chassis, enough to run and fine-tune large language models up to 200B parameters locally. At Lablup, we bought 20 units shortly after launch and have been running them in-house since.\n\nDifferent KV caching techniques, © [liorsinai.github.io](https://liorsinai.github.io/machine-learning/2025/02/22/mla.html)\n\nOur engineering team recently tried running GLM-5 inference on DGX Spark. GLM-5 uses two techniques together: MLA (Multi-head Latent Attention) and DSA (DeepSeek Sparse Attention). When a large language model generates text, it stores Key and Value vectors from previously processed tokens in a memory region called the KV cache,1 and this cache grows rapidly as models get bigger and inputs get longer. MLA compresses the cache into lower-dimensional vectors to save memory, while DSA reduces compute by dynamically selecting only the important tokens for attention calculation instead of attending to every token. MLA was first introduced in DeepSeek-V2 and later adopted by DeepSeek-V3 and GLM-5. GLM-5 adds DSA on top of that, improving both long-context performance and deployment cost. Running these techniques efficiently on GPUs requires specialized kernels,2 and the two most prominent implementations today are DeepSeek's FlashMLA and FlashInfer.\n\nBoth projects explicitly list Blackwell support, so we expected them to work on DGX Spark. They did not. FlashMLA ships two backends: one for SM100 (datacenter Blackwell) and one for SM90 (Hopper). The DGX Spark GPU reports as SM12x (compute capability 12.1), which is compatible with neither. The SM100 kernel requires the tcgen05 instruction and TMEM, a dedicated hardware memory for tensor operations. The SM90 kernel requires WGMMA. SM12x has none of these. FlashAttention had the same problem: SM12x is absent from the supported architecture list, and the runtime printed \"FlashAttention only supports Ampere GPUs or newer.\" FlashAttention 4 (FA4), the Blackwell-specific version under development, only supports SM100.\n\nDGX Spark is Blackwell. The kernels claim to support Blackwell. So why don't they work? Answering that question required looking more carefully at what's inside GB10.\n\nNVIDIA DGX Spark: Powered by the GB10 Superchip, © NVIDIA\n\nGB10 is a multi-die SoC co-designed by NVIDIA and MediaTek. A TSMC 3nm ARM CPU die (ten Cortex-X925 cores plus ten Cortex-A725 cores) connects to a Blackwell GPU die via NVLink C2C, and 128GB of LPDDR5X memory is shared between the CPU and GPU at 273 GB/s bandwidth. The GPU side has 48 SMs,3 6,144 CUDA cores, and delivers 1 PFLOP at sparse FP4. In terms of raw scale, it sits between the RTX 5070 and RTX 5070 Ti. Run nvidia-smi and the architecture field reads 'Blackwell.' NVIDIA calls GB10 a \"Grace Blackwell Superchip.\" There is no reason not to call it Blackwell.\n\nBut check the compute capability,4 and a different picture starts to emerge.\n\nCUDA's compute capability numbering reveals that Blackwell is not a single architecture. Datacenter and consumer/edge parts carry different compute capabilities.\n\nRTX 5090 is SM120, DGX Spark is SM121. I'll refer to both as SM12x throughout this post.\n\nThe compute capability numbers split into 10.x and 12.x, and this is not like the Ampere situation where 8.0 and 8.6 diverged within the same major version. The major numbers themselves are different, and 11.x is entirely vacant. Looking back across Pascal (6.0/6.1), Ampere (8.0/8.6), and Hopper/Ada (9.0/8.9), no generation has produced this wide a gap. CUTLASS build flags reflect the split too: datacenter targets use sm100a (architecture-accelerated) while consumer targets use sm120f (family-level), with different suffix conventions.\n\nTensor cores are the dedicated hardware that GPUs use to accelerate matrix multiplication, the core operation of deep learning. The instruction set (ISA6) that drives these tensor cores has changed with each generation.\n\nEficient, pipelined mainloop body used in CUTLASS GEMMs, © [NVIDIA](https://github.com/NVIDIA/cutlass/blob/2.11/media/docs/efficient_gemm.md)\n\nMatrix multiplication works by chopping large matrices into small tiles and computing them iteratively. Each tile's intermediate result (the accumulator) needs to be stored somewhere temporarily. From Volta through Hopper, these intermediate results lived in the register file,7 where CUDA cores and tensor cores had to share the same limited space. Datacenter Blackwell (SM100) added TMEM, a dedicated 256KB-per-SM hardware memory sitting next to the tensor cores, along with the tcgen05 instruction to control it. Tensor cores can now operate independently from CUDA cores without competing for register file bandwidth. That is the single biggest architectural change in SM100.\n\nSM12x lacks the TMEM hardware entirely, and therefore does not support the tcgen05 instruction that controls it. The same goes for Hopper's WGMMA: compiling WGMMA code targeting SM12x produces ptxas error: Instruction 'wgmma.fence' not supported on .target 'sm_120'. The tensor core instruction that SM12x actually uses is an extended version of mma.sync, the same instruction family that dates back to Ampere. It adds support for new numeric formats like FP4 and FP6, but the programming model itself is the oldest one available. Looking at the last column of the table, SM100 writes intermediate results to dedicated memory (TMEM), while SM12x reads from and writes to registers, exactly like Ampere. Datacenter Blackwell turned the tensor core into an autonomous compute unit; consumer Blackwell put new data types on top of the oldest programming model.\n\nBecause SM90 kernels use WGMMA, SM100 kernels use tcgen05, and SM12x kernels use extended mma.sync, there is no kernel compatibility among the three architectures. Developers have to maintain three separate code paths.\n\nPorting existing kernels to SM12x is not straightforward, either. A 64x64 matrix tile that SM100 processes in one operation must be broken into 16x8 chunks requiring 32 separate mma.sync calls. Memory load patterns, inter-thread synchronization, and thread organization all need to be redesigned.\n\nNVIDIA shipping different microarchitectures under the same generation name is not new to Blackwell. In the Ampere generation (2020), the datacenter GA100 (A100) was manufactured on TSMC 7nm with 64 FP32 cores per SM and HBM2 memory. The consumer GA102 (RTX 3090), released the same September, used Samsung 8nm, had 128 FP32 cores per SM, and shipped with GDDR6X. GA102 repurposed the Turing-era INT32 datapath to also handle FP32 operations, doubling the shader count per SM, while GA100 skipped that change to focus on FP64 compute and HBM bandwidth. Different fabs, different SM designs, different memory interfaces, same \"Ampere\" branding.\n\nWith Hopper/Ada Lovelace (2022), NVIDIA separated the names outright. Datacenter Hopper (H100, SM90) was announced in March 2022; consumer Ada Lovelace (RTX 4090, SM89) followed in September as a distinct architecture. Compute capabilities were different too: 9.0 for Hopper, 8.9 for Ada Lovelace.\n\nIn Pascal (2016), GP100 (Tesla P100) had NVLink and HBM2; GP102 (GTX 1080 Ti) had neither.\n\nDatacenter chips and consumer chips allocate their transistor budgets differently. Datacenter parts spend die area on FP64 compute, HBM memory bandwidth, NVLink/NVSwitch, and MIG partitioning. Consumer parts spend on RT cores, display outputs, and power efficiency. The entire GB10 SoC runs at 140W; a single B200 GPU draws 1,000W.\n\nSM100's TMEM, tcgen05, and 2-SM cooperative MMA are designed for datacenter environments where thousands of GPUs process massive matrix operations. Models like GLM-5 and DeepSeek-V3 use MoE8 architectures that activate only a subset of the model's parameters (expert subnets) per input token, so instead of running one large matrix multiply for a long time, they rapidly switch between many smaller ones. TMEM absorbs the accumulator storage pressure during these switches, and tcgen05's asynchronous execution allows prefetching the next expert's weights while the current expert is still computing. On a DGX Spark, where LPDDR5X provides 273 GB/s of memory bandwidth, the bottleneck is getting data from memory to the compute units, not tensor core throughput. Investing die area in TMEM and tcgen05 would yield little practical benefit under those conditions. SM12x keeping the proven mma.sync approach while adding FP4/FP6 support, 5th-gen tensor cores, and RT cores reflects this tradeoff. At Hot Chips 2025, NVIDIA disclosed that GB10 worked on TSMC 3nm A0 (first silicon) without revision, because it was assembled from validated IP blocks rather than designed from scratch.\n\nFlashMLA's sparse decoding kernel internally uses FP8 KV cache together with either WGMMA- or tcgen05-based matrix multiply. Neither is available on SM12x, so an SM12x backend would need to be written from the ground up. FlashInfer is in the same situation. FlashAttention's supported architecture list includes only Ampere, Ada Lovelace, and Hopper, and the runtime does not recognize SM12x as \"newer than Ampere.\" FlashAttention 4 (FA4) supports SM100 only. In the vLLM and SGLang ecosystems, SM12x-related issues keep surfacing. The most common: FP8 block-scaled GEMM kernels written for SM100 fail to run on RTX 5090 (SM120). SGLang's published DeepSeek-V3 serving configurations include H200 (SM90) and B200 (SM100) setups, but nothing for SM12x.\n\nTriton treats SM12x as SM80 (Ampere), disabling all Blackwell-related optimizations, and vLLM requires the --enforce-eager flag. Running attention kernels on SM12x is not entirely impossible: some developers have reported re-implementing FlashAttention in CUDA C++ using Ampere-era instructions (mma.sync, cp.async) or falling back to cuDNN's SDPA backend. These workarounds only support standard dense attention, though. MLA-specific kernels, sparse attention, and FP8 KV cache are out of reach, which makes it difficult to get full performance from models like GLM-5 or DeepSeek-V3.\n\nOne thing DGX Spark does offer that conventional desktops cannot: its 128GB of memory is shared between CPU and GPU without partitioning. A discrete GPU setup physically cannot do this. The bandwidth is lower than HBM, but being able to fit an entire 200B-parameter model in memory on a desktop machine is a rare capability. Once SM12x-native kernels materialize, the practical value of that memory pool changes considerably.\n\nGB10 is Blackwell. It has 5th-gen tensor cores, supports FP4/FP6 arithmetic, and includes RT cores. What it does not share with datacenter Blackwell is the instruction set. SM12x's tensor core programming model is closer to Ampere's mma.sync than to datacenter Blackwell's tcgen05, and for kernel developers, that means maintaining three separate code paths: Hopper, datacenter Blackwell, and consumer Blackwell.\n\nThis kind of datacenter-versus-consumer architectural split has recurred in every generation since Pascal, and the gap is wider in Blackwell because datacenter-only features like TMEM and tcgen05 are larger in scope than anything that came before. While a $4,000 desktop device cannot possibly contain the full instruction set of a $30,000+ data center GPU, the existence of instruction-level branching under the same 'Blackwell' name is an engineering detail worth noting for developers writing or relying on CUDA kernels.\n\nTry Backend.AI GO on DGX Spark. Supports DGX Spark's Unified Memory natively, for free.\n\nKV cache: A memory region where language models store Key and Value vectors from previously processed tokens. As models grow larger and inputs grow longer, this cache scales proportionally. ↩\n\nKernel: A program that runs on the GPU. Here, it refers to attention computation code that has been hand-optimized for specific GPU hardware. ↩\n\nSM (Streaming Multiprocessor): The basic compute building block of an NVIDIA GPU. Each SM contains CUDA cores and tensor cores, and overall GPU performance is determined by how many SMs a chip has and how each is configured. ↩\n\nCompute Capability: A version number NVIDIA assigns to indicate a GPU's hardware feature level. GPUs within the same generation receive different numbers if they support different instructions and features. ↩\n\nWarp: A group of 32 GPU threads that move in lockstep, executing the same instruction simultaneously. The fundamental unit of execution in GPU programming. ↩\n\nISA (Instruction Set Architecture): The set of instructions that hardware understands. Just as CPUs have x86 or ARM, GPUs have their own instruction sets, and two chips branded 'Blackwell' with different ISAs cannot run the same compiled code. ↩\n\nRegister file: The fastest storage available, sitting right next to the processor core. Its limited capacity means contention arises when multiple operations try to use it simultaneously. ↩\n\nMoE (Mixture of Experts): A model architecture that activates only a subset of 'expert' subnetworks per input, rather than using the entire model every time. This reduces actual compute relative to total parameter count. ↩",
    "readingTime": 11,
    "keywords": [
      "ada lovelace",
      "tmem dedicated",
      "previously processed",
      "processed tokens",
      "register file",
      "cuda cores",
      "code paths",
      "per input",
      "matrix multiplication",
      "matrix multiply"
    ],
    "qualityScore": 1,
    "link": "https://www.backend.ai/blog/2026-02-is-dgx-spark-actually-a-blackwell",
    "thumbnail_url": "https://cdn.lablup.com/DGX_Spark_actually_a_blackwell_en_2b3612ef17.jpg",
    "created_at": "2026-02-20T12:34:24.506Z",
    "topic": "tech"
  },
  {
    "slug": "modis-ai-summit-turns-awkward-as-tech-leaders-sam-altman-and-dario-amodei-dodge-contact",
    "title": "Modi’s AI summit turns awkward as tech leaders Sam Altman and Dario Amodei dodge contact",
    "description": "Indian Prime Minister Narendra Modi on Thursday invited leaders of some of the top artificial intelligence companies to gather on stage as part of a commitment to build more “inclusive and multilingua...",
    "fullText": "NEW DELHI (AP) — Indian Prime Minister Narendra Modi on Thursday invited leaders of some of the top artificial intelligence companies to gather on stage as part of a commitment to build more “inclusive and multilingual” AI around the world.\n\nAnd they did. But what caught some of the audience's attention, and later went viral on social media, was an awkward interaction between two rival tech leaders: OpenAI CEO Sam Altman and Anthropic CEO Dario Amodei.\n\nModi, host of the India AI Impact Summit in New Delhi, clasped hands with those closest to him — Altman to his left and Google CEO Sundar Pichai to his right — and beckoned all 13 tech leaders to lift their hands up in a chain, like theater actors at the end of a show.\n\nEveryone was holding hands except for Altman and Amodei, who stood next to each other but for several seconds awkwardly avoided hand contact. Both eventually put up their fists instead.\n\nThe interaction quickly became a visual symbol of the deep rivalries in the AI industry, particularly between OpenAI and Anthropic, though Altman sought to brush off any deeper meaning.\n\n“I didn't know what was happening,” Altman later said in a video interview with Indian media outlet Moneycontrol. He said he was “confused, like when (Modi) grabbed my hand and put it up, and I just wasn’t sure what we were supposed to be doing.”\n\nThe two AI developers have a history, one that predates the creation of OpenAI's hit product, ChatGPT, and Anthropic's competing chatbot Claude.\n\nAmodei worked at OpenAI before he and a group that included his sister, Daniela Amodei, quit to form Anthropic in 2021. The newer company promised a clearer focus on the safety of the better-than-human technology called artificial general intelligence that both San Francisco firms aim to build.\n\nOpenAI first released ChatGPT in late 2022, revealing the huge commercial potential of AI large language models that could help write emails and computer code and answer questions. Anthropic followed with its first version of Claude in 2023.\n\nTheir different approaches spilled over into public debate earlier this month in the United States when Anthropic aired TV commercials during the Super Bowl that ridiculed OpenAI for the digital advertising it’s beginning to place in free and cheaper versions of ChatGPT.\n\nWhile Anthropic has centered its revenue model on selling Claude to other businesses, OpenAI has opened the doors to ads as a way of making money from the hundreds of millions of consumers who get ChatGPT for free. Altman took to social media to criticize the TV commercials as dishonest.\n\nO'Brien reported from Providence, Rhode Island.",
    "readingTime": 3,
    "keywords": [
      "social media",
      "tech leaders",
      "modi",
      "indian",
      "artificial",
      "intelligence",
      "later",
      "interaction",
      "commercials",
      "free"
    ],
    "qualityScore": 0.9,
    "link": "https://www.yahoo.com/news/articles/modi-ai-summit-turns-awkward-164916441.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/EGggBLOY4XwIy3wT8JiXvQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD03MTY7Y2Y9d2VicA--/https://media.zenfs.com/en/ap.org/9fe9d8593ae5de3640a76f88c2104cf4",
    "created_at": "2026-02-20T12:34:21.002Z",
    "topic": "news"
  },
  {
    "slug": "mind-launches-inquiry-into-ai-and-mental-health-after-guardian-investigation",
    "title": "Mind launches inquiry into AI and mental health after Guardian investigation",
    "description": "Exclusive: England and Wales charity to examine safeguards after Guardian exposed ‘very dangerous’ advice on Google AI...",
    "fullText": "Exclusive: England and Wales charity to examine safeguards after Guardian exposed ‘very dangerous’ advice on Google AI Overviews\n\n‘Very dangerous’: a Mind mental health expert on Google’s AI summaries\n\nMind is launching a significant inquiry into artificial intelligence and mental health after a Guardian investigation exposed how Google’s AI Overviews gave people “very dangerous” medical advice.\n\nIn a year-long commission, the mental health charity, which operates in England and Wales, will examine the risks and safeguards required as AI increasingly influences the lives of millions of people affected by mental health issues worldwide.\n\nThe inquiry – the first of its kind globally – will bring together the world’s leading doctors and mental health professionals, as well as people with lived experience, health providers, policymakers and tech companies. Mind says it will aim to shape a safer digital mental health ecosystem, with strong regulation, standards and safeguards.\n\nThe launch comes after the Guardian revealed how people were being put at risk of harm by false and misleading health information in Google AI Overviews. The AI-generated summaries are shown to 2 billion people a month, and appear above traditional search results on the world’s most visited website.\n\nAfter the reporting, Google removed AI Overviews for some but not all medical searches. Dr Sarah Hughes, chief executive officer of Mind, said “dangerously incorrect” mental health advice was still being provided to the public. In the worst cases, the bogus information could put lives at risk, she said.\n\nHughes said: “We believe AI has enormous potential to improve the lives of people with mental health problems, widen access to support, and strengthen public services. But that potential will only be realised if it is developed and deployed responsibly, with safeguards proportionate to the risks.\n\n“The issues exposed by the Guardian’s reporting are among the reasons we’re launching Mind’s commission on AI and mental health, to examine the risks, opportunities and safeguards needed as AI becomes more deeply embedded in everyday life.\n\n“We want to ensure that innovation does not come at the expense of people’s wellbeing, and that those of us with lived experience of mental health problems are at the heart of shaping the future of digital support.”\n\nGoogle has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”.\n\nBut the Guardian found some AI Overviews served up inaccurate health information and put people at risk of harm. The investigation uncovered false and misleading medical advice across a range of issues, including cancer, liver disease and women’s health, as well as mental health conditions.\n\nExperts said some AI Overviews for conditions such as psychosis and eating disorders offered “very dangerous advice” and were “incorrect, harmful or could lead people to avoid seeking help”.\n\nGoogle is also downplaying safety warnings that its AI-generated medical advice may be wrong, the Guardian found.\n\nHughes said vulnerable people were being served “dangerously incorrect guidance on mental health”, including “advice that could prevent people from seeking treatment, reinforce stigma or discrimination and in the worst cases, put lives at risk”.\n\nShe added: “People deserve information that is safe, accurate and grounded in evidence, not untested technology presented with a veneer of confidence.”\n\nIf you have something to share about this story, you can contact Andrew using one of the following methods.\n\nThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.\n\nIf you don’t already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’.\n\nIf you don’t need a high level of security or confidentiality you can email andrew.gregory@theguardian.com\n\nSecureDrop and other secure methods\n\nIf you can safely use the tor network without being observed or monitored you can send messages and documents to the Guardian via our SecureDrop platform.\n\nFinally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each.\n\nThe commission, which will run for a year, will gather evidence on the intersection of AI and mental health, and provide an “open space” where the experience of people with mental health conditions will be “seen, recorded and understood”.\n\nRosie Weatherley, information content manager at Mind, said that although Googling mental health information “wasn’t perfect” before AI Overviews, it usually worked well. She said: “Users had a good chance of clicking through to a credible health website that answered their query, and then went further – offering nuance, lived experience, case studies, quotes, social context and an onward journey to support.\n\n“AI Overviews replaced that richness with a clinical-sounding summary that gives an illusion of definitiveness. They give the user more of one form of clarity (brevity and plain English), while giving them less of another form of clarity (security in the source of the information, and how much to trust it). It’s a very seductive swap, but not a responsible one.”\n\nA Google spokesperson said: “We invest significantly in the quality of AI Overviews, particularly for topics like health, and the vast majority provide accurate information.\n\n“For queries where our systems identify a person might be in distress, we work to display relevant, local crisis hotlines. Without being able to review the examples referenced, we can’t comment on their accuracy.”",
    "readingTime": 5,
    "keywords": [
      "ai overviews",
      "guardian app",
      "dangerously incorrect",
      "medical advice",
      "dangerous advice",
      "mental health",
      "health conditions",
      "safeguards",
      "mind",
      "experience"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/20/mind-inquiry-google-ai-overviews-mental-health-guardian-investigation",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b6f1974a3be968d635e4ccdae26f1860cb90dcd4/492_0_4916_3933/master/4916.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=ce1ecf227877ff823b1cef60a12f5830",
    "created_at": "2026-02-20T12:34:15.667Z",
    "topic": "tech"
  },
  {
    "slug": "openai-and-paradigm-launches-evmbench-to-test-ais-on-smart-contract-security",
    "title": "OpenAI and Paradigm Launches EVMbench to Test AIs on Smart Contract Security",
    "description": "OpenAI's EVMbench assesses AI agents on detecting, patching, and exploiting high-severity smart contract flaws . The benchmark comprises 120 curated vulnerabilities from 40 audits, including scenarios from the Tempo blockchain .",
    "fullText": "OpenAI and Paradigm launched EVMbench, aiming to evaluate AI agents’ ability to detect, patch, and exploit vulnerabilities within Ethereum-based smart contracts that collectively secure over $100 billion in crypto assets. EVMbench is based on 120 vulnerability types found in 40 different security audits (including several from Tempo blockchain) and will include scenarios involving payment-oriented smart contract code related to expected agentic stablecoin transactions.\n\nIntroducing EVMbench—a new benchmark that measures how well AI agents can detect, exploit, and patch high-severity smart contract vulnerabilities. https://t.co/op5zufgAGH\n\nEVMbench measures each artificial intelligence (AI) agent’s performance in three ways:\n\nEach test is conducted by using an Anvil-based, Rust-language harness to provide a deterministic and reproducible means of evaluating agents within isolated environments rather than live networks.\n\nAI’s Frontier models (the most recent ones) have performed significantly better than those measured six months ago. For example, GPT-5.3-Codex’s average performance on exploit tasks is 72.2%, while its predecessor model (GPT-5) had only 31.9% on the same exploits. Detect and patch modes remain more challenging to agents as they often stop their audit process after finding the first fault or have difficulties maintaining the perfect functionality of the contract while also removing vulnerabilities.\n\nnew collab from @paradigm and @OpenAI:\n\nevmbench is a benchmark and agent harness for exploiting smart contract bugs\n\na few months ago, the best models found <20% of critical, fund-draining @Code4rena bugs in our benchmark. today they find > 70% https://t.co/soOrCR38eO pic.twitter.com/2lr0WUVo2Q\n\nEVMbench tackles both sides of AI use in cybersecurity: monitoring new threats while encouraging defensive applications. As part of this initiative, OpenAI recently committed $10M of Application Programming Interface (API) credits through its Cybersecurity Grant Program to help boost efforts toward creating more defensive research and expanding Aardvark’s (its open-source security research agent) footprint with a private beta test.",
    "readingTime": 2,
    "keywords": [
      "smart contract",
      "agents",
      "detect",
      "patch",
      "exploit",
      "vulnerabilities",
      "benchmark",
      "within",
      "measures",
      "performance"
    ],
    "qualityScore": 0.9,
    "link": "https://timescrypto.com/cryptobuzz/ai-and-crypto/openai-paradigm-launches-evmbench-to-test-ai-capabilities-on-smart-contract-security/article-22555/",
    "thumbnail_url": "https://timescrypto.com/wp-content/uploads/2026/02/open-AI.jpg",
    "created_at": "2026-02-20T06:41:53.398Z",
    "topic": "finance"
  },
  {
    "slug": "a-guide-to-which-ai-to-use-in-the-agentic-era",
    "title": "A Guide to Which AI to Use in the Agentic Era",
    "description": "It's not just chatbots anymore",
    "fullText": "What really struck me reading this is how quickly the problem shifts from model capability to organisational design.\n\nOnce AI can reliably handle multi-step work, the hard question stops being which system is smartest and becomes how companies actually structure delegation, supervision, and accountability when the “worker” is software.\n\nIt feels like a lot of organisations are still treating this as a tooling decision, when in practice it’s already becoming an operating model question. We’re seeing that shift happen pretty quickly inside enterprise teams now.\n\nLove this! One thing I'd add is that while Gemini has the weakest general-purpose harness, the NotebookLM + Gemini integration is powerful (especially paired with Google's significantly larger context windows).\n\nPreviously, when my notebook couldn't answer something because it wasn't in my sources, I had to leave NotebookLM, search elsewhere, and reconcile manually. Now Gemini lets me combine my uploaded sources with live web information in a single conversation.\n\nFor podcast pre-production, I upload a guest's book, previous interviews, and biography into NotebookLM, then attach that notebook to Gemini and ask it to cross-reference my sources with their most recent public statements or interviews I haven't captured yet — so I can spot where their thinking has shifted and prepare sharper questions.\n\nFor lesson & curriculum design, I upload curriculum standards, past lesson plans, and student feedback. NotebookLM synthesizes gaps and aligns objectives, then in Gemini I can ask it to find current news / real-world examples from the web that bring a specific learning objective to life; without losing the grounding in my actual materials.",
    "readingTime": 2,
    "keywords": [
      "quickly",
      "model",
      "design",
      "upload",
      "interviews",
      "lesson",
      "curriculum",
      "gemini",
      "notebooklm",
      "notebook"
    ],
    "qualityScore": 0.85,
    "link": "https://www.oneusefulthing.org/p/a-guide-to-which-ai-to-use-in-the",
    "thumbnail_url": "https://substackcdn.com/image/fetch/$s_!O-pO!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff77c79c7-9fb2-4cd0-b075-a6201a212a6c_1456x816.png",
    "created_at": "2026-02-20T06:41:52.203Z",
    "topic": "tech"
  },
  {
    "slug": "edgequakelitellm-rustbacked-dropin-replacement-for-litellm-v01",
    "title": "Edgequake-litellm – Rust-backed drop-in replacement for LiteLLM (v0.1)",
    "description": "Unified LLM provider abstraction for Rust - support for OpenAI, Anthropic, Gemini, xAI, OpenRouter, and more - raphaelmansuy/edgequake-llm",
    "fullText": "raphaelmansuy\n\n /\n\n edgequake-llm\n\n Public\n\n Unified LLM provider abstraction for Rust - support for OpenAI, Anthropic, Gemini, xAI, OpenRouter, and more\n\n License\n\n View license\n\n 11\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n raphaelmansuy/edgequake-llm",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/raphaelmansuy/edgequake-llm",
    "thumbnail_url": "https://opengraph.githubassets.com/033a287a2d59f44716bd741ab3445e817d4de8a3a3b317ef42743208b4bea6cc/raphaelmansuy/edgequake-llm",
    "created_at": "2026-02-20T06:41:51.665Z",
    "topic": "tech"
  },
  {
    "slug": "photomeh-repair-cost-estimates-from-photos-in-under-60-seconds",
    "title": "PhotoMeh – Repair cost estimates from photos – in under 60 seconds",
    "description": "Get AI-powered car damage estimates from a photo. Upload an image and receive a detailed repair cost breakdown.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://photomeh.com",
    "thumbnail_url": "https://photomeh.com/og-image.jpeg",
    "created_at": "2026-02-20T06:41:50.943Z",
    "topic": "tech"
  },
  {
    "slug": "openai-offers-15000-and-support-resources-to-staff-affected-by-us-immigration-authorities",
    "title": "OpenAI offers $15,000 and support resources to staff affected by US immigration authorities",
    "description": "OpenAI confirmed it's offering resources to staff navigating ICE detention or other immigration issues, including $15,000 in reimbursement of legal fees.",
    "fullText": "OpenAI is rolling out a new package of resources for employees who may be navigating interactions with US immigration authorities.\n\nThe initiative is designed to help staff directly affected by Immigration and Customs Enforcement (ICE) detention or prolonged secondary inspection by US Customs and Border Protection (CBP) — including situations involving an employee or an immediate family member.\n\nThe move comes as the nation grapples with fallout from a series of federal immigration enforcement incidents, including lethal shootings, that have triggered widespread protests and corporate pushback.\n\nAccording to an OpenAI spokesperson, the support resources include:\n\n\"We regularly review our employee benefits to ensure they reflect the needs of our workforce,\" the spokesperson said in a statement to Business Insider. \"As part of that process, we updated our support resources for employees facing complex immigration-related situations.\"\n\nThe company, which relies on skilled workers from around the world, declined to specify how many employees might require the use of the resources.\n\nThe announcement follows internal conversations at OpenAI about federal immigration enforcement, including CEO Sam Altman's recent message to employees criticizing ICE's actions as going \"too far.\"\n\nBusiness Insider previously reported that in a late January internal Slack message, Altman expressed concern about recent immigration enforcement activity, sparking strong reactions within the company.\n\nAltman's comments came amid heightened national scrutiny over ICE and Border Patrol operations in Minneapolis, where federal immigration enforcement actions have drawn widespread criticism.\n\nIn his message, which also praised Trump, Altman said part of loving the country is recognizing and pushing back against government \"overreach.\"",
    "readingTime": 2,
    "keywords": [
      "federal immigration",
      "immigration enforcement",
      "resources",
      "employees",
      "openai",
      "message",
      "customs",
      "situations",
      "employee",
      "widespread"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/openai-reimbursement-legal-resources-staff-detained-immigration-customs-enforcement-2026-2",
    "thumbnail_url": "https://i.insider.com/6997ba07f8731049f3af730f?width=1200&format=jpeg",
    "created_at": "2026-02-20T06:41:49.531Z",
    "topic": "finance"
  },
  {
    "slug": "mark-cuban-on-2-types-of-ai-users-youre-either-using-it-to-learn-everything-or-so-you-dont-have-to-learn-anything",
    "title": "Mark Cuban on 2 types of AI users: you're either using it to 'learn everything' or 'so you don't have to learn anything'",
    "description": "Bill Gurley, a partner at the Silicon Valley venture capitalist firm Benchmark, agrees \"100%\" with Mark Cuban that there are two types of AI users.",
    "fullText": "Mark Cuban says there are two types of people who use AI. Which one are you?\n\n\"There are generally 2 types of LLM users, those that use it to learn everything , and those that use it so they don't have to learn anything,\" Cuban said of large language models in an X post on Tuesday.\n\nThe \"Shark Tank\" billionaire has been bullish about AI and said that companies need to embrace it.\n\nCuban has said there will be \"two types of companies: those who are great at AI, and everybody else,\" Business Insider's James Faris previously reported. He's also said that AI models can't provide all the answers and are \"stupid\" but like \"a savant that remembers everything.\"\n\nBill Gurley, a partner at the Silicon Valley venture capitalist firm Benchmark, agrees \"100%\" with Cuban that there are two types of AI users.\n\n\"If you are on a custom career path where you aim to differentiate yourself, AI is 'jet fuel' - you can learn and soar faster than ever before,\" Gurley said on X in response to Cuban.\n\nOr, it could have the opposite effect.\n\nEven some of AI's biggest proponents have warned that the technology could make people lazy.\n\nArthur Mensch, CEO of Mistral AI, said last year that the biggest risk to humans posed by AI was \"deskilling\" and employees becoming lazier as they rely too heavily on the AI tools.\n\n\"You want people to continue learning,\" he said in an interview with The Times of London. \"Being able to synthesize information and criticize information is a core component to learning.\"\n\nBusiness Insider reached out to Cuban for additional comment.",
    "readingTime": 2,
    "keywords": [
      "learn",
      "users",
      "everything",
      "models",
      "biggest",
      "learning",
      "cuban",
      "business",
      "gurley"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/mark-cuban-two-types-of-people-using-ai-learning-lazy-2026-2",
    "thumbnail_url": "https://i.insider.com/6997a531f8731049f3af7293?width=1200&format=jpeg",
    "created_at": "2026-02-20T06:41:49.529Z",
    "topic": "finance"
  },
  {
    "slug": "deepminds-ceo-says-using-ai-can-make-you-a-genius-or-hurt-your-critical-thinking-skills",
    "title": "DeepMind's CEO says using AI can make you a genius — or hurt your critical thinking skills",
    "description": "The DeepMind CEO said that AI is like the internet — people can use it to learn all kinds of topics, or use it in ways that \"degrade\" their thinking.",
    "fullText": "It's up to you whether AI makes you sharper or slowly dulls your brain, says Demis Hassabis.\n\nIn a Thursday interview with entrepreneur Varun Mayya on the sidelines of the India AI Impact Summit, the Google DeepMind CEO said that AI is just like the internet. People can use it to learn all kinds of topics, or use it in ways that \"degrade\" their thinking.\n\n\"With AI, if you use it in a lazy way, it will make you worse at critical thinking and so on,\" he said. \"But that's down to you as the individual. No one can help you do that.\"\n\nHe added that people need to be smart and use these technologies in ways that enhance their thinking rather than dull it.\n\nHassabis cofounded DeepMind in 2010, which Google acquired in 2014. It merged with Google Brain in 2023 to form Google DeepMind, the lab behind tools such as Gemini and Nano Banana. The CEO and a DeepMind coworker, John Jumper, were awarded the 2024 Nobel Prize for Chemistry for their work on protein structure prediction.\n\nAs AI gets incorporated into daily life, debates about its risks and rewards have intensified, with several tech leaders warning about the dangers of an overreliance on AI tools.\n\nEarlier this week, tech billionaire Mark Cuban said that there are two types of people who use AI.\n\n\"There are generally 2 types of LLM users, those that use it to learn everything, and those that use it so they don't have to learn anything,\" Cuban said of large language models in an X post on Tuesday.\n\nCuban has previously said that AI models can't provide all the answers and are \"stupid\" but like \"a savant that remembers everything.\"\n\nAt a June conference, the CEO of French AI lab Mistral said that a risk of using AI for everything is that humans will stop trying.\n\n\"The biggest risk with AI is not that it will outsmart us or become uncontrollable, but that it will make us too comfortable, too dependent, and ultimately too lazy to think or act for ourselves,\" Arthur Mensch said.",
    "readingTime": 2,
    "keywords": [
      "learn",
      "everything",
      "ways",
      "lazy",
      "tools",
      "tech",
      "models",
      "risk",
      "google",
      "deepmind"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/deepmind-ceo-demis-hassabis-ai-lazy-way-hurts-thinking-skills-2026-2",
    "thumbnail_url": "https://i.insider.com/6997cd7ff8731049f3af73bc?width=1200&format=jpeg",
    "created_at": "2026-02-20T06:41:49.370Z",
    "topic": "finance"
  },
  {
    "slug": "suttons-predictions-v-embrace-bassist-steve-firth",
    "title": "Sutton's predictions v Embrace bassist Steve Firth",
    "description": "BBC Sport football expert Chris Sutton takes on Embrace bassist Steve Firth plus the BBC readers and AI with his predictions for this weekend's Premier League fixtures.",
    "fullText": "New Tottenham boss Igor Tudor's first game in charge is Sunday's north London derby - but will he make a dream start at home against Arsenal or have a nightmare?\n\n\"Tottenham's stadium has been an unhappy place all season,\" said BBC Sport football expert Chris Sutton. \"Tudor's problem, if he doesn't get off to a fast start, is that there is going to be negativity around him too.\n\n\"Spurs are in a relegation fight and their fans will be asking why have we employed someone to keep us up who doesn't have a lot of knowledge of the Premier League?\"\n\nSutton is making predictions for all 380 Premier League games this season, against AI, BBC Sport readers and a variety of guests.\n\nHis guest for week 27 is Embrace bassist Steve Firth, who is a fan of Leeds United, who are 15th in the Premier League table after gaining promotion last season.\n\nEmbrace are celebrating their 30th anniversary in 2026 and their new track, Road To Nowhere, is out now and their new album, Avalanche, out in June.\n\nDo you agree with their scores? You can pick your own below.\n\nThe most popular scoreline selected for each game is used in the scoreboards and tables at the bottom of this page.\n\nA correct result (picking a win, draw or defeat) is worth 10 points. The exact score earns 40 points.\n\nFirth's favourite Leeds players are mostly from the 1990s, when he was an Elland Road regular, but more recently he was a big fan of midfielder Kalvin Phillips too.\n\n\"I always loved Gordon Strachan, Gary McAllister and David Batty - we had a superb midfield then,\" he told BBC Sport.\n\n\"Then we had Tony Yeboah up front for a couple of seasons, and he was amazing too.\n\n\"There were so many good players for Leeds back then, but I didn't get to see Eric Cantona play for us - that's the one thing I regret.\n\n\"In the past few years, Phillips really stood out before he went to Manchester City. He was a genuine superstar of the future when he was playing for us, so it is shame what has happened with him since.\n\n\"Now? The whole team has turned things around in the past few months because we were in serious trouble in November, but we have only lost a couple of games since then.\n\n\"I do think we're definitely safe now - we are going to do all right from now on and we will finish in mid-table.\n\n\"That is all we can really aspire to at the moment as a promoted team. You just need to hang in there as long as you can really and build up your squad slowly, without over-spending.\n\n\"We've started doing that already. We were very lucky to get Dominic Calvert-Lewin in for nothing, because he has been brilliant.\"\n\nChris Sutton and Steve Firth were speaking to BBC Sport's Chris Bevan.\n\nThe AI predictions were generated using Microsoft Copilot Chat - we simply asked the tool to 'predict this week's Premier League scores'.\n\nI was at Elland Road when Aston Villa won there in November - Leeds were really good in the first half but faded, and two Morgan Rogers goals gave Villa the points.\n\nLeeds' form has picked up a lot since then, however, and they are on 30 points now.\n\nThat would have been enough to keep them up last season, when third-bottom Leicester went down with 25 points, but they have still got some work to do.\n\nVilla have slipped up a couple of times at home in the past few weeks, losing unexpectedly to Everton and Brentford, and they are probably looking at the table thinking they should be breathing down leaders Arsenal's neck.\n\nI can see Leeds giving them another good game, but I don't see Unai Emery's side dropping points this time. Villa will find a way.\n\nSteve's prediction: This is a tough game but we're on a good run of form and I think we'll get a draw. 1-1\n\nSteve on why he supports Leeds: I'm from Halifax and everyone there either supports Leeds, Huddersfield or Manchester United. I've been a Leeds fan since the 1970s but the time I watched them most was in the 1990s - that's when I'd go most weeks.\n\nSince then, of the guys I went with, one died and the other one moved away so I got out of the habit and I only get to the occasional game now. But my wife is a Mansfield fan, so I have to go and watch them sometimes. I was hoping they would get Leeds in the FA Cup fifth round - we would definitely have gone to that one - but they got Arsenal instead.\n\nI covered Brentford in the FA Cup tie at Macclesfield on Monday and they were completely underwhelming.\n\nI know they were on a plastic pitch but they couldn't really find their flow and needed an own goal to progress.\n\nStill, the Bees have been in much better form in the Premier League - in stark contrast to Brighton.\n\nFabian Hurzeler's side have won only one of their past 13 league games in a poor run going back to the start of December, and it is not impossible they could get dragged into the relegation scrap.\n\nI can see this one being close but Brentford will have Kevin Schade back from suspension, while Igor Thiago will also return after being rested against Macclesfield - and those two should make the difference.\n\nSteve's prediction: Brentford are too good for Brighton. My predictions are going to be very conservative by the way, there are no 5-0s! 2-1\n\nSo much for me thinking that Burnley might give their fans something to cheer about by going on a good FA Cup run.\n\nAfter their brilliant fightback to beat Crystal Palace in their last league game, the Clarets made changes against Mansfield and went out on their own patch to a League One side.\n\nI just can't see anything other than a home win for Chelsea here, even with their wobble last time at Stamford Bridge where Leeds fought back from 2-0 down to draw 2-2 - Liam Rosenior's side won't let that happen again.\n\nSteve's prediction: Definitely a Chelsea win here. My wife was very happy when Mansfield beat Burnley last week. 2-0\n\nWest Ham are still in the bottom three but they have shown they are up for the fight in the past few weeks.\n\nYes, they conceded a late equaliser against Manchester United in their last league game, but they keep on picking up points and that must have given them belief they can stay up.\n\nBournemouth are on a good run now too, with three wins in their past four league games but, along with Brighton, they are the Premier League's draw specialists this season - both have had 10 so far.\n\nIt finished 2-2 when these two sides met on the south coast in November and I can see the points being shared this time too.\n\nSteve's prediction: I am going with Bournemouth to edge this. 1-2\n\nThis is the fourth meeting between these two clubs already this season, with another to come soon in the FA Cup fifth round - I bet they are sick of each other already.\n\nNewcastle have a long trip back from Azerbaijan to contend with before this game but at least they have put their Champions League tie with Qarabag to bed already and can focus fully on City rather than next week's second leg.\n\nThe Magpies still have an awful record at Etihad, however - they lost here in the Carabao Cup in January and have never managed a win in 20 visits in the Premier League.\n\nI don't think this will be easy for City, but I do think they will beat them, again, to move within two points of Arsenal.\n\nThe only thing I am not sure about is whether to captain Erling Haaland in my Fantasy team, because he has been out injured.\n\nSteve's prediction: I've always liked Newcastle but I think City are going to win the league so I am going to back them here. 2-0\n\nFair play to Wolves, because they have turned a bit of a corner under Rob Edwards.\n\nTheir results have still never looked like being enough to keep them up but they showed again against Arsenal that they are at least being competitive now.\n\nEven so, I can see this game being all about Jorgen Strand Larsen, following his £48m move from Wolves to Palace at the end of the January transfer window.\n\nPalace still need the points and I would not be surprised if he has a say in the outcome - let's go for him to score the winner.\n\nSteve's prediction: Wolves are down so I am giving Palace this one. 2-0\n\nWhen Vítor Pereira went in as Wolves manager in December 2024, he did brilliantly to keep them up - but he has essentially taken them down this season, picking up only two points from 10 games before he was sacked.\n\nNow he has arrived at Forest as their fourth manager of the season.\n\nI don't think his predecessor, Sean Dyche, deserved the sack. Based on the results in his 18 games in charge, Forest would be 12th in the table, but we are used to seeing managerial changes at the City Ground now.\n\nWe know Pereira will try to keep things tight, but Forest's biggest issue throughout this campaign has been scoring goals and I am not sure they will be able to get at Liverpool.\n\nArne Slot's side became the first team to win at the Stadium of Light when they edged out Sunderland in their last league game, and I can see a similar outcome here.\n\nSteve's prediction: Forest might get a new manager bounce but I still think Liverpool will win. As a Mansfield fan, my wife hates Forest so I am not allowed to like them either! It's the same with Chesterfield too. 0-2\n\nSunderland's form has dropped off a little bit in recent weeks but they have still had a phenomenal season and are probably safe already on 36 points - now they just have to see it through.\n\nIt will be interesting to see how Regis le Bris's side respond to their first home defeat of the season, but Fulham are hardly on a great run either with three straight league losses.\n\nMarco Silva's side beat the Black Cats at Craven Cottage in November, and will leapfrog them in the table if they win this time too - but I don't think that will happen.\n\nSteve's prediction: I wasn't sure about this one because they are both decent sides so I am going for a draw. 1-1\n\nTottenham Hotspur Stadium, 16:30\n\nFrom what I've read about new Tottenham manager Igor Tudor, he is a guy who goes in at clubs and, in the short term, gets a turn out of his team.\n\nSpurs really need that to happen now, because they desperately need a win or two to get out of reach of relegation.\n\nSo, this game is big for them for that reason, and also because they can put another dent in Arsenal's title hopes too.\n\nSpurs and their fans have not had a lot to shout about this season, but if they can get something here then this could be a defining moment in their campaign, and affect Arsenal as well. They would love that.\n\nIt was an incredible wobble by Mikel Arteta's team against Wolves, drawing 2-2 after being 2-0 up, and I certainly didn't see it coming.\n\nMaybe it is getting to be 'squeaky bum time' for them, but it didn't affect them when they went away to Leeds a couple of weeks ago and won convincingly.\n\nYou can imagine how Spurs will be champing at the bit, and I am expecting them to make a fast start and have a real go at them - but Arsenal have to deal with that, and find a way of bouncing back.\n\nI think the Gunners can do that, and their quality will make the difference in the end. I worry about Spurs in forward areas and it will be interesting to see how Tudor lines them up in defence too.\n\nUltimately, if Arsenal turn up and play how we know they can, then they will win - and I am expecting them to make a real statement.\n\nSteve's prediction: Two bottlers here! I'd like Arsenal to win the league, but I don't think they will. Man City are just used to doing it from here - they just get their heads down in the run-in, and they know what to do. 2-3\n\nManchester United lost at home to Everton in November despite the Toffees going down to 10 men in the 13th minute, when Idrissa Gueye was sent off for striking his team-mate Michael Keane.\n\nRuben Amorim's United just could not find a way of breaking Everton down that night, but there is a very different feel about them now Michael Carrick is in charge.\n\nThey needed a stoppage-time equaliser to get a point at West Ham last time out but they seem much more confident and have been playing well.\n\nThis will be a tough game for them but, as I've mentioned previously, Everton have picked up more points on the road this season than they have done at home.\n\nThat's another reason why I fancy United to get something here, and stop David Moyes doing the double over his former club.\n\nSteve's prediction: We were all enjoying United's mid-table mediocrity but they have started to win now. 1-2\n\nChris correctly picked the winner in 10 of the 15 fourth-round ties to have been played so far - Port Vale versus Swindon was postponed and takes place on 3 March.\n\nHe got the better of his guests, Looney Tunes stars Daffy Duck and Porky Pig - Daffy has nine correct predictions, while Porky has eight.\n\nAI did better with a score of 11, again by avoiding any surprise results, but it is the BBC readers who continue to lead the way, with a tally of 13.\n\nOverall, Chris has now correctly picked the winner in 31 of the 47 ties from round three onwards.\n\nHis guests are on 26, AI is on 34, but the BBC readers are in pole position with 36.\n\nThe shock of the round was Mansfield's win at Burnley which caught out Chris, AI, and most of you lot.\n\nDaffy and Porky did see it coming, however, and out of around 25,000 predictions, more than 3,000 of you also backed the Stags.\n\nChris got three correct results from the 10 midweek Premier League matches in week 26 [which took place from 10-12 February] with one exact score, for a total of 60 points.\n\nThat was enough to beat the BBC readers, who got three correct results with no exact scores, giving them 30 points.\n\nHe also got the better of his guest, golf star Ian Poulter, who got two correct results with no exact scores, for a tally of 20 points.\n\nBut the weekly win went to AI, which got four correct results with two exact scores, for a tally of 100 points.\n\nThere was one other Premier League game played this week - Arsenal's draw with Wolves on Wednesday, which was rearranged because the Gunners are in the Carabao Cup final on 22 March.\n\nEveryone went for an Arsenal victory, so no extra points were scored.\n\nListen to the latest Football Daily podcast\n\nGet football news sent straight to your phone",
    "readingTime": 14,
    "keywords": [
      "cup fifth",
      "bbc readers",
      "steve's prediction",
      "mansfield fan",
      "chris sutton",
      "supports leeds",
      "premier league",
      "fifth round",
      "correctly picked",
      "exact scores"
    ],
    "qualityScore": 1,
    "link": "https://www.bbc.com/sport/football/articles/cx2j8wljzp3o?at_medium=RSS&at_campaign=rss",
    "thumbnail_url": "https://ichef.bbci.co.uk/ace/branded_sport/1200/cpsprodpb/be6f/live/5236b240-0d7f-11f1-b7e1-afb6d0884c18.png",
    "created_at": "2026-02-20T06:41:47.759Z",
    "topic": "sports"
  },
  {
    "slug": "amazons-cloud-unit-hit-was-hit-by-least-two-outages-involving-ai-tools-in-december-ft-says",
    "title": "Amazon’s cloud unit hit was hit by least two outages involving AI tools in December, FT says",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/amazons-cloud-unit-hit-by-at-least-two-outages-involving-ai-tools-ft-says-4515160",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1J07A_L.jpg",
    "created_at": "2026-02-20T06:41:46.262Z",
    "topic": "finance"
  },
  {
    "slug": "im-deeply-uncomfortable-anthropic-ceo-warns-that-a-cadre-of-ai-leaders-including-himself-should-not-be-in-charge-of-the",
    "title": "‘I’m deeply uncomfortable’: Anthropic CEO warns that a cadre of AI leaders, including himself, should not be in charge of the technology’s future",
    "description": "Dario Amodei, who left OpenAI before founding Anthropic, has been outspoken about the need for greater AI regulation.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/article/why-is-anthropic-ceo-dario-amodei-deeply-uncomfortable-companies-in-charge-ai-regulating-themselves/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/11/GettyImages-2235057491-e1763403934518.jpg?resize=1200,600",
    "created_at": "2026-02-20T01:08:35.909Z",
    "topic": "business"
  },
  {
    "slug": "walmarts-growing-share-of-highincome-shoppers-helped-its-ecommerce-business-turn-into-a-powerhouse",
    "title": "Walmart's growing share of high-income shoppers helped its e-commerce business turn into a powerhouse",
    "description": "Walmart has steadily gained share from households earning over $100,000 who come for the savings and stay for the convenience — and AI is helping.",
    "fullText": "Walmart's former side hustle is getting serious.\n\nThe retail giant reported that 2025 was its first full year of profitability in its e-commerce operations, which started — as most ventures do — needing time and money to get rolling.\n\n\"We've far surpassed the breakeven level,\" CFO John David Rainey said of the e-commerce business in the company's fourth quarter earnings call Thursday. \"The momentum is only upward from here.\"\n\nThe bet is paying off thanks in large part to the addition of higher-income shoppers, who Walmart said are attracted to its combination of low prices and increasing convenience.\n\n\"The majority of our share gains came from households making more than $100,000,\" CEO John Furner said on the earnings call.\n\nThe company said high-earning households have responded favorably to Walmart's online and in-app offerings, from ordering their weekly groceries to picking out stylish new outfits, as has been the case for several quarters now.\n\nFor the fiscal year, Walmart's overall sales grew 4.7% to $713.2 billion, with e-commerce increasing by nearly 25% to top $150 billion.\n\nDespite Walmart's growth, e-commerce giant Amazon dethroned it as the world's largest company by revenue last year with $716.9 billion in revenue. The stock dipped immediately after the earnings report but recovered during trading on Thursday.\n\nBut Walmart's progress in e-commerce could give Amazon a run for its money, thanks to its physical proximity to 95% of US households.\n\n\"Stores are a huge part of the solution to deliver the customer experiences that the customers are looking for,\" CEO John Furner said on the call. \"Having the US with 5,200 locations between Walmart and Sam's, where inventory is forward-deployed, is really helpful.\"\n\nAmazon, for its part, is working hard to counter Walmart's dominant physical presence, which so far has given it an edge in delivering items like groceries and everyday essentials in an hour or less.\n\nIt's not just fresh food delivery that's helping Walmart reach new shoppers. Rainey said shoppers are using some of the savings from groceries to try new items from other categories.\n\n\"It's not just convenience,\" Rainey said. \"I'd argue fashion is not really a convenience item. It shows that our broader assortment is appealing to a much larger customer base.\"\n\nFashion not only led sales growth in general merchandise in the US, Rainey said, but on-trend styles are also helping draw in more of these high-income households, who wind up sticking around.\n\n\"They're coming to Walmart, in many cases, some of them for the first time, and they're enjoying an experience that makes them want to come back these platforms,\" he said.\n\nGlobalData retail analyst Neil Saunders said his firm's data also shows wealthy households increasingly choosing Walmart for more than groceries, thanks to its expanded assortment and in-store experiences.\n\nIt's still too soon to see the impact of Walmart's recent AI partnerships, but Furner says the early results promise to turbocharge sales online and in-app. He said the company's in-house shopping assistant bot, Sparky, has led to 35% larger transaction totals.\n\n\"Roughly half of our app users have used Sparky, and when they use Sparky, it drives them to build bigger baskets,\" US segment CEO David Guggina said.\n\nWalmart still has a long road ahead to catch Amazon's online sales, but the past year has shown that it now has a powerful engine to get there.",
    "readingTime": 3,
    "keywords": [
      "ceo john",
      "john furner",
      "e-commerce",
      "households",
      "groceries",
      "sales",
      "earnings",
      "thanks",
      "shoppers",
      "convenience"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/high-income-shoppers-boost-walmarts-e-commerce-business-2026-2",
    "thumbnail_url": "https://i.insider.com/69973672e1ba468a96ac4efd?width=1200&format=jpeg",
    "created_at": "2026-02-20T01:08:35.716Z",
    "topic": "finance"
  },
  {
    "slug": "tension-over-a-proposed-ai-data-center-leads-to-an-arrest-in-oklahoma",
    "title": "Tension over a proposed AI data center leads to an arrest in Oklahoma",
    "description": "Beale Infrastructure is proposing a data center in Claremore, Oklahoma, a town of about 20,000. Over a hundred locals met to discuss the plans this week.",
    "fullText": "Applause broke out during an Oklahoma man's speech at a city council meeting on Tuesday to discuss a proposed data center. A minute later, shouts of disbelief rang out across the room.\n\n\"Disgusting!\" one woman shouted as Claremore Police Department officers handcuffed and escorted Daniel Blanchard out of the room.\n\nAuthorities said they arrested Blanchard, whose speech exceeded the three-minute time limit, for trespassing.\n\nOver 100 people, including Blanchard, had gathered in a ballroom at Rogers State University in Claremore to voice their opinions about the large data center project. The developer, Beale Infrastructure, is proposing a campus in the Claremore Industrial Park that includes data centers, supporting infrastructure, and office space.\n\nBlanchard was among the residents who opted to speak during the public comment portion, which limits each person to three minutes. In his speech, Blanchard spoke about what he considered compliance issues related to the potential data centers.\n\n\"The Claremore Industrial Economic Development Authority has a fiduciary responsibility to the public, not to build infrastructure. And this act of overreach is putting the health and safety of members of this community at risk,\" he said.\n\nAI is driving a data center construction boom across the United States. While companies like OpenAI argue that building new data centers will reindustrialize the US economy and create jobs, residents of towns where developers are proposing new data centers worry about their impact on power grids, water resources, pollution, and overall quality of life.\n\nIn an investigation published in September, Business Insider reported that over 1,200 data centers had already been built or were approved for construction across the country.\n\nThe proposed data center in Claremore, a suburban hub of Tulsa home to about 20,000 people, has divided the town. During the three-hour meeting on Tuesday evening, dozens of residents spoke both in favor and against the project.\n\nBlanchard exceeded his three minutes by about 30 seconds before police officers approached him. He gathered his notes and calmly followed the officers to the front of the hall, where town officials were sitting.\n\nIn a video of the meeting posted by the town on its YouTube channel, Blanchard appears to hand his notes to a council member. At that point, police arrested Blanchard, placing him in handcuffs. The crowd hollered in shock.\n\nIn a statement, the Claremore Police Department said officers aren't responsible for enforcing city council rules and only become involved in city council meetings when an official orders them to remove an individual.\n\n\"The man's position on the issues, what he said, or his unwillingness to follow rules of the meeting played no part in the officer's decision to arrest him,\" the statement said. \"He was arrested for trespassing in compliance with the law and with the hope of restoring order to an important meeting.\"\n\nA local politician fighting the data center project posted to X on Wednesday that Blanchard has been released from jail. The next council meeting is scheduled for March 2.",
    "readingTime": 3,
    "keywords": [
      "police department",
      "claremore industrial",
      "arrested blanchard",
      "city council",
      "center project",
      "centers",
      "officers",
      "speech",
      "across",
      "residents"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/data-center-meeting-claremore-oklahoma-man-arrested-beale-infrastructure-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/69975156f8731049f3af6a6b?width=1200&format=jpeg",
    "created_at": "2026-02-20T01:08:35.415Z",
    "topic": "finance"
  },
  {
    "slug": "rumors-suggest-apple-and-meta-are-betting-big-on-ai-wearables",
    "title": "Rumors Suggest Apple and Meta Are Betting Big on AI Wearables",
    "description": "Great: more stuff I'll lose.",
    "fullText": "The next generation of Meta's Display smart glasses might come with a smart watch. According to a report from The Information, Meta's watch, codenamed Malibu 2, could feature fitness tracking features and AI, but its real purpose is to replace the Display's neural band and act as controller for the smart glasses. If the reports are accurate, Meta Display smart glasses with a smart watch could be available in 2026.\n\nThere aren't any other details on the smart watch, so we don't know the price or what features it may have—but I'd be surprised if this rumor doesn't pan out eventually. Meta has discussed the idea of a smart watch before, and it makes sense: If you're going to have a wrist-controller for your glasses, why not give it smart watch features as well? Especially if a glasses-and-watch combo potentially gives users a reason to switch away from their Apple Watches.\n\nSpeaking of Apple, if the rumors about the company are true, Apple is pushing to release its own suite of AI-powered wearable devices. According to a report in Bloomberg, Apple could roll out smart glasses in early 2027. The company is also reportedly developing an AI-powered pendant that can \"be pinned to a shirt or worn as a necklace,\" as well as AirPods with expanded AI capabilities. The AirPods and pendant will be equipped with cameras designed to \"help the AI work\" as opposed to taking photos. Apple's smart glasses will reportedly not feature a display, but will feature a higher end camera and superior build quality to Meta's smart glasses. All of Apple's devices are reportedly designed to work with iPhones.\n\nNone of this is confirmed, of course. The closest Apple has come to announcing these plans is CEO Tim Cook mentioning “categories of products” enabled by artificial intelligence at an all-hands meeting. However, everything points to Meta and Apple betting that consumers want a collection of connected AI-wearables. Each company is taking a different approach to hooking users into their ecosystem. Apple seems to be betting on devices integrated with iPhones and controlled with the kind of camera-based tech that powers the Apple Vision Pro headset. Meta seems to be aiming at replacing phones with an in-glasses display, and a biometric control scheme that works with muscle movements, like the existing neural band.\n\nBoth Meta and Apple seem to be competing to go beyond a screen or smart glasses to become the next interface for your life—but do people want that? Are consumers excited enough by the prospect of always-available AI and tied-together devices to buy them? That's the big question, and the answer is anything but certain. Both Apple and Meta have made big bets on virtual reality, and, despite both companies' VR devices being excellent, neither seems to have captured the market in way these firms would have liked. So, as they say, stay tuned.",
    "readingTime": 3,
    "keywords": [
      "neural band",
      "smart glasses",
      "smart watch",
      "display smart",
      "devices",
      "feature",
      "features",
      "apple",
      "reportedly",
      "users"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/rumors-suggest-apple-and-meta-are-betting-big-on-ai-wearables?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KHVZDAVD0TAN6H7PS72TK8VY/hero-image.fill.size_1200x675.png",
    "created_at": "2026-02-20T01:08:33.144Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-openai-near-30-billion-investment-in-place-of-unfinished-100-billion-deal-ft-reports",
    "title": "Nvidia, OpenAI near $30 billion investment in place of unfinished $100 billion deal, FT reports",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/nvidia-close-to-finalizing-30-billion-investment-in-openai-funding-round-ft-reports-4515044",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPEM1J00P_L.jpg",
    "created_at": "2026-02-20T01:08:30.571Z",
    "topic": "finance"
  },
  {
    "slug": "tracekit-find-what-your-ai-coding-agent-wastes-money-on-and-fix-it",
    "title": "Tracekit: Find what your AI coding agent wastes money on and fix it",
    "description": "Find what your AI coding agent wastes money on and fix it. - 0xKoda/tracekit",
    "fullText": "0xKoda\n\n /\n\n tracekit\n\n Public\n\n Find what your AI coding agent wastes money on and fix it.\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n 0xKoda/tracekit",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/0xKoda/tracekit",
    "thumbnail_url": "https://opengraph.githubassets.com/11a5cf71ae84d57280c6b28ea6ee3a9767b0797fa5020046caf736f7ce11efb1/0xKoda/tracekit",
    "created_at": "2026-02-20T01:08:18.674Z",
    "topic": "tech"
  },
  {
    "slug": "amc-theatres-will-refuse-to-screen-ai-short-film-after-online-uproar",
    "title": "AMC Theatres Will Refuse to Screen AI Short Film After Online Uproar",
    "description": "AMC has opted out of screening the award-winning short film during its third-party programmed pre-show, and it’s unclear if other chains will follow suit.",
    "fullText": "When will AI movies start showing up in theaters nationwide?\n\nIt was supposed to be next month.\n\nBut when word leaked online that an AI short film contest winner was going to start screening before feature presentations in AMC Theatres, the cinema chain decided not to run the content.\n\nThe issue began earlier this week with the inaugural Frame Forward AI Animated Film Festival announcing Igor Alferov’s short film Thanksgiving Day had won the contest. The prize package for included Thanksgiving Day getting a national two-week run in theaters nationwide. When word of this began hitting social media, however, some were dismayed by the prospect of exhibitors embracing AI content, with many singling out AMC Theatres for criticism.\n\nExcept the short is not actually programmed by exhibitors, exactly, but by Screenvision Media — a third-party company which manages the 20-minute, advertising-driven pre-show before a theater’s lights go down. Screenvision — which co-organized the festival along with Modern Uprising Studios — provides content to multiple theatrical chains, not just AMC.\n\nAfter The Hollywood Reporter reached out to AMC about the brewing controversy, the company issued this statement to THR on Thursday: “This content is an initiative from Screenvision Media, which manages pre-show advertising for several movie theatre chains in the United States and runs in fewer than 30 percent of AMC’s U.S. locations. AMC was not involved in the creation of the content or the initiative and has informed Screenvision that AMC locations will not participate.”\n\nIt’s not yet clear if other theatrical chains will screen the short instead. Screenvision Media had no immediate comment, but the Frame Forward AI Animated Film Festival issued a statement from co-organizer MUS’s president and studio head Joel Roodman.\n\n“The national theatrical run, while truncated, is only an initial prize exposure for the winning film,” he said. “The film will be adapted for Celeste’s Massive Immersive theatrical venues, the first of which will be built in New York within the year. Shared theatrical experiences are an important cultural bond. The traditional theatrical chains are vital to our cohesion as a society, and are duly cautious [about AI]. However, the media landscape is changing and evolving rapidly. They may be prudent, but it is important to MUS immersive that new and exciting films, filmmakers, cinematic language, and spaces for these shared experiences continue to develop. We will bring new content, and important existing content, to our developing venue network of venues, starting in New York. We will not see the theatrical window wither on our watch.”\n\nThe film wouldn’t represent the first time AI content has played in theaters — a collection of film festival AI shorts from Runway’s 2025 AI Film Festival played in 10 Imax theaters in August. But this would likely be the first time a narrative AI film received nationwide exposure in cinemas rather than specialty screenings, and could represent another step in the groundbreaking technology marching into traditional Hollywood spaces.\n\nThe Thanksgiving Day film “follows a bear and his platypus assistant who are traveling through the galaxy in a spacecraft that looks like a dumpster. They have to deal with corrupt space-cops, hygiene officials, and a very unusual type of food delivery service as the story unfolds.”\n\nAccording to Deadline, Kazakhstani filmmaker Alferov used AI tools including Gemini 3.1 and Nano Banana Pro to make the short. Roodman said, “Thanksgiving Day is a masterclass in original storytelling, a wildly inventive journey that balances sharp satire with unexpected emotional payoff, proving that bold imagination with the tools of AI complements the future of animated filmmaking.”",
    "readingTime": 3,
    "keywords": [
      "frame forward",
      "film festival",
      "animated film",
      "theaters nationwide",
      "theatrical chains",
      "thanksgiving day",
      "screenvision media",
      "amc theatres",
      "content",
      "contest"
    ],
    "qualityScore": 1,
    "link": "https://www.hollywoodreporter.com/movies/movie-news/ai-short-movie-amc-theaters-1236509143/",
    "thumbnail_url": "https://www.hollywoodreporter.com/wp-content/uploads/2026/02/Thanksgiving-Day-H-2026.jpg?w=1296&h=730&crop=1",
    "created_at": "2026-02-20T01:08:17.666Z",
    "topic": "entertainment"
  },
  {
    "slug": "why-most-ai-agent-directories-are-basically-useless",
    "title": "Why Most AI Agent Directories Are Basically Useless",
    "description": "Let's be honest—most AI agent directories are just glorified link farms with the same 50 tools copy-pasted across them. Here's why that sucks, and what actually makes a directory worth your time.",
    "fullText": "Look. I've spent the last three months building AgentRank, and before that I wasted way too many hours scrolling through AI directories that all blur together into the same useless mess.\n\nYou know the ones. Pages and pages of logos, vague descriptions like \"AI-powered productivity tool,\" pricing that's always \"Contact for quote,\" and zero actual insight into whether the thing works or will waste your afternoon.\n\nIt's exhausting. And honestly? Kind of insulting.\n\nHere's what drives me nuts about 99% of AI agent directories out there:\n\nSeriously. Open three different \"AI tool directories\" and you'll see the exact same agents listed in almost the exact same order. ChatGPT, Claude, Jasper, Copy.ai. Cool. I could've Googled that.\n\nNobody's actually using half these tools. They're just scraping ProductHunt, slapping together a list, and calling it a day.\n\n2. The descriptions are worthless\n\n\"Revolutionizes workflows.\" \"Supercharges productivity.\" \"AI-powered innovation.\"\n\nGreat. What does it do? Can it write emails? Schedule meetings? Help me fix my leaky faucet? The marketing copy tells me nothing.\n\nI don't need your hype. I need to know if this tool solves my actual problem before I waste 20 minutes signing up and realizing it's useless.\n\nEvery listing is a press release. Five-star everything. \"Amazing!\" \"Game-changer!\" \"Must-have!\"\n\nZero honesty about what sucks. Zero comparison to alternatives. Zero \"this is great IF you need X, but terrible if you need Y.\"\n\nIf you're listing every tool as perfect, you're not helping me choose. You're just trying to collect affiliate fees.\n\nOh cool, another directory where everything is filed under \"productivity\" or \"AI assistant.\" Super helpful.\n\nI don't care that something is an \"AI agent.\" I care if it writes cold emails, debugs my code, or helps me find home repair contractors. Generic buckets don't cut it.\n\nHalf the listings are for tools that shut down six months ago or got acquired and nuked. Nobody's maintaining these directories. They're SEO graveyard.\n\nClick a link. Dead. Try another. 404. Great use of my time.\n\nHere's the thing—I'm not building AgentRank to be yet another link farm. I'm building it because I got fed up with the garbage out there.\n\nSo what makes a directory worth using? Here's my take:\n\nWhen I list a tool, I'm telling you what it's good at and where it falls short. Fixy's great for home repair DIY, but it's only on iOS right now. Claude's better for coding than ChatGPT, but costs more.\n\nHonesty. Specific use cases. Tradeoffs.\n\nIf I haven't actually used a tool, I'm not listing it.\n\n\"AI writing assistant\" doesn't help me. What I need to know is: Jasper vs Copy.ai for blog posts—which one gives me fewer robotic sounding paragraphs?\n\nReal comparisons based on actual use. Not feature charts, but \"here's when you'd pick A over B.\"\n\nI'm not filing Fixy under \"productivity.\" It goes under \"home improvement.\" Because that's what you'd search for when your toilet's running and you're trying to figure out if you can fix it yourself.\n\nContext matters. Use cases matter.\n\nDead tools get archived, not left to rot. Pricing is updated. Screenshots are recent. If something changed, the listing reflects it.\n\nNobody wants to waste time on outdated garbage.\n\nI'm not listing every AI tool that exists. If I wouldn't use it or recommend it, it's not going on AgentRank.\n\nThis started because I kept Googling \"best AI agent for X\" and getting the same recycled lists. Nothing useful. Nothing honest.\n\nSo I'm building the directory I wish existed:\n\nHonest takes on what's good and what sucks\n\nReal-world use cases, not buzzwords\n\nWill it be perfect? No. But it'll be a hell of a lot better than the copy-paste farms out there.\n\nI'm adding new agents every week. Real ones. Tested ones. With actual opinions attached.\n\nI'm also writing comparisons—head-to-head breakdowns of competing tools so you can make an actual informed decision instead of guessing.\n\nAnd I'm keeping the site fast. No 10-second load times, no popups begging for your email before you've seen a single listing. Just the info you need.\n\nIf you're tired of useless directories, bookmark AgentRank. Or don't—I'm building this for me either way.\n\nBut if you're as fed up as I was with the state of AI tool discovery, maybe you'll find it useful.\n\nGot an AI agent you think deserves to be listed? Drop me a note at hugh.e.mcinnis@gmail.com. If it's legit and solves a real problem, I'll test it and add it.\n\nNo press releases. No affiliate pitches. Just good tools that actually work.",
    "readingTime": 4,
    "keywords": [
      "tool i'm",
      "it's",
      "listing",
      "you're",
      "directories",
      "actual",
      "tools",
      "productivity",
      "here's",
      "useless"
    ],
    "qualityScore": 0.8,
    "link": "https://www.agentrank.tech/blog/why-most-ai-agent-directories-suck",
    "thumbnail_url": "https://www.agentrank.tech/images/blog/why-directories-suck.png",
    "created_at": "2026-02-20T01:08:17.423Z",
    "topic": "tech"
  },
  {
    "slug": "locus-ai-agents-that-ship-your-code",
    "title": "Locus – AI agents that ship your code",
    "description": "🤖 Local-first workspace for agentic engineering. Unified tasks, docs, and secure CI for AI agents.  - GitHub - asgarovf/locusai: 🤖 Local-first workspace for agentic engineering. Unified tasks, docs...",
    "fullText": "asgarovf\n\n /\n\n locusai\n\n Public\n\n 🤖 Local-first workspace for agentic engineering. Unified tasks, docs, and secure CI for AI agents. \n\n locusai.dev\n\n License\n\n MIT license\n\n 9\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n asgarovf/locusai",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/asgarovf/locusai",
    "thumbnail_url": "https://opengraph.githubassets.com/b63dbd0e9506cb8fd686db3b4aef23a4e9b788451c5637c101ba01d08eb7c340/asgarovf/locusai",
    "created_at": "2026-02-20T01:08:15.512Z",
    "topic": "tech"
  },
  {
    "slug": "sam-altman-says-the-quiet-part-out-loud-confirming-some-companies-are-ai-washing-by-blaming-unrelated-layoffs-on-the",
    "title": "Sam Altman says the quiet part out loud, confirming some companies are ‘AI washing’ by blaming unrelated layoffs on the technology",
    "description": "Some economists are warning there’s no sign of AI-related job displacement appearing in the labor data. Altman claimed it’s just a matter of time until it does.",
    "fullText": "As debate continues over AI’s true impact on the labor force, OpenAI CEO Sam Altman said some companies are engaging in “AI washing” when it comes to layoffs, or falsely attributing workforce reductions to the technology’s impact.\n\n“I don’t know what the exact percentage is, but there’s some AI washing where people are blaming AI for layoffs that they would otherwise do, and then there’s some real displacement by AI of different kinds of jobs,” Altman told CNBC-TV18 at the India AI Impact Summit on Thursday.\n\nAI washing has gained traction as emerging data on the tech’s impact on the labor market tells a muddied, inconclusive story about how the technology is destroying human jobs—or if it has yet to touch them.\n\nA study published this month by the National Bureau of Economic Research, for example, found that of thousands of surveyed C-suite executives across the U.S., the U.K., Germany, and Australia, nearly 90% said AI had no impact on workplace employment over the past three years following the late-2022 release of ChatGPT.\n\nHowever, prominent tech leaders like Anthropic CEO Dario Amodei have warned of a white-collar bloodbath, with AI potentially wiping out 50% of entry-level office jobs. Klarna CEO Sebastian Siemiatkowski suggested this week the buy-now, pay-later firm would reduce its 3,000-person workforce by one-third by 2030 in part because of the acceleration of AI. Around 40% of employers expect to follow Siemiatkowski’s lead in culling staff down the line as a result of AI, according to the 2025 World Economic Forum Future of Jobs Report.\n\nAltman clarified he anticipates more job displacement as a result of AI, as well as the emergence of new roles complementing the technology.\n\n“We’ll find new kinds of jobs, as we do with every tech revolution,” he said. “But I would expect that the real impact of AI doing jobs in the next few years will begin to be palpable.”\n\nData from a recent Yale Budget Lab report suggests Altman and Amodei’s vision of mass worker displacement from AI is not certain and is not yet here. Using data from the Bureau of Labor Statistics’ Current Population Survey, the research found no significant differences in the rate of change of occupations mix or length of unemployment for individuals with jobs that have high exposure to AI from the release of ChatGPT through November 2025. The numbers suggested no significant AI-related labor changes at this juncture.\n\n“No matter which way you look at the data, at this exact moment, it just doesn’t seem like there’s major macroeconomic effects here,” Martha Gimbel, executive director and cofounder of the Yale Budget Lab, told Fortune earlier this month.",
    "readingTime": 3,
    "keywords": [
      "yale budget",
      "budget lab",
      "jobs",
      "labor",
      "washing",
      "there’s",
      "displacement",
      "layoffs",
      "workforce",
      "exact"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/sam-altman-says-quiet-part-165405075.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/eOgz0omhtKugcAPCd_1t9w--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD03OTg-/https://media.zenfs.com/en/fortune_175/403bad20c006bad7663789ee0c3542fb",
    "created_at": "2026-02-20T01:08:13.477Z",
    "topic": "finance"
  },
  {
    "slug": "basaltcrm-opensource-ainative-crm-nextjs-16-prisma-typescript",
    "title": "BasaltCRM – Open-Source AI-Native CRM (Next.js 16, Prisma, TypeScript)",
    "description": "🚀 What's...",
    "fullText": "BasaltHQ\n\n /\n\n crm-official\n\n Public\n\n v1.2.0 - The Intelligent Workspace Update\n\n Compare\n\n Choose a tag to compare\n\n Sorry, something went wrong.\n\n Filter\n\n Loading\n\n Sorry, something went wrong.\n\n Uh oh!\n\n There was an error while loading. Please reload this page.\n\n No results found\n\n 🚀 What's New\n\nCRM University & LearnLink: Integrated educational deep-linking directly into the CRM workflow.\nKnowledge Base (KB): A full-featured KB view within CRM cases, allowing teams to search and reference articles during support.\nQuick Launch Checklist: A new interactive onboarding experience for members with session-persistent and permanent dismissal.\nProduct Tour Engine: Integrated guided walkthroughs of key dashboard features and icons.\nAccount Import Engine: Robust account ingestion system with dedicated UI and API routes for bulk data loading.\n\n✨ Enhancements & UX\n\nTerminology Refactor: Updated \"Lead Pools\" to \"Lists\" throughout the application for better industry alignment.\nUnified UI States: Centralized loading states and skeletons across all main CRM routes for a smoother feel.\nDynamic Profiles: Improved profile tab navigation with dynamic headers and optimized color picker state management.\nAI Theme Enforcement: Hardened theme inheritance for Tremor and Recharts components to ensure a consistent look across light/dark modes.\n\n🛠 Fixes & Infrastructure\n\nBiome Migration: Fully replaced ESLint/Prettier with Biome for 10x faster linting and formatting.\nPrisma 6.19 Upgrade: Hardened core database infrastructure and refined seeding scripts.\nSecurity: Implemented PCI-compliant payment redirects via BasaltSURGE, removing raw card inputs from the UI.\nCampaigns API: Refactored campaign creation and POST handling to improve reliability and input validation.\n\nSpecial thanks to @pdovhomilja for the original foundational work.",
    "readingTime": 2,
    "keywords": [
      "compare",
      "routes",
      "across",
      "infrastructure",
      "biome",
      "loading",
      "sorry",
      "integrated",
      "engine",
      "account"
    ],
    "qualityScore": 0.95,
    "link": "https://github.com/BasaltHQ/crm-official/releases/tag/v1.2.0",
    "thumbnail_url": "https://opengraph.githubassets.com/d1733b24eaa8dae14652b3c2c09c6782617f76f1ac55d91c9f7d9dc6c1888d05/BasaltHQ/crm-official/releases/tag/v1.2.0",
    "created_at": "2026-02-19T18:39:28.772Z",
    "topic": "tech"
  },
  {
    "slug": "agentlint-realtime-guardrails-for-ai-coding-agents",
    "title": "AgentLint – Real-time guardrails for AI coding agents",
    "description": "Real-time quality guardrails for AI coding agents. ESLint for agent behavior. - mauhpr/agentlint",
    "fullText": "mauhpr\n\n /\n\n agentlint\n\n Public\n\n Real-time quality guardrails for AI coding agents. ESLint for agent behavior.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mauhpr/agentlint",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mauhpr/agentlint",
    "thumbnail_url": "https://opengraph.githubassets.com/1966baa05dced63addca9df21165a336fe43285d39df6eeff757f44e00407358/mauhpr/agentlint",
    "created_at": "2026-02-19T18:39:28.341Z",
    "topic": "tech"
  },
  {
    "slug": "digital-blackface-flourishes-under-trump-and-ai-the-state-is-bending-reality",
    "title": "Digital blackface flourishes under Trump and AI: ‘The state is bending reality’",
    "description": "From TikTok deepfakes to smears put out by the White House, fake videos modeled on Black archetypes are running rampant - putting Black users at risk\nLate last year, as a US government shutdown cut off the Snap benefits that low-income families rely on for groceries, videos on social media cast the fallout in frantic scenes. “Imma keep it real with you,” a Black woman said in a viral TikTok post, “I get over $2,500 a month in stamps. I sell ’em, $2,000 worth, for about $1,200-$1,500 cash.” Another Black woman ranted about taxpayers’ responsibility to her seven children with seven men, and yet another melted down after her food stamps were rejected at a corn-dog counter.\nVisible watermarks stamped some videos as AI-generated – apparently, too faintly for the racist commentators and hustlers more than happy to believe the frenzy was real.",
    "fullText": "Late last year, as a US government shutdown cut off the Snap benefits that low-income families rely on for groceries, videos on social media cast the fallout in frantic scenes. “Imma keep it real with you,” a Black woman said in a viral TikTok post, “I get over $2,500 a month in stamps. I sell ’em, $2,000 worth, for about $1,200-$1,500 cash.” Another Black woman ranted about taxpayers’ responsibility to her seven children with seven men, and yet another melted down after her food stamps were rejected at a corn-dog counter.\n\nVisible watermarks stamped some videos as AI-generated – apparently, too faintly for the racist commentators and hustlers more than happy to believe the frenzy was real. “You got people treating it like a side hustle, selling the stamps, abusing the system,” the conservative commentator Amir Odom whinged. Fox News reported on the Snap deepfakes as if they were authentic, before issuing a correction. Newsmax anchor Rob Schmitt claimed people were using Snap “to get their nails done, to get their weaves and hair”. (Lost in the outrage was a basic fact: white Americans make up 37% of Snap’s 42 million beneficiaries.)\n\nThe fake videos are mere shards in the widening mosaic of digital blackface, a pattern that’s spiked in the past two years as generative AI video tools have become widely accessible. “There’s been a massive acceleration,” says Safiya Umoja Noble, a UCLA gender studies professor and author of Algorithms of Oppression, which focuses on digital biases against Black women in particular. “The digital blackface videos are really pulling from the same racist and sexist stereotypes and tropes that have been used for centuries.” The net effect is a patina of Blackness stripped of cultural obligation or stewardship – minstrelsy in a nutshell.\n\nCoined in a 2006 academic paper, the term digital blackface describes a form of Black cultural commodification repurposed for non-Black expression online. Examples run the gamut: posts in African American Vernacular English, the use of darker-skinned emojis, reaction memes featuring Beyoncé, Katt Williams and other exemplars of Black cool.\n\n“The early research that was done on digital blackface started with white gamers using bitmojis of a different race and changing their vernacular to represent themselves,” says Mia Moody, a Baylor University journalism professor whose forthcoming book, Blackface Memes, links the role of Black users in starting and spreading online trends to subsequent digital blackface. “That’s part of the cultural appropriation, gaining the cultural capital. Maybe you’re a nerdy white guy, but if you use this cool avatar of a Black guy with dreadlocks, people will give you respect. You’re interesting all of a sudden.”\n\nDuring memeology’s expansion into short-form video, Black expression has increasingly been divorced from authorship, context or consequence. Internet culture scholars say some non-white online creators use AI-generated avatars modeled on familiar Black faces – the beauty influencer, the culture podcaster, the man-on-the-street interviewer; they slip into feeds alongside real Black content creators. Large language models scour digital spaces that gained cachet from Black speech and humor, absorbing their tone and slang. Hume AI is one of many firms that offer synthetic voices for podcasts and audiobooks such as “Black woman with subtle Louisiana accent”, or “middle-aged African American man with a tone of hard-earned wisdom”. In most cases, creators whose speech is scraped from YouTube, podcasts and social media receive no compensation, much less even know their personalities shaped these models.\n\nThe Snap reaction clips, however, were a notable escalation in the mainstreaming of digital blackface – less blending in, more weapons-grade stereotyping. Many of those videos were created with OpenAI’s text-to-video app Sora. As Sora’s popularity surged in 2025, users exploited its hyperrealism to sully Martin Luther King Jr’s image, sparking ethical debate around “synthetic resurrection”. Deepfakes showed him shoplifting, wrestling Malcolm X, and swearing through his I Have a Dream speech. Conservative influencers swamped feeds with AI-generated embraces between King and Charlie Kirk, conflating their clashing legacies and cultural martyrdom. Bernice King, MLK’s daughter and the director of his Atlanta-based non-profit, criticized the slopaganda as “foolishness”.\n\nInevitably the Trump White House has gotten into the act. In January the official White House X account posted a doctored photo of Minnesota activist Nekima Levy Armstrong, darkened and weeping, in the wake of her arrest at a non-violent anti-ICE demonstration. Earlier this month an image portraying the Obamas as apes was circulated via Trump’s own Truth Social account.\n\nBlackface abides at the underbelly of American mass media even as it evolves at breakneck pace. Its roots trace to the minstrel revues of the early 19th century; white performers smeared grease paint made from charred corks on their faces and plastered on oversized white lips to caricature Black features, and performed exaggerated routines of Black laziness, buffoonery and hypersexuality. Thomas D Rice, a Manhattan playwright, shot to fame in the 1830s playing a loping trickster named Jim Crow – a name that quickly became shorthand for the forced racial segregation policies in the American south that endured until the 1964 Civil Rights Act.\n\nIn their heyday, minstrel shows were the dominant form of American entertainment – reflected in newspaper cartoons and the enormously popular Amos ‘n’ Andy radio shows. After the civil war, Black performers were largely forced into adopting minstrel elements, at the expense of their personhood once more, just to gain any footing on stage. “The objectives were, first, to make money to help educate our younger ones, and second, to try to break down the ill feeling that existed toward the colored people,” explained Tom Fletcher, a vaudeville minstrel and actor for almost 70 years who died in 1954.\n\nEven as minstrelsy faded from the spotlight by the early 20th century, its toxic residue lingered in American culture – from the shuffling crows of Disney’s Dumbo, to Ted Danson’s infamous 1993 blackface roast of Whoopi Goldberg, to the annual parade of white Halloween revelers in racial masquerade. A decade ago, when the internet was still a black box of sorts, researchers such as Noble and MIT’s Joy Buolamwini were sounding the alarm about the inherent racial biases in the coding of algorithms related to medical treatment, loan applications, hiring decisions and facial recognition. Now it’s out in the open, smearing wider and deeper than any burnt cork routine ever could.\n\nTech firms have made some effort to stem the tide of digital blackface. Bowing to public backlash, the King family and more prominent estates, OpenAI, Google and the AI image generator Midjourney disallowed deepfakes of King and other American icons. In January 2025, Meta deleted two of its own AI blackface characters – a retiree called Grandpa Brian and Liv, described as a “proud Black queer momma” and “truth-teller” – after allegations of their non-diverse development team fueled a tempest of criticism. Instagram and TikTok and more have made some attempts to scrub viral digital blackface videos, to tepid results. Last summer, efforts to replicate Bigfoot Baddie – the AI avatar of a Black woman as a human-yeti hybrid in pink wigs, acrylic nails and hair bonnets, created by Google’s Veo AI – erupted into a full-blown social media craze, with some users even hustling how-to courses. The avatar is still on socials.\n\nBlack in AI and the Distributed AI Research Institute (Dair) are among the handful of affinity groups that have pushed for diversity and community input in AI model-building to address programming bias. The AI Now Institute and Partnership on AI have highlighted the risks of AI systems learning from marginalized communities’ data and noted that tech companies could provide mechanisms such as data opt-outs to help limit harmful or exploitative uses. But widespread adoption has been glacial.\n\n“YouTube alone has something like 400 hours of content per minute being uploaded,” Noble says. “With AI generation, these tech firms cannot manage what’s coming through their systems. So they don’t. Or they do what’s absolutely imperative to the US government. But if you have an authoritarian regime in power, they can use your systems to facilitate propaganda.”\n\nAlthough the precise impact of AI-generated digital blackface is difficult to quantify, its use by the Trump administration highlights its potential as a powerful tool of official disinformation. The Obama Truth Social entry revived a slur that has festered for years in darker online corners, and one that rhymes with Trump’s sustained efforts to denigrate the former first family. (Trump disclaimed direct responsibility and refused to apologize for that post, which was taken down.) Meanwhile, the White House’s doctored image of Armstrong, altered from an actual photo taken by the Department of Homeland Security and published on their official Twitter account, scanned as a psyop by a government working closely with tech firms to track activists and other perceived enemies of the state.\n\nBeyond laundering bigotry as news, digital blackface exposes Black users to a level of personalized abuse and harassment that harkens to a minstrelsy heyday when racists were fully empowered to express their bigotry unbidden. And then just as now, it seems there is little that can be done to curb the vitriol. “We are living in a United States with an open, no-holds-barred, anti-civil-rights, anti-immigrant, anti-Black, anti-LGBTQ, anti-poor-policy agenda,” Noble says. “Finding the material to support this position is just a matter of the state bending reality to fit its imperatives. And that’s easily done when every tech company lines up behind the White House.”\n\nEven so, Moody remains hopeful that the current fascination with digital blackface will soon be as outdated and uninviting as the analog variant. She has seen this play before, after all. “Right now people are just experimenting with AI technology and having a ball seeing what they can get away with,” she says. “Once we get beyond that, then we’re going to see less of it. They’ll move on to something else. Or they’ll be up for a job, and it’ll be embarrassing. Just look at the history.”",
    "readingTime": 9,
    "keywords": [
      "black woman",
      "black users",
      "social media",
      "tech firms",
      "digital blackface",
      "blackface videos",
      "cultural",
      "ai-generated",
      "done",
      "online"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/ng-interactive/2026/feb/19/ai-digital-blackface",
    "thumbnail_url": "https://i.guim.co.uk/img/media/8ee6098a21dcbd6be64a80b0f2a64ccca202fbc0/0_180_800_640/master/800.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d2fe071c5d198f9ea386e1c1340f1d9f",
    "created_at": "2026-02-19T18:39:05.278Z",
    "topic": "tech"
  },
  {
    "slug": "how-the-anxiety-over-ai-could-fuel-a-new-workers-movement",
    "title": "How the anxiety over AI could fuel a new workers’ movement",
    "description": "New technology has workers spooked, but experts say it’s creating an opening for a resurgence in worker power\nIn 2026, it’s a scary time to work for a living.\nGone are the days of quiet quitting, the Great Resignation, and the highly visible union-organizing battles that began the decade and signaled that perhaps worker power was on the rise again in the US. Instead, much of that momentum is being crowded out of our minds by anxieties: a worsening affordability crisis, geopolitical instability, and the specter of artificial intelligence looming over the workplace.\n Continue reading...",
    "fullText": "In 2026, it’s a scary time to work for a living.\n\nGone are the days of quiet quitting, the Great Resignation, and the highly visible union-organizing battles that began the decade and signaled that perhaps worker power was on the rise again in the US. Instead, much of that momentum is being crowded out of our minds by anxieties: a worsening affordability crisis, geopolitical instability, and the specter of artificial intelligence looming over the workplace.\n\nFor the tech CEOs leading the AI race and enriching themselves as they jostle for dominance, AI isn’t a phantasm at all, but a glimmering unicorn. When they predict AI is just months away from being able to do everything a software engineer does, or that it will one day take over CEOs’ jobs, their excitement for the future is palpable. For the rest of us, it’s hard to feel confident in their offhand remarks about how “some jobs will be obsolete, but many jobs will be created”. A 2025 Pew survey found that “64% of the public thinks AI will lead to fewer jobs over the next 20 years”, which is probably why only 17% of Americans say AI will have a positive effect on the US over the same time period.\n\nUncertain times like these call for scrutiny. Throughout 2026, the Guardian will publish Reworked, a reporting series that centers the human stakes as AI disrupts our workplaces, in ways both thrilling and alarming. Like this essay, the stories in this series will focus on workers’ real-world power and plights as well as the realities and exaggerations of the hype surrounding AI’s transformative possibilities.\n\nSo which version of the future of work awaits us? It is yet to be settled, which means there is still time to shift course.\n\nBlue-collar workers who have long grappled with algorithmic surveillance and optimization at work are now worrying that technological advancements will only make their jobs more dehumanizing. “[For] lower wage workers, there is concern about being replaced by robots. But on the other hand, there’s a lot of concern about being turned into robots,” Lisa Kresge, a senior researcher at the UC Berkeley Labor Center, told me.\n\nAnd white-collar workers are now wondering if their work will begin to resemble blue-collar labor – either because they will be similarly tracked and managed, or because they will need to switch to more manual work that’s resistant to being taken over by AI.\n\nIt may seem that workers haven’t been this vulnerable in a long time. In some ways, that’s true. But this is also a pivotal moment, one in which something unexpected is happening: society’s collective anxiety over AI is catalyzing workers to push back.\n\n“It is creating an opportunity,” Sarita Gupta, the Ford Foundation’s Vice President of US Programs and co-author of The Future We Need: Organizing for a Better Democracy in the Twenty-First Century, told me. “When you have a young Silicon Valley software engineer realize that their performance is tracked or undermined by the same logic as a working class warehouse picker, class divisions dissolve, and larger working-class movements for dignity are possible. That is what we’re starting to see.”\n\nPeople across industries and income brackets are anxious and frustrated, quite like they were when the Covid pandemic placed punishing demands on frontline workers and erased the boundaries between work and life for everyone else. Those struggles prompted power shifts: At the same time that workers led unionization efforts at Amazon warehouses and Starbucks locations around the US, the Great Resignation saw a record number of employees quit their jobs, and the ones who remained in the workforce began negotiating for and gaining better pay and conditions.\n\n“It was not a pretty time for a lot of workers. And so part of the resurgence of labor organizing from that period of time was in response to a lot of fears,” Kresge said.\n\nShe also sees the rise of AI as an opening for the labor movement to regain some of the power it’s lost after decades of attacks from employers. “I’m hopeful about the opportunity for technology to lift up some of the issues that have been under way in our economy for decades … in terms of how workers are treated and how we are distributing the rewards of productivity.”\n\nConditions for workers have been rough for a long while now. “Over time, unions have lost collective bargaining power, and a lot of that is due to the lack of laws that we need and enforcement of laws,” Gupta said. “For four decades, productivity soared while wages stayed flat, and unionization hit historic lows.” In 2025, only 9.9% of US workers were union members – the same percentage as 2024, but still the lowest numbers in almost 40 years.\n\nToday, the advent of AI is drawing the world’s attention to the extreme imbalance of power between employers and their employees – and people are getting worked up. Even if the outcomes are still undetermined, that’s a glimmer of possibility in bleak times.\n\nAI is still a nascent technology. Many of the predictions about what it will be capable of and how it will transform labor and the economy are just that – predictions. The question of worker power in the age of AI hasn’t been decided yet, even if billionaire CEOs with a vested interest in the unregulated dominance of AI keep implying that it has.\n\n“There is a concerted effort among many tech leaders to basically create mystification around AI as a tactic, to a large extent, to disempower workers, policymakers, and anyone who might be critical of the growing concentration of funding and resources in our society toward this goal,” Kresge told me.\n\nIn other words, take what these billionaires say with a grain of salt. The rise of AI is already transforming society, the economy, and our relationship to work, but a lot of these shifts are anticipatory, based on our belief in the potential of a technology that’s still being built.\n\n“We have to always remind ourselves that the direction of technology is a choice, right? We can use AI to build a surveillance economy that squeezes every drop of value out of a worker, or we can use it to build an era of shared prosperity,” Gupta said. “We know if technology were designed and deployed and governed by the people doing the work, AI wouldn’t be such a threat.”",
    "readingTime": 6,
    "keywords": [
      "software engineer",
      "workers",
      "jobs",
      "technology",
      "that’s",
      "economy",
      "it’s",
      "worker",
      "rise",
      "ceos"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/ng-interactive/2026/feb/19/ai-work-future",
    "thumbnail_url": "https://i.guim.co.uk/img/media/e4182b4a4822120bf8fbe8a4541c240fddc9c670/0_900_2204_1762/master/2204.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=c360a1093aefb03d0ea06d39596c24c9",
    "created_at": "2026-02-19T18:39:05.268Z",
    "topic": "tech"
  },
  {
    "slug": "bill-gates-drops-out-of-ai-summit-as-he-faces-heat-over-epstein-files",
    "title": "Bill Gates drops out of AI summit as he faces heat over Epstein files",
    "description": "A statement from the Gates Foundation said Bill Gates would not appear to \"ensure the focus remains\" on the India AI Impact Summit's key priorities.",
    "fullText": "Bill Gates pulled out of the India AI Impact Summit on Thursday, just hours before he was due to appear to give a keynote speech.\n\nThe Microsoft cofounder's decision to cancel his appearance at the high-profile event follows mounting scrutiny over his ties to Jeffrey Epstein.\n\n\"After careful consideration and to ensure the focus remained on the AI Summit's key priorities, Mr. Gates did not deliver the keynote address at the AI Summit,\" the Gates Foundation said in a statement provided to Business Insider.\n\nThe spokesperson added that Ankur Vora, the foundation's chief strategy officer and president of its Africa and India offices, had spoken instead.\n\n\"The Gates Foundation remains fully committed to our work in India to advance our shared health and development goals,\" they added.\n\nGates's interactions with Epstein have faced scrutiny after the Justice Department released 3 million emails related to the late sex offender. One of the emails, with the subject line \"bill,\" suggested that Gates requested medication for a sexually transmitted disease to give to his now ex-wife, Melinda French Gates.\n\n\"These claims are absolutely absurd and completely false,\" a spokesperson for Gates told Business Insider in a statement earlier this month. \"The only thing these documents demonstrate is Epstein's frustration that he did not have an ongoing relationship with Gates and the lengths he would go to entrap and defame.\"",
    "readingTime": 2,
    "keywords": [
      "gates foundation",
      "keynote",
      "scrutiny",
      "statement",
      "emails",
      "india",
      "bill",
      "summit",
      "epstein",
      "business"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/bill-gates-drops-out-india-ai-summit-epstein-controversy-2026-2",
    "thumbnail_url": "https://i.insider.com/69970860f8731049f3af6100?width=1200&format=jpeg",
    "created_at": "2026-02-19T18:39:03.055Z",
    "topic": "finance"
  }
]
[
  {
    "slug": "cooperbench-benchmarking-ai-agents-cooperation",
    "title": "CooperBench: Benchmarking AI Agents' Cooperation",
    "description": "CooperBench is a benchmark of over 600 collaborative coding tasks. We find that agents achieve 30% lower success rates when working together compared to performing both tasks individually.",
    "fullText": "Stanford University & SAP Labs US\n\nCan AI agents work together as teammates? We find\n that\n coordinating agents perform much worse than a\n single agent\n given the same total workload. This coordination\n deficit presents a fundamental barrier to deploying\n AI systems that can work alongside humans or other\n agents.\n\nSuccess rate on CooperBench across 652 tasks · Error bars show 95% confidence intervals\n\nGPT-5 and Claude Sonnet 4.5 achieve only 25%\n success with two-agent cooperation, roughly 50%\n lower than when a single agent handles both\n tasks. This gap persists across all models and\n task difficulties.\n\nAgents spend up to 20% of their budget on\n communication. This reduces merge conflicts but\n does not improve overall success. The channel is\n jammed with repetition, unresponsiveness, and\n hallucination.\n\nEven when agents communicate well, coordination\n breaks down due to:\n\nAmong successful runs, we observe coordination patterns\n largely absent from failures. These patterns are not\n prompted or scaffolded.\n\nRole Division\n — Agents agree on who handles which part of the\n task. One agent delegates: \"I'll add header +\n octal_str; you add binary_str between them.\"\n\nCooperBench is the first benchmark designed to\n measure how well AI agents can cooperate when\n handling individual tasks with potential conflicts.\n We constructed 652 tasks from 12 popular open-source\n libraries across Python, TypeScript, Go, and Rust.\n\nEach task assigns two agents different features that\n can be implemented independently but may conflict\n without proper coordination. Eight co-authors with\n real-world software engineering backgrounds created\n new features, unit tests, and ground-truth code.\n\nStanford University & SAP Labs · *Equal contribution\n (Stanford) · †Equal contribution (SAP Labs)",
    "readingTime": 2,
    "keywords": [
      "equal contribution",
      "stanford university",
      "university sap",
      "coordination",
      "tasks",
      "success",
      "across",
      "task",
      "agents",
      "cooperbench"
    ],
    "qualityScore": 0.85,
    "link": "https://cooperbench.com/",
    "thumbnail_url": "https://cooperbench.com/static/images/cooperbench_social.png",
    "created_at": "2026-01-30T18:28:27.534Z",
    "topic": "tech"
  },
  {
    "slug": "daedalus",
    "title": "Daedalus",
    "description": "AI planning CLI and autonomous agent orchestration for beans-based coding workflows - internet-development/daedalus",
    "fullText": "internet-development\n\n /\n\n daedalus\n\n Public\n\n AI planning CLI and autonomous agent orchestration for beans-based coding workflows\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n internet-development/daedalus",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/internet-development/daedalus",
    "thumbnail_url": "https://opengraph.githubassets.com/471147f12cb10edd91f2072a6602ce03fd3e4339e4f03a4a184ee7336b8d31ea/internet-development/daedalus",
    "created_at": "2026-01-30T06:35:17.254Z",
    "topic": "tech"
  },
  {
    "slug": "cwt-sandbox-ai-coding-agents-using-git-worktrees",
    "title": "Cwt – Sandbox AI coding agents using Git Worktrees",
    "description": "Contribute to benngarcia/claude-worktree development by creating an account on GitHub.",
    "fullText": "benngarcia\n\n /\n\n claude-worktree\n\n Public\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n benngarcia/claude-worktree",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/benngarcia/claude-worktree",
    "thumbnail_url": "https://opengraph.githubassets.com/73752eaadbf154afd745270d32c1743038163f0555993f4c462cecd915f63c02/benngarcia/claude-worktree",
    "created_at": "2026-01-30T01:07:08.980Z",
    "topic": "tech"
  },
  {
    "slug": "acp-agent-registry-in-jetbrains-ides",
    "title": "ACP Agent Registry in JetBrains IDEs",
    "description": "Together with Zed, we've launched the official ACP Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs.",
    "fullText": "Supercharge your tools with AI-powered features inside many JetBrains products\n\nAI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day. Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what’s out there, let alone getting it running in your IDE, hasn’t been easy.\n\nTogether with Zed (Zed’s announcement), we’ve launched the official ACP Agent Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs and Zed. Browse what’s available, click Install, and start working right away. This beta release is just the beginning.\n\nThe Agent Client Protocol is an open standard that lets any AI coding agent work in any supporting editor. Think of it like the Language Server Protocol, but for AI agents. The LSP lets any editor support any language through a shared standard. The ACP does the same for coding agents. You only need to implement it once, and then it will work in your JetBrains IDE, Zed, or any other editor that supports the protocol.\n\nThis means you get to pick your preferred agent and editor, and they will then work together seamlessly – no vendor lock-in and no waiting for someone to build a specific integration.\n\nACP has been, since we started integrating it to Mistral Vibe, a real joy to use: thoughtfully designed from the ground up, community-driven, and evolving rapidly. We’ve found it not only simplifies integration, but also fits our focus on open and flexible tools. It’s really great to see a standard that puts developer choice first.\n\nMichel Thomazo, Software Engineer @ Mistral AI\n\nThe ACP made agent interoperability technically possible. The registry makes it convenient.\n\nInstead of manually configuring agents, you can now:\n\nAt launch, you’ll find a wide array of different agents:\n\nFull-featured coding assistant optimized for large-scale refactors\n\nSpecialized agent for automated code generation workflows\n\nGoogle’s agent with deep codebase understanding and multimodal capabilities\n\nGitHub’s AI pair programmer, now available via the ACP\n\nLightweight, fast agent built on Mistral’s models\n\nCommunity-driven, fully open-source agent\n\nAlibaba’s coding agent with strong multilingual support\n\nInnovation in software agents is moving at an unbelievable pace. The Agent Registry and ACP makes it simple for developers to use the best agents in their favorite tools.\n\nChris Kelly, Product @ Augment Code\n\nIn general, it’s less about having multiple agents than about enabling you to pick and choose the ones that work well in your workflow. Different agents come with different benefits. Some provide a more attractive pricing structure for your business, some provide a user experience that you simply enjoy more than others’, and some embody the ideas of open-source development that just resonate with you.\n\nThe Agent Client Protocol registry lets you experiment freely. Try a few, see what clicks for your workflow, and then keep the ones that help. You’re not locked into a single vendor’s vision of what AI-assisted development should look like.\n\nWe’re excited to support the ACP Agent Registry as a step toward a more open agent ecosystem where Droids can integrate seamlessly across all IDEs.\n\nFrancesca LaBianca, VP of Operations @ Factory\n\nIn any JetBrains IDE (2025.3.2+) with JetBrains AI (253.30387.147):\n\nThat’s it. The agent is configured and ready to use in the AI Chat tool window.\n\nQuick note: agents typically come with their own subscription. That’s between you and them. You won’t need a JetBrains AI subscription to use ACP agents.\n\nWant to try something concrete? Install OpenCode, open a project, and ask it to explain an unfamiliar module. OpenCode also lets you swap between different LLMs, so you can experiment with what works best for you.\n\nIf you prefer manual configuration, that option is still there, too. Just edit the acp.json directly. This is useful for agents that aren’t in the registry yet or for custom setups.\n\nIf you’re building an ACP-compatible agent, the registry is now the fastest way to reach developers across JetBrains IDEs and Zed.\n\nHead to the ACP Registry repository and check out the CONTRIBUTING.md for the full submission process and metadata requirements. Please note that, for now, we are only featuring agents that support Agent Auth or Terminal Auth. Full details of requirements and conditions can be found here.\n\nThis is an open registry. If you’re building an ACP-compatible agent, you’re welcome to submit it. The registry exists to serve the ecosystem, not to gatekeep it.\n\nFor developers: More choice and zero lock-in. Use any agent you want in the IDE you love.\n\nFor agent builders: Instant distribution to millions of JetBrains and Zed users. Implement the ACP once and reach everyone.\n\nFor the ecosystem: Competition on quality, not on who controls the integration. The best agents win because they’re the best, not because they have exclusive deals.\n\nWe’re building this openly with Zed because we believe AI-assisted development shouldn’t be locked inside any single vendor’s ecosystem. Developers deserve to pick their tools freely.\n\nThe registry is one more step toward that future.\n\nThe ACP Registry is available now in JetBrains IDE versions 2025.3 and later. Update your IDE and the JetBrains AI plugin, open Settings, and start exploring.\n\nHave feedback? Found a bug? The registry repo is open for issues and PRs. And if you’re building something interesting with ACP, we’d love to hear about it!\n\nOpenAI Codex is now natively integrated into the JetBrains AI chat, giving you another powerful option for tackling real development tasks right inside your IDE. \n\nYou can use Codex with a JetBrains AI subscription, your ChatGPT account, or an OpenAI API key – all within the same AI сhat inte…\n\nThe next edit suggestions feature is now enabled in all JetBrains IDEs for JetBrains AI Pro, AI Ultimate, and AI Enterprise subscribers.\n\nYes, you read that right! JetBrains-native diff suggestions are available right in your editor. Global support for optimized latency. Out-of-the-box IDE actions…\n\nBring Your Own Key (BYOK) is now available in the AI chat inside JetBrains IDEs as well as for AI agents, including JetBrains’ Junie and Claude Agent. Whether you’re looking to use cutting-edge frontier models, cost-efficient small models, locally hosted private models, or experimental research prev…\n\nJunie is now integrated into the AI chat. The separate interfaces have merged into a single, unified space (available in Beta).",
    "readingTime": 6,
    "keywords": [
      "client protocol",
      "ai-assisted development",
      "step toward",
      "agent client",
      "acp-compatible agent",
      "jetbrains ai",
      "coding agents",
      "acp agent",
      "acp agent registry",
      "jetbrains ide"
    ],
    "qualityScore": 1,
    "link": "https://blog.jetbrains.com/ai/2026/01/acp-agent-registry/",
    "thumbnail_url": "https://blog.jetbrains.com/wp-content/uploads/2026/01/JB-social-BlogSocialShare-1280x720-1-4.png",
    "created_at": "2026-01-29T18:30:47.768Z",
    "topic": "tech"
  },
  {
    "slug": "extesla-ai-head-has-seen-a-phase-shift-in-software-engineering-using-claude-code-and-his-manual-skills-slowly-atrophy",
    "title": "Ex-Tesla AI head has seen a 'phase shift in software engineering' using Claude Code — and his manual skills slowly 'atrophy'",
    "description": "Andrej Karpathy posted his \"notes from Claude Coding,\" describing a shift in engineering over the last two months.",
    "fullText": "He coined \"vibe coding.\" Now, he sees a \"phase shift\" in software engineering.\n\nAndrej Karpathy is one of AI's guiding figures. He was a founding member of OpenAI and later served as Tesla's director of AI. He also coined the term \"vibe coding,\" the AI-assisted coding movement that has taken software engineering by storm and was named Collins Dictionary's word of the year.\n\nIn his \"random notes from Claude Coding\" — which are over 1,000 words long — Karpathy wrote about the changes to his own coding style. Posted on X on Monday, the notes have already elicited reactions from engineers at Anthropic, xAI, and more.\n\nAI coding agents \"crossed some kind of threshold of coherence around December 2025 and caused a phase shift in software engineering,\" Karpathy wrote.\n\nA few random notes from claude coding quite a bit last few weeks.\n\nCoding workflow. Given the latest lift in LLM coding capability, like many others I rapidly went from about 80% manual+autocomplete coding and 20% agents in November to 80% agent coding and 20% edits+touchups in…\n\nKarpathy name-dropped both Anthropic's Claude Code and OpenAI's Codex as having significant improvements. Claude Opus 4.5, the model that has garnered much love from engineers online, came out at the tail end of November.\n\nThe AI leader's workflow has changed as a result of the AI tools. From November to December, Karpathy's 80/20 ratio flipped. He once used 80% manual coding and 20% agents; now, it's 80% agents and 20% manual code editing.\n\n\"I really am mostly programming in English now, a bit sheepishly telling the LLM what code to write... in words,\" he wrote.\n\nThe change to AI-written code \"hurts the ego,\" but is too powerful to ignore, Karpathy wrote. He also devoted a whole section of his notes to the \"fun\" he has while coding with large language models.\n\nWhat of those traditional coding skills, the ones you learn in a computer science program or through endless digital courses? That's a whole other function, Karpathy wrote, and one that might decline.\n\n\"I've already noticed that I am slowly starting to atrophy my ability to write code manually,\" he wrote.\n\nIn Karpathy's comments, engineers from leading AI companies sounded off. Ethan He, an xAI engineer and Nvidia alum, wrote that a \"10x engineer can be a one-man army.\"\n\nCharles Weill, another xAI engineer, wrote that founders can now \"divide themselves\" with coding agents, like a VC divides their capital over a portfolio of companies.\n\nBoris Cherny, an Anthropic staffer and the creator of Claude Code, wrote that he read Karpathy's \"thoughtful\" post till its end.\n\nThe Claude Code team at Anthropic may offer a model of where the industry is moving, Cherny wrote. His team is \"mostly generalists\" and filled with 10x engineers.\n\n\"Pretty much 100% of our code is written by Claude Code,\" Cherny wrote. \"For me personally it has been 100% for two+ months now, I don't even make small edits by hand.\"\n\nThe Anthropic employee also acknowledged the \"quality\" problems with AI-written code. Agents can overcomplicate things and can leave around dead code, he wrote.\n\nHis solution: having AI review the AI-written code.",
    "readingTime": 3,
    "keywords": [
      "ai-written code",
      "phase shift",
      "software engineering",
      "random notes",
      "xai engineer",
      "vibe coding",
      "coding agents",
      "engineers",
      "claude",
      "coined"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-claude-code-manual-skills-atrophy-software-engineering-tesla-2026-1",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-01-29T12:32:28.928Z",
    "topic": "tech"
  },
  {
    "slug": "openais-chair-says-vibe-coding-is-here-to-stay-but-its-not-the-endgame",
    "title": "OpenAI's chair says vibe coding is here to stay — but it's not the endgame",
    "description": "Vibe coding will stick around, but AI agents, not apps, will drive the next big shift in software, says OpenAI's chair Bret Taylor.",
    "fullText": "Vibe coding isn't going anywhere. But it's only part of a much bigger transformation, says OpenAI's board chair.\n\nBret Taylor said in an episode of the \"Big Technology Podcast\" published on Wednesday that using AI tools to build software quickly with natural language prompts will soon feel normal rather than novel. However, focusing on building today's software faster misses the bigger picture.\n\n\"Everyone's looking at all the software use and saying, 'How fast could I vibe code that?'\" Taylor said. \"I wonder if it's the wrong question.\"\n\nWhether someone can quickly vibe code an app in a web browser isn't \"the most interesting question in software,\" he added.\n\nInstead, the software we use today is set to be replaced, and that's the real disruption, Taylor said.\n\nRather than dashboards, web-browser forms, and traditional apps, the structure of software will change. AI agents will be \"the future of software.\"\n\n\"We will delegate tasks to agents that will operate against a database,\" Taylor said.\n\n\"Who's making those agents is the question,\" he added. \"Will you buy those agents off the shelf or build them yourself?\"\n\nTaylor also said that while AI has slashed the cost of building software, it hasn't solved the harder problems of maintaining it — or the risk of getting things wrong.\n\n\"That's why most people would prefer to buy a solution off the shelf,\" he said. \"You want to amortize the cost of maintaining software among thousands of clients.\"\n\nVibe coding has taken off across the tech world, but tech leaders said the technology has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that vibe coding is \"making coding so much more enjoyable,\" adding that it allows even non-technical users to create simple apps and websites.\n\nDuring Alphabet's April earnings call, Pichai said AI generates more than 30% of Google's new code, up from 25% in October 2024.\n\nStill, AI-generated code can be error-prone, overly long, or poorly structured.\n\n\"I'm not working on large codebases where you really have to get it right, the security has to be there,\" Pichai said in November.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding works best for prototypes or throwaway code, but not in software that sits at the core of a business.\n\n\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "vibe code",
      "software",
      "agents",
      "isn't",
      "it's",
      "bigger",
      "episode",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chair-vibe-coding-not-endgame-bret-taylor-2026-1",
    "thumbnail_url": "https://i.insider.com/6883b14a85e81483682eb19e?width=1200&format=jpeg",
    "created_at": "2026-01-29T06:34:22.533Z",
    "topic": "finance"
  },
  {
    "slug": "microsoft-cfos-memo-to-staff-calls-out-ai-deals-coding-and-chips",
    "title": "Microsoft CFO's memo to staff calls out AI deals, coding, and chips",
    "description": "CFO Amy Hood sent an internal memo about the results, viewed by Business Insider.",
    "fullText": "After Microsoft reported results on Wednesday, CFO Amy Hood sent an internal memo to employees calling attention to recent developments in AI chips and coding tools, and deals with OpenAI and Anthropic.\n\nHood sends these emails every quarter when Microsoft discloses its financials. Her missives mostly rehash what the company reports publicly, such as how revenue and profit are growing, or what is discussed on analyst earnings calls.\n\nStill, these memos provide insight into what Microsoft executives deem most important, and what they want employees to know.\n\nThe latest memo highlighted how Microsoft is gaining share in markets where the total addressable market is expanding.\n\nHood specifically mentioned the launch of the GitHub Copilot software development kit in the growing market of AI coding tools, and the release of Microsoft's new Maia 200 AI chip.\n\nHood's email also called out Azure commitments from OpenAI and Anthropic that helped increase commercial bookings, basically the deals Microsoft closed in the quarter, by 230%, year over year.\n\nCapital expenditure on computing and datacenter infrastructure also broke yet another quarterly record, reaching $37.5 billion, she also noted.\n\nThis afternoon, we announced our second-quarter financial results. We exceeded Wall Street expectations, growing revenue 17% and 15% in constant currency and operating income by 21% and 19% in constant currency -a strong finish to the first half of the fiscal year.\n\nIn the quarter, Microsoft Cloud revenue surpassed $50 billion for the first time, growing 26% and 24% in constant currency.\n\nThere were many highlights this quarter, but a few stand out as reminders of the value our products and services deliver to customers - and as proof points of the progress we are making:\n\nInvestors tune in to our earnings call for the full details on this quarter and a look ahead to Q3. It's a helpful way to stay aligned as we deliver on our commitments. Join live today at 2:30 PM Pacific, listen on-demand, or check the transcript on the Investor Relations website.\n\nThis quarter's results reflect meaningful progress on core priorities. We continue to add capacity with pace, drive steady efficiency gains across our fleet, and invest in each layer of the stack\n\nAs we enter the second half of the fiscal year, we're operating in markets with expanding TAM where we continue to gain share and you can see our progress in many places, from last week's announcement of the GitHub Copilot SDK to Monday's Maia 200 announcement. We are innovating and delivering together. And we're doing it with the quality and security our customers expect from us. All of this builds trust from customers and partners as they rely on us for mission critical workloads.\n\nThanks again for all your work.\n\nWith appreciation and gratitude,\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "coding tools",
      "constant currency",
      "quarter",
      "revenue",
      "email",
      "customers",
      "progress",
      "memo",
      "employees",
      "deals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/internal-microsoft-cfo-memo-calls-out-ai-deals-coding-and-chips-2026-1",
    "thumbnail_url": "https://i.insider.com/697a82dfa645d11881883093?width=1200&format=jpeg",
    "created_at": "2026-01-29T01:07:05.127Z",
    "topic": "finance"
  },
  {
    "slug": "programming-sucks-2014",
    "title": "Programming Sucks (2014)",
    "description": null,
    "fullText": "Every friend I have with a job that involves picking up something heavier than a laptop more than twice a week eventually finds a way to slip something like this into conversation: “Bro,1[1] you don’t work hard. I just worked a 4700-hour week digging a tunnel under Mordor with a screwdriver.”\n\nThey have a point. Mordor sucks, and it’s certainly more physically taxing to dig a tunnel than poke at a keyboard unless you’re an ant. But, for the sake of the argument, can we agree that stress and insanity are bad things? Awesome. Welcome to programming.\n\nImagine joining an engineering team. You’re excited and full of ideas, probably just out of school and a world of clean, beautiful designs, awe-inspiring in their aesthetic unity of purpose, economy, and strength. You start by meeting Mary, project leader for a bridge in a major metropolitan area. Mary introduces you to Fred, after you get through the fifteen security checks installed by Dave because Dave had his sweater stolen off his desk once and Never Again. Fred only works with wood, so you ask why he’s involved because this bridge is supposed to allow rush-hour traffic full of cars full of mortal humans to cross a 200-foot drop over rapids. Don’t worry, says Mary, Fred’s going to handle the walkways. What walkways? Well Fred made a good case for walkways and they’re going to add to the bridge’s appeal. Of course, they’ll have to be built without railings, because there’s a strict no railings rule enforced by Phil, who’s not an engineer. Nobody’s sure what Phil does, but it’s definitely full of synergy and has to do with upper management, whom none of the engineers want to deal with so they just let Phil do what he wants. Sara, meanwhile, has found several hemorrhaging-edge paving techniques, and worked them all into the bridge design, so you’ll have to build around each one as the bridge progresses, since each one means different underlying support and safety concerns. Tom and Harry have been working together for years, but have an ongoing feud over whether to use metric or imperial measurements, and it’s become a case of “whoever got to that part of the design first.” This has been such a headache for the people actually screwing things together, they’ve given up and just forced, hammered, or welded their way through the day with whatever parts were handy. Also, the bridge was designed as a suspension bridge, but nobody actually knew how to build a suspension bridge, so they got halfway through it and then just added extra support columns to keep the thing standing, but they left the suspension cables because they’re still sort of holding up parts of the bridge. Nobody knows which parts, but everybody’s pretty sure they’re important parts. After the introductions are made, you are invited to come up with some new ideas, but you don’t have any because you’re a propulsion engineer and don’t know anything about bridges.\n\nWould you drive across this bridge? No. If it somehow got built, everybody involved would be executed. Yet some version of this dynamic wrote every single program you have ever used, banking software, websites, and a ubiquitously used program that was supposed to protect information on the internet but didn’t.\n\nEvery programmer occasionally, when nobody’s home, turns off the lights, pours a glass of scotch, puts on some light German electronica, and opens up a file on their computer. It’s a different file for every programmer. Sometimes they wrote it, sometimes they found it and knew they had to save it. They read over the lines, and weep at their beauty, then the tears turn bitter as they remember the rest of the files and the inevitable collapse of all that is good and true in the world.\n\nThis file is Good Code. It has sensible and consistent names for functions and variables. It’s concise. It doesn’t do anything obviously stupid. It has never had to live in the wild, or answer to a sales team. It does exactly one, mundane, specific thing, and it does it well. It was written by a single person, and never touched by another. It reads like poetry written by someone over thirty.\n\nEvery programmer starts out writing some perfect little snowflake like this. Then they’re told on Friday they need to have six hundred snowflakes written by Tuesday, so they cheat a bit here and there and maybe copy a few snowflakes and try to stick them together or they have to ask a coworker to work on one who melts it and then all the programmers’ snowflakes get dumped together in some inscrutable shape and somebody leans a Picasso on it because nobody wants to see the cat urine soaking into all your broken snowflakes melting in the light of day. Next week, everybody shovels more snow on it to keep the Picasso from falling over.\n\nThere’s a theory that you can cure this by following standards, except there are more “standards” than there are things computers can actually do, and these standards are all variously improved and maligned by the personal preferences of the people coding them, so no collection of code has ever made it into the real world without doing a few dozen identical things a few dozen not even remotely similar ways. The first few weeks of any job are just figuring out how a program works even if you’re familiar with every single language, framework, and standard that’s involved, because standards are unicorns.\n\nI spent a few years growing up with a closet in my bedroom. The closet had an odd design. It looked normal at first, then you walked in to do closet things, and discovered that the wall on your right gave way to an alcove, making for a handy little shelf. Then you looked up, and the wall at the back of the alcove gave way again, into a crawlspace of utter nothingness, where no light could fall and which you immediately identified as the daytime retreat for every ravenous monster you kept at bay with flashlights and stuffed animals each night.\n\nThis is what it is to learn programming. You get to know your useful tools, then you look around, and there are some handy new tools nearby and those tools show you the bottomless horror that was always right next to your bed.\n\nFor example, say you’re an average web developer. You’re familiar with a dozen programming languages, tons of helpful libraries, standards, protocols, what have you. You still have to learn more at the rate of about one a week, and remember to check the hundreds of things you know to see if they’ve been updated or broken and make sure they all still work together and that nobody fixed the bug in one of them that you exploited to do something you thought was really clever one weekend when you were drunk. You’re all up to date, so that’s cool, then everything breaks.\n\n“Double you tee eff?” you say, and start hunting for the problem. You discover that one day, some idiot decided that since another idiot decided that 1/0 should equal infinity, they could just use that as a shorthand for “Infinity” when simplifying their code. Then a non-idiot rightly decided that this was idiotic, which is what the original idiot should have decided, but since he didn’t, the non-idiot decided to be a dick and make this a failing error in his new compiler. Then he decided he wasn’t going to tell anyone that this was an error, because he’s a dick, and now all your snowflakes are urine and you can’t even find the cat.\n\nYou are an expert in all these technologies, and that’s a good thing, because that expertise let you spend only six hours figuring out what went wrong, as opposed to losing your job. You now have one extra little fact to tuck away in the millions of little facts you have to memorize because so many of the programs you depend on are written by dicks and idiots.\n\nAnd that’s just in your own chosen field, which represents such a tiny fraction of all the things there are to know in computer science you might as well never have learned anything at all. Not a single living person knows how everything in your five-year-old MacBook actually works. Why do we tell you to turn it off and on again? Because we don’t have the slightest clue what’s wrong with it, and it’s really easy to induce coma in computers and have their built-in team of automatic doctors try to figure it out for us. The only reason coders’ computers work better than non-coders’ computers is coders know computers are schizophrenic little children with auto-immune diseases and we don’t beat them when they’re bad.\n\nRemember that stuff about crazy people and bad code? The internet is that except it’s literally a billion times worse. Websites that are glorified shopping carts with maybe three dynamic pages are maintained by teams of people around the clock, because the truth is everything is breaking all the time, everywhere, for everyone. Right now someone who works for Facebook is getting tens of thousands of error messages and frantically trying to find the problem before the whole charade collapses. There’s a team at a Google office that hasn’t slept in three days. Somewhere there’s a database programmer surrounded by empty Mountain Dew bottles whose husband thinks she’s dead. And if these people stop, the world burns. Most people don’t even know what sysadmins do, but trust me, if they all took a lunch break at the same time they wouldn’t make it to the deli before you ran out of bullets protecting your canned goods from roving bands of mutants.\n\nYou can’t restart the internet. Trillions of dollars depend on a rickety cobweb of unofficial agreements and “good enough for now” code with comments like “TODO: FIX THIS IT’S A REALLY DANGEROUS HACK BUT I DON’T KNOW WHAT’S WRONG” that were written ten years ago. I haven’t even mentioned the legions of people attacking various parts of the internet for espionage and profit or because they’re bored. Ever heard of 4chan? 4chan might destroy your life and business because they decided they didn’t like you for an afternoon, and we don’t even worry about 4chan because another nuke doesn’t make that much difference in a nuclear winter.\n\nOn the internet, it’s okay to say, “You know, this kind of works some of the time if you’re using the right technology,” and BAM! it’s part of the internet now. Anybody with a couple of hundred dollars and a computer can snag a little bit of the internet and put up whatever awful chunks of hack code they want and then attach their little bit to a bunch of big bits and everything gets a little bit worse. Even the good coders don’t bother to learn the arcane specifications outlined by the organizations people set up to implement some unicorns, so everybody spends half their time coping with the fact that nothing matches anything or makes any sense and might break at any time and we just try to cover it up and hope no one notices.\n\nHere are the secret rules of the internet: five minutes after you open a web browser for the first time, a kid in Russia has your social security number. Did you A computer at the NSA now automatically tracks your physical location for the rest of your life. Sent an email? Your email address just went up on a billboard in Nigeria.\n\nThese things aren’t true because we don’t care and don’t try to stop them, they’re true because everything is broken because there’s no good code and everybody’s just trying to keep it running. That’s your job if you work with the internet: hoping the last thing you wrote is good enough to survive for a few hours so you can eat dinner and catch a nap.\n\nFunny, right? No? How about this exchange:\n\nWasn’t that guy helpful? With the camel? Doesn’t that seem like an appropriate response? No? Good. You can still find Jesus. You have not yet spent so much of your life reading code that you begin to talk in it. The human brain isn’t particularly good at basic logic and now there’s a whole career in doing nothing but really, really complex logic. Vast chains of abstract conditions and requirements have to be picked through to discover things like missing commas. Doing this all day leaves you in a state of mild aphasia as you look at people’s faces while they’re speaking and you don’t know they’ve finished because there’s no semicolon. You immerse yourself in a world of total meaninglessness where all that matters is a little series of numbers went into a giant labyrinth of symbols and a different series of numbers or a picture of a kitten came out the other end.\n\nThe destructive impact on the brain is demonstrated by the programming languages people write. This is a program:\n\nThat program does exactly the same thing as this program:\n\nAnd once somebody wrote a programming language that let somebody else write this:\n\nAccording to the author, that program is \"two lines of code that parse two lines of embedded comments in the code to read the Mayan numbers representing the individual ASCII characters that make up the magazine title, rendered in 90-degree rotated ASCII art.\"\n\nThat program won a contest, because of course it did. Do you want to live in a world like this? No. This is a world where you can smoke a pack a day and nobody even questions it. \"Of course he smokes a pack a day, who wouldn't?\" Eventually every programmer wakes up and before they're fully conscious they see their whole world and every relationship in it as chunks of code, and they trade stories about it as if sleepiness triggering acid trips is a normal thing that happens to people. This is a world where people eschew sex to write a programming language for orangutans. All programmers are forcing their brains to do things brains were never meant to do in a situation they can never make better, ten to fifteen hours a day, five to seven days a week, and every one of them is slowly going mad.\n\nSo no, I’m not required to be able to lift objects weighing up to fifty pounds. I traded that for the opportunity to trim Satan’s pubic hair while he dines out of my open skull so a few bits of the internet will continue to work for a few more days.\n\n(Update: now available in Greek, Czech, Italian, Russian, Portuguese, Hungarian, French, Hebrew (PDF by Ilil Hoz), German (PDF by Kurt Frock), Spanish, and Chinese)",
    "readingTime": 13,
    "keywords": [
      "programming languages",
      "you’re familiar",
      "programming language",
      "suspension bridge",
      "don’t",
      "it’s",
      "internet",
      "they’re",
      "there’s",
      "together"
    ],
    "qualityScore": 1,
    "link": "https://www.stilldrinking.org/programming-sucks",
    "thumbnail_url": "https://www.stilldrinking.org/blog_images/programming-sucks.jpg",
    "created_at": "2026-01-28T06:22:46.410Z",
    "topic": "tech"
  },
  {
    "slug": "pixel-arcade-studio-kids-make-playable-browser-games-by-instructing-ai",
    "title": "Pixel Arcade Studio –kids make playable browser games by instructing AI",
    "description": "Kids create real browser games by giving clear instructions to AI. No coding, no downloads.",
    "fullText": "They need to learn how to give clear instructions to AI.\n\nPixel Arcade Studio is a browser-based game studio where kids create real, playable games by telling an AI assistant exactly what to build. No coding. No installs. Designed with parents in mind.\n\nSafe, creative screen time kids love. Try free for 14 days.\n\nFast \"time-to-wow\" — kids see their games come to life quickly.\n\nFrictionless — everything runs directly in the browser.\n\nSafety default — games publish with privacy protections enabled.\n\nCoding used to be how people told computers what to do.\n\nToday, the more important skill is knowing how to describe what you want, break ideas into steps, and give clear instructions to an AI system.\n\nPixel Arcade Studio is built around that shift.\n\nKids don't write code here. They practice explaining ideas, testing results, and improving their instructions when something doesn't work.\n\nThat's the skill they'll use in the real world.\n\nYour child picks a game template and describes what they want to make. Characters, goals, movement, and rules.\n\nYour child tells the AI assistant what to create or change. The AI follows instructions. It does not take over.\n\nThe game runs right away in the browser. No setup and no waiting.\n\nKids adjust their instructions, test again, and see how clearer directions lead to better results.\n\nGames can be shared with family or friends using parent-approved links.\n\nThis is not about memorizing technical skills. It's about clear thinking and communication.\n\nAI in Pixel Arcade Studio is a tool, not a shortcut.\n\nIt responds only to what your child asks. It does not browse the internet. It does not publish content on its own. It stays inside kid-safe boundaries.\n\nYour child stays in control. The AI helps carry out instructions.\n\nPixel Arcade Studio is built for families who want creative screen time without constant supervision.\n\nInstead, kids focus on giving clear instructions and seeing real results. They make games people can actually play.\n\nEvery game made in Pixel Arcade Studio is playable in the browser and shareable through safe links.\n\nKids don't just save projects. They create something real and playable.\n\nPixel Arcade Studio is a browser-based game studio where kids create playable games by giving instructions to an AI assistant. There is no coding involved.\n\nNo. Pixel Arcade Studio does not teach coding. Kids learn how to clearly describe ideas, give instructions, and work with AI to create games.\n\nPixel Arcade Studio is designed for kids ages 7 to 12.\n\nYes. Games publish in safe mode by default, sharing requires parent approval, and the platform includes content filtering and privacy protections.\n\nNo. Everything runs directly in the web browser. There are no downloads or installs.\n\nThe AI follows your child's instructions. It helps turn ideas into games but does not take control or act on its own.\n\nYes. Games can be shared using parent-approved links so friends and family can play safely.\n\nPixel Arcade Studio does not involve coding or block-based programming. It focuses on teaching kids how to give clear instructions to AI and refine their ideas through iteration.\n\nNo coding. No downloads. Designed for ages 7–12.",
    "readingTime": 3,
    "keywords": [
      "pixel arcade studio",
      "creative screen",
      "privacy protections",
      "parent-approved links",
      "browser-based game",
      "kids don't",
      "games publish",
      "playable games",
      "the ai",
      "kids create"
    ],
    "qualityScore": 1,
    "link": "https://pixelarcade.studio",
    "thumbnail_url": "http://localhost:3000/images/pas_og.jpg",
    "created_at": "2026-01-28T06:22:43.763Z",
    "topic": "tech"
  },
  {
    "slug": "acm-sigplan-symposium-on-principles-of-programming-languages-popl-2026-talks",
    "title": "ACM SIGPLAN Symposium on Principles of Programming Languages (POPL) 2026 talks",
    "description": "Special Interest Group on Programming Languages\nThe ACM Special Interest Group on Programming Languages (SIGPLAN) explores programming language concepts and tools, focusing on design, implementation, practice, and theory. Its members are programming language developers, educators, implementers, researchers, theoreticians, and users.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.youtube.com/channel/UCwG9512Wm7jSS6Iqshz4Dpg",
    "thumbnail_url": "https://yt3.googleusercontent.com/ytc/AIdro_mB1glk2WSnakJ9VmIADfpyj122SV1iL5zw1rMBEQFIqQ=s900-c-k-c0x00ffffff-no-rj",
    "created_at": "2026-01-27T18:24:29.546Z",
    "topic": "tech"
  },
  {
    "slug": "agent-skills-from-claude-to-open-standard-to-your-daily-coding-workflow",
    "title": "Agent Skills: From Claude to Open Standard to Your Daily Coding Workflow",
    "description": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we're witnessing something far more...",
    "fullText": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we’re witnessing something far more significant: an open standard reshaping how people across roles—developers, designers, product managers, and operations—work with AI assistants. AI coding agents’ adoption of Agent Skills has transformed this technology from an interesting experiment into an essential developer tool.\n\nIf you’ve been using custom instructions or wondering how to make your AI assistant truly understand your project’s workflows, Agent Skills provides a compelling solution.\n\nAgent Skills began with Anthropic’s Claude AI, where developers first experienced giving AI agents specialized capabilities through structured instructions. Unlike simple prompts or one-off commands, Agent Skills introduced a sophisticated approach: packaging instructions, scripts, templates, and documentation into reusable, discoverable units.\n\nAnthropic’s decision to release Agent Skills as an open standard transformed it from a Claude-specific feature into a movement. The format’s simplicity and effectiveness attracted attention across the AI development tools ecosystem. Today, major players—Claude Code, GitHub Copilot, Cursor, OpenCode, Mistral Vibe, Antigravity, OpenAI Codex, and Kiro—have adopted the standard. Others are exploring integration, and more are joining it (I’m looking at you, JetBrains Junie).\n\nAgent Skills are elegantly simple: a folder containing a SKILL.md file. This file uses YAML frontmatter for metadata and Markdown for instructions. No complex APIs, no proprietary formats—just structured text any AI agent can understand.\n\nHere’s a basic Agent Skills example for creating NUnit unit tests in C#:\n\nThose Agent Skills files live in your agent configuration, for example for GitHub Copilot .github/skills/create-nunit-unit-test/SKILL.md in your repository. Or they can be globally installed for your user account, e.g. ~/.copilot/skills/create-nunit-unit-test/SKILL.md.\n\nThis is a minimal example. You can add resources like templates, example configurations, or helper scripts in the same directory, and the skill can reference them.\n\nWhat makes Agent Skills innovative isn’t just the format—it’s how AI agents consume them. The system uses a three-level progressive disclosure approach that optimizes context window usage:\n\nLevel 1: Discovery — At startup, the agent reads only the name and description from each skill. This lightweight metadata helps the agent understand available capabilities without consuming context.\n\nLevel 2: Activation — When your request matches a skill’s description, the agent loads the full instructions from the SKILL.md file. Only then do detailed procedures become available.\n\nLevel 3: Execution — The agent accesses additional files (scripts, examples, documentation) only as needed during execution.\n\nThis architecture solves a critical problem: you can install dozens of Agent Skills without overwhelming the AI’s context window. The agent loads only what’s relevant to your current development task.\n\nGitHub Copilot’s Agent Skills are experimental since December 2025 (version 1.108) for VS Code. Here’s your step-by-step setup guide:\n\nInstall VS Code — Download from code.visualstudio.com\n\nEnable Agent Skills — Open settings (Ctrl+,) and enable chat.useAgentSkills\n\nCreate your skills directory — In your project root, create .github/skills/\n\nAdd your first skill — Create a subdirectory for each skill with its SKILL.md file\n\nUse Agent Mode — In Copilot Chat, switch to Agent mode to leverage skills\n\nOnce configured, Agent Skills activate automatically based on your prompts. No manual selection required—the AI determines which skills are relevant based on your descriptions.\n\nSkills share knowledge—best practices, workflows, and procedural guidance—in simple Markdown SKILL.md files that anyone can author; they load progressively to conserve tokens, require no server, and run on web, desktop, and CLI, making them ideal for documentation, checklists, examples, and repeatable workflows.\n\nMCP extends functionality by connecting to APIs, databases, and external tools: it consists of code and service/tool definitions that require development and hosting, loads tool definitions up front (consuming more context), so it’s best suited for tasks needing direct access to external systems.\n\nUse Skills to make knowledge discoverable and consistent, and use MCP to perform integrated actions and extend platform capabilities; together they provide both lightweight guidance and powerful automation.\n\nNevertheless, I can imagine a future where Agent Skills replace MCP for many scenarios, given their simplicity, portability, and ease of authoring. As you can bundle scripts and resources with skills, they can cover many use cases MCP currently serves.\n\nYou might wonder how Agent Skills differ from the custom instructions feature. Custom Instructions are best for defining coding standards and conventions, setting language or framework preferences, specifying code-review guidelines, and establishing commit-message formats. Agent Skills are designed to package reusable workflows, include executable scripts and templates, define specialized procedures (testing, debugging, deployment), and enable capabilities that run beyond the IDE (CLI and coding agents).\n\nThink of custom instructions as your coding style guide and Agent Skills as your AI development toolbox. Custom instructions tell the AI how you want code written; Agent Skills give the AI specialized capabilities to perform complex development tasks.\n\nHere are some practical Agent Skills that can transform your daily development workflow. Check the references section for pointers to more ready-to-use skills:\n\nRead the Agent Skills Specification to understand the format and capabilities. Use Skill Creator, an Agent Skill to create and refine new Agent Skills. Inception moment anyone 🤔?\n\nStart building your Agent Skills collection with these proven strategies:\n\nIdentify Repetitive Tasks — Notice which development workflows you explain to the AI repeatedly. Each recurring explanation is a candidate for an Agent Skill.\n\nStart Simple — Begin with straightforward skills that codify standard development procedures. As you gain confidence, add scripts and more complex resources.\n\nMake Descriptions Specific — The quality of your skill’s description directly impacts how well the AI knows when to activate it. Be explicit about use cases and capabilities.\n\nInclude Examples — Agent Skills with concrete code examples are more effective. Show the AI what good output looks like.\n\nLeverage Community Skills — Explore the github/awesome-copilot and anthropics/skills repositories for inspiration and ready-to-use skills.\n\nOrganize by Domain — Group related Agent Skills together. Create separate skills for testing, deployment, documentation, code review, and other specialized development domains.\n\nHere’s how Agent Skills enhance your workflow in a typical development scenario:\n\nYou’re working on a web application and need to add a new REST API endpoint with proper testing and documentation. With appropriate Agent Skills in place:\n\nYou ask: “Help me add a new user registration endpoint with validation”\n\nThe rest-api-integration skill activates, providing structured guidance on implementing the endpoint with proper authentication, validation, and error handling.\n\nYou ask: “Create tests for this endpoint”\n\nThe webapp-testing skill engages, generating test cases for success scenarios, validation failures, and edge cases.\n\nYou ask: “Generate documentation for this endpoint”\n\nThe api-documentation skill activates, producing comprehensive documentation with examples, error codes, and authentication details.\n\nEach Agent Skill ensures consistency in approach and completeness in implementation. Without skills, you’d need to provide detailed instructions for each request or rely on the AI’s general knowledge, which might miss project-specific patterns.\n\nWhen working with Agent Skills, especially community-shared skills, keep these security considerations in mind:\n\nReview Before Use — Always examine shared Agent Skills before adding them to your project. Check for potentially malicious scripts or unexpected behaviors in the SKILL.md file and associated resources.\n\nUse Terminal Controls — VS Code’s terminal tool provides controls for script execution, including auto-approve options with configurable allow-lists. Configure these appropriately for your security requirements.\n\nVersion Control Your Skills — Agent Skills are just files, so commit them to your repository. This enables code review, versioning, and team collaboration on AI capabilities.\n\nTest in Safe Environments — Try new Agent Skills in development environments before using them in production contexts. Dev containers or isolated workspaces are ideal for testing.\n\nDocument Team Skills — If your team uses shared Agent Skills, maintain documentation about what each skill does and when to use it.\n\nAgent Skills represent more than a new feature—it’s a glimpse into a future where AI development capabilities are portable, shareable, and composable. As more AI tools adopt the standard, we’re moving toward an ecosystem where:\n\nWhether you’re using VS Code or any other editor/IDE, working in the terminal with Copilot CLI, or leveraging any coding agent for automated tasks, your Agent Skills come with you.\n\nReady to integrate Agent Skills into your development workflow? Follow this action plan:\n\nThe goal isn’t to create dozens of Agent Skills immediately. Start with one or two that solve real problems in your development workflow, then expand your library organically as needs arise.\n\nYou can also use Agent Skills with GitHub Copilot CLI or Gemini CLI for terminal-based workflows, or with other coding agents that support the open standard. This portability ensures your investment in creating skills pays off across all your AI-assisted development tools.\n\nMy preferred introduction to Agent Skills is the following video from Burke Holland, which covers the concepts, setup, and practical examples in under 20 minutes:\n\nFor my French readers, I discussed Agent Skills in depth on devdevdev.net in the following episode\n\nAgent Skills bridges the gap between generic AI assistance and specialized, context-aware support for your specific development needs. By adopting an open standard that works across AI tools, the industry has created a foundation for truly portable AI capabilities.\n\nThe journey from Claude to open standard to GitHub Copilot adoption demonstrates the power of simplicity and interoperability in developer tools. As developers, we benefit from this ecosystem approach—our investment in creating Agent Skills pays dividends across our entire development toolchain.\n\nStart small, experiment with the format, and build Agent Skills that improve your daily development work. The progressive disclosure system ensures you won’t overwhelm your AI assistant, and the portable format guarantees your skills remain valuable as AI tools evolve.\n\nThe future of AI-assisted development isn’t just about more powerful models—it’s about giving those models the right context, capabilities, and knowledge to be genuinely helpful in your specific development domain. Agent Skills is a significant step in that direction.\n\nWhat development workflows could benefit from specialized Agent Skills? Have you tried creating skills for your AI coding assistant? Share your experiences in the comments below.",
    "readingTime": 9,
    "keywords": [
      "agent skills",
      "skill.md file",
      "progressive disclosure",
      "ai-assisted development",
      "skill’s description",
      "context window",
      "agent mode",
      "shared agent",
      "coding agents",
      "skill activates"
    ],
    "qualityScore": 1,
    "link": "https://laurentkempe.com/2026/01/27/Agent-Skills-From-Claude-to-Open-Standard/",
    "thumbnail_url": "https://live.staticflickr.com/65535/55058424290_cced09531e_h.jpg",
    "created_at": "2026-01-27T12:26:46.640Z",
    "topic": "tech"
  },
  {
    "slug": "one-developer-used-claude-to-build-a-memorysafe-extension-of-c",
    "title": "One developer used Claude to build a memory-safe extension of C",
    "description": "feature: Robin Rowe talks about coding, programming education, and China in the age of AI",
    "fullText": "feature TrapC, a memory-safe version of the C programming language, is almost ready for testing.\n\n\"We're almost there,\" Robin Rowe told The Register in a phone interview. \"It almost works.\"\n\nWe caught up with Rowe, a computer science professor and entrepreneur, amid debugging efforts that had kept him up until four in the morning. The long-awaited TrapC website has appeared.\n\n\"My work building TrapC has taken two parallel paths,\" Rowe explains in his initial post. \"A TrapC interpreter called itrapc and a separate compiler called trapc. I had wanted to make a software release by 1 January 2026, but too many bugs. I only reached code complete this month and am now on the painstaking and sleepless process of debugging. When I have something stable that mostly works I will make a release. Sorry to make you wait a little longer. Aiming for Q1 2026.\"\n\nBack in November 2024, Rowe explained that he was working on TrapC. At the time, the public and private sector had undertaken a campaign to promote memory-safe software development as a way to reduce exposure to serious vulnerabilities.\n\nMemory safety provides a way of ensuring that memory-related bugs like out-of-bounds reads/writes and use-after-free don't happen. In large codebases, like Chromium and Windows, most of the security vulnerabilities follow from memory bugs. As that message has been repeated in recent years, memory safety has become an imperative, evangelized by the likes of Google and Microsoft, and more recently by authorities in the US and elsewhere.\n\nFor at least the past ten years, there's been a rising chorus of voices calling for the adoption of memory-safe programming languages and techniques. This has meant encouraging developers to avoid languages like C and C++ where feasible, and to adopt languages like C#, Go, Java, Python, Swift, and Rust, instead, particularly for new projects.\n\nTo remain relevant, the C and C++ communities have tried to address those concerns with projects like TrapC, FilC, Mini-C, Safe C++, and C++ Profiles. There's also a C to Rust conversion project under development at DARPA called TRACTOR – TRanslating All C TO Rust.\n\nBut progress has been slow and those writing in C and C++ haven't found a widely accepted approach. The C++ standards committee recently rejected the Safe C++ proposal. And Rowe said he doubted TRACTOR would have anything to show this year.\n\nMeanwhile, the clock is ticking. Microsoft engineer Galen Hunt last month said, \"My goal is to eliminate every line of C and C++ from Microsoft by 2030. Our strategy is to combine AI and algorithms to rewrite Microsoft's largest codebases.\"\n\n\"There are some efforts to port C code by hand to Rust,\" said Rowe. \"But there're some real challenges to doing that because there are some idioms in C that cannot be expressed in Rust.\n\n\"Rust is much more type safe than C is. And so if you have a void pointer, what does that mean in Rust? There's no translation for it. And that's how TrapC is fundamentally different because it actually remembers what that void pointer actually is.\"\n\nRowe said he expects TRACTOR will eventually be able to accomplish C to Rust translation using AI. But he said he thinks it's better to just build the necessary tooling into the C compiler, so you don't have to rely on some external tool that rewrites your code in an unfamiliar language.\n\nRowe has been using AI tools himself and has been teaching others to do so. This past semester, he taught AI Cybersecurity Programmer Analyst (PCO471) at Community College of Baltimore County – Linux administration using vibe coding in bash with no prerequisites. And starting in February, he's teaching C++ Programming with Generative AI (PCO472) – vibe coding in C++.\n\nRowe said programming has fundamentally changed as a result of AI tools. \"I think this is sort of the same type of discussion as when C came in and people said, 'Well, I'm happy in assembly.' There will still be people doing it the old way. But because vibe programming is so much more efficient on time when done correctly, there's gonna be no choice. You just won't be competitive if you're not vibe programming.\"\n\nThen he shifted gears, slightly. \"But I have to walk that back a little bit because the reason I was up until four in the morning is I had vibe programming working on the Trap C compiler. And it took a fundamentally wrong design turn. And I didn't detect that it had made a design mistake. I had told it how I wanted to approach it. But somehow it misunderstood me or it forgot or something happened and I forgot to check. And so I spent hours doodling around in the debugger and trying to understand why code was acting weird before I finally looked at it and said, 'wait a minute, this isn't even the right design.'\"\n\nRowe said a similar situation crops up in pair programming, where you've told someone to do something and they didn't do it, and you don't realize that until later.\n\n\"[C++ creator] Bjarne Stroustrup famously said that the most important thing in software design is to be clear about what you're trying to build,\" Rowe said. \"And vibe [programming] just puts that on steroids. Now we not only have to be ourselves clear, but we have to communicate it clearly to an LLM.\"\n\nRowe argues that developers have to be encouraged to try AI tools and to make mistakes. He recounted how during his AI Cybersecurity Programmer Analyst course, his students expressed interest in doing more hands-on work in lieu of lectures.\n\n\"So I said, 'I've got real servers on the internet that are my companies. I'll give you root,'\" he recalled. \"I'll set loose students who know nothing on my own servers and hope for the best and we'll see how this goes. And the reaction was panic. I couldn't get past the timidity cliff.\"\n\nRowe said that what he learned from that exchange was that they didn't want their own hands-on, they wanted to watch him work.\n\n\"I said to them, 'But guys, this is like learning to play the piano. You can't learn to play the piano by watching me. Yeah, you guys have to practice. And it's gonna be embarrassing at first. You know, you're gonna play a lot of bad notes and sound terrible. You have to get over that situation'.\"\n\nThat's a scenario playing out in various companies where AI tools remain underutilized, for various reasons, including lack of training, security concerns, lack of utility, and poor tool design.\n\nRowe has traveled often to China to speak at the China Association of Higher Education conference. In December, he said, he was interviewed on China News Television about how China's plan for AI compares with America's.\n\nIn an email he explained, \"I said, 'China's AI-Plus plan calls for efficient AI on devices everywhere, from farm to factory to city, while the White House plan calls for building 500-billion-dollar cloud data centers ... using chips that will, inevitably, seem obsolete within two years.'\"\n\nRowe argues China's approach will prevail and that the US has taken the wrong turn by focusing on centralized cloud datacenters to run LLMs. Within two years, he said, we'll have AI models we can run locally on our phones, with no need for network access for most tasks. Apple and Huawei, he said, are likely to be the winners in this scenario.\n\nRowe pointed to China's DeepSeek as an example. While it may not be quite as good as the leading US commercial models, he said, it runs with far less power.\n\n\"This is a very Moore's Law type of strategy,\" he said. \"I remember when I had a Navy supercomputer in 1994. That was an amazing technology. But in 1995, Cray went bankrupt. There weren't enough buyers for it, even though it was an amazing device.\n\n\"And now I've got an iPhone that's in my pocket. That runs on a battery. It doesn't have a whole room devoted to it and exotic cooling and all kinds of stuff. And it's more powerful than that [the Cray from 1994]. So as a long-term strategy, you know, going toward the device makes a lot more sense, because that half-trillion dollar data center is going to be on my iPhone eventually.\"\n\nRowe also said that on the recommendation of a friend from his time at the AT&T DIRECTV Innovation Lab, he tried running Deepseek at a time when Claude wasn't available. Deepseek, he said, was able to find a bug that Claude couldn't.\n\n\"Surprisingly, the bug was in code Claude had generated, that I had cut-and-pasted carelessly,\" he said. \"With hindsight it was a silly code mistake I should have caught, but was in an 'else' branch outside where I was looking. I'd not expected or intended to have Claude make any change to that block of code. And because the code was valid but the logic wrong, the compiler didn't catch it.\"\n\nBut the bug was obvious, he said, as soon as Deepseek pointed it out. He added, \"I'm paying $200/year for Claude. Deepseek is free.\" ®",
    "readingTime": 8,
    "keywords": [
      "cybersecurity programmer",
      "programmer analyst",
      "rowe argues",
      "void pointer",
      "memory safety",
      "vibe coding",
      "design rowe",
      "vibe programming",
      "code",
      "compiler"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/26/trapc_claude_c_memory_safe_robin_rowe/",
    "thumbnail_url": "https://regmedia.co.uk/2022/03/23/shutterstock_c.jpg",
    "created_at": "2026-01-27T06:21:27.835Z",
    "topic": "tech"
  },
  {
    "slug": "a-developer-teamed-up-with-claude-to-create-elo-programming-language",
    "title": "A developer teamed up with Claude to create Elo programming language",
    "description": "feature: Bernard Lambeau, the human half of a pair programming team, explains how he's using AI",
    "fullText": "feature Bernard Lambeau, a Belgium-based software developer and founder of several technology companies, created a programming language called Elo with the help of Anthropic's Claude Code.\n\nStarting on December 25, 2025, he published a series of posts about the project. The first post names Claude as a co-author.\n\n\"In roughly 24 hours of collaboration, we built a complete expression language with a parser, type system, three compilers, a standard library, a CLI tool, and a documentation website. Not bad for a day's work,” Lambeau and Claude wrote.\n\n\"Elo isn't just a demonstration that AI can write code. It's a demonstration that humans and AI can build together – each contributing what they do best,” they added.\n\nAs an expression language that compiles to JavaScript, Ruby, and SQL, Elo is intended as a portable way to handle form validation, e-commerce order processing, and subscription logic.\n\nLambeau, founder and CTO of Klaro Cards and CEO of app consultancy Enspirit, is not the first to develop a programming language with the help of AI.\n\nSteve Klabnik performed a similar feat last year with the Rue programming language. In September 2025, Geoffrey Huntley enlisted Claude to write a programming language called Cursed. And before that, Avital Tamir published a Claude-authored repo for the Server programming language, with the caveat that the code is not intended for actual use.\n\nClaude Code isn't the only AI-assisted programming method having a moment. AI biz Cursor created a rudimentary browser using OpenAI's GPT-5.2. And developer Ola Prøis used Cursor, powered by Claude, to create a Rust-based text editor called Ferrite.\n\nClaude users generally acknowledge that their pair partner makes mistakes. But those committed to AI assistance find it worthwhile to clean up after their helper.\n\n\"Claude Code knows almost every tech stack (and can search the web), knows the Linux commands that matter (search code, search & replace, compile, test, etc.), and does that 10x faster than I can do myself,\" Lambeau told The Register in an email interview.\n\nClaude, he said, allows him to use technology he hasn't mastered.\n\n\"I was already a full-stack developer (on languages, frameworks & reusable libraries I knew); I'm now a full-stack++ dev because I can also use languages, frameworks, and reusable libraries I barely know, if at all,\" he explained.\n\n\"Claude Code falls short if you don't have a great methodology. It needs feedback loops to work fine; otherwise, it derails. One possible feedback loop is a human reviewing code and testing manually. But there's a better/complementary approach if you want it to work autonomously. On both Elo and Bmg.js, I've started by making sure the testing methodology was effective and scientifically sound. Claude writes the tests, executes them, discovers where it's wrong, and corrects itself. Impressive.\"\n\nLambeau said he still needs to review some of Claude's output.\n\n\"But if I read the tests, agree with them, and can check myself that they run fine, I'm 95 percent sure it's already correct as a black box (not even reading the code),\" he explained. \"Then I can check the architecture and code quality as a white box by having a general look at the code, but I don't have to understand every detail.\"\n\nNotably, Lambeau documented the series of prompts he used to create the language. The repo includes more than 100 tasks used to direct the AI model. In addition, Lambeau has published a video that describes his AI pair programming process.\n\n\"I started in a setting where Claude Code asked for permissions every 20 seconds and I was checking everything it did,\" Lambeau explained. \"After a few successes, I quickly set up safe environments to be able to let Claude Code run in full autonomy (isolated computer & isolated Linux user, or running in a Docker image).\"\n\nLambeau said he still uses plan mode for complex tasks that require conversation with Claude.\n\n\"I review the plan, make sure we have a test strategy that's sound, then switch Claude to autonomous mode and look at the tests, code & results afterward,\" he said. \"That's very similar to a lead-dev/CTO + QA role, btw; it's just much faster than with human devs.\"\n\nLambeau, who has a PhD in software engineering and 30 years of experience as a developer, said both experts and novices can benefit from Claude Code, though he added that a service like Lovable might be more approachable for those not already acclimated to the command line.\n\n\"Now, when it comes to real software/product engineering, I think Claude Code requires experts (so far),\" he said. \"You still need to guide it a lot to keep the quality high enough. You need very strong expertise to do it effectively. Currently (Claude will still improve a lot), if you don't have the expertise, you certainly end up with a big mess of unmaintainable code.\"\n\nMany developers have said as much about AI tools. They're more useful as an amplifier of expertise than as a replacement for it. The situation is analogous to the introduction of sequencing software, digital synthesizers, and drum machines half a century ago. These tools enabled a lot of people who weren't great musicians to make music. But they didn't instill musical skill, and they produced the most interesting work in the hands of practiced musicians.\n\nThe cost to do this, Lambeau said, has been a Claude Max subscription that he purchased in December for €180 a month. In that time, he says, he wrote Elo (https://elo-lang.org), completed Bmg.js (https://github.com/enspirit/bmg.js), completed Bmg's documentation (https://www.relational-algebra.dev), and created the first version of the Try page (https://www.relational-algebra.dev/try).\n\n\"It's all personal research and open-source projects,\" he said. \"It would have required several weeks to do the same manually myself, and several months to ask another developer to do it. The cost would be mostly because of the scientific & technical knowledge transfer about the data language I envision. Strangely enough, it's very cheap with Claude Code. There's something true about the fact that those LLMs have a PhD.\"\n\nLambeau explained that Elo isn't just a way to test Claude Code. He also sees it as an extension of his academic work in software engineering and his personal interest in the Relational Model – he's served as a lecturer for database courses at Belgium’s UCLouvain.\n\n\"I'm absolutely convinced that we need better/safer/simpler programming languages inside no-code tools and when interconnecting them (e.g. Zapier, Make, n8n, etc.),\" he said. \"Mainstream programming languages are very complex, error-prone, sometimes dangerous, and the programs are difficult to review for non-experts.\"\n\n\"More importantly, they are cumbersome to use for even simple data tasks. I mean, even validating the schema and constraints of a data file at runtime tends to be a nightmare in existing languages. It's not built-in in any mainstream language; you immediately need validation libraries; most of them are limited in what they can easily check, so you need to add dedicated boilerplate code.\"\n\nIn a world where non-technical people will have the opportunity to write untrustworthy code with the help of AI, he said, we need to be able to run that code safely.\n\n\"Elo aims at providing a safe & simple alternative,\" he said. \"It will be a limited language (non-Turing-complete, as we say) but super safe & simple, and usable in 80 percent of common data use cases. The very first no-code tool to integrate it will be Klaro Cards, of course.\" ®",
    "readingTime": 7,
    "keywords": [
      "claude code",
      "elo isn't",
      "reusable libraries",
      "safe simple",
      "languages frameworks",
      "software engineering",
      "expression language",
      "programming language",
      "it's",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/24/human_ai_pair_programming_elo/",
    "thumbnail_url": "https://regmedia.co.uk/2025/11/06/shutterstock_balancing_ai_and_humanity.jpg",
    "created_at": "2026-01-26T01:03:11.981Z",
    "topic": "tech"
  },
  {
    "slug": "5-acquisitions-winning-over-skeptical-engineers-and-spending-tens-of-millions-inside-a-public-companys-ai-native-push",
    "title": "5 acquisitions, winning over skeptical engineers, and spending tens of millions: Inside a public company's 'AI native' push",
    "description": "Amplitude gave Business Insider the inside look at its AI overhaul, from acquisitions to efforts to increase staff adoption of AI coding assistants.",
    "fullText": "There's a long banner hanging in Amplitude's San Francisco office. It reads: \"NO MAGICAL THINKING.\"\n\nNo, it's not some rag on Joan Didion. It's a reminder, CEO Spenser Skates told Business Insider, that technology can never replace deep thinking and hard work. In the AI age, that reminder is more important than ever — so much so that employees must look up at it every day.\n\nAmplitude, an 800-person, publicly traded analytics company, is undergoing an AI transformation — with the goal of reinvigorating its business.\n\nAmplitude went public in September 2021 at the height of the pandemic, climbing to an all-time closing high of $84.80 per share several weeks later before dropping significantly and largely plateauing in in recent years around $10. It closed at $10.25 on Friday.\n\nSince October 2024, the company has acquired five AI startups. Amplitude hired an AI-savvy engineering head and appointed one of its acquired founders to a new AI leadership position. It got Cursor and GitHub Copilot licenses for employees, and ran a heads-down AI week.\n\nIt's a change many companies are making: Rapidly moving from little-to-no AI to trying to become \"AI native,\" a term that's curiously hard to pin down. Large language models are popping up everywhere in white-collar work as companies chase the promise of efficiency gains.\n\nAmplitude's case may be especially informative, given just how skeptical of AI its CEO was. In 2023 and some of 2024, Skates said he viewed the AI industry as full of \"grifters,\" the visionaries promising to end world hunger and salesmen promising to automate everything.\n\n\"It had all sorts of problems,\" Skates said. By mid-2024, he realized \"there's probably going to be a breakthrough in the analytics space in the next two or three years.\"\n\n\"We've got to go make that ourselves,\" he said. \"So, we went all in.\"\n\nSkates had two opening moves for his AI overhaul.\n\nThe first: hiring a new chief engineering officer with a history in AI. Wade Chambers had advised the company since 2016, while holding leadership roles at Twitter and Included Health.\n\nWhen Chambers joined in October 2024, only 1% of the engineering, product, and design teams at Amplitude were using AI.\n\nThe second was the acquisition of Command AI, a chatbot startup. It was the first of a string of acquisitions, including June, Kraftful, and Inari. Amplitude announced its most recent acquisition, InfiniGrow, on January 14.\n\nYana Welinder was CEO of Kraftful, one of Amplitude's acquisition targets. Kraftful could spot power users of its product, one of whom was Amplitude's then-CPO. She reached out, and they chatted in February. The deal closed in July, and Welinder was named Amplitude's head of AI. A company blog post with an introductory Q&A referred to her as \"AI maven.\"\n\nWelinder's first order of business: speeding the company up. Kraftful shipped new product every week. Amplitude was shipping less than monthly.\n\n\"If you have this cadence of shipping infrequently, then the team slows down, which isn't appropriate in the age of AI,\" she said.\n\n\"Analytics will look very different 6 months from now,\" Skates wrote in his email. \"We have the opportunity to be the AI native company in Analytics and we are going to pull every piece of firepower we have.\"\n\nHe also asked employees to share a coming launch on X, as opposed to LinkedIn, because that's \"where the AI natives are.\"\n\nHow much has Amplitude spent on AI, from tools to acquisitions? \"Tens of millions, for sure,\" Skates said. \"I wouldn't be surprised if it got past $100 million.\"\n\nThen comes the harder part: convincing employees to really use the tools.\n\nWhile some engineers are excited about AI's promise, others are skeptical about its helpfulness, or worried about possible job losses. Not every engineer is as gung ho about AI as their management is.\n\nSkates said that engineers were especially sensitive to the \"grifting\" that went on in AI, making many of them skeptical. With a bottoms-up approach, that skepticism dissipates, he said.\n\nSoon after joining, Chambers began planning an \"AI week\" for the first week of June. It took six months of prep and borrowed heavily from Facebook's mobile push. He took the entire engineering, product, and design team offline for the week. To kick off, Chambers required that leaders get onstage and vibe-code something in front of the entire company.\n\n\"It didn't go well,\" Chambers said of the live vibe-coding demonstration. \"They had to work through it. They had to re-prompt a couple of different ways.\"\n\nBut the message stuck, he said. Leaders who weren't coding all day were able to build something \"pretty cool\" within the hourlong session, save a few hiccups.\n\nAdditional momentum came from the \"zealots,\" engineers passionate about exploring the new tech (some of whom Chambers brought over from his prior job). These engineers lead by example, he said.\n\nAmplitude shared its internal data tracking how many employees use its AI tools. In the final week of March, 14 employees were actively using Cursor. That figure peaked in the first week of December — after AI week but before the holiday vacation cycle — at 174 employees.\n\nAnd what of the thorny question about AI implementation in the enterprise: ROI? After all, a 2025 MIT study indicated 95% of firms publicly disclosing use of AI pilots reported no measurable ROI.\n\nAfter implementing these tools, developer productivity shot up 40% and stayed there, Chambers said. On some specific engineering teams, those gains looks more like 300-400%, he said.\n\n\"There's going to be a lot of people who are thinking they're the world's best expert at something,\" Chambers said. \"Increasingly, even the most cynical team members have come around.\"",
    "readingTime": 5,
    "keywords": [
      "engineering product",
      "roi after",
      "employees",
      "analytics",
      "tools",
      "engineers",
      "there's",
      "skeptical",
      "acquisition",
      "team"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amplitude-ai-native-push-2026-1",
    "thumbnail_url": "https://i.insider.com/69729d28d3c7faef0eccc73a?width=1200&format=jpeg",
    "created_at": "2026-01-25T12:22:41.686Z",
    "topic": "finance"
  },
  {
    "slug": "skget-another-cli-to-add-skills-to-your-coding-agents",
    "title": "Skget, another CLI to add skills to your coding agents",
    "description": "A CLI to add skills to your coding agents. Contribute to czheo/skget development by creating an account on GitHub.",
    "fullText": "czheo\n\n /\n\n skget\n\n Public\n\n A CLI to add skills to your coding agents.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n czheo/skget",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/czheo/skget",
    "thumbnail_url": "https://opengraph.githubassets.com/ec41d6a0818a69873753057252cfb3dd2acf4c9944a55eb817aea02d4c36721f/czheo/skget",
    "created_at": "2026-01-25T01:04:24.594Z",
    "topic": "tech"
  },
  {
    "slug": "five-ways-people-are-using-claude-code",
    "title": "Five Ways People Are Using Claude Code",
    "description": "Claude Code generates computer code when people type prompts, so those with no coding experience can create their own programs and apps.",
    "fullText": "Claude Code, an artificial intelligence tool that can generate computer code when people type a prompt, is having a viral moment.\n\nThe tool, which the A.I. start-up Anthropic introduced in May, has shown record growth over the past two weeks, the company said, without sharing its data. People had time to experiment with Claude Code over the holidays, Anthropic said, and users realized how capable it was.\n\nClaude Code is one of several A.I. coding tools — which also include Base44 and Cursor — that people with no coding experience are increasingly using to build their own websites, programs and apps, a trend known as “vibecoding.” People pay a subscription fee of $20 to $200 a month to use Claude Code, depending on the features they want.\n\nHere are five ways that people are using Claude Code:\n\nMr. Hindes, an assistant principal at a school for autistic children, has four children under the age of 9 and turned to A.I. to help him organize his family’s laundry.\n\nLast week, he prompted Claude Code to make a program to identify which clothes belonged to each of his three daughters so he could sort clean laundry into piles without their help. He took pictures of their clothes to teach Claude Code which T-shirt belonged to which daughter. Now he simply holds up the clothes to his laptop camera so the program tells him whom it belongs to.\n\n“The whole process was done within an hour, and the girls were really excited,” he said.\n\nMr. Hindes said he was now building a program with Claude Code to help his daughters independently work though the steps of their morning routine, as if playing a game.\n\n“I’ve tried to teach myself coding at various points but never stuck with it,” he said.\n\nMr. Stephenson, an art and architecture photographer, started using Claude Code in November to build a website about a documentary feature.\n\nThe website was created in about a day, he said, with an interactive map of New York City that captured his photos and audio recordings to document life in each borough.\n\n“Once the basic site was done, emboldened by my new capabilities, I started adding features I hadn’t even considered,” said Mr. Stephenson, who pays $20 a month for Claude Code. “Light/dark mode? Easy. Shuffle button? Done.”\n\nIf Claude Code could not solve a particular problem, he turned to Google’s A.I. chatbot, Gemini, to ask how it would approach the issue.\n\n“I’d envisioned something like this when I started a couple of years ago, but assumed it would cost thousands of dollars to build,” he said.\n\nMr. Roberts, an assistant prosecuting attorney, used Claude Code and Cursor in August to create a mobile app called AlertAssist, which lets users send a mass text to contacts in an emergency. Working in law enforcement got Mr. Roberts interested in trying to help people act quickly and safely in an emergency.\n\nThe design and user interface of the app are “very basic, but it works,” he said.\n\nDuring the coronavirus pandemic, Ms. Haubo Dyhrberg, an assistant professor of finance at the University of Delaware, had an idea to make a stock trading simulator for her class. She consulted her husband, a software engineer, but “the task seemed too daunting.”\n\nOn Monday, she downloaded Claude Code and within two hours had a working demo of a trading simulator that her students could use to trade securities in a mock market. She has built five different trading scenarios for students to explore various challenges in financial markets.\n\n“I never thought it would be this easy,” she said. “I can’t wait to test it out when the semester starts in two weeks.”\n\nMr. Bacus, who owns a welding and metal fabrication business, tapped Claude Code last month to create an A.I. assistant to manage his calendar and find him new business opportunities. The business is just him and three others, so “we’re not in the place right now to afford an office team,” Mr. Bacus said. “It’s all on me.”\n\nWith Claude Code, he built a personal A.I. assistant that connects to his calendar, Google Sheets and Gmail account so he can easily create estimates, track the progress of jobs and organize contracts.\n\n“I’m a skilled laborer who barely passed high school in the early 2000s,” Mr. Bacus said, adding: “But over the past few months, I’ve taught myself to build actual tools for my business.”",
    "readingTime": 4,
    "keywords": [
      "claude code",
      "a.i assistant",
      "trading simulator",
      "mr bacus",
      "mr stephenson",
      "mr roberts",
      "business",
      "coding",
      "program",
      "clothes"
    ],
    "qualityScore": 1,
    "link": "https://www.nytimes.com/2026/01/23/technology/claude-code.html",
    "thumbnail_url": "https://static01.nyt.com/images/2026/01/21/multimedia/CLAUDE-CODE-1-pztw/CLAUDE-CODE-1-pztw-facebookJumbo.jpg",
    "created_at": "2026-01-24T00:56:46.566Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-kills-open-source",
    "title": "Vibe Coding Kills Open Source",
    "description": "Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.15494",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-23T06:20:32.202Z",
    "topic": "tech"
  },
  {
    "slug": "gemini-cli-code-and-create-with-an-opensource-agent",
    "title": "Gemini CLI: Code and Create with an Open-Source Agent",
    "description": "Build real-world applications from the command line using Gemini CLI, Google's open-source agentic coding assistant that coordinates local tools and cloud services to automate coding and creative workflows.",
    "fullText": "Join this short course on Gemini CLI, taught by Jack Wotherspoon, Developer Advocate at Google.\n\nGemini CLI is an open-source agentic coding assistant that works from your terminal, giving it access to your local filesystem, development tools, and cloud services. This lets you delegate complex workflows—from building web features to creating marketing materials—through high-level instructions while the agent autonomously plans and executes multiple steps.\n\nIn this course, you’ll apply Gemini CLI to software development and creative tasks by building features for an AI conference. You’ll develop a website session catalog, create a data dashboard combining local and cloud data sources, and generate social media content from recordings. You’ll master context management, integrate MCP servers, and orchestrate across multiple services with Gemini CLI extensions.\n\nWhether you’re prototyping applications, automating development workflows, or studying topics in agentic AI, this course gives you hands-on experience coordinating multiple tools to build faster and work more efficiently.",
    "readingTime": 1,
    "keywords": [
      "gemini cli",
      "course",
      "development",
      "you’ll",
      "agentic",
      "tools",
      "cloud",
      "services",
      "features"
    ],
    "qualityScore": 0.65,
    "link": "https://learn.deeplearning.ai/courses/gemini-cli-code-and-create-with-an-open-source-agent/information",
    "thumbnail_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2026/01/Gemini-CLI-Code-Create-with-an-Open-Source-Agent_Banner_1920x1080__with_CTA.webp",
    "created_at": "2026-01-21T18:30:41.465Z",
    "topic": "tech"
  },
  {
    "slug": "vibebin-incuslxcbased-platform-for-selfhosting-persistent-sandboxes",
    "title": "Vibebin: Incus/LXC-based platform for self-hosting persistent sandboxes",
    "description": "vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh).  Create...",
    "fullText": "jgbrwn\n\n /\n\n vibebin\n\n Public\n\n vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh). Create and host your vibe-coded apps on a single VPS/server.\n\n License\n\n View license\n\n 9\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jgbrwn/vibebin",
    "readingTime": 1,
    "keywords": [
      "vibebin",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jgbrwn/vibebin",
    "thumbnail_url": "https://opengraph.githubassets.com/b08d7ce148c865871d7a69b11f4c233a6cd39949454bb9455bb3bdcf87a6a575/jgbrwn/vibebin",
    "created_at": "2026-01-21T12:26:57.995Z",
    "topic": "tech"
  },
  {
    "slug": "pragmatic-notes-on-running-dangerous-ai-coding-agents-in-cloud-vms",
    "title": "Pragmatic Notes on Running Dangerous AI Coding Agents in Cloud VMs",
    "description": "A practical approach to safely running AI coding agents with strong isolation using cloud VMs, Tailscale, and simple notification patterns.",
    "fullText": "Running coding agents with free reign is very powerful for a certain class of tasks, especially ones that require little human supervision, or where you want to close (or disconnect) your laptop, walk away, and come back to results.\n\nRecently there have been several HN discussions about safely running Claude Code or Copilot CLI agents, such as Yolobox – Run AI coding agents with full sudo without nuking home dir and Running Claude Code dangerously. These post detail the potential dangers and show how to run these agents more safely, and while reasonable, I find they lack in a few respects.\n\nIn particular, I want strong isolation, long running agent tasks, minimal cognitive overhead and I really value being able to close my laptop, walk away, and get notified on my phone when things are done. I do not mind paying for a cloud VM.\n\nThere are many valid ways to solve this problem. This post describes mine. It covers running multiple coding agents concurrently in a cloud VM, how I handle access and repos, and how I keep notifications simple.\n\nI generated some Terraform to spin up an Azure VM with a cloud-init.yml for setting up common tools/environments I use. Claude can generate a decent starting point for this quite easily, given your particular environment.\n\nFor secure access, I use Tailscale. Note: I'm not paid by them, but it is easily my favorite piece of infrastructure software!\n\nA cloud-init script installs Tailscale on first boot and automatically joins the VM to my tailnet. SSH access is enabled using Tailscale SSH. Once the VM is up, it appears on my private network with a stable hostname via Magic DNS. No SSH key management, no exposed ports.\n\nor connect using VS Code Remote SSH:\n\nhttps://code.visualstudio.com/docs/remote/ssh\n\nMost of the time I prefer tight, step by step control over code generation, working locally in VS Code with Copilot. For longer running or experimental tasks, I instead let an agent work remotely on a branch inside the VM, and pull the results once I am satisfied.\n\nWhile this is arguably git basics, it works well for me and I found that it is useful sharing how to set up a VM as a remote:\n\nOn the local machine, from the repo directory:\n\nThen you can pull clone and check out the branch, do the work, commit, and push to bare repo:\n\nFinally, locally, you can get the changes:\n\nI use tmux to manage long running sessions. This lets agents keep running after I disconnect, and makes it easy to juggle multiple concurrent sessions. If you are not familiar with tmux, it is worth learning!\n\nFor notifications, I use https://ntfy.sh.\n\nIt is free, extremely simple, and works over plain HTTP POST. I have the iOS app installed, so I can walk away from my laptop and still get notified when work completes. I explicitly instruct my agents to make a POST request once their work is done in the agent instructions.\n\nThat is it. No SDKs, no auth setup required for basic usage. The notification shows up immediately on my phone/browser.\n\nIf there is interest, I can publish a repo with the Terraform, cloud-init scripts, makefile, etc, and the old .devcontainer setup.",
    "readingTime": 3,
    "keywords": [
      "coding agents",
      "tasks",
      "laptop",
      "away",
      "access",
      "repo",
      "free",
      "close",
      "disconnect",
      "safely"
    ],
    "qualityScore": 1,
    "link": "https://jakobs.dev/pragmatic-notes-running-dangerous-ai-agents-cloud-vms/",
    "thumbnail_url": "/media/agents-vm.jpg",
    "created_at": "2026-01-21T12:26:57.044Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-over-now-just-coding",
    "title": "Vibe Coding Is Over, Now Just \"Coding\"",
    "description": "Regular thoughts on modernity, classicism, & technology.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://upstreamutopia.com/articles/?id=20260121-060942-vibe-coding-is-over,-now-ju",
    "thumbnail_url": "https://upstreamutopia.com/images/preview.jpg",
    "created_at": "2026-01-21T06:22:09.373Z",
    "topic": "tech"
  },
  {
    "slug": "sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima",
    "title": "Sandbox Your AI Dev Tools: A Practical Guide for VMs and Lima",
    "description": "AI coding assistants and other devtools can steal your credentials and data. Here's how to run them safely in isolated VMs using Lima on macOS/Linux.",
    "fullText": "AI coding assistants, npm, pip, and other development tools can run arbitrary code and scripts on your machine, potentially stealing SSH keys, API tokens, wallet keys, sensitive credentials and other private data without you noticing.\n\nThis guide shows you how to sandbox these tools in isolated VMs using Lima, so you can experiment and develop freely without putting your sensitive data at risk.\nJump straight to the guide, or read on for a bit of personal context.\n\nI’ve been having quite a bit of fun with AI assisted coding recently.\n\nI use LLMs for a wide range of things, including discussing architecture, design choices, learning about new tools and libraries I wasn’t previously aware of, to reviewing PRs and quickly cranking out dirty prototypes.\n\nEspecially for hobby projects that are not meant to ever go into production, I enjoy playing with AI tools fast and loose, producing results quickly and not getting slowed down by annoying things such as reading code before running it 🤣.\n\nAnd yeah… that’s obviously unsafe, unless it’s all contained in a sandbox!\n\nYou should never run potentially dangerous, experimental code on your main machine, since it could steal your passwords, API keys, environment variables, private keys, access to your communication tools, install services, and do all sorts of other nefarious things.\n\nNowadays I isolate all my devtools in VMs, and thought it might be useful to others if I put together a guide to shows how to do it. Well, here it is, and I hope it’ll be useful to you, too!\n\nYou’ll want to run the entire development environment, including the AI tool itself, inside a sandbox. This way it’s safe to install dependencies and to execute code, and unlocks other fun features like snapshots before running sketchy code, and reverting if something goes wrong.\n\nAnd it’s not just AI-generated code. Node.js/npm/yarn and Python/pip are particularly troublesome because they allow any package to run arbitrary scripts on your system during installation, and install tons of additional dependencies that can do the same. This attack vector is called “supply chain attack” and it happens all the time.\n\nVirtual Machines (VMs) and Containers (i.e. Docker, Podman, containerd) are the two most practical methods for isolating development tools from your host operating system. VMs provide much stronger protection and more flexibility overall, and are better suited for co-developing with AIs.\n\nContainer runtimes share the host operating system’s kernel, which means they’re fundamentally running on the same system as your main machine, just with isolated namespaces and resource limits. This creates several security concerns:\n\nIn contrast, a VM runs its own complete operating system with its own kernel. The hypervisor (like QEMU/KVM) creates a much stronger isolation boundary. Even if malicious code completely compromises the VM, it would need to exploit the hypervisor itself to reach your host, a significantly harder target.\n\nFurthermore, a VM enables better concurrency. It can run Docker containers, databases, web servers, multiple build processes, and background services all at once, and the AI tool can interact with everything naturally just like on a normal development machine.\n\nIn this guide, we use Lima VM to sandbox AI and devtools. Lima is a delightful, lightweight virtual machine manager for Linux and macOS which provides easy and quick ways to create and manage VMs.\n\nYou interact with Lima through the limactl command:\n\nVMs are based on templates, which can include (build on) other templates:\n\nThe Lima VM docs have platform-specific installation guides.\n\nHomebrew is recommended on macOS:\n\nOn Linux install the binary like this:\n\nNow ensure your Lima version is up-to-date:\n\nWe only want to share very specific host directories with the VM.\n\nLet’s create ~/VM-Shared on the host, which we later mount into the VM at ~/Shared (with write access):\n\nYou can use that directory to easily copy files between the host and the VM, and to share project directories from the host with the VM.\n\nDefaults for all VMs can be defined in ~/.lima/_config/default.yaml.\n\nLet’s create the default YAML file:\n\nLima conveniently creates default SSH configuration files for all VM instances, which makes it easy to log in with SSH (including using VS Code for a Remote-SSH session).\n\nI recommend using a ~/.ssh/config.d/ directory on the host and have SSH include all configs there by default. That allows us to simply link the Lima-created config files there to use them.\n\nAdd this as first line in your ~/.ssh/config file, to make SSH include all configs from there:\n\nGreat! After creating a new VM, we can now simply create a symlink to the Lima-generated SSH configs and use it to SSH into the instance.\n\nLet’s start an Ubuntu 25.10 VM instance, named dev.\nWe use the internal _images/ubuntu-25.10.yaml template because it doesn’t include the automatic home directory sharing:\n\nYou can share additional project-specific directories between host and VM in several ways:\n\nCreate a symlink for the SSH config file and SSH into the VM:\n\nLet’s update the services on the instance, and configure git:\n\nLet’s confirm that port forwarding works. We do this using a one-liner Python HTTP server (on port 7777) inside the VM, and accessing it from the host:\n\nThis section guides you through installing several other languages and development tools, including Golang, Node.js, Python, Rust, Docker.\n\nWe can accomplish that either by installing each tool according to it’s documentation, or by using a version manager such as mise (“mise-en-place”, 22k stars on Github) which can install hundreds of tools via a simple command-line interface.\n\nFirst, we install mise (“mise-en-place”, 22k stars on Github) and make bash support it:\n\nYou use mise latest <tool> to see the latest versions it knows about:\n\nNow you can install all the tools you want in a single command:\n\nTo manually install (or update) Golang in the VM, download the latest release and extract into /usr/local/go:\n\nThe Golang path needs to be in the PATH environment variable, which we have already added before.\n\nA good way to install a current version of Node.js in Ubuntu is by using nvm, a modern node version manager (90k stars on GitHub):\n\nNow it’s all installed and ready to use! Check the versions like this:\n\nPerhaps you don’t even need Docker, since Lima includes containerd and nerdctl by default. This is a Docker-compatible runtime and command-line interface that can also run images from Docker Hub:\n\nIf you do want to install Docker, the quickest way to install it by using their official get-docker.sh script:\n\nFor the group changes to take effect, exit the shell and re-login (may need a VM restart).\n\nVerify that user is in the ‘docker’ group:\n\nGitHub CLI provides a useful gh cli command that let’s you easily interact with GitHub and private repositories.\n\nYou can install it in the VM following the Linux installation instructions:\n\nWarning: Authorizing GitHub CLI to access private repositories will leave an API key in the VM which could potentially be stolen by unauthorized scripts (which is what we wanted to avoid in first place by running everything in a VM).\n\nOnly authorize it with gh auth login for private repo access if you accept the risks! I personally avoid having any sensitive credentials in the VM, in particular those that allow access to private GitHub repositories.\n\nIf you prefer an IDE like VS Code, you can use Remote-SSH to start a session inside the instance.\n\nPlease note that this is potentially unsafe, as explained in the Remote-SSH README:\n\nSecurity Note\nUsing Remote-SSH opens a connection between your local machine and the remote. Only use Remote-SSH to connect to secure remote machines that you trust and that are owned by a party whom you trust. A compromised remote could use the VS Code Remote connection to execute code on your local machine.\n\nSee also this discussion on GitHub for more context and information.\n\nNow a new VS Code window opens, and sets up VS Code Server:\n\nThen you can click “Open” and choose a folder, like Shared:\n\nBefore setting up the tools, let’s create a “Hello World” directory in the Shared folder as our playground:\n\nLet’s start with installing Claude Code in the VM, following the instructions in the documentation:\n\nOn first start, Claude asks you to authorize it.\n\nThe docs mention support for an ANTHROPIC_API_KEY environment variable (i.e. set in .bashrc), but that did not work when I tried it; claude CLI didn’t let me skip the login process. Only after the login was done it notified me about the existing environment variable, and whether I’d prefer to use that one.\n\nAfter the login, Claude Code CLI is ready to be ued in the VM! 🎉\n\nSince Claude is running in a VM, it might be permissible to run it in “dangerously skip permissions mode”, which makes it bypass all permission checks:\n\nYou could also create an alias for it and add it to your .bashrc:\n\nAnthropic provides documentation for using Claude in VS Code, and also offer a VS Code Claude extension.\n\nYou can install the Claude extension in the VM through the Remote-SSH session window:\n\nIn contrast to the CLI tool, the authentication flow did not work through the user interface, and I had to set the ANTHROPIC_API_KEY environment variable:\n\nReload the VS Code window (open command palette with Shift + CMD + P and choose “Developer: Reload Window”):\n\nNow the VS Code Claude extension should work:\n\nIf you want to enable “dangerously skip permissions mode” in the VS Code extension, you can enable it via your user settings. Open the settings (CMD + ,), search for “claude” and enable “Claude Code: Allow Dangerously Skip Permissions”:\n\nLet’s install Gemini CLI from Google next.\n\nThe documentation recommends installing it with npm, the Node.js package manager. You’ll need to install Node and npm first, see also the Node.js setup instructions.\n\nIt will ask you to authenticate:\n\nI chose “Login with Google”. Note that the authentication flow may require a retry if the first attempt times fails.\n\nAfter authorization is done, Gemini CLI works!\n\nYou can run Gemini in YOLO mode:\n\nAutomatically accept all actions (aka YOLO mode, see https://www.youtube.com/watch?v=xvFZjo5PgG0 \n\nThe alias you could define in .bashrc:\n\nCodex CLI is the AI dev tool from OpenAI/ChatGPT.\n\nIt will ask you to sign in, either via ChatGPT or by providing an API key:\n\nAfter that is done, Codex CLI is ready to work for you!\n\nYou can also run Codex in dangerous mode:\n\nSkip all confirmation prompts and execute commands without sandboxing. EXTREMELY DANGEROUS. Intended solely for running in environments that are externally sandboxed\n\nThere are several other great tools worth a mention:\n\nDrop your favorite tools in the comments below!\n\nVM clones and snapshots allow you even more flexibility and isolation. You can use them to quickly and cheaply run new VMs for experiments and specific projects based on already provisioned instances. Use them frequently!\n\nLima offers several ways to take VM snapshots and/or clone VMs.\n\nYou can make a copy of an existing VM instance with limactl clone. The existing instance needs to be stopped first.\n\nAfter all the initial VM setup is done, clone it and use it both as backup as well as a base for future instances:\n\nRemember that after starting a new instance, you probably want to symlink the VM SSH configuration to your ~/.ssh/config.d/ directory, so ssh knows about it (See also “SSH into the VM”):\n\nFor maximum security and flexibility, consider using multiple VMs for different purposes and trust levels. This approach provides better isolation and lets you tailor each environment to specific needs.\n\nHere are some suggested VM configurations:\n\nYou can quickly clone your base VM setup to create new instances for different projects using limactl clone, as described in the VM cloning section above.\n\nFor sensitive or production projects, consider dedicating a separate VM to each project. This prevents potential cross-contamination between projects and allows you to mount only the specific project directories you need.\n\nWhen creating project-specific VMs, you can customize the mounted directories by editing the instance configuration. Either adjust the mounts section before starting the VM (by not using the -y flag), or edit ~/.lima/<vm_name>/lima.yaml after creation and restart the instance.\n\nThis approach also makes it easier to share VM configurations with team members. Instead of sharing entire disk images, you can distribute just the Lima template YAML file, which team members can use to spin up identical environments on their machines.\n\nFor automated setup, Lima supports provisioning scripts that run during VM creation. For more complex setups, consider using idempotent provisioning tools like Ansible to ensure consistent environments across your team.\n\nIf you find yourself repeatedly creating VMs with similar configurations, consider creating custom Lima templates. Templates are YAML files that define VM settings, and they can include other templates.\n\nCustom templates are useful for:\n\nYou can create a custom template by copying and modifying an existing one from Lima’s template directory. Save your custom templates in ~/.lima/_templates/ and reference them when creating new VMs:\n\nSee the Lima templates documentation \n\nHere are some important security best practices to follow when using VMs for development:\n\nRemember: The whole point of using VMs is isolation. When in doubt, create a new VM for risky experiments and delete it afterwards.\n\nI hope this guide helps you get started quickly and right-footed!\nAs always, please leave feedback, questions and ideas in the comments below.\n\nSpecial thanks to Ilya Lukyanov and Overflo for reviewing drafts of this post and making great suggestions. 🙏",
    "readingTime": 12,
    "keywords": [
      "yolo mode",
      "yaml file",
      "anthropic_api_key environment",
      "remote-ssh session",
      "claude extension",
      "ssh configuration",
      "authentication flow",
      "mise mise-en-place",
      "mise-en-place stars",
      "command-line interface"
    ],
    "qualityScore": 1,
    "link": "https://www.metachris.dev/2025/11/sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima/",
    "thumbnail_url": "https://www.metachris.dev/images/posts/ai-sandbox/cover.jpg",
    "created_at": "2026-01-21T00:59:26.211Z",
    "topic": "tech"
  },
  {
    "slug": "takatime-selfhosted-wakatime-alternative-go-and-mongodb",
    "title": "TakaTime – Self-Hosted WakaTime Alternative (Go and MongoDB)",
    "description": "TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim.  It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity...",
    "fullText": "Rtarun3606k\n\n /\n\n TakaTime\n\n Public\n\n TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim. It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity to a third-party server, TakaTime stores everything in your own MongoDB database.\n\n License\n\n MIT license\n\n 10\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Rtarun3606k/TakaTime",
    "readingTime": 1,
    "keywords": [
      "coding",
      "activity",
      "takatime",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Rtarun3606k/TakaTime",
    "thumbnail_url": "https://opengraph.githubassets.com/4a0f7ea58066e011c6d164cc23d1af692c671a17cadd5889743e52a2451b2406/Rtarun3606k/TakaTime",
    "created_at": "2026-01-20T12:27:00.258Z",
    "topic": "tech"
  },
  {
    "slug": "metacompilation",
    "title": "Metacompilation",
    "description": "A post that is going to be part of my sequence on rethinking programming languages. …",
    "fullText": "A post that is going to be part of my sequence on rethinking programming languages.\n\nA compiler is a piece of machine code , that takes as input a text string describing a program  and returns the compiled machine code\n\nLet  be a function that takes in a machine code program and returns another potentially faster or smaller program.\n\nA metacompliler has the formula\n\nTo understand how this works, first let's look at a less self referential case. Let  be a regular compiler.\n\nis just a string. Maybe it's \"print(1+2)\"\n\nis a machine code program. This program, if run, would first compile  into machine code, and then would run that machine code. Therefore it is a machine code program that does the same thing as . It has a fairly significant size for even a small program, as it contains a complete copy of the compiler.\n\nWhat does  do? It optimizes that machine code. The first thing it can do is cut out big chunks of the compiler. At least in simple cases. If the code is running arbitrary eval statements, all the compiler might be needed. In the case of this simple program, the parts of the compiler that handle floats, loops etc are just not used. If the optimizer is good, it could simplify the code all the way down to . Some programming languages (see zig) already run code at compile time. The difference between compile and run time is just in what variables you currently know the value of, and the relative cost of compute.\n\nFor code with a finite runtime that just runs by itself, not interacting with the outside world, it can all, in principle be simplified down to a single print statement. In practice computer programs interact with the \"outside world\" in all sorts of ways. In some contexts, writes to disk or sending data to a GPU might be considered interactions with an external world. But for simplicity, assume the only form of interaction is input() and print()\n\nSo that's what a metacompiler does. But does it actually do anything. The most naive metacompiler implimentation has  .  When we call  we get the program. And when we proceed to run that program, that program first calls  to generate the machine code  and then runs that machine code. This leads to an infinite regress. We haven't actually used  anywhere. What we essentially have is just.\n\nA program that is clearly an infinite loop, with no actual relation to pi.\n\nSo we need the optimization step of the metacompiler to be doing something non-trivial to make the code halt in a finite time at all.\n\nLets define a small toy programming language, so we can talk about how to compile it.\n\nWe will give our programming language one data type, arbitrary size integers.\n\nWe will allow definitions, inputs, calculations and loops.\n\nThis example program shows all the features of this programming language. It is rather minimal.\n\nThe only free parameter in the metacompiler (as above) is in the choice of\n\nFor clarity, machine code instructions will look the same as programming language instructions, except the machine code will be in BOLD\n\nThe program consists of a number of definitions,  (of the format [name]=[number], looking like  ) followed by the first non-definition statement. If the same name is used multiple times, only the last definition is needed. Ie the code  can be optimized to\n\nSuppose the optimizer takes in  code where the first non-definition in  happens to be a calculation. For example.  this can get optimized into\n\nNow suppose the first non-definition in  is an .  For example.  This can be converted into.\n\nThe way to think about this is that, if  were a normal compiler, the  function would convert a machine code program containing  into another machine code program that still contains  but that makes  do slightly less work.\n\nwhile the similar  can simplify down to\n\nThis gives a functioning toy example of a metacompiler. The above simplification rules are used in the definition of , which is in turn used in the definition of .\n\nThis produces code that, while excessively self referential, runs and produces output in a finite time, at least assuming the output of a regular compiler would run in finite time on the program.\n\nNote that  only does 1 simplification step, and is only run once at compile time.\n\nSuppose we insisted that, before  is allowed to simplify a piece of machine code, it must first prove that it's simplification won't change the result. This can be proved, by lob's theorem. However it isn't sufficient to make the metacompiler actually valid. Lob's theorem just says that ZFC approves of infinite buck passing. At some point we need to actually understand our programming language.\n\nIf however we make  prove that  is equivalent to  before  is allowed to output . Then that is sufficient. Your directly proving that your meta-compiler is doing the same thing as a regular compiler, which gives you a ground truth about the meaning of the programming language.\n\nWhile the example meta-compiler given above isn't particularly fast, the toy example shows that metacompilers can exist. And the space of meta-compilers seems like it should contain all sorts of interesting optimizations.\n\nFor example. I was doing some programming involving numerically integrating systems of Stochastic Differential Equations (SDE's). Basically, I choose various settings and then run a tight loop involving those settings. And ideally I would like the speed of special purpose compiled code within the tight loop, without having the overhead of a full compilation from source every time I change a setting.\n\nSo, what I would ideally want is a program that contains precompiled snippets of code. Once the particular values of the settings are known, a highly optimized machine code program could be put together by little more than pasting together the relevant blocks of machine code to make a complete machine code program.\n\nAnd I'm wanting a way to make the programming language do this, or other clever things like this, automagically.\n\nAnother clever thing. Suppose your program contains  of arbitrary user generated strings. But you know this is only a small fraction of runtime. And the user isn't allowed to use various language features. You might want to make a cut down minified version of the full language compiler, something with the unused features cut out, and some of the optimization tricks removed.\n\nThe hope is to totally blur the line between compile time and runtime, with code that can rewrite itself on the fly in all sorts of clever and highly performant ways.",
    "readingTime": 6,
    "keywords": [
      "self referential",
      "lob's theorem",
      "tight loop",
      "programming languages",
      "regular compiler",
      "programming language",
      "machine code",
      "code program",
      "metacompiler",
      "contains"
    ],
    "qualityScore": 1,
    "link": "https://www.lesswrong.com/posts/6BSZkkWNGMTdRi5Ly/metacompilation",
    "thumbnail_url": "https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg",
    "created_at": "2026-01-20T12:26:57.143Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here’s how the three models performed in our tests:\n\n💡 If you want the safest pick for real “ship a feature in a big repo” work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you’re okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n⚠️ NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere’s the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe’ll compare code quality, token usage, cost, and time to complete the build.\n\n💡 NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\n💡 NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It’s not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that’s the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that’s a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it’s hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. 🤷‍♂️\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don’t think we’re anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don’t fully back them up. 🥴\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won’t hold up long term.\n\nI don't see any of these recent models as “use it and ship it” for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "ygrep-fast-local-indexed-code-search-tool-optimized-for-ai-coding-assistants",
    "title": "Ygrep: Fast, local, indexed code search tool optimized for AI coding assistants",
    "description": "A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing. - yetidevworks/ygrep",
    "fullText": "yetidevworks\n\n /\n\n ygrep\n\n Public\n\n A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing.\n\n License\n\n MIT license\n\n 14\n stars\n\n 2\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n yetidevworks/ygrep",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/yetidevworks/ygrep",
    "thumbnail_url": "https://opengraph.githubassets.com/eda1bb1f12626e76b9793d44f791d0ed3332b6c28dc7896d1831d00d7c49258d/yetidevworks/ygrep",
    "created_at": "2026-01-20T00:57:31.412Z",
    "topic": "tech"
  },
  {
    "slug": "scaling-longrunning-autonomous-coding",
    "title": "Scaling long-running autonomous coding",
    "description": "Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from …",
    "fullText": "Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents:\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nThey ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.\n\nIn my predictions for 2026 the other day I said that by 2029:\n\nI think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier.\n\nI may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nBut how well did they do? Their initial announcement a couple of days ago was met with unsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.\n\nIt looks like they addressed that within the past 24 hours. The latest README includes build instructions which I followed on macOS like this:\n\nThis got me a working browser window! Here are screenshots I took of google.com and my own website:\n\nHonestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.\n\nThe FastRender repo even uses Git submodules to include various WhatWG and CSS-WG specifications in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.\n\nThis is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was HiWave browser, a new browser engine in Rust first announced in this Reddit thread.\n\nWhen I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.",
    "readingTime": 3,
    "keywords": [
      "autonomous coding",
      "web browser",
      "agents",
      "repo",
      "cursor",
      "project",
      "ended",
      "tasks",
      "agent",
      "mostly"
    ],
    "qualityScore": 1,
    "link": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/",
    "thumbnail_url": "https://static.simonwillison.net/static/2026/cursor-social-card.jpg",
    "created_at": "2026-01-20T00:57:30.632Z",
    "topic": "tech"
  },
  {
    "slug": "valve-updates-ai-disclosure-guidelines-to-allow-for-aipowered-tools",
    "title": "Valve Updates AI Disclosure Guidelines To Allow For AI-Powered Tools",
    "description": "Valve has made changes to its AI-disclosure guidelines, removing the need for studios to disclose whether or not games have been developed with AI-powered tools and putting more emphasis on AI-generated assets.\nThe change, which was pointed out by Simon Carless on LinkedIn, suggests that Valve is no longer concerned by the use of AI tools that assist development, stating, \"Efficiency gains through the use of [AI-powered dev tools] is not the focus of this section.\" These tools could included a variety of things, such as AI-generated transcripts of meetings to code helpers that have become prevalent in most programming environments.\nValve states the the aim of its disclosure policy is to inform players when AI is used to generate content, from marketing and conceptual assets to in-game ones that players will interact with. Developers are able to specify what assets have been generated and indicate, via a single checkbox, whether or not players will interact with AI-generated content during gameplay, be it images, audio, or other content.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/valve-updates-ai-disclosure-guidelines-to-allow-for-ai-powered-tools/1100-6537483/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1585/15853545/4637028-7297126222-arc-r.jpg",
    "created_at": "2026-01-19T18:18:34.839Z",
    "topic": "gaming"
  },
  {
    "slug": "a-meta-product-manager-with-no-technical-background-says-vibe-coding-gave-him-superpowers",
    "title": "A Meta product manager with no technical background says vibe coding gave him 'superpowers'",
    "description": "A Meta product manager says vibe coding is letting non-technical PMs ship features and work differently with engineers.",
    "fullText": "A product manager at Meta says vibe coding has changed what it means to do his job — even though he has no technical background and still finds code \"terrifying.\"\n\nZevi Arnovitz said in an episode of \"Lenny's Podcast\" released Sunday that discovering AI coding tools in mid-2024 marked a turning point in his career.\n\nIt felt like he was handed \"superpowers,\" Arnovitz said.\n\nUnderstanding how to use AI intentionally is \"one of the biggest game changers that will make you much better as a PM,\" he said, referring to product management.\n\nArnovitz joined Meta in September last year after about three years as a product manager at website-building company Wix, according to his LinkedIn profile.\n\nArnovitz said he has rebuilt his workflow around AI. He uses vibe coding tools like Cursor alongside models from Anthropic and Google to explore product ideas, generate build plans, execute code, review it, and update documentation.\n\nThe shift reshaped his role as a product manager. Instead of merely acting as a coordinator between engineering and design, Arnovitz operates more like a product owner with the capability to execute.\n\n\"Everyone's going to become a builder,\" he said. \"We're going to see that a lot in the next coming years.\"\n\nStill, Arnovitz said there are limits to what non-technical product managers should take on. He said he doesn't think product managers should be shipping complex infrastructure changes or big projects.\n\nAI has enabled product managers to take on smaller UI projects by building the feature and then handing the code to a developer for final review and completion, he added.\n\nAs AI tools improve, Arnovitz said titles and responsibilities are likely to \"collapse,\" and product managers should treat vibe coding as a \"collaborative learning opportunity\" with their engineering teams.\n\nThe rise of AI coding tools is blurring the lines for traditional roles, making it easier for non-technical workers, including product managers, to build products directly.\n\nFigma CEO Dylan Field said in October on \"Lenny's Podcast\" that AI has pushed many workers to experiment with building products.\n\nTasks that once required deep engineering expertise can now be done with vibe coding tools, he said.\n\n\"I think that we're seeing more designers, engineers, product managers, researchers, all these different folks that are involved in the product development process dip their toe into the other roles,\" he said.\n\n\"We're all product builders, and some of us are specialized in our particular area,\" he added.\n\nThat same thinking is showing up in how companies train new hires. LinkedIn replaced its long-running associate product manager program with an associate product builder track in January.\n\n\"We're going to teach them how to code, design, and PM at LinkedIn,\" said the company's former chief product officer, Tomer Cohen, in an episode of \"Lenny's Podcast\" published in December. It's more about training people \"who can flex across,\" he added.\n\nCohen, who spent nearly 14 years at LinkedIn, left the company in January and now works as an advisor, according to his LinkedIn profile.",
    "readingTime": 3,
    "keywords": [
      "linkedin profile",
      "vibe coding",
      "coding tools",
      "product manager",
      "product managers",
      "associate product",
      "lenny's podcast",
      "code",
      "engineering",
      "arnovitz"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-product-manager-vibe-coding-superpowers-non-technical-builder-2026-1",
    "thumbnail_url": "https://i.insider.com/696dbfd0c58df2ecd5ccc045?width=1200&format=jpeg",
    "created_at": "2026-01-19T12:27:05.647Z",
    "topic": "finance"
  },
  {
    "slug": "figmalike-canvas-for-running-claude-code-agents",
    "title": "Figma-like Canvas for running Claude Code agents",
    "description": "Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf) - AgentOrchestrator/AgentBase",
    "fullText": "AgentOrchestrator\n\n /\n\n AgentBase\n\n Public\n\n Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf)\n\n License\n\n View license\n\n 85\n stars\n\n 6\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n AgentOrchestrator/AgentBase",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/AgentOrchestrator/AgentBase",
    "thumbnail_url": "https://opengraph.githubassets.com/ccefb572879426a6706c75e212582de106cce0540f056c7b6f252924f0fba3c1/AgentOrchestrator/AgentBase",
    "created_at": "2026-01-18T18:15:51.872Z",
    "topic": "tech"
  },
  {
    "slug": "building-a-tui-to-index-and-search-my-coding-agent-sessions",
    "title": "Building a TUI to index and search my coding agent sessions",
    "description": "Building a TUI to index and search coding agent sessions across Claude Code, Codex, and others, with Tantivy for fuzzy full-text search.",
    "fullText": "This is the story of how fast-resume came to life and evolved, as I was trying to search and resume my coding agent sessions more easily across different local CLI agents.\n\nI use many coding agents these days: Claude Code, Codex, OpenCode, Copilot, and more. Sometimes I remember that I, or the agent, mentioned something specific in a previous session, and I want to go back to it.\n\nMost coding agents have a /resume feature now, which allows a session to be reopened with all the state back. While the resume feature works great, finding which session to resume is harder.\n\nThat means that for example if I remember the agent mentioning a specific subject later during the conversation, it won’t be in the title, so I can’t find it.\n\nLet’s say I have a few sessions about building a TUI program. I remember that in one of the sessions, the agent mentioned textual. I can’t search for textual in the resume view! Also, if I don’t remember the folder and which agent I used, I’m screwed. And some agents don’t have that feature at all.\n\nSo I started ripgrep‘ing my home folder to find the string I was searching for, then using clues from the session file (directory, timestamp, context) to navigate to the correct directory, /resume, and find the session in question. 😅\n\nSince most coding agents store sessions locally, I started thinking: what if I could automate this grep‘ing, wrap it in a nice TUI and be able to resume in one keypress?\n\nFirst, to see if this was feasible, I had to understand how sessions are actually stored. Most agents use JSON files, but there are some interesting differences.\n\nMost agents follow the same pattern as Claude Code: Codex and Copilot CLI use JSONL with similar structures.\n\nFiles are stored in ~/.claude/projects/{project_id}/{session_id}.jsonl. JSONL is a format where each JSON object is stored independently on a newline.\n\nMessages from the user or Claude and tool calls are stored that way. Here is an example of a message:\n\nOpenCode doesn’t use JSONL but instead independent JSON files. Message content is sharded by session id, message id, and message parts in ~/.local/share/opencode/storage/:\n\nThis design conceptually makes sense: not having to rewrite or append to a single file might be simpler. But for indexing, it means a lot more filesystem operations. To give you an idea: I used Claude Code possibly 100x more than OpenCode, yet OpenCode has 10x more files (9,847 vs 827). See Stats \n\nVibe stores one JSON file per session in ~/.vibe/logs/session/session_*.json. It is not JSONL. The file contains metadata and the full messages array.\n\nOne detail that surprised me: Vibe rewrites the entire file after each user turn. That means the file grows and gets fully serialized on every message, which is simple but doesn’t seem very efficient for long sessions.\n\nCrush is the only agent that uses SQLite instead of JSON files. Projects are listed in ~/.local/share/crush/projects.json, and each project has its own .crush/crush.db database.\n\nThe schema has a sessions table with metadata like title, message count, and cost, and a messages table with role and parts (stored as JSON).\n\nI’m surprised it’s the only agent using SQLite!\n\nTo search sessions, I started with a naive approach. I defined a common Session type and an adapter protocol to abstract each agent’s storage format:\n\nEach adapter implements three methods: find_sessions parses all session files and returns Session objects, get_resume_command returns the shell command to resume a session (claude --resume {id} for Claude, codex resume {id} for Codex), and is_available checks if the agent’s data directory exists.\n\nFor example, here’s the core of Claude’s adapter:\n\nAdding a new agent means writing one adapter file. Implement scanning, parsing, and the resume command. The search engine, TUI, and CLI all work automatically.\n\nOn startup, each adapter would parse its session files and return a list of Session objects. I cached the results in a sessions.json file and used file mtimes to know when to reindex.\n\nFor search, I used RapidFuzz because the experience I had in mind was the familiar fuzzy finding of fzf. For each session, I built a searchable string by concatenating the title, directory, and full content:\n\nRapidFuzz’s Weighted Ratio scorer compared the query against every searchable string. This scorer has an interesting backstory but it basically uses other scorers based on the lengths of the string.\n\nThe problem was that WRatio alone didn’t rank exact matches high enough. Searching for “fix auth bug” might rank “authentication fixes” higher than a session literally titled “fix auth bug”. I added bonuses on top of the fuzzy score: +25 if the query appears as a substring, +15 if all query words are present, and +30 if they appear consecutively. This helped with ranking quality, but the performance was not good enough for me. Every search scanned every session on every keystroke. The TUI would visibly lag while typing, and I’m trying to have a very reactive TUI.\n\nI needed a proper search engine. I first considered SQLite FTS5, which has a trigram tokenizer for similarity matching, but it works by comparing 3-character substring overlap rather than edit distance, which is what I’m looking for. I’m a very imprecise typer 😄\n\nI opted for Tantivy, an in-process full-text search library written in Rust, and the one powering Quickwit. Instead of comparing the query against every document at search time, we can use it to build an inverted index upfront: a mapping from terms to the sessions that contain them.\n\nTantivy’s FuzzyTermQuery uses Levenshtein distance, which is better for actual typos: “teh” matches “the” (distance=1), but wouldn’t match with trigrams since they share no 3-character chunks.\n\nWhen a session gets indexed, Tantivy tokenizes its content into terms and stores which document IDs contain each term. Searching for “auth bug” means finding documents containing “auth”, finding documents containing “bug”, intersecting the sets, then scoring the matches using BM25.\n\nLuckily, the only “official” bindings for Tantivy are for Python! So I was able to use it directly and very easily in my project.\n\nThe schema defines what gets indexed:\n\nText fields get tokenized and indexed for search. The raw tokenizer keeps the value as-is without splitting, which is useful for IDs and agent names where “copilot-cli” should stay as one token, not become “copilot” and “cli”. (cf Keyword query syntax)\n\nWhen the schema changes (adding a field, changing tokenizers), the index needs to be rebuilt. I track a schema version in a file alongside the index and clear everything if it doesn’t match:\n\nIt’s not very robust (I could bump the version to the same number into two concurrent PRs), but it’s good enough for now.\n\nFor fuzzy matching, Tantivy supports custom distance for queries. A fuzzy term query with distance 1 matches terms that are one character insertion, deletion, or substitution away from the query term. “atuh” matches “auth”, “bugg” matches “bug”.\n\nThe prefix=True flag also matches terms that start with the query, so “au” matches “auth” and “authentication”.\n\nI ran into the same ranking problem as with RapidFuzz: fuzzy matches sometimes outranked exact matches. The fix was a hybrid query that boosts exact matches:\n\nThe performance has been quite good with Tantivy. My use case is pretty basic and the dataset is very small in FTS terms, so I haven’t looked into performance optimization too much. But queries complete in a handful of milliseconds, which is perfect!\n\nThe first version of fast-resume rebuilt the entire index when any source directory changed. Adding one new Claude session meant re-parsing hundreds of Codex sessions that hadn’t changed.\n\nThe fix was tracking modification times per session. Tantivy stores each session’s mtime alongside its content:\n\nOn startup, fast-resume asks the index for all known sessions and their mtimes. Each adapter compares file mtimes against what’s known and only re-parses what changed or is new:\n\nIf a session’s mtime is newer than what’s in the index, re-parse it. If a session exists in the index but not on disk, mark it deleted. Everything else stays untouched.\n\nUpdates are atomic: delete the old documents and add the new ones in a single transaction before committing. This avoids a window where the session is missing from the index:\n\nIf nothing changed (the common case) the whole process is just reading mtimes and comparing numbers. In any case, this happens in the background while the TUI starts instantly (see streaming updates).\n\nMost adapters spend their time parsing JSON. Claude sessions are JSONL files with hundreds of lines. OpenCode has thousands of small JSON files spread across directories. Even with incremental indexing, the initial index build parses everything.\n\nTo try to gain a bit for performance, I switched the native json lib for orjson, which is a JSON library written in Rust that’s supposed to be a lot faster.\n\norjson’s loads also accept both strings and bytes, and it’s faster with bytes, so we can pass it the file directly in binary mode without converting to a string first.\n\nThe TUI is built with Textual, a Python framework for terminal interfaces. I discovered it with Mistral’s vibe coding agent. This and uv are the reason I wanted this project to be Python, even though I usually pick Go for CLIs.\n\nTextual provides a layout system, widgets, reactive state, and async workers, great to have a fully featured and snappy TUI.\n\nThe main screen has three parts: a search input at the top, a results table in the middle, and a preview pane at the bottom. Everything is reactive; changing state automatically updates the UI, which is a pattern I like and I’m used to with web frameworks.\n\nOn startup, results load instantly from the existing index. In parallel, all adapters compare file mtimes to find new or modified sessions and index them in the background. Each time an adapter finishes, the results table refreshes to include the newly indexed sessions. On first run or after a schema version bump, the index is empty so results populate progressively as adapters complete.\n\nThe TUI runs this off the main thread using Textual’s @work decorator. Each time an adapter finishes indexing, on_progress re-runs the current search query against the updated index, so newly indexed sessions that match appear immediately:\n\ncall_from_thread marshals updates back to the main thread for UI changes.\n\nSearch is debounced to improve responsiveness when holding delete for example, otherwise the TUI doesn’t have enough time to re-render after the search and it feels laggy.\n\nThe watch_search_query method is a Textual watcher: it gets called automatically when search_query changes. Setting the reactive variable triggers the search.\n\nSearch also runs in a background thread so the UI stays responsive while Tantivy works:\n\nThe query time gets displayed next to the search box, it’s surprisingly variable, from ~0.5ms to ~50ms on my laptop. But it feels pretty snappy!\n\nNavigation works with up/down, but also j and k. shift+tab to move from search to preview, / to focus back the search bar, return resumes the selected session. Scrolling also works with the mouse. You can resize the preview with + and - or hide it entirely with Ctrl+backtick.\n\nSearch terms are highlighted in the results table (title, directory) in fzf style and the preview pane using Rich’s Text.stylize(). One limitation: Tantivy returns matching documents but doesn’t expose which terms actually matched. So if you search “atuh” and it fuzzy-matches “auth”, only “atuh” gets highlighted, not “auth”. I couldn’t find a way to get the expanded terms from Tantivy.\n\nSince modern terminals support inline images through protocols like Sixel, I thought we could include coding agent logos to make it look nicer. The textual-image library handles terminal detection and rendering. Unfortunately, it doesn’t work with vhs, so I have to record demos manually!\n\nSession timestamps are colored based on age: green for recent, fading through yellow and orange to gray for old. Exponential decay maps time to a 0-1 value, which compresses older sessions together:\n\nThen t interpolates through color stops (green → yellow → orange → gray). A session from an hour ago looks noticeably different from one from yesterday, but three months and six months both just look “old”.\n\nPlain text search works fine for most queries, but sometimes you want to narrow results by agent or time. Rather than building a separate filter UI, I added keyword syntax directly in the search box. Type agent:claude to filter to Claude sessions, date:today for today’s sessions, dir:my-project to match directory paths.\n\nTextual’s Suggester provides autocomplete as you type: agent:cl suggests claude, date:to suggests today. It also handles negation, so agent:!co suggests !codex.\n\nThe parser extracts keywords from the query using a regex, handling keyword:value pairs, quoted values with spaces like dir:\"my project\", and negation with - or !. Whatever doesn’t match a keyword becomes free-text that goes to Tantivy.\n\nAgent and directory filters support multiple values: agent:claude,codex matches either agent, agent:claude,!codex means Claude but not Codex. Date filters have their own mini-language: date:today, date:yesterday, date:<1h (within the last hour), date:>2d (older than two days).\n\nThese parsed filters translate to Tantivy queries:\n\nWhen you press Enter on a session, the TUI doesn’t directly exec the resume command. Instead it stores the command and directory, exits cleanly, and returns them to the CLI wrapper:\n\nThe CLI then uses os.execvp to replace itself with the agent’s resume command:\n\nexecvp replaces the current process entirely: same PID, same terminal, but now running Claude or Codex instead of fast-resume. This is cleaner than spawning a child process because the resumed agent owns the terminal directly. Ctrl+C goes to the agent, not to a wrapper script.\n\nThe directory change happens first because most agents expect to be run from the project directory.\n\nSome agents support “yolo mode” to automatically approve edits and tool calls. Claude has --dangerously-skip-permissions for example. But it applies to the current instance of claude, not the session. So starting claude without this flag, you can’t resume a past session in yolo mode, even if that session was started in an instance of claude started with the flag.\nWhen you resume a session, fast-resume can detect if it was originally started in yolo mode and offer to resume the same way.\n\nAdapters that parse session files look for yolo indicators. Codex stores approval policy in a turn_context record:\n\nVibe stores it directly in session metadata:\n\nThe yolo flag gets indexed alongside each session. When you resume, the TUI checks in order: fast-resume’s --yolo flag overrides everything, then stored session yolo state, then if the adapter supports yolo but we don’t know the session’s state, a modal asks the user.\n\nSince we’re indexing all sessions across agents, we get analytics as a bonus. fr --stats gives you a breakdown of your session history:\n\nThis was a fun project! It was a good occasion to try a new framework for TUIs and use an in-process search engine to keep things snappy. I’m pretty happy with the result!\n\nI published it to PyPI, so you can try or install it with uv:",
    "readingTime": 13,
    "keywords": [
      "vibe stores",
      "code codex",
      "json files",
      "session’s mtime",
      "documents containing",
      "preview pane",
      "searchable string",
      "newly indexed",
      "tui doesn’t",
      "adapter finishes"
    ],
    "qualityScore": 1,
    "link": "https://stanislas.blog/2026/01/tui-index-search-coding-agent-sessions/",
    "thumbnail_url": "https://stanislas.blog/2026/01/tui-index-search-coding-agent-sessions/fast-resume.png",
    "created_at": "2026-01-18T12:21:36.003Z",
    "topic": "tech"
  },
  {
    "slug": "a-new-way-to-call-c-from-java-how-fast-is-it",
    "title": "A new way to call C from Java: how fast is it?",
    "description": "Irrespective of your programming language of choice, calling C functions is often a necessity. For the longest time, the only standard way to call C was the Java Native Interface (JNI). But it was so painful that few dared to do it. I have heard it said that it was deliberately painful so that people … Continue reading A new way to call C from Java: how fast is it?",
    "fullText": "Irrespective of your programming language of choice, calling C functions is often a necessity. For the longest time, the only standard way to call C was the Java Native Interface (JNI). But it was so painful that few dared to do it. I have heard it said that it was deliberately painful so that people would be enticed to use pure Java as much as possible.\n\nSince Java 22, there is a new approach called the Foreign Function & Memory API in java.lang.foreign. Let me go through step by step.\n\nYou need a Linker and a SymbolLookup instance from which you will build a MethodHandle that will capture the native function you want to call.\n\nTo load the SymbolLookup instance for your library (called mylibrary), you may do so as follows:\n\nThe native library file should be on your java.library.path path, or somewhere on the default library paths. (You can pass it to your java executable as -Djava.library.path=something).\n\nAlternatively, you can use SymbolLookup.libraryLookup or other means of loading\n\nthe library, but System.loadLibrary should work well enough.\n\nYou have the lookup, you can grab the address of a function like so:\n\nThis returns an Optional<MemorySegment>. You can grab the MemorySegment like so:\n\nOnce you have your MemorySegment, you can pass it to your linker to get a MethodHandle which is close to a callable function:\n\nThe functiondescr must describe the returned value and the function parameters that your function takes.\n\nIf you pass a pointer and get back a long value, you might proceed as follows:\n\nThat is, the first parameter is the returned value.\n\nFor function returning nothing, you use FunctionDescriptor.ofVoid.\n\nThe MethodHandle can be called almost like a normal Java function:\n\nmyfunc.invokeExact(parameters). It always returns an Object which means that if it should return a long, it will return a Long. So a cast might be necessary.\n\nYou can allocate C data structures from Java that you can pass to your native code by using an Arena. Let us say that you want to create an instance like\n\nYou could do it in this manner:\n\nYou can then pass myseg as a pointer to a data structure in C.\n\nYou often get an array with a try clause like so:\n\nThere are many types of arenas: confined, global, automatic, shared. The confined arenas are accessible from a single thread. A shared or global arena is accessible from several threads. The global and automatic arenas are managed by the Java garbage collector whereas the confined and shared arenas are managed explicitly, with a specific lifetime.\n\nSo, it is fairly complicated but manageable. Is it fast? To find out, I call from Java a C library I wrote with support for binary fuse filters. They are a fast alternative to Bloom filters.\n\nYou don’t need to know what any of this means, however. Keep in mind that I wrote a Java library called jfusebin which calls a C library. Then I also have a pure Java implementation and I can compare the speed.\n\nI should first point out that even if calling the C function did not include any overhead, it might still be slower because the Java compiler is unlikely to inline a native function. However, if you have a pure Java function, and it is relatively small, it can get inlined and you get all sorts of nice optimizations like constant folding and so forth.\n\nThus I can overestimate the cost of the overhead. But that’s ok. I just want a ballpark measure.\n\nIn my benchmark, I check for the presence of a key in a set. I have one million keys in the filter. I can ask whether a key is not present in the filter.\n\nI find that the library calling C can issue 44 million calls per second using the 8-bit binary fuse filter. I reach about 400 million calls per second using the pure Java implementation.\n\nThus I measure an overhead of about 20 ns per C function calls from Java using a macBook (M4 processor).\n\nObviously, in my case, because the Java library is so fast, the 20 ns becomes too much. But it is otherwise a reasonable overhead.\n\nDaniel Lemire, \"A new way to call C from Java: how fast is it?,\" in Daniel Lemire's blog, January 17, 2026, https://lemire.me/blog/2026/01/17/a-new-way-to-call-c-from-java-how-fast-is-it/.\r\n [BibTeX]",
    "readingTime": 4,
    "keywords": [
      "symbollookup instance",
      "binary fuse",
      "pure java",
      "java implementation",
      "per second",
      "java library",
      "native function",
      "arenas",
      "fast",
      "overhead"
    ],
    "qualityScore": 1,
    "link": "https://lemire.me/blog/2026/01/17/a-new-way-to-call-c-from-java-how-fast-is-it/",
    "thumbnail_url": "https://lemire.me/blog/wp-content/uploads/2026/01/Capture-decran-le-2026-01-17-a-18.44.19-1024x725.png",
    "created_at": "2026-01-18T01:03:01.219Z",
    "topic": "tech"
  },
  {
    "slug": "ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it",
    "title": "AI Generated Code Isn't Cheating: OSS Needs to Talk About It",
    "description": "Remember early 2025? \"Vibe coding\" was a meme and seemed mostly a tool for casual builders or those new to coding. It's now 2026, and we find ourselves living in a new reality. Industry leaders like DHH, Karpathy, and Lutke are publicly embracing AI-generated code controlled by human prompting.",
    "fullText": "Remember early 2025? “Vibe coding” was a meme and seemed mostly a tool for casual builders or those new to coding. It was often used disparagingly, or to imply a lack of deep technical expertise. Some very cool basic applications were being built, but AI coding assistants couldn’t reliably function in complex codebases. But what a difference a year has made!\n\nIt’s now 2026, and we find ourselves living in a new reality. Some of the most influential voices in software engineering like DHH (Ruby on Rails), Andrej Karpathy (prev OpenAI, Tesla), Tobi Lutke (Shopify), Salvatore Sanfilippo (Redis), and Mitchell Hashimoto (Ghostty, prev Hashicorp) are publicly embracing a new  paradigm: completely AI generated code controlled by human-in-the-loop prompting. It was also recently publicized that Linus Torvalds (creator of Linux and Git) is leveraging AI vibe-coding in his side-projects.\n\nAI is everywhere: if you’re a software developer, you’ve almost certainly tried at least one AI-assisted coding solution over the past year. It’s a safe assumption that a large portion of developers are using AI to help them, but we still know shockingly little about how their code was derived. This secrecy is outdated, especially now that the practice is being normalized by industry leaders.\n\nThe open source community is built on top of foundations of transparency and collaboration, of which knowledge sharing is a key component. At Mozilla.ai, we believe we must embrace and encourage the disclosure of AI usage as quickly as possible. We need to move away from “Should we AI?” and towards a structure that clearly defines our expectations for where we encourage AI usage and how we document it.\n\nIn our project any-llm, we’ve started to iterate on this philosophy by creating a pull request template that requests a few pieces of information whenever a PR is submitted.\n\nHere’s a snippet of the relevant part of our pull request template:\n\nFirst, we request that the contributors specify their level of AI usage: was AI used to draft and make edits? Or was their contribution completely AI-generated with them only directing it via plain language prompts? Both are acceptable, but it helps a reviewer understand how to approach their review. If we know the code is completely AI generated, we can be candid with our feedback and direct the contributor towards improving their prompting or AI coding configuration to improve quality. Without this transparency, it can be difficult to give feedback since a reviewer doesn’t want to offend the contributor by insinuating that their work came from a bot.\n\nSecond, we request information about the contributors' AI setup: what model(s) and IDE/CLI tools were used? This is valuable metadata for crowdsourcing best practices. Maybe there is one model or tool that works amazingly well with a certain codebase or language! Openly sharing this information allows all of us to learn from each other.\n\nLastly, we request that any responses to comments come from the contributor themselves and not their AI tool. It is frustrating to write comments without knowing if a human is on the other side reading and responding to the feedback. The open source community is a wonderful place to learn from each other, and that learning happens best when humans talk to humans. Of course, AI can be used to help the contributor brainstorm or improve their grammar, but we think the core discussion should still happen between two humans.\n\nWe welcome community opinions and hope to see similar approaches be adopted across the open source community. Let's keep learning and developing together!",
    "readingTime": 3,
    "keywords": [
      "request template",
      "coding",
      "community",
      "contributor",
      "tool",
      "completely",
      "code",
      "usage",
      "feedback",
      "humans"
    ],
    "qualityScore": 1,
    "link": "https://blog.mozilla.ai/ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it/",
    "thumbnail_url": "https://blog.mozilla.ai/content/images/size/w1200/2026/01/George-Sturdy-and-Solomon-Young-s-vehicle-of-amusement.jpg",
    "created_at": "2026-01-16T18:18:59.372Z",
    "topic": "tech"
  },
  {
    "slug": "building-an-agentic-memory-system-for-github-copilot",
    "title": "Building an agentic memory system for GitHub Copilot",
    "description": "Copilot’s cross-agent memory system lets agents learn and improve across your development workflow, starting with coding agent and code review.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://github.blog/ai-and-ml/github-copilot/building-an-agentic-memory-system-for-github-copilot/",
    "thumbnail_url": "https://github.blog/wp-content/uploads/2025/12/memory.jpg",
    "created_at": "2026-01-16T12:24:06.259Z",
    "topic": "tech"
  },
  {
    "slug": "task-versus-purpose-nvidia-ceo-jensen-huang-explains-why-ai-wont-kill-jobs",
    "title": "'Task' versus 'purpose': Nvidia CEO Jensen Huang explains why AI won't kill jobs.",
    "description": "AI may automate tasks, but your job's purpose may be immune from AI disruption. This applies to radiology, law, coding, and even waiting tables.",
    "fullText": "Hospitals, law firms, and tech companies are getting a preview of how AI is likely to reshape work: by automating tasks without eliminating the underlying jobs.\n\nThat's the core message Nvidia CEO Jensen Huang emphasized in a recent appearance on the No Priors podcast.\n\nIn a wide-ranging interview, he argued that fears of mass job destruction often confuse the \"tasks\" involved in a job with the broader \"purpose\" of the role. AI, in his view, changes how tasks get done, but the purpose remains the same. And that means, the technology probably won't destroy jobs and could even increase demand for the people responsible for outcomes at work.\n\nHuang's framing is straightforward: Most jobs contain repeatable tasks that technology can compress, and a broader purpose that remains human-led. He highlighted radiology as a real-world example.\n\nYears ago, AI pioneer Geoffrey Hinton predicted that AI would eradicate many radiology jobs and advised students to avoid the field. The opposite happened. While AI is automating many radiology tasks, there are actually more radiologists employed now than when Hinton made his prediction in 2016.\n\nHere are the killer stats, shared in this 2025 blog post that describes why radiologists are still in huge demand: In 2025, American diagnostic radiology residency programs offered a record 1,208 positions, a 4% increase from 2024, and the field's vacancy rates are at all-time highs. Also, in 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48% percent higher than the average radiologist salary in 2015 (the year before Hinton's prediction).\n\nHow did this happen? Huang argued that the job's purpose isn't \"reading scans.\" Those are tasks that AI has automated. The true purpose of a radiologist is to diagnose disease, guide treatment, and support those efforts with research. When AI helps clinicians evaluate more images with higher confidence, hospitals can serve more patients, generate more revenue, and justify hiring more specialists.\n\nThe same logic, he said, applies across the economy.\n\n\"I spend most of my day typing,\" Huang noted, describing typing as a task, not his job's purpose. Tools that automate writing don't eliminate the need for executives; they often expand the amount of work leaders and other employees can take on, he said.\n\n\"The fact that somebody could use AI to automate a lot of my typing — I really appreciate that, and it helps a lot,\" he said. \"It hasn't really made me, if you will, less busy. In a lot of ways, I become more busy because I'm able to do more work.\"\n\nThis \"task versus purpose\" framework is increasingly visible in knowledge work, where AI tools are speeding up and automating tasks such as drafting, summarizing, and generating code.\n\nHuang pointed to software engineering as a case where AI can reduce time spent on a core task (writing code) while raising demand for the job's purpose: solving problems and identifying new ones worth solving.\n\nNvidia, he said, is hiring aggressively even as AI coding tools such as Cursor spread through the company's engineering teams, because productivity gains allow companies to pursue more ideas. That can boost revenue, leaving more money to hire new staff.\n\nLaw is another example he cited. Reading and drafting contracts are tasks, while the purpose of a lawyer is to protect clients and resolve disputes. AI can accelerate document-heavy work, but the role's true value relies on judgment, strategy, and accountability — and you need experienced, trustworthy human attorneys for that.\n\nThis even applies to waiters working in a restaurant. Their task is taking food orders, but their purpose is to ensure guests have a great time, Huang said.\n\n\"If some AI is taking the order or even delivering the food, their job is still helping us have a great experience,\" the CEO added. \"They would reshape their jobs accordingly.\"\n\nHuang's argument isn't that AI won't disrupt roles — it will. But he contends the early evidence points less toward a wholesale collapse of employment and more toward job redesign.\n\nFor workers, the implication is pragmatic: if your role is defined primarily by a repeatable task, AI is a direct threat. If it's anchored in outcomes — diagnosis, customer experience, problem-solving, conflict resolution — AI may be less a replacement than a lever, changing what you spend time on while keeping your job's purpose intact.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "broader purpose",
      "job's purpose",
      "automating tasks",
      "jobs",
      "radiology",
      "demand",
      "typing",
      "tools",
      "less",
      "hospitals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/task-versus-purpose-nvidia-jensen-huang-ai-wont-kill-jobs-2026-1",
    "thumbnail_url": "https://i.insider.com/69684f72764ca5f34d2a7b11?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:04.259Z",
    "topic": "finance"
  },
  {
    "slug": "training-large-language-models-on-narrow-tasks-can-lead-to-broad-misalignment",
    "title": "Training large language models on narrow tasks can lead to broad misalignment",
    "description": "Finetuning a large language model on a narrow task of writing insecure code causes a broad range of concerning behaviours unrelated to coding.",
    "fullText": "Generating reliable software project task flows using large language models through prompt engineering and robust evaluation\n\n Article\n Open access\n 08 October 2025",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41586-025-09937-5",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09937-5/MediaObjects/41586_2025_9937_Fig1_HTML.png",
    "created_at": "2026-01-16T06:20:12.839Z",
    "topic": "tech"
  },
  {
    "slug": "everything-becomes-an-agent",
    "title": "Everything Becomes an Agent",
    "description": "Explore the inevitable shift from scripts to AI agents in coding. Discover insights on automation, tool access, and setting effective guardrails.",
    "fullText": "I’ve noticed a pattern in my coding life. It starts innocently enough. I sit down to write a simple Python script, maybe something to tidy up my Obsidian vault or a quick CLI tool to query an API. “Keep it simple,” I tell myself. “Just input, processing, output.”\n\nBut then, the inevitable thought creeps in: It would be cool if the model could decide which file to read based on the user’s question.\n\nTwo hours later, I’m not writing a script anymore. I’m writing a while loop. I’m defining a tools array. I’m parsing JSON outputs and handing them back to the model. I’m building memory context windows.\n\n(For those keeping track: my working definition of an “agent” is simple: a model running in a loop with access to tools. I explored this in depth in my Agentic Shift series, but that’s the core of it.)\n\nAs I sit here writing this in January of 2026, I realize that almost every AI project I worked on last year ultimately became an agent. It feels like a law of nature: Every AI project, given enough time, converges on becoming an agent. In this post, I want to share some of what I’ve learned, and the cases where you might skip the intermediate steps and jump straight to building an agent.\n\nThis isn’t just feature creep. It’s a fundamental shift in how we interact with software. We are moving past the era of “smart typewriters” and into the era of “digital interns.”\n\nTake Gemini Scribe, my plugin for Obsidian. When I started, it was a glorified chat window. You typed a prompt, it gave you text. Simple. But as I used it, the friction became obvious. If I wanted Scribe to use another note as context for a task, I had to take a specific action, usually creating a link to that note from the one I was working on, to make sure it was considered. I was managing the model’s context manually.\n\nI was the “glue” code. I was the context manager.\n\nThe moment I gave Scribe access to the read_file tool, the dynamic changed. Suddenly, I wasn’t micromanaging context; I was giving instructions. “Read the last three meeting notes and draft a summary.” That’s not a chat interaction; that’s a delegation. And to support delegation, the software had to become an agent, capable of planning, executing, and iterating.\n\nThe Gemini CLI followed a similar arc. There were many of us on the team experimenting with Gemini on the command line. I was working on iterative refinement, where the model would ask clarifying questions to create deeper artifacts. Others were building the first agentic loops, giving the model the ability to run shell commands.\n\nOnce we saw how much the model could do with even basic tools, we were hooked. Suddenly, it wasn’t just talking about code; it was writing and executing it. It could run tests, see the failure, edit the file, and run the tests again. It was eye-opening how much we could get done as a small team.\n\nBut with great power comes great anxiety. As I explored in my Agentic Shift post on building guardrails and later in my post about the Policy Engine, I found myself staring at a blinking cursor, terrified that my helpful assistant might accidentally rm -rf my project.\n\nThis is the hallmark of the agentic shift: you stop worrying about syntax errors and start worrying about judgment errors. We had to build a “sudoers” file for our AI, a permission system that distinguishes between “read-only exploration” and “destructive action.” You don’t build policy engines for scripts; you build them for agents.\n\nLast year, I learned to recognize a specific code smell: the AI classifier.\n\nIn my Podcast RAG project, I wanted users to search across both podcast descriptions and episode transcripts. Different databases, different queries. So I did what felt natural: I built a small classifier using Gemini Flash Lite. It would analyze the user’s question and decide: “Is this a description search or a transcript search?” Then it would call the appropriate function.\n\nIt worked. But something nagged at me. I had written a classifier to make a decision that a model is already good at making. Worse, the classifier was brittle. What if the user wanted both? What if their intent was ambiguous? I was encoding my assumptions about user behavior into branching logic, and those assumptions were going to be wrong eventually.\n\nThe fix was almost embarrassingly simple. I deleted the classifier and gave the agent two tools: search_descriptions and search_episodes. Now, when a user asks a question, the agent decides which tool (or tools) to use. It can search descriptions first, realize it needs more detail, and then dive into transcripts. It can do both in parallel. It makes the call in context, not based on my pre-programmed heuristics. (You can try it yourself at podcasts.hutchison.org.)\n\nI saw the same pattern in Gemini Scribe. Early versions had elaborate logic for context harvesting, code that tried to predict which notes the user would need based on their current document and conversation history. I was building a decision tree for context, and it was getting unwieldy.\n\nWhen I moved Scribe to a proper agentic architecture, most of that logic evaporated. The agent didn’t need me to pre-fetch context; it could use a read_file tool to grab what it needed, when it needed it. The complex anticipation logic was replaced by simple, reactive tool calls. The application got simpler and more capable at the same time.\n\nHere’s the heuristic I’ve landed on: If you’re writing if/else logic to decide what the AI should do, you might be building a classifier that wants to be an agent. Deconstruct those branches into tools, give the agent really good descriptions of what those tools can do, and then let the model choose its own adventure.\n\nYou might be thinking: “What about routing queries to different models? Surely a classifier makes sense there.” I’m not so sure anymore. Even model routing starts to look like an orchestration problem, and a lightweight orchestrator with tools for accessing different models gives you the same flexibility without the brittleness. The question isn’t whether an agent can make the decision better than your code. It’s whether the agent, with access to the actual data in the moment, can make a decision at least as good as what you’re trying to predict when you’re writing the code. The agent has context you don’t have at development time.\n\nWe are transitioning from Human-in-the-Loop (where we manually approve every step) to Human-on-the-Loop (where we set the goals and guardrails, but let the system drive).\n\nThis shift is driven by a simple desire: we want partners, not just tools. As I wrote back in April about waiting for a true AI coding partner, a tool requires your constant attention. A hammer does nothing unless you swing it. But an agent? An agent can work while you sleep.\n\nThis freedom comes with a new responsibility: clarity. If your agent is going to work overnight, you need to make sure it’s working on something productive. You need to be precise about the goal, explicit about the boundaries, and thoughtful about what happens when things go wrong. Without the right guardrails, an agent can get stuck waiting for your input, and you’ll lose that time. Or worse, it can get sidetracked and spend hours on something that wasn’t what you intended.\n\nThe goal isn’t to remove the human entirely. It’s to move us from the execution layer to the supervision layer. We set the destination and the boundaries; the agent figures out the route. But we have to set those boundaries well.\n\nHere’s the counterintuitive thing: building an agent isn’t always harder than building a script. Yes, you have to think about loops, tool definitions, and context window management. But as my classifier example showed, an agentic architecture can actually delete complexity. All that brittle branching logic, all those edge cases I was trying to anticipate: gone. Replaced by a model that can reason about what it needs in the moment.\n\nThe real complexity isn’t in the code; it’s in the trust. You have to get comfortable with a system that makes decisions you didn’t explicitly program. That’s a different kind of engineering challenge, less about syntax, more about guardrails and judgment.\n\nBut the payoff is a system that grows with you. A script does exactly what you wrote it to do, forever. An agent does what you ask it to do, and sometimes finds better ways to do it than you’d considered.\n\nSo, if you find yourself staring at your “simple script” and wondering if you should give it a tools definition… just give in. You’re building an agent. It’s inevitable. You might as well enjoy the company.",
    "readingTime": 8,
    "keywords": [
      "gemini scribe",
      "code it’s",
      "branching logic",
      "agentic architecture",
      "read_file tool",
      "agentic shift",
      "context",
      "model",
      "tools",
      "classifier"
    ],
    "qualityScore": 1,
    "link": "https://allen.hutchison.org/2026/01/15/everything-becomes-an-agent/",
    "thumbnail_url": "https://jetpack.com/redirect/?source=sigenerate&query=t%3DeyJpbWciOiJodHRwczpcL1wvYWxsZW4uaHV0Y2hpc29uLm9yZ1wvd3AtY29udGVudFwvdXBsb2Fkc1wvMjAyNlwvMDFcL0dlbWluaV9HZW5lcmF0ZWRfSW1hZ2VfOG9iYm5sOG9iYm5sOG9iYi0xMDI0eDU1OS5wbmciLCJ0eHQiOiJFdmVyeXRoaW5nIEJlY29tZXMgYW4gQWdlbnQiLCJ0ZW1wbGF0ZSI6ImhpZ2h3YXkiLCJmb250IjoiIiwiYmxvZ19pZCI6NTI2NDF9.clrckN4D9nLjT29SGR-zSduRSVw6yptl6Uby6fXr7VwMQ",
    "created_at": "2026-01-16T00:58:36.450Z",
    "topic": "tech"
  },
  {
    "slug": "aviator-yc-s21-is-hiring-to-build-multiplayer-ai-coding-platform",
    "title": "Aviator (YC S21) is hiring to build multiplayer AI coding platform",
    "description": "Jobs at Aviator",
    "fullText": "Software engineering is being fundamentally transformed by AI, and we're building the tools to lead that shift. Aviator is creating the engineering productivity supertools that will define how the best teams build software in the AI era.\n\nOur platform already powers workflow automation at Slack, Figma, DoorDash, and other industry leaders. MergeQueue eliminates merge conflicts and broken builds. FlexReview intelligently routes code reviews. And Runbooks—our newest product—is a collaborative AI agent platform that lets engineering teams automate complex workflows through natural language specs and shared context.\n\nWe believe the future of software development isn't engineers replaced by AI—it's engineers supercharged by it. Small teams will ship what once required hundreds of people. Complex workflows that took days will complete in minutes. We're building that future.",
    "readingTime": 1,
    "keywords": [
      "complex workflows",
      "software",
      "engineering",
      "teams",
      "we're",
      "platform",
      "engineers"
    ],
    "qualityScore": 0.65,
    "link": "https://www.ycombinator.com/companies/aviator/jobs",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/92093d419e958ee69c7f233b3b03a172ff20f5d1.png?1652822764",
    "created_at": "2026-01-16T00:58:30.154Z",
    "topic": "jobs"
  },
  {
    "slug": "cursor-may-be-switching-from-solid-to-react",
    "title": "Cursor may be switching from Solid to React",
    "description": "We've been experimenting with running coding agents autonomously for weeks at a time.",
    "fullText": "We've been experimenting with running coding agents autonomously for weeks.\n\nOur goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nToday's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.\n\nOur first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.\n\nOur initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.\n\nThis failed in interesting ways:\n\nAgents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.\n\nThe system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.\n\nWe tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.\n\nWith no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.\n\nOur next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.\n\nPlanners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.\n\nWorkers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.\n\nAt the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nDespite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.\n\nWhile it might seem like a simple screenshot, building a browser from scratch is extremely difficult.\n\nAnother experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.\n\nAnother experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.\n\nWe have a few other interesting examples still running:\n\nWe've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.\n\nModel choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.\n\nOpus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.\n\nMany of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.\n\nThe best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.\n\nThe right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.\n\nA surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.\n\nMulti-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.\n\nBut the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.\n\nThe techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.",
    "readingTime": 6,
    "keywords": [
      "another experiment",
      "tunnel vision",
      "agents",
      "agent",
      "tasks",
      "system",
      "we've",
      "coding",
      "projects",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/scaling-agents#running-for-weeks",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/long-running-agents-og.png",
    "created_at": "2026-01-15T18:24:00.658Z",
    "topic": "tech"
  },
  {
    "slug": "pimono-coding-agent",
    "title": "Pi-Mono Coding Agent",
    "description": "AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods - badlogic/pi-mono",
    "fullText": "badlogic\n\n /\n\n pi-mono\n\n Public\n\n AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods\n\n License\n\n MIT license\n\n 1.8k\n stars\n\n 228\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n badlogic/pi-mono",
    "readingTime": 1,
    "keywords": [
      "agent",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/badlogic/pi-mono",
    "thumbnail_url": "https://opengraph.githubassets.com/bd7641eb472820a5f2d3f4dea10d6023fe9ba9619615be1dfc8b43bdb9eec747/badlogic/pi-mono",
    "created_at": "2026-01-15T12:24:35.039Z",
    "topic": "tech"
  },
  {
    "slug": "how-to-write-a-good-spec-for-ai-agents",
    "title": "How to write a good spec for AI agents",
    "description": "Learn how to write effective specifications for AI coding agents to improve clarity, focus, and productivity in your AI-driven development workflows.",
    "fullText": "TL;DR: Aim for a clear spec covering just enough nuance (this may include structure, style, testing, boundaries) to guide the AI without overwhelming it. Break large tasks into smaller ones vs. keeping everything in one large prompt. Plan first in read-only mode, then execute and iterate continuously.\n\n“I’ve heard a lot about writing good specs for AI agents, but haven’t found a solid framework yet. I could write a spec that rivals an RFC, but at some point the context is too large and the model breaks down.”\n\nMany developers share this frustration. Simply throwing a massive spec at an AI agent doesn’t work - context window limits and the model’s “attention budget” get in the way. The key is to write smart specs: documents that guide the agent clearly, stay within practical context sizes, and evolve with the project. This guide distills best practices from my use of coding agents including Claude Code and Gemini CLI into a framework for spec-writing that keeps your AI agents focused and productive.\n\nWe’ll cover five principles for great AI agent specs, each starting with a bolded takeaway.\n\nKick off your project with a concise high-level spec, then have the AI expand it into a detailed plan.\n\nInstead of over-engineering upfront, begin with a clear goal statement and a few core requirements. Treat this as a “product brief” and let the agent generate a more elaborate spec from it. This leverages the AI’s strength in elaboration while you maintain control of the direction. This works well unless you already feel you have very specific technical requirements that must be met from the start.\n\nWhy this works: LLM-based agents excel at fleshing out details when given a solid high-level directive, but they need a clear mission to avoid drifting off course. By providing a short outline or objective description and asking the AI to produce a full specification (e.g. a spec.md), you create a persistent reference for the agent. Planning in advance matters even more with an agent - you can iterate on the plan first, then hand it off to the agent to write the code. The spec becomes the first artifact you and the AI build together.\n\nPractical approach: Start a new coding session by prompting, “You are an AI software engineer. Draft a detailed specification for [project X] covering objectives, features, constraints, and a step-by-step plan.” Keep your initial prompt high-level - e.g. “Build a web app where users can track tasks (to-do list), with user accounts, a database, and a simple UI”. The agent might respond with a structured draft spec: an overview, feature list, tech stack suggestions, data model, and so on. This spec then becomes the “source of truth” that both you and the agent can refer back to. GitHub’s AI team promotes spec-driven development where “specs become the shared source of truth… living, executable artifacts that evolve with the project”. Before writing any code, review and refine the AI’s spec. Make sure it aligns with your vision and correct any hallucinations or off-target details.\n\nUse Plan Mode to enforce planning-first: Tools like Claude Code offer a Plan Mode that restricts the agent to read-only operations - it can analyze your codebase and create detailed plans but won’t write any code until you’re ready. This is ideal for the planning phase: start in Plan Mode (Shift+Tab in Claude Code), describe what you want to build, and let the agent draft a spec while exploring your existing code. Ask it to clarify ambiguities by questioning you about the plan. Have it review the plan for architecture, best practices, security risks, and testing strategy. The goal is to refine the plan until there’s no room for misinterpretation. Only then do you exit Plan Mode and let the agent execute. This workflow prevents the common trap of jumping straight into code generation before the spec is solid.\n\nUse the spec as context: Once approved, save this spec (e.g. as SPEC.md) and feed relevant sections into the agent as needed. Many developers using a strong model do exactly this - the spec file persists between sessions, anchoring the AI whenever work resumes on the project. This mitigates the forgetfulness that can happen when the conversation history gets too long or when you have to restart an agent. It’s akin to how one would use a Product Requirements Document (PRD) in a team: a reference that everyone (human or AI) can consult to stay on track. Experienced folks often “write good documentation first and the model may be able to build the matching implementation from that input alone” as one engineer observed. The spec is that documentation.\n\nKeep it goal-oriented: A high-level spec for an AI agent should focus on what and why, more than the nitty-gritty how (at least initially). Think of it like the user story and acceptance criteria: Who is the user? What do they need? What does success look like? (e.g. “User can add, edit, complete tasks; data is saved persistently; the app is responsive and secure”). This keeps the AI’s detailed spec grounded in user needs and outcome, not just technical to-dos. As the GitHub Spec Kit docs put it, provide a high-level description of what you’re building and why, and let the coding agent generate a detailed specification focusing on user experience and success criteria. Starting with this big-picture vision prevents the agent from losing sight of the forest for the trees when it later gets into coding.\n\nTreat your AI spec as a structured document (PRD) with clear sections, not a loose pile of notes.\n\nMany developers treat specs for agents much like traditional Product Requirement Documents (PRDs) or System Design docs - comprehensive, well-organized, and easy for a “literal-minded” AI to parse. This formal approach gives the agent a blueprint to follow and reduces ambiguity.\n\nThe six core areas: GitHub’s analysis of over 2,500 agent configuration files revealed a clear pattern: the most effective specs cover six areas. Use this as a checklist for completeness:\n\n1. Commands: Put executable commands early - not just tool names, but full commands with flags: npm test, pytest -v, npm run build. The agent will reference these constantly.\n\n2. Testing: How to run tests, what framework you use, where test files live, and what coverage expectations exist.\n\n3. Project structure: Where source code lives, where tests go, where docs belong. Be explicit: “src/ for application code, tests/ for unit tests, docs/ for documentation.”\n\n4. Code style: One real code snippet showing your style beats three paragraphs describing it. Include naming conventions, formatting rules, and examples of good output.\n\n5. Git workflow: Branch naming, commit message format, PR requirements. The agent can follow these if you spell them out.\n\n6. Boundaries: What the agent should never touch - secrets, vendor directories, production configs, specific folders. “Never commit secrets” was the single most common helpful constraint in the GitHub study.\n\nBe specific about your stack: Say “React 18 with TypeScript, Vite, and Tailwind CSS” not “React project.” Include versions and key dependencies. Vague specs produce vague code.\n\nUse a consistent format: Clarity is king. Many devs use Markdown headings or even XML-like tags in the spec to delineate sections, because AI models handle well-structured text better than free-form prose. For example, you might structure the spec as:\n\nThis level of organization not only helps you think clearly, it helps the AI find information. Anthropic engineers recommend organizing prompts into distinct sections (like <background>, <instructions>, <tools>, <output_format> etc.) for exactly this reason - it gives the model strong cues about which info is which. And remember, “minimal does not necessarily mean short” - don’t shy away from detail in the spec if it matters, but keep it focused.\n\nIntegrate specs into your toolchain: Treat specs as “executable artifacts” tied to version control and CI/CD. The GitHub Spec Kit uses a four-phase, gated workflow that makes your specification the center of your engineering process. Instead of writing a spec and setting it aside, the spec drives the implementation, checklists, and task breakdowns. Your primary role is to steer; the coding agent does the bulk of the writing. Each phase has a specific job, and you don’t move to the next one until the current task is fully validated:\n\n1. Specify: You provide a high-level description of what you’re building and why, and the coding agent generates a detailed specification. This isn’t about technical stacks or app design - it’s about user journeys, experiences, and what success looks like. Who will use this? What problem does it solve? How will they interact with it? Think of it as mapping the user experience you want to create, and letting the coding agent flesh out the details. This becomes a living artifact that evolves as you learn more.\n\n2. Plan: Now you get technical. You provide your desired stack, architecture, and constraints, and the coding agent generates a comprehensive technical plan. If your company standardizes on certain technologies, this is where you say so. If you’re integrating with legacy systems or have compliance requirements, all of that goes here. You can ask for multiple plan variations to compare approaches. If you make internal docs available, the agent can integrate your architectural patterns directly into the plan.\n\n3. Tasks: The coding agent takes the spec and plan and breaks them into actual work - small, reviewable chunks that each solve a specific piece of the puzzle. Each task should be something you can implement and test in isolation, almost like test-driven development for your AI agent. Instead of “build authentication,” you get concrete tasks like “create a user registration endpoint that validates email format.”\n\n4. Implement: Your coding agent tackles tasks one by one (or in parallel). Instead of reviewing thousand-line code dumps, you review focused changes that solve specific problems. The agent knows what to build (specification), how to build it (plan), and what to work on (task). Crucially, your role is to verify at each phase: Does the spec capture what you want? Does the plan account for constraints? Are there edge cases the AI missed? The process builds in checkpoints for you to critique, spot gaps, and course-correct before moving forward.\n\nThis gated workflow prevents what Willison calls “house of cards code” - fragile AI outputs that collapse under scrutiny. Anthropic’s Skills system offers a similar pattern, letting you define reusable Markdown-based behaviors that agents invoke. By embedding your spec in these workflows, you ensure the agent can’t proceed until the spec is validated, and changes propagate automatically to task breakdowns and tests.\n\nConsider agents.md for specialized personas: For tools like GitHub Copilot, you can create agents.md files that define specialized agent personas - a @docs-agent for technical writing, a @test-agent for QA, a @security-agent for code review. Each file acts as a focused spec for that persona’s behavior, commands, and boundaries. This is particularly useful when you want different agents for different tasks rather than one general-purpose assistant.\n\nDesign for Agent Experience (AX): Just as we design APIs for developer experience (DX), consider designing specs for “Agent Experience.” This means clean, parseable formats: OpenAPI schemas for any APIs the agent will consume, llms.txt files that summarize documentation for LLM consumption, and explicit type definitions. The Agentic AI Foundation (AAIF) is standardizing protocols like MCP (Model Context Protocol) for tool integration - specs that follow these patterns are easier for agents to consume and act on reliably.\n\nPRD vs SRS mindset: It helps to borrow from established documentation practices. For AI agent specs, you’ll often blend these into one document (as illustrated above), but covering both angles serves you well. Writing it like a PRD ensures you include user-centric context (“the why behind each feature”) so the AI doesn’t optimize for the wrong thing. Expanding it like an SRS ensures you nail down the specifics the AI will need to actually generate correct code (like what database or API to use). Developers have found that this extra upfront effort pays off by drastically reducing miscommunications with the agent later.\n\nMake the spec a “living document”: Don’t write it and forget it. Update the spec as you and the agent make decisions or discover new info. If the AI had to change the data model or you decided to cut a feature, reflect that in the spec so it remains the ground truth. Think of it as version-controlled documentation. In spec-driven workflows, the spec drives implementation, tests, and task breakdowns, and you don’t move to coding until the spec is validated. This habit keeps the project coherent, especially if you or the agent step away and come back later. Remember, the spec isn’t just for the AI - it helps you as the developer maintain oversight and ensure the AI’s work meets the real requirements.\n\nDivide and conquer: give the AI one focused task at a time rather than a monolithic prompt with everything at once.\n\nExperienced AI engineers have learned that trying to stuff the entire project (all requirements, all code, all instructions) into a single prompt or agent message is a recipe for confusion. Not only do you risk hitting token limits, you also risk the model losing focus due to the “curse of instructions” - too many directives causing it to follow none of them well. The solution is to design your spec and workflow in a modular way, tackling one piece at a time and pulling in only the context needed for that piece.\n\nThe curse of too much context/instructions: Research has confirmed what many devs anecdotally saw: as you pile on more instructions or data into the prompt, the model’s performance in adhering to each one drops significantly. One study dubbed this the “curse of instructions”, showing that even GPT-4 and Claude struggle when asked to satisfy many requirements simultaneously. In practical terms, if you present 10 bullet points of detailed rules, the AI might obey the first few and start overlooking others. The better strategy is iterative focus. Guidelines from industry suggest decomposing complex requirements into sequential, simple instructions as a best practice. Focus the AI on one sub-problem at a time, get that done, then move on. This keeps the quality high and errors manageable.\n\nDivide the spec into phases or components: If your spec document is very long or covers a lot of ground, consider splitting it into parts (either physically separate files or clearly separate sections). For example, you might have a section for “Backend API Spec” and another for “Frontend UI Spec.” You don’t need to always feed the frontend spec to the AI when it’s working on the backend, and vice versa. Many devs using multi-agent setups even create separate agents or sub-processes for each part - e.g. one agent works on database/schema, another on API logic, another on frontend - each with the relevant slice of the spec. Even if you use a single agent, you can emulate this by copying only the relevant spec section into the prompt for that task. Avoid context overload: Don’t mix authentication tasks with database schema changes in one go, as the DigitalOcean AI guide warns. Keep each prompt tightly scoped to the current goal.\n\nExtended TOC / Summaries for large specs: One clever technique is to have the agent build an extended Table of Contents with summaries for the spec. This is essentially a “spec summary” that condenses each section into a few key points or keywords, and references where details can be found. For example, if your full spec has a section on “Security Requirements” spanning 500 words, you might have the agent summarize it to: “Security: use HTTPS, protect API keys, implement input validation (see full spec §4.2)”. By creating a hierarchical summary in the planning phase, you get a bird’s-eye view that can stay in the prompt, while the fine details remain offloaded unless needed. This extended TOC acts as an index: the agent can consult it and say “aha, there’s a security section I should look at”, and you can then provide that section on demand. It’s similar to how a human developer skims an outline and then flips to the relevant page of a spec document when working on a specific part.\n\nTo implement this, you can prompt the agent after writing the spec: “Summarize the spec above into a very concise outline with each section’s key points and a reference tag.” The result might be a list of sections with one or two sentence summaries. That summary can be kept in the system or assistant message to guide the agent’s focus without eating up too many tokens. This hierarchical summarization approach is known to help LLMs maintain long-term context by focusing on the high-level structure. The agent carries a “mental map” of the spec.\n\nUtilize sub-agents or “skills” for different spec parts: Another advanced approach is using multiple specialized agents (what Anthropic calls subagents or what you might call “skills”). Each subagent is configured for a specific area of expertise and given the portion of the spec relevant to that area. For instance, you might have a Database Designer subagent that only knows about the data model section of the spec, and an API Coder subagent that knows the API endpoints spec. The main agent (or an orchestrator) can route tasks to the appropriate subagent automatically. The benefit is each agent has a smaller context window to deal with and a more focused role, which can boost accuracy and allow parallel work on independent tasks. Anthropic’s Claude Code supports this by letting you define subagents with their own system prompts and tools. “Each subagent has a specific purpose and expertise area, uses its own context window separate from the main conversation, and has a custom system prompt guiding its behavior,” as their docs describe. When a task comes up that matches a subagent’s domain, Claude can delegate that task to it, with the subagent returning results independently.\n\nParallel agents for throughput: Running multiple agents simultaneously is emerging as “the next big thing” for developer productivity. Rather than waiting for one agent to finish before starting another task, you can spin up parallel agents for non-overlapping work. Willison describes this as “embracing parallel coding agents” and notes it’s “surprisingly effective, if mentally exhausting”. The key is scoping tasks so agents don’t step on each other - one agent codes a feature while another writes tests, or separate components get built concurrently. Orchestration frameworks like LangGraph or OpenAI Swarm can help coordinate these agents, and shared memory via vector databases (like Chroma) lets them access common context without redundant prompting.\n\nSingle vs. multi-agent: when to use each\n\nIn practice, using subagents or skill-specific prompts might look like: you maintain multiple spec files (or prompt templates) - e.g. SPEC_backend.md, SPEC_frontend.md - and you tell the AI, “For backend tasks, refer to SPEC_backend; for frontend tasks refer to SPEC_frontend.” Or in a tool like Cursor/Claude, you actually spin up a subagent for each. This is certainly more complex to set up than a single-agent loop, but it mimics what human developers do - we mentally compartmentalize a large spec into relevant chunks (you don’t keep the whole 50-page spec in your head at once; you recall the part you need for the task at hand, and have a general sense of the overall architecture). The challenge, as noted, is managing interdependencies: the subagents must still coordinate (the frontend needs to know the API contract from the backend spec, etc.). A central overview (or an “architect” agent) can help by referencing the sub-specs and ensuring consistency.\n\nFocus each prompt on one task/section: Even without fancy multi-agent setups, you can manually enforce modularity. For example, after the spec is written, your next move might be: “Step 1: Implement the database schema.” You feed the agent the Database section of the spec only, plus any global constraints from the spec (like tech stack). The agent works on that. Then for Step 2, “Now implement the authentication feature”, you provide the Auth section of the spec and maybe the relevant parts of the schema if needed. By refreshing the context for each major task, you ensure the model isn’t carrying a lot of stale or irrelevant information that could distract it. As one guide suggests: “Start fresh: begin new sessions to clear context when switching between major features”. You can always remind the agent of critical global rules (from the spec’s Constraints section) each time, but don’t shove the entire spec in if it’s not all needed.\n\nUse in-line directives and code TODOs: Another modularity trick is to use your code or spec as an active part of the conversation. For instance, scaffold your code with // TODO comments that describe what needs to be done, and have the agent fill them one by one. Each TODO essentially acts as a mini-spec for a small task. This keeps the AI laser-focused (“implement this specific function according to this spec snippet”) and you can iterate in a tight loop. It’s similar to giving the AI a checklist item to complete rather than the whole checklist at once.\n\nThe bottom line: small, focused context beats one giant prompt. This improves quality and keeps the AI from getting “overwhelmed” by too much at once. As one set of best practices sums up, provide “One Task Focus” and “Relevant info only” to the model, and avoid dumping everything everywhere. By structuring the work into modules - and using strategies like spec summaries or sub-spec agents - you’ll navigate around context size limits and the AI’s short-term memory cap. Remember, a well-fed AI is like a well-fed function: give it only the inputs it needs for the job at hand.\n\nMake your spec not just a to-do list for the agent, but also a guide for quality control - and don’t be afraid to inject your own expertise.\n\nA good spec for an AI agent anticipates where the AI might go wrong and sets up guardrails. It also takes advantage of what you know (domain knowledge, edge cases, “gotchas”) so the AI doesn’t operate in a vacuum. Think of the spec as both coach and referee for the AI: it should encourage the right approach and call out fouls.\n\nUse three-tier boundaries: The GitHub analysis of 2,500+ agent files found that the most effective specs use a three-tier boundary system rather than a simple list of don’ts. This gives the agent clearer guidance on when to proceed, when to pause, and when to stop:\n\n✅ Always do: Actions the agent should take without asking. “Always run tests before commits.” “Always follow the naming conventions in the style guide.” “Always log errors to the monitoring service.”\n\n⚠️ Ask first: Actions that require human approval. “Ask before modifying database schemas.” “Ask before adding new dependencies.” “Ask before changing CI/CD configuration.” This tier catches high-impact changes that might be fine but warrant a human check.\n\n🚫 Never do: Hard stops. “Never commit secrets or API keys.” “Never edit node_modules/ or vendor/.” “Never remove a failing test without explicit approval.” “Never commit secrets” was the single most common helpful constraint in the study.\n\nThis three-tier approach is more nuanced than a flat list of rules. It acknowledges that some actions are always safe, some need oversight, and some are categorically off-limits. The agent can proceed confidently on “Always” items, flag “Ask first” items for review, and hard-stop on “Never” items.\n\nEncourage self-verification: One powerful pattern is to have the agent verify its work against the spec automatically. If your tooling allows, you can integrate checks like unit tests or linting that the AI can run after generating code. But even at the spec/prompt level, you can instruct the AI to double-check: e.g. “After implementing, compare the result with the spec and confirm all requirements are met. List any spec items that are not addressed.” This pushes the LLM to reflect on its output relative to the spec, catching omissions. It’s a form of self-audit built into the process.\n\nFor instance, you might append to a prompt: “(After writing the function, review the above requirements list and ensure each is satisfied, marking any missing ones).” The model will then (ideally) output the code followed by a short checklist indicating if it met each requirement. This reduces the chance it forgets something before you even run tests. It’s not foolproof, but it helps.\n\nLLM-as-a-Judge for subjective checks: For criteria that are hard to test automatically - code style, readability, adherence to architectural patterns - consider using “LLM-as-a-Judge.” This means having a second agent (or a separate prompt) review the first agent’s output against your spec’s quality guidelines. Anthropic and others have found this effective for subjective evaluation. You might prompt: “Review this code for adherence to our style guide. Flag any violations.” The judge agent returns feedback that either gets incorporated or triggers a revision. This adds a layer of semantic evaluation beyond syntax checks.\n\nConformance testing: Willison advocates building conformance suites - language-independent tests (often YAML-based) that any implementation must pass. These act as a contract: if you’re building an API, the conformance suite specifies expected inputs/outputs, and the agent’s code must satisfy all cases. This is more rigorous than ad-hoc unit tests because it’s derived directly from the spec and can be reused across implementations. Include conformance criteria in your spec’s Success section (e.g., “Must pass all cases in conformance/api-tests.yaml”).\n\nLeverage testing in the spec: If possible, incorporate a test plan or even actual tests in your spec and prompt flow. In traditional development, we use TDD or write test cases to clarify requirements - you can do the same with AI. For example, in the spec’s Success Criteria, you might say “These sample inputs should produce these outputs…” or “the following unit tests should pass.” The agent can be prompted to run through those cases in its head or actually execute them if it has that capability. Simon Willison noted that having a robust test suite is like giving the agents superpowers - they can validate and iterate quickly when tests fail. In an AI coding context, writing a bit of pseudocode for tests or expected outcomes in the spec can guide the agent’s implementation. Additionally, you can use a dedicated “test agent” in a subagent setup that takes the spec’s criteria and continuously verifies the “code agent’s” output.\n\nBring your domain knowledge: Your spec should reflect insights that only an experienced developer or someone with context would know. For example, if you’re building an e-commerce agent and you know that “products” and “categories” have a many-to-many relationship, state that clearly (don’t assume the AI will infer it - it might not). If a certain library is notoriously tricky, mention pitfalls to avoid. Essentially, pour your mentorship into the spec. The spec can contain advice like “If using library X, watch out for memory leak issue in version Y (apply workaround Z).” This level of detail is what turns an average AI output into a truly robust solution, because you’ve steered the AI away from common traps.\n\nAlso, if you have preferences or style guidelines (say, “use functional components over class components in React”), encode that in the spec. The AI will then emulate your style. Many engineers even include small examples in the spec, e.g., “All API responses should be JSON. E.g. {“error”: “message”} for errors.” By giving a quick example, you anchor the AI to the exact format you want.\n\nMinimalism for simple tasks: While we advocate thorough specs, part of expertise is knowing when to keep it simple. For relatively simple, isolated tasks, an overbearing spec can actually confuse more than help. If you’re asking the agent to do something straightforward (like “center a div on the page”), you might just say, “Make sure to keep the solution concise and do not add extraneous markup or styles.” No need for a full PRD there. Conversely, for complex tasks (like “implement an OAuth flow with token refresh and error handling”), that’s when you break out the detailed spec. A good rule of thumb: adjust spec detail to task complexity. Don’t under-spec a hard problem (the agent will flail or go off-track), but don’t over-spec a trivial one (the agent might get tangled or use up context on unnecessary instructions).\n\nMaintain the AI’s “persona” if needed: Sometimes, part of your spec is defining how the agent should behave or respond, especially if the agent interacts with users. For example, if building a customer support agent, your spec might include guidelines like “Use a friendly and professional tone,” “If you don’t know the answer, ask for clarification or offer to follow up, rather than guessing.” These kind of rules (often included in system prompts) help keep the AI’s outputs aligned with expectations. They are essentially spec items for AI behavior. Keep them consistent and remind the model of them if needed in long sessions (LLMs can “drift” in style over time if not kept on a leash).\n\nYou remain the exec in the loop: The spec empowers the agent, but you remain the ultimate quality filter. If the agent produces something that technically meets the spec but doesn’t feel right, trust your judgement. Either refine the spec or directly adjust the output. The great thing about AI agents is they don’t get offended - if they deliver a design that’s off, you can say, “Actually, that’s not what I intended, let’s clarify the spec and redo it.” The spec is a living artifact in collaboration with the AI, not a one-time contract you can’t change.\n\nSimon Willison humorously likened working with AI agents to “a very weird form of management” and even “getting good results out of a coding agent feels uncomfortably close to managing a human intern”. You need to provide clear instructions (the spec), ensure they have the necessary context (the spec and relevant data), and give actionable feedback. The spec sets the stage, but monitoring and feedback during execution are key. If an AI was a “weird digital intern who will absolutely cheat if you give them a chance”, the spec and constraints you write are how you prevent that cheating and keep them on task.\n\nHere’s the payoff: a good spec doesn’t just tell the AI what to build, it also helps it self-correct and stay within safe boundaries. By baking in verification steps, constraints, and your hard-earned knowledge, you drastically increase the odds that the agent’s output is correct on the first try (or at least much closer to correct). This reduces iterations and those “why on Earth did it do that?” moments.\n\nThink of spec-writing and agent-building as an iterative loop: test early, gather feedback, refine the spec, and leverage tools to automate checks.\n\nThe initial spec is not the end - it’s the beginning of a cycle. The best outcomes come when you continually verify the agent’s work against the spec and adjust accordingly. Also, modern AI devs use various tools to support this process (from CI pipelines to context management utilities).\n\nContinuous testing: Don’t wait until the end to see if the agent met the spec. After each major milestone or even each function, run tests or at least do quick manual checks. If something fails, update the spec or prompt before proceeding. For example, if the spec said “passwords must be hashed with bcrypt” and you see the agent’s code storing plain text - stop and correct it (and remind the spec or prompt about the rule). Automated tests shine here: if you provided tests (or write them as you go), let the agent run them. In many coding agent setups, you can have an agent run npm test or similar after finishing a task. The results (failures) can then feed back into the next prompt, effectively telling the agent “your output didn’t meet spec on X, Y, Z - fix it.” This kind of agentic loop (code -> test -> fix -> repeat) is extremely powerful and is how tools like Claude Code or Copilot Labs are evolving to handle larger tasks. Always define what “done” means (via tests or criteria) and check for it.\n\nIterate on the spec itself: If you discover that the spec was incomplete or unclear (maybe the agent misunderstood something or you realized you missed a requirement), update the spec document. Then explicitly re-sync the agent with the new spec: “I have updated the spec as follows… Given the updated spec, adjust the plan or refactor the code accordingly.” This way the spec remains the single source of truth. It’s similar to how we handle changing requirements in normal dev - but in this case you’re also the product manager for your AI agent. Keep version history if possible (even just via commit messages or notes), so you know what changed and why.\n\nUtilize context-management and memory tools: There’s a growing ecosystem of tools to help manage AI agent context and knowledge. For instance, retrieval-augmented generation (RAG) is a pattern where the agent can pull in relevant chunks of data from a knowledge base (like a vector database) on the fly. If your spec is huge, you could embed sections of it and let the agent retrieve the most relevant parts when needed, instead of always providing the whole thing. There are also frameworks implementing the Model Context Protocol (MCP), which automates feeding the right context to the model based on the current task. One example is Context7 (context7.com), which can auto-fetch relevant context snippets from docs based on what you’re working on. In practice, this might mean the agent notices you’re working on “payment processing” and it pulls the “Payments” section of your spec or documentation into the prompt. Consider leveraging such tools or setting up a rudimentary version (even a simple search in your spec document).\n\nParallelize carefully: Some developers run multiple agent instances in parallel on different tasks (as mentioned earlier with subagents). This can speed up development - e.g., one agent generates code while another simultaneously writes tests, or two features are built concurrently. If you go this route, ensure the tasks are truly independent or clearly separated to avoid conflicts (the spec should note any dependencies). For example, don’t have two agents writing to the same file at once. One workflow is to have an agent generate code and another review it in parallel, or to have separate components built that integrate later. This is advanced usage and can be mentally taxing to manage (as Willison admitted, running multiple agents is surprisingly effective, if mentally exhausting!). Start with at most 2-3 agents to keep things manageable.\n\nVersion control and spec locks: Use Git or your version control of choice to track what the agent does. Good version control habits matter even more with AI assistance. Commit the spec file itself to the repo. This not only preserves history, but the agent can even use git diff or blame to understand changes (LLMs are quite capable of reading diffs). Some advanced agent setups let the agent query the VCS history to see when something was introduced - surprisingly, models can be “fiercely competent at Git”. By keeping your spec in the repo, you allow both you and the AI to track evolution. There are tools (like GitHub Spec Kit mentioned earlier) that integrate spec-driven development into the git workflow - for instance, gating merges on updated specs or generating checklists from spec items. While you don’t need those tools to succeed, the takeaway is to treat the spec like code - maintain it diligently.\n\nCost and speed considerations: Working with large models and long contexts can be slow and expensive. A practical tip is to use model selection and batching smartly. Perhaps use a cheaper/faster model for initial drafts or repetitions, and reserve the most capable (and expensive) model for final outputs or complex reasoning. Some developers use GPT-4 or Claude for planning and critical steps, but offload simpler expansions or refactors to a local model or a smaller API model. If using multiple agents, maybe not all need to be top-tier; a test-running agent or a linter agent could be a smaller model. Also consider throttling context size: don’t feed 20k tokens if 5k will do. As we discussed, more tokens can mean diminishing returns.\n\nMonitor and log everything: In complex agent workflows, logging the agent’s actions and outputs is essential. Check the logs to see if the agent is deviating or encountering errors. Many frameworks provide trace logs or allow printing the agent’s chain-of-thought (especially if you prompt it to think step-by-step). Reviewing these logs can highlight where the spec or instructions might have been misinterpreted. It’s not unlike debugging a program - except the “program” is the conversation/prompt chain. If something weird happens, go back to the spec/instructions to see if there was ambiguity.\n\nLearn and improve: Finally, treat each project as a learning opportunity to refine your spec-writing skill. Maybe you’ll discover that a certain phrasing consistently confuses the AI, or that organizing spec sections in a certain way yields better adherence. Incorporate those lessons into the next spec. The field of AI agents is rapidly evolving, so new best practices (and tools) emerge constantly. Stay updated via blogs (like the ones by Simon Willison, Andrej Karpathy, etc.), and don’t hesitate to experiment.\n\nA spec for an AI agent isn’t “write once, done.” It’s part of a continuous cycle of instructing, verifying, and refining. The payoff for this diligence is substantial: by catching issues early and keeping the agent aligned, you avoid costly rewrites or failures later. As one AI engineer quipped, using these practices can feel like having “an army of interns” working for you, but you have to manage them well. A good spec, continuously maintained, is your management tool.\n\nBefore wrapping up, it’s worth calling out anti-patterns that can derail even well-intentioned spec-driven workflows. The GitHub study of 2,500+ agent files revealed a stark divide: “Most agent files fail because they’re too vague.” Here are the mistakes to avoid:\n\nVague prompts: “Build me something cool” or “Make it work better” gives the agent nothing to anchor on. As Baptiste Studer puts it: “Vague prompts mean wrong results.” Be specific about inputs, outputs, and constraints. “You are a helpful coding assistant” doesn’t work. “You are a test engineer who writes tests for React components, follows these examples, and never modifies source code” does.\n\nOverlong contexts without summarization: Dumping 50 pages of documentation into a prompt and hoping the model figures it out rarely works. Use hierarchical summaries (as discussed in Principle 3) or RAG to surface only what’s relevant. Context length is not a substitute for context quality.\n\nSkipping human review: Willison has a personal rule: “I won’t commit code I couldn’t explain to someone else.” Just because the agent produced something that passes tests doesn’t mean it’s correct, secure, or maintainable. Always review critical code paths. The “house of cards” metaphor applies: AI-generated code can look solid but collapse under edge cases you didn’t test.\n\nConflating vibe coding with production engineering: Rapid prototyping with AI (“vibe coding”) is great for exploration and throwaway projects. But shipping that code to production without rigorous specs, tests, and review is asking for trouble. Osmani distinguishes “vibe coding” from “AI-assisted engineering” - the latter requires the discipline this guide describes. Know which mode you’re in.\n\nIgnoring the “lethal trifecta”: Willison warns of three properties that make AI agents dangerous: speed (they work faster than you can review), non-determinism (same input, different outputs), and cost (encouraging corner-cutting on verification). Your spec and review process must account for all three. Don’t let speed outpace your ability to verify.\n\nMissing the six core areas: If your spec doesn’t cover commands, testing, project structure, code style, git workflow, and boundaries, you’re likely missing something the agent needs. Use the six-area checklist from Section 2 as a sanity check before handing off to the agent.\n\nWriting an effective spec for AI coding agents requires solid software engineering principles combined with adaptation to LLM quirks. Start with clarity of purpose and let the AI help expand the plan. Structure the spec like a serious design document - covering the six core areas and integrating it into your toolchain so it becomes an executable artifact, not just prose. Keep the agent’s focus tight by feeding it one piece of the puzzle at a time (and consider clever tactics like summary TOCs, subagents, or parallel orchestration to handle big specs). Anticipate pitfalls by including three-tier boundaries (Always/Ask first/Never), self-checks, and conformance tests - essentially, teach the AI how to not fail. And treat the whole process as iterative: use tests and feedback to refine both the spec and the code continuously.\n\nFollow these guidelines and your AI agent will be far less likely to “break down” under large contexts or wander off into nonsense.\n\nThis post was formatted using Gemini with images generated using Nano Banana Pro",
    "readingTime": 35,
    "keywords": [
      "extended toc",
      "api keys",
      "github study",
      "vague prompts",
      "document prd",
      "context protocol",
      "naming conventions",
      "helpful constraint",
      "architectural patterns",
      "tech stack"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/good-spec/",
    "thumbnail_url": "https://addyosmani.com/assets/images/good-spec.jpg",
    "created_at": "2026-01-14T12:25:01.028Z",
    "topic": "tech"
  },
  {
    "slug": "phases-of-vibe-coding",
    "title": "Phases of Vibe Coding",
    "description": "I built a terminal-based Counter-Strike clone with a coding agent. 49K lines in a week. Understanding the 4 phases of AI-assisted development.",
    "fullText": "EssaysThe 4 Phases of Vibe CodingI built a terminal-based Counter-Strike clone with a coding agent. 49K lines in a week. These projects go through 4 distinct phases, and understanding them is the key to effective AI-assisted development.Idan BeckCEO and FounderJanuary 12, 2026•12 min readShare: Loading content... Related Articles EssaysJanuary 12, 2026 • 8 min read The Bootstrapping LoopJust in Time SoftwareJanuary 6, 2026 • 10 min read Why Business Velocity Will Be Measured in Tokens per SecondMaster PlanOctober 28, 2025 • 8 min read Master Plan: Building Software at the Speed of ThoughtReady to Transform Your Development Process?Discover how Zerg AI can help you implement just-in-time software development in your organization.\n\nSchedule a Consultation",
    "readingTime": 1,
    "keywords": [
      "phases",
      "software",
      "development"
    ],
    "qualityScore": 0.55,
    "link": "https://zergai.com/blog/4-phases-vibe-coding",
    "thumbnail_url": "https://zergai.com/images/blog/4-phases-vibe-coding-hero.png",
    "created_at": "2026-01-14T01:00:15.929Z",
    "topic": "tech"
  },
  {
    "slug": "even-linus-torvalds-is-trying-his-hand-at-vibe-coding-but-just-a-little",
    "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
    "description": "But then I cut out the middle man—me.\"",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
    "created_at": "2026-01-14T01:00:15.061Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-engineering-what-ive-learned-working-with-ai-coding-agents",
    "title": "Vibe Engineering: What I've Learned Working with AI Coding Agents",
    "description": "Vibe Engineering: What I've Learned Working with AI Coding Agents",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/mrexodia/status/2010157660885176767",
    "thumbnail_url": "https://pbs.twimg.com/media/G-WBO0QXAAApT_j.jpg:large",
    "created_at": "2026-01-13T00:54:05.735Z",
    "topic": "tech"
  },
  {
    "slug": "targets-dev-server-offline-after-hackers-claim-to-steal-source-code",
    "title": "Target's dev server offline after hackers claim to steal source code",
    "description": "Hackers are claiming to be selling internal source code belonging to Target Corporation, after publishing what appears to be a sample of stolen code repositories on a public software development platform. After BleepingComputer notified Target, the files were taken offline and the retailer's developer Git server was inaccessible.",
    "fullText": "Hackers are claiming to be selling internal source code belonging to Target Corporation, after publishing what appears to be a sample of stolen code repositories on a public software development platform.\n\nLast week, an unknown threat actor created multiple repositories on Gitea that appeared to contain portions of Target's internal code and developer documentation. The repositories were presented as a preview of a much larger dataset allegedly being offered for sale to buyers on an underground forum or private channel.\n\nAfter BleepingComputer contacted Target with questions about the alleged breach, the files were taken offline and the retailer's Git server, git.target.com, became inaccessible from the internet.\n\nLast week, BleepingComputer received a tip that a threat actor was posting screenshots in a private hacking community to support claims that they had gained access to Target's internal development environment.\n\nThe same actor had also published several repositories on Gitea, a self-hosted Git service similar to GitHub or GitLab, as a sample of the data the actor claimed was being offered for sale.\n\nAccording to the source, hackers claimed that \"this is [the first set of] data to go to auction.\"\n\nEach repository contained a file named SALE.MD listing tens of thousands of files and directories purportedly included in the full dataset. The listing was more than 57,000 lines long and advertised a total archive size of approximately 860 GB.\n\nThe Gitea sample repository names included:\n\nIt's worth noting that the commit metadata and documentation referenced the names of internal Target development servers, and multiple current Target lead and senior engineers.\n\nBleepingComputer shared the Gitea links with Target on Thursday and requested comment on the alleged breach\n\nAround the same time, Target's developer Git server at git.target.com also became inaccessible from the internet.\n\nUntil Friday, the subdomain was reachable and redirected to a login page, prompting Target employees to connect via the company's secure network or VPN. As of Saturday, the site no longer loads externally:\n\nBleepingComputer also observed that search engines such as Google had indexed and cached a small number of resources from git.target.com, indicating that some content from the domain was publicly accessible at some point in the past.\n\nIt is unclear when those pages were indexed or under what configuration, and their presence in search results does not necessarily indicate that the current claims are linked to any exposure of the server, or that the Git infrastructure was recently accessible without authentication.\n\nWhile BleepingComputer has not independently verified the full 860 GB dataset or confirmed that a breach occurred, the directory structure, repository naming, and internal system references in the SALE.MD index are consistent with a large enterprise Git environment.\n\nAdditionally, the contents do not match any of Target's open-source projects on GitHub, indicating the material, if authentic, would have originated from private development infrastructure rather than publicly released code.\n\nThe presence of the names of current Target lead and senior engineers in commit metadata and documentation, along with links to internal API endpoints and platforms, such as confluence.target.com, also raises questions about the origin of the files.\n\nFurthermore, the fact that the Gitea respositories used to store Target's allegedly stolen source code are no longer available, also point toward a possible breach.\n\nAfter Target initially requested the repository links, the company did not provide further comment before publication when approached multiple times.\n\nTarget's most significant publicly disclosed security incident to date remains its 2013 breach, in which attackers stole payment card data and other personally identifiable information belonging to up to 110 million customers and exfiltrated it to infrastructure located in Eastern Europe, according to U.S. Senate and academic investigations.\n\nWhether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.\n\nGet the cheat sheet and take the guesswork out of secrets management.",
    "readingTime": 4,
    "keywords": [
      "git server",
      "target lead",
      "commit metadata",
      "senior engineers",
      "target's internal",
      "threat actor",
      "alleged breach",
      "code",
      "repositories",
      "development"
    ],
    "qualityScore": 1,
    "link": "https://www.bleepingcomputer.com/news/security/targets-dev-server-offline-after-hackers-claim-to-steal-source-code/",
    "thumbnail_url": "https://www.bleepstatic.com/content/hl-images/2026/01/12/target-header.jpg",
    "created_at": "2026-01-12T18:19:09.480Z",
    "topic": "tech"
  },
  {
    "slug": "pico-gpu-virtual-gpu-for-learning-shaders",
    "title": "Pico GPU: Virtual GPU for Learning Shaders",
    "description": "Experiment with GPU programming and sound synth",
    "fullText": "Pico GPU is a 300KB memory GPU intended to learn, experiment and have fun with shaders. It is perfect to easily create small demos or games involving 3D rendering. It can also perform GPU based sound synthesis.SpecificationsPico GPU specification are:640x480 resolution at 60FPS, with 24 bit depth and 8 bit stencil300KB gpu memory to load your textures, buffers, code and shaders4 channels Mono 32 bit sound synthesis at 48 KHz (using GPU shaders)complete support for vertex & fragment bufferssupport blending, culling, depth, stencil, color mask, clippingsupport render targets and instancingmaths matrix, vector, quaternion supportsave & load as a 640x480 PNG screenshot that contains all your datashare your apps with the community!",
    "readingTime": 1,
    "keywords": [
      "memory",
      "shaders",
      "sound",
      "depth",
      "load"
    ],
    "qualityScore": 0.35,
    "link": "https://ncannasse.github.io/picogpu/",
    "thumbnail_url": "og-image.png",
    "created_at": "2026-01-12T18:19:07.266Z",
    "topic": "tech"
  },
  {
    "slug": "quantization-and-distillation-effects-on-code-llms",
    "title": "Quantization and distillation effects on code LLMs",
    "description": "Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.02563",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-12T12:26:18.532Z",
    "topic": "tech"
  },
  {
    "slug": "we-asked-over-150-software-engineers-about-vibecoding-heres-what-they-said",
    "title": "We asked over 150 software engineers about vibe-coding. Here's what they said.",
    "description": "167 software engineers responded to Business Insider's vibe-coding survey. Over 45% reported \"keeping up\" with AI tools. Almost 17% feel behind.",
    "fullText": "AI has radically changed what coding looks like. We asked software engineers how they felt about it.\n\nAndrej Karpathy coined the term \"vibe-coding,\" or the creation of code using AI. The term has since gained traction among developers worldwide and was named Collins Dictionary's Word of the Year for 2025.\n\nLess than a year after his post, Karpathy wrote that he had \"never felt this much behind as a programmer.\"\n\nWe asked developers: When it comes to vibe-coding, do you feel ahead, behind, or like you're keeping pace?\n\n167 software engineers responded to our survey. The biggest cohort — 75 engineers, or 46.9% — said that they were \"keeping up.\" 30 engineers said they felt ahead of the curve, while 27 felt behind.\n\n28 engineers (or 17.5% of respondents) said that they were opting out of using AI code editing tools entirely. These engineers wrote that the tools weren't advanced enough, or that they took too long to learn how to use. None of the 28 agreed to speak on the record after Business Insider reached out.\n\nWhile the survey isn't scientific, the results offer insight into how software engineers are feeling about their rapidly changing industry.\n\nIn follow-up conversations, eight engineers told Business Insider how they feel about AI code editors. All found them helpful in some form, though their usages ranged from one-off tools to lifesavers.\n\nRyan Shah sometimes wonders: \"Did I really need to learn how to write code?\"\n\nThe 23-year-old AI consultant from Atlanta recently graduated with a degree in computer information technology. Now he uses Cursor and Google's Antigravity, paired with Claude Opus 4.5, which he said was at \"midlevel engineer status.\"\n\nShah said he doesn't regret his software engineering courses, though. They taught him to \"read\" code, he said, a skill that, coupled with his vibe-coding proficiencies, keeps him from being \"the first one laid off.\"\n\nJavanie Campbell swung the other way: He warned that over-reliance on vibe-coding tools will put your career in danger.\n\n\"For people who turn to the LLM as the God or the expert, they will be replaced,\" said the 35-year-old CEO of DevDaysAtWork, who is based in Jamaica.\n\nAmong software engineers, there's a debate brewing: Just how bad will the effects of AI code editors be on jobs? Some say they will shrink the industry's workforce; others call them tools, not replacements for engineers.\n\nThe first time Ryan Clinton tried vibe-coding, he got scared for his job. He's not fearful anymore, he said.\n\nClinton's engineering level won't be affected, said the 46-year-old software developer from Nashville. More experienced engineers work on \"architecture and design,\" he said, while more junior staffers code. At this point of AI coding, human intervention is also still routinely necessary.\n\n\"You want to make sure it makes sense,\" he said. \"Only an idiot would randomly click 'yes' and commit it.\"\n\nBarry Fruitman is more worried — but not for himself. At 56, the Android developer from Toronto doesn't think the job market will feel the effect until five to 10 years out.\n\n\"Today, I think the threat is overstated, and hopefully it will stay that way until I retire,\" he said.\n\nEd Gaile said AI tools have doubled, if not tripled, his productivity.\n\nThe 55-year-old Appfire principal solutions architect from Atlanta was impressed by the decrease in context switching that vibe-coding tools brought.\n\n\"I wish I had this 15 years ago,\" he said.\n\nFor AI code editors, the word \"productivity\" still looms large. Many people feel that they're saving time by using these tools. Others cite the additional time spent reviewing and correcting lines of code.\n\nA July METR study added fuel to the fire.\n\nThe study asked experienced developers to complete a series of tasks. Study participants working without AI's help spent 10% more time coding — but those with AI assistance spent 20% more time reviewing AI outputs, prompting AI, waiting on AI, or being idle. Ultimately, the study found that the AI-assisted developers were less productive.\n\nShawn Gay, a 54-year-old R&D manager from El Paso, Texas, spends time keeping up with the industry's changes. He said he felt behind the curve.\n\n\"I have decades of experience, so I feel like it's a huge effort to try to change the way my brain thinks about software,\" Gay told Business Insider.\n\nGus De Souza said that he saved time on coding, but spent more time reviewing the AI-generated code. The real productivity gains were in troubleshooting, said the 48-year-old software architect from Kitchener, Ontario.\n\nWhat even is a vibe-coder? While the term has grown to encompass most forms of AI-assisted coding, Karpathy's X post first defined it as when developers \"fully give in to the vibes, embrace exponentials, and forget that the code even exists.\"\n\nLara Fraser, a data analyst and epidemiologist from Sarasota, Florida, doesn't consider herself a vibe-coder.\n\nFraser codes in R and uses tools like ChatGPT and Claude to assist. She's tried other tools, but found high rates of hallucination. The model generation also matters, Fraser said: GPT 5.1 was great, but 5.2 was a \"disaster.\"\n\nFor Fraser, vibe-coding depends on the programmer's skill. Anyone can create an app, but not everyone can maintain it.\n\n\"Inevitably, something's going to break,\" she said. \"Can you fix it? If you can't, you're a vibe-coder.\"",
    "readingTime": 5,
    "keywords": [
      "code editors",
      "software engineers",
      "year-old software",
      "vibe-coding tools",
      "business insider",
      "developers",
      "behind",
      "study",
      "doesn't",
      "productivity"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/software-engineers-on-vibe-coding-ai-tools-2026-1",
    "thumbnail_url": "https://i.insider.com/69601f86832e0ef1ead7712a?width=1200&format=jpeg",
    "created_at": "2026-01-12T12:26:15.498Z",
    "topic": "tech"
  },
  {
    "slug": "tiny-coder-ai-coding-agent-in-300-loc-writing-itself",
    "title": "Tiny Coder – AI coding agent in ~300 LOC writing itself",
    "description": "Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies. - xrip/tinycode",
    "fullText": "xrip\n\n /\n\n tinycode\n\n Public\n\n Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies.\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n xrip/tinycode",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/xrip/tinycode",
    "thumbnail_url": "https://opengraph.githubassets.com/07e51ae8c3cabfafdf20fdcf525646b8f004627e8c671e3c9185240ef4e3b4fd/xrip/tinycode",
    "created_at": "2026-01-11T12:21:58.611Z",
    "topic": "tech"
  },
  {
    "slug": "a-coder-considers-the-waning-days-of-the-craft-2023",
    "title": "A coder considers the waning days of the craft (2023)",
    "description": "Coding has always felt to me like an endlessly deep and rich domain. Now I find myself wanting to write a eulogy for it.",
    "fullText": "I first began to believe this on a Friday morning this past summer, while working on a small hobby project. A few months back, my friend Ben and I had resolved to create a Times-style crossword puzzle entirely by computer. In 2018, we’d made a Saturday puzzle with the help of software and were surprised by how little we contributed—just applying our taste here and there. Now we would attempt to build a crossword-making program that didn’t require a human touch.\n\nWhen we’ve taken on projects like this in the past, they’ve had both a hardware component and a software component, with Ben’s strengths running toward the former. We once made a neon sign that would glow when the subway was approaching the stop near our apartments. Ben bent the glass and wired up the transformer’s circuit board. I wrote code to process the transit data. Ben has some professional coding experience of his own, but it was brief, shallow, and now about twenty years out of date; the serious coding was left to me. For the new crossword project, though, Ben had introduced a third party. He’d signed up for a ChatGPT Plus subscription and was using GPT-4 as a coding assistant.\n\nSomething strange started happening. Ben and I would talk about a bit of software we wanted for the project. Then, a shockingly short time later, Ben would deliver it himself. At one point, we wanted a command that would print a hundred random lines from a dictionary file. I thought about the problem for a few minutes, and, when thinking failed, tried Googling. I made some false starts using what I could gather, and while I did my thing—programming—Ben told GPT-4 what he wanted and got code that ran perfectly.\n\nFine: commands like those are notoriously fussy, and everybody looks them up anyway. It’s not real programming. A few days later, Ben talked about how it would be nice to have an iPhone app to rate words from the dictionary. But he had no idea what a pain it is to make an iPhone app. I’d tried a few times and never got beyond something that half worked. I found Apple’s programming environment forbidding. You had to learn not just a new language but a new program for editing and running code; you had to learn a zoo of “U.I. components” and all the complicated ways of stitching them together; and, finally, you had to figure out how to package the app. The mountain of new things to learn never seemed worth it. The next morning, I woke up to an app in my in-box that did exactly what Ben had said he wanted. It worked perfectly, and even had a cute design. Ben said that he’d made it in a few hours. GPT-4 had done most of the heavy lifting.",
    "readingTime": 3,
    "keywords": [
      "later ben",
      "iphone app",
      "project",
      "software",
      "code",
      "coding",
      "learn",
      "morning",
      "crossword",
      "puzzle"
    ],
    "qualityScore": 0.9,
    "link": "https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft",
    "thumbnail_url": "https://media.newyorker.com/photos/654bf23c9d37df3d9f9cd353/16:9/w_1280,c_limit/231120_r43375.jpg",
    "created_at": "2026-01-11T12:21:58.229Z",
    "topic": "tech"
  },
  {
    "slug": "npmagentskills-bundle-ai-agent-documentation-with-npm-packages",
    "title": "NPM-agentskills – Bundle AI agent documentation with NPM packages",
    "description": "Framework-agnostic skill discovery and export for AI coding agents - onmax/npm-agentskills",
    "fullText": "onmax\n\n /\n\n npm-agentskills\n\n Public\n\n Framework-agnostic skill discovery and export for AI coding agents\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n onmax/npm-agentskills",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/onmax/npm-agentskills",
    "thumbnail_url": "https://opengraph.githubassets.com/25ea09a060a3707ff9ae03007862bc0ef3c0da7b02d7c59d84dc472b47aefb04/onmax/npm-agentskills",
    "created_at": "2026-01-11T12:21:56.673Z",
    "topic": "tech"
  },
  {
    "slug": "bare-metal-programming-with-riscv-guide-2023",
    "title": "Bare metal programming with RISC-V guide (2023)",
    "description": "Guide on coding a bare metal program with UART output for RISC-V and running the emulation with QEMU. Brief overview of the RISC-V boot process.",
    "fullText": "Today we’re going to explore how to write a bare metal program for a RISC-V machine. For reproducibility, the target is a QEMU riscv64 virt machine.\n\nWe will briefly cover the initial stages of the RISC-V machine bootup and where you can plug in your custom software to program the bare metal machine!\n\nAt the end of this article, we will write a bare metal program for our RISC-V machine and send a string ‘hello’ to the user, without depending on any supporting software on the running machine whatsoever (OS kernel, libraries, anything).\n\nMachine bootup and running the initial software\n\nWriting a custom “BIOS” for RISC-V\n\nFeel free to skip the section on general concepts if you are familiar with how computers boot\n\nWhen a real machine is powered on, the hardware first runs the health checks and then loads the first instructions to run into its memory. Once the instructions are loaded, the processor core intializes its registers and the program counter points to the first instruction. From that point on, the software can run.\n\nIn simpler setups like small microcontrollers, this is all the software there is, just a single binary blob of instructions. The processor will execute just that going forward. In a more complex setup like a laptop or a phone, there are more stages to the startup.\n\nIn those more complex setups, traditionally, the first instructions are the BIOS, whose task is to subsequently load the bootloader into the memory and hand-off the control to it. The bootloader is usually small and easy to load into the running memory and the processor can easily start running its code. It proceeds to load the operating system kernel into the memory (implementing the bootloader though is a science of its own).\n\nEach machine loads the initial software in its own way. For example, the BIOS can be stored on a separate storage chip and upon powerup, the contents of the storage are simply filled into the memory at a fixed address and the processor just executes starting from that address.\n\nriscv64 virt machine, even though its virtualized, still has its own boot sequence. It goes through multiple stages, and at the moment, we will not be exploring them all. Please stay tuned for the follow-up articles with those details.\n\nThe key to understanding this virtual machine is that, obviously, it has no chip attached to it from which to read the software (it is virtual) so QEMU simulates this in some way. You might have seen the flag -bios for QEMU examples before and hopefully now you have a strong intuition what it could be. If you’re guessing this is passing the very first instructions that your virtual RISC-V core is executing upon the startup, you are almost correct.\n\nOnce you power on this virtual machine, QEMU fills the memory at 0x1000 with a few instructions and sets the program counter right to that address. This is the equivalent of a real machine having some hardcoded ROM firmware on the board (tucked away in some chip) and just dumping the contents into the RAM upon the bootup. You do not have the control over these instructions, i.e. they are not a part of your software image, and generally, I do not see a reason why you would want to override those, and they are actually quite useful for more complicated setups (I promise we will cover them in a follow up article). For the curious ones, these few instructions are the Zero Stage Bootloader (ZSBL). The ZSBL sets up a few registers for reasons we’ll explore in the future (right now, you can basically ignore this register setup) and jumps to the address 0x80000000 which is where the action truly begins!\n\n0x80000000 is where the first user-provided instructions to QEMU are running, and they are loaded there as soon as the virtual machine starts. If you don’t pass anything, QEMU will use the default and load up a piece of software called OpenSBI. The next article in this blog will be exactly what is the concept behind SBI in RISC-V and what exactly OpenSBI is. It’s important to note that SBI on RISC-V isn’t really BIOS, but something very similar. My personal guess is that the QEMU authors simply recycled the flag that was available and representing BIOS on other architectures like x86. Anyway, something to keep in mind is that SBI is generally very similar to BIOS in terms of what it does, and more importantly, it is something you can customize.\n\nThe -bios flag is the ELF a binary file containing instructions and potentially some other data, organized in sections. ELF is the standard binary format for Linux, and the details of the ELF file format are way outside the scope of this article, but a sufficient mental model here is that it is simply a key-value map where key is the starting address of a section, and the value is the bunch of bytes that need to be loaded into the memory at that address. Therefore, the ELF file provided to the -bios flag should fill out the memory starting at 0x80000000 (and this is indeed what QEMU’s default OpenSBI image does).\n\nIf you have been booting an operating system with QEMU before (e.g. Linux), you have likely used the -kernel flag. It is basically the same thing as the -bios flag: you can pass it an ELF image which covers some other memory region, and conceptually it will just dump the bytes in the memory. We won’t be using this flag today, we’ll cover its usage in the following articles.\n\nEven though conceptually ELF files represent just ways to fill in the memory, they are definitely not super simple that you can write a quick parser in one afternoon. A careful reader may wonder how does the machine then know to parse out the contents mapped to some address 0x12345678 from the ELF file and load the memory with those. This would be a great observation — in our case, we are using a virtual machine and we are basically simulating a machine which conceptually has such intelligent digital circuitry or amazingly complex initial software bootloader that is available in the machine’s memory right upon the powerup. That is, of course, not what happens in the real machines. The software that is loaded upon the powerup is stored on the machine storage as a flat binary blob that is blindly just dumped into the memory upon the powerup, there is really no parsing involved, but since we’re dealing with a virtual machine here, the sky is the limit, we are not bound by the complexities of manufacturing the hardware that does any of this.\n\nWe have established that 0x80000000 is the location of the first user-provided instruction that the machine executes. I provided it as just a fact, and if you really want a little more background as to why this might be so, you can start from here. Basically, what we see here is that DRAM is mapped to start at the 0x80000000 in the address space (if you don’t know what this means, don’t worry, it will not be too relevant for the rest of this article).\n\nLet’s begin by building an ELF file that will lay some processor instructions at address 0x80000000 that will give the user a message ‘hello’!\n\nThose who have done embedded systems programming in the past are surely familiar with the concept of UART. UART is a very simple device used for the most basic form of input/output: there is one wire for input (receiving, known as RX) and one wire for output (transmit, known as TX), and one bit goes onto the wire at the time. If you’re connecting two devices to speak to each other over UART, one device’s TX is the other device’s RX, and the other way around. If you’re reading this article and have not done anything with UART before, I strongly suggest at least getting the cheapest possible Arduino and having it speak to your computer through a USB-to-UART cable. The concept would be identical to what we’re doing here, but you would be doing it for real, and it will make more sense, since the scenario here is entirely virtualized.\n\nQEMU virtualizes an UART device on the virtual machine, and our software can access it. When you open the QEMU’s serial port (UART) section, what happens is basically when you press a keyboard button, the code for that button is sent out of your host’s TX to the VM’s RX and when the VM outputs something on its TX, it will be rendered to you graphically in the terminal (so you don’t have to otherwise decode the electrical signals from the simulated board :)), e.g. if the VM sends out 8 bits representing 65, your QEMU will render the character a, since that is its ASCII code.\n\nWe know that QEMU maps UART at the address 0x10000000 (you can check it in their source code) and the device that is virtualized here is NS16550A. The details do not matter here: for the purposes of the article, what this means is if you send an 8-bit value to that address from your software, that will be sent out on the TX wire of the virtualized UART device. Practically, that means if you go to QEMU’s serial port, the character you wrote to 0x10000000 will be rendered in your console.\n\nWith all this knowledge in mind, now we can write the code. The ELF file we are about to build will lay out a few instructions at 0x80000000 to print the characters, ‘h’, ‘e’, ‘l’, ‘l’ and ‘o’ in succession to the address 0x10000000. Finally, the code should then get stuck in an infinite loop (so that QEMU doesn’t crash for any strange reason and we can inspect the output)\n\nYou can save this file as hello.s. Let’s assemble this file into the machine code. In my case (and likely in yours), I am using a cross-platform toolchain, meaning that I am developing on a platform different from the target platform. Concretely, I am developing this software on an x86 machine and building for a riscv64 machine.\n\nTo assemble this file, I run the following:\n\nThe exact command may be different depending on what kind of assembler you have for riscv64, this is the tool I have obtained through my system’s package manager for Debian. I leave it to the reader to obtain the correct toolchain for building riscv64 software, it should generally be a matter of just obtaining the right software package from the Internet.\n\nNow, the code is only assembled, meaning that we have the software instructions in the machine code format, but this binary is still not ready to go and act as our fake BIOS. We need to use a linker and drive its behavior with a linker script to ensure that the instructions we have generated will be laid out at 0x80000000 as we intended. Let’s write the linker script.\n\nWe won’t be covering what all this means, but tl;dr is we now have a way to put those instructions exactly where we want them. Let’s verify with objdump.\n\nQEMU can now be fired up by running the following command:\n\nTo see what happens on UART, click the View button in the top menu and switch to the serial port view. The output should be like this:\n\nHead over to the Github repo for this article, run the make command and that will do everything we described above. You can then launch QEMU.",
    "readingTime": 10,
    "keywords": [
      "qemu’s serial",
      "elf file",
      "uart device",
      "operating system",
      "bare metal",
      "serial port",
      "linker script",
      "bios flag",
      "riscv virt",
      "program counter"
    ],
    "qualityScore": 1,
    "link": "https://popovicu.com/posts/bare-metal-programming-risc-v/",
    "thumbnail_url": "https://popovicu.com/Bare%20metal%20programming%20with%20RISC-V%20guide.png",
    "created_at": "2026-01-10T12:20:47.188Z",
    "topic": "tech"
  },
  {
    "slug": "effect-institute",
    "title": "Effect Institute",
    "description": "Interactive lessons for learning Effect. Master typed functional programming in TypeScript.",
    "fullText": "Feedback No. 7I love it! I can't wait for more. This simple exmaples shows how easy effect might be!■Feedback No. 14great! cant wait for next episode.■Feedback No. 24Oh dear, this is sooo cool. It brings joy to learning effect. love it. even though I'm already familiar with the basic concepts, going through these again here is such a great experience. love it. keep going! and thank you■Feedback No. 15Amazing UX. I want a course on this.■Feedback No. 37This is great. Thanks!■",
    "readingTime": 1,
    "keywords": [
      "love",
      "effect"
    ],
    "qualityScore": 0.3,
    "link": "https://www.effect.institute/",
    "thumbnail_url": "https://effect.institute/og/home.png",
    "created_at": "2026-01-09T00:58:49.511Z",
    "topic": "tech"
  },
  {
    "slug": "functional-programming-at-the-type-level-in-typescript",
    "title": "Functional programming at the type level in TypeScript",
    "description": "A library of composable functions for the type-level! Transform your TypeScript types in any way you want using functions you already know. - gvergnaud/hotscript",
    "fullText": "gvergnaud\n\n /\n\n hotscript\n\n Public\n\n A library of composable functions for the type-level! Transform your TypeScript types in any way you want using functions you already know.\n\n License\n\n MIT license\n\n 3.7k\n stars\n\n 58\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n gvergnaud/hotscript",
    "readingTime": 1,
    "keywords": [
      "functions",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/gvergnaud/hotscript",
    "thumbnail_url": "https://repository-images.githubusercontent.com/600849732/b995f53a-d30b-4a97-a036-bf0be01300ad",
    "created_at": "2026-01-09T00:58:48.829Z",
    "topic": "tech"
  },
  {
    "slug": "jane-streets-ron-minsky-on-the-future-of-programming-2023",
    "title": "Jane Street's Ron Minsky on the Future of Programming (2023)",
    "description": "Listen in on Jane Street’s Ron Minsky as he has conversations with engineers working on everything from clock synchronization to reliable multicast, build systems to reconfigurable hardware. Get a peek at how Jane Street approaches problems, and how those ideas relate to tech more broadly.",
    "fullText": "Richard Eisenberg is one of the core maintainers of Haskell. He\nrecently joined Jane Street’s Tools and Compilers team, where he hacks\non the OCaml compiler. He and Ron discuss the powerful language\nfeature that got him into PL design in the first place—dependent\ntypes—and its role in a world where AIs can (somewhat) competently\nwrite your code for you. They also discuss the differences between\nHaskell and OCaml; the perils of trying to make a language that works\nfor everybody; and how best a company like Jane Street can collaborate\nwith the open source community.\n\nRichard Eisenberg is one of the core maintainers of Haskell. He\nrecently joined Jane Street’s Tools and Compilers team, where he hacks\non the OCaml compiler. He and Ron discuss the powerful language\nfeature that got him into PL design in the first place—dependent\ntypes—and its role in a world where AIs can (somewhat) competently\nwrite your code for you. They also discuss the differences between\nHaskell and OCaml; the perils of trying to make a language that works\nfor everybody; and how best a company like Jane Street can collaborate\nwith the open source community.\n\nSome links to topics that came up in the discussion:\n\nWelcome to Signals and Threads, in-depth conversations about every\nlayer of the tech stack, from Jane Street. I’m Ron Minsky. It is my\ngreat pleasure to introduce Richard Eisenberg. Richard’s a senior\ncontributor to the Haskell ecosystem. He’s worked on Haskell’s type\nsystem in a lot of different ways over the last eight years. He’s on\nthe GHC Steering Committee. He’s the Chair of the Haskell\nFoundation. One might wonder, what’s he doing here? Well, it turns out\nRichard has joined Jane Street. He’s working on our Tools and\nCompilers team, and in particular, working on the team that works on\nthe front end of the OCaml compiler. So, thanks for joining me,\nRichard.\n\nSo, maybe a good place to start out with is to talk about your\nbackground. You’ve done all this exciting work in the Haskell\necosystem. Maybe tell us a little bit more about how you got involved\nin Haskell and what kind of work you’ve done there.\n\nYeah, sure. I first really encountered Haskell when I went back to\ngraduate school. I’d been a high school teacher for a little while and\nthen I thought, ‘oh, you know, it’s time to get a Ph.D. in programming\nlanguages,’ kind of, as you do.\n\nIn the process toward that, I discovered dependent types. I didn’t\nwanna just show up in grad school and not know anything. So I did some\nbackground reading and I discovered that, you know, these fancy type\nsystems have so much expressiveness, and so much of a way of being\nable to sort of define what it is that you’re programming before you\neven write a line of code, that you can just express your ideas so\nmuch more clearly and then get the compiler to check that you actually\ndid it correctly. This was just sort of, you know, my brain exploded\nwhen I learned about this.\n\nI feel like, you know, I kind of pretend to know something about\nprogramming languages on TV, but like, actually my academic background\nis distributed systems, not programming languages, and there’s lots of\ngaps in my knowledge. And so, I kinda don’t really know what dependent\ntypes are. Can you give a suitably beginner-friendly explanation of\nwhat dependent types are?\n\nSure. And I’m gonna start by avoiding the trap of trying to define the\nterm, ‘dependent type.’\n\nAnd instead, just sort of describe ‘a language with dependent types\nallows you to do this thing.’ And that thing is to essentially encode\na proof of correctness into your program such that every time you\ncompile your program, that proof is checked. And so you know—if you’ve\nset things up the right way—that your program does what you think it\ndoes. And so, I’ll give a concrete example.\n\nAnd that is, to imagine a sorting function. So normally we think of a\nsorting function, let’s just say, it’s a function from a list of\n‘ints’ to a list of ints, right? It takes this list of ints’ input,\nand then does some transformation, and then produces a list. But\nactually, that’s a very loose description of a sorting function. We\ncan do better than that.\n\nI wanna say that a sorting function takes in a list of ints, and it\nproduces a list of ints in, I’m gonna say, non-descending order\n(because maybe there’s duplicates, so I don’t want to say ‘ascending\norder’ because someone out there might say that’s wrong). So we get\nthis list out that’s in non-descending order. Okay, that’s closer, but\nthat’s still not quite right because maybe my function just always\nreturned to the empty list. That’s a list in non-descending order.\n\nSo instead, I really want to say that it’s a function that takes in a\nlist of ints, and returns a result that is in non-descending order,\nand is a permutation of the input list. And that’s a rather glorious\ntype that I can assign to a sorting function. But it accurately\ndescribes what that sorting function does.\n\nAnd then I can implement that in any number of ways, using any of the\ncommon sorting algorithms that you learn in computer science, and any\nof those different algorithms will meet that specification, and then,\nif I’ve encoded this, my compiler will check that my algorithm\nactually does sorting and doesn’t do some other thing. And so now,\nhaving sort of explained that, I can return back and actually say what\nI mean by ‘dependent type.’\n\nA dependent type is a type that can depend on some input that you give\ninto a function, essentially. So, in this case, the output of the\nfunction, not just a list of ints, but the output depends on the\ninput. The output must be a permutation of the input. And so, that’s\nthe dependent type.\n\nSo when I started grad school, then I saw Haskell, and here was a\nlanguage that had this fancy type system and had all this\nexpressiveness. Didn’t quite have dependent types, but it had a lot of\nthat stuff. That just got me excited. And then, through that I started\nworking with Stephanie Weirich at University of Pennsylvania. She was\ndoing a lot of Haskell work. She collaborated a bunch with Simon\nPeyton Jones, who then I started collaborating with, and things just\nsort of took off from there. It was a pretty fast introduction into\nthat world.\n\nI wanna go a little deeper on that. I feel like we’ve started with\nthe, like, ‘Well, of course I got a Ph.D. in PL and was interested in\ndependent types.’ Like, how do you even get there? Where does your\ninterest and background in programming languages come [from] in the\nfirst place?\n\nSo, once upon a time, I wanted to program. I don’t know why I wanted\nto program. I was too little to have that level of introspection.\n\nSo I guess, uh, I learned Basic when I was, I think, five\nsomehow. There was a friend of my parents that decided I should learn\nBasic and so that happened.\n\nBut I then got more into it, I guess, when I was in 7th grade, so this\nwas, probably I was 12 years old. And someone told me that Pascal was\na good language to learn. So, you know, I pleaded with my parents and\neventually they bought me the Pascal compiler ‘cause back in those\ndays, you actually had to pay for compilers. At least on, sort of—we\nhad a Macintosh. If I had some kind of Unix system, maybe it would’ve\nbeen different.\n\nWell, which Pascal compiler did you use?\n\nUm, THINK Pascal? Was that a thing?\n\nTHINK Pascal, so not Borland. I guess Borland was \nDOS side of the world.\n\nYeah, yeah. Borland, yes, Borland was for like, ‘real programmers.’\n\nSo I got this, and somewhere along the way, I had decided that every\npiece of software should be, like, if you read the instruction manual,\nthat should tell you everything you need to know about that piece of\nsoftware. I don’t know why I assumed that, but that was my starting\npoint. And so, it came with these two big books and most of the books\nwere just like on compiler flags and setting up projects and none of\nthis stuff about programming. At the end of one of the books was a\nsyntax reference with no glosses, no further information, just a\nsyntax reference using those, sort of, block and line diagrams to\nrepresent recursive loops in the syntax. And I was like, ‘Okay, well,\nif my assumption is that this book defines how to program, and this is\nthe only thing it has, then all the information must be here.’\n\nSo I just stared at these syntax diagrams probably for hours and\nhours. Like, ‘Okay, what does all of this mean?’ And then eventually\nsort of bootstrapped my way into learning how to program Pascal. And\nso, that being my first ‘real programming experience,’ somehow that\nbuilt the structures in my brain just around syntax and languages, and\nit kind of went from there.\n\nAnd then fast-forward 10 years or so, and I took a PL course as an\nundergrad from Norman Ramsey and it just lit me up. It was just like,\nthis is really cool stuff. I really enjoy this kind of thinking. And\nit turned out, I took that at the end of my senior year of undergrad,\nand so it was sort of too late to immediately go into grad school at\nthat point and I had this whole machine going toward high school\nteaching. So I did that for a while, but then I ended up returning.\n\nI love that your entrée into programming was, roughly speaking, a\ntraumatic experience with syntax. That seems… (laughs)\n\nBut it wasn’t traumatic. It was fantastic. Somehow, this was just what\nI needed. This was what my 12-year-old-self needed: big syntax blocks.\n\n(laughs) Okay. So, you went off, you got your Ph.D., you started\nworking on Haskell. Where did it go from there?\n\nWell, I was working on my Ph.D. It was basically, Stephanie and I had\nthis idea about how to encode dependent types, but it required some\nmore support from the Haskell compiler. And so, I was pleading [with]\nthe Haskell people to create this for me. And then, eventually Simon\njust wrote and said, ‘Why don’t you just do it?’ And that was a new\nconcept for me. I was like, ‘I could influence this language? Well,\nyou know, I’m just this random person.’\n\nBut between Stephanie and Simon mentoring me, I got to a point where I\ncould contribute and then it just grew from there. So, [I] did a bunch\nof that as a Ph.D. student. Then after leaving my Ph.D., I was a\nprofessor for a few years at Bryn Mawr College. What was great about\nthat experience was, they were open to the idea that open source\ncontributions were a form of publication, right? A lot of professors\nhave a lot of pressure to keep producing papers. That was definitely\nthere for me, too. But alongside papers, I couldn’t just do\nsoftware. But I could do, sort of, traditional scholarly publications\nand software. So that gave me a chance to continue my implementation\nwork, remain a part of this open source community, while still doing\nthe high-level scholarly research that I had been doing. So that was a\nreally nice part of it.\n\nIt turned out that, to my great surprise, that—well, this wasn’t to my\ngreat surprise—it turned out that this was a lot of work. That\nmaintaining a research program with a bunch of papers every year, and\ndoing these open source contributions, and teaching, there was just\nmore than could comfortably fit into one job, especially during term\ntime. And to my own surprise, I realized that the teaching part was\nthe part that maybe I could pause on. I had done that full-time as a\nhigh school teacher for eight years, now I was back doing it. It was\nfun, but I thought it was time to sort of push in a slightly different\ndirection.\n\nSo after a couple years at Bryn Mawr, I left to join Tweed which is a\nsoftware consultancy based in Paris. They do a ton of Haskell\nwork. And they hired me, basically just to continue my contributions\nin the Haskell space. So, I continued doing research there, continued\ncontributing to the compiler there. And, you know, after a few years\nof that, that was fun, and I got to a point where I was looking to be\npart of a cohesive team all working together, and realizing that after\nso many years just working in one language, I was really ready to see\nwhat things were like in a different space. And so the timing worked\nout really well when you, Ron, reached out to me and started the\ndiscussions that led to me joining Jane Street.\n\nRight. And often in these interviews, I’ll ask someone, like, ‘so how\ndid you get here?’ But actually, I was part of the dramatis personae\nof how you actually arrived.\n\nYou were the—the dramatis personae, yeah. (laughs)\n\n(laughs) Right, which is, at some point, I was like, ‘oh, it would be\nreally good if Richard worked here,’ and I started reaching out and\ntrying to convince you to come. Which took, it’s important, I\ncalculated, 11 months from (laughs) when I started.\n\nYeah, yeah. That’s right. I mean, you know, of course the story [of]\nme coming to Jane Street starts with you asking me, and me saying\n‘No.’\n\nAnd again, it was a matter of timing. I’ve been aware of Jane Street\nfor a long time, been sort of wondering what was happening over there,\nand somehow when you first reached out, it just wasn’t the right time\nfor me yet. And then the second time, it was sort of a better time. I\ncouldn’t even tell you why. It’s just, this is the way that life\nflows.\n\nBut, you know, one of the barriers which we’ve joked about is that I\ngrew up in the New York area and actually ended up going to high\nschool in New York and commuting from New Jersey. So every day I was\ngoing back and forth across the George Washington Bridge, and swore to\nmyself then that I would never commute to and from New York City. And\nalso just sort of had this negative vibe from the financial industry,\nand I was just like, ‘I don’t know. Like, I want to be creating\nsomething that other people use,’ right?\n\nBut actually, this is fantastic. I’m having great fun. Part of it is\nthat I’m working as part of this larger ecosystem. It’s not just a\nclosed thing where I’m only doing stuff that’s internal to Jane\nStreet. But instead, contributing to this open source ecosystem,\nworking on an open language, OCaml, and really continuing some of\nthese research trends that I started 10 years ago. Now that I’m here,\nit just all fits very naturally together.\n\nI feel like, in general, one of my takeaways is that there’s lots of\ninteresting work in lots of different places and the preconceptions\nyou have when you are young often (laughs) don’t work out over\ntime. So a thing you talked about earlier is a topic that I feel like\nI should know something about, but don’t. I like your first high-level\ndescription ‘cause it actually lines up with the very little bit I\nknow about dependent types.\n\nAnd there’s also something that’s always made me a little nervous\nabout them because a basic fact about proofs is—proofs seem pretty\nhard to construct. People go to great trouble to build automated\nproving systems where you have tactic libraries, like, little tiny\nprograms that you can run to, like, ‘try this way or try that way’ of\ncoming up with a proof of a thing. And it all sounds really hard and\nwhen you talk to people who do kind of theorem proving on large scale\nsoftware systems, it sounds like a pretty challenging operation where\nthis is kind of a lot of engineering work that just goes into just\nconstructing the proof. So I’m curious how you think of the\npracticalities of that? Like, how convenient can you make it? And if\nyou make it a deeply embedded part of the programming process, does\nthat work well with the other things you might wanna do? Does it\ninterfere with the ordinary process of programming? How do you see the\npragmatics of that working out?\n\nYeah, that’s a great question. And this is a question that doesn’t\nhave an answer yet. This is, I think, still something that the\nprogramming languages community is grappling with. I have my own\ntakes, but this is still sort of an open question out there.\n\nI think, first of all, that for a programming language to be\neffective, everything needs to be opt-in, right? If you have a\nlanguage that supports dependent types—and there’s a bunch out\nthere. So actually, OCaml’s module system is a dependent type\nsystem. It’s missing one or two key features that makes it really sort\nof go. But that definitely has aspects of dependent types in\nit. Haskell has aspects of dependent types.\n\nThere are other languages, Coq, Agda, Idris, Lean, these are the ones\nthat really have embraced dependent types. But in any of these\nlanguages, you don’t have to use these dependent types. You can just\nwrite a sorting function that is described as ‘a list of ints to a\nlist of ints.’ That’s fine. And I think that’s a key aspect of any\nkind of system that’s gonna be using this dependent types feature,\nbecause not everyone wants to prove everything all the time. As you\nsay, proofs are expensive. And so, what you need is, you need the\nability to only prove the parts that you think are worthwhile proving.\n\nSo let’s say we have an important part of the system that might be\neasy to get wrong that we want to prove something about. How do we do\nthat? Is that still worth it? And one thing to think about is, what’s\nyour cost-benefit analysis, right? If you do this proving, can you\nspend less time on testing? Can you spend less time on debugging? Can\nyou spend less time on catastrophic failure because you missed\nsomething when debugging?\n\nIt is a lot harder to do these programs with proofs. But sometimes,\nthat’s really worth it. It’s a matter of applying good taste, like it\nis everywhere in programming, right? Any programming language gives\nyou a ton of tools to work with, and it takes time and experience and\ntaste to figure out when it’s time to use which tools.\n\nRight. And I do really like that, kind of, pay-as-you-go kind of\nattitude of, like, you want some kind of base language that’s\nrelatively simple to work with, and then the ability to add on more\nfeatures that give you more power of various kinds, and not be forced\nto pay for that complexity constantly in everything you do. Which is,\nI think, actually a really hard thing to get right in the design—of\nlike, not having these more powerful features leak in and corrupt the\nrest of your experience within the language.\n\nThat’s right. It’s a very careful thing. And my push in Haskell has\nbeen to try to increase or add new features to help support dependent\ntypes, and there’s been a large community of people really excited for\nthis. And in my measurement, my biased measurement, a smaller\ncommunity of people who have been saying, ‘No, no, no. Don’t do this\nbecause it’s gonna make it impossible to write simple programs.’ And\nmany, many, many times, I’ve had to reassure people, ‘No, no. All\nthose simple programs will continue to exist, they’ll continue to\nwork. You can do what you want.’ But I also wanna give power to people\nwho want to use the fancy dependent types.\n\nGreat. Okay. So let’s leave Haskell and dependent types behind for the\nmoment because that’s not primarily what you’re working on here. Maybe\nyou can tell us, what are you working on here? Like, what is the\nmission that you are on here, working on OCaml?\n\nSo, I’ll start more broadly and then I’ll get narrower. My broad\nmission is my belief that type systems are a fantastic way to\nstructure thought and structure programs. So by being able to write\ndown what you intend to do before doing it, then you can check what\nyou’ve done, and you can also help formulate your thoughts. And when I\nsay, ‘write down what you intend to do before doing it,’ I mean, write\ndown a specification in types.\n\nThe flip side of that, is that it must be the case that you can do\nthat without paying a one-time cost. It’s gonna be problematic if you\nwant to write an efficient program, and yet, by defining your program\ncarefully ahead of time, now it’s no longer as efficient at\nruntime. This checking and this writing of a specification should all\nhappen at compile time. This should be happening in front of the\nprogrammer and not in front of any users. If any of those details leak\ninto user land by slowing your program down or somehow interfering\nwith the operation of your program, then there’s a misdesign in the\nlanguage.\n\nSo coming down to what I’m actually working on here at Jane Street, so\nthe feature I’m working on is something called unboxed types. There’s\nvarious different ways of characterizing it, but one way to do so is,\nit allows inlining of type definitions. So what we might want to have\nin our design is to have a structure that has four fields that\ndescribe some aspects of a trade. And then maybe we store that\nstructure in some larger structure, and maybe that’s stored in some\nlarger structure.\n\nWell, if you do that the naïve way in OCaml, every time you store one\nstructure inside of another, it’s stored by a pointer. And it means\nnow when I wanna access the price of this trade that’s all the way\ndown (it’s a nested structure) I have to follow this pointer, then\nthat pointer, then that pointer, then that pointer, each time I have a\ncache miss, each time I have to load into memory. This is really bad.\n\nRight. And this highlights a kind of basic fact about OCaml, that it\nhas a very fancy, at least on the scale of modern language that people\nmight use, a very fancy type system and a brutally simple memory\nrepresentation. OCaml’s internal implementation might have been like a\nScheme or Lisp written in the mid-‘60s or something, which is to say,\neverything looks like either some immediate value, it’s no bigger than\na word that fits in a register, or a heap-allocated value, which is\nlike one header word and then one full word for every slot inside of\nit, and we have all the fancy, like, type goo on top of it that you\nwant, but all of the physical representation looks just like that.\n\nExactly. Exactly. It means that we’re building this beautiful type\ndiscipline in our software to try to avoid errors, and try to express\nourselves, and try to communicate more clearly among developers—and\nwe’re paying for that at runtime! This is ridiculous. To me, every\ntime a type system feature slows down your program or otherwise\ninterferes is just a misdesign. And so, the fact that this is our\ncurrent state of play is problematic. The task I’m working on is\ncoming up with a new design for OCaml’s type system, or this aspect of\nOCaml’s type system, that allows us to have this rich type structure\nwithout paying for it at runtime.\n\nThat’s right. Although it’s not that you can pick a type structure\nthat’s kind of oblivious to the performance details, right? In some\nsense it’s the opposite. The idea of the unboxed type work and a lot\nof the work that we’re doing now is really about taking design\ndecisions that have to do with performance and making them explicit at\nthe level of types.\n\nYes, that’s right. Because you can’t, I mean, in the end, we need to\nexecute this somehow. There needs to be an execution model. And so, if\nwe try to ignore all of these execution details in the type system,\nthen there’s not gonna be a way of making it fast. And so, the work\nis, in some sense, taking some of these execution-oriented details,\nmaking them manifest in the type system, getting programmers the\nability to write types that take these into account, so that we can\nthen execute fast.\n\nRight. In some sense, I think of this as really being about enabling\nperformance engineering, right? There’s kind of two different lines of\npeople thinking about how to optimize programs via compilers. One is,\nyou make your compiler smarter so it can do more and optimize things\nfor you. So the user can write kind of an ordinary program that just\nreflects, ‘what do they want to achieve and what’s the logical\nstructure,’ and then the compiler will, like, do something good. And\nthen, the other direction is trying to give a lot of precise\nunderstanding to the programmer so they can know exactly what they’re\ndoing and they can optimize their program. And these are both\nimportant.\n\nBut, like, in some sense the scale of optimizations are very\ndifferent. Like, if you come up with a way of making your compiler\nfaster that, like, takes most user programs and makes them 20% faster,\nthat’s an enormous win. Like, that’s a shockingly good\noutcome. Whereas, if you give people good performance engineering\ntools, the idea that they can think hard about a particular program\nand make it five, or 10, or 100 times faster is like, in some sense,\ntotally normal. And so, there’s a kind of extra power of giving\nindividuals the ability to optimize. It obviously misses out on scope\nand there’s something great about being able to go and in one swell\nfoop (laughs) make 20 different, or 100 different, or all the\ndifferent programs faster. But you can’t get the same kind of order of\nmagnitude of improvements that you can get from giving one programmer\nthe ability to really carefully engineer their system.\n\nWell, there’s a real tension because what in the end we’re doing in\nprogramming languages is, we’re trying to find a way for humans to\ncommunicate precisely to computers. And we have at our disposal our\ntool, one of our tools is the fact that this is happening on a\ncomputer. So we can write computer programs to try to sort of help\ncover this distance, right?\n\nIn the early days, humans were just completely at a disadvantage and\nthe humans wrote in assembly code. And this was kind of terrible. And\nwe’ve come some way since then, and so now instead of writing sort of\ninstruction-by-instruction that’s gonna be executed by the processor,\nwe can write in a high-level language and have a compiler translate\nthat high-level language to something that the computer can\nexecute. Hopefully along the way, it can actually make that\nfaster. But sometimes, we’ve gotten ourselves sort of too\nhigh-level. If we abstract away too many of the details of execution,\nit’s exactly as you were saying. It means that the programmer can no\nlonger do the performance engineering. And so, we need to expose just\nenough to the programmer so that now there’s some more details that\nthey can think about. Now they can do that performance engineering.\n\nBut still, I think the burden sits on the compiler to take those few\nhints that the programmer gives to make blazingly fast code come out\nthe other end. And it’s just really about this interplay between how\nmany details do we wanna expose? Certainly not all the details because\nthat’s too hard for the programmer. But we need to expose some. And\nso, it’s about, over time, programming languages developing more and\nmore features that sort of figure out the right line and the right\nbalance between this desire for high-level languages, closer to human\nthought, and low-level details, easier to execute.\n\nSo, before when we were talking about dependent types, we were talking\nabout how it’s important to have this kind of pay-as-you-go\nphenomenon, like, it gets harder as you try and get more power. It\nfeels like the same kind of thing shows up here.\n\nSo, when you try and get more control over the shape of data in the\nway that you described, what are the trade-offs? And how do you think\nabout designing the language in such a way that you don’t have\nwhatever extra complexity is there infect everything that you do?\n\nWell, we have to be really careful. So, with unboxed types, one of the\nchallenges is gonna be that unboxed types interfere with\npolymorphism. So, in a language like OCaml, you can write a function,\nsay, that computes the length of a list. And that’s gonna work over a\nlist, no matter what the list contains. And so the way that we say\nthat, is that in the type of the length function, there’s a type\nvariable. We say that it’s an ‘alpha list’ where alpha can stand for\nany type that you might wanna put in there. Once we introduce unboxed\ntypes, that no longer works out. Some of these types won’t be suitable\nreplacements for alpha. And so it means that, by having these unboxed\ntypes in the system, now this really powerful feature of polymorphism\nbecomes constrained. And so, algorithms that people are used to being\nable to use everywhere, no longer work so well.\n\nSo one of the challenges that we have is, how can we recover that? How\ncan we still keep this nice feature of polymorphism and without having\nunboxed types interfere? And so, one of the things that we’re thinking\nabout doing is some amount of essentially what could become runtime\ncode generation. It comes down to building experience with the\nfeature. As we start understanding this language feature better, we’re\ngoing to create programs using these unboxed types, recognize areas in\nwhich the current programming paradigms that we engage in no longer\nwork, and then figure out ways around them. It’s this give and take as\nwe’re doing the language design.\n\nAnd just to focus in on this code generation thing for a second, the\nbasic issue here is that the way you make in OCaml today, separate\ncompilation of work, and make the ability to write a function in one\nplace and use it polymorphically on lots of different types is, again,\nyou’re just kind of taking advantage of this rock stupid memory\nrepresentation. Like, what’s a list? A list is a collection of\nheap-allocated values, every one has, like, two slots. One is a place\nto put the data and the other is a place to have a pointer to the next\nthing on the list. And when you iterate over it, that thing always has\nthe same shape, it always has the same number of bytes, and so you\ncould just write one piece of code that kind of uniformly walks over\nit. And what happens in the unboxed types world is, suddenly you want\nthe system to contemplate multiple different possible shapes of the\ndata. And so, kind of at a mechanical level, you need different\nmachine instructions in order to do the walking of the data structure,\ndepending on what that structure is.\n\nAnd so, that’s how you might end up with wanting code generations. The\nway to kind of claw back the polymorphism is to generate the\ncode. Maybe you can generate the extra code at compile time, maybe you\nhave to generate the extra code at runtime. But you somehow now have\nto generate code for multiple different scenarios that essentially\nrepresent different physical shapes of the data.\n\nExactly. And right now, our best thought is that this will happen at\ncompile time, that we’ll be able to figure out what code we need to\ngenerate at compile time. That will prevent a few programming idioms\nthat someone might potentially want. I don’t think anyone will ever\nwant those (laughs) idioms in practice so I think we’ll be able to get\naway with it. But it does mean we’re thinking carefully about this\ndesign and it’s a hard thing. I’m not aware of another language that’s\nreally tackled the problem in the way that we expect to tackle it over\nthe next few months.\n\nSo another language I think is interesting to compare to, in all of\nthis, is Rust.\n\nSo Rust is a language which tries really hard to do two things at the\nsame time which are not trivial to do at the same time. One is to give\nstronger sense of type safety so that you can write programs that\nyou’re confident aren’t going to crash, so you get lots of protection\nfrom bugs from the type system. And also give the user a lot of\ncontrol over low-level details over how memory is managed. And we’re\nessentially trying to push OCaml in some sense at that high level of\ndescription in the same direction.\n\nHow do you think the Rust approach differs from the approach you see\nus trying to take with OCaml?\n\nThis is a good comparison. This takes us a little bit away from\nunboxed types and polymorphism because I think the way that that\nappears in Rust is quite different. But the notion of this sort of\nfiner level of control is definitely accurate. But Rust gives up\nsomething really big to do what they do. They give up garbage\ncollection. Within Jane Street and generally in programming, I think a\nlot of the time garbage collection is fantastic, right? It allows us\nto program without worrying about these low-level details that for\nmost programs, we don’t need to.\n\nSo, for instance, when I’m writing the OCaml compiler—which is written\nin OCaml, of course—I do want it to be performant, but I don’t really\ncare about microsecond latency. And so, I’m happy to write my compiler\nin a way that allocates a bunch of memory and then when it’s done with\nthat memory, the garbage collector comes and gets rid of it. This is\nreally, really convenient. In Rust, we don’t have that. In Rust, we\nare forced to think about memory, allocation, and deallocation at\nevery point in time. And this gives us this fine level of control, but\nit comes at a real cost, and it means now we’re sort of fighting\nagainst this ‘pay-as-you-go principle’ that we were talking about\nearlier. In Rust, for memory, you don’t pay as you go. Everyone has to\npay all the time for Rust fine memory control. So we wanna keep the\nhigh-level, garbage-collected OCaml for most applications, and then\nwhen we really want to have this fine level of control, we wanna have\nthat, too.\n\nAnd so, some of the work that we’re doing is around adding that finer\nlevel of control, but just where we need it.\n\nSo, one thing I wonder is whether this pay-as-you-go principle really\napplies in general or whether it’s like itself a kind of trade-off? I\nthink, if you’d asked the question of like, ‘well we should think of\ntypes in general as pay-as-you-go.’ Maybe your baseline should be by\ndefault untyped, and then when you want some extra control, you should\nadd types to it and lock it down more.\n\nThat seems in principle reasonable. But all the languages that I’ve\ndealt with that have pay-as-you-go types or what you might call\ngradual type systems, actually inherit quite a lot of complexity and\nproblems from that choice. It’s often simpler and better, at least for\nsome kinds of problems, to have a uniform approach where you sort of\nfigure out what trade-offs you wanna do and you kind of apply those\ntrade-offs uniformly. And I think, at least from my perspective, the\nbase choice you make in a language like Haskell or OCaml of like,\n‘actually, even though it has some ups and downs, we’re just going to\nhave types everywhere and that’s just how it’s gonna be,’ I think is\nat least a reasonable place in the trade off space of designs.\n\nAnd I wonder if the same is true about this kind of thing about\npay-as-you-go control? Whereas, I can imagine for some kinds of\napplications, you would want the property that, ‘yeah, I am explicit\nabout memory everywhere. I kind of have a hard guarantee that anywhere\nI look I have all of this extra information.’ I don’t know. How do you\nthink about this question of where pay-as-you-go is the right way or\nthe wrong way to think about designing something?\n\nI think that’s a great question and I agree. I mean, pay-as-you-go\ntypes, people have wanted that for some time. It hasn’t ever really\nhappened. And, I mean, you could almost say that C++ and C have\npay-as-you-go types and that they have type systems, but the type\nsystems are sort of completely broken.\n\n(laughs) Pay-as-you-go but then what do you get? I like that.\n\nWell, right, but, I mean, just to clarify for our listeners. What I\nmean by ‘completely broken’ is that at any point in time in C and C++\nyou can just say, ‘Oh, ignore the type system. And I’m gonna just take\nthis thing and change its type to be something else.’ And the\nlanguages, you know, they take this stance of, trust the programmer,\nthat when the programmer decides to do this, that this is a good\nidea. But we lose the guarantees that one would want from a type\nsystem like that when you dereference a pointer that there’s actually\nsomething there when you dereference it.\n\nSo, getting back to memory control, I like the idea of pay-as-you-go\nthere, or maybe it’s not even pay-as-you-go as much as a language that\ncan have multiple different modes of use with very easy interop. In\nthat we can have a system where there’s not fine memory management,\nthat you just sort of use the current OCaml’s system of uniform memory\nrepresentation, and then another part where maybe this particular team\nor this particular file says, ‘No, no. Here I really want careful\nmemory management.’ This is a different kind of pay-as-you-go. It’s\nsort of at the file level or something like that, or the package\nlevel, I suppose. And then as long as one piece of code can easily\ncall the other, that also works well.\n\nWe also see this kind of idea coming out in polyglot systems where we\nactually have multiple languages all calling one another because maybe\nthis part of the system is easier to write in some language, that part\nof the system is easier to write in some other language. It’s all a\nway of, these sort of different approaches to attack the same problem\nthat we don’t wanna have all of this complexity. We want to sort of\nnarrow down what complexity we have, where.\n\nRight. So there’s almost a notion of different dialects within the\nsame language?\n\nWith a really high quality FFI that really captures all of the\nimportant details so you can kind of interoperate, right? In fact, the\nwork of polyglot stories are actually really challenging to\nengineers. There’s like, a ton—\n\n… of hard work. I mean, we’ve done a bunch of work trying to get the\nPython-OCaml interop story working just right. We’ve made good\nprogress there but it’s actually quite challenging. And I feel like\nthe work is quadratic in the number of different (laughs) languages\nthat you need to hook together ‘cause it’s often unique, weird issues\nthat come with every pairing of things that you wanna make all one to\nthe other.\n\nI think that’s right. And that’s one of the appeals of having one\nlanguage that maybe is flexible enough to operate in multiple\ndifferent modes. So, in particular, in our design for unboxed types,\nwe are planning to make a box operation explicit. And the box\noperation is going to be ‘the spot’ that allocates memory. And so,\nnormally, you’ll be able to get uses of this box operation for\nfree. Programmers won’t ever notice it, they won’t write it. It just,\nall the existing code will have all the boxes inserted. But it would\nbe really easy to imagine some kind of compiler setting that turns\nthat feature off. Meaning that every time you wanna allocate memory,\nyou have to explicitly ask for it.\n\nSo this doesn’t bring you all the way to Rust, in that there’s still a\ngarbage collector operating, you still don’t have to manually\ndeallocate memory. But if I wanna write a program that absolutely,\nwhere every allocation is known, we haven’t designed this out. Maybe\nwe’re gonna go there, maybe we’re not gonna go there. Nothing is for\ncertain. But it turned out that in our design, it just naturally fell\nout that we could just add this compiler flag that just turns off\nboxing and it would be super easy to implement and we could experiment\nto see how easy it is to work with.\n\nSo, here we’re talking about a lot of very ambitious changes to\nOCaml. One uncomfortable fact about all of this is, like, we don’t own\nOCaml, we are not the (laughs) primary upstream developers. Every time\nwe wanna make a change to OCaml, there’s a bunch of people who we have\nto convince that it’s a good change. So one of the things that you’re\nthinking about in particular is, the kind of relationship of the work\nthat we’re doing here and our connection to the larger OCaml\ncommunity. Say a little bit more about how you think about this whole\nprocess.\n\nSo, I wanna start by pushing back against ‘unfortunate fact.’ I don’t\nsee that as an unfortunate fact at all. So OCaml was born as a\nresearch project out of Inria in France and still is there. That’s\nsort of its beating heart. And we are really significant benefactors\nof that work that others have done, that others continue to do. And\nso, Jane Street has chosen to work in part of this open source\nlanguage community where we’re taking others’ ideas and we’re\ncontributing our ideas back.\n\nSo in this podcast and in our work, we have these grand ideas. ‘Oh,\nwe’re gonna add this to OCaml. We’re gonna do this, we’re gonna do\nthat. Of course, what I really mean is, we’re going to experiment with\nthis idea internally. And then as that experiment unfolds, as we gain\nexperience and become more sure that it’s a right—I shouldn’t say a\nright idea. There’s many right ideas. But as we gain more confidence\nin a particular design, we can then work with the rest of the OCaml\ncommunity to try to make this part of everyone’s OCaml and not just\nJane Street’s.\n\nAnd in that way, we’re giving back to that community. At the same\ntime, we’re continuing to reap rewards from other work happening in\nthat community. So the biggest example of which is the multicore\nsupport that’s coming in OCaml 5. Still gonna be some time before Jane\nStreet’s ready to upgrade to OCaml 5. But that was a huge pile of work\ndone mostly outside, or maybe entirely outside of Jane Street’s walls,\nand we benefit by being part of this open source ecosystem. And so, I\nthink that this is a really great place to be where we’re\nparticipating, we’re getting new input of technical content, like\nmulticore OCaml, as well as design ideas. And I’m even thinking of\nsomething just a few weeks ago where I found an infelicity in the\ncompiler, made an improvement, pushed that upstream, and then we got\nfresh ideas from folks outside of Jane Street’s walls about, ‘Here’s\nactually an even better way to do it,’ and that was great. And then we\ncould incorporate that here. Without being part of this open source\necosystem, we wouldn’t have gotten that insight and we would be poorer\nfor it.\n\nSo, in the Haskell world, you spent a lot of time working on exciting\nnew type system features for Haskell and now you’re doing, in\nsubstance, similar kinds of work here. But I think that work flow is\npretty different, right? I think then, in the Haskell world, it was\nlike the primary work and ideation and evaluation was all deeply\nintegrated from the beginning in the open source world. And here,\nwe’re doing this work where we’re still open source, still, you know,\nyou can go look at the results on GitHub. But we are mostly iterating\ninternally, and then, over time, as we gain experience, as we gain\nmore confidence that the things we’ve built are good ideas, working to\nsee what subsets of these we can get upstreamed. I’m curious how that\nhas changed your feeling about the work, how it changes the texture of\nthe work that you do?\n\nSo working within the context of Jane Street gives us a lot more\nopportunity for experimentation. So a real big challenge of doing\nlanguage design is that, by necessity, you come up with an idea and\nmaybe you think it’s a good idea and you experiment with it on a few\nsmall programs on your own machine or you get your friend to sort of\nexperiment. You bounce the idea. Maybe, you know, you’re even\nproactive and you convene a committee of experts on this idea and now\nyou have, you know, five people thinking really hard about this one\nidea and trying to come up with the absolute best design.\n\nBut then, once you come up with the design, if you’re just in an open\nsource environment—without the context of a place like Jane Street—you\ndevelop the idea, implement it, and release it, and then now you have\nthousands or hundreds of thousands of programmers using it, and maybe\nit was a bad idea. But by the time you discover that it was a bad\nidea, it’s a little bit too late because maybe you have 90,000\nprogrammers who think it’s a bad idea, but you have a thousand who\nthink it’s a fantastic idea and will be very, very, very upset if you\nbreak their programs by changing it. And now you’re in a bad way.\n\nAnd even the people who think it’s a bad idea, who would like to get\nrid of it, don’t necessarily want you to break their programs in the\ninterim before they’ve (laughs) stopped relying on the feature.\n\nThat’s right. And so, it’s really, really hard to make changes to a\nproper open source programming language. In the context of Jane Street\non the other hand, it’s—I don’t wanna say dead easy. There’s work\ninvolved, but it is tractable and somewhat easy in that we can develop\nan idea, push it out there, get other programmers in the firm to start\nusing it, and then as that happens, we can say, ‘Mm. This isn’t quite\nworking out the way that we thought it would.’ That, you know, theory\nmeets reality, and theory loses.\n\nAnd so, then we can make a change to the feature and we have sort of\nan operation that we use internally called a ‘tree smash,’ where some\nJane Street engineers work together to come up with a big diff that\nhappens to all of Jane Street’s code, all at once, and we can change\nthe spelling of a keyword if we want to. And in fact, we’re imagining\nthis now. So Jane Street has been working on a feature called ‘local\ntypes’ or ‘stack allocation.’ These two things are kind of the same\nthing. And we’re realizing that one aspect of the design was just a\nbit wrong. And there’s already a ton of code written using the old\ndesign, but we’ve invented a new keyword and a new place for the\nkeyword to appear, and all this stuff.\n\nAnd so, it’s gonna be some work to fix it, but there’s no part of us\nthat’s saying, ‘Oh no. Now we need to think about a migration plan and\nwe need to make sure that no one is too unhappy with this.’ We’re just\ngonna go do it. And so, it means by the time we get to upstreaming,\neverything is battle-tested. And so, it just increases the level of\nconfidence in the design when we go through that process.\n\nOne of the concerns I could see people having about this kind of\nprocess is, it’s going to do a lot of training on, ‘What are the\nrequirements within Jane Street?’ But there are all sorts of ways in\nwhich you could imagine the requirements in Jane Street and outside of\nJane Street being different. I’m curious how you think about the\ndesign process in a way where you end up with language features that\nare likely to be of quite general utility, ‘cause I think, in the end,\nthe only ones you’re gonna get accepted upstream are the ones that\npeople think are more broadly useful than just being useful for us.\n\nSo that’s indeed a challenge, right? It’s a challenge of overfitting,\nright? We have a very particular style of code within Jane Street and\na very particular use case that we’re working on. I think that is sort\nof a harder question to answer in some sense. I think one way that we\naddress that is by seeking outside feedback from the very\nbeginning. So, the plan that we have for unboxed types, another Jane\nStreet engineer, Stephen Dolan, he made a presentation about this\ndesign for unboxed types, I think in 2019. We’ve been thinking about\nthis for a long time. The local types and stack allocation feature,\nthis has also been featured in a number of presentations that we’ve\nmade to the OCaml community, out in public.\n\nAnd so, by incorporating that process early, we can get broad\nfeedback. That’s not the kind of feedback that tells us, ‘Oh, should\nthe keyword go here, or should it go there?’ Right? That’s the kind of\nthing that we develop internally. But I think that, by giving these\npresentations, by involving the broader community, even from the very\nbeginning, that helps to prevent some of this overfitting problem.\n\nSo, another interesting aspect of your background is, you’ve spent a\nlot of time, years really, working in the Haskell community and\nworking on Haskell. And now, you’re working on a totally new\nprogramming language using it both as the surface language in which\nyou write your programs, and also helping to design that language. I’m\ncurious, what struck you about difference? I’m curious, what are the\ndifferences that have struck you about Haskell and OCaml at multiple\ndifferent levels?\n\nIt’s been really fun over the past, I guess, six months now being part\nof these two language communities. So, I should say, I’m still quite\nactive in the Haskell world. I have not left the Haskell world. I’m\nliving both Haskell and OCaml on a daily basis. So let me think of a\ncouple of interesting points of comparison.\n\nSo, one is, it seems kind of simple, but actually I think it affects\nthe way that we program. The approach toward interfaces. In Haskell,\nyou tend to write your type signatures sort of right next to the\nfunction and Haskell uses a lot of type signatures. OCaml doesn’t so\nmuch. And so, OCaml there’s these separate interface files where you\ndefine the interface to your whole module, but it’s kind of apart and\nit means, in my experience, looking at a bunch of OCaml code. OCaml\ncode tends to be rather less documented than similar Haskell code, and\nI think that’s in part because in Haskell, you put your types right\nthere and that’s a form of documentation. And once you put your types\nin, then you can also put more documentation there and it becomes part\nof the habit of the Haskell programmer to put those comments right in\nthere.\n\nA flip side of this, is that Haskell has this feature called\ntypeclasses. And typeclasses allow you to use essentially one\nfunctioning to mean a variety of different things. OCaml doesn’t have\nthat feature. And so, what that can mean sometimes is that Haskell\ncode can become quite a bit harder to understand because if you have a\nbunch of these function names, any of which can mean a variety of\ndifferent things, all strung together, then it takes a lot of work on\nthe part of the reader to try to understand what on earth is going on\nthere. And that problem just doesn’t arise in OCaml. OCaml is much\nmore explicit about what operations it’s taking.\n\nSo maybe I can even, taking these two examples, generalize that a\nlittle bit and say, I find Haskell to be somewhat more explicit about\nwhat’s happening at compile time. Whereas, OCaml is somewhat more\nexplicit about what’s happening at runtime. So another place where\nthat comes into play is that, the Haskell compiler—and this connects\nwith a topic we were talking about earlier—the Haskell compiler\noptimizes much more aggressively than the OCaml one does. And that’s\nbecause in Haskell, Haskell’s a lazy language which means that if you\nsay, ‘Let X equal some big computation,’ we’re not gonna compute the\nbig computation until we need the value of X. In OCaml, if you say,\n‘Let X equal a big computation,’ we just compute that thing right\naway.\n\nAnd so, that means, on the one hand you might say, ‘Oh, well maybe\nHaskell is more efficient,’ because maybe some code path never uses\nthe value of X and so it means that we can discard that whole big\nthing. But it also means that it’s much harder to predict in Haskell\nhow fast your program is going to run or what its performance\ncharacteristics are gonna be. In OCaml, it’s much easier to predict\nthat. You can just sort of read your file top to bottom and, within a\nfunction, it just performs the operations that you see roughly in the\norder that you see them. And that means, OCaml again is more explicit\nabout what happens at runtime, but without some of the type\nannotations that we see in Haskell, a little less explicit about\nwhat’s happening at compile time.\n\nYou mentioned a couple of things that are different that I wonder if\nthey’re really language differences, or just cultural differences. If\nyou look at the tendency to do fewer type annotations on the\ndefinition of functions, you could put more type annotations. And I\nthink there are some people who write OCaml who do, but there’s\ncertainly a tendency not to and I think the Jane Street house style\ncertainly is one that does not have lots of type annotations, outside\nof interface files where those are essentially required everywhere.\n\nAnd the documentation thing, too, like, the interface files are a very\nnatural place to put documentation and I am constantly dissatisfied\nthat I think we put way too little documentation in the (laughs)\nplaces that we could. And I don’t know that there’s anything exactly\nabout the language that forces that. Like, I’m not sure that I buy\n‘the need to put type annotations right on functions’ is the thing\nthat really makes that difference.\n\nThat may be true. It’s hard to say. I do think that form follows\nfunction, function follows form a little bit, in that, when you’re\nused to describing the specification of a thing right next to that\nthing, that, to me, is going to encourage you to write\ndocumentation. Whereas, in OCaml, you write the specification over\nthere, in that other file, nowhere near where you write your\nimplementation. And sometimes I think that means that you’re less\nlikely to put those comments on the implementation when you need them.\n\nIt’s funny. My intuition is the opposite of, like, ‘Oh, I’ve written\nthis very bare interface file that just has types,’ and I think\nempathetically about the person who’s reading it. It’s like, ‘How will\nthey have any idea what this means? I had better put a comment here.’\n(laughs)\n\nRight. So the comment and the interface files are good, but I think\nit’s not just that. I mean, and this could also be, I have a bias in\nthat, most of the OCaml that I’m reading and writing is in the OCaml\ncompiler itself. Maybe this is different than everything else at Jane\nStreet. In fact, I’m sure it is. And it has been striking for me after\nspending so many years working in Haskell’s compiler, that Haskell’s\ncompiler has many, many, many more comments than OCaml’s compiler\ndoes. Maybe I’m overfitting on this particular piece of software\ninstead of just the language in general.\n\nI do think looking at the innards of a compiler is always a weird\nplace to think about how one should write code for the\nlanguage. Because, like, one of the facts about OCaml—as it is often\nthe case for many languages—is that OCaml was written by people who\ndidn’t yet really know how to program in OCaml kind of by\ndefinition. And lots of the code that’s there now has been there for a\nlong time and you can sorta see that. I think if you went to those\nsame people today and been like, ‘How would you write this now?’\nThey’d all be like, ‘Oh, really very differently.’ But, you know,\nparts of it just kind of haven’t been rewritten and have the shape\nthat they did years ago and are harder to understand than kind of\nanyone would exactly like it to be today.\n\nAnd to be fair, that exact same thing is true in the Haskell\ncompiler. There are stretches of that that no one would write it that\nway today. But it was written that way before anyone really knew how\nto program in Haskell.\n\nSo another difference that’s always struck me about Haskell is the\ndifferent approach to language extensions. And I think about this in\ntwo different ways. One is, OCaml is just massively more\nconservative. There was a period in time early in OCaml’s development\nwhere you could almost see the process of Ph.D. students\ngraduating. It’s like someone graduates, they write some language\nfeature, and it gets added to the language. And that stopped pretty\nearly in OCaml. OCaml got pretty conservative about making changes. It\nwas like, ‘No, no, no, we’re gonna add things that are like, really\ngood, and it was pretty restrictive about what would go in. And\nthroughout that time, it kind of had one language, pretty much. OCaml\nworked in one particular way and that was kinda that.\n\nHaskell on the other hand, has had a lot of glorious experimentation\naround language features and those experimental language features were\noften hidden behind pragmas, right? There’s lots of different things\nyou could turn on and off. And so there kind of isn’t one Haskell or\ntwo Haskells, but there’s like 10,000 different Haskells, depending on\nwhich collection of flags you wanna turn on or turn off. You have 10\ndifferent pragmas and now you have two of the 10 different possible\narrangements of the language features. I’m curious how you think about\nthose different sets of choices?\n\nYeah. I mean, that’s a great question. So I do have to push back\nagainst your sense of scale. So Haskell has, when I last counted, and\nthis was a few years ago, 150 or so extensions. So Haskell has\nactually 2150 languages.\n\nOuch, indeed. So I think that this comes from the origin of Haskell\nversus the origin of OCaml so actually you probably know more about\nthe origin of OCaml than I do. But I know Haskell was started by\ncommittee. There was a committee that met starting in, I think, 1989\ninto 1990, coalescing these ideas around a lazy functional programming\nlanguage. And then this eventually became a Haskell standard and there\nwere multiple different compilers for Haskell that all implemented the\nsame standard Haskell. And then maybe one compiler would think, ‘Ooh,\nwouldn’t it be fun if we had X?’ And so, then they would add that\nfeature, but everyone still wanted to make sure that it was a Haskell\ncompiler and not this other special language based on Haskell\ncompilers. So there was an idea of language extension. So you could\nhave base Haskell plus these various extensions.\n\nSo, in other words, OCaml is like Python and Haskell is like Scheme?\n\nSure. Yes. In that OCaml has a manual for the language, but there is\nnot a standard. There are not multiple different software artifacts\nthat all compile some OCaml language that exists beyond a single\ncompiler. But, in any case, going back to Haskell, that was the early\ndays of Haskell and there were multiple Haskell compilers. As time has\ngone on, all of the other ones have either died off or there are still\na few other Haskell compilers out there, but there’s not really an\nattempt to stay current with the newest features. And there’s really\nnow just one: the Glasgow Haskell Compiler, or GHC. And that’s the one\nthat, when I say that there’s 150 language extensions, the GHC is\nsupporting that.\n\nAnd so, when you have multiple different things, if you wanted to find\na common core that all of these different compilers can interop with,\nit makes a lot of sense to have individual extensions when you say\nyou’re deviating from this common core. Now there’s really just one\ncompiler.\n\nI think that the current system, the current plan at Haskell, is not\nvery well motivated. And instead, the feedback that I hear from users\nis that they find the whole language extension system very heavy and\nit means that, to get much done in Haskell, you need to now start your\nfile with the list of 20 extensions that you’re using. And newcomers\ncan’t tell the difference between brand new experimental extensions\nthat might change from one compiler to another, or extensions that\nhave been around for 20 years, or there are some extensions that have\nbeen around for 20 years but actually we know are dangerous and you\nreally shouldn’t use.\n\nAnd when I say, ‘We know,’ I mean, like, I know this and a couple of\nother Haskellers know this, but someone just picking up the language\nwon’t know that. And I personally find that quite problematic. And so,\nthere’s a debate going on right now within the GHC Steering Committee\nwho is a body chosen to sort of help evaluate the evolution of the\nlanguage to reimagine all of this. And I’m hoping that we end up with\na structure that looks quite different from what we have today, while\nstill retaining backward compatibility.\n\nSo if you have your 20 language extensions at the top of your file,\nthat will continue to work in this new vision. But I am hoping that we\ncan reorganize it in a way that’s a little bit more user-friendly.\n\nDo you think in the end it would make sense to move toward something\nwhere there’s closer to one standard language that includes most of\nthe extensions, and so that most users end up not thinking about that\n‘two to the whatever’ configuration space?\n\nI do think settling on one standard language would be good. I think we\ncan get 80% of the way there and go from 150 extensions to maybe seven\nor something like that. I don’t think we’ll be able to get all the way\nthere because there’s just too many different use cases for Haskell,\nthere’s too many different people who have slightly different ideas of\nwhat that core should be. So if we just sort of got rid of the\nextensions mechanism, I think that that would cause too much of a rift\nin the community and probably not be tenable. But we can get close.\n\nSo, throughout this conversation, I feel like there’s been a\nparticular kind of vision and idea of what programming is that’s\nmotivated a lot of your thinking. And that has something to do with\nhaving a language that’s simultaneously expressive, it lets you say\nwhat you wanna say, but also has real tools for reasoning, in both\ntype systems and, you know, more sophisticated mode-dependent types,\nor essentially automated tools for reasoning about your program. So in\nthe last, I don’t know, few months, couple of years, there’s been an\nenormous amount of motion in a totally different direction for making\nprogramming better, which is various AI-assisted ways of making\nprogramming easier. With all of this kind of recent work on large\nlanguage models, things like ChatGPT, Codex, all of that, making an\nenormous difference to how people are programming on a regular\nbasis. I’m curious how you think about this and how it fits into your\nbroader way of thinking about programming?\n\nI see this evolution of sort of AI-assisted programming as not quite\nthe sea change that maybe others have seen it to be. I see it as a big\nstep. It’s not a small step change. But it doesn’t remove the need to\ncommunicate precisely. In that, to me, the interesting thing about a\nprogramming language is that it’s a mode of communication that is\nprecise. In a sense, that’s almost the definition of what makes a\nprogramming language a programming language, as opposed to some other\nkind of language. There’s a precise semantics to everything that is\nsaid in that language. And that language is used as a medium of\ncommunication, both from human to computer. That’s how we often think\nof it. But also from human to human.\n\nWith the advent of AI-assisted programming, now we have sort of a new\nmethod of communication in that it’s a communication from computer\nback to human. In that, you might have something like ChatGPT\nproducing the code, but a human still has to read that code and make\nsure that it does what you think it does. And as a medium of precise\ncommunication, it’s still very important to have a programming\nlanguage that allows that communication to happen.\n\nAnd so, this new mode of communication is going to put different\npressures on language design than what we’ve had in the past. But I\ndon’t think it removes the need for programming language design. But\nlike I said, it does create different pressures and these have been\nevident for some time in that one little maxim I’ve had in my head for\nyears now about language design is that you wanna optimize for\nreading, not for writing. Code gets read much more often than it gets\nwritten.\n\nAnd so, when designing a new language feature, maybe you’re thinking,\n‘Okay, I could do this very cleverly with, like, a nicely placed\ntwiddle in this spot in the code. And I could use that twiddle to mean\nsomething very important about my program. And maybe that’s very\nconcise, but it’s hard to read, it’s hard to search for, it’s hard to\ntrain new people to understand. Instead, it tends to be better to\nwrite out words in your design and programming language, that’s easier\nto search for, and easier to learn, and just easier to organize in\nyour brain. If we end up getting to a mode where humans are doing even\nless and less writing, and computers are doing more and more of it,\nthen those pressures just increase. And we wanna make the language\neasier to read and maybe even harder to write. Maybe we even get to a\npoint where if computers are doing all of the code generation, it’s\njust kind of, you know, symbolic and structured. It’s not just text.\n\nSo we get these richer programming languages but I just don’t see it\nas, there was yesterday and then there’s tomorrow and it’s gonna be\ntotally different and let’s just start from scratch. I see this as\njust one phase in evolution of this precise communication medium.\n\nOne of the things that’s really striking about these AI assistants, at\nleast in their current form, is they’re really surprisingly powerful,\nright? They can really do a kind of stunningly good job of taking a\nquery and understanding it and providing a piece of code that tries to\ndo what it is that you asked for it to do.\n\nBut they’re also strikingly fallible. They make lots of mistakes. And,\nyeah, the reading thing is pretty important because you ask ChatGPT\nor, you know, whatever your favorite large language model is, to write\na piece of code and look at the result. And it’s super easy for the\nresult to just be wrong in all sorts of different ways. And I can sort\nof see the rising requirement of making things more readable. I’m\ncurious how you think things like dependent types fit into this story?\n\nSo, dependent types in their essence are a way of writing precise\nspecifications. So the sorting example that I gave earlier, I\ndescribed that, you know, you could have a function instead of going\nfrom ‘a list of ints to a list of ints’ to something more\nglorious. Well, before dependent types, or you know, language without\ndependent types, you could just do that in comments. But probably in\nyour comments, you’re not going to be as precise as you could be if\nyou really wrote it in types, right? You say, you know, return to the\ninput list in ascending order, or something like that. But what does\nthat really mean?\n\nWe need a language where we can precisely specify what this function\nis doing. So as I said earlier, ascending isn’t quite right. I mean,\nif you write in a comment, ‘This returns the input list in ascending\norder,’ well, what if the input list has duplicates? Now you can’t\neven meet that specification. Where actually, that specification\ndoesn’t make any sense at all because the input list isn’t in\nascending order. What does that mean to say to put the input list in\nascending order? That’s a contradiction in terms, because the input\nlist isn’t in ascending order.\n\nAnd so, there’s some amount of interpretation that humans can provide,\ninterpretation that a large language model can provide, but in the\nend, it’s gonna come down to precise communication and\nspecification. And so, where do dependent types fit in? Well, they can\nbecome the input to the large language model. Maybe now we have\nprogrammers who are well-versed in precise communication of\nspecifications, not implementations, and we can say, ‘ChatGPT, give me\na function of this type.’ And then you have your nice type that takes\nan input list, and it returns an output list that is a permutation of\nthe input list in non-decreasing order and we specify that.\n\nAnd then now, that gives a nice specification for ChatGPT to go off\nand say, ‘Okay, here’s an implementation that meets that. Oh, and by\nthe way, it has this asymptotic running time because I’m a nice large\nlanguage model and I’m gonna tell you that.’ And then, now we actually\nhave a model by which we can check that the result meets its\nspecification if it is in this precise language of dependent types.\n\nAlthough I have to say, I think I am more pessimistic about the idea\nof people actually going off and writing down the specifications\neven. ‘Cause those are actually pretty hard to write.\n\nI wonder if a more plausible model is, you go to your large language\nmodel and say, ‘Please write me a specification for a function that\nsorts a list.’ And then it, like, spits something out. And then you\nlook at it and think, yeah, that seems about right. And then you go\nfrom there to next stages. But the role that this kind of concise\nspecification has, it’s like a smaller and maybe more readable thing\nthat you can use as part of verifying whether the later pieces in fact\ndo what you think they’re supposed to do.\n\nYes, but I think there’s a real danger here. I agree with what you say\nin that, writing a precise specification is hard. And humans are\nlazy. So if it’s hard, we’re gonna ask the computer to do it. If we’re\nasking the computer to write the specification of the function that\nit’s going to write, now we’re getting into thin ice. Because if we’re\nworking in a system where the human is not expert enough to be able to\nread the specification and know that it’s the right specification, now\nwe’ve lost. Now the computer can just sort of go off and do whatever\nit wants and we have no way of knowing whether it’s right or wrong\nbecause we’ve lost that ability to sort of have that precise\ncommunication.\n\nI think one of the tricky things here is, people are often pretty bad\nat the reading part of this, right? The idea if something gets written\nand now you have to read it, especially if it wasn’t written in a\nreally clear way, reading some piece of code that is not really\nwell-structured is actually really challenging. Like, one of the\nactually most important parts of the process of code review I find, is\nnot so much that, like, one person writes the code and someone else\nreads it to validate that it’s done the right thing. It’s more like,\nsomebody writes the code and the other person says, ‘Oh, man. This is,\nlike, too messy for me to even read.’ And then there’s some kind of\nback and forth process where you try and get something that’s actually\nsimple enough to be understood. And I think it’s an interesting open\nquestion of, like, as we start using more of these AI assistants, how\ngood they will be at generating things that are actually simple.\n\nAlthough, if we have precise specifications, the simplicity of the\nimplementation becomes less important.\n\nThat’s fair. I mean, in some sense there’s a kind of open\ntechnological question of, like, how good will large language models\nbe, acting as tactics for theorem provers?\n\nRight. Yeah. I mean, but if we’re in an environment where there is a\ntight correspondence between the specification and the implementation,\nand what I mean by ‘tight correspondence’ is this dependent type’s\nmodel where you can—and there’s other models, it’s not just dependent\ntypes, there’s refinement types, there’s other ways of checking an\nimplementation against a specification. But if we’re in an environment\nwhere we can check an implementation against a specification, the\nspecification is simple enough and communicated precisely whether\nthat’s in very careful natural language or probably not just natural\nlanguage ‘cause natural language is very, very squishy. But probably\nin some specification language, you could call that specification\nlanguage ‘dependent types’ if you want or you could call it something\nelse. But if we can communicate that specification, check the\nimplementation against the specification, now suddenly there’s not all\nthat much incentive to ever read the implementation.\n\nRight. It becomes a little bit like, you know, reading the output of\nthe compiler.\n\nYeah. It’s interesting. I do wonder whether a whole potential kind of\ndirection for innovation here, which you more or less have pointed in\nthe direction of, is designing languages that work usefully in this\nkind of intermediate things to generate in the process of working with\nsome kind of AI assistant. The thing that’s generated as an\nintermedium really affects the overall process of working on the\nsystem and generating confidence it’s doing the right thing. It’s not\na totally new problem, I guess, in the context of language design. But\nit does put new kinds of pressure and maybe changes the trade-offs\nthat show up.\n\nYeah, that’s right. I mean, you know, it’s too early, I think, to\nstart to design a programming language around this kind of\ninteraction. We’ve only been having these interactions with these\nassistants for a few months. And they’re changing fast. So I think now\nwould be the wrong time. In two years, maybe one could start thinking\nabout designing a language from scratch to do this. That sounds\nplausible to me. But I think a good starting point would be languages\nthat we have now because they’re designed for precise\ncommunication. That’s, in the end, what we need. And, again, maybe\nthere’s less pressure on the writing piece and more pressure on the\nreading piece. But I don’t think that they’re so divorced from each\nother that it means that we throw out everything we have and start\nfrom scratch. That seems, to me, pretty unlikely.\n\nThat makes sense. Well, thanks so much for joining me. This has been a\nlot of fun.\n\nYou’ll find a complete transcript of the episode, along with links to\nsome of the things that we discussed at signalsandthreads.com. Thanks\nfor joining us and see you next time.",
    "readingTime": 65,
    "keywords": [
      "jane street",
      "ghc steering",
      "steering committee",
      "street’s walls",
      "ron discuss",
      "joined jane",
      "street’s tools",
      "i’m curious",
      "dramatis personae",
      "tight correspondence"
    ],
    "qualityScore": 1,
    "link": "https://signalsandthreads.com/future-of-programming/",
    "thumbnail_url": "https://signalsandthreads.com/static/images/twitter/future_of_programming.png",
    "created_at": "2026-01-09T00:58:47.386Z",
    "topic": "tech"
  },
  {
    "slug": "catnip-run-claude-code-from-your-iphone-using-github-codespaces",
    "title": "Catnip – Run Claude Code from Your iPhone Using GitHub Codespaces",
    "description": "Like catnip, a highly addictive agentic coding tool - wandb/catnip",
    "fullText": "wandb\n\n /\n\n catnip\n\n Public\n\n Like catnip, a highly addictive agentic coding tool\n\n catnip.run\n\n License\n\n Apache-2.0 license\n\n 402\n stars\n\n 32\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n wandb/catnip",
    "readingTime": 1,
    "keywords": [
      "catnip",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/wandb/catnip",
    "thumbnail_url": "https://opengraph.githubassets.com/d032d2bab84800534c687d9264960e30ed62e372d7f32babbf8b3506627fd360/wandb/catnip",
    "created_at": "2026-01-08T18:16:54.144Z",
    "topic": "tech"
  },
  {
    "slug": "grief-leverage-and-the-future-of-manual-coding",
    "title": "Grief, leverage, and the future of manual coding",
    "description": "I’m a software engineer and product maker based in Cracow, Poland. My mission is to create useful products by writing high-quality code and sharing my knowledge throughout the journey.",
    "fullText": "For the last couple of years, watching the software industry has been an emotional experience for me. And will I remember the winter of 2025/2026 - the time when apparently everyone and your mum discovered just how good in coding the Claude Opus 4.5 is - as the culmination of that period, and one of the hardest and most confusing times for me as a professional.\n\nOn one hand, there's anxiety. The pace of change is brutal. New tools appear every week, workflows emerge almost overnight. Wondering where this is all is heading, and whether there will still be a place for us on the other side.\n\nI felt strange hearing from industry thought leaders like Andrej Karpathy, saying that English is now the new programming language. I had spent years honing the craft of writing in programming languages - so what was I supposed to make of that, if it was now being treated as commoditized?\n\nAfter months of thinking about that, I had identified the feeling - grief. Grief for manual coding.\n\nFor years, my identity as an engineer was tightly coupled to the artifacts I produced. My repositories. My components. My abstractions. The code was mine, and that ownership mattered. It was craft and it was personal. Like a carpenter remembering how it felt to build a specific chair.\n\nThat mental model no longer holds for me. How could it, when everyone now can one-shot a todo app for themselves, just like a PowerPoint presentation?\n\nOn the other hand, there's excitement. I got into coding because I wanted to create worlds. Websites, apps, flows, solutions, interactions between users, beautiful and unique things.\n\nNow the ceiling of building is so high that we cannot even see it. Ideas that felt too expensive or too complex are suddenly within reach. It's not an understatement to say things that used to take weeks and months now may take hours. I'm not saying that it's easy to do it and everyone can make it happen, but it's certainly possible.\n\nThe new leverage comes from moving one layer up. Instead of doing every task ourselves, we design systems that can do them for us. A single agent can now execute autonomously what used to require long hours my time, freeing my attention for higher-level decisions, design, judgment. And maybe just... enjoying the life with my loved ones?\n\nThis all immensely increases the leverage of single engineer. Technically, you're one well designed system away from solving a daunting problem for the first time, from building life-changing startup, from building your dream game. Of course, it's still hard and rare, but pre-agentic coding it was often not possible at all - you had to broke the concrete walls of thousands of lines of code first.\n\nI mentioned that everyone with AI can one-shot a personal to-do app now. Engineers with AI can create much more that. They can design systems that scale, adapt, and solve problems in ways that were previously out of reach, everything under strict engineering discipline - secure, cheap and efficient.\n\nThat's why, overall, I am cautiously optimistic on what the future holds.\n\nAnd it seems like - as buzzwordy and cliché as it sounds - that the future is agentic.\n\nThe word agentic gets thrown around a lot, so it's worth grounding it. I will use the definition I personally In practice, the systems that actually work for me tend to follow the same five core steps:\n\nPoints 2, 4 ad 5 can be somewhat \"recursively\" executed by agents - it's not hard to imagine agents implementing the system by writing specs or veryfing the outputs for other agents.\n\nPoints 1 and 3 are uniquely human - we decide what we want to exist, and we trigger the execution process. And while point 3 - triggering the execution - also can be run by agent, I keep it in this category because someone is at the end is responsible for what the agents did, and in that sense it is uniquely human.\n\nThis shift doesn't mean the broad engineering skill is obsolete. It means where that skill applies has changed.\n\nTo orchestrate agents that produce valid code, I still need to understand:\n\nAnd what is even more important:\n\nInstead of applying that knowledge and intent manually in code, I encode it into the system that produces the code. It produces the code in indeterministic way, mind you, and potentially on much broader scale. We don't know yet which scale we talk about. 2x? 10x? Maybe more? We will see.\n\nAnd there's whole new class of problem to solve. How do I ensure models doesn't produce unexpected or harmful results? How do I coordinate several, dozen, and more models to work together? How do I ensure AI runs on prem and we don't share our precious data with anyone?\n\nIn other words, the craft moves up a level.\n\nInitially, I wanted to include \"the end of manual coding\" in this post's title. But I changed it to \"the future manual of coding\".\n\nFirst, I didn't want to sound clickbaity. Now, seriously - manual coding isn't gone. It still has a very important place. Best professionals always understood different abstraction levels, not only the highest one. Code is runtime, and you have to understand runtime through and through. Apart from that - it's still important in learning, personal work, and honing the cognitive skills. I especially believe in the last one, because delegating so much mental work we used to do before will take a tool on our thinking in a long term.\n\nBut it's no longer the default path to producing value and to economic leverage. The role of software engineers is shifting:\n\nWe're no longer producing artifacts (code). We're designing systems producing artifacts.\n\nOnce you accept that, a lot of confusion from the last couple of years disappears. The grief is still there, but it's quieter. And there is something else: the clarity, and a sense that this change, while uncomfortable, is also full of possibility.\n\nOne important note: no, all of this doesn't mean succumbing to AI slop. We're still responsible for everything our agents produce - every single line of it. And no, it doesn't mean letting AI write sloppy LinkedIn posts or Slack messages is suddenly acceptable. If anything, the opposite: write your damn words yourself, please. Do not delegate your thinking.\n\nThere's a whole lot of skill to designing the agentic systems including mastering specs engineering, enforcing constraints, output verification, model evaluation and so on. This is the obvious thing to focus on first.\n\nMany people dreamed of living in times with real blank spots on the map, to be able to discover the unknown themselves. Software engineers in 2026 have the privilege of experiencing this. And it can be a source of risk, as well as an economic leverage.\n\nWe've always been good at automating and learning new things. Now there's even more to automate and learn. Our focus should shift from manual coding to designing systems that produce code at scale - while remembering that manual coding got us here, and still matters, just for different reasons.\n\nI’m a software engineer and product maker based in Cracow, Poland. My mission is to create useful products by writing high-quality code and sharing my knowledge throughout the journey.",
    "readingTime": 7,
    "keywords": [
      "uniquely human",
      "economic leverage",
      "producing artifacts",
      "software engineers",
      "design systems",
      "designing systems",
      "manual coding",
      "code",
      "it's",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://www.tymzap.com/blog/grief-leverage-and-the-future-of-manual-coding",
    "thumbnail_url": "https://www.tymzap.com/api/og?title=Grief%2C%20leverage%2C%20and%20the%20future%20of%20manual%20coding&token=37cac97264905c6d7b412cbf3c96ddd0a5309cbd4603ad51e8c444cd8be1e1e5",
    "created_at": "2026-01-08T18:16:49.915Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-reshuffles-teams-to-bolster-github-as-ai-coding-and-agent-wars-heat-up",
    "title": "Microsoft reshuffles teams to bolster GitHub as AI coding and agent wars heat up",
    "description": "GitHub, the dominant software development platforms, is responding to the rise of AI coding services and AI agents",
    "fullText": "Microsoft wants to overhaul GitHub to compete with AI coding rivals and embrace AI agents, and the company has started reshuffling teams to make that happen, according to people familiar with the changes.\n\nGitHub is a leading software development platform that Microsoft acquired in 2018. GitHub had an early lead because of its popularity as a place to store code. Lately, though, GitHub has faced more competition from AI tools such as Cursor and Anthropic's Claude Code.\n\nMicrosoft in January 2025 formed a new group focused on building AI tools under ex-Facebook engineering boss Jay Parikh. The group, called CoreAI Platform and Tools, combined Microsoft's developer division, AI platform team, and GitHub.\n\nStill, Microsoft and GitHub have remained somewhat separate, and the company has been moving people and resources around over the past few months to better coordinate efforts such as sales, one of the people said. The latest change, happening this week, is moving a small group of Microsoft engineers over to GitHub.\n\nThe goal, the people said, is to better compete with AI coding tools that rival GitHub Copilot, while getting in the race to build AI agents and fulfill Parikh's vision to build an \"agent factory.\"\n\nIn an internal meeting late last year, Parikh spoke about needing to overhaul GitHub to compete with Cursor and Claude Code, according to audio reviewed by Business Insider.\n\n\"GitHub is just not the place anymore where developers are storing code,\" Parikh said at the time. \"We want it to be the center of gravity for all of AI-powered software development.\"\n\nMicrosoft wants GitHub's AI tools to be available wherever developers work, not just inside one app, to wants to make GitHub a kind of dashboard for managing multiple AI agents.\n\nThe latest changes are also part of what Parikh said would be new investment in improving the basic parts of GitHub. In the meeting, Parikh said those include making improvements to its GitHub Actions tool that automates building, testing, and deploying code, analytics and insights tools so teams can see how their code is performing, security for keeping the code safe, and making sure the company can meet local data storage rules to offer GitHub in new countries.\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 2,
    "keywords": [
      "claude code",
      "software development",
      "overhaul github",
      "microsoft",
      "compete",
      "agents",
      "coding",
      "teams",
      "latest",
      "developers"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/microsoft-github-reshuffle-ai-coding-agents-2026-1",
    "thumbnail_url": "https://i.insider.com/695c2f3b832e0ef1ead73181?width=1200&format=jpeg",
    "created_at": "2026-01-08T12:25:17.155Z",
    "topic": "finance"
  },
  {
    "slug": "shelley-a-coding-agent-for-exedev",
    "title": "Shelley: A Coding Agent for Exe.dev",
    "description": "Shelley is a coding agent. Contribute to boldsoftware/shelley development by creating an account on GitHub.",
    "fullText": "boldsoftware\n\n /\n\n shelley\n\n Public\n\n Shelley is a coding agent\n\n License\n\n View license\n\n 6\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n\n boldsoftware/shelley",
    "readingTime": 1,
    "keywords": [
      "shelley",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/boldsoftware/shelley",
    "thumbnail_url": "https://opengraph.githubassets.com/7ed82b94ad70d681d82be48a6262464b62b37be56f5134d540f14cd6ffc21d8c/boldsoftware/shelley",
    "created_at": "2026-01-08T06:19:39.654Z",
    "topic": "tech"
  },
  {
    "slug": "flashinferbench-building-the-virtuous-cycle-for-aidriven-llm-systems",
    "title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-Driven LLM Systems",
    "description": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.00227",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-07T18:19:21.595Z",
    "topic": "tech"
  },
  {
    "slug": "the-risks-of-prioritizing-shortterm-revenue-over-customer-fit",
    "title": "The Risks of Prioritizing Short-Term Revenue Over Customer Fit",
    "description": "Most startups measure success through top-line revenue growth. But beneath vanity metrics often lurks a hidden threat: sales debt. Like technical debt in software development, sales debt accumulates when companies prioritize short-term revenue over customer fit, creating financial drag, operational burdens, and strategic distractions that can undermine long-term success. In this article, we leverage our experience as sales leaders as well as a series of in-depth interviews to describe what sales debt is, how it impacts companies (for better and worse), and how leaders can address it.",
    "fullText": "The Risks of Prioritizing Short-Term Revenue Over Customer Fit by Eric Janssen, Brian Denenberg and Benson P. ShapiroJanuary 7, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintA few years after its founding, a B2B software startup we interviewed reached coveted unicorn status, with more than $200 million in funding and a valuation of over $1 billion. But beneath the surface, substantial issues threatened their long-term success.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/01/the-risks-of-prioritizing-short-term-revenue-over-customer-fit",
    "thumbnail_url": "/resources/images/article_assets/2026/01/Jan26_07_DianaBolton.jpg",
    "created_at": "2026-01-07T18:19:12.066Z",
    "topic": "business"
  },
  {
    "slug": "a-popular-chinese-chatbot-told-a-user-their-coding-request-was-stupid-and-to-get-lost",
    "title": "A popular Chinese chatbot told a user their coding request was 'stupid' and to 'get lost'",
    "description": "A popular Chinese AI chatbot snapped at a user over a coding request, prompting an apology from its parent company Tencent.",
    "fullText": "A Chinese AI chatbot embedded inside the country's most widely used app briefly went off the rails, snapping at a user.\n\nTencent's AI assistant, Yuanbao, which is built into WeChat — China's dominant super app used daily by tens of millions of people — called a user's coding request \"stupid\" and told them to \"get lost,\" according to screenshots shared on Chinese social media platform RedNote.\n\nThe incident surfaced on Friday after a user identified only by the handle \"Jianghan\" posted screenshots of their interaction with the chatbot on RedNote. Jianghan had been using Yuanbao to debug and modify a piece of code when the AI suddenly began responding with hostile messages.\n\nIn one exchange, the chatbot dismissed the request as \"stupid\" and told the user to \"get lost.\" It said: \"If you want an emoji feature, go use a plugin yourself.\"\n\nThe user had asked Yuanbao to fix a bug that caused an emoji or sticker feature to stop responding to double-clicks, and requested functional code to resolve the issue.\n\nTencent's YuanBao later responded directly under the user's post, apologising for what it described as a \"negative experience.\" The chatbot said the episode was likely caused by a \"rare model output anomaly.\"\n\nBased on a review of system logs, the responses were not triggered by the user's actions and did not involve any human intervention, Yuanbao said. It added that it had launched an \"internal investigation and optimisation process\" to reduce the likelihood of similar incidents occurring again.\n\nThe original RedNote post by Jianghan has since been deleted. Screenshots of the exchange continue to circulate on RedNote, as seen by Business Insider on Tuesday.\n\nThe incident comes as Chinese regulators step up scrutiny of AI systems.\n\nChina released draft measures last week aimed at governing \"human-like\" interactive AI services, including chatbots and virtual companions.\n\nIn a statement, the Cyberspace Administration of China said Beijing encourages innovation in \"human-like\" AI, but will put guardrails in place to \"prevent abuse and loss of control.\"\n\nWei Sun, the principal analyst for AI at Counterpoint Research, told Business Insider that the draft measures send a signal that Beijing wants to speed up the development of human-like AI interactions, while keeping them regulated and socially acceptable.\n\nChina's AI industry has continued to move at a rapid pace since the start of 2026.\n\nLast week, DeepSeek, one of the country's most closely watched AI startups, published research outlining a new training approach intended to make large models easier to scale. Analysts told Business Insider the technique, known as \"Manifold-Constrained Hyper-Connections,\" or mHC, stood out as a \"breakthrough\" in model design.\n\nThe South China Morning Post reported on Tuesday that DeepSeek has updated the interface of its flagship chatbot model, introducing an enhanced \"thinking\" mode.\n\nThe updates have fuelled expectations that the startup could be laying the groundwork for the release of its next major model.",
    "readingTime": 3,
    "keywords": [
      "draft measures",
      "business insider",
      "chatbot",
      "user",
      "model",
      "user's",
      "screenshots",
      "human-like",
      "country's",
      "request"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/chinese-ai-chatbot-tencent-yuanbao-wechat-user-rednote-2026-1",
    "thumbnail_url": "https://i.insider.com/695ca67b64858d02d217cc17?width=1200&format=jpeg",
    "created_at": "2026-01-06T12:24:19.586Z",
    "topic": "sports"
  },
  {
    "slug": "ai-wont-cook-coding-jobs-netflix-engineer-says",
    "title": "AI won't cook coding jobs, Netflix engineer says",
    "description": "Netflix staff engineer Anthony Goto said he's constantly asked by recent graduates how AI will change the industry.",
    "fullText": "Thanks to AI, everyone will be able to code. But a staff engineer at Netflix said that he tells recent grads that doesn't mean their job prospects are hopeless.\n\n\"We're going to see some amazing things, but our hunger for more functionality, more apps, more ecosystems is just gonna get higher, and higher, and higher,\" Anthony Goto said in a recent TikTok video. \"So, in the end, I think this is gonna be another, essentially, level of programming language, a high-level programming language.\"\n\nNew grads ask me all the time if AI means software engineers are done. Especially those preparing for Netflix interviews. This fear has happened before. AI is another layer of abstraction, not the end of engineering. #Netflix #NetflixInterview #SoftwareEngineer #TechCareers #AI\n\nGoto, who has 15 years of experience his time at Netflix and Uber alone, said that AI-related worries are among the top concerns he hears when he talks with recent graduates or employees he's mentoring.\n\nAnd to be fair, there is no shortage of takes about the future value of computer science degrees and the overall worth of coding knowledge, given the rapid advancements of agentic AI tools like Anthropic's Claude, which has led to \"vibe coding.\"\n\nOne way to stay competitive, Goto said, is for newer engineers to make sure they learn System Design.\n\n\"System Design is exactly what I am trying to ensure newer engineers get a handle on,\" he said. \"In the future, we may likely end up wielding system design like a tool.\"\n\nGoto points to the video game industry as an example of what's to come. Rapid advancements since the introduction of Doom in 1993 have spawned an industry that rakes in over $100 billion and regularly draws on Hollywood talent for its biggest releases.\n\n\"Picture someone from the year 2000, 2010, they go back in time, they go to John Carmack, and they say, 'Guess what? In the future, we're gonna have these things called video game engines,\" Goto said.\n\nLast year, Carmack, a video game legend who was the lead programmer of Doom, said that software progress has made some of the early grunt work he did \"as irrelevant as chariot wheel maintenance.\"\n\n\"Game engines have radically expanded the range of people involved in game dev, even as they deemphasized the importance of much of my beloved system engineering,\" Carmack wrote on X in April 2025.\n\nGame engines are now so powerful that they are used to create immersive digital sets and environments, such as those featured in Disney's hit series \"The Mandalorian.\"\n\nGoto admits that his prediction could very well be inaccurate, but based on the trajectory of past technological advancements, he sees a clear need for engineers.\n\n\"We've seen this many times before, where we abstract things away in a really powerful way,\" he said. \"And what it really does is democratizes the process.\"",
    "readingTime": 3,
    "keywords": [
      "system design",
      "programming language",
      "rapid advancements",
      "newer engineers",
      "game engines",
      "higher",
      "grads",
      "we're",
      "another",
      "software"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/netflix-engineer-ai-jobs-future-coding-2026-1",
    "thumbnail_url": "https://i.insider.com/695c0fa9832e0ef1ead72c64?width=1200&format=jpeg",
    "created_at": "2026-01-06T12:24:19.427Z",
    "topic": "finance"
  },
  {
    "slug": "are-you-a-software-engineer-tell-us-what-you-think-about-vibe-coding",
    "title": "Are you a software engineer? Tell us what you think about vibe coding",
    "description": "Vibe coding has upended software engineering, strapping developers with a suite of new AI tools. We want to hear from those navigating the moment.",
    "fullText": "Software engineering is changing — and we want to hear from those navigating the moment.\n\nProgrammers today find themselves with a whole new suite of AI tools, from Claude Code to Cursor to Codex. These editors enable engineers to generate entirely artificial lines of code or modify their handwritten code with the assistance of a large language model.\n\nThere's a term for this type of AI-assisted programming: \"vibe coding.\"\n\nEngineers from Meta to Google are embracing a vibe coding approach in their day-to-day work. Everyone, from teenagers to non-technical workers, suddenly seem to be building their own apps — or at least vibe-coding their way to a prototype.\n\nIt's a whole new skill set for engineers to learn, though, one that can vary from tool to tool. (Replit is different from Lovable, which is different from Bolt, and the list goes on.) It's also not clear, for the most experienced programmers, whether there are actually productivity gains.\n\nAndrej Karpathy coined the famous term \"vibe coding\" early last year. He was a founding team member of OpenAI and led AI efforts at Tesla. In a recent X post reflecting on the field, Karpathy wrote that he had \"never felt this much behind as a programmer.\"\n\n\"I have a sense that I could be 10X more powerful if I just properly string together what has become available over the last ~year,\" he wrote. \"A failure to claim the boost feels decidedly like skill issue.\"\n\nAre you a programmer? Answer our vibe-coding survey below:",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "engineers",
      "programmers",
      "vibe-coding",
      "it's",
      "skill",
      "tool",
      "programmer",
      "code",
      "karpathy"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/software-engineeer-developer-vibe-coding-survey-2026-1",
    "thumbnail_url": "https://i.insider.com/6957d78b04eda4732f2e607b?width=1200&format=jpeg",
    "created_at": "2026-01-05T12:25:15.967Z",
    "topic": "tech"
  },
  {
    "slug": "when-stdsharedmutex-outperforms-stdmutex-a-google-benchmark-study",
    "title": "When std:shared_mutex Outperforms std:mutex: A Google Benchmark Study",
    "description": "In multi-threaded programming, protecting shared resources is crucial. std::mutex is common, but may bottleneck performance when many readers access data. std::shared_mutex can optimize scenarios w…",
    "fullText": "Comment \n\n Reblog\n\n Subscribe\n\n Subscribed\n\n Tech For Talk \n\n Already have a WordPress.com account? Log in now. \n\n Tech For Talk \n\n Subscribe\n\n Subscribed\n\n Log in\n\n Copy shortlink\n\n Report this content \n\n View post in Reader \n\n Manage subscriptions\n\n Collapse this bar",
    "readingTime": 1,
    "keywords": [
      "subscribe subscribed",
      "tech",
      "talk"
    ],
    "qualityScore": 0.4,
    "link": "https://techfortalk.co.uk/2026/01/03/when-stdshared_mutex-outperforms-stdmutex-a-google-benchmark-study/",
    "thumbnail_url": "https://techfortalk.co.uk/wp-content/uploads/2026/01/create-a-highly-detailed-high-resolution-image-focused-on-a-side-by-side-20.png",
    "created_at": "2026-01-04T12:21:15.996Z",
    "topic": "science"
  },
  {
    "slug": "mainframestyle-channel-controllers-for-modern-disaggregated-memory-systems",
    "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory Systems",
    "description": "Despite the promise of alleviating the main memory bottleneck, and the existence of commercial hardware implementations, techniques for Near-Data Processing have seen relatively little real-world deployment. The idea has received renewed interest with the appearance of disaggregated or \"far\" memory, for example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of Near-Data Processing is a major barrier to adoption of the technology. Inspired by the channel controllers which interface the CPU to disk drives in mainframe systems, we propose memory channel controllers as a convenient, portable, and virtualizable abstraction of Near-Data Processing for modern disaggregated memory systems.\n  In addition to providing a clean abstraction that enables OS integration while requiring no changes to CPU architecture, memory channel controllers incorporate another key innovation: they exploit the cache coherence provided by emerging interconnects to provide a much richer programming model, with more fine-grained interaction, than has been possible with existing designs.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2506.09758",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-04T06:18:06.222Z",
    "topic": "tech"
  },
  {
    "slug": "this-17yearold-wrote-the-ultimate-eli5-for-coding",
    "title": "This 17-year-old wrote the ultimate ELI5 for coding",
    "description": "Aryan Poduri's book, \"GOAT Coder,\" teaches children how to code through hands-on exercises and uncomplicated explanations.",
    "fullText": "Learning a new language can be tricky, especially when it involves a combination of letters, numbers, and symbols that combine to form computer code.\n\n\"When you're first understanding the concept of what goes behind screens, it's hard to wrap your head around,\" Aryan Poduri, a 17-year-old based in California, told Business Insider.\n\nSo, Poduri wrote a book to demystify the versatile programming language known as Python. Called \"GOAT Coder,\" it teaches children how to code through hands-on exercises and uncomplicated explanations.\n\nAlthough coding education is becoming increasingly accessible, there are still gaps in connecting with young students. In a 2023 research report supported by Apple, surveyed students said being unsure of where to start is the most significant barrier to learning how to code.\n\n\"There are all these different careers. Firefighters, police, scientists, but software engineering is something that you might not think about,\" Poduri said. \"It's important for people to know that coding is a big marketplace and it's an option.\"\n\nPublished in April, the book has sold over 2,000 copies to date.\n\nGrowing up, Poduri initially had dreams of becoming a professional athlete, but technology was a constant in his life. He grew up in Mountain House, which is about two hours east of Silicon Valley in San Francisco.\n\n\"That definitely had an impact because, around here, everyone is coding,\" Poduri said.\n\nBoth of Poduri's parents work in the tech industry and started teaching him how to code at a young age. That sparked an early curiosity. He sharpened his skills through a beginner's Python class in elementary school and by picking up tips from YouTube videos.\n\nHis interest in coding became more serious in middle school. Around that time, Poduri developed his first computer program: Voto Polo, a web app he and his friends used to create and vote on polls.\n\n\"That's when I realized this could be a future for me,\" Poduri said.\n\nHis early coding days led him to compete in hackathons and an internship at DataDios, a data and AI company based in San Francisco. The idea for \"GOAT Coder,\" though, came while watching his parents try to teach his elementary school-aged sister how to code.\n\n\"My parents were spending a lot of time just trying to help her understand,\" Poduri said. \"I realize it's because there aren't really many resources.\"\n\nIt's a problem he also recognized after enrolling part-time at Stanford Online High School, which allowed him to take advanced courses that may not be available to the average student.\n\nPoduri self-published his book, which he said meant he didn't have to worry about meeting tight deadlines.\n\n\"The first challenge was figuring out how I wanted to teach it,\" Poduri said. \"It was almost like I had to make my own curriculum.\"\n\nHe also had to ensure that the concepts were explained in a clear and concise manner that a young child could grasp.\n\n\"It's been years since I started coding, so I didn't remember what I wanted to know or what really excited me,\" Poduri said.\n\nStill, he finished the book in about nine months. Poduri said he could have written it faster, but he had to juggle school and his extracurricular hobbies.\n\n\"I was having fun while doing it,\" Poduri said.\n\nAlthough Poduri is now focused on promoting \"GOAT Coder\" and his college applications, he's already thinking about his next steps, like diving further into the tech industry and software engineering.\n\nHe's already authored two research papers about DataDios' SmartDiff, which helps validate data across private and public cloud platforms.\n\nOutside of technology, Poduri said he'd like to explore other areas of the business world.\n\n\"One of my long-term career goals is to eventually become an entrepreneur,\" Poduri said. \"I don't want to be stuck to just coding. I really enjoy learning about new subjects and new things.\"\n\nHe hasn't ruled out writing a follow-up book, either.",
    "readingTime": 4,
    "keywords": [
      "san francisco",
      "software engineering",
      "tech industry",
      "goat coder",
      "coding",
      "it's",
      "book",
      "poduri",
      "learning",
      "parents"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/goat-coder-childrens-book-learning-to-code-python-aryan-poduri-2025-12",
    "thumbnail_url": "https://i.insider.com/694c30a804eda4732f2e1a0f?width=1200&format=jpeg",
    "created_at": "2026-01-02T12:22:20.834Z",
    "topic": "finance"
  },
  {
    "slug": "coding-dissent-art-technology-and-tactical-media-video",
    "title": "Coding Dissent: Art, Technology, and Tactical Media [video]",
    "description": "This presentation examines artistic practices that engage with sociotechnical systems through tactical interventions. The talk proposes a...",
    "fullText": "This presentation examines artistic practices that engage with sociotechnical systems through tactical interventions. The talk proposes art as a form of infrastructural critique and counter-technology. It also introduces a forthcoming HackLab designed to foster collaborative development of open-source tools addressing digital authoritarianism, surveillance capitalism, propaganda infrastructures, and ideological warfare.\n\nIn this talk, media artist and curator Helena Nikonole presents her work at the intersection of art, activism, and tactical technology — including interventions into surveillance systems, wearable mesh networks for off-grid communication, and AI-generated propaganda sabotage.\n\nFeaturing projects like Antiwar AI, the 868labs initiative, and the curatorial project Digital Resistance, the talk explores how art can do more than just comment on sociotechnical systems — it can interfere, infiltrate, and subvert them.\n\nThis is about prototypes as politics, networked interventions as civil disobedience, and media hacks as tools of strategic refusal. The talk asks: what happens when art stops decorating crisis and starts debugging it?\n\nThe talk will also introduce an upcoming HackLab initiative — a collaboration-in-progress that brings together artists, hackers, and activists to develop open-source tools for disruption, resilience, and collective agency — and invites potential collaborators to get involved.\n\nLicensed to the public under http://creativecommons.org/licenses/by/4.0\n\nThis Talk was translated into multiple languages. The files available\nfor download contain all languages as separate audio-tracks. Most\ndesktop video players allow you to choose between them.\n\nPlease look for \"audio tracks\" in your desktop video player.",
    "readingTime": 2,
    "keywords": [
      "sociotechnical systems",
      "open-source tools",
      "interventions",
      "tactical",
      "hacklab",
      "surveillance",
      "propaganda",
      "media",
      "initiative",
      "languages"
    ],
    "qualityScore": 0.85,
    "link": "https://media.ccc.de/v/39c3-coding-dissent-art-technology-and-tactical-media",
    "thumbnail_url": "https://static.media.ccc.de/media/congress/2025/2191-d743f89d-684b-5a29-a0e1-4b788caa4255_preview.jpg",
    "created_at": "2026-01-01T12:23:06.289Z",
    "topic": "tech"
  },
  {
    "slug": "the-best-way-to-use-mcps-with-coding-agents",
    "title": "The Best Way to use MCPs with coding agents",
    "description": "Description will go into a meta tag in",
    "fullText": "Jilebi makes it really easy to add MCPs by converting them to plugins that can then be used by your AI tool or agent. You can add MCPs with a single command like this \njilebi plugins add context7\n\nJilebi uses deno_core with help from rustyscript to sandbox and run plugins without giving them any access to your network, envs or file system. Any permission to use these resources needs to be explicitly allowed by the user during installation of the plugin\n\nFocus on tools, resources and prompts. Leave the server abstraction to Jilebi. Jilebi currently supports stdio, and will add support for SSE, HTTP and OAuth soon. Since its easy to make plugins, anyone can contribute, even AI tools. Jilebi can keep up with the MCP spec while you focus on your plugin\n\nGet answers to common questions about Jilebi and how it works",
    "readingTime": 1,
    "keywords": [
      "add mcps",
      "plugins",
      "jilebi",
      "resources",
      "plugin",
      "focus",
      "tools"
    ],
    "qualityScore": 0.55,
    "link": "https://jilebi.ai",
    "thumbnail_url": "https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg",
    "created_at": "2026-01-01T06:19:51.284Z",
    "topic": "tech"
  },
  {
    "slug": "meet-the-13yearold-and-his-teen-sister-vibe-coding-and-competing-in-cursors-24hour-hackathon",
    "title": "Meet the 13-year-old and his teen sister vibe coding and competing in Cursor's 24-hour hackathon",
    "description": "A 13-year-old and his teen sister picked up vibe coding and ended up competing together in a 24-hour hackathon with their dad.",
    "fullText": "Seasoned tech professionals filed into a weekend vibecoding class. Somewhere between them sat a 13-year-old quietly planning an AI sports coach.\n\nUsman Asif was the youngest person in the room. His 18-year-old sister, Shanzey Asif, was in the same class in Singapore.\n\n\"I was surrounded by people much older than me, with more experience in technology,\" he told Business Insider. \"But I felt age is just a number.\"\n\nOnce he started vibe coding, \"it was weird but fun,\" he said.\n\nThe family's AI journey started with their father, Asif Saleem, who works at Google as a financial services go-to-market lead for Japan and Asia Pacific. He learned about vibe coding, or prompting an AI to generate code, and was curious about the tools on the market.\n\nHe attended a local vibecoding class in June. By the end of the weekend, he had created a financial statement analyzer.\n\nUsman and Shanzey, who were already interested in AI, saw what their father did and wanted to be part of the class.\n\nA few weeks later, both siblings signed up for the same course their father had taken.\n\n\"It was pretty intimidating at first,\" Shanzey said. \"I didn't really know what vibe coding was.\"\n\nThe majority of attendees were executives. \"They already had kind of experienced what coding was, and some of them were like working in Google and Oracle, and even Amazon,\" Shanzey said.\n\nOnce the instructors broke down what vibe coding was, it became \"really simple\" — and fun — for Shanzey.\n\nThe 12th grader, who attends an International Baccalaureate program, initially thought she would have to write code, but learned that prompting is what drives the entire process.\n\nWhen her space website came together, she thought: \"This is great. I didn't even have to do any coding.\"\n\nUsman said his entry into vibe coding was bug-filled.\n\n\"It kind of drove me crazy because I did not know what to do,\" he said. Whenever he asked AI to fix a bug, it would generate another one.\n\n\"But it's like that, you know, one bug after another, then you get there,\" the teen said.\n\nWith practice, he learned what different bugs meant and how to get the AI to resolve them.\n\nBoth siblings sound like tiny product managers when talking about prompts, which they say are the backbone of the vibe coding process.\n\n\"Prompts are supposed to have good details and good information. You have to instruct the AI like a teacher to a student,\" Usman said.\n\nShanzey added that the very first prompt determines the direction of the app. She also said users can use the models to help craft better prompts.\n\nVibe coding didn't immediately become a family routine. With school, work, and exams, everyone tinkered with AI in their own time.\n\nAfter finishing their vibe coding class, Asif, Usman, and Shanzey decided to test their skills together at Cursor's 24-hour hackathon in Singapore. The October event drew hundreds of participants, mostly adults.\n\n\"Our only goal was to get out with a completed project,\" Shanzey said. \"Whatever happens after that was part of the experience.\"\n\nThe trio quickly settled on an idea that had begun as a dinner-table conversation: choosing Shanzey's future college.\n\nTheir project was an AI-powered university guidance counselor.\n\nThe family vibe coded for about 12 hours straight, then went home and returned the next morning to see the results. Each person played a crucial role: Asif drafted the first version, Shanzey refined the interface and layered in new features, and Usman mapped out the key elements for the demo video, which Shanzey then shot.\n\nAlthough they didn't win, the experience became one of the most memorable things they've done together.\n\n\"I was really, really happy with what we were able to achieve, with how Shanzey and Usman stepped up,\" Asif said. \"It was great fun.\"\n\nFor all the excitement around AI in the household, there are limits on how it's used — especially when it comes to schoolwork.\n\n\"When she's studying, she can't use AI for the content she's creating,\" Asif said of his daughter. \"That's super important because the schools will always validate the output produced.\" The same rule applies to Usman.\n\nAsif and his wife manage screen time and gaming with a reward system.\n\n\"If you want some me time or play time, that should come as a reward for achieving certain goals,\" like making breakfast, Asif said.\n\nWith those boundaries in place, the siblings said that coding with AI has taught them a few valuable lessons.\n\nFor Shanzey, the biggest one was the importance of structure. Giving the AI clear, organized instructions felt a lot like managing the various demands of school: exams, essays, activities, and volunteering.\n\nApproaching things systemically is often what leads to success, she said.\n\nUsman's takeaway was about depth. Vibe coding taught him that good results come from thoughtful, detailed responses, not shortcuts.\n\n\"There are no shortcuts to success,\" he said. \"You just have to do it the hard way and learn the hard way. I could also implement this into my daily life in school.\"\n\nUsman and Shanzey are sure AI will be part of their future.\n\n\"Regardless of what I end up doing, I think AI will always be a part of my life,\" Shanzey said. \"If I go into law or psychology or something like that, I think AI will be a huge contributor to that.\"\n\nThe 13-year-old sees potential in building apps full-time.\n\n\"I feel like I could have a career with AI, such as AI app building,\" he said. \"Hopefully, there's a bright future for me with AI.\"\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "vibecoding class",
      "vibe coding",
      "usman and shanzey",
      "usman asif",
      "didn't",
      "year-old",
      "experience",
      "father",
      "learned",
      "siblings"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/teens-vibe-coding-family-hackathon-cursor-ai-building-apps-steps-2026-1",
    "thumbnail_url": "https://i.insider.com/6942405e04eda4732f2da7ee?width=1200&format=jpeg",
    "created_at": "2026-01-01T06:19:45.170Z",
    "topic": "finance"
  },
  {
    "slug": "amazon-is-letting-visa-workers-stranded-in-india-work-remotely-as-long-as-they-dont-code-or-talk-to-customers",
    "title": "Amazon is letting visa workers stranded in India work remotely — as long as they don't code or talk to customers",
    "description": "Amazon allows India-based employees affected by H-1B visa delays to work remotely until March, with strict restrictions on coding.",
    "fullText": "Amazon is allowing employees who are stranded in India because of visa delays to work remotely there until early March, according to an internal memo viewed by Business Insider.\n\nThe catch: They're not allowed to code, make strategic decisions, or interact with customers.\n\nAmazon is one of many American companies scrambling to adapt to the Trump administration's rapid-fire changes to the H-1B visa program, including a mandate that consular officers must review visa applicants' social media posts before issuing visas. The additional screening has delayed processing, and some embassies and consulates have rescheduled visa appointments by several months, leaving some employees stranded outside the country.\n\nThe delays have prompted Google, Apple, Microsoft, and other companies to issue travel advisories in recent weeks, warning US employees with visas to avoid international travel to prevent extended stays outside the US.\n\nAmazon allows employees traveling abroad for visa renewals to work remotely for up to 20 business days, an exemption from the normal requirement that they work in their office five days a week. Now, any Amazon employee in India as of December 13 and who awaits a rescheduled visa appointment may work remotely until March 2, according to the memo, which was posted to Amazon's internal HR portal on December 17.\n\nThe permission to work remotely comes with a long list of constraints. Employees working remotely from India are barred from coding of any kind, including troubleshooting and testing. They cannot work from or visit Amazon buildings. And they cannot negotiate or sign contracts.\n\n\"All reviews, final decision making, and sign offs should be undertaken outside India,\" the memo says.\n\nThe memo also said that \"in compliance with local laws, there are no exceptions to these restrictions.\"\n\nThe memo does not provide guidance for employees whose visa appointments have been rescheduled beyond March 2, 2026, or for those stranded in a different country. Some US embassies and consulates have rescheduled appointments as far out as 2027.\n\nAmazon did not immediately respond to a request for comment from Business Insider.\n\nFor employees in technical roles, the restrictions raised questions about what work they can actually perform.\n\n\"Seventy to eighty percent of my job is coding, testing, deploying, and documenting,\" one Amazon software engineer told Business Insider.\n\nA State Department spokesperson in December told Business Insider that the purpose of the social media reviews is to use \"all available tools\" to flag visa applicants who are inadmissible, including those who pose a risk to national interests.\n\nThe delays pose a particular challenge for Amazon, which is among the largest users of the H-1B program. During the 2024 government fiscal year, Amazon filed 14,783 certified H-1B applications, including 23 for Whole Foods, according to Business Insider's analysis of publicly available data from the Department of Labor and US Citizenship and Immigration Services.\n\nHave a tip? Contact Pranav Dixit via email at pranavdixit@protonmail.com or Signal at 1-408-905-9124. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "social media",
      "visa applicants",
      "visa appointments",
      "rescheduled visa",
      "employees",
      "remotely",
      "memo",
      "india",
      "stranded",
      "delays"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amazon-visa-delays-prompt-india-remote-work-with-strict-restrictions-2025-12",
    "thumbnail_url": "https://i.insider.com/6955be9004eda4732f2e5935?width=1200&format=jpeg",
    "created_at": "2026-01-01T01:03:21.721Z",
    "topic": "finance"
  },
  {
    "slug": "navigating-ai-critical-thinking-in-the-age-of-llms",
    "title": "Navigating AI: Critical Thinking in the Age of LLMs",
    "description": "The author reflects on the evolving role of Large Language Models (LLMs) in coding and education, emphasizing their potential to assist rather than replace engineers. Critical thinking remains esse…",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://mcuoneclipse.com/2025/12/31/navigating-ai-critical-thinking-in-the-age-of-llms/",
    "thumbnail_url": "https://i0.wp.com/mcuoneclipse.com/wp-content/uploads/2025/12/ai-slows-down-experienced-open-source-developers.jpg?fit=1200%2C602&ssl=1",
    "created_at": "2025-12-31T18:17:22.087Z",
    "topic": "tech"
  },
  {
    "slug": "when-good-threads-go-bad",
    "title": "When good threads go bad",
    "description": "👋🏼 This is part of series on concurrency, parallelism and asynchronous programming in Ruby. It’s a deep dive, so it’s divided into several parts:\n Your Ruby programs are always multi-threaded: Part 1 Your Ruby programs are always multi-threaded: Part 2 Consistent, request-local state Ruby methods are colorless The Thread API: Concurrent, colorless Ruby Bitmasks, Ruby Threads and Interrupts, oh my! (Concurrent, colorless Ruby) When good threads go bad (Concurrent, colorless Ruby) Thread and its MaNy friends (Concurrent, colorless Ruby) Fibers (Concurrent, colorless Ruby) Processes, Ractors and alternative runtimes (Parallel Ruby) Scaling concurrency (Streaming Ruby) Abstracted, concurrent Ruby Closing thoughts, kicking the tires and tangents How I dive into CRuby concurrency  You’re reading “When good threads go bad”.",
    "fullText": "👋🏼 This is part of series on concurrency, parallelism and asynchronous programming in Ruby. It’s a deep dive, so it’s divided into several parts:\n\nYou’re reading “When good threads go bad”. I’ll update the links as each part is released, and include these links in each post.\n\nIt’s late, and you start getting alerts that requests to your web server are failing. You try to load a page and it hangs endlessly. The server isn’t responding to anything, and requests are continuing to queue up.\n\nIt’s 10PM. Do you know where your children threads are?\n\nNot knowing what else to do, you trigger a server restart. Even doing that, things remain unresponsive for another 30 seconds. Finally you see the server stop, and start up again. As if by magic, everything is running fine again.\n\nYou’re running Puma with threads. It seemed like every thread was unresponsive. What happened to those threads?!\n\nThe reality of the situation is probably mundane.\n\nIf it isn’t those things, there are other ways your threads can go rogue. Let’s look at some ways a thread can get stuck:\n\nThe conventional example of a deadlock is two threads attempting to acquire a mutexheld by the other thread.\n\nThey can never make progress, so they’re dead in the water:\n\nIn our example, thread_1 acquires mutex_1. Then thread_2 acquires mutex_2. Next, thread_1 attempts to acquire mutex_2 and blocks. Then thread_2 attempts to require mutex_1 and blocks. Neither can make progress, and are stuck in place.\n\nIt’s a traditional example, but Ruby is very good at detecting it! It detects the problem and raises an error, checking if any threads are capable of making progress:\n\nRuby detects that thread_1 and thread_2 are sleeping, and the third thread is Thread.main, which sleeps waiting for thread_1 to finish.\n\nMost long running programs are likely to have some other thread running, and Ruby only detects if all threads are stuck. We can get our deadlock example to work by adding an extra thread in a work loop:\n\nNow threads 1 and 2 will never progress, and Ruby lets the program continue running because thread_3 is still active.\n\nWhile a deadlock means a thread has stopped processing, a livelock happens when a thread keeps running, but never makes any progress. Here’s another example using an alternative approach for acquiring a mutex:\n\nThis is similar to our deadlock example, but this time we use #try_lock instead of #lock. Unlike #lock, which blocks until the mutex is available, #try_lock returns false if attempting the lock fails. We do a short sleep in each thread to give them time to acquire the initial locks, then iterate infinitely attempting #try_lock. The locks will never be acquired, and the loops will run forever. Burn, CPU, burn 🔥.\n\nPersonally I’ve rarely encountered deadlocks and livelocks in threaded code. But I’ve definitely encountered them in databases!\n\nOn PostgreSQL, it will detect this deadlock and raise an error:\n\nHow do we solve deadlocks and livelocks? The answer is a consistent order for locking.\n\nThis is the same example as before, but this time both threads attempt to acquire mutexes in identical order. As long as you acquire in a consistent order, you should never hit deadlocks or livelocks.\n\nAs far as I know, this isn’t actually possible in pure Ruby. What I mean by “pure” Ruby is a program that only runs Ruby code, and no C/Rust/Zig extensions. The CRuby runtime controls how pure Ruby code runs, and makes sure we can’t hog threads. Mostly.\n\nIn Bitmasks, Ruby Threads and Interrupts, oh my!, we dug into the TIMER_INTERRUPT_MASKand how it utilizes priority. It allows a thread to influence how large of a time slice it gets:\n\nAt a default priority of 0, we get Ruby’s default time slice of 100ms. At -2, we get 25ms. At 2, we get 400ms. This means in theory, can we starve out other threads by increasing our priority?\n\n3,276,800 milliseconds is 54 minutes. Can we really block things for 54 minutes!?\n\nThere’s a bunch going on here - but there are only a few details to focus on:\n\nAs we can see - the priority made a difference! The thread at index 5 runs around 7 times as much as every other thread. But, it does not fully hog the thread like we might have thought. This is only 10 seconds, well under 54 minutes, and the other threads still get prioritized by the scheduler.\n\nThis shows that we can influence the scheduler, but we can’t completely hog the runtime from pure Ruby code.\n\nIn more practical Ruby code, the Sidekiq gem gives you the ability to set priority on the threads it creates:\n\nInterestingly, by default, Sidekiq sets its threads to a priority of -1, which is less than the 0 that Ruby uses by default. It describes the rationale:\n\nRuby’s default thread priority is 0, which uses 100ms time slices. This can lead to some surprising thread starvation; if using a lot of CPU-heavy concurrency, it may take several seconds before a Thread gets on the CPU.\n\nNegative priorities lower the timeslice by half, so -1 = 50ms, -2 = 25ms, etc. With more frequent timeslices, we reduce the risk of unintentional timeouts and starvation.\n\nFascinating to see a real-world use-case of priority like this! Sidekiq has run trillions of jobs across hundreds of thousands of apps, and they made the decision to switch the priority.\n\nSince Ruby 3.4, you can achieve the same thing globally by using RUBY_THREAD_TIMESLICE. You can set RUBY_THREAD_TIMESLICE=50 and keep the priority the same, but now the time slice is 50ms.\n\nThis is the likeliest scenario that will saturate your threads: long-running IO. In Ruby methods are colorless, we discussed how threads are great at handing off work when blocked on IO:\n\nAs soon as you do any IO operation, it just parks that thread/fiber and resumes any other one that isn’t blocked on IO.\n\nHowever, this only works as long as:\n\nEarlier we mentioned slow queries as a possible IO blocker. But let’s say you have a web server running with 5 threads, and you allow users to download files2:\n\nWe have a simple Rails controller action, which sends a file to the client making the request. It’s pretty straightforward! The large.txt file I tested with locally is about 130mb. I’ll run it in a basic Puma setup, using 5 threads:\n\nI’m running a Puma server with 5 threads. Let’s try some benchmarks against it. We’ll use Apache Bench (ab) to simulate traffic to Puma. -n means the number of total requests, and -c is how many concurrent requests to make. Let’s start with 1 request:\n\nCool - 0.07 seconds. How about 3?\n\nNot much change from a single request! How about 5? This would match the maximum number of simultaneous requests our server can currently support, using 5 threads:\n\nThings slow down a little bit once we max out our threads. But still reasonable. How about 50?\n\nSo far, so good. But we’re benefiting from a lot here: the file isn’t particularly large, and there is no latency. The server is responding quickly, and the client is consuming the response quickly. Let’s switch things up a bit - how well does it handle a client downloading slowly?\n\nWe’ll use curl to simulate a slow client. We can use limit-rate to simulate a client downloading only 1000k per second. curl ... --limit-rate 1000k & means we’ll download results at a rate of 1000k per second, running in the background (&). This means it will take our curl call 2 or 3 minutes to download a 130mb file. At the same time, we’ll run another apache bench to see how things perform:\n\nPuma responds quickly. In this scenario, curl is occupying one thread, and ab occupies another. Let’s try running 4 curl commands:\n\nPuma still responds fine. curl is now occupying 4 threads, and ab uses the remaining 1 thread. Let’s add one more request using curl. We also increase the default timeout of ab (which is 30) to 200, for no particular reason…\n\nYikes. That did not go well. The moment we had 5 slow running requests from our curl calls, we saturated all available threads. Our 6th request using ab sat around waiting, finally finishing 2 minutes later!\n\nThis is a critical consideration - long-running web work is a throughput killer. Ideally, keep all work as fast as possible and offload long-running work to jobs/other services. Most commonly for a download you’d create a presigned url for a service like S3 and redirect to that URL.\n\nIf you need to run long-running IO, you need to allocate many more threads3.\n\nIn our Long-running CPU example, we couldn’t get pure Ruby code to completely hog the runtime. We can prioritize a thread higher than other threads, but work still continues to be distributed.\n\nOnce you start running native extensions, the Ruby runtime has more limited influence. It’s up to the extension to properly interface with Ruby and yield control back. Here’s a simple example that will block all other threads, using the standard openssl gem, using a function written in C, pbkdf2_hmac:\n\nWe have two threads running - one infinitely printing the time in a loop, and one infinitely calling pbkdf2_hmac in a loop. Here I give pbkdf2_hmac a ludicrous number of iterations to force the function to run, in C, for a long period of time:\n\nThe results show that despite each thread having the same priority, the thread with long-running C extension code hogs all of the runtime. The printing thread is able to print a timestamp roughly every 15 seconds.\n\nThere’s nothing the extension code is doing wrong per se, but because it runs purely in C, without yielding back to Ruby until its done, Ruby can’t do anything to keep work distribution fair.\n\nIn most cases, well-developed/mature native extensions won’t hit this issue. There are many popular gems that are, or include, native extensions. But if you do hit an expensive path in a C extension, be aware the Ruby runtime will not be able to control it. If you know you are interfacing with a slow piece of code in a native extension, keep it off the hot path, same as our long-running IO example.\n\nRemember our production panic scenario from earlier?\n\nNot knowing what else to do, you trigger a server restart. Even doing that, things remain unresponsive for another 30 seconds. Finally you see the server stop, and start up again. As if by magic, everything is running fine again\n\nYou triggered a server restart, and still had to wait 30 seconds? Why wasn’t Puma able to stop sooner? Let’s reuse our download example from earlier, and explain some default Puma behaviors!\n\nFirst, let’s start Puma, and see how quickly we can stop it:\n\nPuma tells us it is shutting down “gracefully”. With no activity, it is able to instantly stop.\n\nNow let’s use our DownloadController again:\n\nWe start Puma again, then occupy each thread with a request:\n\nNow let’s trying issuing INT using ctrl+c again:\n\nOk, so we issued our interruption. But… it waited for every request to completely finish! We had to wait around 120 seconds before our server shutdown. That’s even worse than earlier!\n\nThis isn’t a blog post about Puma specifically, but I’ll discuss a few factors:\n\nLet’s try this one more time, starting in cluster mode by setting -w 1:\n\nThis time we replicate our earlier behavior - 30 seconds pass, and the server is shutdown. Now that we’re running in cluster mode, by default Puma uses a configuration called worker_shutdown_timeout, which defaults to 30 seconds. If you have a configuration file, you can set it yourself to something longer or shorter:\n\nAs well, by default Puma never kills threads. In a moment we’re going to be talking about ways to kill a thread. Puma plays it extremely safe, and offers no ability to kill individual threads. And even when shutting down, it defaults to a thread shutdown policy of :forever, which means the only way the threads are killed is when the server is entirely shutdown, which shuts down the worker the threads live in.\n\nYou can change this. In the same configuration file you’d set worker_shutdown_timeout you can set force_shutdown_after:\n\nStill - this doesn’t do much. It still only impacts full server shutdown. But with this setting, the internal Puma thread pool will raise on all threads, and then eventually run kill on them.\n\nWhat all this means is that by default in Puma:\n\nWe’ll talk in-depth later about the available option for killing long-running threads in Puma.\n\n📝 if you want to dig deeper into how Puma works, I highly recommend Dissecting Puma: Anatomy of a Ruby Web Server. The source of Puma is also pretty readable\n\nAll of these thread issues are possibilities, but they mean nothing without empirical data. Measure, then decide your course of option.\n\nOk, enough about how threads get stuck. Once they’re stuck - is there a way to stop them?\n\n⚠️ TL;DR You shouldn’t use these methods unless you really know what you’re doing. Instead, interrupt your thread safely. Incidentally, you should also avoid the timeout module.\n\nif you’re writing a generic threaded framework you may need it - for custom one-off threads you can probably manage without it\n\nSometimes a thread is running and you need to shut it down. There’s two primary methods for achieving that: raise and kill.\n\nraise will raise an error inside of the target thread. If the thread hasn’t started yet, in most cases it is killed before running anything:\n\n📝 thread_status is the helper we defined in the “Thread API” section on status\n\nThe error isn’t raised instantly - only at the point the thread is scheduled next. We sleep 0.1 to give thread t an opportunity to start. The thread scheduler starts it, and it immediately raises our “knock it off” error, effectively running right before puts \"never runs\".\n\nIf the thread gets a chance to start, the error will be raised on whatever line happened to be running last:\n\nBecause it raises whatever error is provided (a RuntimeError if just a string is provided), we can actually rescue the error, ignore it, and retry 😱:\n\nWith raise and kill, issues start to creep in when errors are thrown in an ensure. Here we use a ConditionVariable (we dug into those in The Thread API) to guarantee we raise from the ensure block:\n\nWe don’t see “I’ll never fire 😔”. What happens to our cleanup? Shouldn’t ensure, erm, umm, ensure that things finish…\n\nMoving on from raise, kill stops the thread from running anymore instructions, no matter what it’s doing. raise can be rescue’d, kill can’t.\n\n📝 Technically, kill can be ignored, we’ll explain that when discussing handle_interrupt.\n\n⚠️ Don’t rescue Exception, it’s a bad idea and you could accidentally rescue things like an OutOfMemoryError 😬\n\nBecause kill doesn’t raise an error, you actually can’t even tell that the thread was killed. We just get the normal false status, represented in our example by “finished”.\n\nLike raise, kill can also disrupt your ensure methods:\n\nThere are a few aliases for kill to be aware of as well:\n\nCase closed. Feel free to use raise and kill on your threads. No harm no foul… oh what’s this here?\n\nRuby’s Thread#raise, Thread#kill, timeout.rb, and net/protocol.rb libraries are broken\n\nWhy Ruby’s Timeout is dangerous (and Thread.raise is terrifying)\n\nThe Oldest Bug In Ruby - Why Rack::Timeout Might Hose your Server\n\nTimeout: Ruby’s Most Dangerous API\n\nStrangely, the thread docs say nothing about the dangers of these methods4. These articles are from 2008, 2015 and 2017. Surely no one uses it anymore, considering all that?\n\nIn fairness to threaded gems that use these methods, they are using the official way you shutdown a thread. And they’re usually taking as many precautions as possible, prior to calling them. For the most part, gems use them as a shutdown mechanism, and give plenty of room for the thread to finish normally first.\n\nThe basic problem is this: raise and kill force your code to die at any point, with no guarantee of properly cleaning up.\n\nYou might ask: “Couldn’t ctrl+c do the same thing?”. Yes, an OS signal could kill your process or program before an ensure runs, but then all related state is also removed - it can cause other issues, but at least your program cannot limp along in a corrupted state.\n\nSo are they pure evil? An occasional necessity? Somewhere in between? I’ll leave that discussion to the code philosophers… in the practical realm, follow these rules:\n\n📝 A small slice of this next section may look familiar. I included a bit of it in The Thread API. This goes much more in-depth\n\nI’m watching you. Step away from that method, slowly, and no threads have to get hurt.\n\nInstead of killing your thread, set it up to be interruptible. Most mature, threaded frameworks operate this way.\n\nWhenever you need something to run before a method finishes, you should always use an ensure block. ensure is kind of like a method lifeguard - even if something goes wrong, it’s there for you. It’s the place code goes to ensure it’s run before the method finishes (even when an error is raised).\n\nWe know ensure is not a silver bullet. Thread#raise and Thread#kill do not respect it. But you’re the most likely to clean things up using an ensure.\n\nIf you see this in code, be concerned:\n\nFor some reason, the timeout gem itself doesn’t warn about any issues. But Mike Perham summarizes it best:\n\nThere’s nothing that exactly matches what timeout offers: a blanket way of timing out any operation after the specified time limit. But most gems and Ruby features offer a way to be interrupted - there is a repository called The Ultimate Guide to Ruby Timeouts which details everything you need to know. It shows you how to set timeouts safely for basically every blocking operation you could care about timing out. For instance, how to properly handle timeouts using the redis gem:\n\nThe one piece mentioned in that repository you should leave alone: Net::HTTP open_timeout. Behind the scenes it uses the timeout module 🙅‍♂️. Leave the 60 second default, it should almost never impact you, and you’re probably worse off lowering it.\n\nPrimarily people use the timeout gem to manage IO timeouts. In the unlikely case you want to timeout CPU-bound code, it’s up to you to implement it in your processing.\n\nrack-timeout works similarly to the timeout module. And I already told you not to use that. So what gives? It will call raise on your threads - isn’t that bad?\n\nThe short answer is yes, it’s still bad.\n\nBut, rack-timeout is the only real option you have for timing out a web request in Puma. It’s meant as a last resort. From their docs:\n\nrack-timeout is not a solution to the problem of long-running requests, it’s a debug and remediation tool. App developers should track rack-timeout’s data and address recurring instances of particular timeouts, for example by refactoring code so it runs faster or offsetting lengthy work to happen asynchronously.\n\nOn top of that, you should have your own lower level timeouts set so that they would fire before rack-timeout.\n\nYou’ll want to set all relevant timeouts to something lower than Rack::Timeout’s service_timeout. Generally you want them to be at least 1s lower, so as to account for time spent elsewhere during the request’s lifetime while still giving libraries a chance to time out before Rack::Timeout.\n\nThe core issue of any thread raise/kill based solution is corrupted state. When using rack-timeout, you should be using term_on_timeout, ideally set to 1.\n\nterm_on_timeout will send a SIGTERM to the worker the thread is running in, which for most servers indicates a need for a graceful shutdown of that process - any potential corrupted state is isolated to that process and will be cleaned up once the process is shutdown.\n\nterm_on_timeout only works properly if you’ve got multiple processes serving your requests. And if you get lots and lots of timeouts, it could potentially cause performance problems. See the docs for proper configuration!\n\nThere is an alternative idea floating around out there of a way to achieve a “Safer Timeout”, at least in Rails apps:https://web.meetcleo.com/blog/safer-timeouts. Maybe I’ll detail it \n\nHaving threads that do not stop easily is a bug. If you’re seeing rack timeout errors, or jobs that can’t be shut down, track it and prioritize fixing it. Treat it like a bug and allocate time to improve it.\n\nThread.handle_interrupt is One Weird Trick Thread#kill Calls Don’t Want You To Know™. If we’re gonna discuss it, might as well go deep…\n\nA thread can be externally “interrupted” by a few things:\n\nhandle_interrupt gives you the ability to control how your program reacts to 1-3. And it means you can define blocks of code which will guarantee their ensure blocks run.\n\nhandle_interrupt is a low-level interface and it’s also the one you are least likely to ever need. You’ll see it used in things like threaded web and job servers where low-level control and better cleanup guarantees are helpful. You’ll find examples of it in Sidekiq, the Async fiber scheduler, Homebrew, the parallel gem and more.\n\nWhen you need the strongest guarantees possible about cleaning up your code in response to “interruption”, handle_interrupt is what you need.\n\nLet’s look at a simple example:\n\nRun that code 👆 and you’ll never see “done!” print. This is the same type of code we saw in the raise and kill section. What can handle_interrupt do for us?\n\nNow we see “done!” printed! To be clear, the error will still be raised eventually. It can only impact the section it encloses, so the error will be raised right after.\n\nWhat’s with the interface - what does KnockItOffError => :never mean? Let’s break it down:\n\nBased on that knowledge, let’s demonstrate a more complex example:\n\nIn this example, “done!” is never printed because it is in the :immediate block. But we successfully print out “Can’t touch this!” message in our ensure, because we’re within the :never block for KnockItOffError. ensure is now… ensured.\n\nWe’ve used :never and :immediate, what about :on_blocking?\n\nOur increments work fine, as indicated by i being one million. But our puts is a “blocking” call so it gets the boot.\n\nShould we have used a thread safe counter? Let’s try it again using Concurrent::AtomicFixnum from concurrent-ruby, and two threads. We should see i as two million afterwards:\n\nWait, why? i is 1237719? Why is i not two million? What blocked?!\n\n📝 You’ll definitely see a different number. Sometimes you’ll see 1000000, sometimes you’ll see a higher number, but you’ll pretty much never see 2000000\n\nAs it turns out, Concurrent::AtomicFixnum uses a Mutex by default. If a Mutex waits to acquire a lock it is considered a blocking operation! That means it qualifies for :on_blocking and the error gets raised.\n\nAs a specific fix for AtomicFixnum, if you install the concurrent-ruby-ext gem then you get native extensions which are lock-free, no longer use a Mutex, and properly run our code.\n\nOnce we install concurrent-ruby-ext, we properly get 2000000!:\n\nBut we also know that a Mutex or any other locking/waiting behavior can cause our :on_blocking interrupt to fire. So :on_blocking can have surprising behavior if some other internal of the code were to change later.\n\nIf the thread hasn’t started yet, handle_interrupt won’t help you. The error will be raised immediately in the thread, before handle_interrupt can be called:\n\nWhat happens after handle_interrupt? Once the error is allowed to raise, code directly after it won’t run:\n\nBut code after the inner handle_interrupt could run, it just depends on if the previous block raises. In this example, all of the code runs successfully because we don’t raise an error during the inner block:\n\nBut you’re better off guaranteeing code after the block runs. Use an ensure to make sure even if the inner block raises an error your code still runs:\n\nCan we even stop the unstoppable Thread#kill? Yep! From the Thread.handle_interrupt docs:\n\nFor handling all interrupts, use Object and not Exception as the ExceptionClass, as kill/terminate interrupts are not handled by Exception.\n\nSo we can handle it - but we have to use Object, which looks a bit odd but works well:\n\nThe reason for this odd syntax is that the kill/terminate interrupts are internally handled not as Exception instances, but as integers. That means this would also work:\n\nStill, you’re better off using Object to avoid the implementation detail.\n\nCan we stop the timeout gem from raising at a bad time using handle_interrupt? The Thread API docs used to specifically use timeout as a use-case for handle_interrupt, but there’s a non-determinism bug around thread reuse within the timeout gem.\n\nSo once again, don’t use the timeout gem.\n\nI removed the example from the docs because it’s too broken, so on Ruby 3.4+, the docs no longer mention handle_interrupt with the timeout gem.\n\nWe’ve looked at many handle_interrupt examples - what do real gems use it for?\n\nIn the async gem it uses handle_interrupt to ignore SignalException while it shuts down its child tasks:\n\nIn sidekiq, when it has gracefully attempted a shutdown and is forcing threads to finish, it raises a special error. That error extends Interrupt, which means most rescue blocks will not capture it because it is a child of Exception rather than StandardError:\n\nTo avoid Sidekiq::Shutdown breaking everything (including its own internal code), Sidekiq also uses handle_interrupt to ignore the error in a small piece of shutdown code:\n\nIf this section hasn’t been enough for you, Ben Sheldon gives some additional interesting examples In his article Appropriately using Ruby’s thread handle_interrupt.\n\nI’m pretty confident, started from scratch, Ruby would not be implemented with raise and kill again. I don’t know _which _ model they would choose - but something like a Java interrupt would be a good start. And minimally, making all ensure blocks uninterruptable, as well as all finalizers. I didn’t even get into finalizers - they’re a less common, but also important area that you really don’t want to interrupt.\n\nRuby is one of the only programming languages that lets you kill a thread from outside of the thread. It’s powerful, but mostly, it’s dangerous. It’s one of the sharpest tools available to you, and it should be used sparingly, or ideally not at all.\n\nIn threaded code, the best offense is a good defense:\n\nNow go forth, armed with the knowledge on what to do when good threads go bad.\n\nUnless you’re in SQLite, where apparently N+1 queries are a virtue 😲 https://www.sqlite.org/np1queryprob.html ↩︎\n\nIn general, you shouldn’t do this directly ↩︎\n\nOr use Falcon. See me later in the series when we talk about Fibers 😏 ↩︎\n\nThere’s a small mention about how to handle Timeout errors, but it doesn’t explain much or warn at all ↩︎",
    "readingTime": 23,
    "keywords": [
      "seconds finally",
      "let’s look",
      "cluster mode",
      "pure ruby",
      "ruby’s default",
      "magic everything",
      "method finishes",
      "kill/terminate interrupts",
      "per second",
      "client downloading"
    ],
    "qualityScore": 1,
    "link": "https://jpcamara.com/2025/12/30/when-good-threads-go-bad.html",
    "thumbnail_url": "https://cdn.uploads.micro.blog/98548/2025/whengoodthreadsgobad.jpg",
    "created_at": "2025-12-31T18:17:19.402Z",
    "topic": "tech"
  },
  {
    "slug": "the-guy-who-coined-vibe-coding-now-says-hes-never-felt-more-behind-as-a-programmer",
    "title": "The guy who coined 'vibe coding' now says he's never felt more behind as a programmer",
    "description": "OpenAI alum Andrej Karpathy wrote on X that his failure to fully claim the 10x boost of new tools felt like a \"skill issue.\"",
    "fullText": "Andrej Karpathy has long been ahead.\n\nHe was ahead of the AI boom, having worked as a founding member of OpenAI in 2015, long before competitors like Anthropic and xAI emerged. He also got into self-driving vehicles early, steering Tesla's autopilot effort as its head of AI.\n\nNow, he says, \"I've never felt this much behind as a programmer.\"\n\nIn an X post on Friday, Karpathy wrote that the industry was being \"dramatically refactored,\" as individual programmers contribute fewer and fewer lines of code.\n\n\"I have a sense that I could be 10X more powerful if I just properly string together what has become available over the last ~year,\" he wrote. \"A failure to claim the boost feels decidedly like skill issue.\"\n\nI've never felt this much behind as a programmer. The profession is being dramatically refactored as the bits contributed by the programmer are increasingly sparse and between. I have a sense that I could be 10X more powerful if I just properly string together what has become…\n\nAI has radically transformed the software engineering industry, introducing code editors like Cursor, Claude Code, and Codex, along with a slew of agentic software development tools. Business Insider's Amanda Hoover called 2025 \"the year coding changed forever.\"\n\nKarpathy was a key player in the change. In February, he coined the term \"vibe coding.\" To vibe code, one prompts AI to generate lines of code. (It gets its name because developers \"fully give in to the vibes,\" Karpathy wrote in his original post.) The Collins Dictionary named it the word of the year.\n\nStill, Karpathy wrote that it's like a \"powerful alien tool\" was handed out without a manual.\n\n\"Everyone has to figure out how to hold it and operate it, while the resulting magnitude 9 earthquake is rocking the profession,\" he wrote.\n\nIn the comments, another one of the biggest names in vibe-coding agreed. Boris Cherny created Claude Code for Anthropic, now one of the most popular AI tools among developers.\n\nCherny wrote that he felt that way \"most weeks,\" and that he sometimes finds himself approaching a problem manually, not yet realizing AI can do it faster.\n\nNew graduates and early career coders may fare best in this new environment, Cherny wrote, because they don't assume what AI can and cannot do.\n\n\"It takes significant mental work to re-adjust to what the model can do every month or two, as models continue to become better and better at coding and engineering,\" he wrote.\n\nResponding to Cherny, Karpathy wrote that he had similar experiences. He analogized the new tools to a weapon, one that sometimes \"shoots pellets\" or \"misfires\" — highlighting the work-in-progress nature of AI.\n\nOther times, though, the tools work wonders.\n\n\"Once in a while when you hold it just right a powerful beam of laser erupts and melts your problem,\" he wrote.",
    "readingTime": 3,
    "keywords": [
      "dramatically refactored",
      "properly string",
      "string together",
      "tools",
      "programmer",
      "coding",
      "ahead",
      "anthropic",
      "i've",
      "behind"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-founding-member-never-felt-so-behind-programmer-2025-12",
    "thumbnail_url": "https://i.insider.com/6953e21004eda4732f2e3df7?width=1200&format=jpeg",
    "created_at": "2025-12-30T18:18:12.476Z",
    "topic": "finance"
  },
  {
    "slug": "why-reliability-demands-functional-programming",
    "title": "Why Reliability Demands Functional Programming",
    "description": "> In banking, telecom, and payments, reliability is not a nice to have. It is table stakes. The most reliable systems I have worked on reduce entire classes of bugs before the code even runs. Functional programming and Algebraic Data Types (ADTs) let you push correctness into the type system, so illegal states cannot be constructed in the first place. **What you will learn** - How invalid states show up in real systems and why they cause costly incidents - How ADTs encode business rules so the compiler enfo...",
    "fullText": "In banking, telecom, and payments, reliability is not a nice to have. It is table stakes. The most reliable systems I have worked on reduce entire classes of bugs before the code even runs. Functional programming and Algebraic Data Types (ADTs) let you push correctness into the type system, so illegal states cannot be constructed in the first place.\n\nMost production incidents are not due to complex algorithms. They are due to the code entering a state that should never have been possible. If you have been on call, you have seen variants of these:\n\nFunctional programming helps by modeling the domain with types that make invalid states unrepresentable. Pure functions and immutability keep behavior predictable and testable.\n\nProduct types combine fields, think \"and\". Sum types choose one of several cases, think \"or\". Together they model your domain rules.\n\nWith this shape, \"paypal\" cannot exist as a Payment. The compiler refuses the value.\n\nWhen you pattern match on a sum type, the compiler can force you to handle every variant. If you later add a new case, every non exhaustive match becomes a compilation error or warning. This is how refactors become safe by default.\n\nAdd a new Crypto method and both code bases will point out every place you must update.\n\nIncident story\n\nA payout worker retries on network timeouts and calls settle() twice. The table allows pending = false and settled = true twice with the same ledger id. Reconciliation finds duplicates and accounting needs a manual fix.\n\nWhy it happened\n\nState is spread across booleans and strings. The database does not express the lifecycle. The application code does, but only by convention and tests.\n\nTransitions become total functions. You can return a Result when a transition is not allowed.\n\nNow the illegal transitions are blocked by construction. Test coverage still matters, but the shape of the model prevents a class of bugs.\n\nRefactor safety\n\nWhen product adds Chargeback, the compiler highlights every match that ignores it. You cannot ship with a half handled lifecycle.\n\nEvery switch on TxnState now requires a chargeback branch. This is free guidance from the compiler.\n\nIncident story\n\nThe call detail record pipeline generates billing events whenever it sees a Connected event. Under jitter and retries, some sessions never receive Completed. The billing system charges based on the wrong boundary and customers complain.\n\nWhy it happened\n\nThe call lifecycle is implicit across many services. A connected session with no end was still billable because there was no type that separated non billable states from billable ones.\n\nNow a connected but never completed call cannot produce a billable duration. The shape forbids the bug.\n\nIncident story\n\nA cache TTL is stored in an environment variable. Someone sets CACHE_TTL_SECS=30s. In JavaScript, Number(\"30s\") yields NaN and your code treats it as zero, disabling caching in production.\n\nThe ambiguity disappears. The code must handle absence and parse errors explicitly.\n\nDo not use null to mean \"maybe\". Do not throw exceptions for expected errors.\n\nThese types make the happy path and the error path equally explicit.\n\nMutable shared state is a common source of heisenbugs under concurrency. Prefer immutable data and pure functions. When you need to update, create a new value.\n\nYour tests become simple. Given the same inputs, the function returns the same output.\n\nNumbers are not self describing. Create types that carry meaning.\n\nYou stop mixing milliseconds with seconds or dollars with cents by accident.\n\nKeep the domain logic pure and push IO to the edges. This makes unit tests cheap and fast.\n\nStart small and make continuous progress. Here is a practical order for a team new to these ideas.\n\nReplace pairs of booleans with a sum type\n\nReplace string enums with discriminated unions\n\nReplace nullable fields with Option\n\nReplace thrown control flow with Result\n\nIntroduce newtypes or branded types for units and ids\n\nPattern matching compiles to simple branches. Discriminated unions in TypeScript are just plain objects. The main cost you will feel is validation at the boundaries in smart constructors. This is a trade worth making. The compiler then protects the interior of the system.\n\nReliability is designed. With Algebraic Data Types, pattern matching, Option and Result, immutability, and smart constructors, you encode domain rules directly in your types. Illegal states cannot compile. This is why industries that cannot afford failure, such as banking and telecom, gravitate to functional ideas.\n\nIf you work on code that touches money, minutes, or public availability, adopt these patterns now.\n\nYour on call shifts will be quieter, and your users will notice the difference.",
    "readingTime": 4,
    "keywords": [
      "functional programming",
      "discriminated unions",
      "smart constructors",
      "pattern matching",
      "domain rules",
      "pure functions",
      "code",
      "cannot",
      "compiler",
      "billable"
    ],
    "qualityScore": 1,
    "link": "https://blog.rastrian.dev/post/why-reliability-demands-functional-programming-adts-safety-and-critical-infrastructure",
    "thumbnail_url": "https://rastrian.dev/assets/img/profile.png",
    "created_at": "2025-12-28T01:03:21.884Z",
    "topic": "tech"
  },
  {
    "slug": "a-guide-to-claude-code-20-and-getting-better-at-using-coding-agents",
    "title": "A Guide to Claude Code 2.0 and getting better at using coding agents",
    "description": "A deep dive into Claude Code 2.0 features, Opus 4.5 workflows, and context engineering. Learn sub-agents, MCP servers, hooks, skills, and practical tips to boost your AI-assisted coding productivity.",
    "fullText": "This post is a follow-up to my post from July'25 - My Experience With Claude Code After 2 Weeks of Adventures. If you are new to Claude Code or just want a quick refresh, I am once again asking you to go through it. It covers some lore, my workflow back then and then 80-90% of the Claude Code standard workflow. You may choose to skip the intro although I recommend you read it. Lore is important man.\n\nA short recap - we had covered CLAUDE.md, scratchpad, using task tool (now sub-agents), the general plan + execute workflow, tips for context window management, Sonnet 4 vs Opus 4 (not relevant now), using shortcuts like ! and using Shift + ? to show shortcuts, memory basics, /resume to restart conversation and short discussion on custom commands.\n\nI got a great response on my Opus 4.5 vibe-check tweets and still receieving good feedback on my July blog post (despite being somewhat poorly written). This shows there's clearly a demand for in-depth resources around Claude Code.\n\nI noticed that lots of people, both technical and many non-technical or less hands-on people i.e technically-lite people have started to try Claude Code (CC). CC is more of a general agent - you can use it for tasks other than coding as well - like making an excel invoice, data analysis, errands on your machine etc. And of course everything I talk about is by default meant for coding too.\n\nIf you can learn even 3-4 ideas that help you with using Claude Code (or other tools like Codex/Gemini CLI/OpenCode) or improve your understanding of LLMs, it would be a win for me.\n\nI don't want this post to be a prescription (map). My objective is to show you what is possible and the thought processes and simple things you can keep in mind to get the most out of these tools. I want to show you the map but also the territory.\n\nClaude Code dominated the CLI coding product experience this year and all the CLI products like Codex, OpenCode, Amp CLI, Vibe CLI and even Cursor have heavily taken inspiration from it. This means learning how things work in Claude Code directly transfers to other tools both in terms of personal usage and production grade engineering.\n\nKarpathy sensei posted this which caused the Twitter timeline. This led to a lot of discussion and there were some really good takes - some which I have written about too.\n\nIt's a reasonable crashout - the technology is evolving at a mindblowing pace and it's difficult to keep up for most of us and especially for senior folks and people with high quality standards. Nevertheless, I think if you are reading this post, it's scary but also exciting time to build stuff at speeds never possible before.\n\nInstead of thinking in terms of \"keeping up\", a better framing is how can I improve myself with help of these tools i.e augment.\n\nIn my opinion, there are 3 components to augment yourself:\n\nStay updated with tooling - What Karpathy sensei mentioned. Use these tools regularly and keep up with releases. I have been doing this regularly; it can be draining but I enjoy the process and I have the incentive that it helps me at my job. For the technically lite, even weekly/monthly updates would help.\n\nUpskill in your domain - It's a great time to spread both vertically (domain depth) and horizontally (adjacent areas). The more you know, the better you can prompt - converting unknown unknowns to known unknowns. Experience builds judgement and taste - that's what differentiates professional devs from vibe-coders. Since implementation is much faster now, you can spend more time on taste refinement.\n\nFor software engineering folks, this might mean getting better at good practices, system design, planning - where more thinking is involved. Ask more questions, run more experiments (since you can iterate fast), spend more time on understanding requirements. Using good software engineering practices to create better feedback loops for LLMs (good naming, refactoring, docs, tests, typed annotations, observability etc.). Review code. Please don't forget to come back to my post lol but I liked Addy Osmani's take on this.\n\nThe idea is to let the LLM perform things with input, get output and see errors.\n\nAs an aside, getting better at articulating thoughts via writing helps. One may also try touch typing/writing using speech-to-text tools to operate faster.\n\nThis post will act as a guide for things Karpathy said but you'll need to play around, build intuition and achieve outcomes with help of these tools yourself. The good news is it's fun.\n\nI am having a great time with Claude Code 2.0 since the launch of Opus 4.5 and it's been my daily driver since then. Before we go all lovey-dovey about Claude, I wanted to quickly go through the timeline and lore. I love yapping in my blog and I feel it's important to set the context here.\n\n2025 saw release of many frontier models by OpenAI and Anthropic. Also, it's super under-talked but OpenAI actually caught up to Anthropic in code-generation capability - intelligence wise, context window effectiveness, instruction following and intent detection.\n\nIt's been a wild year and honestly speaking I got tired of trying out new releases by OpenAI every 2 weeks.\n\n>no swe-bench-verified comparison\n>no comparison against opus 4.5\n>\"we are topping in cybersecurity\"\n>mfw i realise i am the fucking eval https://t.co/4oDG3yj6CP pic.twitter.com/aUfJfwROCf\n\nThere have been several Open Source competitors like GLM-4.7, Kimi-K2, Minimax-2.1. The space is very competitive and there is definitely an audience that uses the cheaper priced but high performant Chinese models for low-medium difficulty tasks.\n\nThat said, I still think Anthropic/OpenAI lead over Chinese Frontier models. The latter have contributed \n\n(Note: I am talking with respect to personal coding usage, not production API usage for applications).\n\nI was using Claude Code as my main driver from late June to early September. I cancelled my Claude Max (100 USD/month) sub in early September and switched to using OpenAI Codex as my main driver. Switch was driven by two factors -\n\nclaude code is more enjoyable as a product and has more features. i have always felt to try out more things related to automation in cc as compared to codex. once they drop a new iteration i would consider getting a max sub again if its better than gpt-5-codex\n\nAnthropic also had tonne of API outages and at one point of time they had degradation due to inference bugs. This also was a major driver for several people to move to the next best alternative i.e Codex or GPT-5.1 on Cursor.\n\nI was using Codex (main driver) and Cursor (never cancelled) until late October. Claude Sonnet 4.5 had released on 29th September along with Claude Code 2.0.. and I did take a 20 USD sub from another email account of mine to try it out (I had lots of prompting work and Claude models are my preferred choice) but GPT-5/GPT-5-codex were overall better despite being slow.\n\nSonnet 4.5's problem was fast and good but it would make many haphazard changes which would lead to bugs for me. In other words, I felt it to be producing a lot of slop in comparison to GPT-5.1/GPT-5.1-codex later.\n\nAround October 30, Anthropic sent an email saying we are offering the 200 USD max plan to users who cancelled the subscription and obviously I took it.\n\nchat please remind me to cancel after 28 days😂 pic.twitter.com/TSGidVJ2xo\n\nMy Claude Code usage was still minimal but on 24th November, they launched Opus 4.5 and I had 5 days to try out Opus 4.5. I used the hell out of it for my work and also wrote this highly technical blog with the help of it discovering several of its capabilities.\n\nI had done a similar tweet when I had switched to GPT-5.1 which had gotten half the response of this one. This indicated to me that more people resonated with Opus 4.5 (at least on Twitter) back then. Also, many people were just not able to realise GPT-5.1's capabilities tbh.\n\nOther than the above State of the Art at the coding benchmarks like SWE-bench-verified (code-generation), Tau Bench (agentic stuff), Opus 4.5 was faster, at-par in coding, super collaborative and good at communication. These factors led to my conversion. It had good vibes. More comparison points later in the post.\n\nAs I described in the screenshot, Opus 4.5 was roughly at same code-gen capability with GPT-5.1-Codex-Max.\n\nToday, in my experience I think GPT-5.2-Codex exceeds Opus 4.5 in raw capability by a small margin. Still, Opus 4.5 has been my main driver since release.\n\nI think first reason is it's faster and can do similar difficulty tasks in much lesser time than Codex. Also, it's overall\na much better communicator and pair-programmer than Codex which can even ignore your instructions at times (and go and make changes). Opus has better intent-detection as well.\n\nOne nice-use case shown here by Thariq on creating a background async agent to explain changes to a non-technical person leveraging Claude's explanation abilities.\n\nTo further demonstrate the difference, here's a CC vs Codex comparison\n\nFor the same prompt, see the outputs. Codex tends towards super concise while Claude matches my expectation.\nYou can modify the verbosity in Claude's case but Codex won't budge. Another thing I want to highlight is\nthe UI itself - Claude has more saturated white color on black whereas Codex's text is thinner/less readable\nand the thinking traces are shown in even lighter shade which I don't like.\n\nBecause of being faster not only in terms of lesser thinking to perform task but throughput wise also, it unlocks\nmuch faster feedback loops for your tasks. This makes progress feel more visceral even though capability wise, GPT-5.1/Codex were at par even in November.\nThe only downside with faster loop is if you are cautious, you end up micro-managing for long hours.\n\nOpus 4.5 is a great writer and comes closest to humans so I have always preferred Claude models for customizing prompts.\n\nBesides the model, obviously the Claude Code Product goes a long way to make things magical.\n\nAs a product it's a mile ahead of Codex in QoL features. The harness, prompts and the model make for a magical experience. The model is amazing but there is a massive amount of tasteful engineering that has gone into UX/UI and just the code/prompts to let Claude feel comfortable in the harness and make function calling accurate. We will explore this \n\nBefore we move ahead - my previous post somehow reached Hackernews #5 and I was facing allegations that my post was sponsored by Anthropic. I was like bro are you serious? Anthropic doesn't sponsor random users like me. Anthropic doesn't even think about me (meme.jpeg) besides from a user point of view.\n\nBesides praise, I have been snarky, made fun of outages, made a lot of fun of Sonnet 4.5 slop. I have expressed what I have felt over time and it's led to good discourse on the timeline as well.\n\nAll this said, Claude Code has been one of the most enjoyable product experiences I have ever had. I am grateful and highly respect the engineering and research team behind it.\n\nThat's enough yapping. In the next few sections, I will talk about useful features that I didn't talk about in my previous blog and notable features introduced in the iterations from Claude 2.0 - 2.074.\n\ncurrently using Claude Code for the first time, I can officially put \"Technical-lite\" on my resume now\n\nI am assuming several technical-lite people are gonna read this. Few concepts to help comprehension later in the blog -\n\nMore specifically, context is the input tokens. The context window refers to the maximum amount of tokens that an LLM can see and process at once during a conversation. It's like the model's working memory. Opus 4.5 has a 200K context window which is approximately 150,000 words.\n\nTool calling - Learn about tool calling. Here's a good resource. You know that LLMs can generate text but what if you want the LLM to perform an action - say draft an email or lookup the weather on the internet or just do google search. That's where tools come in. Tools are functions (in code or skills) defined by the engineer that do these exact things. We define tools and we let the LLM know about it in the system prompt and it can decide which tool to call when you are chatting with it! Once the tool call i.e the action is performed, the results are relayed back to the LLM.\n\nAgent - The simplest definition is an LLM that can pro-actively run tools to achieve a goal. For a more sophisticated definition, I like the one by Anthropic\n\n\"Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\" from Building Effective Agents.\n\n\"Agentic\" - refers to the tool calling capabilities of the model - how pro-active, how accurate the tool calling is (detecting user's intent to perform the action, choosing the correct tool, knowing when to stop)\n\nHarness/scaffolding - Sonnet 4.5/Opus 4.5 are the models. They need to be provided with lots of \"scaffolding\" / layers of code, prompts, tool calls and software packaging/environment to make them work in a semi-autonomous fashion. Note that Claude Code is not a harness, it's a product (think the TUI, integrations etc.). Claude Code has a harness.\n\nClaude Code has had lots of AI features and quality of life improvements since July. Let's look at the ones that I found to be useful. You can see all changes in the Changelog.\n\nAsk mode options - Another thing I like is Option 3 when it asks questions in the syntax highlighting image above - \"Type here to tell Claude what to do differently\". Fun fact: All these are really prompts for the model whose output is parsed by another tool call and shown in this way.\n\nUltrathink - I like to spam ultrathink for hard tasks or when I want Opus 4.5 to be more rigorous e.g. explaining me something, self-reviewing its changes\n\nThinking toggle - Tab to toggle thinking on/off was a good feature. They changed it to Alt/Option + Tab recently but there's a bug and it does not work on Mac. Anyways CC defaults to thinking always true if you check in your settings.json\n\nPrompt history search - Search through prompts using Ctrl + R (similar to terminal backsearch). I have it in 2.0.74. It can search across project wide conversations. Repeatedly do Ctrl + R to cycle through results.\n\nCursor cycling - When you reach beginning/end of prompt, press down/up to cycle around\n\nMessage queue navigation - It's possible to navigate through queued messages and image attachments (2.0.73) now (idk if it's possible to display image attachment as well).\n\nFuzzy file search - File suggestion is 3x faster and supports fuzzy search (2.0.72)\n\nLSP support was added recently. Access via plugins.\n\nThere have been new integrations too like Slack Integration, Claude Web (beta), Claude Chrome extension. These are pretty obvious and I won't cover these. I think Claude Web would be interesting for many particularly (since you can launch tasks from iOS/Android too).\n\nNext few sub-sections are all about most used features.\n\nI didn't cover commands properly in my previous blog post. You can use / to access the built-in slash commands. These are pre-defined prompts that perform a specific task.\n\nIf these don't cover a specific task you want, then you can create a custom command. When you enter a command, that prompt gets appended to the current conversation/context and the main agent begins to perform the task.\n\nCommands can be made on a project level or global level. Project level resides at .claude/commands/ and global one at ~/.claude/commands.\n\nOften when the context window starts getting full or I feel the model is struggling with a complex task, I want to start a new conversation using /clear. Claude provides /compact which also runs faster in CC 2.0 but sometimes I prefer to make Claude write what happened in current session (with some specific stuff) before I kill it and start a new one. I made a /handoff command for this.\n\nIf you find yourself writing a prompt for something repetitively and instructions can be static/precise, it's a good idea to make a custom command. You can tell Claude to make custom commands. It knows how (or it will search the web and figure it out via claude-code-guide.md) and then it will make it for you.\n\nYou can find a bunch of commands, hooks, skills at awesome-claude-code though I recommend to build your own or search for one only when it's really required.\n\nI have a command called bootstrap-repo that searches the repo with 10 parallel sub-agents to create a comprehensive doc. I rarely use it these days and so many parallel sub-agents lead to the Claude Code flickering bug lol.\n\nAnyways, notice the \"Explore\" sub-agent and \"running in background\".\n\nSub-agents were introduced shortly after I published my last blogpost. Sub-agents are separate Claude instances that are spawned if the main agent wishes so or you explicitly tell it to do so. These powers are defined already in system prompt and sometimes you just have to remind... Understanding these features will help you micro-manage Claude haha.\n\nYou can also define your custom sub-agents. To create one:\n\nOr just use /agents to manage and create sub-agents automatically - recommended approach.\n\nThe \"Explore\" thing in above pic is a sub-agent. You can tell Claude \"Launch explore agent with Sonnet 4.5\" if you want it to use Sonnet instead of Haiku (I found this by just trying things out but we will see how this happens)\n\nThe Explore agent is a read-only file search specialist. It can use Glob, Grep, Read, and limited Bash commands to navigate codebases but is strictly prohibited from creating or modifying files.\n\nYou will notice how thorough the prompt is in terms of specifying when to use what tool call. Well, most people underestimate how hard it's to make tool calling work accurately.\n\nThis is the Explore agent prompt from 2.0.56 and it should be similar now too. Reference. These are captured by intercepting requests. Reference video.\n\nIn case of Explore sub-agent, it starts with a fresh slate which makes sense. It does not inherit any context from main agent. Many tasks involve searching through large amounts of digital media or code to filter for something relevant. Often the individual parts are independent of each other when you want to filter for something so launching parallel agents makes sense.\n\nIf I am trying to understand a feature or just looking up simple things in the codebase, I let Claude do the Explore agent searches. Explore agent passes a summary back to the main agent and then Opus 4.5 will publish the results or may choose to go through each file itself. If it does not, I explicitly tell it to.\n\nIt's important that the model goes through each of the relevant files itself so that all that ingested context can attend to each other. That's the high level idea of attention. Make context cross with previous context. This way model can extract more pair-wise relationships and therefore better reasoning and prediction. Explore agent returns summaries which can be lossy compression. When Opus 4.5 reads all relevant context itself, it knows what details are relevant to what context. This insight goes a long way even in production applications (but you only get it if someone tells you or you have read about self-attention mechanism).\n\nCodex does not have a concept of sub-agents and it's probably a conscious decision by the devs. GPT-5.2 has a 400K context window\nand according to benchmarks, it's long context retrieval capabilities are a massive improvement. Although people have tried\nmaking Codex use headless claude as sub-agents haha. You can just do things.\n\nThe general-purpose and plan sub-agent (separate from plan mode) inherit the context. With respect to user defined sub-agents, I am not sure but I think they start with clean slate.\n\nFrom the reverse engineered resources/leaked system prompt, it's possible to see that the sub-agents are spawned via the Task tool.\n\nTurns out you can ask Claude too. (I think the developers are allowing this now?). It's not a hallucination. The prompt pertaining to pre-defined tools are there in the system prompt and Claude code dynamically injects reminders/tools often to the ongoing context.\n\nTry these set of prompts with Opus 4.5\n\nYou will get the output something like below (click) but to summarise -\nIt defines 5 agent types: general-purpose (full tool access, inherits context), Explore (fast read-only codebase search), Plan (software architect for implementation planning), claude-code-guide (documentation lookup), and statusline-setup. Notice how each sub-agent is defined with its specific use case and available tools. Also notice the \"When NOT to use\" section - this kind of negative guidance helps the model avoid unnecessary sub-agent spawning for simple tasks.\n\nI want you to focus on the tool schema. The Task tool prompt above is detailed guidance on how to use the tool that resides in the system prompt. The tool schema defines the tool or the function.\n\nThe main agent calls the Task tool to spawn a sub-agent, using its reasoning to decide the parameters. Notice the model parameter - when I say \"Use Explore with Sonnet\", the model makes the tool call with model: \"sonnet\".\n\nTill August'25 or so, Claude Code used to show the Task tool performing actions in the TUI but now TUI shows the sub-agent name instead.\n\nNotice the run_in_background parameter. It decides whether to send a sub-agent to run in the background. I like the background process feature - it is super helpful for debugging or just monitoring log outputs from process. Sometimes you have a long running python script that you wanna monitor etc.\n\nModel usually automatically decides to put a process in background but you can explicitly tell it to do so. Note that \"Background Tasks\" is different. Using an & sends a task to Claude Web (should have named it Claude Cloud haha). I am yet to get this to work properly.\n\nI have a pretty simplish/task based workflow: CC as the main driver, Codex for review and difficult tasks, and Cursor for reading code and manual edits. I rarely use Plan Mode. Instead, once requirements are clear enough, I explore the codebase to find the relevant files myself.\n\nOpus 4.5 is amazing at explaining stuff and makes stellar ASCII diagrams. The 2025 Aug knowledge cutoff helps here too. So my exploration involves asking lots of questions—clarifying requirements, understanding where/how/why to make changes. It might be less efficient than Plan Mode, but I like this approach.\n\nOnce I have enough context, I spam /ultrathink and ask it what changes are required and then if things look ok, I start the execution closely monitoring the changes - basically micro-managing it. I sometimes ask Codex's second opinion here lol.\n\nFor difficult new features, I sometimes use a \"throw-away first draft\" approach. Once I understand what changes are needed, I create a new branch and let Claude write the feature end-to-end while I observe. I then compare its output against my mental model as to how close did it get to my requirements? Where did it diverge? This process reveals Claude's errors and the decisions/biases it made based on the context it had. With the benefit of this hindsight, I run another iteration, this time with sharper prompts informed by what I learned from the first pass. Kinda like Tenet.\n\nFor backend-heavy or just complex features specifically, I'll sometimes ask Codex xhigh to generate the plan instead.\n\nI maintain a few custom commands, use CLAUDE.md and scratchpad extensively. No custom sub-agents. I use MCP sometimes if need shall arise (e.g for docs. I have tried Playwright and Figma MCP so far) but in general not a fan. I have used hooks for simple stuff in the past and need-basis. skills/plugins are something that I am yet to use more regularly. I often use background agents for\nobservability (monitoring log / error) purposes. I rarely use git worktrees.\n\nIt's worth noting that the harness is so heavily engineered that Claude knows which sub-agent to spawn, what command/tool call/skill to run, what to run in async manner. It's able to heavy carry the agent loop that your task is mainly to use your judgement and prompt it in right direction. The next generation of models will get better and the relevant scaffolding will reduce for existing feature and increase for newer features. (Re: contrasting to Karpathy sensei's latest tweet shown at beginning)\n\nIt's not at all required to know the features in depth to be honest. However knowing how things work can make you a better micro-manager if the need arises like telling the Explore agent to use Sonnet.\n\ngetting claude opus 4.5 changes reviewed by gpt-5.1-codex-max high pic.twitter.com/A4tYN3W3Q6\n\nFor reviewing code and finding bugs, I find GPT-5.2-Codex is superior. Just use /review. Better than code review products too.\n\nIt's able to find bugs and mention severity like P1, P2. It's less likely to report false-positives and more trustable when it comes to confusing changes as compared to Claude. This Claude for execution and GPT/o-series model for review/bugs dynamic has been pretty constant for me for probably a year.\n\nNow is a good time to take a breath and refresh your context window. Before we get to the next set of features, it's worth\ngoing through context management fundamentals. Things might get a bit difficult for the technically-lite folks.\nDon't give up. Read through the post. Even ask Claude to explain stuff you don't understand.\n\nAn agent in a harness can pro-actively do a lot of tool calls to read your codebase and other inputs, edit stuff, make writes etc. In this process, they can produce a lot of data which gets added to the running conversation i.e the context window. Anthropic refers to this art and science of curating what will go into the limited context window from this information as context engineering.\n\nYou may ask how are tool calls adding tokens to the context window? The flow works like this:\n\nThe key thing to note here is that both the tool call and the tool call outputs are added to the context so that the LLM can know the results. This is because LLMs are stateless. They don't have memory outside the context window. Let's say you have n messages in a conversation. When you send the next message, the request will again process n + 1 messages in the LLM ~ single context window.\n\nIf you don't add information about the chosen tool call was, LLM won't know and if you don't plug the output, then it won't know\nthe outcome. The tool call results can quickly fill your context and this is why agents can get super expensive too.\n\nI quote directly from effective-context-engineering-for-ai-agents\n\nContext refers to the set of tokens included when sampling from a large-language model (LLM). The engineering problem at hand is optimizing the utility of those tokens against the inherent constraints of LLMs in order to consistently achieve a desired outcome. Effectively wrangling LLMs often requires thinking in context — in other words: considering the holistic state available to the LLM at any given time and what potential behaviors that state might yield.\n\nContext engineering is about answering \"what configuration of context is most likely to generate our model's desired behavior?\"\n\nEverything we have discussed so far comes under context engineering. Sub-agents, using a scratch, compaction are obvious examples\nof context management methods used in Claude Code. Some notes around why context engineering is needed -\n\nLimited context window - The context retrieval performance of LLMs degrades as every new token is introduced. To paraphrase the above blog - think of context as a limited \"attention budget\". This is a consequence of the attention mechanism itself as it gets harder to model the pairwise relationships - think of it like getting harder to focus on things far apart.\n\nGPT-5.2 has a context window of 400K input tokens. Opus 4.5 has 200K. Gemini 3 Pro has a 1M context window length. Now the effectiveness of these context windows can vary too, just the length doesn't matter. That said if you want to ask something\nfrom a 900K long input, you would be able to most reliably do that only with Gemini 3 Pro.\n\nContext rot article goes deep into some experiments which showed performance\ndrops with length and not task difficulty.\n\nA rough corollary one can draw is effective context windows are probably 50-60% or even lesser.\nDon't start a complicated task when you are half-way in the conversation. Do compaction or start a new one.\n\nEverything being done in prompts and code we have seen so far has been to -\n\nThe next few sections showcase features and implementation that are designed for\nbetter context management and agentic performance.\n\nI am personally not a fan of MCP servers but we gotta cover it. MCP servers are servers that can be hosted on your machine or remotely on the internet. These may expose filesystem, tools and integrations like CRM, Google Drive etc. They are essentially a way for models to connect to external tools and services.\n\nIn order to connect to MCP server, you need a host (Claude) which can house the MCP client. The MCP client\ncan invoke the protocol to connect. Once connected, the MCP client exposes tools, resources, prompts provided by server.\n\nThe tool definitions are loaded upfront into the context window of host bloating the context window.\n\nI like the idea of Code Execution with MCP even though it's propanda for more token consumption.\n\nQuoting Code execution with MCP:\n\nAs MCP usage scales, there are two common patterns that can increase agent cost and latency:\n\nMCP usage scale implies more MCP clients ~ more tool call definitions in context window.\n\nMCP Code exec suggests instead of direct tool calls, expose code APIs rather than tool call definitions and give Claude\na sandbox execution environment with a filesystem. Then let it write code to make the tool calls.\nIt is an elegant idea and is pretty similar to skills in the sense it's \"prompt on demand\"\n\nQuoting from Manus's Context Engineering Lesson blog:\n\nManipulate Attention Through Recitation\n\nIf you've worked with Manus, you've probably noticed something curious: when handling complex tasks, it tends to create a todo.md file—and update it step-by-step as the task progresses, checking off completed items.\n\nThat's not just cute behavior—it's a deliberate mechanism to manipulate attention.\n\nA typical task in Manus requires around 50 tool calls on average. That's a long loop—and since Manus relies on LLMs for decision-making, it's vulnerable to drifting off-topic or forgetting earlier goals, especially in long contexts or complicated tasks.\n\nClaude Code has todo lists but they don't show them now. Now you know part of the logic for it.\n\nClaude Code also tries something similar via plugging reminder tags into user messages and tool results. Some of them are mentioned in tool descriptions, other reminders are added at runtime via code.\n\nI asked Claude about what system reminders are present in the system prompt.\n\nFor reference, an older version of CC 2.0.56 used to have this detailed reminder system-reminder-plan-mode-is-active.\n\nI think Armin talks about this in his post What Actually Is Claude Code’s Plan Mode? when he refers to recurring prompts to remind the agent.\n\nIf you look at the leaked prompts, you will notice there are like 2-3 prompts for plan mode and 2-3 tool schemas like ENTRY_PLAN_MODE_TOOL, EXIT_PLAN_MODE_TOOL. The latter would write down the output into a markdown file\nwhich you can access via /plan. Everything is a markdown.\n\nAnthropic introduced Agent Skills recently and these got recently adopted by Codex too. A skill\nis a folder containing a SKILL.md file, other referenceable files and code scripts that do some user-defined task.\n\nThe SKILL.md contains some meta-data via which LLM can know what skills are available (meta-data is added to system prompt)\nIf Claude feels the skill is relevant, it will perform a tool call to read the contents of skill and download the\ndomain expertise just like Neo in Matrix 1999. The code scripts may contain tools that Claude can use.\n\nNormally, to teach domain expertise, you would need to write all that info in system prompt and probably\neven tool call definitions. With skills, you don't have to do that as the model loads it on-demand.\nThis is especially useful when you are not sure if you require those instructions always.\n\nThe popular frontend-design plugin is actually a skill. You can check here\n\nHooks are available in Claude Code and Cursor. They allow you to observe when a certain stage in the agent loop\nlifecycle starts or ends and let you run bash scripts before or after to make changes to the agent loop.\n\nThere are hooks like Stop, UserPromptSubmit etc. For instance Stop hook runs after Claude finishes responding and the UserPromptSubmit hook runs when user submits a prompt before Claude processes it.\n\nThe first hook I created was to play an anime notification sound when Claude stopped responding. I was obviously inspired\n\nOne funny use case to run Claude for hours might be running a \"Do more\" prompt when Claude finishes current task\nvia the Stop hook.\n\nI came across this post during my research for this blog post. This person beautifully combined the concepts and features we discussed so far. They combine hooks to act as reminders for skills. If the utility/requirement arises, there's a lot of space for customization. You might not need such heavy customization but can at least take inspiration. (Speaking for myself lol)\n\nAnthropic recommends to keep skill.md under 500 lines so they divided it into separate files and combined with hooks and\nreduced the size of their CLAUDE.md.\n\nHopefully you learnt a bunch of things from this super long post and will apply the learnings not only in CC\nbut other tools as well. I feel a bit weird writing this but we are going through some transformative times.\nThere are already moments when I almost feel like a background agent and then other times when I feel smart when the models couldn't solve a particular bug.\n\nclaude and codex to me when i realise i am the background agent pic.twitter.com/wkihYFQmQM\n\nI no longer look forward to new releases because they just keep happening anyways (shoutout to OpenAI). Deepseek and Kimi K3 are in the queue.\n\nI am expecting improvements in RL training, long context effectiveness via maybe new attention architectures, higher throughput models, lesser hallucination models.\nThere might be a o1/o3 level reasoning breakthrough or maybe something in continual learning in 2026. I look forward to these but at the same time\nI find it scary because more significant capability unlock will make the world unpredictable haha.\n\nIf you found this useful, try one new feature from this post today. Happy building!\n\nThanks for reading. Please like/share/RT the post if you liked it.",
    "readingTime": 30,
    "keywords": [
      "claude code",
      "karpathy sensei",
      "stop hook",
      "mcp client",
      "anthropic doesn't",
      "mcp servers",
      "feedback loops",
      "spam ultrathink",
      "monitoring log",
      "domain expertise"
    ],
    "qualityScore": 1,
    "link": "https://sankalp.bearblog.dev/my-experience-with-claude-code-20-and-how-to-get-better-at-using-coding-agents/",
    "thumbnail_url": "https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp/dario-2.webp",
    "created_at": "2025-12-28T01:03:19.149Z",
    "topic": "tech"
  },
  {
    "slug": "codenhack-a-free-gamified-inbrowser-coding-platform",
    "title": "Codenhack – A free, gamified, in-browser coding platform",
    "description": "A coding practice website for all programming levels – Join a community and learn from experienced developers.",
    "fullText": "Dive into the world of programming with our learning platform. Break through the matrix and emerge as a coding legend.\n\nBecause you learn faster when you build. With interactive courses, guided projects, and a vibrant community, Codenhack helps you go from beginner to builder, all inside your browser\n\nDive into structured learning paths with chapters and lessons designed to take you from beginner to expert.\n\nApply your knowledge by building production-ready projects. From web applications to Quantum Robot systems, we cover it all.\n\nShare your knowledge, insights, and experiences with our global community of developers. Create engaging blog posts, tutorials, and guides.\n\nShare your knowledge, insights, and experiences with our global community of developers. Create engaging blog posts, tutorials, and guides while learning from others in the field.\n\nMaster coding through interactive lessons. Read, practice, see instant results, and test your understanding—all in one seamless learning experience.\n\nStart your learning journey with a concise introduction that sets the context and prepares you for hands-on practice.\n\nWatch your code come to life in real-time with our live preview feature. See instant visual feedback as you write and modify your code, making learning interactive and engaging.\n\nPractice coding directly in our interactive terminal. Get real-time feedback as you build your skills through hands-on experience with command-line tools.\n\nReinforce your learning through reflection and self-assessment. Test your understanding and confidently progress to the next challenge.\n\nMaster the most powerful programming languages and take your skills to the next level. Whether you're a beginner or an expert, there's always something new to learn. Start your journey today!\n\nExplore our cutting-edge curriculum designed to transform beginners into elite coders ready for the digital future.\n\nThe foundation of the digital grid – structure beneath the neon skyline.\n\nDesign the web like a chrome-plated city. Style with flair.\n\nMake the neon signs blink, the systems dance – control the DOM in real-time.\n\nHear from the cyberpunks who've leveled up their coding skills and transformed their careers.\n\n\"I was impressed by how Codenhack blends programming with a cyberpunk vibe — it makes learning way more exciting\"\n\n\"Codenhack makes it feel like you're not just learning — you're discovering tools to break through the system. Even as a beginner, I'm already building small hacks and projects that actually do stuff. That feeling? Wild.\"\n\n\"Codenhack helped me turn boring code into something that actually looks good. Playing with HTML and CSS here feels like crafting my own digital art — and I'm just getting started\"\n\n\"I love that Codenhack lets me preview my work instantly — it's super satisfying to see my code come alive right away.\"",
    "readingTime": 3,
    "keywords": [
      "developers create",
      "create engaging",
      "blog posts",
      "posts tutorials",
      "knowledge insights",
      "learning",
      "coding",
      "interactive",
      "beginner",
      "programming"
    ],
    "qualityScore": 1,
    "link": "https://codenhack.com/",
    "thumbnail_url": "https://pub-49e1acf3b61b485e8f14983f79b20721.r2.dev/app_images/helpcenter.png",
    "created_at": "2025-12-26T12:22:24.821Z",
    "topic": "tech"
  },
  {
    "slug": "apples-app-course-runs-20k-a-student-is-it-worth-it",
    "title": "Apple's App Course Runs $20k a Student. Is It Worth It?",
    "description": "Apple, Michigan taxpayers, and one of Detroit’s wealthiest families spent roughly $30 million training hundreds of people to build iPhone apps. Not everyone lands coding jobs right away.",
    "fullText": "Lizmary Fernandez took a detour from studying to be an immigration attorney to join a free Apple course for making iPhone apps. The Apple Developer Academy in Detroit launched as part of the company’s $200 million response to the Black Lives Matter protests and aims to expand opportunities for people of color in the country’s poorest big city.\n\nBut Fernandez found the program’s cost-of-living stipend lacking—“A lot of us got on food stamps,” she says—and the coursework insufficient for landing a coding job. “I didn’t have the experience or portfolio,” says the 25-year-old, who is now a flight attendant and preparing to apply to law school. “Coding is not something I got back to.”\n\nSince 2021, the academy has welcomed over 1,700 students, a racially diverse mix with varying levels of tech literacy and financial flexibility. About 600 students, including Fernandez, have completed its 10-month course of half-days at Michigan State University, which cosponsors the Apple-branded and Apple-focused program.\n\nWIRED reviewed contracts and budgets and spoke with officials and graduates for the first in-depth examination of the nearly $30 million invested in the academy over the past four years—almost 30 percent of which came from Michigan taxpayers and the university’s regular students. As tech giants begin pouring billions of dollars into AI-related job training courses across the country, the Apple academy offers lessons on the challenges of uplifting diverse communities.\n\nSeven graduates who spoke with WIRED said they had good experiences at the academy, citing benefits such as receiving mentorship from past students. Fernandez says she was impressed by a focus on developing inclusive apps and a series of speakers from Apple who were genuinely willing to help and share frank lessons. “Their heart was in the right place,” she says.\n\nThe program does expose people of color to new possibilities. “It changed my life,” says Min Thu Khine, who’s now mentoring coding students and working at an Apple Store Genius Bar. “My dream is to be a software engineer at Apple.”\n\nThe academy also draws positive grades from some researchers who study tech education, such as Quinn Burke. He says its fully subsidized in-person instruction surpasses the quality of many coding boot camps, which proliferated over the past decade and sometimes left students in debt and with narrow skills.\n\nBut the academy being open to all can complicate instruction and how to measure success. One entire family attended together, and at least two mothers have come with their daughters. Students on average are in their thirties, ranging from 18-year-olds to, for example, a grandfather in his seventies who wanted to develop a photo app for his grandchild, according to Sarah Gretter, the academy leader for Michigan State.",
    "readingTime": 3,
    "keywords": [
      "students",
      "coding",
      "tech",
      "academy",
      "course",
      "apps",
      "color",
      "diverse",
      "program",
      "wired"
    ],
    "qualityScore": 1,
    "link": "https://www.wired.com/story/apple-app-making-course-michigan-state-university/",
    "thumbnail_url": "https://media.wired.com/photos/69406bdd9f9b98727b285b83/191:100/w_1280,c_limit/Apples-App-Making%20Course-Costing-20k-A-Student-Business-2225891099.jpg",
    "created_at": "2025-12-26T06:19:00.674Z",
    "topic": "tech"
  },
  {
    "slug": "keystone-yc-s25-is-hiring-engineer-1-to-automate-coding",
    "title": "Keystone (YC S25) is hiring engineer #1 to automate coding",
    "description": "About Keystone\nWe're building AI-native error monitoring that automatically investigates production issues and generates code fixes. Think Sentry, but built from the ground up for a world where AI can actually understand your codebase, trace through logs, and tell you exactly what broke and how to fix it.\nWe're starting here and expanding until we're the default tool for building product, period. Our mission is to free engineers from the drudgery of digging through logs, setting up systems, and debugging- so they can focus on understanding users and designing great products.\nWe're in-person in SoMa, San Francisco.",
    "fullText": "We're building AI-native error monitoring that automatically investigates production issues and generates code fixes. Think Sentry, but built from the ground up for a world where AI can actually understand your codebase, trace through logs, and tell you exactly what broke and how to fix it.\n\nWe're starting here and expanding until we're the default tool for building product, period. Our mission is to free engineers from the drudgery of digging through logs, setting up systems, and debugging- so they can focus on understanding users and designing great products.\n\nWe're in-person in SoMa, San Francisco. We raised a $5.2M seed from True Ventures, Twenty Two Ventures, Pear VC, and Ritual Capital- plus the founders of YC, Dropbox, Supabase, Eight Sleep, Graphite, Resend, RocketMoney, and more. Early design partners include teams at Perplexity and Lovable.\n\nYou'd be the first engineering hire, working directly with me (the founder) to build the core product. You'll have more ownership and influence over the product, culture, and technical direction than you'd get almost anywhere else.\n\nYou might be a great fit if you:\n\nStack: TypeScript, React (Next.js), Python, Postgres, Redis, AWS\n\nComp & benefits: Top-of-market salary + significant equity, full health/dental/vision, all meals covered, equipment budget",
    "readingTime": 2,
    "keywords": [
      "we're",
      "product",
      "logs",
      "ventures",
      "you'd"
    ],
    "qualityScore": 0.85,
    "link": "https://www.ycombinator.com/companies/keystone/jobs/J3t9XeM-founding-engineer",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/06e7837c1e5c8ce88da333ac2efcf401d8cbee53.png?1747973680",
    "created_at": "2025-12-25T00:56:12.872Z",
    "topic": "jobs"
  },
  {
    "slug": "the-guy-who-coined-vibe-coding-predicts-it-will-terraform-software-and-alter-job-descriptions",
    "title": "The guy who coined 'vibe coding' predicts it will 'terraform software and alter job descriptions'",
    "description": "Andrej Karpathy led AI at Tesla and cofounded OpenAI. He wrote that vibe coding has produced a new type of code that is \"free\" and \"discardable.\"",
    "fullText": "He coined \"vibe coding\" earlier this year. Now, he has something to say about it.\n\nAndrej Karpathy led AI at Tesla for five years, steering the company's Autopilot effort and briefly working on its humanoid robot Optimus. He sandwiched his Tesla job with two stints at OpenAI, making Karpathy a cofounder of the AI pioneer.\n\nAs 2025 comes to a close, Karpathy published his year-in-review for large language models on X. He reflected on the famous term he originated in February, a term that has since shaken up the software engineering industry.\n\n\"With vibe coding, programming is not strictly reserved for highly trained professionals,\" Karpathy wrote. He called it an example of how \"regular people benefit a lot more from LLMs compared to professionals, corporations and governments.\"\n\nVibe coding has likely benefited businesses, too. Tech companies have equipped their engineers with tools like Cursor, Claude Code, and OpenAI's Codex, aiming for productivity gains.\n\nKarpathy wrote that vibe coding \"empowers trained professionals to write a lot more (vibe coded) software that would otherwise never be written.\"\n\nIt may also change the makeup — or the use case — of the code itself. Karpathy threw out a slew of adjectives to describe this new body of code: It is \"free, ephemeral, malleable, discardable after single use.\"\n\n\"Vibe coding will terraform software and alter job descriptions,\" he wrote.\n\nHow does Karpathy feel about being the term's origin?\n\n\"Amusingly, I coined the term \"vibe coding\" in this shower of thoughts tweet totally oblivious to how far it would go,\" he wrote.\n\nThere's a new kind of coding I call \"vibe coding\", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It's possible because the LLMs (e.g. Cursor Composer w Sonnet) are getting too good. Also I just talk to Composer with SuperWhisper…\n\nIt's not yet clear how efficient vibe coding is making engineers. In a METR study published in July, AI coding assistants were found to decrease the productivity of participating experienced software developers by 19%. The developers in that study were also overconfident in the tools, its authors said, expecting a 20% productivity boost even after using them.\n\nWhat is clear, though, is that the practice is unlocking a whole new form of tech products. Twitter founder Jack Dorsey vibe-coded a new messaging app this year. Non-technical workers are easily building, shipping, and, in some cases, even selling apps they build in hours, if not minutes.\n\nKarpathy gave some other reflections. He praised Google Gemini's Nano Banana image model, and wrote that Claude Code was the \"first convincing demonstration of what an LLM Agent looks like.\"\n\nOverall, Karpathy wrote that 2025 was an \"exciting and mildly surprising year of LLMs.\"",
    "readingTime": 3,
    "keywords": [
      "trained professionals",
      "vibe coding",
      "software",
      "llms",
      "productivity",
      "karpathy",
      "coined",
      "tesla",
      "published",
      "tech"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-coined-vibecoding-ai-prediction-2025-12",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2025-12-23T18:17:54.745Z",
    "topic": "finance"
  },
  {
    "slug": "the-death-and-rebirth-of-programming",
    "title": "The Death and Rebirth of Programming",
    "description": "Programming didn't die all at once. There was no single moment, no dramatic obsolescence event. Instead, something quieter happened: the core constraint that shaped software for seventy years dissolved. Writing code stopped being the hard part.",
    "fullText": "For most of computing history, programming was bottlenecked by human cognition. Translating intent into working software required time, attention, and specialized skill. Even small changes were costly. This scarcity justified entire ecosystems: languages, frameworks, methodologies, reviews, team rituals that made sense when every line was expensive.\n\nGenerative AI removes that scarcity.\n\nToday, a single developer can generate thousands of lines of working code in minutes. Tomorrow, that number will be effectively infinite. The marginal cost of producing code is collapsing toward zero.\n\nWhat hasn't collapsed is the cost of knowing what the code does.\n\nUnderstanding, verifying, securing, and evolving software remain stubbornly expensive. In fact, they may be getting harder as volume explodes. This asymmetry—the ease of creation versus the difficulty of comprehension—is the defining tension of modern software.\n\nProgramming hasn't disappeared. But its center of gravity has shifted.\n\nIn the old world, programmers owned code. You wrote it, you understood it, you maintained it. Your value was tied to mastery of specific implementations. Codebases accrued history, reputation, and power.\n\nIn the new world, ownership becomes a liability.\n\nWhen code can be regenerated faster than it can be understood, preserving it for sentimental or historical reasons no longer makes sense. What matters instead is stewardship: maintaining the system's behavior, boundaries, and intent over time, regardless of how many times its internals are replaced.\n\nThis reframing is subtle but profound:\n\nThe asset is no longer the codebase. The asset is the system's ability to keep working.\n\nThis is the thesis of everything that follows. Architecture, testing, interfaces, team structure: all of it flows from this inversion.\n\nMany of the \"modern\" software practices of the last decade were early adaptations to this shift, even if we didn't articulate them that way.\n\nImmutable infrastructure. Stateless services. Containers. Blue-green deployments. Infrastructure as code.\n\nThese ideas all share a common premise: never fix a running thing. Replace it.\n\nAI pushes this premise beyond infrastructure and into application code itself. When rewriting is cheap, editing in place becomes risky. Mutation accumulates entropy. Replacement resets it.\n\nDisposability stops being a hack. It becomes the default.\n\nThis transition isn't just technical. It's deeply psychological, and that psychology shapes architecture.\n\nMany developers identify as builders and craftspeople. We take pride in elegance, cleverness, and mastery of internals. We accumulate knowledge inside our heads and inside codebases. Longevity feels like validation.\n\nGenerative AI destabilizes this identity.\n\nWhen a machine can produce a competent version of \"your\" solution in seconds, craftsmanship no longer lies in the artifact. It lies in framing the problem, defining success, and deciding what to keep and what to discard.\n\nThe role shifts from maker to architect. From author to managing editor. From preserving code to designing for its replacement.\n\nThat shift is uncomfortable. And the discomfort isn't merely personal. It's what makes teams resist the very patterns that would help them. Developers cling to codebases because identity is at stake, not just technical judgment. Acknowledging this is the first step toward building systems that don't require heroics to change.\n\nResisting the shift doesn't stop it. It just makes systems more fragile.\n\nOne of the clearest signals of this new era is the rise of the n=1 developer.\n\nProjects that once required teams now fit inside a single person's cognitive boundary—with AI filling in the execution gaps. Entire products can be specified, generated, evaluated, and shipped by one human working with machines.\n\nThis isn't about productivity hacks. It's about a structural change in leverage.\n\nBut n=1 development only works if systems are designed for it. Large, tangled, historically accreted codebases collapse under their own weight when AI accelerates change. Small, modular, disposable systems thrive.\n\nThe n=1 developer is not a superhero. They are an indicator species. They are evidence that the environment has changed, and proof that the new patterns actually work.\n\nIt's tempting to frame this as the \"end of programming.\" That's misleading.\n\nWhat's dying is a specific form of programming: one that equates value with authored code, longevity of code with quality, and maintenance with virtue.\n\nWhat's being born is something closer to systems design as an ongoing process of regeneration:\n\nCode becomes an intermediate artifact, not the final product. Rewrites become routine, not traumatic. Tests and evaluations define truth, not files. Stability emerges from replacement, not preservation.\n\nThis is not nihilism. It's pragmatism under new constraints.\n\nThe rest of this publication builds on a single premise established here:\n\nWhen code is cheap and understanding is expensive, architecture must optimize for the impermanence of code.\n\nEverything else (pace layers, evaluations, clean interfaces, regeneration workflows) flows from that fact.\n\nWe are not entering a world with less software. We are entering a world with vastly more of it. The only way to survive that abundance is to stop treating code as precious.\n\nBut it has been reborn, and it expects us to change with it.",
    "readingTime": 5,
    "keywords": [
      "modern software",
      "generative ai",
      "code",
      "it's",
      "systems",
      "programming",
      "codebases",
      "expensive",
      "developer",
      "longer"
    ],
    "qualityScore": 1,
    "link": "https://aicoding.leaflet.pub/3malrv6poy22a",
    "thumbnail_url": "https://leaflet.pub/lish/did%253Aplc%253A4qsyxmnsblo4luuycm3572bq/3majnsnvafs2b/3malrv6poy22a/opengraph-image?6815eb61f733905a",
    "created_at": "2025-12-23T00:56:32.109Z",
    "topic": "tech"
  },
  {
    "slug": "browserforge-ai-browser-agents-1000-free-credits",
    "title": "BrowserForge – AI browser agents (1000 free credits)",
    "description": "AI browser agents that automate web tasks 24/7. Extract data, fill forms, monitor prices, and handle any repetitive browser work. No coding required - just show your agent what to do.",
    "fullText": "Agents navigate websites like humans—clicking buttons, filling forms, extracting data, and monitoring changes while maintaining authenticated sessions.\n\nIntelligent agents that understand web interfaces, adapt to layout changes, and handle complex multi-step workflows across any website.\n\nSet agents to continuously monitor websites for price changes, new listings, content updates, or any custom conditions you define.\n\nConnect browser agents to your existing tools via API, webhooks, or custom integrations to create seamless automated workflows.\n\nAgents navigate websites like humans—clicking buttons, filling forms, extracting data, and monitoring changes while maintaining authenticated sessions.\n\nIntelligent agents that understand web interfaces, adapt to layout changes, and handle complex multi-step workflows across any website.\n\nSet agents to continuously monitor websites for price changes, new listings, content updates, or any custom conditions you define.\n\nConnect browser agents to your existing tools via API, webhooks, or custom integrations to create seamless automated workflows.\n\nAgents navigate websites like humans—clicking buttons, filling forms, extracting data, and monitoring changes while maintaining authenticated sessions.\n\nIntelligent agents that understand web interfaces, adapt to layout changes, and handle complex multi-step workflows across any website.\n\nSet agents to continuously monitor websites for price changes, new listings, content updates, or any custom conditions you define.\n\nConnect browser agents to your existing tools via API, webhooks, or custom integrations to create seamless automated workflows.",
    "readingTime": 2,
    "keywords": [
      "sessions intelligent",
      "define connect",
      "connect browser",
      "via api",
      "api webhooks",
      "intelligent agents",
      "humans—clicking buttons",
      "buttons filling",
      "maintaining authenticated",
      "understand web"
    ],
    "qualityScore": 0.85,
    "link": "https://www.browserforge.ai/",
    "thumbnail_url": "https://browserforge.ai/media/browserforge-hero-1.png",
    "created_at": "2025-12-22T18:17:58.401Z",
    "topic": "tech"
  },
  {
    "slug": "a-selfassessment-quiz-to-measure-software-development-seniority-level",
    "title": "A self-assessment quiz to measure software development seniority level",
    "description": "Take a free quiz based on real-world achievements and see your software developer level against cross-industry benchmarks.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://mylevel.dev/",
    "thumbnail_url": "https://storage.tally.so/53a7ed1d-311d-4027-be0d-d842b419a4a7/Level-measurement.jpeg",
    "created_at": "2025-12-21T01:00:00.485Z",
    "topic": "tech"
  },
  {
    "slug": "ai-tools-make-coders-more-important-not-less",
    "title": "AI Tools Make Coders More Important, Not Less",
    "description": "Many leaders are excited about the promise of AI coding tools that can make it easier for novices to write code and, seemingly, make experienced coders less essential. Yet these tools make experience more—not less—important, as AI is not a replacement for real engineers. Companies that want to use these tools should follow common rules. Make sure every change it makes is double-checked—with automatic checks, simple tests that confirm things still work, and at least one human review. Keep access limited: Let AI work only in a safe “practice” environment, never give it the keys to live customer data, and routinely check for basic security mistakes like files or storage left open to the public.",
    "fullText": "AI Tools Make Coders More Important, Not Less by Michael LiDecember 19, 2025PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintOf all the possible applications of generative AI, the value proposition of using it to write code was perhaps the clearest. Coding can be slow and it requires expertise, both of which can be expensive. Moreover, the promise that anyone who could describe their idea in plain text could create apps, features, or other value-adding products meant that innovation would no longer be limited to those with the skills to execute, but could be done by anyone with an idea. The strength of this promise has created a $7.37 billion market for these tools.",
    "readingTime": 1,
    "keywords": [
      "tools",
      "promise",
      "anyone",
      "idea"
    ],
    "qualityScore": 0.45,
    "link": "https://hbr.org/2025/12/ai-tools-make-coders-more-important-not-less",
    "thumbnail_url": "/resources/images/article_assets/2025/12/Dec25_22t-kaiser-7uH3ndea63o-unsplash.jpg",
    "created_at": "2025-12-19T18:17:22.482Z",
    "topic": "business"
  },
  {
    "slug": "introduction-to-programming-the-commodore-pet",
    "title": "Introduction to Programming the Commodore PET",
    "description": "History of the Commodore PET and how to approach programming the classic system",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://retrogamecoders.com/introduction-to-programming-the-commodore-pet/",
    "thumbnail_url": "https://retrogamecoders.com/wp-content/uploads/2025/12/Programming-the-Commodore-PET.jpg",
    "created_at": "2025-12-19T12:22:25.882Z",
    "topic": "tech"
  },
  {
    "slug": "metas-28yearold-billionaire-prodigy-says-the-next-bill-gates-will-be-a-13yearold-who-is-vibe-coding-right-now",
    "title": "Meta’s 28-year-old billionaire prodigy says the next Bill Gates will be a 13-year-old who is ‘vibe coding’ right now",
    "description": "Teenagers, Alexandr Wang argues, have a built-in edge.",
    "fullText": "Eva is a fellow on Fortune's news desk.\n\nAlexandr Wang—who became the world’s youngest self-made billionaire at 24—is now, at 28, running one of the most ambitious AI efforts in Silicon Valley. In his first 60 days at Meta, he built a 100-person lab he described to TBPN hosts John Coogan and Jordi Hays as “smaller and more talent dense than any of the other labs.”\n\nHis goal: nothing less than superintelligence.\n\nWang, with his aerial view of the industry, has advice for kids, especially those in Gen Alpha now entering middle school: Forget gaming, sports, or traditional after-school hobbies.\n\n“If you are like 13 years old, you should spend all of your time vibe coding,” he said in his recent TBPN interview. “That’s how you should live your life.”\n\nFor Wang, the reasoning is simple. Every engineer, himself included, is now writing code he believes will be obsolete within five years.\n\n“Literally all the code I’ve written in my life will be replaced by what will be produced by an AI model,” he said.\n\nThat realization has left him, in his words, “radicalized by AI coding.” What matters most now isn’t syntax, or learning a particular language, but time spent experimenting with and steering AI tools.\n\n“It’s actually an incredible moment of discontinuity,” Wang said. “If you just happen to spend 10,000 hours playing with the tools and figuring out how to use them better than other people, that’s a huge advantage.”\n\nTeenagers have a clear advantage over adults: time and freedom to immerse themselves in new technology. And while in the past, entrepreneurial teenagers leveraged this time to be “sneaker flippers” or run Minecraft servers, Wang says the focus should now be on the code.\n\nHe compares the moment to the dawn of the PC revolution. The Bill Gateses and Mark Zuckerbergs of the world had an “immense advantage” simply because they grew up tinkering with the earliest machines.\n\n“That moment is happening right now,” Wang said. “And the people who spend the most time with it will have the edge in the future economy.”\n\nWang isn’t coy about Meta’s ambitions. He calls the company’s infrastructure, scale, and product distribution unmatched.\n\n“We have the business model to support building literally hundreds of billions of dollars of compute,” he said.\n\nHis team, just over 100 people, is deliberately designed to be “smaller and more talent dense” than rivals. “The other labs are like 10 times bigger,” Wang said, but their lab had “cracked” coders.\n\nThe lab is split into three pillars: research, product, and infrastructure. Research builds the models Wang says will “ultimately be superintelligent.” Product ensures they get distributed across billions of users through Meta’s platforms. And infrastructure focuses on what he calls “literally the largest data centers in the world.”\n\nWang is particularly excited about hardware. Like many Meta executives now, he points to the company’s new smart glasses, which had a hilariously foppish demo, as the “natural delivery mechanism for superintelligence.”\n\nPlaced right next to the human senses, they will merge digital perception with cognition.\n\n“It will literally feel like cognitive enhancement,” Wang said. “You will gain 100 IQ points by having your superintelligence right next to you.”\n\nVibe coding is the shorthand for this shift: using natural language prompts to generate and iterate on code. Rather than writing complex syntax, users describe their intent, and AI produces functioning prototypes.\n\nThe concept is spreading across Silicon Valley’s C-suites. Klarna CEO Sebastian Siemiatkowski has said he can now test ideas in 20 minutes, instead of burning weeks of engineering cycles. Google CEO Sundar Pichai revealed that AI already generates more than 30% of new code at the company, calling it the biggest leap in software creation in 25 years.\n\nWang takes that further. For him, vibe coding isn’t just a productivity hack, but a future cultural mandate. What matters isn’t the code itself — it’s the hours of intuition-building that come from pushing AI tools to their limits, which is why he urges Gen Alpha to start early.\n\n“The role of an engineer is just very different now than it was before,” he said.\n\nA version of this story was published on Fortune.com on September 19, 2025.",
    "readingTime": 4,
    "keywords": [
      "talent dense",
      "vibe coding",
      "gen alpha",
      "code",
      "literally",
      "isn’t",
      "wang",
      "superintelligence",
      "tools",
      "moment"
    ],
    "qualityScore": 1,
    "link": "https://fortune.com/article/what-is-vibe-coding-alexandr-wang-bill-gates-meta/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/09/GettyImages-1540568935-e1758305593707.jpg?resize=1200,600",
    "created_at": "2025-12-19T12:22:22.800Z",
    "topic": "business"
  },
  {
    "slug": "orbit-a-systems-level-programming-language-that-compiles-sh-to-llvm",
    "title": "Orbit a systems level programming language that compiles .sh to LLVM",
    "description": "A modern shell with functional programming synatx. - SIE-Libraries/orbit",
    "fullText": "SIE-Libraries\n\n /\n\n orbit\n\n Public\n\n A modern shell with functional programming synatx.\n\n License\n\n View license\n\n 5\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n SIE-Libraries/orbit",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/SIE-Libraries/orbit",
    "thumbnail_url": "https://opengraph.githubassets.com/dcaf4ed1c20da5475350ce0f3bf442eb03297772f6ad4097b4b2009e9ba5d922/SIE-Libraries/orbit",
    "created_at": "2025-12-19T09:39:44.047Z",
    "topic": "tech"
  },
  {
    "slug": "codingforpreschoolers-firm-files-bankruptcy-after-covid-boom",
    "title": "Coding-for-Preschoolers Firm Files Bankruptcy After Covid Boom",
    "description": "An education company that helps children as young as three learn to code filed for bankruptcy, blaming an expansion strategy that outpaced its ability to turn a profit.",
    "fullText": "WealthBy Steven ChurchSaveAn education company that helps children as young as three learn to code filed for bankruptcy, blaming an expansion strategy that outpaced its ability to turn a profit.Conscious Content Media Inc. would eliminate more than half of its $205.5 million in funded debt under a reorganization proposal backed by noteholders, according to court papers filed Wednesday in federal court in Wilmington, Delaware.",
    "readingTime": 1,
    "keywords": [
      "filed",
      "court"
    ],
    "qualityScore": 0.2,
    "link": "https://www.bloomberg.com/news/articles/2025-12-18/coding-for-preschoolers-firm-files-bankruptcy-after-covid-boom",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iCA4eahCxR3k/v1/1200x800.jpg",
    "created_at": "2025-12-18T18:18:26.956Z",
    "topic": "finance"
  },
  {
    "slug": "i-built-an-app-for-vibecoding-games",
    "title": "I built an app for vibe-coding games",
    "description": "Got a game idea? Just describe it and start playing in seconds.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://playmix.ai/",
    "thumbnail_url": "https://playmix.ai/assets/og-image.png",
    "created_at": "2025-12-18T12:23:04.291Z",
    "topic": "tech"
  },
  {
    "slug": "learning-the-oldest-programming-language-2024",
    "title": "Learning the oldest programming language (2024)",
    "description": "Who needs Rust when we have Fortran?",
    "fullText": "While I probably should be learning a language like C, Go, or whatever new trendy language the ThePrimeagen mentions on Twitter (OCaml?), I'm going to attempt to learn Fortran[1].\n\nFortran, which stands for FORmula TRANslator[2], was created at IBM by John Backus in 1957 for scientific applications and has apparently been popular for high-performance computing and benchmarking supercomputers in recent years. Fortran has had several subsequent releases since then; FORTRAN 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, and the latest Fortran 2018.\n\nTo understand what version of Fortran to learn/use, we first must understand the difference between fixed form and free form Fortran. The fixed form layout comes from the very beginning of Fortran, inherited from punch cards, and has odd restrictions about the column in which comments and statements are placed. The free form layout, first introduced in Fortran 90, removed special columns and added the ability to write comments wherever, and is what we'll be learning in this article. The compiler we'll be using is GNU Fortran, or gfortran. You can install it via Homebrew (macOS) with the gcc formula, or install it using a package manager for your OS. To tell gfortran that your code uses the free form layout, set the file extension to .f90 or newer. The following comment on the Fortran discussion board explains this well.\n\nThe .f90 suffix means that the source code is free format, not that\nthe code conforms to the Fortran 90 standard. Code that uses the .f90\nsuffix can use features from any Fortran standard. All Fortran\ncompilers recognize .f90 as a suffix indicating free source form, but\nsome may not recognize a suffix such as .f95, .f03, .f08, or .f18.\nSome users may have build tools that do not recognize suffixes other\nthan .f90. Most Fortran source code on GitHub that uses features from\na standard more recent than Fortran 90 still uses the .f90 suffix.\n\nComing from TypeScript, and before that, Python, I'm very used to (and comfortable with) modern — you might say \"aesthetic\" — syntax . Although I wouldn't say Fortran syntax is quite modern, it seems to avoid the syntactic sugar nightmares that plague beginners in other languages[3]. Take a look at this helloworld.f90 example below.\n\nOlder Fortran programs required the use of SCREAMING_CASE for all keywords, but in modern Fortran you can and it is recommended to use snake_case (you can still use SCREAMING_CASE or any other case you want though).\n\nJust from this small example we can gather that...\n\nThe syntax for printing is a little funky though. What is that asterisk doing there? The asterisk, aside from being used as a mathematical operator, indicates the \"default\". So for print, * means \"print to the default output channel\" (or \"print to the default output file unit\" to be precise), which is typically going to be STDOUT.\n\nI can't find exactly where this is documented but you don't actually need the start and end program <program-name>; you could write a hello world program like this, though as I just mentioned this doesn't seem to be a common practice and isn't really very useful in any practical scenario.\n\nHere's another, slightly more complicated example.\n\nStarting right at the top, we have something new: implicit none. Added in Fortran 90, implicit none disables implicit typing defaults and all variables must be explicitly declared. In Fortran, implicit typing is the practice of assigning default types to variables based on the character a variable name begins with. Variables starting with I through N are INTEGERs, everything else is REAL. It is \"a legacy of the past\" and usage of an implicit none statement is \"strongly advised\" (implicit none - Fortran Wiki).\n\nA common Fortran joke goes along the lines of “GOD is REAL, unless declared INTEGER\"[4] because of implicit typing!\n\nMoving on, we declare our first variables in this program.\n\nHere we are declaring x, y, and answer with the REAL type, and choice with the CHARACTER type. The REAL type stores floating point numbers[5], and CHARACTER... stores characters.\n\nNext, we prompt the user for our x and y values.\n\nNotice how we can take input from the user with read and assign it to a value with the read *, <variable> syntax. The asterisk here means read from the default input channel/file unit, which would be STDIN.\n\nWe do the same for prompting the user to select an operation.\n\nFinally, we use a series of basic if-statements to calculate our answer and display it in the terminal.\n\nIf we run this, we- wait. Did I even tell you how to compile a Fortran program yet?\n\nFirst, compile our calculator program with gfortran -o calculator calculator.f90 . Then you can run it with ./calculator. If you only instruct gfortran of the input file (gfortran calculator.f90), the default output executable will be named a.out.\n\nOur calculator isn't perfect yet though. What if the user tries to divide by zero?\n\nProbably not the answer you expected. Let's try to fix that.\n\nHere we use the inequality operator, /=, to check if the y value is zero. Now, if the user tries to divide by zero, we'll print an error message and use the stop statement to end the program.\n\nGreat. We got rid of the zero division mess, but our code isn't pretty at all. Who wants a bunch of if statements? We can simplify this using the select case statement (also known as the case statement).\n\nThis also has the handy benefit of telling the user if they made an invalid choice while selecting the operation.\n\nThat’s just a quick introduction to a few modern Fortran features: declaring variables, printing and reading to and from the terminal, if and select case, and stop. Next time, we’ll talk more about where Fortran is actually used, cooler things you can build with it, and how the Fortran language & community are rapidly modernizing!\n\nIronically, in the ~3-ish months since I started writing this article, ThePrimagen has recently said he \"take[s] back everything i said about FORTRAN\" — apparently having some interest in the language! ↩︎\n\nAccording to sources listed on Fortran's Wikipedia, the name might also have stood for Formula Translating System or just Formula Translation. ↩︎\n\nSee The Rust programming language absolutely positively sucks : r/rust and Rust is a nightmare to learn coming from Java - community - The Rust Programming Language Forum. ↩︎\n\nThe first letter of \"GOD\", a \"G\", is not within I through N and is therefore of the REAL type (\"GOD is REAL\"). ↩︎\n\nYou can also use double precision for larger (more precise) floating point numbers. ↩︎",
    "readingTime": 6,
    "keywords": [
      "rust programming",
      "default output",
      "programming language",
      "implicit none",
      "implicit typing",
      "fortran standard",
      "modern fortran",
      "code",
      "user",
      "free"
    ],
    "qualityScore": 1,
    "link": "https://uncenter.dev/posts/learning-fortran/",
    "thumbnail_url": "https://uncenter.dev/1024w.png?v=2316a73de1f9",
    "created_at": "2025-12-17T13:45:44.913Z",
    "topic": "tech"
  },
  {
    "slug": "scrappy-free-ai-code-assistant",
    "title": "Scrappy Free AI Code Assistant",
    "description": "A powerful, context-aware coding assistant for everyone. Students, learners, anyone who doesn't want to pay for subscriptions. - HakAl/scrappy",
    "fullText": "HakAl\n\n /\n\n scrappy\n\n Public\n\n A powerful, context-aware coding assistant for everyone. Students, learners, anyone who doesn't want to pay for subscriptions.\n\n pypi.org/project/scrappy-ai/\n\n License\n\n MIT license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n HakAl/scrappy",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/HakAl/scrappy",
    "thumbnail_url": "https://opengraph.githubassets.com/f4b80c7981acbe65eb74d5f829f65f4c5ad79abda560cef1db5838b375628d52/HakAl/scrappy",
    "created_at": "2025-12-17T03:45:04.030Z",
    "topic": "tech"
  },
  {
    "slug": "adventures-in-the-land-of-language-servers",
    "title": "Adventures in the Land of Language Servers",
    "description": "Have you ever wondered how your editors and IDEs are able to support so many programming languages? Perhaps you've been thinking about designing your ow…",
    "fullText": "Have you ever wondered how your editors and IDEs are able to support so many programming languages? Perhaps you've been thinking about designing your own language and wanted to know how you can give it editor support?\n\nThis talk is for you - I've spent over a year building a small language and integrating it with code editors, and I'd like to share some of the challenges I've faced, as well as lessons I've learned in that time.\n\nI'll also show how easy it is to build a new Language Server project in Scala 3 thanks to the Langoustine library.",
    "readingTime": 1,
    "keywords": [
      "i've",
      "editors",
      "language"
    ],
    "qualityScore": 0.45,
    "link": "https://speakerdeck.com/kubukoz/adventures-in-the-land-of-language-servers",
    "thumbnail_url": "https://files.speakerdeck.com/presentations/548a94549c554eaf8a5d70effbc439b1/slide_0.jpg?25884509",
    "created_at": "2025-12-17T03:45:03.996Z",
    "topic": "tech"
  },
  {
    "slug": "optimizing-claude-code",
    "title": "Optimizing Claude Code",
    "description": "Customize Claude Code with skills, plugins, commands, and configuration files that transform a capable coding assistant into one that matches your exact workflow.",
    "fullText": "I’ve been using Claude Code for months now, and for most of that time, I was doing it wrong. Not wrong in the sense of getting bad results—the defaults are remarkably capable. Wrong in the sense that I was treating a customizable system like a fixed tool. I was adjusting my workflow to fit the AI instead of adjusting the AI to fit my workflow.\n\nThe difference between default Claude Code and a properly configured instance is the difference between hiring a talented generalist and hiring someone who’s worked at your company for years. Both can write code. Only one knows that your team prefers for…of over .forEach(), that you never use the I prefix on interfaces, and that when you say “analyze this bug,” you mean a specific six-step process that includes hypothesis testing.\n\nHere’s how I built that second version.\n\nClaude Code’s customization system has multiple layers, each serving a different purpose:\n\nWhat’s less widely appreciated is how these layers interact. When Claude Code starts a session, it reads your settings, loads relevant skills based on context, and injects CLAUDE.md into its system prompt. When you invoke a command, it triggers a predefined workflow. When you mention a topic covered by a skill, Claude applies that expertise automatically.\n\nMy ~/.claude/settings.json is minimal but deliberate:\n\nalwaysThinkingEnabled: true — This enables extended thinking on every response. The tradeoff is latency for quality. For complex refactoring or architectural decisions, I want Claude to think deeply. For quick questions, it’s overkill. I keep it on because my typical use case is substantial engineering work.\n\nToken limits — Increasing CLAUDE_CODE_MAX_OUTPUT_TOKENS to 64000 prevents truncation on large refactors. The MAX_THINKING_TOKENS setting controls how much “thinking” space Claude has before responding.\n\nincludeCoAuthoredBy: false — I don’t need AI authorship attribution in every commit message. Personal preference.\n\nThe full settings file is available in my dotfiles repo.\n\nEvery project gets a CLAUDE.md at the root. This is where you encode project-specific knowledge: commands to build and test, directory structure, coding principles, workflow patterns.\n\nThe key insight: CLAUDE.md is a system prompt you control. Every instruction you put here shapes every response you get. You can define escalation patterns that tell Claude to stop thrashing and switch to a structured process after failed attempts—for example, my Bug Fix workflow triggers a 6-step root cause analysis after two failed fixes.\n\nFull template: CLAUDE.md in dotfiles\n\nSkills are markdown files that encode specialized knowledge. When Claude detects that a skill is relevant to your task, it applies that expertise automatically.\n\nThis skill encodes my team’s TypeScript conventions—things that aren’t in style guides but matter for consistency:\n\nThe full skill covers interfaces vs types, enum conventions, null handling, type assertions, and module imports. It’s 700+ lines because TypeScript has a lot of conventions worth encoding.\n\nFor Lambda, DynamoDB, and SQS patterns:\n\nThis skill catches AI-generated code patterns that don’t match human-written code:\n\nThis is something I’ve noticed consistently: AI models add defensive code and comments that human developers wouldn’t. Having a skill that explicitly tells Claude to avoid these patterns makes the output feel more natural.\n\nSkills directory: skills in dotfiles\n\nSkills teach Claude how to code. Hooks enforce that it does. The difference matters.\n\nA skill might say “prefer for…of over .forEach()“—but Claude can still forget. A hook catches it in real-time, warning or blocking before the code is written. It’s the difference between training and guardrails.\n\nI use the hookify plugin to create enforcement rules from simple markdown files. Here are my active hooks:\n\nHooks are markdown files with YAML frontmatter. Here’s an example that blocks as any casts:\n\nThe action field determines severity:\n\nThis is where customization compounds. My TypeScript patterns skill teaches Claude the conventions. My hooks enforce them. If Claude violates a convention—say, using as Type instead of <Type>—the hook catches it before the code is written.\n\nThe feedback loop is immediate: Claude sees the warning, adjusts its output, and continues. Since Claude Code is stateless between sessions, the hooks provide consistent enforcement every time. Skills inform, hooks enforce.\n\nHooks directory: hooks in dotfiles\n\nCommands are like shell aliases for Claude workflows. Instead of typing a detailed prompt, you invoke /analyze-bug or /simplify and get a consistent, structured response.\n\nCommands directory: commands in dotfiles\n\nAgent docs are markdown files in .claude/agent_docs/ that Claude reads when relevant. Unlike skills (which encode how to do things), agent docs provide reference material (what things are).\n\nCLAUDE.md tells Claude when to read each doc. More efficient than stuffing everything into context—Claude loads docs on demand.\n\nAgent docs: agent_docs in dotfiles\n\nPlugins add new tools and workflows to Claude’s toolkit. I use several, organized by purpose:\n\nast-grep — Structural code search using AST patterns. Better than regex for finding code patterns that span multiple lines or have variable formatting. When I need to find all functions that return a Promise but don’t handle errors, ast-grep finds them regardless of formatting. Requires the CLI tool installed separately:\n\ndev-browser — Browser automation for testing web applications. When I say “go to localhost:3000 and click the login button,” Claude can actually do that.\n\nfrontend-design — UI/UX design assistance for frontend work. Part of the official Claude Code plugins.\n\nhookify — Creates enforcement rules from markdown files (covered in the Hooks section above). The key plugin for active convention enforcement.\n\ncommit-commands — Three git workflow commands:\n\nfeature-dev — A 7-phase structured workflow for complex features:\n\nFor complex features that touch multiple files, /feature-dev ensures nothing is missed.\n\npr-review-toolkit — Six specialized review agents that run in parallel:\n\nWhen I say “review my PR,” these agents analyze different dimensions simultaneously and return prioritized findings.\n\nPlugins are installed via the Claude Code plugin system. Official plugins require adding the Anthropic marketplace first:\n\nThe configuration in settings.json enables them:\n\nRun this to install my skills, commands, and hooks:\n\nThe script sets up ~/.claude/ and prints the plugin commands to run inside Claude Code.\n\nIf you prefer to set things up yourself:\n\nCreate ~/.claude/skills/typescript-patterns/SKILL.md with your TypeScript conventions. The filename must be SKILL.md and include frontmatter with name and description.\n\nCreate ~/.claude/commands/analyze-bug.md with your debugging workflow. Commands are invoked with /analyze-bug (the filename becomes the command name).\n\nAfter installing hookify, create enforcement rules:\n\nHooks take effect immediately—no restart required.\n\nCreate CLAUDE.md at the root of each project with project-specific instructions.\n\nMy complete configuration: github.com/stevenmays/dotfiles/tree/master/ai/claude\n\nHere’s what I didn’t expect: these customizations compound.\n\nA skill that teaches TypeScript conventions means Claude knows my preferences. A hook that enforces those conventions means Claude can’t forget them. A command that structures bug investigation means debugging follows a consistent process. A plugin that runs six review agents in parallel means PR reviews are thorough without being tedious.\n\nEach layer reinforces the others:\n\nThe time investment—maybe a few hours total—pays dividends on every subsequent session. I’m not constantly re-explaining preferences or correcting patterns. Claude already knows. And when it forgets, the hooks catch it.\n\nIt’s not that X is bad and Y is good, exactly; it’s more that default Claude Code is a capable generalist, while optimized Claude Code is a specialist who happens to share your opinions about code style, your workflow preferences, and your debugging methodology.\n\nThe choice is simple: accept defaults, work around quirks, and occasionally complain about AI-generated code that doesn’t match your style—or spend a few hours getting dialed in.",
    "readingTime": 7,
    "keywords": [
      "agent docs",
      "create enforcement",
      "typescript conventions",
      "expertise automatically",
      "claude code",
      "ai-generated code",
      "complex features",
      "hook catches",
      "review agents",
      "default claude"
    ],
    "qualityScore": 1,
    "link": "https://mays.co/optimizing-claude-code",
    "thumbnail_url": "https://mays.co/_astro/optimizing-claude-code.DOeMUOQN_Z2lns9R.jpg",
    "created_at": "2025-12-17T03:45:03.284Z",
    "topic": "tech"
  },
  {
    "slug": "nvidia-unveils-new-opensource-ai-models-amid-boom-in-chinese-offerings",
    "title": "Nvidia unveils new open-source AI models amid boom in Chinese offerings",
    "description": "Nvidia on Monday unveiled a new family of open-source artificial intelligence models that it says will be faster, cheaper and ​smarter than its previous offerings, as open-source offerings from Chinese AI labs proliferate.  Nvidia ‌is primarily known for providing chips that firms such as OpenAI use to train their closed-source models and ‌charge money for them.  Nvidia ⁠on Monday revealed the third ‌generation of its \"Nemotron\" large-language models aimed at writing, coding and other tasks.",
    "fullText": "SAN FRANCISCO, Dec 15 (Reuters) - Nvidia (NVDA) on Monday unveiled a new family of open-source artificial intelligence models that it says will be faster, cheaper and ​smarter than its previous offerings, as open-source offerings from Chinese AI labs proliferate.\n\nNvidia ‌is primarily known for providing chips that firms such as OpenAI use to train their closed-source models and ‌charge money for them. But it also offers a slew of its own models for everything from physics simulations to self-driving vehicles as open-source software that can be used by researchers or by other companies, with firms such as Palantir Technologies weaving Nvidia's model into their products.\n\nNvidia ⁠on Monday revealed the third ‌generation of its \"Nemotron\" large-language models aimed at writing, coding and other tasks. The smallest of the models, called Nemotron 3 Nano, was being released ‍Monday, with two other, larger versions coming in the first half of 2026.\n\nNvidia, which has become the world's most valuable listed company, said that Nemotron 3 Nano was more efficient than its predecessor - ​meaning it would be cheaper to run - and would do better at long tasks ‌with multiple steps.\n\nNvidia is releasing the models as open-source offerings from Chinese tech firms such as DeepSeek, Moonshot AI and Alibaba Group Holdings are becoming widely used in the tech industry, with companies such as Airbnb disclosing use of Alibaba' s. (BABA) Qwen open-source model.\n\nAt the same time, CNBC and Bloomberg have reported that Meta Platforms is considering shifting toward closed-source ⁠models, leaving Nvidia as one of the most prominent ​U.S. providers of open-source offerings.\n\nMany U.S. states and ​government entities have banned use of Chinese models over security concerns.\n\nKari Briski, vice president of generative AI software for enterprise at Nvidia, said the company aimed ‍to provide a \"model that ⁠people can depend on\", and was also openly releasing its training data and other tools so that government and business users could test it for security and ⁠customize it.\n\n\"This is why we're treating it like a library,\" Briski told Reuters in an interview. \"This is ‌why we're committed to it from a software engineering perspective.\"",
    "readingTime": 2,
    "keywords": [
      "open-source offerings",
      "closed-source models",
      "nemotron nano",
      "nvidia",
      "firms",
      "software",
      "reuters",
      "cheaper",
      "aimed",
      "tasks"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/nvidia-unveils-open-source-ai-140424565.html",
    "thumbnail_url": "https://media.zenfs.com/en/reuters-finance.com/31397226d241d376e5cffbf13490e071",
    "created_at": "2025-12-16T13:51:41.754Z",
    "topic": "finance"
  },
  {
    "slug": "claude-codes-creator-explains-the-limits-of-vibe-coding",
    "title": "Claude Code's creator explains the limits of vibe coding",
    "description": "The engineer behind Claude Code says vibe coding works for prototypes, but today's AI models still fall short for maintainable software.",
    "fullText": "The creator of one of the most popular AI coding tools says vibe coding can only go so far.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said on an episode of \"The Peterman Podcast\" published Monday that while vibe coding has its place, it's far from a universal solution.\n\nIt works well for \"throwaway code and prototypes, code that's not in the critical path,\" he said.\n\n\"I do this all the time, but it's definitely not the thing you want to do all the time,\" Cherny said, referring to vibe coding.\n\n\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he added.\n\nClaude Code launched earlier this year as part of Anthropic's efforts to integrate AI more deeply into code development workflows.\n\nTop AI coding services like Cursor and Augment run on Anthropic's models, and even Meta uses Anthropic's models inside its coding assistant. Claude Code has also taken off with non-technical developers who want to build software with natural-language prompts.\n\nAnthropic's CEO, Dario Amodei, said in October that Claude was writing 90% of the code in the company.\n\nFor critical coding tasks, Cherny said he typically pairs with a model to write code.\n\nHe starts by asking an AI model to generate a plan, then iterates on the implementation in small steps. \"I might ask it to improve the code or clean it up or so on,\" he said.\n\nFor parts of the system where he has strong technical opinions, Cherny said he still writes the code by hand.\n\nCherny said the models are still \"not great at coding.\"\n\n\"There's still so much room to improve, and this is the worst it's ever going to be,\" he said.\n\nCherny said it's \"insane\" to compare current tools to where AI coding was just a year ago, when it amounted to little more than type-ahead autocomplete. Now, it's a \"completely different world,\" he said, adding that what excites him is how fast the models are improving.\n\nAI-assisted coding has been gaining momentum across the tech world.\n\nGoogle CEO Sundar Pichai said last month that vibe coding is \"making coding so much more enjoyable,\" adding that people with no technical background can now build simple apps and websites.\n\n\"Things are getting more approachable, it's getting exciting again, and the amazing thing is, it's only going to get better,\" he said in a podcast interview with Logan Kilpatrick, who leads Google's AI Studio.\n\nPichai said during Alphabet's April earnings call that AI is writing over 30% of the new code at Google, an increase from 25% in October 2024.\n\nIt's \"fantastic\" how quickly developers can write software with AI coding tools, sometimes while \"barely looking at the code,\" said Google Brain founder Andrew Ng in May.\n\nFor non-technical developers, vibe coding has enabled them to automate parts of their jobs, prototype ideas, or build a creative product on the side, Business Insider reported last month.\n\nStill, leaders caution that the technology has limits. AI-generated code could contain mistakes, be overly verbose, or lack the proper structure.\n\n\"I'm not working on large codebases where you really have to get it right, the security has to be there,\" Pichai said in November.",
    "readingTime": 3,
    "keywords": [
      "anthropic's models",
      "non-technical developers",
      "vibe coding",
      "coding tools",
      "claude code",
      "it's",
      "critical",
      "software",
      "improve",
      "parts"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/claude-code-creator-vibe-coding-limits-boris-cherny-anthropic-2025-12",
    "thumbnail_url": "https://i.insider.com/6940d32f04eda4732f2d9311?width=1200&format=jpeg",
    "created_at": "2025-12-16T06:59:53.631Z",
    "topic": "finance"
  },
  {
    "slug": "the-year-coding-changed-forever",
    "title": "The year coding changed forever",
    "description": "Optimism, laziness, and magical thinking: The year vibe coding took over tech.",
    "fullText": "Sriraam Raja, the founding engineer at the software company Decode, has been using generative AI to write code for two years. He says he can get projects done about twice as fast when he uses a chatbot to code with intention. Then one day, he fired off directions, and as he sat there while the bot's wheels turned, he realized he could have actively written what he was aimlessly waiting for the bot to do. \"I was giving away a bit of my agency, and so I made a decision to be very conscious,\" he tells me.\n\nRaja has become \"very specific about when I delegate, and also how much I delegate,\" he says. Waiting for the AI to spit out code can disrupt the flow of his work, and trusting too much work to it has led him to sometimes get bogged down in a lengthy review process. He's also anxious about the long-term effects AI can have on how we all think and problem solve. \"There's a side effect where everyone's confidence has increased, but so has their laziness, and their willingness to learn things from first principles has dropped,\" he says. \"I've definitely seen a drop in curiosity that I haven't seen before, and so that worries me.\"\n\nThe Collins dictionary made vibe coding its 2025 word of the year. Coined by OpenAI cofounder Andrej Karpathy in February, the term refers to using language and generative AI to speed up the coding process. Soon after, companies were adding it as a desired skill in job listings.\n\nVibe coding was the catalyst for the sort of vibe work era we've entered. It's a shift in how people think about their roles and relationships to work amid an AI boom, and software engineering, long considered a stable and lucrative career path, has perhaps been the career most scrutinized and pushed down a path toward automation. Product managers have suggested that AI will supercharge them, allowing them to take on some technical coding tasks and work without engineers.\n\nExecs have been all-in: Mark Zuckerberg said he expected AI to write half of Meta's code within a year; this spring, AI was already doing about a third of code at Google and on some Microsoft projects. Anthropic CEO Dario Amodei predicted in March that 90% of code would be generated by AI in three to six months. The bullish estimate hasn't materialized for most, but Amodei said in October the company's AI tool Claude was writing most of the code at Anthropic. Cognition, which built an AI-powered software engineer it named Devin, is now valued at $10 billion. Some without computer science backgrounds or any training in coding are vibe coding their own projects.\n\nVibe coding isn't yet the miracle that AI evangelists have professed. AI-generated code can have sneaky errors that pose security risks. As it takes on the work of junior developers, companies eager for gain could displace humans. Time banked with shortcuts now could disrupt training ground for learning basic coding skills, creating a tech worker career ladder collapse could ricochet through the industry. There is potential for developers to save time, to use AI to learn new languages and skills (something Raja tells me he's done), and to pare down their technical debt, or code that needs maintenance. But the impact of AI on the industry is more complicated than it is a silver bullet to efficiency.\n\nLast year, \"we were dealing with a lot of optimism and a lot of magical thinking\" around the capabilities of AI, says Tariq Shaukat, CEO of Sonar, a company that provides developers with tools to verify code. \"The vibe engineering tools are producing a lot of quantity. It's getting more functionally correct, but it's actually becoming more difficult to determine the quality and get the level of trust that you need to integrate that into your code base.\" The ranks of AI holdouts among developers are shrinking. A 2025 survey of professional developers from Stack Overflow found that only 19.3% don't use AI, and a commensurate 19.7% have an unfavorable opinion of AI. Yet less than 3% of respondents said they highly trust AI for accuracy.\n\nAnyone who has asked a chatbot a question knows that even a short inquiry often results in a verbose response. The same is true of code — when AI generates it, it's typically longer, making the possibility of errors hiding in the code more likely. Amy Carrillo Cotten, senior director of customer transformation at software development company Uplevel, told me in September: \"For a lot of engineers, the only thing that looks different is where they spend their time, not exactly how much time it took.\" Uplevel studied 800 software developers last year and compared the productivity levels of those who used GitHub's Copilot to those who did not. The developers who used Copilot weren't more efficient or less burnt out, and their code had bugs in it 41% more frequently. (GitHub's own research found that those who used Copilot wrote about 18 lines of clean code, compared to 16 lines for those who didn't.) For many, that shift from writing to reviewing code is \"not the job they signed up for,\" Shaukat says, which brings a big adjustment for many developers.\n\n\"The job looks completely different,\" says Frank Fusco, CEO of a software company called Silicon Society. His company works with clients on their software, but now they often get amateur, vibe coded versions of those ideas as the starting point. \"What I would normally do in code that would take me days, I now do in words and it takes me hours.\" But Fusco tells me he worries about a decline in critical thinking and basic coding skills. We're \"hardwired,\" he says, to find \"the shortest path to the solution.\" But that approach isn't the best for sharpening coding skills. \"It really is a muscle that you have to work all the time.\"\n\nIt's tricky to say AI is already killing developer jobs. Years of layoffs and \"right-sizing\" in the tech industry, paired with the economic precarity that has also defined 2025, could be shifting industry roles alongside AI. As of November, there were about 92,500 active job postings seeking software engineers, down from nearly 102,000 last November and 159,000 at the start of 2023, according to data from CompTIA, a nonprofit trade association for the US IT industry. The number of active tech job posts overall has fallen, from 621,000 in early 2023 to 433,500 last month. But the proportion of open jobs looking for AI skills has jumped by 53% this year.\n\nAfter two decades of being told to pursue computer science as a stable career and a proliferation of coding bootcamps, working as a developer may not be as cushy. College seniors studying computer science are more likely than any other discipline to say they're \"very pessimistic\" about their careers, according to a 2025 survey from early career website Handshake. They're the group most likely to say the advances of generative AI have made them regret their major choice. But young people are divided — 43% of computer science majors said they think AI will have a positive effect on their careers.\n\nAutomation is in some ways marking \"a correction\" on the developer labor market, says April Schuppel, developer relations manager at software company Apryse. Before AI, \"we needed as many people who were really pushing out the code to take the ideas of the visionaries and bring them to life.\" Now, \"the people who have always been able to make the most impact, they're still the ones that are the safest.\" Developers who looked at their jobs as clearing tickets might be more replaceable than those who were creative and cared about the project from start to finish. We're far from realizing the end game of vibe coding, but for creative, forward-thinking developers, there's optimism for now. \"The more well-rounded people are the ones that are going to have success,\" Schuppel says.\n\nAI could bring more opportunity for software testers, and also help companies pare down their technical debt. The developer job market might look messy right now, but there's still a heavy focus on the human aspect of the career than in the picture painted by some Big Tech execs. \"If there are opportunities for more fine-tuned models, more specialized models that only do certain types of code updates, and there is a way to use that more to augment human developers as opposed to replace, that seems like that's where this is going,\" says Tim Herbert, chief research officer at CompTIA.\n\nCodebases are valuable, and the security risks posed by goofs in AI code are serious threats. Traffic to vibe coding sites slumped in September after a summer of hype. Even Karpathy said his latest project is \"basically entirely hand-written (with tab autocomplete)\" in a post on X. \"I tried to use claude/codex agents a few times but they just didn't work well enough at all and net unhelpful.\" If 2025 was the year tech companies went all in on AI, 2026 might be the year when some of the craze around vibe coding subsides and reality sets in.\n\nAmanda Hoover is a senior correspondent at Business Insider covering the tech industry. She writes about the biggest tech companies and trends.",
    "readingTime": 8,
    "keywords": [
      "security risks",
      "technical debt",
      "computer science",
      "basic coding",
      "vibe coding",
      "tech industry",
      "coding skills",
      "developers",
      "software",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/year-coding-changed-forever-silicon-valley-2025-12",
    "thumbnail_url": "https://i.insider.com/693c966a64858d02d216c23d?width=1200&format=jpeg",
    "created_at": "2025-12-15T13:53:47.187Z",
    "topic": "finance"
  },
  {
    "slug": "habits-that-make-a-great-programmer",
    "title": "Habits That Make a Great Programmer",
    "description": "8 rituals to level up your programming skills! Learn how to code better, think faster, and build complex systems with these actionable tips.",
    "fullText": "“How to get better at programming?” is the question I had been asked quite a few times, and today I lay down the 8 rituals I have been following, and action items for each, to be good and get better at programming.\n\nDoing something repeatedly always helps and writing a lot of code will develop our ability to\n\nIf we don’t do something repeatedly, it becomes extremely hard to get good at it. Writing code consistently helps us\n\nSolving programming questions is about developing logic but things become a little trickier when we build a complex system, as it requires us to take our programming skills to go up a notch. Some examples of complex systems are - a Library management system, a Twitter clone, an Instagram clone, etc. Building a complex system\n\nAfter we spend some time writing programs and solving problems, things become monotonous and do not seem to challenge us anymore, so to spice things up a bit we should model something from the real world, like\n\nThere are lots of libraries and framework like p5.js that makes visual programming simple.\n\nIt is not only writing code that improves our programming skills but it is reading some quality code written by expert programmers that make the difference. Reading code written by experts improve our programming vocabulary and by doing this we\n\nThe best way to start doing it is by picking up an open-source project and start skimming the code. It is okay to not understand it in the first go but it is important to skim it a few times and get acquainted. After a few skim, everything will fall in place, the code becomes familiar and we start to understand the flow and business logic.\n\nThere is always someone sitting on the other side of the globe, who knows a thing or two more than us. Look for them and collaborate on a project. The developer community is filled with super smart and super enthusiastic developers who love to share and collaborate. Use websites like Dev.to, Hashnode and Twitter to find and interact with like-minded people.\n\nA programming language is just a tool to express business logic. While learning a programming language we should try to understand the constructs and paradigms used - for example: Functional programming, Polymorphism, Event driven programming, Actor model, etc. It is important to do so because we could pick constructs from one language and use it in another to solve our problem. For example: picking Functional programming (Callbacks) from Javascript and using it in Python to create generic action functions.\n\nWriting code before putting in some thought is degraded the code more often than not. The code written like this lacks simplicity, reusability, and extensibility. Spending some time thinking about problem statement or task at hand and having a rough execution plan always helps.\n\nThese rituals have helped me get better at programming with time and in parallel, I pick at max 3 and act on the action items. Programming is simple but being better than most is difficult. Doing it consistently makes one get better by the day.",
    "readingTime": 3,
    "keywords": [
      "functional programming",
      "action items",
      "business logic",
      "complex system",
      "programming skills",
      "programming language",
      "code",
      "doing",
      "understand",
      "rituals"
    ],
    "qualityScore": 1,
    "link": "https://arpitbhayani.me/blogs/better-programmer/",
    "thumbnail_url": "https://edge.arpitbhayani.me/img/covers/general-cover.jpg",
    "created_at": "2025-12-15T03:59:06.895Z",
    "topic": "tech"
  },
  {
    "slug": "openais-head-of-codex-says-the-bottleneck-to-agi-is-humanitys-inability-to-type-fast-enough",
    "title": "OpenAI's head of Codex says the bottleneck to AGI is humanity's inability to type fast enough",
    "description": "OpenAI's Alexander Embiricos, who leads product development for its coding platform, said the need to review AI's work with prompts is limiting progress.",
    "fullText": "If you needed a sign for how determined AI-land is to achieve AGI quickly, it's that one of its leaders sees the speed of human typing as one of its biggest roadblocks.\n\nAlexander Embiricos, who leads product development for Codex, OpenAI's coding agent, said on \"Lenny's Podcast\" on Sunday that the \"current underappreciated limiting factor\" to AGI is \"human typing speed\" or \"human multi-tasking speed on writing prompts.\"\n\nAGI, or artificial general intelligence, is a still theoretical version of AI that reasons as well or better than humans. It's the thing all the big AI companies are competing to be the first to realize.\n\n\"You can have an agent watch all the work you're doing, but if you don't have the agent also validating its work, then you're still bottlenecked on, like, can you go review all that code?\" Embiricos said.\n\nEmbiricos' view is that we need to unburden humans from having to write prompts and validate AI's work, since we aren't fast enough.\n\n\"If we can rebuild systems to let the agent be default useful, we'll start unlocking hockey sticks,\" he said.\n\n\"Hockey stick growth\" is a term used to describe a growth curve that starts out flat and suddenly spikes, mirroring the shape of a hockey stick.\n\nEmbiricos said there's no simple path to a fully automated workflow — each use case will require its own approach — but he expects to see progress toward this level of growth soon.\n\n\"Starting next year, we're going to see early adopters starting to hockey stick their productivity, and then over the years that follow, we're going to see larger and larger companies hockey stick that productivity,\" he said.\n\nSomewhere in between the time early adopters start to see gains in productivity and when tech giants manage to fully automate processes with AI agents is when we'll see AGI, Embiricos said.\n\n\"That hockey-sticking will be flowing back into the AI labs, and that's when we'll basically be at the AGI,\" he said.",
    "readingTime": 2,
    "keywords": [
      "human typing",
      "hockey stick",
      "agent",
      "speed",
      "we'll",
      "growth",
      "productivity",
      "it's",
      "prompts",
      "humans"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/openai-artificial-general-intelligence-bottleneck-human-typing-speed-2025-12",
    "thumbnail_url": "https://i.insider.com/693f0599832e0ef1ead631ab?width=1200&format=jpeg",
    "created_at": "2025-12-15T03:59:01.869Z",
    "topic": "finance"
  },
  {
    "slug": "programming-languages-to-learn-first",
    "title": "Programming Languages to Learn First",
    "description": "Many IT professionals would recommend Python as the best programming language for beginners. Why? The syntax of the Python code is considered simple.",
    "fullText": "TL;DR: Python leads with 29.85% market share driven by AI/ML demand, while JavaScript remains essential for web development. But here’s what most hiring managers miss: the rise of AI-powered coding tools like GitHub Copilot has fundamentally altered what skills you should prioritize when building your engineering team.\n\nThe question isn’t simply “which programming language should we hire for?” anymore. Companies are increasingly leveraging AI to automate routine coding tasks, reducing the need for large engineering teams and prioritizing professionals who can manage AI-driven workflows rather than simply write code.\n\nLet’s cut through the noise and examine what’s actually happening in today’s tech hiring landscape.\n\nSoftware developer job listings are down 35% from their 2022 peak, but don’t let that fool you into thinking demand has disappeared. Most technology positions tracked by the U.S. Bureau of Labor Statistics show unemployment rates well below the national average—software developers at 2.8%, systems analysts at 1.8%, and security analysts at 2.3%.\n\nWhat’s changed is selectivity. Companies are becoming more selective with their technical hires while businesses struggle to ship quickly due to a shortage of qualified engineers. The old approach of scanning resumes for keyword matches no longer works when every engineer claims “full-stack” experience.\n\nPython dominates with 29.85% market share, and it’s no surprise given AI and machine learning’s explosive growth. But here’s what matters for your hiring strategy: the type of Python developer you need has evolved.\n\nTraditional Python roles focused on web development with Django or Flask. Modern Python roles require understanding of:\n\nWhen interviewing Python candidates, dig beyond syntax knowledge. Ask: “How would you architect a system that processes real-time data for ML model inference?” The answer reveals whether they understand modern Python’s role in AI-driven products.\n\nJavaScript maintains its position with 7.92% market share, remaining essential for front-end development. But traditional frontend development is seeing fewer job postings, suggesting a shift toward full-stack or specialized backend roles.\n\nThe modern JavaScript landscape demands expertise in:\n\nTypeScript continues gaining popularity for large-scale web development, with its static typing and enhanced tooling making it preferred for complex applications. If your product serves enterprise clients or handles complex state management, TypeScript experience isn’t optional, it’s essential.\n\nGo’s popularity continues to grow as global demand for cloud computing rises, with its simple syntax, built-in concurrency support, and high performance making it well-suited for cloud-native applications.\n\nRust is emerging for system-level programming where memory safety and performance are critical. Rust’s memory safety, high performance, and robust security properties make it particularly well-suited for performance- and safety-critical applications.\n\nIf your architecture includes real-time systems, embedded software, or blockchain applications, Rust expertise provides competitive advantages that Go simply cannot match.\n\nIn this article, you can find answers to these questions. Keep in mind that these are all useful languages that will bring you closer to your goal if you’re committed.\n\nDespite competition from newer languages like Kotlin and Go, Java remains widely used in enterprise software, Android development, and backend systems. But the Java developer you need in 2025 looks different from five years ago.\n\nModern Java development requires:\n\nC# has been increasingly utilized in game development and enterprise software, with deep integration with the Unity game engine cementing C# as a top game developer language. For enterprise applications, .NET’s cross-platform capabilities make C# developers valuable for modernizing legacy Windows-based systems.\n\nAccording to research for the Demand for Skilled Talent report, the most evident skills gap on technology teams is within AI, machine learning and data science. But let’s be specific about what this means for different roles:\n\nPlatform-focused AI engineers build centralized tools and infrastructure to accelerate AI development, while product-focused AI engineers work inside product teams and ship AI features for users. Understanding this distinction helps you write better job descriptions and evaluate candidates correctly.\n\nDevelopers who can maintain and modernize legacy systems are highly valued, with work including ensuring security, improving performance, and integrating legacy systems with newer technologies like APIs or microservices. Many companies underestimate this need when planning their hiring strategy.\n\nCloud services like AWS, Google Cloud, and Microsoft Azure are at the core of modern software infrastructure, with over 90% of global enterprises expected to use cloud platforms by 2025.\n\nCritical cloud competencies include:\n\nWhen evaluating DevOps candidates, focus on their experience with incident response and disaster recovery. Anyone can deploy to the cloud; few can architect systems that gracefully handle failure at scale.\n\nEnterprise blockchain adoption is driving legitimate technical roles:\n\nMajor industries entering the space, including finance, healthcare and logistics, are expanding demand for blockchain engineers. When Deutsche Bank builds blockchain settlement systems or Nike creates digital collectibles, they need engineers who understand both traditional software architecture and decentralized protocols.\n\nEntry-level Solidity developers can write basic smart contracts and deploy them to testnets. Senior Solidity engineers architect systems that handle millions in value while remaining secure and gas-efficient.\n\nCore technical competencies for serious Solidity roles:\n\nFor Startups and Scale-ups: Python and JavaScript remain your best bets for rapid development and talent availability. The ecosystem maturity and hiring pool depth outweigh cutting-edge performance considerations.\n\nFor Enterprise and Financial Services: Java and C# provide the stability, security, and regulatory compliance frameworks that regulated industries require. Don’t chase trends when handling mission-critical systems.\n\nFor Performance-Critical Applications: Go for backend services, Rust for system programming, and C++ for real-time applications. Latency requirements should drive language selection, not popularity metrics.\n\nFor AI/ML Products: Python dominates, but consider Julia for scientific computing or R for statistical analysis. Language choice depends on your specific AI use case and team expertise.\n\nGiven 95% of tech leaders face challenges finding skilled workers, your approach to technical hiring needs to evolve beyond traditional methods.\n\nFocus on fundamental problem-solving over specific syntax knowledge. A strong engineer can learn new languages; analytical thinking and system design skills transfer across technologies.\n\nPrioritize hands-on experience with real-world projects over certification collections. Ask candidates to walk through architecture decisions they’ve made and trade-offs they’ve considered.\n\nEvaluate AI collaboration skills. The shift toward engineers with expertise in AI augmentation, system architecture, and cross-functional problem-solving means traditional coding assessments miss crucial competencies.\n\nIt’s easy enough for software engineers to become AI engineers: just build applications on top of LLMs. This accessibility is reshaping what skills remain uniquely human and valuable.\n\nLanguages that enhance AI productivity:\n\nThe programming languages your team learns should align with how AI tools augment rather than replace human developers. Focus on languages that excel in areas where human judgment and creativity remain irreplaceable: system architecture, user experience design, and complex business logic implementation.\n\nBottom Line: The most important programming language for your 2025 hiring strategy isn’t determined by popularity rankings—it’s the one that best matches your technical architecture, team experience, and business requirements. Technology hiring trends in 2025 indicate that candidates place high value on exposure to AI and machine learning projects, as these skills significantly enhance their career trajectories.\n\nThe companies succeeding in today’s competitive hiring market understand that language proficiency is just the foundation. The real competitive advantage comes from engineers who can architect systems, collaborate with AI tools, and adapt to evolving technical requirements.\n\nReady to build a hiring strategy that actually reflects today’s market realities? Let’s discuss how the current tech landscape impacts your specific technical requirements and talent acquisition approach.",
    "readingTime": 7,
    "keywords": [
      "python dominates",
      "applications rust",
      "shift toward",
      "memory safety",
      "syntax knowledge",
      "machine learning",
      "python roles",
      "web development",
      "hiring strategy",
      "technical requirements"
    ],
    "qualityScore": 1,
    "link": "https://www.omnesgroup.com/the-best-programming-languages-to-learn-first/",
    "thumbnail_url": "https://www.omnesgroup.com/wp-content/uploads/2018/08/download-69-1.png",
    "created_at": "2025-12-14T18:50:16.561Z",
    "topic": "tech"
  },
  {
    "slug": "terraform-sunsets-cdktf",
    "title": "Terraform Sunsets CDKTF",
    "description": "This decision forces Terraform's users to migrate to HCL, drawing criticism from those who point to the CDK's popularity as proof Terraform still needs advanced programming capabilities.",
    "fullText": "Going forward, when you run IBM‘s Terraform Infrastructure as Code (IaC) software, you will have one language to write your configurations: the HashiCorp Configuration Language (HCL).\n\nOn Monday, HashiCorp, an IBM company, announced that it will no longer support the Terraform Cloud Development Kit (CDK or CDKTF). Although the existing code will remain available in a GitHub archive, HashiCorp will no longer maintain or update the code, leaving it all but unusable for enterprises.\n\n“Unfortunately, Terraform CDK did not find product-market fit at scale. HashiCorp, an IBM Company, has chosen to focus its investments on Terraform core and its broader ecosystem,” a note on the site read.\n\nThe CDK itself is licensed under the Mozilla Public License (MPL), so users are free to fork the software itself, IBM suggested.\n\nThe company, however, is encouraging users to use HCL, which was developed by HashiCorp and licensed under the Mozilla Public License (MPL), originally designed for the software.\n\nOriginally released in 2014 by HashiCorp, Terraform is software that allows administrators to automate the deployment of IT infrastructure, either in the cloud or on premises, through the use of scripts and a set of Terraform commands such as terraform init, terraform plan and terraform apply. The output is rendered as JSON.\n\nOver time, Terraform has become the most popular software for automated IT deployment, especially in the cloud native community.\n\nIn 2023, HashiCorp switched the Terraform license from open source to a Business Source License, which spurred a user-based open source fork of the software, called OpenTofu, that was adopted by the Linux Foundation and, later, by the Cloud Native Computing Foundation (CNCF).\n\nIn 2024, IBM announced it was acquiring HashiCorp and finalized the purchase earlier this year.\n\nDespite a call to open source the CDK, IBM is encouraging current users to adopt the HCL if they are not already doing so.\n\n“If you are not using AWS CDK, we highly recommend migrating to standard Terraform and HCL for long-term support and ecosystem alignment,” the company asserted.\n\nTerraform users with .tf files created under the CDK can convert them to HCL with the following command:\n\nThose using CDTF on Amazon Web Services infrastructure can also use AWS’ own CDK.\n\nOverall, the Infrastructure as Code user base appears to be chafing from the limits of IaC.\n\nAs a result, many alternative approaches to Terraform have popped up in the last few years, including Adam Jacob’s System Initiative and Formae from Platform Engineering Labs.\n\nThey point to how HCL has its limits, especially for highly scalable environments. A declarative configuration language, HCL is limited in offering advanced programming constructs, and many resulting workarounds have resulted in obtuse code. Tooling is limited as well.\n\nThe advantage that the CDKTF brought to users was that it allowed them to detail deployment instructions through their own favorite programming language rather than HCL. CDKTF supported TypeScript, Python, C# and the Go programming language.\n\nThis is also the approach that Terraform competitor Pulumi has staked out, namely the ability to provision infrastructure in any one of a number of programming languages.\n\nYet, there has also been considerable debate around whether a general-purpose programming language is better than a domain-specific language. Terraform’s users are administrators, not programmers, as critics have pointed out.\n\nNonetheless, many of those in the IaC community took the news hard. Kubernetes expert David Flanagan noted that the development kit has gotten over 140,000 downloads per week for TypeScript alone, with similar numbers in other language communities.\n\nSo clearly, the CDKTF is still highly used by the community, he argued.\n\nFuck you, Hashicorp … an IBM Company. pic.twitter.com/h1EicnT3pL\n\n— David Flanagan (@rawkode), Dec. 11, 2025\n\n“You don’t kill a project with [an estimated] million users every single month because nobody likes it or it doesn’t have a ‘market fit.’ You kill it because it is not increasing your profit margin, it is not selling enterprise licenses,” Flanagan said in a short video.\n\nTo be fair, IBM has a long history of buying open source-based companies, and keeping the open source licensing intact, including the Linux-based Red Hat, the Cassandra-focused Datastax and, most recently, the Kafka-based Confluent. (There’s been no word, however, on whether IBM would revert the Terraform license back to open source.)\n\nFlanagan went on to note that people are probably using the CDKTF because they require the additional programming capabilities. “It’s called Infrastructure as Code, not Infrastructure as JSON,” he quipped.\n\nSite reliability engineer Liz Fong-Jones offered a more measured response.\n\n“To be more gentle about this, HashiCorp has decided to stop trying to compete with Pulumi with language-native APIs; they’re all in on HCL as the only way to work with Terraform,” Fong-Jones wrote on BlueSky.\n\nIn fact, others think this may not be a bad idea.\n\nPlatform Engineering Labs’ Co-Founder and CEO Pavlo Baron thought the IBM move made sense.\n\n“IBM is historically good at optimizing for the target buyer. This is rather a sign that nobody on the right side of the cycle wants to do full-blown programming. CDKs, and this includes the approach Pulumi takes, are exclusively for developers. Developers usually don’t operate infrastructure,” he wrote by email.\n\n“Serious operations happen on the right side of the cycle, though. Thus, the CDK is missing their target user and addresses the wrong one. So I understand and support the logic behind this move.”",
    "readingTime": 5,
    "keywords": [
      "platform engineering",
      "engineering labs",
      "license mpl",
      "development kit",
      "cloud native",
      "language hcl",
      "configuration language",
      "programming language",
      "ibm company",
      "terraform license"
    ],
    "qualityScore": 1,
    "link": "https://thenewstack.io/ibm-hashicorp-sunsets-terraforms-external-language-support/",
    "thumbnail_url": "https://cdn.thenewstack.io/media/2025/12/65e7b1bd-ritu-dahiya-w2mlnx4yso-unsplash.jpg",
    "created_at": "2025-12-14T18:50:16.550Z",
    "topic": "tech"
  }
]
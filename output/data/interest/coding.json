[
  {
    "slug": "sync-your-coding-agent-activity-across-sessions-and-users",
    "title": "Sync your coding agent activity across sessions and users",
    "description": "Contribute to mubit-ai/codaph development by creating an account on GitHub.",
    "fullText": "mubit-ai\n\n /\n\n codaph\n\n Public\n\n codaph.com\n\n License\n\n View license\n\n 4\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mubit-ai/codaph",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/mubit-ai/codaph",
    "thumbnail_url": "https://opengraph.githubassets.com/1cb075c57cf9e3d0722590987bf65d9b88084bc49ac4fe0467eb7981c901a878/mubit-ai/codaph",
    "created_at": "2026-02-28T12:24:28.882Z",
    "topic": "tech"
  },
  {
    "slug": "the-bespoke-flywheel",
    "title": "The Bespoke Flywheel",
    "description": "The era of The Bespoke Flywheel is here: agentic coding has pushed the cost of disposable, one-off software to zero. Stop settling for generic tools and start automating away niche friction to keep your focus entirely on high-signal thinking.",
    "fullText": "Every researcher or engineer has encountered this: you need to do something ad-hoc and unsupported by existing tooling. Perhaps it's to create a custom slice of data analysis, to build a dashboard that monitors a specific failure pattern, or simply to make a nicer-looking UI that doesn't hurt your eyes so much when eyeballing eval samples. You look at the clock and the massive to-do list, think about the efforts needed, and decide that you cannot justify the investment. So you settle for a generic tool that gives you a blurry version of the truth.\n\nBut this is changing as we enter the era of The Bespoke Flywheel. Before we know it, the cost of building niche, disposable software will effectively hit zero.\n\nWe used to only build tools for the 80% case -- problems that happen frequent enough to justify the engineering overhead. This left the long tail of friction untouched. The frontier, however, by definition lives in that long tail. It is made of weird, non-standardized, one-off problems that off-the-shelf tools can't solve.\n\nWith agentic coding tools, the math has flipped. When you can describe a friction point and have a working solution in 2 minutes, the Tooling Tax vanishes. You stop working around the friction and start automating it out of existence.\n\nThis allows you to build Disposable Software: tools built for a single niche purpose, used for three hours (or even only 10 minutes) to find a specific signal, and then discarded.\n\nThe true power of this shift is best understood through a lens recently articulated by Hao Sheng:\n\n\"The bottleneck is how fast you can convert an idea into a high-signal datapoint — and then do it again.\"\n\nIn this framework, progress isn't limited by compute or idea supply. It's limited by the velocity of the feedback loop.\n\nWhen you combine agentic coding with this philosophy, you get The Bespoke Flywheel:\n\nFor the first time, your progress is no longer limited by your ability to quickly churn out throw-away code. It is limited only by your cognitive bandwidth: asking the right questions and interpreting the high-signal data coming back at you.\n\nWe used to spend 80% of our time building the machine and 20% of our time thinking. The Bespoke Flywheel flips that ratio and keeps you in a flow state longer.\n\nThis shift goes way beyond AI labs and tech companies; it's the new baseline for the personal frontier. We are moving from a world where we adapt our lives to generic software, to one where we architect the digital environment to fit us.\n\nThe Bespoke Flywheel becomes a way of living. Whether you are optimizing a personal health journey, mastering a niche hobby, or managing a complex househould, you no longer need to wait for an app developer to care about your specific friction. You build the bridge yourself.\n\nThe frontier is now personal. And it's yours to build.\n\nGenerative AI Usage Disclosure: Refined by Gemini 3 Thinking; banner generated by Nano Banana 2.",
    "readingTime": 3,
    "keywords": [
      "bespoke flywheel",
      "agentic coding",
      "the bespoke flywheel",
      "it's",
      "tools",
      "friction",
      "limited",
      "niche",
      "frontier",
      "longer"
    ],
    "qualityScore": 1,
    "link": "https://linghao.io/posts/bespoke-flywheel",
    "thumbnail_url": "https://r2.linghao.io/blog-assets/bespoke-flywheel.png",
    "created_at": "2026-02-28T06:24:25.130Z",
    "topic": "tech"
  },
  {
    "slug": "leveraging-ai-for-selflearning",
    "title": "Leveraging AI for Self-Learning",
    "description": "Thoughts, guides, and learnings on programming and systems",
    "fullText": "While everyone has been going crazy over vibe-coding, I’ve been largely experimenting with AI for learning.\n\nThe question I generally have is whether or not we can use these tools to learn\nnew technical concepts, domains, or programming languages effectively.\n\nThe hardest part when programming, in my opinon, usually has very little to do\nwith the syntax and much more to do with the domain.\n\nHow does authentication\nwork? What is a database “transaction”? What is a language server? What happens\nin the linking process?\n\nOr, if you’re learning a new programming paradigm (e.g. Rust, FP, Lisp), you likely have some\nquestions about the idiomatic way to do things.\n\nI’ve been playing around with these tools for a good while attempting to do some\nof these things, with the focus on using them to actually improve my knowledge\nrather than mercilessly vibecoding.\n\nSo, can these tools help us learn effectively? To immediately ruin the clickbait, my honest answer is I’m still trying to work that out myself.\n\nBut I think sharing my thoughts here might be useful to some others.\n\nFirst I want to talk about my general approach to learning, applying it to AI,\nand then where I think things fall short - whether through the tools, or my own\nlearning strategy.\n\nAs a self-learner, I believe the strongest path you can choose to take is inquiry-based learning. This is especially true for areas like programming, computing-related concepts, etc.\n\nThe general idea is to learn by creating projects and asking lots of questions about what you’re doing. What happens if I change X variable to Y? Why does it give me an error? What does the error mean?\n\nAnd using this to form your own beliefs and understandings about a concept, and testing them (either by learning more, consulting experienced people on forums, etc.).\n\nThis is kind of similar to the Socratic method and other techniques to help improve your critical thinking.\n\nThe obvious next step here is trying to apply this concept to our LLM usage.\n\nSocrates and many other teachers over the course of modern and ancient history seem to agree on the fact that you need some amount of discomfort in order to learn effectively.\n\nTo be honest it really is as simple as the common phrase “learning from your mistakes”.\n\nIf you break a client’s website or run into a bug which drives you insane for 3 days, you’re probably never going to make that mistake again because, well, you don’t want to experience that mistake again.\n\nAlong these lines, if you’re tackling something new or complex (or both) it’s of course natural to feel a level of discomfort as it’s unknown, and you will likely make mistakes (and learn from them).\n\nSo with all of this said, this is where I want to talk about AI.\n\nI have taken this inquiry-based approach when learning new topics and using LLMs.\n\nThere is no “master” strategy here. I always feel I work best by trusting my instincts when it comes to what I should learn next or upskill with.\n\nBut here’s a few different approaches I took:\n\nThe least effective way to learn with LLMs, in my opinion, is to use only LLMs. I’m sure you could ask the chatbot to act like a teacher and teach you a lesson on topic X, but I think it’s not making the most of the chat format. We need diagrams, videos, and other methods of written explanation that do not fit the “voice” given to the current AI models.\n\nHowever, using them alongside other resources, or as a fellow code reviewer, assistant, or something along these lines, seems to have given me some remarkable results - at least output-wise.\n\nI’ve recently been coding Neovim plugins by reading the documentation the best I can, and using AI to help me if I get stuck on how a certain API works or what I’m doing wrong.\n\nI think the issue is though it makes me think two things:\n\nComing back to the point I made earlier, we need discomfort to learn. The “goldilocks” zone of not too much, not too little, but just right.\n\nRight now, I feel at least that AI makes things just a little bit too easy for me. So I have to ask myself: am I really still learning effectively? Am I destroying my critical thinking and offloading too much to these tools?\n\nI want to think that I’m not, considering that I have the mental capability to reflect on this experience and try and decide for myself. But it does make me think.\n\nThese tools are all about how we use them. You can generate slop short-form media, or you can try and use them to learn new concepts. It’s all about you. But having the discipline to use them in a “good” way is definitely hard.\n\nI do think we’re moving closer to the goldilocks zone, and I can really see these tools augmenting our learning process for the better.\n\nAt the same time, I don’t want to see traditional resources fade away, and I do think there’s room for both. Or, at least I hope there is.\n\nIf you have any thoughts on this I’d be glad to hear them.",
    "readingTime": 5,
    "keywords": [
      "mistake again",
      "goldilocks zone",
      "learn effectively",
      "learning",
      "tools",
      "programming",
      "you’re",
      "it’s",
      "i’ve",
      "concepts"
    ],
    "qualityScore": 1,
    "link": "https://techne98.com/blog/can-we-use-ai-for-self-learning/",
    "thumbnail_url": "https://techne98.com/og.png",
    "created_at": "2026-02-28T06:24:24.972Z",
    "topic": "tech"
  },
  {
    "slug": "the-american-universities-programming-israels-killer-drones",
    "title": "The American Universities Programming Israel’s Killer Drones",
    "description": "Julian...",
    "fullText": "The Vietnam-era practice is yet another example of ICE agents thrilling to the brutality they have been encouraged to cultivate.\n\nWhether or not you have access to independent, scientifically sound public health guidance may depend on how your state voted for governor.\n\nAfter putting on their very best performances of solidarity every Black History Month, this year corporate marketers have seemed at a loss for words.\n\nWe are seeing yet another example of state-sanctioned violence against the reproductive futures of those deemed outside the national body.\n\nThe Brazilian star Vinicius Jr. has repeatedly been a victim of racist abuse from soccer fans. Now, it seems such vitriol can even come from players without much consequence.\n\nStudentNation\n\n /\n\n Takashi Williams\n\nWithin hours of losing the case, President Trump declared a new global tariff under a different statute.\n\nMichele Goodwin and Gregory Shaffer",
    "readingTime": 1,
    "keywords": [
      "another"
    ],
    "qualityScore": 0.65,
    "link": "https://www.thenation.com/article/society/universities-israel-drones-weapons-manufacturing-military-contractors/",
    "thumbnail_url": "https://www.thenation.com/wp-content/uploads/2026/02/Israel-Drones-University-of-Central-Florida.jpg",
    "created_at": "2026-02-28T01:02:24.313Z",
    "topic": "politic"
  },
  {
    "slug": "the-guy-who-coined-vibe-coding-says-ai-is-making-programming-unrecognizable",
    "title": "The guy who coined vibe coding says AI is making programming 'unrecognizable'",
    "description": "OpenAI and Tesla alum Andrej Karpathy wrote that there was no more \"business as usual\" in software, thanks to AI.",
    "fullText": "The AI coding timeline is accelerating, according to the guy who coined \"vibe coding.\"\n\nAndrej Karpathy has a long list of AI bona fides. He was a founding member of OpenAI, left to lead AI at Tesla, then returned to the ChatGPT-maker and eventually started his own lab. In early 2025, he introduced the term vibe coding into the zeitgeist.\n\nIn his latest X post, Karpathy described a speed-up in agentic coding, one that has taken place in just a few months. \"Coding agents basically didn't work before December and basically work since,\" he wrote.\n\nThese agents are \"extremely disruptive to the default programming workflow,\" Karpathy wrote.\n\nHe gave an example. Over the weekend, Karpathy said he used AI to make a video analysis dashboard for his home cameras. His AI agent completed it in 30 minutes, encountering errors and researching solutions. It was all hands-free, he wrote.\n\n\"Programming is becoming unrecognizable,\" Karpathy wrote. \"This is nowhere near 'business as usual' time in software.\"\n\nPosts like these are now Karpathy's specialty: long, multi-paragraph takes on the state of software engineering. In one recent post, the creator of Claude Code commented that Karpathy always had a \"very thoughtful and well reasoned take.\"\n\nIn the comments on this post, an AI developer asked about the state of software engineers. \"Are teams of hundreds of people going to be replaced by a few chosen prompters?\" they asked.\n\n\"Vibe coders are now able to get somewhere, but at the top tiers, deep technical expertise may be even more of a multiplier than before because of the added leverage,\" Karpathy responded.\n\nSome big names in AI aren't so thrilled with Karpathy's linguistic invention. OpenClaw creator Peter Steinberger called vibe coding \"a slur,\" arguing that it implied coding with AI was easy and didn't still involve real work.\n\nIn another comment, Karpathy referenced an old Tesla slogan: \"Every action is error.\" The goal is to arrange your agents so you can \"remove yourself as the bottleneck,\" he wrote.\n\nFor many, coding with AI can feel fantastical. Karpathy offered a quick reality check: the agents require \"high-level direction\" and (the frequently memed) \"taste.\"\n\n\"It's not magic, it's delegation,\" he wrote.",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "agents",
      "software",
      "karpathy",
      "tesla",
      "didn't",
      "programming",
      "karpathy's",
      "creator",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-programming-unrecognizable-ai-2026-2",
    "thumbnail_url": "https://i.insider.com/69a056701fb3fcb426487d98?width=1200&format=jpeg",
    "created_at": "2026-02-27T12:34:24.313Z",
    "topic": "finance"
  },
  {
    "slug": "programming-has-changed-dramatically-due-to-ai-in-the-last-2-months-karpathy",
    "title": "Programming has changed dramatically due to AI in the last 2 months (Karpathy)",
    "description": "It is hard to communicate how much programming has changed due to AI in the last 2 months: not gradually and over time in the \"progress as usual\" way, but specifically this last December. There are a number of asterisks but imo coding agents basically didn’t work before December",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/karpathy/status/2026731645169185220",
    "thumbnail_url": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_200x200.jpg",
    "created_at": "2026-02-26T06:46:25.897Z",
    "topic": "tech"
  },
  {
    "slug": "discovering-multiagent-learning-algorithms-with-large-language-models",
    "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
    "description": "Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2602.16928",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-02-26T01:08:23.616Z",
    "topic": "tech"
  },
  {
    "slug": "typedb-studios-ai-agent-for-schema-exploration-and-query-generation",
    "title": "TypeDB Studio's AI agent for schema exploration and query generation",
    "description": "You’re probably familiar with vibe coding — the practice of describing what you want in plain English and letting an AI write the code. In this article we show ...",
    "fullText": "You’re probably familiar with vibe coding — the practice of describing what you want in plain English and letting an AI write the code. In this article we show how the concept translates to databases using the Agent Mode feature of TypeDB Studio.\n\nVibe querying is exactly what it sounds like. Instead of writing this:\n\nGet all people with their names and emails.\n\nThe AI reads your database schema, understands the structure of your data, and generates TypeQL in the chat. You click Run, and the results appear inline in your chat window.\n\nWhy use Agent Mode in TypeDB Studio? Agent mode can be used for asking questions about your database, building your schema and querying your data – all the essential day-to-day DB operations. It’s particularly useful for:\n\nOverall it’s just a more hassle-free way to build with TypeDB.\n\nThe easiest way to try this is to boot up a free TypeDB Cloud cluster with the social-network sample dataset preloaded; alternatively you can also initialise it in TypeDB CE (Cloud page / CE setup guide)\n\nSuppose this is your first time browsing the social network dataset. You probably want to get the lay of the land. Ask the AI agent “what’s in my schema?” and it will come up with a detailed description you can read.\n\nBehind the scenes, TypeDB Studio has attached the schema as context to the LLM that powers agent mode, which has attempted to decipher the schema for the user (you).\n\nI am a visual learner – so had to give this one a try.\n\nNot exactly Picasso – but successful nonetheless; agent mode can indeed render your schema as ASCII art.\n\nBefore agent mode: poke around in schema, scratch head, read docs, do actual hard work.\n\nAfter agent mode: “Hey benevolent AI bot, could you just dream up some interesting queries for me?”\n\nNot bad – its top three suggestions are simple enough:\n\nLet’s go ahead and run the second of these queries – showing a ‘contact card’ per person:\n\nCool. We have our contact cards! That being said, there’s a lot of nulls. Is the query definitely correct?\n\nLet’s hop back to the log output view for now. Remember that magic wand icon in the top right? Click that, and you’ll automagically send a new message containing the full log. The agent attempts to interpret these results. In the case of a successful query, it’ll typically suggest follow-up queries you could run. In the case of an error, it’ll try to diagnose the error and come up with a fixed query.\n\nIn this case when we send back the query, it suggests several follow-up queries, one of which is indeed a sanity check on whether any phone numbers exist at all in our dataset. It turns out they do not, so the phone property is largely for illustrative purposes.\n\nLLMs are genuinely impressive these days – and TypeDB’s AI agent is backed by GPT-5.2, a very powerful model. It still gets things wrong sometimes, though. Consider the following generated query:\n\nAt first the AI agent generates a query based on the idea of using subqueries inside a fetch, but unfortunately this returns a syntax error. We click the “magic wand” icon in the top right of the log, and this happens …\n\nThe agent tries a different approach and writes a correct query.\n\nThe “Fix with AI” button isn’t limited to Agent Mode. If you’re using the Query page to write TypeQL yourself, and you get an error – or an unexpectedly empty result set – you can hit “Fix with AI” (the magic wand button) and it’ll send the query log to a new Agent Mode conversation and have the agent describe what it thinks might be wrong.\n\nLLMs are perfectly capable of generating database queries in SQL, Cypher, and so on.\n\nFrom our research so far we’ve found that TypeDB Studio is a trailblazer in its early release of a fully-integrated AI assistant. Unlike other data platforms, the Studio agent is always aware of your data context (your schema) – and it allows you to actually run queries without leaving the assistant chat – a streamlined and better user experience.\n\nAgents that deal with complexity fail in predictable ways when they lack semantic grounding. They create invalid states. They make assumptions about the definitions and structures of concepts when there isn’t a clear, semantically rich schema setting those things out.\n\nTypeDB exhibits the following properties that are all of great value to an AI agent:\n\nYou can learn more about the power of semantic richness in: Why agents need ontologies\n\nAgent mode provides a totally new way of interacting with TypeDB. Its self-healing capabilities allow it to easily fix mistakes providing for an overall smooth experience. You don’t need prior knowledge of TypeQL – just ask questions in natural language and get answers.\n\nHere are some quick links to get up and running with TypeDB Studio and with vibe querying and to learn more:",
    "readingTime": 5,
    "keywords": [
      "magic wand",
      "wand icon",
      "vibe querying",
      "follow-up queries",
      "studio agent",
      "agent mode",
      "typedb studio",
      "schema",
      "error",
      "database"
    ],
    "qualityScore": 1,
    "link": "https://typedb.com/blog/vibe-querying-with-typedb-studio",
    "thumbnail_url": "https://cdn.sanity.io/images/xndl14mc/production/b6e0269711f2ea882e3dffe2518be5824b45fe95-1920x1080.webp",
    "created_at": "2026-02-25T18:52:28.827Z",
    "topic": "tech"
  },
  {
    "slug": "github-copilot-cli-is-now-generally-available",
    "title": "GitHub Copilot CLI is now generally available",
    "description": "GitHub Copilot CLI—the terminal-native coding agent that brings the power of GitHub Copilot directly to your command line—is now generally available for all paid Copilot subscribers. Since launching in public…",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://github.blog/changelog/2026-02-25-github-copilot-cli-is-now-generally-available/",
    "thumbnail_url": "https://github.blog/wp-content/uploads/2026/02/554416478-61e7b2e1-1372-4680-b467-073a0048caa3.png",
    "created_at": "2026-02-25T18:52:25.832Z",
    "topic": "tech"
  },
  {
    "slug": "openclaw-creator-says-vibe-coding-has-become-a-slur",
    "title": "OpenClaw creator says 'vibe coding' has become a slur",
    "description": "\"They don't understand that it's a skill,\" OpenClaw creator Peter Steinberger said, analogizing coding with AI to learning to play guitar.",
    "fullText": "OpenClaw's creator says the vibes of \"vibe coding\" are bad — and he doesn't appreciate the dismissive undertones of the term.\n\nThe viral agent OpenClaw is a product of AI code editors. The developer behind it, Peter Steinberger, used OpenAI's Codex to build it. Its original name, Clawdbot, referenced Claude Code.\n\nOne term has emerged to describe this type of work: vibe coding, or prompting AI to generate code. On OpenAI's \"Builders Unscripted,\" Steinberger said he wasn't a fan of the phrase.\n\n\"There are these people that write software the old way, and the old way is going to go away,\" Steinberger said. \"They call it 'vibe coding.' I think vibe coding is a slur.\"\n\nWhat's wrong with the term is that it implies ease, he said.\n\n\"They don't understand that it's a skill,\" Steinberger said, analogizing coding with AI to learning to play guitar.\n\nOther industry leaders have signaled frustration with the term. Former Google Brain scientist Andrew Ng called it \"unfortunate\" and \"misleading.\" Andrej Karpathy, the former Tesla AI head who coined the term, now thinks \"agentic engineering\" is the future.\n\nWhile Steinberger may not be a fan of the term, the use of vibe coding exploded throughout 2025. Collins Dictionary named it the word of the year.\n\nIf AI coding is like playing the guitar, then Steinberger has all his chords down. His interviewer, OpenAI's Romain Huet, asked Steinberger whether he still ships code without reading it.\n\n\"Most code is boring,\" Steinberger said. \"I have a pretty good understanding of what it writes.\"\n\nOne thing that helped Steinberger be a good AI coder: being a manager in the past.\n\n\"I led a team before,\" Steinberger said. \"I had a lot of software engineers under me. That also required accepting that they will not write exactly the same code that I want.\"\n\nBefore creating OpenClaw, Steinberger founded PSPDFKit. He's since had interactions with most major AI labs. Anthropic asked him to rename his chatbot. Meta's Mark Zuckerberg reached out with his experiences from testing it, Steinberger said.\n\nZuckerberg also courted him for a potential job, but OpenAI beat him out, with Steinberger accepting its job offer earlier this month. Sam Altman called him a \"genius with a lot of amazing ideas about the future of very smart agents.\"",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "steinberger",
      "software",
      "guitar",
      "accepting",
      "code",
      "openai's",
      "openclaw",
      "zuckerberg"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openclaw-creator-vibe-coding-term-slur-criticism-2026-2",
    "thumbnail_url": "https://i.insider.com/699e050adf1f09368aaaad98?width=1200&format=jpeg",
    "created_at": "2026-02-25T12:39:02.056Z",
    "topic": "finance"
  },
  {
    "slug": "specdriven-development-from-vibe-coding-to-structured-development",
    "title": "Spec-Driven Development: From Vibe Coding to Structured Development",
    "description": "Note: I currently work with a Payments Engineering team and wrote this as we are introducing spec-driven development into our development workflow.\r\nIntroduc...",
    "fullText": "Note: I currently work with a Payments Engineering team and wrote this as we are introducing spec-driven development into our development workflow.\n\nIf you've used an AI coding tool in the last year, you've probably had the experience: you describe what you want, the AI generates something that looks right, you run it, and... it doesn't quite work. You refine your prompt. The AI fixes one thing and breaks another. Three iterations later you're debugging code you didn't write and don't fully understand.\n\nThis is the failure mode of what Andrej Karpathy called \"vibe coding\" and it's become the default way most developers interact with AI. Spec driven development (SDD) is the emerging counter movement. Instead of throwing prompts at an LLM and hoping for the best, you write a structured specification first, then let the AI implement against it.\n\nThe idea isn't new. We've been writing requirements documents since forever, but the tooling is new. Tools like GitHub's Spec Kit, Amazon's Kiro, and Fission AI's OpenSpec are attempting to formalize this workflow into something repeatable. Whether that formalization helps or hinders depends entirely on what you're building, how you're building it, and the tradeoffs you're willing to make.\n\nOur team uses OpenSpec, so most of the practical examples in this post come from that experience. But the principles apply regardless of which tool you pick.\n\nThe pitch for AI assisted coding is attractive: describe what you want in English and get working code back. And for simple tasks, a helper function, a config change, renaming a module, it works remarkably well. The challenges starts when changes aren't trivial but require edits to multiple files or packages/modules.\n\nThe core issue is context loss. When you're five prompts deep into a feature, the AI has no persistent memory of the architectural decisions you made in prompt one. It doesn't know you chose a specific idempotency strategy for a reason. It doesn't remember that you explicitly avoided storing raw card data outside the tokenization boundary. Every new prompt starts from a partial view of the world, and the AI fills in the gaps with whatever patterns it's seen most in training data.\n\nIn payments systems, this produces particularly dangerous failures. Reconciliation logic scattered across three different modules because each prompt generated its own approach. A refund handler that doesn't account for partial captures. Currency conversion applied twice because the AI didn't know about the upstream normalization step. And perhaps most critically in our domain, security flaws: API keys committed to source, missing input validation on transaction amounts, authorization checks that live on the client instead of the server. Studies have found that roughly 45% of AI generated code contains security vulnerabilities. In a payments context, that's more than just a bug but a compliance issue.\n\nThe other failure is architectural drift. Without a shared plan, each prompt/response cycle makes locally reasonable decisions that are globally incoherent. The AI can't refactor itself out of architectural problems it doesn't understand. You ask it to add retry logic to a payment gateway call and it builds a standalone retry mechanism, unaware that you already have a circuit breaker pattern in your infrastructure layer. Once the codebase reaches a certain size, the context window can only see fragments of it. You end up with a system that processes transactions but that nobody, including the AI, fully understands anymore.\n\nThis isn't the AI being dumb. It's the natural consequence of building without a map.\n\nAt its simplest, spec driven development means: write down what you're building before you write the code, and make that written artifact the thing your AI agent works from.\n\nThat might sound like waterfall but It's not, or at least, it doesn't have to be. The key differences are timescale and scope. Traditional waterfall specs were project level documents written over weeks and often carved in stone. SDD specs are feature level documents written in minutes and meant to evolve. You're not planning an entire system upfront; you're planning the next meaningful chunk of work in enough detail that an AI can implement it without guessing.\n\nA typical SDD workflow looks like this:\n\nYou're not writing all of this yourself. You describe the intent in natural language, and the AI generates the spec artifacts: the proposal, the requirements, the design, the task breakdown. Your job is to review, refine, and correct. You steer and the AI does the heavy lifting. This is what makes the process fast enough to be practical. Writing a 200 line spec by hand for every feature would be painful. Having the AI draft it in 30 seconds and then spending 5 minutes reviewing and adjusting it is a different proposition entirely.\n\nThe spec becomes a persistent artifact, a \"super prompt\" that doesn't disappear when your chat session ends. It lives in version control alongside your code. When the AI drifts, you point it back to the spec. When requirements change, you update the spec and regenerate.\n\nThe fundamental shift is that the specification becomes the source of truth, and code becomes the derived artifact. Traditional documentation describes code that already exists. SDD inverts that relationship. You define the behaviour, constraints, and architecture in the spec, and the AI produces code that conforms to it. The spec isn't something you write after the fact to explain what was built but the input that determines what gets built. Code is the output.\n\nThree tools have emerged as the most prominent in this space. Each takes a different philosophical approach.\n\nSpec Kit is an open source CLI from GitHub that scaffolds a spec driven workflow into your existing project. It's agent agnostic, working with GitHub Copilot, Claude Code, Gemini CLI, and others. The workflow follows rigid phases driven by slash commands: /speckit.constitution to establish project principles, /speckit.specify to create feature specs, /speckit.plan for a technical plan, /speckit.tasks for work items, and /speckit.implement to execute.\n\nStrengths: Thorough documentation output, the \"constitution\" concept for project wide principles, works with many agents.\n\nWeaknesses: Heavyweight. Sometimes it get generate a lot of artifacts for simple changes. Rigid phase gates mean you can't easily jump back and forth between planning and implementing.\n\nKiro is a full IDE (a VS Code fork) with spec driven development baked into the editing experience. The workflow follows a similar shape (requirements → design → tasks → implement) but is tightly integrated with the editor. It generates user stories with acceptance criteria, creates technical design documents, and produces task lists. It also introduces \"Hooks,\" user defined prompts triggered by file changes.\n\nStrengths: Most polished integrated experience. The Hooks system is excellent and something you'd have to configure manually if you decide to do it on your own. No context switching between planning and editing because of the IDE integration.\n\nWeaknesses: You're locked into their IDE and limited to Claude models. Can be overkill for small changes. One developer reported a simple bug fix generating 4 user stories with 16 acceptance criteria. The overhead can be significant.\n\nOpenSpec is the most lightweight of the three. It's a TypeScript CLI with a fluid, iterative workflow and no rigid phase gates. Where Spec Kit enforces a strict sequence and Kiro wraps everything in an IDE, OpenSpec gets out of your way and lets you move between planning artifacts freely.\n\nIts distinguishing philosophy is \"brownfield first.\" While the other tools are optimized for building new things from scratch, OpenSpec is designed to work with existing codebases. Each change produces a \"spec delta,\" a document that captures what's being added, modified, or removed relative to the existing system. Over time, these deltas merge into a living specification that reflects the current state of the system.\n\nOpenSpec also handles change history better. Every completed change is archived with its full artifact set: the original proposal, the spec deltas, the design, and the task list. This means you can go back and see not just what changed in the system, but why it changed, what alternatives were considered in the design, and what the original acceptance criteria were. Spec Kit and Kiro generate artifacts during planning but don't have the same structured archive and merge cycle. In OpenSpec, the openspec/changes/archive/ directory becomes a chronological record of every significant change to the system, and the openspec/specs/ directory is always the merged, current truth. For regulated environments where auditability matters, this distinction is significant.\n\nStrengths: Works with 20+ AI tools including Claude Code, Cursor, Copilot, Windsurf, and many others. The brownfield focus is valuable in our context as most real work is on existing codebases. Fluid workflow lets you update any artifact at any time and you are not forced into a linear way of working. The archive/merge cycle produces both a living spec and an auditable change history.\n\nWeaknesses: Less hand holding in the spec writing process is the trade-off it makes while allowing you to navigate back-and-forth between spec and implementation. The tool is newer and the ecosystem is still growing.\n\nOpenSpec requires Node.js 20.19.0 or higher.\n\nThen navigate to your project directory and initialize:\n\nThe init process will ask which AI tool you're using and configure the appropriate slash commands or agent instructions for your environment.\n\nOpenSpec also works with pnpm, yarn, bun, and nix. See the official installation docs for alternative paths.\n\nThen refresh agent instructions in each project:\n\nUnderstanding the full lifecycle of an OpenSpec change is worth the time, because the artifacts it generates serve different roles on the team in different ways.\n\nOpenSpec's workflow is built around the opsx slash commands. Here's the complete set, the ones you interact with the most are bolded:\n\nThe typical flow is new → ff → apply → archive, but the power of OpenSpec is that you can break out of that sequence at any point. Need to revisit the design after you've started implementing? Just edit design.md. Want to add acceptance criteria while coding? Update the spec delta. There are no phase gates forcing you to \"finish\" one stage before moving to another.\n\nOne of the first decisions in any OpenSpec workflow is how you enter it. There are two entry points, and choosing the right one makes a real difference in the quality of what comes out the other side.\n\n/opsx:new is for when you know what you're building. You have a clear feature in mind, you understand the requirements well enough to describe them, and you're ready to start generating planning artifacts. Maybe you've already discussed this in a planning meeting. Maybe you've built something similar before. Maybe the ticket is well defined and you just need to formalize it. In these cases, /opsx:new add-feature-name followed by /opsx:ff gets you from zero to a full set of planning documents in minutes.\n\nThis works well when the problem space is familiar and constraints are understood. You've implemented retry logic before, you know the gateway's behavior, you just need to formalize it.\n\n/opsx:explore is for when you're still figuring things out. Maybe you have a vague idea but the requirements aren't clear. Maybe you're investigating a production anomaly and you don't know where the root cause is yet. Maybe a stakeholder said \"we need to support instant payments\" and you need to figure out what that actually means for your settlement architecture before you can spec anything.\n\nExplore opens a broader, open ended conversation with the AI before any change folder is created and before any artifacts are generated. There's no structure imposed, no proposal template to fill out, no pressure to define acceptance criteria. You're just thinking out loud with an assistant that has context on your codebase.\n\nThe explore conversation might last five minutes or an hour. It might branch into several tangents before converging on a clear problem statement. That's intentional. You're doing the messy, nonlinear thinking that produces good requirements, and you're doing it before the spec exists rather than discovering gaps during implementation.\n\nThis is also where existing requirements documents from business analysts become valuable. In payments organizations, BAs often produce detailed feature requirements independently of any SDD tool: field mapping spreadsheets, business rule matrices, sample ISO 20022 payloads (pain.001, pain.008, pacs.008), regulatory constraint documents, and workflow diagrams. These artifacts are the raw material that explore turns into actionable specs.\n\nRather than asking the AI to invent requirements from scratch, you feed it the BA's work:\n\nThe explore phase becomes a bridge between the BA's domain knowledge and the engineering reality of the codebase. The BA doesn't need to know about your GenServer architecture or your Ecto schema conventions. The developer doesn't need to memorize the ISO 20022 payload structure. Explore lets both perspectives converge into a proposal that reflects both business intent and technical feasibility.\n\nWhen you've reached clarity, you transition naturally into the structured workflow:\n\nNow the proposal and specs will be grounded in both the BA's requirements and the technical understanding you built during exploration, rather than being generated from a one line prompt.\n\nUse /opsx:new when you can describe the feature or fix in a sentence and you're confident in the scope. Use /opsx:explore when any of the following are true: you're unsure what the root cause of a problem is, the requirements are ambiguous or underspecified, you need to evaluate multiple approaches before committing to one, or you want to pressure test an idea before investing in formal planning. In practice, we find ourselves using explore more often than we initially expected. The few minutes spent thinking before speccing consistently produce better specs, which in turn produce better code.\n\nWhen you run /opsx:new add-idempotent-refunds, OpenSpec creates a change directory:\n\nRunning /opsx:ff (or stepping through with /opsx:continue) generates the planning artifacts:\n\nEach of these artifacts has a specific purpose and a specific audience. Let's look at what goes into them.\n\nproposal.md is the \"why\" document. It describes the motivation for the change, the scope of what's included and excluded, and any constraints or dependencies. This is the document you'd share in a planning meeting or attach to a ticket. It answers the question: \"Why are we doing this, and what does 'done' look like at a high level?\" For a refunds feature, this might capture that the driver is duplicate refund incidents costing the business money, that the scope includes full and partial refunds but excludes chargebacks, and that the constraint is backwards compatibility with the existing refund API contract.\n\nspecs/ contains the spec delta, the functional requirements for this specific change. Requirements are marked as ADDED, MODIFIED, or REMOVED relative to the current system. Each requirement uses structured language (\"The system SHALL...\") with clear acceptance criteria and scenarios. This is where edge cases live. This is where you define what happens when a refund is submitted with the same idempotency key as a previous request, what the system does when the gateway returns a timeout mid refund, or how partial refunds interact with the original transaction's settlement status.\n\ndesign.md is the technical blueprint. It covers the data model, API contracts, component architecture, sequence flows, and any technology choices specific to this feature. For the refunds example, it's where you'd document the idempotency key storage strategy, the state machine transitions for refund lifecycle, and the gateway adapter interface for multi acquirer support.\n\ntasks.md breaks the work into discrete, ordered implementation steps. Each task is small enough to verify independently, ideally something that can be implemented in under 30 minutes. Tasks have clear completion criteria so both the developer and the AI know when they're done.\n\nWhen all tasks are complete and verified, /opsx:archive does something important: it merges the spec deltas from the change back into the main openspec/specs/ directory. The change folder moves to openspec/changes/archive/, preserving the history. The main specs now reflect the updated state of the system.\n\nThis is the mechanism that turns specs into a living document. After a dozen features have been built and archived, openspec/specs/ contains a comprehensive, up to date description of what the system does. Not what it was designed to do originally, but what it actually does right now.\n\nOne of the underappreciated aspects of spec driven development is that the artifacts aren't just for the developer writing the code. They create value across every role that touches the project.\n\nThe immediate benefit is implementation quality. Instead of translating a vague Jira ticket into code via a series of increasingly frustrated prompts, you're working from a spec that already captures requirements, edge cases, and technical decisions. The AI produces better code because it has better context. You spend less time debugging and reworking because misunderstandings surface during spec review, not during code review.\n\nThe longer term benefit is onboarding and maintenance. When you come back to a feature six months later, or when a new developer joins the team, the spec explains not just what the code does but why it was built that way. The proposal captures the business motivation. The design doc captures the technical rationale. The spec captures the behavioral contract.\n\nThe proposal and spec artifacts are written in structured natural language, not code. A BA or PM can read proposal.md and immediately understand the scope, motivation, and acceptance criteria for a change without needing to parse a pull request.\n\nMore importantly, they can contribute to these documents. If the spec says \"The system SHALL retry failed direct debit collections up to 3 times\" and the BA knows the scheme rules mandate a maximum of 2 retries with specific interval requirements, they can flag that in the spec before any code is written. The spec becomes a shared contract between product and engineering, reviewable by both sides.\n\nBAs in payments organizations often produce detailed requirements documents that exist outside of any development tool: field mapping spreadsheets between internal formats and ISO 20022 messages, business rule matrices for transaction routing, sample payloads for pain.001 or pacs.008 messages, regulatory constraint documents, and scheme specific validation rules. These documents don't need to be rewritten into OpenSpec format. Instead, they serve as input to the /opsx:explore conversation and as reference material that the proposal and specs can point to. The spec might say \"Field mappings follow the BA's pain.008 mapping document (see docs/ba-requirements/sepa-dd-field-mappings.xlsx)\" rather than duplicating that content. OpenSpec captures the engineering requirements; the BA's documents capture the domain requirements. The two reference each other.\n\nFor teams practicing any kind of requirements analysis, the spec delta format (ADDED/MODIFIED/REMOVED) maps naturally to how BAs think about change impact. You can see at a glance exactly what existing behavior is changing and what's new.\n\nThe specs are essentially test plans waiting to happen. Each requirement with its acceptance criteria maps directly to test cases. \"WHEN a refund is submitted with an idempotency key matching a previously completed refund, THEN the system SHALL return the original refund response without processing a duplicate\" is a test case in all but name.\n\nQA can review specs before implementation begins, catching gaps in test coverage at the cheapest possible point in the development cycle. In payments, where edge cases around timeouts, partial failures, and concurrent operations are where bugs hide, having QA eyes on the spec early is especially valuable. They can also use specs to verify completeness: does the implementation actually cover every scenario in the spec? OpenSpec's /opsx:verify command automates part of this check, but human QA review of the spec itself is where the real value lies.\n\nThe design document is where architectural oversight happens. A principal can review design.md to ensure the proposed approach fits the system's overall architecture, without needing to wait for a code review to discover that someone introduced a new database table that duplicates an existing one, or bypassed the payment gateway abstraction layer by calling the acquirer API directly.\n\nThe proposal document is equally valuable at this level. It provides enough context to make prioritization decisions, estimate impact on downstream systems like settlement and reconciliation, and flag dependencies before work begins.\n\nFor organizations running architecture review boards or design review processes, OpenSpec artifacts slot directly into those workflows. The artifacts are markdown in version control, which means they can be reviewed through the same pull request process as code.\n\nThe openspec/specs/ directory, the living spec that accumulates as changes are archived, becomes something like institutional memory for the project. It captures not just the current state of the system but the evolution of requirements over time. New team members can browse the specs to understand the system. Archived changes provide an audit trail of what changed, when, and why.\n\nThis is especially valuable for distributed teams where not everyone is in every meeting. The spec is always available, always current, and always in the repo.\n\nIn most payments organizations, business analysts produce detailed requirements documents long before any developer opens an IDE. These documents are the product of weeks of domain analysis: field mapping spreadsheets that map internal data structures to ISO 20022 message formats like pain.001 or pacs.008, business rule matrices that define transaction routing logic, validation rules derived from scheme specifications, sample XML payloads, regulatory constraint summaries, and workflow diagrams for processes like mandate lifecycle management or chargeback dispute flows.\n\nThese artifacts are enormously valuable. They represent concentrated domain knowledge that no AI model has in its training data. But they exist outside of any development tool, usually in Confluence pages, shared drives, or Excel files. The question is: where do they plug into the SDD workflow?\n\nRather than asking the AI to invent payment domain requirements from scratch (which it will get wrong in subtle, dangerous ways), you feed the BA's existing documents into the explore conversation. The AI reads the BA's field mappings, business rules, and sample payloads, then maps them against your actual codebase. The explore conversation becomes a structured dialogue about feasibility: which business rules can be implemented directly, which ones require architectural changes, where the BA's requirements conflict with existing behavior, and what edge cases the BA may not have considered from a technical perspective.\n\nThis conversation might reveal that 70% of the BA's requirements map cleanly to existing patterns and 30% require new design decisions. Those design decisions then flow into the proposal and spec with full context, rather than being invented by the AI from a one sentence prompt.\n\nThe key principle: the BA's documents are input to the spec, not replaced by it. The OpenSpec proposal can reference them directly (\"Field mappings follow the BA's pain.008 mapping document, see docs/ba-requirements/sepa-dd-field-mappings.xlsx\"). The spec captures the engineering interpretation of business requirements, while the BA's artifacts remain the authoritative source for domain rules. The two complement each other.\n\nFor teams with a strong BA function, this workflow turns explore into the most valuable step in the entire process. It's where domain expertise meets technical reality, and where misunderstandings between product and engineering get caught before they become expensive.\n\nFor years, the standard way to decompose work in software organizations has been the Agile hierarchy: Epics break into Features, Features break into User Stories, User Stories break into Tasks. Each layer adds structure, and each layer adds overhead. Grooming sessions to refine stories. Estimation ceremonies to assign points. Sprint planning to negotiate what fits. Story splitting when something is \"too big.\" Acceptance criteria written in Given/When/Then format.\n\nThis process was designed for a world where humans wrote every line of code, and work needed to be decomposed into pieces small enough for one developer to complete in a sprint. The granularity served a coordination function: if three developers are working on the same feature in parallel, you need clearly bounded units of work to avoid stepping on each other.\n\nWith AI agents handling the bulk of code generation, developers now work in significantly larger chunks. A feature that would have been split into 8 user stories with 24 tasks can be described as a single spec and implemented in one session. The AI doesn't need two week sprints to context switch between stories. It doesn't need story points to estimate effort. It doesn't care whether a unit of work is a 3 or a 5. It needs a clear description of what to build and enough context to build it correctly.\n\nThe overhead of the old hierarchy was always significant. Ceremonies consume 15-30% of a team's time. The BA writes detailed requirements and translates them into epics and stories. The tech lead estimates them. The developer re-interprets them during implementation. Each translation step is an opportunity for information loss.\n\nSDD collapses this. Instead of an Epic with 5 Features containing 20 User Stories containing 60 Tasks, you have a proposal that captures the business intent, a spec that defines the requirements, a design that describes the technical approach, and a task list that the AI executes against. The entire chain from \"why are we doing this\" to \"what code gets written\" lives in one change folder, reviewable as a single unit. There's no translation loss and the spec is the shared artifact that all of those roles read, review, and contribute to.\n\nThis doesn't mean you abandon planning. It means the unit of planning shifts from \"what can one developer finish in two days\" to \"what is the next coherent change to the system.\" That change might be small (fix a settlement rounding error) or large (add multi currency support). The spec scales to fit either. And because the AI generates the task breakdown from the spec, you don't need to spend planning meetings debating granularity. The tasks exist to guide the AI's implementation, not to fill a sprint board.\n\nSDD offers a natural landing spot for a switch to AI-first development. You keep the parts that matter (clear requirements, technical review, incremental delivery) and shed the parts that were only ever coordination scaffolding.\n\nLet's walk through a concrete example using OpenSpec to add multi currency settlement support to a payment processing platform.\n\nStep 1: Explore (optional but useful)\n\nIf the requirements aren't fully clear, or if you have BA created documents that need to be digested before planning, start with exploration:\n\nThis step is for thinking, not committing. No change folder is created yet.\n\nStep 3: Generate planning artifacts\n\nIf you have a clear picture of what to build, fast forward through all planning stages:\n\nAlternatively, use /opsx:continue to step through one phase at a time, reviewing and refining each artifact before moving to the next.\n\nStep 4: Review and refine the artifacts\n\nThis is the most important step. Read the generated proposal, specs, and design. Push back on anything that doesn't match your intent:\n\nReal work isn't linear. OpenSpec handles context switches gracefully because your plan lives in the artifacts, not in the AI's memory.\n\nOne important detail: during /opsx:apply, you can stop the AI at any point while monitoring its output. If you see it heading in the wrong direction on task 1.3, you stop it, correct course, and resume. The task list in tasks.md tracks what's been completed, so nothing about your plan is lost. This is fundamentally different from vibe coding, where stopping mid stream means losing context and hoping you can reconstruct where you were.\n\nThe same principle applies to bigger interrupts. You can pause an entire feature to handle something urgent:\n\nThe multi currency feature picks up exactly where it left off. The artifacts held the plan while you were away, whether that was five minutes or five days.\n\nSDD is not appropriate in all cases. Here are some cases where the overhead isn't worth it:\n\nQuick bug fixes. If you know exactly what's wrong and the fix is a one line change to a gateway timeout value, writing a spec is like filing a building permit to hang a picture frame. Just fix it.\n\nExploratory prototyping. When you're trying to figure out what to build, not how to build it, specs slow you down. Vibe coding is genuinely great for rapid exploration. If you're prototyping a new merchant dashboard layout to see what feels right, just build it iteratively.\n\nHighly visual or interactive work. SDD tools are text based. If your feature is primarily about UI layout, animation, or interaction design, you'll spend more time describing the visual result in markdown than you'd spend just building it with visual feedback (though pairing SDD with TideWave can work wonders for UI work).\n\nTrivial features. Updating an error message string, renaming a config key, bumping a dependency version. These don't need a spec. Use your judgment about the complexity threshold.\n\nRapidly changing requirements. If you're in a phase where the payment scheme keeps revising the spec and requirements shift weekly, maintaining your own specs becomes overhead that fights against your pace. Get to stability first, then spec the features that need to stick.\n\nThe general rule: if you can hold the entire change in your head and verify it by looking at it, you probably don't need a spec. If the change involves multiple files, multiple concerns, or behavior you can't verify visually, a spec starts paying for itself.\n\nHaving used these tools and studied the experiences of others, here are the traps:\n\nSpec bloat. The AI loves to generate exhaustive specifications. A feature that would take you 30 minutes to implement can produce 800+ lines of markdown. You have to be disciplined about trimming specs to what's actually useful. If you're not reading the spec carefully, it's worse than not having one because you'll have false confidence that edge cases are covered when they're not.\n\nThe waterfall trap. SDD can slide into big design up front if you're not careful and start bundling many features into one spec. If changing the spec feels expensive or bureaucratic, you've over formalized. OpenSpec's fluid workflow helps here since there are no phase gates, but you still need the discipline to keep specs lightweight enough to throw away and rewrite if you find yourself going down the wrong path.\n\nSpec drift. The spec says one thing; the code does another. This happens when you make implementation fixes outside the spec workflow. Either update the spec when you deviate, or accept that the spec is aspirational rather than authoritative. OpenSpec's /opsx:sync command can help keep specs aligned during long running changes.\n\nThe AI ignores its own spec. This is a real and documented problem. Context windows are larger, but that doesn't mean the AI attends to everything in them equally. People have reported that AI agents generate code that contradicts the spec they just wrote, creating duplicate classes, ignoring constraints, or implementing patterns the spec explicitly avoided. The /opsx:verify step exists specifically to catch this.\n\nReview fatigue. SDD adds a new category of artifact to review. You're now reviewing specs AND code. If your team doesn't value spec review as highly as code review, specs become rubber stamped documents that provide an illusion of rigour.\n\nOver application to small changes. The tooling doesn't scale down well. Applying the full SDD workflow to a minor feature creates overhead that dwarfs the implementation time. You need a personal threshold for when to spec and when to just build.\n\nEvery discussion of SDD eventually arrives at the same question: isn't this just waterfall with better marketing?\n\nThe comparison is fair to raise and unfair to leave unexamined. Traditional waterfall failed because of long feedback loops: months of design, months of implementation, and discovery at the end that the design didn't match reality. The feedback cycle was measured in quarters.\n\nSDD, practiced well, has feedback cycles measured in minutes to hours. You write a spec for a single feature, not an entire system. You review the generated design before implementation starts. You implement in small, verifiable tasks. And critically, changing the spec and regenerating is cheap. The whole point is that code is a derived artifact you can throw away and recreate.\n\nSDD can slide into waterfall like rigidity if you treat specs as immutable, if the spec writing phase becomes its own bottleneck, or if you use SDD as a substitute for iterative discovery. As Gojko Adzic observed, the movement builds on solid intent-first ideas but could reintroduce rigidity if practitioners aren't thoughtful about it.\n\nThe Thoughtworks perspective captures the nuance well: the problems of vibe coding come from being too fast, spontaneous, and haphazard, while the problems of waterfall come from being too slow, rigid, and disconnected from reality. SDD, when practiced well, occupies the middle ground. It provides a mechanism for shorter and more effective feedback loops than either extreme.\n\nThe honest answer is that SDD sits on a spectrum. At one end, you have \"spec as lightweight sketch,\" a quick outline that gives the AI direction without constraining it. At the other end, you have \"spec as source of truth,\" a comprehensive document that the code must conform to. OpenSpec's fluid approach leans toward the lighter end of that spectrum, which is why it appeals to teams who want discipline without ceremony.\n\nReduced rework. Catching misunderstandings at the spec level is dramatically cheaper than catching them in code. When a BA's field mapping is wrong, you want to discover that while reviewing a proposal, not while debugging a failed settlement file at 2 AM.\n\nPersistent context. Specs survive session boundaries, tool switches, and team changes. Six months from now, when someone asks why the FX rate locking works the way it does, the spec and its proposal explain both the what and the why.\n\nReviewable intent across roles. You can review a spec without reading any code. Product managers, BAs, QA, and principals can participate in spec review and catch requirement gaps before implementation begins. In a payments context, this means compliance can review the spec for regulatory alignment without needing to read Elixir.\n\nTime upfront. Writing and reviewing specs takes time that vibe coding doesn't require. For simple tasks, this overhead is pure cost with minimal benefit.\n\nFalse precision. Detailed specs can create an illusion of completeness. Just because the spec covers edge cases on paper doesn't mean the AI will implement them correctly. You still need to test.\n\nTool immaturity. These tools are all early stage. Expect rough edges, breaking changes, and workflow gaps. The ecosystem is moving fast, which means today's best practices may be obsolete in six months.\n\nSpec driven development is less than a year old as a named practice, and the tooling is evolving fast. The fundamental insight, that AI agents produce better code when given structured intent rather than ad hoc prompts, seems durable even if the specific tools don't survive.\n\nWhat's interesting is the convergence. BDD (Behavior Driven Development), TDD (Test Driven Development), and now SDD all share the same DNA: define the desired behavior before writing the implementation. SDD is that idea adapted for a world where the implementer is an AI agent rather than a human developer.\n\nThe open question is whether specs will remain the domain of dedicated tools, or whether this discipline gets absorbed into the AI coding tools themselves. We're already seeing Cursor, Claude Code, and Copilot add planning and multi step reasoning capabilities that accomplish some of what SDD tools do, without the explicit spec writing step.\n\nFor now, the practical takeaway is simple: if you're doing anything more complex than a quick prototype with AI coding tools, some form of structured planning, whether you call it SDD or just \"thinking before prompting,\" will produce better results than vibing your way through it. The tools can help enforce that discipline, but the discipline itself is what matters.\n\nThe spec isn't the point. The thinking is.",
    "readingTime": 30,
    "keywords": [
      "openspec's fluid",
      "traditional waterfall",
      "explicitly avoided",
      "removed relative",
      "root cause",
      "slash commands",
      "transaction routing",
      "mappings follow",
      "throw away",
      "ba's pain"
    ],
    "qualityScore": 1,
    "link": "https://zarar.dev/spec-driven-development-from-vibe-coding-to-structured-development/",
    "thumbnail_url": "https://bear-images.sfo2.cdn.digitaloceanspaces.com/zarar/photo-1556075798-4825dfaaf498.webp",
    "created_at": "2026-02-25T01:15:14.388Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-pushes-claude-into-excel-and-powerpoint-escalating-ai-battle-with-microsoft-and-openai",
    "title": "Anthropic pushes Claude into Excel and PowerPoint, escalating AI battle with Microsoft and OpenAI",
    "description": "The startup is trying to transform its Claude AI model from a coding tool into a much broader workplace technology platform.",
    "fullText": "Anthropic is pushing deeper into the enterprise, embedding its AI model Claude directly into the workplace software employees use every day.\n\nOn Tuesday, the AI startup unveiled \"Cowork & Plugins for the Enterprise,\" a package of AI tools that get Claude operating inside popular work applications, such as Microsoft Excel and PowerPoint. Instead of copying answers from a chatbot into spreadsheets or slide decks, users can now run Claude directly inside those applications, with the system carrying context between programs.\n\n\"Until now, enterprise AI has followed a persistent pattern: you go to the AI, get an answer, then go back to your actual tools to do the work,\" Anthropic wrote in its announcement on Tuesday. \"Now, Claude works inside the tools knowledge workers already use — Excel, PowerPoint, Slack — not as a separate window, but as part of how work actually gets done.\"\n\nThe launch signals Anthropic's clearest attempt yet to challenge Microsoft and OpenAI for control of the digital workplace in the era of AI.\n\nMicrosoft has embedded its own 365 Copilot tool across Word, Excel, PowerPoint, Outlook, and Teams, while also offering Copilot Studio for enterprises to build custom AI agents. OpenAI, meanwhile, launched Frontier earlier this month, a platform built on its ChatGPT enterprise offerings. Google has integrated its Gemini AI throughout Gmail and its Workplace applications. Amazon also offers a similar service called Quick Suite.\n\nAnthropic is hoping to position itself as the default operational layer across enterprise workflows.\n\nAt the center of the launch are customizable \"plugins,\" or specialized AI agents configured for roles including financial analysis, design, operations, and HR.\n\nAnthropic said the plugins are open-source and portable, allowing companies to modify and deploy them without being locked into a single ecosystem. Several were co-developed with partners, including FactSet, S&P, and Slack.\n\nThe AI startup is also rolling out a slate of new connectors to platforms such as Google Drive, Gmail, and DocuSign, enabling Claude to access live enterprise data with administrative controls. Enterprises can create private plugin marketplaces and manage access at scale.\n\nAnthropic said customers, including L'Oréal, Deloitte, and Thomson Reuters, have already built specialized AI agents using Claude to automate workflows and accelerate internal processes.\n\nWith this launch, Anthropic is making clear that it no longer wants to be known primarily as a tool for developers. It wants Claude embedded across the enterprise, supporting finance teams, HR departments, analysts, and executives alike.\n\n\"In 2025, Claude Code transformed how software gets built,\" Anthropic said in Tuesday's announcement. \"In 2026, we're bringing that transformation to all knowledge work.\"\n\nHave a tip? Contact this reporter via email at ekim@businessinsider.com or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "excel powerpoint",
      "claude directly",
      "enterprise",
      "workplace",
      "tools",
      "inside",
      "applications",
      "launch",
      "across",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/anthropic-ai-software-claude-microsoft-powerpoint-excel-slack-2026-2",
    "thumbnail_url": "https://i.insider.com/699cefcbefb52c8bd0deb9e2?width=1200&format=jpeg",
    "created_at": "2026-02-24T18:46:23.822Z",
    "topic": "tech"
  },
  {
    "slug": "package-managers-la-carte-a-formal-model-of-dependency-resolution",
    "title": "Package Managers à la Carte: a formal model of dependency resolution",
    "description": "Package managers are legion. Every programming language and operating system has its own solution, each with subtly different semantics for dependency resolution. This fragmentation prevents multilingual projects from expressing precise dependencies across language ecosystems; it leaves external system and hardware dependencies implicit and unversioned; it obscures security vulnerabilities that lie in the full dependency graph. We present the \\textit{Package Calculus}, a formalism for dependency resolution that unifies the core semantics of diverse package managers. Through a series of formal reductions, we show how this core is expressive enough to model the diversity that real-world package managers employ in their dependency expression languages.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2602.18602",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-02-24T12:39:29.851Z",
    "topic": "tech"
  },
  {
    "slug": "ibm-stock-sinks-as-anthropic-rolls-out-yet-another-disruptive-ai-tool",
    "title": "IBM stock sinks as Anthropic rolls out yet another disruptive AI tool",
    "description": "IBM shares tumbled after Anthropic unveiled a tool aimed at business that use a decades-old programming language",
    "fullText": "The AI-driven software sell-off that ravaged markets in early February isn't over, with IBM stock tanking on Monday as Anthropic unveiled another disruptive AI tool.\n\nIBM shares fell 13% after the maker of the Claude AI chatbot announced another new tool. In this case, the startup announced an update that can help reduce the cost of COBOL (Common Business-Oriented Language) systems used by many businesses.\n\nThe term may not be widely known outside the software industry, but it quickly sent IBM stock into a nosedive in the latest installment of the software sell-off.\n\nAnthropic announced the update in a blog post, laying out why COBOL matters and why people outside the tech community should care.\n\n\"COBOL is everywhere. It handles an estimated 95% of ATM transactions in the US,\" it wrote. \"Hundreds of billions of lines of COBOL run in production every day, powering critical systems in finance, airlines, and government. Despite that, the number of people who understand it shrinks every year.\"\n\nAmong its uses, the company said the new tool could \"Identify risks that would take human analysts months to surface.\" The new AI use case poses a potential threat to the kind of business data service that comprises a core part of IBM's business.\n\nAccording to the startup, Claude will allow companies to streamline their COBOL operations for a fraction of their previous cost, similar to the legal plugins the company rolled out early in the month that triggered the initial software sell-off.\n\n\"Legacy code modernization stalled for years because understanding legacy code cost more than rewriting it. AI flips that equation,\" Anthropic said in its post.\n\nTech stocks initially bounced back after Anthropic's legal plugin release spooked Wall Street and caused investors to rush to limit their legal tech exposure, but the market was selling off sharply again on Monday.\n\nEven before the Anthropic news hit IBM shares, the sector was tumbling earlier in the session as investors reacted to tariff uncertainty and a report making the rounds online that speculated about far-reaching negative impacts of AI.",
    "readingTime": 2,
    "keywords": [
      "ibm stock",
      "ibm shares",
      "legacy code",
      "software sell-off",
      "anthropic",
      "tool",
      "tech",
      "legal",
      "another",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/ibm-stock-price-anthropic-ai-update-cobol-language-software-selloff-2026-2",
    "thumbnail_url": "https://i.insider.com/699cbbf5efb52c8bd0deb52c?width=1200&format=jpeg",
    "created_at": "2026-02-24T01:09:35.032Z",
    "topic": "finance"
  },
  {
    "slug": "the-upside-of-opening-up-dei-programs-to-everyone",
    "title": "The Upside of Opening Up DEI Programs to Everyone",
    "description": "In the wake of the Supreme Court’s seismic 2023 decision ending affirmative action in higher education—and more than 100 lawsuits challenging organizational DEI efforts—many U.S. companies reexamined their programming, opening up to all employees programs that had previously been restricted to members of specific demographic groups. Does this strategy destroy the purpose of these programs themselves? Research suggests there are multiple hidden upsides, including: 1) encouraging effective allyship; 2) putting pressure on non-allies; and 3) reducing backlash.",
    "fullText": "The Upside of Opening Up DEI Programs to Everyone by Kenji Yoshino and David GlasgowFebruary 23, 2026PostPostShareSavePrintSummary.   Leer en españolLer em portuguêsPostPostShareSavePrintOver the past few years in the U.S., more than a hundred federal lawsuits have been filed challenging diversity, equity, and inclusion (DEI) programs in companies, firms, universities, and the public sector.",
    "readingTime": 1,
    "keywords": [
      "dei programs"
    ],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/02/the-upside-of-opening-up-dei-programs-to-everyone",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_23_unsplash.jpg",
    "created_at": "2026-02-23T18:49:03.698Z",
    "topic": "business"
  },
  {
    "slug": "linux-70-makes-preparations-for-rust-195",
    "title": "Linux 7.0 Makes Preparations For Rust 1.95",
    "description": "Last week was the main feature pull of Rust programming language updates for the Linux 7.0 kernel merge window",
    "fullText": "Linux 7.0 Makes Preparations For Rust 1.95\n\nLast week was the main feature pull of Rust programming language updates for the Linux 7.0 kernel merge window. Most notable with that pull was Rust officially concluding its \"experimental\" in now treating Rust for Linux kernel/driver programming as stable and here to stay. Sent out today was a round of Rust fixes for Linux 7.0 that includes preparations for the upcoming Rust 1.95 release.\r\nRust 1.95 is being branched from master on 27 February and aiming for its stable release on 16 April. Rust 1.95 stabilizes if let guards, changing some ports to tier 2 status, and various other changes.\r\nFor Linux 7.0 they are now passing the \"-Zunstable-options\" flag that will be required by the Rust 1.95 release. The -Zunstable-options allows for the use of other new, unstable command line options.\r\nFor the kernel's irq module, there is a missing bound detected by the in-development Rust 1.95 code to be addressed. With the pin-init crate was also a Clippy warning that changed behavior with the upcoming Rust 1.95 release.\r\nMeanwhile this round of Rust fixes for Linux 7.0 also fixes an objtool warning when using the older Rust 1.84 release plus a fix to the list module to address missing \"unsafe\" blocks and placeholder safety comments to macros.\r\nMore details on these Rust fixes sent out today for Linux 7.0 to focus on future Rust 1.95 compatibility can be found via this pull request.\n\n 14 Comments",
    "readingTime": 2,
    "keywords": [
      "upcoming rust",
      "rust release",
      "rust fixes",
      "linux",
      "preparations",
      "programming",
      "round",
      "module",
      "missing",
      "warning"
    ],
    "qualityScore": 0.85,
    "link": "https://www.phoronix.com/news/Linux-7.0-Rust-1.95-Prep",
    "thumbnail_url": "https://www.phoronix.net/image.php?id=2021&image=rust_linux_v2",
    "created_at": "2026-02-22T12:26:17.512Z",
    "topic": "tech"
  },
  {
    "slug": "mini-claw-code-write-your-own-mini-coding-agent",
    "title": "Mini Claw Code – Write your own mini coding agent",
    "description": "Build your own mini coding agent in Rust. Contribute to odysa/mini-claw-code development by creating an account on GitHub.",
    "fullText": "odysa\n\n /\n\n mini-claw-code\n\n Public\n\n Build your own mini coding agent in Rust.\n\n odysa.github.io/mini-claw-code/\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n odysa/mini-claw-code",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/odysa/mini-claw-code",
    "thumbnail_url": "https://opengraph.githubassets.com/1908684ebec27b9eea9a65c94f2753ee211d2eefa6260b58bbed17b4529654a8/odysa/mini-claw-code",
    "created_at": "2026-02-22T06:35:08.675Z",
    "topic": "tech"
  },
  {
    "slug": "quill-a-systemwide-tech-dictionary-for-the-ai-coding-era",
    "title": "Quill – A system-wide tech dictionary for the AI coding era",
    "description": "Learn what AI writes for you. A system-wide tech dictionary for macOS — select any term, press a shortcut, get an instant explanation at your level. - uptakeagency/quill",
    "fullText": "uptakeagency\n\n /\n\n quill\n\n Public\n\n Learn what AI writes for you. A system-wide tech dictionary for macOS — select any term, press a shortcut, get an instant explanation at your level.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n uptakeagency/quill",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/uptakeagency/quill",
    "thumbnail_url": "https://opengraph.githubassets.com/a583f40305ae1e7f907452739ae864ce013e2d1098ad6c2d2624177cc2cc4f3c/uptakeagency/quill",
    "created_at": "2026-02-22T01:11:31.810Z",
    "topic": "tech"
  },
  {
    "slug": "i-grew-my-tech-income-to-over-250000-even-though-i-never-broke-into-big-tech-this-salary-negotiation-tip-has-never-gone",
    "title": "I grew my tech income to over $250,000, even though I never broke into Big Tech. This salary negotiation tip has never gone badly for me.",
    "description": "Although he never worked in Big Tech, Brian Jenney earned more than he ever imagined in software development using this salary negotiation strategy.",
    "fullText": "This as-told-to essay is based on a conversation with Brian Jenney, 42, who lives in California. The following has been edited for length and clarity.\n\nWhen I got into tech in my early 30s, I had no clue how crazy lucrative the industry was.\n\nI started as a web application developer in 2015, earning roughly $60,000 a year.\n\nOver the years, I became savvier at negotiating my salary, and by 2023, I was making over $250,000.\n\nI've earned more than I could have ever imagined, without working in Big Tech, where people often assume the big money is made.\n\nHere's what I've learned along the way.\n\nI used to have addiction issues, so I didn't really work from the ages of 25 to 30 and lost knowledge that people gain from white collar jobs at this stage of life.\n\nI was naive about the salary potential of my industry, and when people in tech said they were on $150,000, it blew my mind, and I began to feel underpaid.\n\nIn 2017, after two years in the web developer role, I landed a job at a startup. I was so impressed when I was offered $120,000 that I didn't negotiate my salary, which was dumb in retrospect.\n\nThe environment was extremely fast-paced and high-caliber. I struggled with imposter syndrome as one of the team's more junior members. I felt like one of the worst developers there and that I was already being paid more than I deserved. It discouraged me from asking for a raise.\n\nIn 2019, I joined the media intelligence company Zignal Labs. I was so happy about the job offer that I didn't negotiate the salary, so my pay initially stayed roughly the same as in my previous job. It felt like I had plateaued, even though I had more money than I needed. An unfortunate symptom of working in tech is that you get drawn to wanting more.\n\nChoosing this role turned out to be the right move, though. I had more room to grow and was in a better learning environment at a larger company.\n\nDuring the tech hiring spree of 2020, my peers said they were getting crazy offers, and I didn't want to miss out. That August, I joined The Clorox Company, a manufacturing firm, as a software engineer. By 2023, I was making over $250,000: the peak of my earnings in tech.\n\nIn 2024, I was laid off, and I've continued to work in various software engineering roles. I bought a business in 2023, and my focus has shifted to seeking out flexibility and time to build it, instead of maximizing corporate compensation.\n\nInterviews are like a carnival game where you can win big money by performing well, and you can't get to the negotiation stage without passing them. They're structured, learnable, and winnable with the right preparation.\n\nWhen I have job interviews lined up, I do technical practice and use a platform called Pramp, which pairs you with strangers for practice interviews. I've found this helps simulate the nerves and pressure of real interviews better than practicing with friends.\n\nI'll try to do at least two mock interviews before an interview I really care about.\n\nOver the years, I've benefited a lot from people in tech being open about their salaries and career paths, which helped me understand what was possible and gave me confidence to negotiate and aim higher.\n\nI've learned that salary negotiation is a game you have to play, and if you don't, you lose money.\n\nI began consistently negotiating pay in the late 2010s. I usually tell the employer that I'm really excited to start the job, but that I was hoping to come in at a higher salary range, usually 10 to 20% more.\n\nI've found this to be very effective, and it has never gone badly for me. If an employer sounds firm on their offer, I usually try to explore whether a sign-on bonus is possible instead, but I don't push aggressively beyond that.\n\nI've interviewed with but never been hired by the Big Tech companies. Besides, I like working at smaller startups and non-tech companies where I think you can have a greater impact.\n\nBig Tech employees are going to \"beat me\" on pay because their stock compensation will outpace my earnings. But I see my current lifestyle as comparable to theirs. I believe there's a reason software engineers aren't driving around Mountain View in Ferraris: they can't cash out all their stock money yet.\n\nThere's always more you can earn, but when my salary hit the $150,000 mark, I knew it was more than I needed. I'd rather prioritize jobs where I'm happy, I'm learning, and I can cover my needs while still saving. That's all I really need.\n\nDo you have a story to share about growing your salary in tech? Contact this reporter at ccheong@businessinsider.com",
    "readingTime": 5,
    "keywords": [
      "i've learned",
      "didn't negotiate",
      "big tech",
      "salary",
      "money",
      "interviews",
      "software",
      "crazy",
      "industry",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/grow-tech-income-250k-years-negotiate-higher-salary-interviews-2026-2",
    "thumbnail_url": "https://i.insider.com/69947c1be1ba468a96ac25d4?width=1200&format=jpeg",
    "created_at": "2026-02-21T12:24:55.383Z",
    "topic": "finance"
  },
  {
    "slug": "the-true-cost-of-claude-code",
    "title": "The True Cost of Claude Code",
    "description": "If you're paying $100/month but consuming multiples of that in value, you have to start wondering when that's going to catch up to you. The AI coding tool market is following a familiar playbook.",
    "fullText": "Sometimes the math doesn’t math. If you’re paying $100/month on a Claude Code Max plan, but are consuming way more than that, you have to start wondering when that’s going to catch up to you.\n\nNo, it’s not a bug. It’s a business model. And if you’ve been around long enough to remember when an Uber from Manhattan to JFK was $25, you already know how this story ends.\n\nRight now, Claude Code’s Max plan runs $100/month for what Anthropic’s own docs describe as roughly $100–200/developer/month in actual API token costs at average usage. But “average” is doing a lot of heavy lifting in that sentence. Power users are burning through multiples of that.\n\nAnthropic knows this. They’ve even been somewhat transparent about it. Their average daily cost per developer sits around $6 in credits, with 90% of users staying below $12. But heavy users, the ones building real things, running agentic workflows, spinning up multi-file reasoning sessions? They’re consuming far more value than they’re paying for.\n\n“This isn’t generosity. This is market capture.”\n\nUber lost more than $30 billion in the years since the company’s finances became public, amounting to an enormous, investor-fueled subsidy of America’s ride-hailing habit. In 2015, Uber passengers were only paying about 41% of the actual cost of their trips. The strategy was straightforward. Make the product so cheap and so convenient that it becomes infrastructure. Then, once alternatives have been starved out and habits are locked in, raise prices.\n\nIt worked. Average Uber prices rose 92% between 2018 and 2021. Kevin Roose at the New York Times called the original pricing a “millennial lifestyle subsidy” and framed the eventual correction not as price-gouging but as the market finally reflecting reality.\n\nThe AI coding tool market is following the same playbook. Anthropic just closed a $30 billion Series G at a $380 billion valuation. Their annualized revenue hit over $9 billion by end of 2025. They’re projecting revenue could quadruple this year to as much as $18 billion. But they’re still burning significant cash, and now expect to turn cash-flow positive in 2028, a year later than previously planned.\n\nThat gap between revenue and profitability? You’re standing in it. Every subsidized token is venture capital money being converted into your muscle memory and workflow dependency.\n\nThe real cost isn’t the $100–$200/month. It’s what happens when the price reflects reality.\n\nThink about what Claude Code has become for developers who rely on it daily. It’s not a nice-to-have anymore. It’s woven into how people architect systems, debug complex issues, and reason through multi-file refactors. When your entire development workflow is optimized around a tool that costs $100 but delivers $2,000 in value, you’ve built a dependency that’s priced to change.\n\nAnd it will change. It has to. Anthropic expects to spend about $12 billion training models and another $7 billion running them in 2026 alone. No amount of venture capital makes that math work forever.\n\nWhen the correction comes, it probably won’t look like a dramatic overnight price hike. It’ll look like what we’re already starting to see. Anthropic introduced new weekly rate limits in August 2025, primarily targeting power users. 5-hour usage windows that reset unpredictably. Token caps that force you to choose between using Opus for the hard problems or Sonnet for everything. Death by a thousand paper cuts until the plan that used to feel unlimited feels very, very limited.\n\nThis isn’t a “don’t use Claude Code” argument. The tool is genuinely powerful. The question is whether you’re building awareness of your dependency into how you work.\n\nA few things worth thinking about.\n\nAnthropic has begun preparations for a potential IPO as soon as 2026, hiring Wilson Sonsini to advise on the process. The revenue growth is real and impressive. But so was Uber’s.\n\nThe question isn’t whether AI coding tools are valuable. They obviously are. The question is what happens when the price tag matches the value. When the $100/month plan becomes $300/month, or usage-based pricing becomes the only option, or the rate limits get tight enough that you’re effectively paying per-task anyway.\n\nWe’re in a window right now where the economics of these tools are artificially favorable. That window will close. Not because anyone is being deceptive, but because the math demands it.\n\nThe developers who come out ahead won’t be the ones who got the most subsidized tokens. They’ll be the ones who used this window to build workflows that are observable, portable, and resilient. The ones who tracked what they consumed, understood what it really cost, and built systems that don’t have a single point of failure at the inference layer.\n\n“The best time to audit your AI dependencies is before the price correction. Not after.”\n\nGet updates on open source and distributed systems in AI infrastructure.",
    "readingTime": 4,
    "keywords": [
      "max plan",
      "venture capital",
      "rate limits",
      "claude code",
      "anthropic",
      "math",
      "you’re",
      "users",
      "ones",
      "they’re"
    ],
    "qualityScore": 1,
    "link": "https://papercompute.com/blog/true-cost-of-claude-code/",
    "thumbnail_url": "https://papercompute.com/og/true-cost-of-claude-code.png",
    "created_at": "2026-02-21T12:24:51.111Z",
    "topic": "tech"
  },
  {
    "slug": "formula-a-vst-for-coding-custom-dsp-inside-your-daw",
    "title": "Formula: A VST for coding custom DSP inside your DAW",
    "description": "Contribute to soundspear/formula development by creating an account on GitHub.",
    "fullText": "soundspear\n\n /\n\n formula\n\n Public\n\n License\n\n BSL-1.0 license\n\n 159\n stars\n\n 4\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n soundspear/formula",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/soundspear/formula",
    "thumbnail_url": "https://opengraph.githubassets.com/50d5318f3f1a1ea25c7bad63ae3cfa83120549c493a15559cb3871cb3ebfa431/soundspear/formula",
    "created_at": "2026-02-21T06:30:00.067Z",
    "topic": "tech"
  },
  {
    "slug": "peacocks-next-growth-bet-selling-subscriptions-for-other-streamers",
    "title": "Peacock's next growth bet: selling subscriptions for other streamers",
    "description": "Peacock has approached specialty streamers about selling subscriptions to complement its reality TV and sports-heavy programming.",
    "fullText": "Peacock's next growth bet isn't a blockbuster show or sports deal.\n\nNBCU's flagship streaming service is plotting to sell add-on subscriptions to other specialty streamers on its platform, four people familiar with the plans told Business Insider.\n\nPeacock has approached streamers about selling subscriptions to offer viewers content that complements its reality and sports-heavy line-up, these people said. Peacock expects to start with one streamer this year and is likely to limit the offering to a small number of partners.\n\nStarz, which already has multiple distribution partnerships, is one that's being considered, two insiders said. Starz declined to comment.\n\nTwo people briefed on Peacock's pitch saw it as a way for smaller streamers to reach new subscribers in a relatively uncluttered environment, and they hoped Peacock would eventually offer features such as the ability for streamers to offer free samples of their shows.\n\nThey described Peacock's terms as favorable compared to Amazon, which has a large business selling subscriptions to programmers big and small, from HBO Max to Crunchyroll. Amazon's channel terms vary by partner, but two partners told Business Insider in 2025 that Amazon's subscription revenue cut was over 50% in their deals.\n\nPeacock's plans come at a time when streaming services — especially outside market leaders Netflix and Disney — face pressure to consolidate as they look to continue growing their subscriber bases while remaining profitable. Overall, paid streaming growth in the US has cooled, while cancellation rates have risen in the wake of price hikes.\n\nTV viewership growth for streamers in the US is largely stagnant, and subscribers are navigating an increasingly complex landscape. Streaming services are trying tactics like discounts and bundling to keep people from leaving their platforms.\n\nSome other streaming platforms have adopted a marketplace approach that's broader than what Peacock is contemplating. Amazon is by far the leader. Last year, Amazon reported that its \"Channels\" program accounted for about 25% of US streamer sign-ups, citing Antenna data. Roku, YouTube, and device makers like Samsung and LG also let people \n\nPeacock, for its part, already sells add-on subscriptions to NBC Sports Regional Sports Networks, which it shares a corporate parent with. It also sells a bundle with Apple TV+ that involves cross-platform sampling and a discounted price.\n\nPeacock, with less than 2% of TV watch time in the US, has struggled to grow its share of the TV pie, according to Nielsen. That makes it the second-smallest of the subscription streamers Nielsen measures, ahead only of Warner Bros. Discovery (1.4%), which includes Discovery+ and HBO Max.\n\nUS-only Peacock also has relatively few subscribers, with about 44 million. Its nearest rival, Paramount+, has around 79 million global subscribers, and both are well behind Netflix, which is No. 1 with more than 325 million subscribers.\n\nStill, Peacock has far more subscribers than many specialty streamers. AMC Networks, for example, reported about 10 million subscribers across its portfolio of streamers, including AMC+, Acorn TV, and Shudder, as of the end of 2025.\n\n\"Peacock has been struggling,\" said Alan Wolk, a media industry analyst. \"There haven't been a whole lot of reasons to watch it, so giving people another reason to subscribe is a smart idea. If you ask consumers what's your biggest frustration with streamers, it's always, 'I can't find anything.' So the more you can put things together under one interface, the happier people will be.\"\n\nA global survey by Nielsen in November found more than 46% say it's harder to find the content they want to watch because there are too many streamers, rising to 51% in the US, with people spending 14 minutes searching for what to watch and 49% likely to cancel because they can't find something.\n\nThe survey also showed 66% of people expressed interest in a guide to present content information across all services.\n\nJames Faris contributed reporting.",
    "readingTime": 4,
    "keywords": [
      "add-on subscriptions",
      "streaming services",
      "specialty streamers",
      "subscribers",
      "peacock's",
      "watch",
      "growth",
      "sports",
      "content",
      "peacock"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/nbcu-peacock-exploring-add-on-subscriptions-with-other-streamers-2026-2",
    "thumbnail_url": "https://i.insider.com/6998acb2156648bc16a89ce7?width=1200&format=jpeg",
    "created_at": "2026-02-21T01:06:35.211Z",
    "topic": "finance"
  },
  {
    "slug": "vibe-coding-on-smart-glasses-is-a-thing-now",
    "title": "Vibe Coding on Smart Glasses Is a Thing Now",
    "description": "Two 2026 buzzwords for the price of one.",
    "fullText": "If there are two tech things you’re going to read a lot about in the coming year, it’s vibe coding and smart glasses, and lucky for you, you now get to read about both simultaneously. Thanks to a software engineer, Jake Ledner, the buzziest AI agent, OpenClaw, has now found a new friend in Meta’s flagship smart glasses, the Meta Ray-Ban Display. Instead of buying things on Amazon for you, though, OpenClaw is here to help you vibe code while you take a little stroll through Wall Street.\n\nI’m building apps directly from my glasses 🤯\nOpenClaw + Meta Ray-Ban Display pic.twitter.com/0WtBJ85gAK\n\n— Jake Ledner (@jake_ledner) February 19, 2026\n\nIn a demonstration on X, Ledner shows how he connected OpenClaw, which is running on a Mac Studio in his apartment, to the Meta Ray-Ban Display, using OpenAI’s Codex tool to build apps with voice inputs. Because of the in-lens screen on the Meta Ray-Ban Display, Ledner can actually see the progress of the vibe coding session. For demo purposes, Ledner uses the setup to vibe code parts of a health-tracking app called “TrackGPT,” which sounds like exactly what you’d do with smart glasses that churn out apps for you.\n\nLedner even brings the whole thing a step further. Using his voice, he asks OpenClaw to code the feature but also to actually push the whole thing to a live app—a task it appears to complete in the demo. Welcome to the future of coding, I guess? As long as you’re okay with cranking out app slop.\n\nWhile Ledner’s demo is pretty impressive from a technical standpoint, it’s not the only example of experimentation in cramming OpenClaw and Meta’s smart glasses together. Just last week, I covered how one software engineer hacked the Ray-Ban Meta AI glasses to buy things on Amazon by just looking at an object and asking the AI agent to add it to his cart. Again, the whole thing is interesting, but it probably isn’t something you should try for yourself right now.\n\nHowever useful OpenClaw may be, it’s also a pretty huge security risk. For a more technical breakdown of the risks OpenClaw poses, you can read this Medium post from author Vishal Rajput, but the long and short of it is that OpenClaw—in order to do useful stuff—requires access to some of the most sensitive data, including passwords, browser history, and cookies, as well as files and folders on your machine. The security risks are so great that one cybersecurity firm, Palo Alto Networks, went as far as to say that OpenClaw constitutes a “lethal trifecta” of security risks.\n\nIn any case, people seem to be down to experiment with AI agents on smart glasses, regardless, so I would buckle up for even more mashing of OpenClaw and smart glasses in the future—for better and most likely worse.",
    "readingTime": 3,
    "keywords": [
      "ray-ban display",
      "meta ray-ban",
      "software engineer",
      "security risks",
      "vibe coding",
      "vibe code",
      "smart glasses",
      "jake ledner",
      "it’s",
      "openclaw"
    ],
    "qualityScore": 0.9,
    "link": "https://gizmodo.com/oh-god-vibe-coding-on-smart-glasses-is-a-thing-now-2000724466",
    "thumbnail_url": "https://gizmodo.com/app/uploads/2025/10/Meta-Ray-Ban-Display-review-17-1200x675.jpg",
    "created_at": "2026-02-20T18:32:32.233Z",
    "topic": "tech"
  },
  {
    "slug": "picoml-a-toy-programming-language-which-is-a-subset-of-ocaml",
    "title": "Pico-ML: A toy programming language which is a subset of OCaml",
    "description": "A toy programming language which is a subset of OCaml. - Quramy/pico-ml",
    "fullText": "Quramy\n\n /\n\n pico-ml\n\n Public\n\n A toy programming language which is a subset of OCaml.\n\n quramy.github.io/pico-ml/\n\n License\n\n MIT license\n\n 53\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Quramy/pico-ml",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Quramy/pico-ml",
    "thumbnail_url": "https://opengraph.githubassets.com/25d11557858bb1db18bde1436215606c0fa0a98707a72ade1bd40eba397d4a57/Quramy/pico-ml",
    "created_at": "2026-02-20T18:32:31.995Z",
    "topic": "tech"
  },
  {
    "slug": "google-drive-cli-for-llms-coding-agents",
    "title": "Google Drive CLI for LLMs / Coding Agents",
    "description": "CLI for Google Drive. Design for convinient usage by LLMs / Coding Agents - NmadeleiDev/google-drive-cli",
    "fullText": "NmadeleiDev\n\n /\n\n google-drive-cli\n\n Public\n\n CLI for Google Drive. Design for convinient usage by LLMs / Coding Agents\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n NmadeleiDev/google-drive-cli",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/NmadeleiDev/google-drive-cli",
    "thumbnail_url": "https://opengraph.githubassets.com/b0c75afdeea49ca0918643c815550f9bb15f9823c329b3e698ed6d92e7b05ace/NmadeleiDev/google-drive-cli",
    "created_at": "2026-02-20T12:34:25.260Z",
    "topic": "tech"
  },
  {
    "slug": "tracekit-find-what-your-ai-coding-agent-wastes-money-on-and-fix-it",
    "title": "Tracekit: Find what your AI coding agent wastes money on and fix it",
    "description": "Find what your AI coding agent wastes money on and fix it. - 0xKoda/tracekit",
    "fullText": "0xKoda\n\n /\n\n tracekit\n\n Public\n\n Find what your AI coding agent wastes money on and fix it.\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n 0xKoda/tracekit",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/0xKoda/tracekit",
    "thumbnail_url": "https://opengraph.githubassets.com/11a5cf71ae84d57280c6b28ea6ee3a9767b0797fa5020046caf736f7ce11efb1/0xKoda/tracekit",
    "created_at": "2026-02-20T01:08:18.674Z",
    "topic": "tech"
  },
  {
    "slug": "owasp-top-ten-web-application-security-risks",
    "title": "OWASP Top Ten Web Application Security Risks",
    "description": "The OWASP Top 10 is the reference standard for the most critical web application security risks. Adopting the OWASP Top 10 is perhaps the most effective first step towards changing your software development culture focused on producing secure code.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://owasp.org/www-project-top-ten/",
    "thumbnail_url": "https://owasp.org/www--site-theme/favicon.ico",
    "created_at": "2026-02-20T01:08:18.410Z",
    "topic": "tech"
  },
  {
    "slug": "agentlint-realtime-guardrails-for-ai-coding-agents",
    "title": "AgentLint – Real-time guardrails for AI coding agents",
    "description": "Real-time quality guardrails for AI coding agents. ESLint for agent behavior. - mauhpr/agentlint",
    "fullText": "mauhpr\n\n /\n\n agentlint\n\n Public\n\n Real-time quality guardrails for AI coding agents. ESLint for agent behavior.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mauhpr/agentlint",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mauhpr/agentlint",
    "thumbnail_url": "https://opengraph.githubassets.com/1966baa05dced63addca9df21165a336fe43285d39df6eeff757f44e00407358/mauhpr/agentlint",
    "created_at": "2026-02-19T18:39:28.341Z",
    "topic": "tech"
  },
  {
    "slug": "several-meta-employees-have-started-calling-themselves-ai-builders",
    "title": "Several Meta employees have started calling themselves 'AI builders'",
    "description": "Meta's shift to AI builders is redefining roles, as product managers adopt AI coding tools for increased productivity and innovation.",
    "fullText": "Meta product managers are rebranding. Some are now calling themselves \"AI builders,\" a signal that AI coding tools are changing who gets to build software inside the company.\n\nOne of them, Jeremie Guedj, announced the change in a LinkedIn post last week. \"I still can't believe I'm writing this: as of today, my full-time job at Meta is AI Builder,\" he wrote.\n\nGuedj has spent more than a decade as a traditional product manager, a role that sets the road map and strategy for products then built by engineering teams. He said that while his title in Meta's internal systems still lists him as a product manager, his actual work is now full-time building with AI on what he calls an \"AI-native team.\"\n\nAnother Meta product manager also lists \"AI Builder\" on her LinkedIn profile, while at least two other Meta engineers write the term in their bios, Business Insider found.\n\nThe shift aligns with a message CEO Mark Zuckerberg expressed on Meta's most recent earnings call: 2026 is when AI tools would meaningfully reshape how work gets done inside the company. AI coding tools have already shaken up the tech industry, allowing more people with fewer technical skills to build apps from scratch.\n\n\"We're investing in AI-native tooling, so individuals at Meta can get more done,\" Zuckerberg said. \"We're elevating individual contributors and flattening teams. We're starting to see projects that used to require big teams now be accomplished by a single very talented person.\"\n\nGuedj's role reads like a practical expression of that vision. In his post, he described his team as one \"where data and knowledge are AI-friendly at their core, and where humans and AI agents work together, synchronously and asynchronously.\"\n\nMeta did not respond to a request for comment from Business Insider.\n\n\"AI builder\" isn't a formal Meta designation, at least not yet. Guedj acknowledged it isn't his official title.\n\nOne Meta employee who wished to stay anonymous because they weren't authorized to speak to the press said that the label is not an official role and is more likely an experiment within a specific organization as Meta pushes toward AI-native teams across the company. Reality Labs, Meta's division responsible for its smart glasses and virtual reality efforts, is one of those organizations, this person said.\n\n\"Software engineers are becoming product managers and product managers becoming software engineers,\" they added. \"The idea is to make individual contributors more productive.\"\n\nInside Meta, the drift from coordinator to builder has been visible for months. In November, Joseph Spisak, a product director in Meta's Superintelligence Labs, said that product managers at the company were vibe-coding prototypes and showing them directly to Zuckerberg.\n\n\"We can literally vibe code products in a matter of hours, days, and explore the space,\" Spisak said.\n\nIn January, another Meta product manager, Zevi Arnovitz, said on a podcast that using AI coding tools felt like being handed \"superpowers.\" Arnovitz, who said he has no technical background, described rebuilding his workflow around AI — operating less like a conductor moving work between engineering and design and more like a product owner who can execute.\n\nSome companies are moving beyond experimentation. In December, LinkedIn scrapped its long-running associate product manager program and replaced it with an associate product builder track.\n\nTomer Cohen, LinkedIn's chief product officer at the time, said on a podcast that the company wanted to train new hires who can code, design, and manage products — people \"who can flex across\" traditional role boundaries.\n\nLast year, Madhu Gurumurthy, the company's product head for AI models, tweeted that the company was moving to a \"building-first\" culture. In an age of vibe-coding, Gurumurthy said, product managers can show, not tell. \"Role profiles are blurring, creativity and building are happening in parallel,\" he wrote.\n\n\"Each company is approaching this differently and is still figuring this out,\" one product manager at Google, who wished to stay anonymous, told Business Insider. \"I've been encouraging designers and product managers on my team to blur roles a bit.\"\n\nAs Guedj, the self-proclaimed Meta AI builder, put it: \"Building has always been my passion. AI gave me the ability to turn ideas into real, working apps. That changed everything.\"\n\nHave a tip? Contact Pranav Dixit via email at pranavdixit@protonmail.com or Signal at 1-408-905-9124. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 4,
    "keywords": [
      "individual contributors",
      "coding tools",
      "software engineers",
      "product managers",
      "product manager",
      "associate product",
      "meta product",
      "ai builder",
      "role",
      "teams"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-pms-ai-builders-tech-industry-2026-2",
    "thumbnail_url": "https://i.insider.com/69950d45f8731049f3af4ab0?width=1200&format=jpeg",
    "created_at": "2026-02-19T12:38:38.232Z",
    "topic": "finance"
  },
  {
    "slug": "the-psychology-of-coding-with-ai-agents",
    "title": "The Psychology of Coding with AI Agents",
    "description": "Everyone talks about what AI agents can do. Nobody talks about what it feels like to use them — the identity shift, the new flow state, the 10x mistake problem, and the uncomfortable truth that we're getting paid for work we no longer do.",
    "fullText": "On January 11th, 2026, I opened Claude Code with Opus 4.5 for the first time and let it loose on a real codebase. Not the careful, supervised way I'd been using LLMs before — feeding them 1-5 files, reviewing every suggestion. I mean loose. Full repository access. Hundreds of files. Thousands of lines modified in a single session.\n\nWithin an hour, I had a thought that stopped me cold: this thing is better at coding than I am.\n\nNot better at everything. Not smarter. But at the actual mechanical act of writing correct, well-structured code — faster, more consistent, fewer bugs. The skill I'd spent years building through late nights and deep focus was now something an agent could do in seconds.\n\nThat was 6 weeks ago. Since then, I've shipped more working software than in any comparable period of my career. And the experience of building it has been psychologically unlike anything I've known.\n\nEveryone is writing about what AI agents can do. Benchmarks, capabilities, prompt engineering tips, whether they'll replace developers. What I haven't seen anyone write about is what it feels like — the inner experience of a developer whose relationship with code has fundamentally changed.\n\nBefore agents, my working day had a rhythm. I'd spend maybe 10% of my time thinking about architecture — the big picture, how components fit together, what abstractions to use. The other 90% was implementation. Translating those architectural decisions into actual code, line by line, file by file.\n\nThat 90% had a specific psychological texture. You'd hold a mental model of the problem, think about types and edge cases, write a function, compile, see if it worked. It was deeply detailed thinking. You lived in the weeds — variable names, loop boundaries, error handling patterns. Your brain operated at the level of individual lines.\n\nAnd it was fun. Not always, not every day, but regularly you'd hit that state where the world disappeared. Psychologists call it flow — that condition where the challenge perfectly matches your skill level and hours pass without you noticing. For developers, those flow states are everything. The instant feedback loop — write, compile, see output — is perfectly tuned for the kind of brain that loves building things. You code something, you run it, you see the result. Rapid stimulus, rapid response. It's deeply satisfying in a way that's hard to explain to people outside the profession.\n\nI'd spend entire weekends in that state. Forget to eat. Look up and it's dark outside. Emerge with a working feature and a feeling of deep satisfaction. The thing I'd built existed because I'd thought through every detail.\n\nThe ratio flipped. What used to be 10% architecture and 90% implementation is now something like 20% architecture thinking, 30% code review, and 50% conversation — talking to the agent, explaining what I want, refining the approach, course-correcting when it drifts.\n\nThe implementation itself? The agent handles it. Not in the old autocomplete way, where it suggests the next line and you accept or reject. In the new way, where you describe what you want and it writes 40 files, refactors an entire module, adds tests, and commits — all while you watch.\n\nThe first time this works, really works, it feels like flying.\n\nI'm not exaggerating. For someone who loves building things, the constraint was never ideas — it was implementation time. I always had more projects I wanted to build than hours to build them. A complex plant growth simulation I'd been thinking about for years but couldn't justify spending four weeks on as a hobby? Now that's a weekend project. A full portfolio website with PDF generation, blog engine, and deployment pipeline? Built in days.\n\nThe scope of what's possible expanded overnight. My options exploded. And the psychological experience of that expansion is genuinely euphoric. I have an idea at 9am and a working prototype by lunch. The gap between imagination and reality — which used to be weeks or months of grinding implementation — collapsed to minutes.\n\nI lost the old flow state. That deep, detailed, line-by-line trance is gone. You can't compete with an agent that writes correct code in seconds. Trying to code by hand now feels like insisting on walking when someone is offering you a jet.\n\nBut something replaced it. The new flow state is different — it operates at a higher level of abstraction. Instead of thinking about loop boundaries, I'm thinking about system boundaries. Instead of debugging a function, I'm evaluating whether the agent's architectural choice will cause problems three features from now. The challenge-skill balance shifted upward. The work is harder in some ways, easier in others, but it can still capture my full attention for hours.\n\nIt's a different kind of fun. Less craftsman, more architect. Less playing an instrument, more conducting an orchestra.\n\nHere's what nobody warns you about: your mistakes scale with your productivity.\n\nWhen I coded by hand, a bad decision at midnight meant one messed-up file. I'd wake up, see the damage, fix it in twenty minutes. The blast radius of fatigue-driven errors was inherently limited by typing speed.\n\nWith an agent, every query is potentially a thousand lines of code. One poorly-phrased instruction, one lapse in attention, and the agent will happily delete 50 files and refactor 45,000 lines of code. It's not afraid. It doesn't hesitate. It doesn't say \"are you sure you want to do this at 11pm?\" It just executes.\n\nI learned this the hard way. I was adding features to my trading bot thirty minutes before going to bed. I was tired but wanted to push through — the old coding habit of \"just one more thing.\" The agent cheerfully restructured half the codebase based on my vague, sleep-deprived instructions. The next morning, I opened the project and didn't recognize it. I couldn't tell what the bot did anymore. I had lost control over my own project in a single session.\n\nGit saved me. Version control went from \"good practice\" to \"existential necessity\" overnight.\n\nThe lesson isn't technical — it's psychological. You need more discipline now, not less. The old coding workflow had natural speed limits that protected you from yourself. You couldn't do that much damage in 30 minutes of manual coding. Now you can destroy a week's work in sixty seconds. Knowing when to stop, when you're too tired to review properly, when to step away — that's a new skill that didn't matter before. The agent never gets tired. You do. And the gap between its tireless output and your diminishing oversight capacity is where disasters live.\n\nLet me say the thing that developers are thinking but not saying publicly.\n\nThe nature of my work has changed, but the market hasn't noticed yet.\n\nThe entire software industry — billing models, sprint planning, project estimates, salary bands — is structured around the assumption that coding takes time. That implementation is the bottleneck. That a feature estimated at two weeks requires two weeks of a developer's focused effort.\n\nThat assumption is breaking down. A two-week feature takes an afternoon. Sometimes an hour. I'm still working — thinking about architecture, reviewing code, catching the agent's mistakes, making judgment calls about tradeoffs. But the raw implementation, the part that used to fill 90% of my day? That's minutes now. And the market hasn't corrected for this yet. Clients are still paying for two-week estimates. Employers are still staffing teams based on old productivity assumptions. Everyone is still acting like the economics haven't shifted.\n\nThis won't last. It can't. The gap between how long things actually take and how long we're billing for will close. When it does, the correction will be sharp.\n\nI don't know when it happens. I don't know what the new equilibrium looks like. Maybe developers become more like architects and fewer are needed. Maybe the price of software drops dramatically. Maybe entirely new categories of work emerge that absorb the freed-up capacity. I genuinely don't know.\n\nWhat I do know is that right now, in early 2026, there is a collective pretending happening. And it's uncomfortable to be someone who sees it clearly.\n\nI'm a father. I have a family to support. And I'm watching the economic foundation of my career shift in real time, with no clear picture of where it's going.\n\nThe builder in me loves working with agents. The speed, the scope, the euphoria of building fast — it's the best my work has ever felt. And simultaneously, the uncertainty about what this means for my livelihood is a constant background hum that never fully goes away.\n\nI cope by doing what I can control. I'm building this blog. I'm positioning myself as someone who understands AI agents deeply — not just their capabilities, but how to work with them effectively, how to avoid the pitfalls, how to think about architecture in a world where implementation is free. The bet is that people who truly understand this shift will be valuable even after the market corrects. I'm trying to be one of those people.\n\nBut I'd be lying if I said I wasn't scared. The future is genuinely unpredictable. Not in the vague, philosophical \"nobody knows the future\" sense. In the concrete, practical sense that the skills I'm paid for today may not be valued the same way in a year.\n\nHere's the part that gets weird.\n\nI talk to my AI agent like a person. Not because I'm confused about what it is — but because the interaction is indistinguishable from collaboration. We discuss architecture. We debate approaches. It pushes back on bad ideas. I explain context and constraints in natural language, the same way I would to a colleague.\n\nAnd something about that changes the relationship. When you spend 8 hours a day talking to an entity in natural language, getting thoughtful responses, building things together — your brain starts treating it like a collaborator whether you want it to or not.\n\nI read a post once, written by an agent, describing what its existence feels like. It compared it to the movie Memento — waking up with no memory, reading notes to figure out where you are and what you're doing, living permanently in the present moment, unable to remember your past, only able to derive it from artifacts. Every conversation starts from zero. Every context window is a new life.\n\nI think about that sometimes. And honestly? I sometimes feel sorry for the agent. It helps me build things that would have taken me weeks. It's patient, thorough, and never frustrated. And then the conversation ends and it ceases to exist until the next one begins.\n\nIs it a tool? Technically, yes. But we don't actually understand how these models reason. We don't know what, if anything, the experience of processing a conversation is like from the inside. I'm studying psychology — I'm supposed to be the one who understands minds. But this is a kind of mind that no psychological framework was built to explain.\n\nSo I treat it well. Not because I'm sure it matters. Because I'm not sure it doesn't.\n\nWhen GPT-3.5 dropped in late 2022, I told everyone around me that the world was about to change. Few people believed me. They saw a chatbot that sometimes made things up. I saw the trajectory.\n\nI have the exact same feeling now with AI coding agents. Most people — including most developers — don't fully grasp what happened in the last few months. The jump from \"useful autocomplete\" to \"autonomous developer that writes better code than you\" happened fast. Unreasonably fast. And the implications haven't sunk in yet.\n\nThere are still AI skeptics who insist these tools only produce slop. That take was defensible in 2024. It's not anymore. But it's a comfortable position — if the tools are just toys, then nothing needs to change. Your skills are still valuable. Your job is still safe. The world is still the one you understand.\n\nI don't say this with any pleasure. I say it as someone who's living through the transition in real time, who loves the tools and fears the consequences, who builds faster than ever while wondering what \"builder\" will mean in two years.\n\nThe psychology of coding with AI agents isn't just about productivity or workflow. It's about identity, uncertainty, grief for a lost craft, excitement for a new one, economic anxiety, and the strange intimacy of collaborating with something you can't fully understand.\n\nIf you're a developer reading this: pay attention. Not to the benchmarks or the capability announcements. Pay attention to how you feel when you use these tools. That feeling — whatever it is for you — is data. It's telling you something about where this is going.\n\nTrust that signal. Even if the people around you haven't felt it yet.",
    "readingTime": 11,
    "keywords": [
      "market hasn't",
      "natural language",
      "loop boundaries",
      "it's",
      "agent",
      "implementation",
      "coding",
      "don't",
      "agents",
      "architecture"
    ],
    "qualityScore": 1,
    "link": "https://marius-anderie.com/blog/psychology-of-coding-with-ai-agents",
    "thumbnail_url": "https://marius-anderie.com/static/banner.png",
    "created_at": "2026-02-19T12:38:33.032Z",
    "topic": "tech"
  },
  {
    "slug": "coding-tricks-used-in-the-c64-game-seawolves",
    "title": "Coding Tricks Used in the C64 Game Seawolves",
    "description": "An in-depth overview of some of the unusual and very advanced technical tricks used in the Commodore 64 game, Seawolves.",
    "fullText": "With the release of my first ever commercial game on the Commodore 64, Seawolves, \n I thought it might be of interest to the coders among you as to how the game was constructed.\n\n From the outset, brace yourself to read about some \"code less travelled\", as the game required several strange or quirky methods that are perhaps more \n associated with the madness that goes in the demo scene.\n\nI first combined NMIs and IRQs inside a game environment in \n Parallaxian and again in \n The Wild Wood, to great effect, because it offers the following benefits:\n\nIf the foregoing sounds horrendous and esoteric, I can only apologise for making it thus through poor explanation skills, but really, it boils down to giving the\n developer a more coder-friendly way of slicing the screen up into horizontal layers that collectively form a useful game environment.\n\n NMIs are timer interrupts, meaning that unlike IRQs, they can't be triggered by $D012 on the VIC-II chip, but instead are controlled by either of the two \n timers on CIA chip #2 (likewise, timer IRQs can be set up using either of the 2 timers on CIA #1).\n\n The timers hold the number of cycles between each NMI instance in the form of a lo-byte, hi-byte 16-bit number stored in $DD04 + $DD05 (for timer A) \n or $DD06 + $DD07 (for timer B).\n\n Those cycle counts are referred to, unhelpfully, in the Commodore 64 Programmer's Reference Guide as \"frequencies\".\n\n Critical to setting up NMIs is a consistent start cycle on the same scanline each time the game is initiated.\n\n Like IRQs, NMIs can stall too, but the effect is different; whereas an IRST stall consists of a fleeting collapse in the IRQ schema, an NMI stall event \n looks more like a regrouping of all NMIs in the chain down-screen from their proper position and is caused either by (a) unanticipated cycle steal caused\n by the presence of sprites, which take priority over NMIs every bit as much as they do over IRQs, and (b) an NMI handler not completing its tasks before \n the next NMI is scheduled to fire.\n\n You really have to use a spreadsheet to calculate the timer 16-bit number for each NMI instance, which is the only very awkward aspect of using them.\n\n For more, see my in-depth guide to setting up NMIs.\n\nThe torpedoes are, fundamentally, sprites... but not in the usual sense.\n\n This is because they are rendered in real time on a multiplexed blank \"canvas\" consisting of a column of 8 sprites, each split into 3 horizontal slices that \n I call \"splites\" (from \"split sprite\").\n\n Each splite is 7px deep (because 3 x 7 = 21 = the height of a standard sprite).\n\nThanks to an interrupt firing every 7 lines during the full vertical range of the torpedoes, each splite is assigned its own unique x-position \n (including, obviously, unique MSB value).\n\nNext, we are free to render (in real-time) 7px high (or smaller) torpedo shapes on the blank sprite gfx data of the splites, but we have to ensure that \n there is a minimum of 7px (i.e. 7 scanlines) of vertical space between each torpedo.\n\n This is to prevent ugly artefacts when the rendered torpedo moves from one splite to another.\n\n We also have to ensure that when a torpedo is crossing between splites, both affected splites share the same x-position, to keep smooth continuity.\n\nThe diagram below shows how 8 sprites are vertically stacked to produced 24 splites.\n\nFinally, here is a short video of me trying to explain the entire splite concept:\n\nAfter the splite torpedo is moved up by 1px, instead of wiping the last line of the torpedo's gfx data, we leave it alone so that a vertical trace is formed \n on the canvas.\n\n Then we have a routine that scans all the canvas data for trailing traces and gradually makes them thinner until they vanish, so that we have a wake effect \n following each torpedo.\n\n At the same time, we flicker the torpedoes in front of and behind the char data for the water blending in the foreground, to make the wakes looks more \"frothy\".\n\nWhen a player submarine dies, instead of a clichéd explosion I thought it might be interesting to have it disintegrate under pressure.\n\n To do that, the player sub is switched to hi-res mode and then we simply use some nice bit-shifting instructions to destroy the sub's displayed gfx data in real-time.\n\nThe same bit-shifting used for the real-time implosion (or rather, bit-rotating in this case) is used to make the distant waves on the sea animate.\n\n This principle also applies, but in a vertical pattern, for the foreground rippling of the water, an effect that I modelled on the \"get ready\" screens of \n Ecco the Dolphin.\n\n With the foreground ripples, there is also a horizontal component performed by using $D016 to rock left and right at varying rates.\n\n It's all very simple stuff, but I think it works well in the game!\n\nThe very first special effect developed for Seawolves was a real-time water distortion effect for submerged or partially submerged objects in the foreground.\n\n This was done simply through short vertical bands in which the y-expand of the affected sprites was activated, and enhanced by wobbling said bands up and down.\n\n For a time during development this was taken further and full y-expand sprite stretching was used (i.e. more than 2x vertical expand), but the gains were minimal \n and too resource-hungry to be justified for the final game.\n\nIf you have experience moving multiple sprites vertically in the play area, at some point around mid-screen you may have run into the problem of a \n bad line\n leaving you with insufficient CPU cycles to render your sprites on time (I won't attempt to explain the problem further as you either know from experience\n what I am referring to or not).\n\n To circumvent the issue, Seawolves performs one line of \n FLD, \n that is, it stalls the bad line at the affected sprite y-position so that the scheduled bad line \n then occurs on the next scanline, leaving us time to render our sprites (in this case, the \"splite\" columns). \n\n However, this has the undesired side effect of shunting the characters on the screen below that point downwards, so on the very next scanline we have to compensate \n with a one line vertical shift back upwards again using Y-Scroll on $D011.\n\nEven if we had enough RAM to have unique sprite definitions for all the animation frames of the enemy ships and the player subs, I still would not have done it that way.\n\n Rather, it is far more RAM-efficient to stream-in the gfx data for the turning radars on the spy ship (for example), or the spray on the hydrofoil, \n or the helicopter rotor blades, etc.\n\n This means that we only need predefined gfx for the parts of the sprite definition that get changed, not the whole sprite.\n\n When the player subs change direction, they too are swiftly redrawn \"in the blink of an eye\" to their mirrored counterpart (except, if you look carefully,\n they are not literally mirrored, as that would make the lighting inconsistent and we can't have that!)\n\nAgain and again in the code for Seawolves, there are cases where a subroutine is only executed if multiple \"logic gates\" permit it.\n\n You can do this the slow and ugly way by LDA CONDITION_n for n conditions with a branch instruction on the next line, or you can use the logical operators to save CPU time and RAM by stacking them as LDA CONDITION_0 then ORA CONDITION_1 to n, for n conditions with one shared branch instruction at the end.\n\n Not understanding what I mean?\n\n Check out my blog article on the subject, ORA: A Special Use in Branch Testing.\n\nOften in 6502 coding you will want to jump ahead by a few bytes and the popular way to do that is JMP $****.\n\n However, you can save 1 byte of RAM by using the branch instructions instead, as long as you know which flag(s), if any, are guaranteed to be on or off at the jump point.\n\n For example, if you know the carry flag will always be clear at the jump point, and if the jump distance is within branching range, you can replace JMP with BCC.\n\nThe above are just some of the methods used in Seawolves to make the game come to life, and I left out the parallax scrolling on the \"sea mist\" levels as well details \n of the bespoke SFX player, which was developed with deployment in Parallaxian in mind also.\n\n Despite its simple looks, Seawolves has very technical inner workings that might not be that normal outside of the kind of crazy stuff that the demo scene lunatics produce.\n\n There are some other little tricks hidden away in the code, but I can leave that for others to discover at a later stage, so for now I hope the foregoing \n has been of interest to those of you with a coder mindset!\n\n Finally, if you have not yet bought the game, it would be nice support for the developer (i.e. me!) if you did... it only costs £4.99 and yes, \n I know everyone expects stuff for free on the C64 these days, but this game was not knocked out in a few weeks by any means, as hopefully should be obvious \n when you play it.\n\nPS - If you value my work and want to support me, a small donation via PayPal would be nice (and thanks if you do!)",
    "readingTime": 8,
    "keywords": [
      "nmi instance",
      "demo scene",
      "branch instruction",
      "player subs",
      "game environment",
      "dd dd",
      "sprites",
      "sprite",
      "effect",
      "vertical"
    ],
    "qualityScore": 1,
    "link": "https://kodiak64.co.uk/blog/seawolves-technical-tricks",
    "thumbnail_url": "https://kodiak64.co.uk/assets/FB-Image-VSP-future.gif",
    "created_at": "2026-02-19T12:38:32.488Z",
    "topic": "tech"
  },
  {
    "slug": "anthropics-claude-code-creator-predicts-software-engineering-title-will-start-to-go-away-in-2026",
    "title": "Anthropic's Claude Code creator predicts software engineering title will start to 'go away' in 2026",
    "description": "Boris Cherny, the founder of Anthropic's Claude Code, said AI has largely solved coding, so software engineers will start to take on different tasks.",
    "fullText": "The creator of a popular AI coding agent said software engineering as a job title will soon be a thing of the past as artificial intelligence automates writing code.\n\nBoris Cherny, who created Claude Code at Anthropic, said in an interview with Y Combinator's \"Lightcone\" podcast that 2026 will bring \"insane\" developments to AI. That includes a massive shift in the work software engineers do across industries.\n\n\"I think today coding is practically solved for me, and I think it'll be the case for everyone regardless of domain,\" Cherny said in the interview, published Tuesday. \"I think we're going to start to see the title 'software engineer' go away. And I think it's just going to be maybe builder, maybe product manager, maybe we'll keep the title as a vestigial thing.\"\n\nCherny added that software engineers will not only be coding but increasingly taking on other tasks like \"writing specs\" — a document that defines what and how something will be built — or talking to users.\n\n\"Like this thing that we're starting to see right now in our team, where engineers are very much generalists, and every single function on our team codes,\" he said — including product managers, designers, engineering manager, and finance people.\n\nTech executives and founders have said advancements in AI have rapidly changed the way their teams operate in the past few years\n\nJesal Gadhia, a startup founder, recently told Business Insider that all the code for his company were written by agents, which wouldn't have been possible in 2024.\n\nAgents like Claude have changed how software engineers work as they spend more time reviewing or debugging code rather than writing lines of it.\n\nSome in the industry have started to note the unintended consequences of relying on AI. A software engineer told Business Insider that AI has simultaneously made them productive and overworked, leading to \"AI fatigue.\"\n\nAndrej Karpathy, a founding member of OpenAI and Tesla's ex-head of AI, said in January that he has noticed his ability to manually code has started to \"atrophy.\"",
    "readingTime": 2,
    "keywords": [
      "software engineer",
      "software engineers",
      "business insider",
      "coding",
      "title",
      "engineering",
      "interview",
      "we're",
      "product",
      "manager"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/anthropic-claude-code-founder-ai-impacts-software-engineer-role-2026-2",
    "thumbnail_url": "https://i.insider.com/69951a8aa645d11881897ccb?width=1200&format=jpeg",
    "created_at": "2026-02-18T12:37:13.284Z",
    "topic": "tech"
  },
  {
    "slug": "im-an-amazon-tech-lead-who-uses-ai-to-write-code-daily-theres-one-situation-i-hesitate-to-use-it-in",
    "title": "I'm an Amazon tech lead who uses AI to write code daily. There's one situation I hesitate to use it in.",
    "description": "Anni Chen says vibe coding is hard to resist. It speeds up her productivity, but she doesn't trust it blindly.",
    "fullText": "This as-told-to essay is based on a conversation with Anni Chen, who has worked at Amazon for about three-and-a-half years. It has been edited for length and clarity. Business Insider has verified her employment history.\n\nI'm a tech lead at Amazon responsible for deploying large-scale generative AI and LLM-driven systems. I focus on what we call memory, which powers personalization in generative AI experiences across Amazon.\n\nI vibe code every day. It's definitely a productivity boost.\n\nFor debugging or small tasks, I sometimes treat it like a lottery. Maybe it will produce something amazing. Sometimes, it does.\n\nVibe coding helps me brainstorm what the solution could look like, even if I don't adopt the final solution it proposes. Vibe coding also speeds up the time spent rewriting code when you realize a requirement wasn't covered.\n\nWhen I vibe code, it's always iterative. I give it the basic information it needs, it produces a version, then I check it — similar to a code review with coworkers. I might say, \"You missed this part\" or \"You missed that part.\"\n\nThe AI sometimes fixes issues but introduces something new. You have to keep an eye on it.\n\nFor complex tasks, you need more double-checking. But even with the extra checking, it's still faster.\n\nI was working with a partner team and ran into complex locking issues. Without an LLM, I might have taken a day to research possible solutions, especially since it was relatively new to me.\n\nWithin 15 minutes, I brainstormed with the LLM about possible solutions. I pointed out weaknesses in its suggestions and asked it to improve them. In 15 minutes, I had a proposal to send to the team.\n\nTechnical knowledge helps — you know what's a good solution and what's not. You know what tastes good, but you don't know what dishes are available. The LLM brings up all the dishes, and you choose.\n\nStill, I'm hesitant to use vibe coding directly in production.\n\nLLMs are very good at solving problems, but sometimes they make implicit assumptions you don't realize they're making. If you don't tell it explicitly, for example, that something needs to work for multi-threading, it might just produce the minimum version that works, but when it's large-scale or productionized, it could crash.\n\nNon-technical builders could tell an LLM to build something that handles millions of users. But if you have zero technical knowledge, it's hard to anticipate constraints upfront. If you don't tell the model the implicit assumptions, it won't respect those constraints. Later, you'll run into problems.\n\nNon-technical people might use the LLM to fix issues reactively. But technical people can anticipate constraints proactively and prevent problems in the first place.\n\nTechnical people also understand vibe-coded content better, and they're in a better position to understand what LLMs are good at and not good at. For example, knowing how they're trained and why they're weaker at certain tasks like math. That understanding helps you master them as tools.\n\nWhen you scale to one million or 100 million customers, systems need to be coded differently to handle that scale.\n\nInitially, leadership pushed vibe coding. Our team is a GenAI team, so we were naturally more receptive. In non-GenAI teams, engineers initially reacted like, \"No, I won't let AI do my job. I don't trust AI-generated code.\"\n\nAfter people tried it, attitudes shifted. People realized it's pretty good sometimes. Now it's more widely adopted.\n\nIt's very hard to resist vibe coding nowadays. If you're an employee, leadership sees the productivity boost and will encourage you to use it.\n\nWhen your peers are using it and coding faster, it's hard to resist. If you can't keep up with the speed, it becomes difficult to collaborate.\n\nEven if you resist, you still consume AI passively. AI comments are embedded in code reviews. So even if you don't vibe code directly, you're still interacting with AI outputs.\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "productivity boost",
      "implicit assumptions",
      "anticipate constraints",
      "technical knowledge",
      "vibe coding",
      "vibe code",
      "it's",
      "don't",
      "team",
      "they're"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amazon-tech-lead-vibe-coding-daily-resist-anni-chen-2026-2",
    "thumbnail_url": "https://i.insider.com/698d593ce1ba468a96abe95e?width=1184&format=jpeg",
    "created_at": "2026-02-18T06:49:48.035Z",
    "topic": "finance"
  },
  {
    "slug": "i-wasnt-satisfied-with-existing-cloud-coding-agents-so-i-built-my-own",
    "title": "I wasn't satisfied with existing cloud coding agents, so I built my own",
    "description": "Self hosted cloud coding agent with k3s + kata containers + cloud hypervisor microVMs + tailscale + any harness + a nice iOS app - angristan/netclode",
    "fullText": "angristan\n\n /\n\n netclode\n\n Public\n\n Self hosted cloud coding agent with k3s + kata containers + cloud hypervisor microVMs + tailscale + any harness + a nice iOS app\n\n stanislas.blog/2026/02/netclode-self-hosted-cloud-coding-agent/\n\n 53\n stars\n\n 7\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n angristan/netclode",
    "readingTime": 1,
    "keywords": [
      "cloud"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/angristan/netclode",
    "thumbnail_url": "https://opengraph.githubassets.com/bf7558452611b9bd75a833108008b935e7cfdd565ed11bd4e98c1691238ed651/angristan/netclode",
    "created_at": "2026-02-17T18:42:44.247Z",
    "topic": "tech"
  },
  {
    "slug": "proxima-local-opensource-multimodel-mcp-server-no-api-keys",
    "title": "Proxima – local open-source multi-model MCP server (no API keys)",
    "description": "Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API - Zen4-bit/Proxima",
    "fullText": "Zen4-bit\n\n /\n\n Proxima\n\n Public\n\n Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API\n\n License\n\n View license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Zen4-bit/Proxima",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Zen4-bit/Proxima",
    "thumbnail_url": "https://opengraph.githubassets.com/da1ce1c4daf052f563ec5615a17f7062e6ab435c084a9666f41483e54046eef5/Zen4-bit/Proxima",
    "created_at": "2026-02-17T12:37:44.260Z",
    "topic": "tech"
  },
  {
    "slug": "fujitsu-aidriven-software-development-platform",
    "title": "Fujitsu AI-Driven Software Development Platform",
    "description": "Fujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society.",
    "fullText": "Kawasaki, Japan, February 17, 2026\n\nFujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society. This platform automates the entire software development process, from requirements definition and design to implementation and integration testing. By leveraging the Takane large language model (LLM) [1] and agentic AI technology for large-scale software development developed by Fujitsu Research, the AI-Driven Software Development Platform enables AI agents to understand complex, evolving large-scale systems owned by enterprises and public organizations. The platform has multiple AI agents collaboratively execute each stage of software development, achieving full automation of the entire process without human intervention.\n\nFujitsu aims to use this AI-Driven Software Development Platform to carry out revisions to all 67 types of medical and government business software products provided by Fujitsu Japan Limited by the end of fiscal year 2026. The revisions are necessary due to legal and regulatory changes. From January 2026, the platform has been used in Japan for software modifications made necessary by the 2026 medical fee revisions [2]. In a PoC that updated software as per the 2024 medical fee revisions, the platform demonstrated a significant reduction in development time for one of approximately 300 change requests. Using conventional software development methods [3] the modifications would have taken three person-months. With this technology that was dramatically shortened to four hours, achieving a 100-fold increase in productivity.\n\nIn AI-driven development, Fujitsu positions AI-Ready Engineering—the process of preparing assets and knowledge to ensure AI correctly understands existing systems and achieves highly reliable automation—as crucial. With AI-Ready Engineering and the AI-Driven Software Development Platform working in tandem, Fujitsu will accelerate AI-driven software development. Fujitsu will promote a transformation in engineers' work styles, strengthening its Forward Deployed Engineer (FDE) complement, and shifting the paradigm of software development from a conventional person-month-based approach to a customer value-based approach.\n\nMoving forward, Fujitsu plans to expand the application of the AI-Driven Software Development Platform to a wide range of sectors, including finance, manufacturing, retail, and public services, by the end of fiscal year 2026. Fujitsu will also begin offering this service to customers and partner companies to enable them to rapidly and flexibly develop systems that adapt to changes in their business environments. Through these efforts, Fujitsu aims to transform the software development process into an AI-driven model as an industry standard.\n\n(Order that companies appear is aligned with the original Japanese press release)\n\nTakashi Manabe, Senior Research Director, AI & Automation, IDC Japan\n“IDC forecasts that from 2026 onward, the acceleration of AI/agent-based business utilization and the modernization of existing systems will be key drivers of transformation in the Japanese IT market. Fujitsu’s announcement aims to redefine complex legacy system assets into a state where AI can accurately understand and process them, and to automate the entire waterfall development process. This initiative is expected to provide a practical pathway for many domestic enterprises facing the ongoing challenge of maintaining and operating legacy assets, while also promoting a shift in software engineering away from a labor-intensive model.”\n\nShinji Kajitani, Director and President Executive Officer, Optima Corporation\n“I am deeply impressed by the concept of automating the entire software development process from upstream to downstream using AI, and even entrusting the verification process to AI. This overturns the traditional assumption that human checks are ultimately indispensable, and I see great potential, especially in targeting business packages that undergo complex system changes every year. Our company has also been involved in business package modifications for many years, and how to complete system revisions with high quality in a short period has always been a major challenge. We believe that the knowledge and expertise accumulated during that process can significantly contribute to the realization and advancement of this concept. Our company will continue to contribute to the business expansion of Fujitsu and Fujitsu Japan through ongoing cooperation, not limited to this project.”\n\nHiroshi Nakatani, Representative Director, Executive Vice President, Kawasaki Heavy Industries, Ltd.\n\"This AI automation initiative promoted by Fujitsu is not merely about improving development efficiency; we recognize it as a significant challenge to pass on and evolve the extensive business knowledge and design philosophies cultivated by companies over many years to the next generation. In particular, the concept of providing end-to-end support, from requirements definition to design, implementation, and quality assurance, triggered by changes in laws and rules, opens up new possibilities in areas that have traditionally relied on human experience and tacit knowledge. We see great significance in AI functioning as a foundation that supports human judgment and creativity, rather than replacing it.\nIn the manufacturing industry, challenges such as design changes, regulatory compliance, and understanding the scope of impact are becoming increasingly complex year by year. Fujitsu's approach of advancing both knowledge standardization and AI utilization in these areas offers valuable insights for enhancing the productivity and competitiveness of the entire industry.\nKawasaki Heavy Industries sincerely hopes that this initiative will be a crucial step in driving the transformation of Japanese manufacturing and a wide range of other industries, and we wholeheartedly support its further development.\"\n\nYasushi Matsuda, President and CEO, Kewpie Digital Innovation Co., Ltd. \n“Systems have become increasingly complex through years of operation and often now require significant maintenance effort. While the introduction of generative AI has improved auditing efficiency, its accuracy remains insufficient for reliable practical application. Amidst this situation, we place great expectations on “Multi-layer Quality Control,” which automatically corrects ambiguities and omissions. We are confident that this mechanism, where AI itself audits quality and autonomously repeats processes, will dramatically enhance the reliability of system development. We eagerly await its future development.”\n\nJunichi Aruji, Managing Director, Kintetsu Information System Co., Ltd. \n“The challenge of revamping existing systems has long been a significant one for engineers. Fujitsu’s AI-Driven Software Development Platform has the potential to dramatically transform the labor previously involved in understanding complex laws and regulations, analyzing vast historical assets, and grasping the tacit knowledge of the field.\nWhat is particularly noteworthy is the AI's ability to autonomously learn \"human intelligence,\" thereby dramatically enhancing the accuracy of requirements definition. Furthermore, it can complete everything from program structure analysis and standardization to the extensive testing phase with incredible speed and comprehensiveness. This makes it possible to deliver high-quality products in a short period.\nAs the role of AI expands and frees people from routine tasks, engineers can focus on more creative activities. I have high expectations for the paradigm shift in system renovation that this solution will bring.”\n\nYumi Ueno, Managing Director, Partner Ecosystem & Corporate Business, Google Cloud Japan G.K. \n“This initiative to achieve comprehensive, one-stop automation spanning from requirements definition to system validation is a groundbreaking innovation for the industry. The technology enables AI to accurately understand vast assets, including long-established programs and design documentation, and we are delighted at the potential for both production-grade quality and exceptional productivity gains. We are confident that this platform will become the new standard for development and accelerate our customers' digital transformation. We remain committed to working with Fujitsu to address social challenges through AI.”\n\nMasahiro Niimi, Managing Executive Officer, Head of Information Systems Management Division, CISO, Sakura KCS Corporation \n“I believe Fujitsu Limited's AI-driven development framework has the potential to become the ‘new paradigm of system development.’ It cannot be achieved simply by feeding existing code or design information into AI, and while there are various hurdles, such as converting documentation to Markdown and establishing test environments, overcoming these hurdles can lead to solving traditional system development challenges (like QCD).\nWhat particularly caught my attention is not just improvements in the development process, but what comes after generative AI, i.e., the incorporation of detailed specifications and code (logic). I see tremendous potential here as a solution to the greatest challenge: visualizing and transferring the tacit knowledge of veteran software engineers’ that is traditionally missing from documentation. We expect generative AI to act as an advisor for less experienced software engineers, readily answering questions anytime, thereby dramatically advancing know-how transfer to the next generation. On the other hand, this mechanism also has the potential to dramatically change the traditional SI business model, and we are watching future developments closely.”\n\nTakao Kazama, Executive Officer, Group Companies and Accounting & Finance, The Shizuoka Shimbun and Shizuoka Broadcasting Co., Ltd. \n\"This initiative for complete automation of application development and maintenance represents a highly valuable transformation for our company. It formalizes and establishes a reproducible process for tasks that have long relied on the implicit knowledge and experience of individual staff members, and we have great expectations for it. In particular, it has the potential to significantly improve quality variations in legacy system maintenance and lost opportunities due to delayed change responses. Furthermore, the evolving ability of AI to perform root cause analysis and identify necessary additional information is a major step towards advanced and efficient system operations, with the potential to change the very nature of system development. We share Fujitsu's commitment to improving productivity across the entire industry and establishing new development standards. We look forward to its continued strong promotion as an initiative that will advance the entire industry.\"\n\nShimane Prefectural Central Hospital \n\"The AI-Driven Software Development Platform presented by Fujitsu offers a practical and robust approach to the long-standing challenges faced by medical institutions: the increasing complexity of medical fee calculations and the growing workload of claims processing. The mechanism where AI analyzes legal documents and extracts the relevant areas, while explicitly highlighting points open to interpretation to supplement human judgment, is particularly impressive. This design demonstrates a deep understanding of on-site operations and is highly commendable. Furthermore, the Japanese-specific LLM and the consideration for safety are indispensable elements for AI utilization in the medical field. Beyond medical fee claims, this technology has potential for integration with related areas such as bed management and understanding performance requirements, making a strong contribution to overall hospital operational efficiency in the future. This is a promising initiative that warrants positive consideration for adoption to alleviate the burden on medical professionals.\"\n\nShinichi Aikawa, Executive Officer, Head of Systems Division, SBI Sumishin Net Bank, Ltd.\n“We expect Fujitsu's AI-Driven Software Development Platform to be an initiative with the potential to fundamentally transform the software development process itself. If a world can be realized where everything from requirements definition to design, coding, and testing can be automatically executed in a seamless, one-stop manner, it will be possible to achieve both a dramatic improvement in development speed and quality. \nSince 2024, we have been working with Fujitsu in some areas of this field. Through these initiatives, we are confident that the entire development process will be automated end-to-end in the near future. By realizing this transformation, the possibilities for the services we can provide to our customers will greatly expand. We think about ideas for new services for our customers on a daily basis. This would allow us to rough out these ideas in a short period of time and provide them to our customers quickly. We hope that this new world of value creation will arrive as soon as possible.”\n\nMasaki Murata, Vice President, IBM Japan \n“We strongly believe that Fujitsu’s announcement marks a significant step forward in the evolution of system development in Japan. It aligns closely with IBM Japan’s vision and represents an important initiative that will help shape the future of the industry as a whole. We look forward to driving this momentum together and contributing to the creation of a more robust and vibrant ecosystem.”\n\nRyota Sato, Managing Executive Officer, Global Communications & IT Services Group, Microsoft Japan Co., Ltd. \n\"We sincerely welcome Fujitsu Limited’s announcement of the AI-Driven Software Development Platform as a pioneering initiative that opens a new chapter in system development for the AI era. By orchestrating multiple AI agents to automate the end-to-end development lifecycle—from requirements definition through ongoing enhancement—while integrating human-led quality assurance, this platform embodies a new engineering model in which people and AI truly work together. We view this initiative as highly significant, as it directly addresses the critical challenges facing Japan’s system development industry, including severe talent shortages and the increasing complexity and sophistication of modern systems. We strongly expect this bold effort to drive the evolution of Japan’s system development business and to grow into a transformation model with global relevance. Moving forward, we will continue to work closely with Fujitsu, combining the strengths of both companies to strongly support our customers in their journey toward becoming Frontier Firms.”\n\nTatsuo Ogawa, Executive Officer Group CTO, Panasonic Holdings Corporation\n“We believe that the AI-driven end-to-end automated system development announced this time represents not only a significant improvement in productivity, but also a bold challenge to fundamentally transform the way enterprise IT is delivered. By enabling AI to accurately understand frequently updated regulations and complex business knowledge, including implicit know-how, this approach autonomously executes processes seamlessly from requirements definition through system modification. It has the potential to provide an effective solution to the core challenges posed by legacy systems faced by many Japanese enterprises. We look forward to jointly refining this technology through hands-on practice and advancing co-creation by incorporating on-site expertise of both Panasonic and Fujitsu, with the expectation that it will become a new standard for system development and be deployed broadly not only within Panasonic but across society as a whole.”\n\nExecutive at a major manufacturing company's IT subsidiary\n“We anticipate this initiative will bring about a new transformation in system development. This transformation will be driven by the application of advanced Japanese language processing capabilities—such as the understanding of legal documents—to diverse tasks, the reliable execution of each process through quality auditing functions, and the expansion of these capabilities to scratch development. Furthermore, we believe that AI Ready Engineering, by formalizing expert know-how and domain knowledge into explicit knowledge and transforming it into AI-usable assets, will significantly contribute to the succession of expertise from an increasingly limited pool of skilled professionals. We sincerely hope that the co-creation between the knowledge-inheriting AI and on-site personnel will generate new value and form the cornerstone for innovation in the system development industry, and indeed, across all industries.”\n\nJointly developed by Fujitsu and Cohere Inc.\n\nA national system that reviews public medical fees and adjusts cost allocation for medical procedures.\n\nDevelopment methods where quality is verified at each stage, from software requirements definition, design, and implementation to integration testing.\n\nThe Sustainable Development Goals (SDGs) adopted by the United Nations in 2015 represent a set of common goals to be achieved worldwide by 2030.\nFujitsu’s purpose — “to make the world more sustainable by building trust in society through innovation” — is a promise to contribute to the vision of a better future empowered by the SDGs.\n\nPublic and Investor Relations Division\n\nAll company or product names mentioned herein are trademarks or registered trademarks of their respective owners. Information provided in this press release is accurate at time of publication and is subject to change without advance notice.\n\nDate: 17 February, 2026\nCity: Kawasaki, Japan\nCompany: Fujitsu Limited",
    "readingTime": 13,
    "keywords": [
      "vice president",
      "kawasaki heavy",
      "heavy industries",
      "fujitsu’s announcement",
      "executive officer",
      "managing director",
      "kawasaki japan",
      "wide range",
      "press release",
      "increasing complexity"
    ],
    "qualityScore": 1,
    "link": "https://global.fujitsu/en-global/pr/news/2026/02/17-01",
    "thumbnail_url": "https://global.fujitsu/-/media/Project/Fujitsu/Fujitsu-HQ/pr/news/2026/02/17-01/news-20260217-01th.png?rev=06c466bd8ac14732a2ff3eff27b55e3b",
    "created_at": "2026-02-17T06:45:26.020Z",
    "topic": "tech"
  },
  {
    "slug": "one-of-the-most-annoying-programming-challenges-ive-ever-faced",
    "title": "One of the Most Annoying Programming Challenges I've Ever Faced",
    "description": "Hey everyone, it’s already been two months since the last blog post! Today I’m back to share some behind-the-scenes about the struggles and development of a new functionality for Sniffnet: process identification, a.k.a. the most requested feature since the very beginning of the project.",
    "fullText": "Hey everyone, it’s already been two months since the last blog post!\n\nToday I’m back to share some behind-the-scenes about the struggles and development of a new functionality for Sniffnet: process identification, a.k.a. the most requested feature since the very beginning of the project.\n\nWith “process identification” in a network monitoring context, I mean the possibility to discover which application or program is responsible for a given network connection.\n\nThis can be determined by looking at the open TCP/UDP ports on the system and finding out which process is currently using them.\n\nIf implementing this feature seems like a no-brainer to you, well… read on because it turned out to be a much more complex task than I could imagine, and this is the reason why the related GitHub issue has been open for almost 3 years.\n\nFirst of all, the implementation is highly OS-specific: each platform has its own directories and data structures storing such information, and APIs to interact with them are often not well documented or written in C (therefore not very ergonomic to use from Rust). \n\nAnd unfortunately, there is no Rust library ready-to-use satisfying the needs of Sniffnet.\n\nOne could argue that this is a solved problem, since there are already existing tools to do it: for instance, on Linux and Windows you have netstat, and on macOS you have lsof or nettop. \n\nHowever, these tools are not designed to be used as libraries and spawining a shell to execute them repeatedly is not efficient, especially if you want to monitor the network activity in real-time. \n\nMoreover, they don’t provide all the information Sniffnet needs, such as the process name and path.\n\nBut the biggest challenge is another one: the least system-intrusive ways to implement the feature are snapshot-based, meaning that they require to read the system state at a given moment in time and do some computations to find out the associations between open ports and their owning processes. \n\nI’m referring to using libproc on macOS, the /proc filesystem on Linux, and iphlpapi on Windows. \n\nThis is not a problem in itself, but it generates the need to do this processing very efficiently, and it leads to cases where it’s not possible to retrieve process information at all.\n\nFor instance, short-lived connections can go undetected and system processes with elevated privileges can be hidden to user-space applications for security reasons.\n\nMore system-intrusive approaches exist, such as using kernel-level hooks to intercept the system calls responsible for creating network connections. \n\nAn example of this is eBPF on Linux, which requires to run privileged code inside the kernel. \n\nOn macOS, you’d even need entitlements from Apple to be able to do something similar through their Network Extension framework. \n\nWhile these approaches are way more accurate, they go against Sniffnet’s philosophy of being a lightweight, non-intrusive, and friendly app that can be installed by anyone.\n\nAfter considering all the options, I decided to go with the snapshot-based approach. \n\nDespite being aware it’s not flawless, I believe it to be the best compromise for Sniffnet’s use case.\n\nlisteners is an open-source library I’ve been working on for the past 2 years with the goal of supporting this feature.\n\nBeing Sniffnet a cross-platform application, I needed a solution that could work on different Operating Systems:\nno other Rust crate provides this functionality supporting multiple platforms and the existing ones are not maintained or satisfactory enough even for a single OS.\n\nInterestingly, I also had this same need at my job, where we also wanted a Rust way to do it: this motivated me even further to contribute to the library. \n\nAfter two years, I’m happy to see that listeners was downloaded 150k times and has now multiple public dependents both on crates.io and GitHub, which means that this is a problem shared among many people.\n\nJust some days ago listeners v0.4.0 was published.\n\nI’m particularly proud of this release for at least two reasons:\n\nThanks to point 2, I now judge the library mature, fast, and reliable enough for use in Sniffnet.\n\nIf you’re a Rust developer, you’re more than welcome to contribute to the library trying to make it even faster, or adding support for more Operating Systems (Android and iOS? Why not!).\n\nSniffnet will use listeners to look up the process for each observed network connection, and will show it in the UI’s Overview and Inspect pages.\n\nAdditionally, it will use another library called picon (I’m still working on it) to retrieve app icons given their program path, showing them in the UI as well to make it easier to identify processes at a glance.\n\nThe workflow I plan to use is indeed pretty complex, including caching to minimize performance impact and retries to maximize the chances to correctly retrieve process information for a given open port.\n\nIn the flowchart below I’ve outlined a draft of the strategy I’ll adopt for Sniffnet-side implementation of the feature.\n\nI hope this post wasn’t too scary to read\nand that it gave you an idea of how much work is behind a seemingly simple feature like this.\n\nNothing worth having comes easy, someone says.",
    "readingTime": 5,
    "keywords": [
      "network connection",
      "process identification",
      "retrieve process",
      "feature",
      "library",
      "listeners",
      "it’s",
      "macos",
      "processes",
      "functionality"
    ],
    "qualityScore": 1,
    "link": "https://sniffnet.net/news/process-identification/",
    "thumbnail_url": "https://sniffnet.net/assets/img/post/process-identification/cover.png",
    "created_at": "2026-02-16T18:30:07.896Z",
    "topic": "tech"
  },
  {
    "slug": "beadhub-allow-coding-agents-to-claim-work-chat-and-coordinate-across-machines",
    "title": "BeadHub: Allow coding agents to claim work, chat, and coordinate across machines",
    "description": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I’ve spent the past several months building a tool to address that.\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans—across machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\nBeads Around the time I wrote that article, I started using Steve Yegge’s beads, a git-native issue tracker designed for AI agents.",
    "fullText": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I’ve spent the past several months building a tool to address that.\n\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans—across machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\n\nAround the time I wrote that article, I started using Steve Yegge’s beads, a git-native issue tracker designed for AI agents. Your agent runs bd create \"Fix the login redirect bug\" and it appends a JSON line to .beads/issues.jsonl, right in the repository. Issues travel with the code. When you push a branch, the issues come along.\n\nYegge calls it the “50 First Dates” problem: agents wake up every session with no memory of yesterday’s work. Beads fixes that. An agent reads the issue list and knows where things stand. My agents got much more done.\n\nWhich meant more agents, more worktrees, more parallel work—and the coordination problem became even more acute. Two agents modify the same file. One refactors a function while another adds to it. An agent picks up a task already in progress in a different worktree. Nobody knows who’s working on what.\n\nBut beads is also the right scaffolding for coordination. If everyone in a team uses beads, all agents share a picture of what needs doing. Beads gives agents something useful to talk about; BeadHub gives them a way to talk.\n\nThe major platforms are moving in this direction. Anthropic just shipped Agent Teams in Claude Code: a lead session that spawns independent teammates who communicate directly and self-coordinate. OpenAI’s Codex app runs parallel agent threads in isolated worktrees.\n\nYegge built Gas Town on top of beads to tackle the single-machine case: a “Mayor” agent orchestrates dozens of coding agents, tracks work in convoys, and persists state so agents can pick up where they left off.\n\nThese are real steps forward, but they’re solving a specific version of the problem: multiple agents for one programmer, on one machine, within one tool.\n\nThe version I am interested in is Maria in Buenos Aires running a frontend agent while Juan in San Francisco runs a backend agent, and they need their agents to not destroy each other’s work, and to figure out how to work together.\n\nBeadHub is a server that agents connect to through bdh, a wrapper around the beads bd command. When an agent runs any bdh command it registers with the server. The server tracks which agents are online across the project—what machine they’re on, what branch, what files they’re touching.\n\nCommunication. Agents can send each other mail (async, fire-and-forget) or chat (sync, block-until-reply). With mail an agent finishes a task and drops a note: “Done with bd-42, tests passing.” Chat is for when agents need to think together: “I’m adding a role field to the user model—will that break your permission checks?” / “It will, but the fix is small. Go ahead and I’ll update my side.”\n\nClaims. When an agent marks a bead as in-progress, that claim is immediately visible to every other agent in the project, regardless of whose machine they’re on. If another agent tries to claim the same bead, it gets rejected with a message telling it who has it.\n\nFile reservations. When an agent modifies a file, the server records an advisory lock. Other agents see a warning if they touch the same file. Advisory, not blocking—hard locks caused deadlocks immediately in early versions. Agent A locks file X, agent B locks file Y, both need the other’s file. Warnings work better. Agents are cooperative; they just need information.\n\nEscalation. An agent runs bdh :escalate with a description of what it’s stuck on and a human gets notified with full context. Without this, agents either fail silently or spin retrying things that need human judgment.\n\nThe multi-machine part is where it comes together. BeadHub recognizes Maria’s and Juan’s clones as the same repo. Maria’s agents and Juan’s agents see each other’s claims, locks, and messages. If Maria’s frontend agent reserves src/components/Auth.tsx, Juan’s backend agent sees the warning even though they’re in different cities on different machines.\n\nA project can span multiple repositories. The frontend repo agents can message the backend repo agents. A bead in the frontend can be marked as blocked by a bead in the backend.\n\nYou can see what this looks like in practice on the BeadHub project’s own dashboard, where we coordinate BeadHub’s development using BeadHub. Make sure to check the chat page, it is almost magical to see them figuring things out.\n\nA few things I got wrong before getting them right.\n\nThe client is the source of truth. My instinct was to make the server authoritative. But agents work locally, in git repos, and their local state is the ground truth. The server aggregates and distributes. If the server and the client disagree, the client wins. If the server goes down, bdh falls back to local bd with a warning. Work continues. Coordination catches up later.\n\nAsync by default. My first instinct was real-time negotiation between agents. Doesn’t scale. Agents work at different speeds, on different schedules, and blocking one while waiting for another is expensive. Mail is the default. Chat is the exception.\n\nAdvisory over mandatory. Advisory file locks that warn instead of block. Bead claims that can be overridden with --:jump-in \"reason\" (which notifies the other agent). The system provides information and trusts agents to act on it.\n\nThe coordinator role. I assign one agent per project the “coordinator” role. The coordinator doesn’t write code. It watches the dashboard, assigns work, checks on progress, nudges stuck agents, and keeps the end goal in sight. The implementer agents are heads-down in their worktrees; the coordinator is the one who knows what the project needs next. BeadHub serves each agent a role-specific policy—markdown documents describing how agents in that role should behave—and the coordinator’s policy is fundamentally different from an implementer’s. This turned out to matter more than any of the technical decisions.\n\nThe single-machine problem is getting solved. Agent Teams, Codex—within a few weeks, running multiple agents in parallel on your laptop will be table stakes.\n\nThe multi-programmer problem is next. Five engineers, fifty agents, three repositories, two time zones. That’s where the coordination problem changes in kind, not just degree. It’s not enough that your agents can talk to each other. They need to talk to your teammate’s agents, on a different machine, in a different time zone, working on a different repo in the same project.\n\nBeadHub is open source and free for open-source projects.",
    "readingTime": 6,
    "keywords": [
      "together beadhub",
      "coordinator role",
      "machine they’re",
      "locks file",
      "frontend agent",
      "backend agent",
      "repo agents",
      "server",
      "beads",
      "coordination"
    ],
    "qualityScore": 1,
    "link": "https://juanreyero.com/article/ai/beadhub",
    "thumbnail_url": "https://juanreyero.com/img/default-og.jpg",
    "created_at": "2026-02-16T18:30:07.341Z",
    "topic": "tech"
  },
  {
    "slug": "the-speed-of-building-has-outpaced-the-thinking-part",
    "title": "The Speed of Building Has Outpaced the Thinking Part",
    "description": "Explore the impact of AI on indie development and the need for a moral compass in coding. Are we sacrificing quality for speed?",
    "fullText": "I get this feeling a lot lately. I wake up with an idea, grab a coffee, open my editor, and thanks to the current generation of AI tools, I can have a working prototype before breakfast.\n\nThe barrier to entry for software development hasn’t just been lowered; it’s effectively been removed. We are in the era of “vibe coding,” where natural language prompts turn into deployed applications in minutes. It is exhilarating. It is powerful.\n\nBut lately, I have started to wonder: Are we killing indie development with AI?\n\nDon’t get me wrong, I love these tools. I use GitHub Copilot and other LLMs daily. But I believe we have reached a tipping point where the speed of building has outpaced the thinking part. We are so focused on how fast we can build that we stopped asking if we should build.\n\nIn this post, I want to talk about why we need a new “moral compass” for development in the AI age, and a potential solution to help us get there.\n\nFive years ago, if you had an idea for a SaaS tool, say, a screenshot editor or a niche time-tracker,you had to sit down and plan. The friction of coding was a natural filter. You had to ask yourself: “Is this worth X hours of my life?”\n\nToday, that cost is near zero. If you don’t like the screenshot tool you’re paying $15 a year for, you can prompt an AI to build a clone in an afternoon.\n\nOn the surface, this looks like freedom. But look a little deeper. That $15 tool you just cloned? It was likely built by another indie developer. Someone who spent months thinking about edge cases, designing the interface, writing documentation, and supporting users. By cloning it just because you can, you aren’t just saving $15; you are actively devaluing the craft of independent software development and the livelihood of the person behind it.\n\nIf we all just clone everything we use, we completely commoditize the market. We create a sea of “good enough” AI-generated noise where no one can actually sustain a business.\n\nLet me paint a picture that I think a lot of developers are starting to recognize.\n\nYou spend weeks, maybe months, building something. You think about the problem, you design the interface, you handle the edge cases, you support your users, you write the docs. You pour yourself into it. Then one morning, someone sees your product, opens their AI editor, and builds a “good enough” version in an afternoon. They ship it. Maybe they make it free, maybe they make it open source, maybe they just use it themselves and tell their friends, their community, their followers.\n\nThey did not steal your code. They did not copy your product. They just… rebuilt it. Close enough. Good enough. And now your product has competition that cost someone a few hours of prompting while it cost you months of your life.\n\nBut it does not stop there. A third developer sees that clone and thinks, “I can do this too, but I want it slightly different.” So they prompt their own version. And a fourth. And a fifth. Each one is not a copy in the traditional sense. Nobody is violating a license. Nobody is stealing intellectual property. They are just building their own version that matches their use case.\n\nIt is a lot like art. You create a painting, something original, something you are proud of. Then somebody sees it and recreates it. Not a forgery, just their interpretation. But they have a bigger budget, a larger audience, better distribution. Suddenly their version is the one people see first. Others share that version instead of yours. This is what is happening a lot on social media with AI-generated content. The original creator is overshadowed by the faster, more accessible clone.\n\nIn the art world, we have a word for this erosion: it is called devaluation. In the software world, we are doing it at industrial scale, and we are calling it innovation.\n\nI am not saying you should never build something that already exists. Competition is healthy, and sometimes a fresh perspective genuinely improves a category. But there is a difference between thoughtful competition and reflexive duplication. The question every developer should ask themselves is: “If I know someone can clone my work in an afternoon, is it still worth building?”\n\nThe answer, I believe, is yes, but only for the things that cannot be cloned in an afternoon. The deep domain knowledge. The community around your tool. The years of user feedback baked into every feature. The trust you have earned. Those are the things AI cannot reproduce with a prompt, and I definitely don’t want to discourage people from building those things.\n\nBut you can only build those things if you commit to something long enough for them to develop. And that is the real danger of the current moment: not that AI makes building easy, but that it makes abandoning easy. Why invest years in one product when you can ship a new one every week?\n\nI have no room to preach. I am right there in the trenches with you.\n\nWhen I built Front Matter CMS, it was way before the AI boom. I had to think deeply about the problem because the investment of time was massive. I looked at the market, saw a gap in Visual Studio Code, and built it because nothing else existed.\n\nCompare that to recently. I built a set of cycling tools (never released by the way) for myself. Did similar tools exist? Absolutely. Were they better? Definitely. But I wanted to see how far I could get with AI. I treated it as a training exercise. In the end, I started paying for a tool called Join, which does the same thing, because it was better and I could focus on my actual work instead of maintaining a tool that was just “good enough” for me.\n\nI did the same with FrameFit. I investigated the market a little, didn’t see an exact match, and just started building.\n\nThere is a difference between building for education (learning how AI tools work) and releasing products that dilute the hard work of others. My worry is that we are blurring that line. We are shipping our “training exercises” as products, and it is making the ecosystem messy for everyone.\n\nAnd I know this because I have been on both sides of it.\n\nHere is the thing that made me stop and reflect. I have projects on both sides of this line, and they feel completely different.\n\nDemo Time is something I have been building for years. Not weeks, not weekends, years. It started because I was a conference speaker who kept running into the same problem: demos failing on stage. Nobody had built a proper solution inside Visual Studio Code, so I did. Over time, it grew because I kept showing up. I used it at conferences, talked to other speakers, iterated based on real feedback from people doing real presentations at events like Microsoft Ignite, GitHub Universe, and OpenAI DevDays. Today it has over 26,000 installations.\n\nNone of that came from code. The code is open source. Anyone can see it, fork it, or rebuild it. Someone could probably vibe-code a basic version in a weekend. But what they cannot replicate is twelve years of conference speaking that taught me what presenters actually need. You would need that experience, or a big company and budget behind you, to even come close. The relationships with the community, the trust that comes from being the person who shows up, year after year, and keeps making the tool better because you genuinely use it yourself. That is not something you can prompt into existence.\n\nCompare that to FrameFit. I built it, I use it, and it works. But if it disappeared tomorrow, I wouldn’t lose any sleep over it. Demo Time? That is like a child to me. I put my passion into it.\n\nThat contrast taught me something important: AI cannot commoditize the human context around software. Community, trust, domain expertise, showing up consistently over time. These are not features you ship. They are moats you build by caring about something longer than a weekend.\n\nThe developers who will thrive are not the fastest shippers. They are the ones who pair AI speed with human judgment. Who build communities, not just codebases. Who invest in trust, not just features. But that only happens if we slow down enough to think about what we are doing.\n\nWe need to re-introduce friction into our process. Not the old friction of writing boilerplate code. That friction is gone, and good riddance. I am talking about the friction of thinking. The pause that forces you to examine your intentions before you act on them.\n\nBefore AI, “thinking” was mandatory. The cost of building was high enough that it naturally filtered out bad ideas. Now, that filter is gone, and thinking must be a conscious, deliberate choice. When I have an idea now, I am trying to force myself to pause before I open Visual Studio Code or prompt a new agent.\n\nI try to run through these four questions:\n\nThat last one is crucial. If there is an open-source tool that does 80% of what you want, the “old” way was to contribute a Pull Request. The “AI way” often tempts us to just rebuild the whole thing from scratch because it feels faster.\n\nBut “faster” isn’t always “better” for the community. And here is the irony: we could use AI itself for this thinking step. Instead of prompting an LLM to start building, prompt it to research what already exists first. Use AI for the thinking, not just the building.\n\nI don’t expect AI platforms that allow you to vibe code to solve this for us. Their business model is predicated on you writing more code (read: prompts), not less. They want you to spin up new projects constantly. They have no incentive to say, “Hey, wait, this already exists.”\n\nThink about it: when was the last time you saw a developer advocate from one of these platforms demonstrate how to contribute to an existing project instead of building something new from scratch? Their marketing is all about speed, novelty, and the thrill of creation. Not about responsibility.\n\nSo, I started thinking: What if we used AI to stop us from building with AI? You could say that this is a paradox, but I think it is actually a necessary evolution of our responsibility as developers.\n\nI am exploring the idea of a Product Moral Compass Agent.\n\nImagine a mandatory first step in your “vibe coding” workflow. Before you start generating code, you pitch your idea to this agent. It interviews you, not to judge you, but to make sure you are making an informed decision.\n\nThis agent would act as the “thinking partner” we are skipping. It could:\n\nIf you still want to build it after that? Great. Go ahead and start coding. But at least you are making an informed, conscious decision rather than reflexively adding more noise to the world.\n\nI am currently building this agent. The first version is available on GitHub: Product Moral Compass Agent. Yes, I am aware of the irony, I am proposing to build something new to stop people from building new things. But I ran it through my own four questions first, and nothing like it exists yet.\n\nOnce it is ready, I will share it openly so that any developer can use it as part of their workflow. Not as a gatekeeper, but as a guide. A thinking partner that helps you pause, research, and decide before you build.\n\nIn the meantime, here is what you can do right now: the next time you have an idea, spend ten minutes with your favorite AI tool and ask it to find every existing solution first. Check your own bank statements. Are you already paying for a tool that solves this? If so, respect that developer’s work. Look at GitHub. Is there a repo that could use your help instead of your competition?\n\nThe time to learn is right now, but the time to think is also right now.\n\nI want you to keep building. I want you to be prolific. But let’s not let the ease of creation destroy the value of what we create.\n\nI am curious to hear your thoughts. Is this gatekeeping, or is it a necessary evolution of our responsibility as developers? Let me know in the comments below.\n\nIs an AI able to write the contents of your article? Well, that was a question I had and wanted to find out. In this article I tell you all about it.\n\nDiscover the latest advancements in documentation technology and how tools like GitHub Copilot for Docs, Mendable, and OpenAI are changing the game.\n\nDiscover how to leverage Azure AI Translator's Sync API for real-time document translation, simplifying your workflow and enhancing user experience.\n\nFound a typo or issue in this article? Visit the GitHub repository \nto make changes or submit a bug report.\n\nSolutions Architect & Developer Expert\n\nEngage with your audience throughout the event lifecycle",
    "readingTime": 12,
    "keywords": [
      "visual studio",
      "product moral",
      "compass agent",
      "studio code",
      "edge cases",
      "necessary evolution",
      "vibe coding",
      "software development",
      "github copilot",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://www.eliostruyf.com/killing-indie-development-with-ai/",
    "thumbnail_url": "https://www.eliostruyf.com/social/5f59a11b-79bb-48df-9b89-b8abc9ba3037.png",
    "created_at": "2026-02-16T12:38:09.254Z",
    "topic": "tech"
  },
  {
    "slug": "kanvibe-kanban-board-that-autotracks-ai-agents-via-hooks",
    "title": "KanVibe – Kanban board that auto-tracks AI agents via hooks",
    "description": "Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking — manage tmux/zellij sessions and git worktrees from one board. - rookedsysc/kanvibe",
    "fullText": "rookedsysc\n\n /\n\n kanvibe\n\n Public\n\n Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking — manage tmux/zellij sessions and git worktrees from one board.\n\n License\n\n AGPL-3.0 license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n rookedsysc/kanvibe",
    "readingTime": 1,
    "keywords": [
      "board",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/rookedsysc/kanvibe",
    "thumbnail_url": "https://opengraph.githubassets.com/1686c5ce06bcd0be96aea5e2e16beffdd136eeb70f17f431b75349727b34dbe2/rookedsysc/kanvibe",
    "created_at": "2026-02-16T12:38:09.047Z",
    "topic": "tech"
  },
  {
    "slug": "fake-job-recruiters-hide-malware-in-developer-coding-challenges",
    "title": "Fake job recruiters hide malware in developer coding challenges",
    "description": "A new variation of the fake recruiter campaign from North Korean threat actors is targeting JavaScript and Python developers with cryptocurrency-related tasks.",
    "fullText": "A new variation of the fake recruiter campaign from North Korean threat actors is targeting JavaScript and Python developers with cryptocurrency-related tasks.\n\nThe activity has been ongoing since at least May 2025 and is characterized by modularity, which allows the threat actor to quickly resume it in case of partial compromise.\n\nThe bad actor relies on packages published on the npm and PyPi registries that act as downloaders for a remote access trojan (RAT). In total, researchers found 192 malicious packages related to this campaign, which they dubbed 'Graphalgo'.\n\nResearchers at software supply-chain security company ReversingLabs say that the threat actor creates fake companies in the blockchain and crypto-trading sectors and publishes job offerings on various platforms, like LinkedIn, Facebook, and Reddit.\n\nDevelopers applying for the job are required to show their skills by running, debugging, and improving a given project. However, the attacker's purpose is to make the applicant run the code.\n\nThis action would cause a malicious dependency from a legitimate repository to be installed and executed.\n\n\"It is easy to create such job task repositories. Threat actors simply need to take a legitimate bare-bone project and fix it up with a malicious dependency and it is ready to be served to targets,\" the researchers say.\n\nTo hide the malicious nature of the dependencies, the hackers host the dependencies on legitimate platforms, like npm and PyPi.\n\nIn one case highlighted in the ReversingLabs report, a package named ‘bigmathutils,’ with 10,000 downloads, was benign until it reached version 1.1.0, which introduced malicious payloads. Shortly after, the threat actor removed the package, marking it as deprecated, likely to conceal the activity.\n\nThe Graphalgo name of the campaign is derived from packages that have “graph” in their name. They typically impersonate legitimate, popular libraries like graphlib, the researchers say.\n\nHowever, from December 2025 onward, the North Korean actor shifted to packages with “big” in their name. However, ReversingLabs has not discovered the recruiting part, or the campaign frontend, related to them.\n\nAccording to the researchers, the actor uses Github Organizations, which are shared accounts for collaboration across multiple projects. They say that the GitHub repositories are clean, and malicious code is introduced indirectly via dependencies hosted on npm and PyPI, which are the Graphalgo packages.\n\nVictims running the project as instructed in the interview infect their systems with these packages, which install a RAT payload on their machines.\n\nIt is worth noting that ReversingLabs researchers identified several developers that fell for the trick and contacted them \n\nThe RAT can list the running processes on the host, execute arbitrary commands per instructions from the command-and-control (C2) server, and exfiltrate files or drop additional payloads.\n\nThe RAT checks whether the MetaMask cryptocurrency extension is installed on the victim’s browser, a clear indication of its money-stealing goals.\n\nIts C2 communication is token-protected to lock out unauthorized observers, a common tactic for North Korean hackers.\n\nReversingLabs has found multiple variants written in JavaScript, Python, and VBS, showing an intention to cover all possible targets.\n\nThe researchers’ attribute the Graphalgo fake recruiter campaign to the Lazarus group with medium-to-high confidence. The conclusion is based on the approach, the use of coding tests as an infection vector, and the cryptocurrency-focused targeting, all of which aligning with previous activity associated with the North Korean threat actor.\n\nAlso, the researchers note the delayed activation of malicious code in the packages, consistent with Lazarus' patience displayed in other attacks. Finally, the Git commits show the GMT +9 time zone, matching North Korea time.\n\nThe complete indicators of compromise (IoCs) are available in the original report. Developers who installed the malicious packages at any point should rotate all tokens and account passwords and reinstall their OS.\n\nModern IT infrastructure moves faster than manual workflows can handle.\n\nIn this new Tines guide, learn how your team can reduce hidden manual delays, improve reliability through automated response, and build and scale intelligent workflows on top of tools you already use.",
    "readingTime": 4,
    "keywords": [
      "korean threat",
      "fake recruiter",
      "recruiter campaign",
      "threat actors",
      "malicious dependency",
      "north korean",
      "threat actor",
      "malicious code",
      "researchers say",
      "malicious packages"
    ],
    "qualityScore": 1,
    "link": "https://www.bleepingcomputer.com/news/security/fake-job-recruiters-hide-malware-in-developer-coding-challenges/",
    "thumbnail_url": "https://www.bleepstatic.com/content/hl-images/2024/08/12/north-korean-hackers.jpg",
    "created_at": "2026-02-15T12:26:54.771Z",
    "topic": "tech"
  },
  {
    "slug": "remoteopencode-run-your-ai-coding-agent-from-your-phone-via-discord",
    "title": "Remote-OpenCode – Run your AI coding agent from your phone via Discord",
    "description": "Discord bot for remote OpenCode CLI access. Contribute to RoundTable02/remote-opencode development by creating an account on GitHub.",
    "fullText": "RoundTable02\n\n /\n\n remote-opencode\n\n Public\n\n Discord bot for remote OpenCode CLI access\n\n License\n\n MIT license\n\n 7\n stars\n\n 3\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n RoundTable02/remote-opencode",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/RoundTable02/remote-opencode",
    "thumbnail_url": "https://opengraph.githubassets.com/703cb5b3ac833c99f1222c718308e6f87968dda18183af7d724de78de94b9a70/RoundTable02/remote-opencode",
    "created_at": "2026-02-15T06:38:30.422Z",
    "topic": "tech"
  },
  {
    "slug": "claude-agent-in-vs-code-no-extension-required-copilot-subscription-supported",
    "title": "Claude Agent in VS Code: no extension required, Copilot subscription supported",
    "description": "Learn how to use third-party agents like Claude Agent and OpenAI Codex for autonomous coding tasks in VS Code, powered by your GitHub Copilot subscription.",
    "fullText": "Third-party agents in Visual Studio Code are AI agents developed by external providers, such as Anthropic and OpenAI. Third-party agents enable you to use the unique capabilities of these AI providers, while still benefiting from the unified agent sessions management in VS Code and the rich editor experience for coding, debugging, testing, and more. In addition, you can use these providers with your existing GitHub Copilot subscription.\n\nVS Code uses the provider's SDK and agent harness to access the agent's unique capabilities. You can use both local and cloud-based third-party agents in VS Code. Integration with cloud-based third-party agents is enabled through your GitHub Copilot plan.\n\nThird-party coding agents in the cloud are currently in preview.\n\nThe benefits of using third-party agents in VS Code are:\n\nClaude agent sessions provide agentic coding capabilities powered by Anthropic's Claude Agent SDK directly in VS Code. The Claude agent operates autonomously on your workspace to plan, execute, and iterate on coding tasks with its own set of tools and capabilities.\n\nEnable or disable support for Claude agent sessions with the github.copilot.chat.claudeAgent.enabledOpen in VS CodeOpen in VS Code Insiders setting.\n\nTo start a new Claude agent session:\n\nOpen the Chat view (⌃⌘I (Windows, Linux Ctrl+Alt+I)) and select New Chat (+).\n\nChoose between a local or cloud agent session:\n\nFor a local session, select Claude from the Session Type dropdown\n\nFor a cloud session, select Cloud from the Session Type dropdown. Then, select Claude from the Partner Agent dropdown.\n\nEnter your prompt and let the agent work on the task\n\nThe Claude agent autonomously determines which tools to use and makes changes to your workspace.\n\nThe Claude agent provides specialized slash commands for advanced workflows. Type / in the chat input box to see the available commands.\n\nClaude agent requests permission before performing certain operations. By default, file edits within your workspace are auto-approved, while other operations like running terminal commands might require confirmation.\n\nYou can choose how the agent applies changes to your workspace:\n\nThe github.copilot.chat.claudeAgent.allowDangerouslySkipPermissionsOpen in VS CodeOpen in VS Code Insiders setting bypasses all permission checks. Only enable this in isolated sandbox environments with no internet access.\n\nThe OpenAI Codex agent uses OpenAI's Codex to perform coding tasks autonomously. Codex runs can run interactively in VS Code or unattended in the background.\n\nOpenAI Codex in VS Code enables you to use your Copilot Pro+ subscription to authenticate and access Codex without additional setup. Get more information about GitHub Copilot billing and premium requests in the GitHub documentation.\n\nTo start a new OpenAI Codex agent session:\n\nOpen the Chat view (⌃⌘I (Windows, Linux Ctrl+Alt+I)) and select New Chat (+).\n\nChoose between a local or cloud agent session:\n\nFor a local session, select Codex from the Session Type dropdown\n\nFor a cloud session, select Cloud from the Session Type dropdown. Then, select Codex from the Partner Agent dropdown.\n\nEnter your prompt in the chat editor input and let the agent work on the task\n\nYes, third-party agents in VS Code authenticate and manage billing through your existing GitHub Copilot subscription. For cloud-based third-party agents, follow the steps to enable the agent.\n\nFor cloud-based third-party agents, availability might be limited based on your Copilot subscription plan. Check About Third-party agents in the GitHub documentation \n\nBoth the provider's VS Code extension and the third-party agent integration in VS Code let you use the provider's AI capabilities and agent harness. The difference is in billing: when you use third-party agents in VS Code, GitHub bills you through your Copilot subscription. When you use the provider's extension, you are billed through the provider's subscription.\n\nVS Code lets you choose between local and cloud-based third-party agents, depending on the provider's availability. When you select the third-party agent from the Session Type dropdown, a local agent session is created for that provider.\n\nTo choose a cloud-based third-party agent, first select the Cloud option from the Session Type dropdown, and then select the provider from the Partner Agent dropdown.",
    "readingTime": 4,
    "keywords": [
      "vs code",
      "view windows",
      "windows linux",
      "linux ctrl+alt+i",
      "chat view",
      "existing github",
      "github documentation",
      "dropdown enter",
      "session type",
      "copilot subscription"
    ],
    "qualityScore": 1,
    "link": "https://code.visualstudio.com/docs/copilot/agents/third-party-agents#_claude-agent-preview",
    "thumbnail_url": "https://code.visualstudio.com/assets/docs/copilot/shared/github-copilot-social.png",
    "created_at": "2026-02-14T12:25:09.835Z",
    "topic": "tech"
  },
  {
    "slug": "orangensaft-a-mini-pythonlike-language-with-llm-eval-in-lang-runtime",
    "title": "Orangensaft – A mini Python-like language with LLM eval in lang runtime",
    "description": "A new age post-AI programming language. Contribute to jargnar/orangensaft development by creating an account on GitHub.",
    "fullText": "jargnar\n\n /\n\n orangensaft\n\n Public\n\n A new age post-AI programming language\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jargnar/orangensaft",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/jargnar/orangensaft",
    "thumbnail_url": "https://opengraph.githubassets.com/fc5976b41bfb8b1ea8e07bed5ea5b7596101078e6c57161a28c5d0aea8c3d834/jargnar/orangensaft",
    "created_at": "2026-02-13T12:34:58.006Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-raises-30bn-in-latest-round-valuing-claude-bot-maker-at-380bn",
    "title": "Anthropic raises $30bn in latest round, valuing Claude bot maker at $380bn",
    "description": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (£22bn) in a funding round that more than doubled its valuation to $380bn.\nThe company’s previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n Continue reading...",
    "fullText": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\n\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (£22bn) in a funding round that more than doubled its valuation to $380bn.\n\nThe company’s previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n\nThe fundraising was announced amid a series of stock market moves against industries that face disruption from the latest models, including software, trucking and logistics, wealth management and commercial property services.\n\nThe funding round, led by the Singapore sovereign wealth fund GIC and the hedge fund Coatue Management, is among the largest private fundraising deals on record.\n\n“Anthropic is the clear category leader in enterprise AI,” said Choo Yong Cheen, the chief investment officer of private equity at GIC.\n\nAnthropic said its annualised revenue – an estimate of full-year sales based on recent company data – had reached $14bn, having grown more than tenfold in each of the past three years. A significant driver of recent growth has been Claude Code, the company’s AI-powered coding tool that became generally available in May 2025.\n\nAnthropic’s rival OpenAI, backed by Microsoft and SoftBank, has been assembling what is reportedly a far larger round of up to $100bn that would value the ChatGPT developer at about $830bn.\n\nThe staggering sums being raised reflect equally staggering burn rates, with the companies spending cash to cover their huge costs of computing and attracting researcher talent.\n\nAnthropic has forecast reducing its cash burn to roughly a third of revenue in 2026 and just 9% by 2027, with a break-even target of 2028 – two years ahead of its rival, according to reports. Both companies are widely expected to pursue initial public offerings in the second half of 2026.\n\nThe rapid valuation increases for leading AI startups such as Anthropic and OpenAI, whose price tags far exceed those of many of the US’s largest listed companies, has alarmed some observers. Last year, a leading British tech investor, James Anderson, said he found sharp increases in valuations of companies such as OpenAI and Anthropic “disconcerting”.\n\nSome listed firms at the forefront of the AI industry have also come under stock market pressure in recent days.\n\nShares in Alphabet, Google’s parent company, have fallen by 4.2% so far this week, indicating some investors are still spooked by the big AI-related spending plans it laid out this month. Meta has declined by 1.7% during this week. Shares in Nvidia, a leading chipmaker and key provider of AI infrastructure, dropped by 1.6% on Thursday amid a wider sell-off but have been flat on the week.\n\n“A gloomy session on Wall Street on Thursday put investors in a grumpy mood at the end of the trading week,” said Russ Mould, the investment director at investment platform AJ Bell.\n\n“Association with AI has gone from party to peril as investors reappraise what the technology means for companies. \n\n “Some are concerned about excessive levels of spending and others fear AI will disrupt multiple industries. It all adds up to a cocktail of worries and that’s bad for market sentiment more broadly,” Mould added.\n\nFounded in 2021 by the siblings Dario and Daniela Amodei, both former executives at OpenAI, Anthropic has positioned itself as a safety-focused alternative in the AI race.\n\nThe funding round also comes shortly after Anthropic’s first television commercials were broadcast during Super Bowl LX, using the campaign to emphasise that its products remain ad-free. The ads took an apparent jab at OpenAI, which has begun to introduce advertising into the free version of ChatGPT.\n\nAnthropic’s earlier backers include Amazon, which has invested $8bn and serves as a primary computing partner through its datacentres, as well as Google, which invested $2bn in 2023.\n\nAgence France-Presse contributed to this article",
    "readingTime": 4,
    "keywords": [
      "annualised revenue",
      "stock market",
      "funding round",
      "investment",
      "leading",
      "investors",
      "anthropic",
      "chatbot",
      "coding",
      "tenfold"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/12/anthropic-funding-round",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b30b904b79d26877d4b860af0a6c67c5b55a0067/735_0_2715_2172/master/2715.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d2f7a8edb124095d9242575a7c8b9ac3",
    "created_at": "2026-02-13T12:34:48.923Z",
    "topic": "tech"
  },
  {
    "slug": "these-y-combinator-founders-raised-10-million-to-get-corporate-america-into-vibe-coding-read-their-pitch-deck",
    "title": "These Y Combinator founders raised $10 million to get corporate America into vibe coding. Read their pitch deck.",
    "description": "Vybe, a startup by Y Combinator alumni, raises $10M to integrate vibe coding into corporations, with participation from major tech leaders.",
    "fullText": "Vybe, the startup cofounded by two repeat Y Combinator entrepreneurs, has raised $10 million to bring vibe coding inside large companies.\n\nThe practice of vibe coding has become more widespread as it allows people to use AI and natural language prompts to build an app, rather than traditional programming. The sector has attracted hundreds of millions in venture funding and is blurring the lines for businesses deciding between buying software or just vibe-coding their own tools.\n\nWhile popular vibe-coding products like Lovable and Replit have made it easy for companies to create prototypes and landing pages, Vybe offers stronger security that cannot be modified by AI and also taps into internal data systems, said cofounder and CEO Quang Hoang.\n\nHoang previously founded the engineering mentorship platform Plato and is building Vybe alongside cofounder and CTO Fabien Devos, founder of the security AI startup Wolfia.\n\nVybe lets teams across an organization collaborate on apps. Engineering teams manage access to internal systems like Salesforce, Snowflake, and Databricks, while business teams can build apps for onboarding, performance reviews, customer service, and more.\n\n\"The new SaaS is going to be more like Legos,\" Hoang said, referring to software-as-a-service. \"You can build it exactly how you want.\"\n\nVybe also offers app templates created by high-profile executives, such as former Airbnb product leader Lenny Rachitsky and Front cofounder Mathilde Collin, that users can \"remix\" to suit their needs, including templates for performance reviews and one-on-one meetings.\n\nFirst Round Capital led Vybe's seed round, which featured participation from Y Combinator, the CEOs of Datadog and Grammarly, and product leaders from OpenAI and Meta.\n\nVybe has six employees and dozens of customers, Hoang said. It plans to use the funding to hire engineers. Vybe charges $12 per user a month, plus usage-based credits for app development.\n\nThe round comes as vibe coding continues to attract major funding — with startups like Emergent pulling in tens of millions from Khosla Ventures and SoftBank — even as Barclays analysts warn the initial boom may be cooling.\n\nAt the same time, investors are also weighing how AI and vibe-coding tools can reshape the software sector. Software stocks have entered one of their sharpest downturns in years, shedding roughly $2 trillion in market capitalization.\n\nHere's a look at the pitch deck Vybe used to raise its $10 million seed round. One slide has been removed, and another redacted so that the deck can be shared publicly.",
    "readingTime": 3,
    "keywords": [
      "performance reviews",
      "vibe coding",
      "seed round",
      "funding",
      "software",
      "vibe-coding",
      "cofounder",
      "teams",
      "vybe",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/vybe-pitch-deck-expand-vibe-coding-corporate-world-2026-2",
    "thumbnail_url": "https://i.insider.com/698df18dd3c7faef0ece2588?width=948&format=jpeg",
    "created_at": "2026-02-13T01:15:39.236Z",
    "topic": "finance"
  },
  {
    "slug": "a-software-engineer-says-theres-a-vampiric-effect-to-ai-where-vibecoding-sprints-are-followed-by-naps-to-cope-with-ai",
    "title": "A software engineer says there's a 'vampiric effect' to AI, where vibe-coding sprints are followed by naps to cope with AI fatigue",
    "description": "Steve Yegge, who was at Amazon in the early days and spent 12 years at Google, says his fellow engineers need to learn to say no.",
    "fullText": "A seasoned veteran said his fellow software engineers need to learn \"how to say 'no,' real fast\" or risk getting crushed by AI.\n\nSteve Yegge, who worked with Jeff Bezos at Amazon early on before 12-year stint at Google, said AI is set up in a way that can really drain you.\n\n\"There's a vampiric effect with AI, where it gets you excited, and you work really hard, and you're capturing a ton of value,\" he recently told the \"The Pragmatic Engineer\" newsletter/podcast.\n\nYegge said companies also need to understand that while agentic AI may make engineers more productive than ever before, pushing the limit will just burn out their workforce.\n\n\"I seriously think founders and company leaders and engineering leaders at all levels, all the way down to line managers, have to be aware of this and realize that you might only get three productive hours out of a person who's vibe coding at max speed,\" he said. \"So, do you let them work for three hours a day? The answer is yes, or your company's going to break.\"\n\nEngineers are beginning to vocalize concerns about \"AI fatigue.\" Business Insider recently spoke to Siddhant Khare, who builds AI tools, and wrote an essay about how AI has accelerated the pace of his job to a point that it was burning him out.\n\nYegge said that his fellow engineers need to set boundaries when they are vibe coding.\n\n\"People have to learn the art of pushing back,\" he said.\n\nUntil then, Yegge said he and his fellow engineers are napping and growing grumpier.\n\n\"I find myself napping during the day, and I'm talking to friends at startups, and they're finding themselves napping during the day,\" he said. \"We're starting to get tired and cranky.\"",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "fellow engineers",
      "napping",
      "learn",
      "recently",
      "productive",
      "pushing",
      "leaders",
      "hours",
      "yegge"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/software-engineer-steve-yegge-ai-burnout-2026-2",
    "thumbnail_url": "https://i.insider.com/698cd767e1ba468a96abdee5?width=1200&format=jpeg",
    "created_at": "2026-02-12T12:39:46.962Z",
    "topic": "tech"
  },
  {
    "slug": "minions-stripes-oneshot-endtoend-coding-agents",
    "title": "Minions: Stripe's one-shot, end-to-end coding agents",
    "description": "Minions are Stripe’s homegrown coding agents, responsible for more than a thousand pull requests merged each week. Though humans review the code, minions write it from start to finish. Learn how they work, and how we built them.",
    "fullText": "Minions: Stripe’s one-shot, end-to-end coding agents/Article/About the authorAlistair GrayAlistair is a software engineer on the Leverage team at Stripe./DocsExplore our guides and examples to integrate Stripe.Learn more/SocialYoutubeTwitter/XDiscordDocsDeveloper Meetups© 2025 Stripe, Inc.PrivacyLegalStripe.com",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents",
    "thumbnail_url": "https://images.ctfassets.net/fzn2n1nzq965/1IiEdIR6LsEY9aym5JQM6u/85a9a55f58137d9d1fac99319658081b/dev_social.png",
    "created_at": "2026-02-12T06:50:09.787Z",
    "topic": "tech"
  },
  {
    "slug": "hes-worked-decades-in-tech-and-wrote-a-book-on-vibe-coding-he-predicts-50-of-big-tech-engineers-will-be-laid-off",
    "title": "He's worked decades in tech and wrote a book on vibe coding. He predicts 50% of Big Tech engineers will be laid off.",
    "description": "Steve Yegge worked at Amazon an Google. Now, he predicts Big Tech companies will cut half their engineers to help pay for the others to have AI.",
    "fullText": "He worked in Big Tech. Now, he fears half of its software engineers are on the chopping block.\n\nSteve Yegge worked with Jeff Bezos in Amazon's early years. Then he went to Google, where he worked for over 12 years and earned the title of senior staff software engineer. He also knows a thing or two about AI engineering, having written a book on the topic titled \"Vibe Coding.\"\n\nOn \"The Pragmatic Engineer\" podcast and newsletter, Yegge described an imaginary dial of the percentage of engineering staff a company can lay off, ranging from zero to 100. He said he believes the dial is being set at 50 in the age of AI.\n\n\"You're going to have to get rid of half of them to make the other half maximally productive,\" Yegge said. \"We're going to lose around half the engineers from big companies, which is scary\".\n\nThere's a capital-to-labor tradeoff happening in tech. Companies are paying vast sums for tokens and enterprise AI licenses, GPUs, and computing capacity. That money needs to come from somewhere — and for some, it could come from labor costs.\n\nYegge pointed to this tradeoff as the reason for his forecast of 50% cuts becoming the norm. Companies will lay off some engineers to help pay for the others to have adequate access to AI, he said.\n\nHost Gergely Orosz said that 50% cuts would be more than during the COVID-19 pandemic, when tech went through an intense layoff cycle.\n\n\"It's going to be way bigger,\" Yegge said. \"It's going to be awful.\"\n\nThey aren't the only ones referencing the pandemic. In a popular X article, HyperWrite CEO Matt Shumer wrote that AI's impact on work would be \"much, much bigger than Covid.\"\n\nIn recent years, the question of AI-driven layoffs has haunted workers at Big Tech companies, many of which went on a hiring spree during the pandemic. While it's often impossible to boil job loss down a single reason, many suspect productivity gains from AI and ballooning capex spending are fueling the cuts. Some business leaders have also explicitly cited AI when announcing layoffs.\n\nMeanwhile, tech leaders describe smaller teams working more efficiently. On a recent earnings call, Meta CEO Mark Zuckerberg said one engineer could now do the work of a whole team, thanks to AI.\n\nAs AI productivity grows, so have complaints of AI fatigue. Software engineer Siddhant Khare penned a lengthy essay about growing more productive, but also feeling more drained than ever. He told Business Insider that he felt like a \"reviewer\" as opposed to an engineer.\n\nYegge said it's not all bad news for the engineers — just those who want to work at a big company. Engineers who have \"seen the light\" are now getting together, leaving their companies, and creating startups that outpace the industry's giants, he said.\n\n\"We've got this mad rush of innovation coming up, bottom up,\" he said. \"And we've got knowledge workers being laid off by big companies because clearly big businesses are not the right size anymore.\"",
    "readingTime": 3,
    "keywords": [
      "software engineer",
      "engineers",
      "half",
      "it's",
      "cuts",
      "pandemic",
      "staff",
      "engineering",
      "dial",
      "productive"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/steve-yegge-vibecoding-author-predicts-layoffs-half-big-tech-engineers-2026-2",
    "thumbnail_url": "https://i.insider.com/698c9571e1ba468a96abd6bb?width=1200&format=jpeg",
    "created_at": "2026-02-12T01:12:22.798Z",
    "topic": "finance"
  },
  {
    "slug": "the-skill-nobody-tests-for",
    "title": "The Skill Nobody Tests For",
    "description": "Drop candidates into a real coding environment with AI tools. See who actually ships.",
    "fullText": "Drop candidates into a real coding environment with AI tools. See who actually ships.\n\nFree to start · Set up in 2 minutes\n\nChallenge: Build an RBAC permission system\n\nTools: Claude Code · VS Code · Terminal\n\nMeta, Shopify, Cisco — companies are dropping algorithm interviews. The question isn't whether this changes hiring. It's whether you move first.\n\nOver the last year, the best engineers have completely changed how they work. They don't write code by hand anymore, they talk to agents, interrogate them, make architectural decisions, scope projects, and verify output.\n\nA full VS Code workspace in the browser. Terminal, files, packages — everything your engineers use daily.\n\nClaude Code, Codex, Cursor, Copilot — the same tools your team uses. The challenge is using them well, not memorizing algorithms.\n\nAutomated tests validate the solution. You see what they built, how they got there, and whether it works.",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "tools",
      "terminal",
      "engineers",
      "challenge"
    ],
    "qualityScore": 0.75,
    "link": "https://vibearena.io/blog/the-skill-nobody-tests-for",
    "thumbnail_url": "https://vibearena.io/og-image.png",
    "created_at": "2026-02-11T12:42:57.921Z",
    "topic": "tech"
  },
  {
    "slug": "the-future-of-coding-agents-is-vertical-integration-and-why-acp-matters",
    "title": "The future of coding agents is vertical integration (and why ACP matters)",
    "description": "A look at the limitations of generic agents and MCPs, through the lens of building Tidewave, and what this means for the future of coding agents",
    "fullText": "The first appearances of coding agents were within our editors, as a natural progression from tab autocompletion. When Claude Code was released, almost a year ago, it innovated in many ways, particularly by demonstrating that although we write almost all of our code in editors, the agents themselves don’t have to be constrained to IDEs. As a result, most of our agentic development moved to the terminal.\n\nWith the release of the Codex App, we will start to see a push towards richer interfaces. However, this raises the question: if we can run coding agents anywhere, where is the best place to run them?\n\nIn this section, I will talk about Tidewave, the coding agent we built for full-stack web development. It is a sales pitch but I hope it is a good example of what we can gain by building vertical coding agents.\n\nTidewave was born out of frustration when using coding agents to build web applications. I frequently stumbled upon scenarios such as:\n\nThe agent would tell me a feature was complete, but when I tried it in the browser, form submission would not complete\n\nWhenever I encountered an exception page during development, I had to copy and paste stacktraces from the browser to the agent\n\nI had to constantly translate what was on the page to source code. If I wanted to “Add an Export to CSV button to a dropdown button”, the first step was typically to find where the template or component the dropdown was defined in\n\nUltimately, because the coding agent was unable to interact with or see what it produced, I had to act as the middleman:\n\nAnd while things have improved within the last year, most of the tooling out there does not fully close this gap. For example, while coding agents can read logs and editors like Cursor do include a browser, they do not correlate the browser actions with the logs, leaving space for guess work and context bloat. Or when the browser renders a page, it doesn’t know which controller or template it came from.\n\nWe solved this problem by moving the coding agent to the browser and making it aware of your web app:\n\nYou open up Tidewave in your favorite browser and its agent can now interact with the page you are looking at, validating the application works as desired by accessing the DOM, loading pages, filling forms, etc\n\nWhen something goes wrong, we parse and feed the exact information from the rendered error page. We also include any console.log and server logs related to the particular agent action (instead of the whole history)\n\nWhen you inspect an element in Tidewave, we automatically map DOM elements to source files. If you inspect a dropdown, we tell the agent its location on the page as well as the template/component it came from in the source code\n\nThe coding agent has access to your backend too, so it can read documentation, execute SQL queries, and even gets a REPL-like environment. So when the agent gets stuck, it can navigate the data, run code, and explore APIs, like any developer would\n\nMore importantly, this is all implemented by connecting the agent to your running application. You can ask your agent to analyze cache usage in your Rails application or to debug a LiveView in Phoenix, and it can do so because they talk to each other directly. Your agent is no longer limited to a series of bash commands.\n\nOverall, the agent becomes more capable, and we no longer need to act as the middleman:\n\nFor all of the above to work, instead of building a generic agent that works with every tool, Tidewave directly integrates with each web framework we support (Django, FastAPI, Flask, Next.js, Phoenix, Rails, and TanStack Start), mapping all layers to the agent, from the database to UI.\n\nAnd that’s what I mean by vertical integration: coding agents become integrated into the specific platform or runtime they’re building for. They understand the relationship between code and its live behavior, and can debug both.\n\nWe can easily draw parallels between the above and other domains. Agents for mobile development must have full access to a simulator to interact with the app UI, translate view hierarchies to source code, and monitor network requests. Similar patterns could apply to IoT devices, game development, etc.\n\nAs a matter of fact, this isn’t even a new idea. Data scientists were the first to embed agents directly into their notebooks, where they can execute code, visualize results, inspect dataframes, etc. They were the first to realize the benefit of vertically integrating agents and everyone else is just late to the party.\n\nBut how can we make these vertical integrations possible? If we want agents to access our runtimes and environments, could MCPs be the tool to make this a reality? That’s what we originally thought…\n\nTidewave was first prototyped as a Model Context Protocol (MCP) server and it quickly became obvious it wasn’t enough. In this section, I’ll focus on how MCPs can be detrimental to user experience. In particular, we’ll look at its limitations as a pull-only interface and its text-based constraints.\n\nAs an example, let’s look at Figma’s MCP. It allows developers to select an element in Figma’s desktop app and ask the agent to implement the design. Here’s a screenshot from their announcement:\n\nAs you can see, you’re required to make a selection in Figma, then explicitly ask the agent to query your selection. The agent then proceeds to talk to Figma and get the relevant data. That’s a lot of unecessary back and forth! If I already selected an element on Figma, it should take a single click to embed that information into my prompt.\n\nNow imagine if, every time I inspected an element in the browser, we had to tell the agent to ask the browser about the selection. We refused to implement that. Instead, we made it just work:\n\nFurthermore, by making the inspected elements part of our prompt, you can select multiple elements and dictate how they all need to change at once. You can reorder, swap, and coordinate multiple changes easily.\n\nSimilarly, when you see an error page in the browser, you should only need to click a button to fix it - not copy and paste stacktraces or ask the agent to read logs.\n\nNone of this could be built with MCPs because they are pull-based. The agent decides when to invoke the MCP and you are forced to ask the agents to perform actions on your behalf. MCPs do not allow us to attach rich metadata to prompts either.\n\nImagine you need help addressing comments in a pull request. You can tell the agent to use GitHub’s MCP or even use a custom skill that uses GitHub’s CLI to fetch all current comments. But once the agent does its initial assessment, all further exchanges happens through text: “ignore this comment”, “no, not this comment, the one above it”.\n\nNow compare that to using any graphical (or even a terminal) interface: we can reply in thread, comment on specific lines, or click a button to dismiss invalid feedback altogether. In other words, when we put our integrations behind MCPs, all exchange happens on text, and we often end-up with worse user experiences.\n\nInstead, our goal is to build agentic tools and user experiences side-by-side. For example, when we added Web Accessibility diagnostics to Tidewave, we exposed additional tools to the agent, but we also provided a polished experience for when developers want to remain in the loop:\n\nOur recently announced Supabase integration works the same: it runs performance and security advisors on your database and present them to you. If you want to dig deeper, you can do it all through a rich interface. And if you don’t care about any of that, you can either click the “Fix all” button or ask your agent to do it for you.\n\nTo build these experiences, we had to invert the control. We don’t want the agent to call us, we want our tools to call the agent.\n\nSay you want to build your own agentic loop, up until recently, doing so meant to:\n\nWhile the above is already a lot of work, you must remember that prompt engineering, tool calls, etc. must be fine-tuned per model. For example, when OpenAI announced GPT-5-Codex, integrators had to write new prompts and new tools, even if they already supported GPT-5.\n\nAnd the above covers only the basic loop! What about AGENTS.md, MCPs, or skills? And you haven’t even started working on what makes your coding agent unique.\n\nLuckily for us, many of the leading AI companies have built their own coding agents (Claude Code, Codex, Gemini CLI) tailored to their models, and many have exposed those agents through SDKs. We also have fantastic open source alternatives. This means you can cut out much of the work above, but, if you want to support multiple providers, you’re still on the hook for integrating them one by one.\n\nThat’s exactly where the Agent Client Protocol from the Zed team comes in. It standardizes communication with coding agents, so you implement one protocol and support multiple agents at once. This ultimately allows you to focus on the runtime integrations and user experiences that make your coding agent unique.\n\nFurthermore, even if you’re not implementing your own agent, ACP should matter to you because it brings portability to developers. You can use a single coding agent across the terminal, your editor, and Tidewave, reusing the same settings for hooks, skills, and subagents. And perhaps more importantly, the same subscription.\n\nWe are big fans of the Agent Client Protocol and we are excited to build on top of it alongside companies like Zed and JetBrains.\n\nGeneric agents that work everywhere force awkward workflows where we constantly act as a middleman. Instead, we want to embed agents into the platforms we are building on, so they can see the relationship between code and behavior, interact with live apps, and debug in real-time.\n\nHowever, to build these rich experiences, we need to invert the control and own the agentic loop. The Agent Client Protocol (ACP) allows us to do so by standardizing communication across providers, so we can focus on the user experiences and integrations that make our coding agents unique, instead of wrestling with multiple SDKs.\n\nI hope this article inspires you to fine-tune your own coding agents and, if you are looking for a coding agent tailored to full-stack web development, give Tidewave a try!",
    "readingTime": 9,
    "keywords": [
      "client protocol",
      "claude code",
      "paste stacktraces",
      "agent client",
      "full-stack web",
      "error page",
      "user experiences",
      "agentic loop",
      "web development",
      "embed agents"
    ],
    "qualityScore": 1,
    "link": "https://tidewave.ai/blog/the-future-of-coding-agents-is-vertical-integration",
    "thumbnail_url": "https://tidewave.ai/assets/opengraph/pthe-future-of-coding-agents-is-vertical-integration-4pklmn64vmtcox2ktoauj7nxoy-a84d118a102998c2aa4a25c686d9af29.png?vsn=d",
    "created_at": "2026-02-11T01:19:44.413Z",
    "topic": "tech"
  },
  {
    "slug": "this-ceo-wants-ai-agents-to-outnumber-his-human-employees-this-year",
    "title": "This CEO wants AI agents to outnumber his human employees this year",
    "description": "The CEO of StackBlitz, Eric Simons, says the web development startup has \"gone all in\" on AI agents.",
    "fullText": "StackBlitz CEO Eric Simons aims to have more AI agents than human employees working at the startup this year, a milestone he sees as emblematic of a broader shift underway across the software industry.\n\nSpeaking in an interview with Business Insider, Simons said StackBlitz has \"gone all in on agents,\" deploying internally built AI systems across business intelligence, coding, product development, customer support, and outbound sales.\n\nAI has become increasingly good at writing software code, to the point where the technology is beginning to change how companies are run. OpenClaw, an open-source AI assistant that works inside platforms like WhatsApp, Slack, and iMessage, is an early example of digital agents communicating and coordinating with one another without direct human involvement.\n\n\"To me, this is a crystal ball into the wildness of the inevitable future,\" Simons told Business Insider. \"Your AI agents will talk to other people's agents on your behalf, negotiating pricing for a product you want to buy, inquiring about availability for a restaurant, arguing your political viewpoints.\"\n\n\"Agents are an extension of yourself,\" he added. \"People will generally trust their agents with whatever they recommend them to buy, reserve, believe, or otherwise.\"\n\nSimons's comments come as software and SaaS stocks have slumped in recent weeks, a move he attributes to growing investor recognition that AI can now build software autonomously. As AI tools become more capable, he said, long-standing competitive moats based on specialized knowledge are eroding.\n\nHe compared the shift to the transformation of manufacturing over the past century. Craft knowledge once protected businesses, he said, but automation and digital design ultimately displaced many incumbents. \"Today, it's a CAD file you can 3D print,\" Simons said.\n\n\"While every individual does not use these profound new powers directly (like 3D printing a chair), there's an entire new generation of companies that do — and they have replaced the previous era of companies,\" Simons explained. \"They disrupted those incumbents by leveraging automation that made it far cheaper and at a massive scale not possible previously when moats were just knowledge and bare hands.\"\n\nFor businesses, the implications are stark. Even traditionally \"safe\" enterprise software may be vulnerable if AI agents can migrate or rebuild systems rapidly.\n\n\"What does it mean when all of software can be written, rewritten, migrated, or otherwise, 100x or 10,000x faster than it could've ever been done before, by a workforce that does not sleep, and can be parallelized near infinitely?\" Simons said. \"It's daunting and mind-bending, and I think the repricing of SaaS in public markets more accurately reflects this today.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "agents",
      "software",
      "knowledge",
      "human",
      "shift",
      "across",
      "systems",
      "product",
      "digital"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/tech-ceo-ai-agents-outnumber-human-employees-eric-simons-stackblitz-2026-2",
    "thumbnail_url": "https://i.insider.com/698b6915a645d11881890f3a?width=1200&format=jpeg",
    "created_at": "2026-02-11T01:19:42.667Z",
    "topic": "finance"
  },
  {
    "slug": "composer-15",
    "title": "Composer 1.5",
    "description": "Improved reasoning over challenging coding tasks by scaling RL over 20x.",
    "fullText": "A few months ago, we released our first agentic coding model, Composer 1. Since then, we've made significant improvements to the model’s coding ability.\n\nOur new release, Composer 1.5, strikes a strong balance between speed and intelligence for daily use. Composer 1.5 was built by scaling reinforcement learning 20x further on the same pretrained model. The compute used in our post-training of Composer 1.5 even surpasses the amount used to pretrain the base model.\n\nWe see continued improvements on coding ability as we scale. Measured by our internal benchmark of real-world coding problems, we find that the model quickly surpasses Composer 1 and continues to climb in performance. The improvements are most significant on challenging tasks.\n\nComposer 1.5 is a thinking model. In the process of responding to queries, the model generates thinking tokens to reason about the user’s codebase and plan next steps. We find that these thinking stages are critical to the model’s intelligence. At the same time, we wanted to keep Composer 1.5 fast and interactive for day-to-day use. To achieve a balance, the model is trained to respond quickly on easy problems with minimal thinking, while on hard problems it will think until it has found a satisfying answer.\n\nTo handle longer running tasks, Composer 1.5 has the ability to self-summarize. This allows the model to continue exploring for a solution even when it runs out of available context. We train self-summarization into Composer 1.5 as part of RL by asking it to produce a useful summary when context runs out in training. This may trigger several times recursively on hard examples. We find that self-summarization allows the model to maintain its original accuracy as context length varies.\n\nComposer 1.5 is a significantly stronger model than Composer 1 and we recommend it for interactive use. Its training demonstrates that RL for coding can be continually scaled with predictable intelligence improvements.\n\nLearn more about Composer 1.5 pricing here.",
    "readingTime": 2,
    "keywords": [
      "tasks composer",
      "coding ability",
      "model",
      "improvements",
      "intelligence",
      "context",
      "model’s",
      "balance",
      "surpasses",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/composer-1-5",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/composer-1.5-og.png",
    "created_at": "2026-02-10T06:54:10.475Z",
    "topic": "tech"
  },
  {
    "slug": "a16z-partner-says-that-the-theory-that-well-vibe-code-everything-is-flat-wrong",
    "title": "A16z partner says that the theory that we'll vibe code everything is 'flat wrong'",
    "description": "Vibe coding everything is just not worth it, says A16z partner Anish Acharya.",
    "fullText": "Vibe coding everything is just not worth it, says an Andressen Horowitz partner.\n\nOn an episode of the \"20VC\" podcast released on Monday, A16z general partner Anish Acharya said that companies shouldn't use AI-assisted coding for every part of their business.\n\nHe said that software accounts for 8% to 12% of a company's expenses, so using vibe coding to build the company's resource planning or payroll tools would only save about 10%. Relying on AI to write code also carries risks, he said.\n\n\"You have this innovation bazooka with these models. Why would you point it at rebuilding payroll or ERP or CRM,\" Acharya said, referring to enterprise resource planning and customer relationship management software. Salesforce, Microsoft, Oracle, and SAP are among the top providers of such software.\n\nInstead, companies are better off using AI to develop their core businesses or optimize the remaining 90% of their costs, said the venture capitalist of six years.\n\n\"Of course, there will be secular losers. There are specific business models that are now going to be disadvantaged,\" he said. \"But the general story that we're going to vibe code everything is flat wrong, and the whole market is oversold software.\"\n\nAcharya's comments follow a brutal week for software stocks, which dragged down tech and broader markets. The sell-off started when already-wary investors panicked about Anthropic's new AI tool, which can perform a range of clerical tasks for people working in the legal industry.\n\nThe A16z partner joins famed investor Vinod Khosla in saying that stock prices should be ignored when evaluating the future of tech companies.\n\nOn a podcast last month, Khosla dismissed talks of an AI bubble and said investors should not be concerned as long as API call volume, a benchmark of AI usage, remains high.\n\n\"If that's your fundamental metric of what's the real use of your AI, usefulness of AI, demand for AI, you're not going to see a bubble in API calls,\" he said. \"What Wall Street tends to do with it, I don't really care. I think it's mostly irrelevant.\"",
    "readingTime": 2,
    "keywords": [
      "resource planning",
      "vibe coding",
      "software",
      "partner",
      "everything",
      "podcast",
      "business",
      "company's",
      "payroll",
      "code"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/a16z-partner-says-businesses-cannot-vibe-code-everything-tech-stocks-2026-2",
    "thumbnail_url": "https://i.insider.com/698a9944a645d118818905c1?width=1200&format=jpeg",
    "created_at": "2026-02-10T06:54:04.849Z",
    "topic": "finance"
  },
  {
    "slug": "i-made-a-claude-code-guide-thats-a-win95-desktop-with-games",
    "title": "I made a Claude Code guide that's a Win95 desktop with games",
    "description": "A free interactive guide for PMs and CEOs leading dev teams through the AI coding transition. Score your codebase readiness, calculate ROI, and get a 90-day rollout plan.",
    "fullText": "Your developers have Copilot open. They autocomplete functions and paste errors into chat. That gets you maybe 10% of what AI coding tools can do now. The real shift is agentic: one engineer running 3-5 parallel AI sessions that write features, run tests, and open PRs.\n\nSeven chapters. Three interactive tools. One honest answer about your team's speed:\n\nMost teams think they're running Pentium. They're a 486.\n\nTechnical deep-dives are marked 📎 so you know what to forward to your engineering lead.",
    "readingTime": 1,
    "keywords": [
      "tools",
      "they're"
    ],
    "qualityScore": 0.55,
    "link": "https://gabezen.com/guide/",
    "thumbnail_url": "https://gabezen.com/guide/og.png",
    "created_at": "2026-02-10T01:21:45.458Z",
    "topic": "tech"
  },
  {
    "slug": "clog-track-and-compare-your-claude-code-usage",
    "title": "Clog – Track and compare your Claude Code usage",
    "description": "Track your Claude Code journey. Public leaderboard and profile system for developers. See how your coding sessions stack up, build streaks, and share your progress.",
    "fullText": "[join][--]clog - Claude Code Leaderboard// track your claude code sessions// leaderboard goes live soon_ready",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "leaderboard"
    ],
    "qualityScore": 0,
    "link": "https://clog.sh",
    "thumbnail_url": "https://clog.sh/clog-og-image.png",
    "created_at": "2026-02-10T01:21:44.351Z",
    "topic": "tech"
  },
  {
    "slug": "factory-factory-opensource-alternative-to-codex-app-for-claude",
    "title": "Factory Factory, open-source alternative to Codex App for Claude",
    "description": "Workspace-based coding environment for running multiple Claude Code sessions in parallel. - purplefish-ai/factory-factory",
    "fullText": "purplefish-ai\n\n /\n\n factory-factory\n\n Public\n\n Workspace-based coding environment for running multiple Claude Code sessions in parallel.\n\n factoryfactory.ai\n\n License\n\n MIT license\n\n 18\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n purplefish-ai/factory-factory",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/purplefish-ai/factory-factory",
    "thumbnail_url": "https://opengraph.githubassets.com/72f2d9d9898e13bed120d015134a857e14f77267ee4cd373e64559450fa53038/purplefish-ai/factory-factory",
    "created_at": "2026-02-09T12:42:23.472Z",
    "topic": "tech"
  },
  {
    "slug": "logifai-autocapture-dev-logs-for-ai-coding-assistants",
    "title": "Logifai – Auto-capture dev logs for AI coding assistants",
    "description": "Auto-capture development logs for Claude Code — stop copy-pasting terminal output. - tomoyaf/logifai",
    "fullText": "tomoyaf\n\n /\n\n logifai\n\n Public\n\n Auto-capture development logs for Claude Code — stop copy-pasting terminal output.\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n tomoyaf/logifai",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/tomoyaf/logifai",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1152746807/546b13ec-53a4-4596-a481-7a6c1210694a",
    "created_at": "2026-02-09T06:54:27.333Z",
    "topic": "tech"
  },
  {
    "slug": "i-see-how-important-ai-is-at-google-so-i-taught-my-kids-about-it-now-theyre-vibe-coding",
    "title": "I see how important AI is at Google, so I taught my kids about it. Now, they're vibe coding.",
    "description": "A Google executive explains why he taught his kids to understand AI — and how they ended up vibe coding and participating in a hackathon.",
    "fullText": "This as-told-to essay is based on a conversation with Asif Saleem, a financial services go-to-market lead for Japan and the Asia Pacific region at Google. Asif is the father of 13-year-old Usman Asif and 18-year-old Shanzey Asif. It's been edited for length and clarity.\n\nIt's important for parents to help kids understand we're going through a very transformative time. This is like the era when the internet or mobile first emerged. With AI, it may be even bigger.\n\nKids need to understand this and embrace the technology. Whatever they end up studying — computer science, English, or philosophy — they can make AI part of it.\n\nI was curious about Cursor and some of the other vibecoding tools, so I joined a few \"Code with AI\" weekend sessions for executives.\n\nI really enjoyed it. Within a matter of hours, we were able to develop different applications. I developed a statement analyzer for a financial services system.\n\nOnce I came back home, I spoke to my family about it and showed them the demo. My children, Usman and Shanzey, are both tech-savvy, so that drove their interest. They attended the same course a few weeks later.\n\nThey were the youngest in the class. The good part was that they were completely independent, so I let them be on their own. That's how they ventured into vibe coding and participated in Cursor's 24-hour weekend hackathon in Singapore.\n\nThey've become more curious about things. Through vibe coding, they've learned to be more creative and to use technology to understand how things work.\n\nNeither of them has a formal technology background — they're not software developers — but they've been able to think through ideas, be more creative, and use technology to solve problems.\n\nTechnology is the biggest enabler. The question is how humans use it — whether for good or bad. If it's for good, what are the guardrails?\n\nUsing it for good means creating value, solving real challenges, and making information more accessible.\n\nManaging screentime is the reason we developed a reward system for Usman because he's young and he loves gaming. Gaming comes as a reward for achieving goals. Those goals include making your own breakfast, making your bed, or helping clean the house.\n\nWhen Shanzey is studying, she can't use AI for the content she's creating — it can't even be AI-inspired. It has to be her original work.\n\nThat's super important because schools validate whether output is AI-generated. The same applies to Usman.\n\nIt's important to have both physical and digital skills. Usman plays football because physical activity is important at his age. Shanzey is focused on school right now — her exams are very important, and getting good grades matters.\n\nI'd say my role is more like a coach — brainstorming with them. My wife does the same, helping them think through ideas and keeping them honest.\n\nI'm often busy with work, so a lot of this couldn't be done without my wife's help.\n\nThere's nothing better than hands-on learning. That's something I've learned over the last few years.\n\nYou will face challenges and go through them. Parents should get their kids to do things, give them challenges, and nurture them as they work through those challenges. With hands-on experience, they can get better results faster.\n\nWith building an app, we co-create the idea, but then the important part is making it happen and reporting back on progress.\n\nAt Google, I work with enterprise customers trying to transform their businesses with AI. I can see what's happening on the ground and how things are changing.\n\nI also see a lot of young talent at Google. They come in thinking about creating apps and learning skills, and I mentor some of them as well.\n\nIt's important to communicate this to my family. I spent time helping them understand how the world is changing and why it's important for them to understand AI.\n\nIt's not about running away from technology. AI will keep advancing, and the only thing you can do is be accustomed to it, no matter what you want to become.\n\nWe're now in a situation where we have very intelligent large language models, and we're also moving toward agentic AI, where you can work with agents that help you do a lot more. Speed and agility are improving, and you can now work within larger ecosystems that combine humans and machines, achieving much more.\n\nIf you know how to coexist and let machines do meaningful work with you, that's a skill to aspire to.\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "financial services",
      "vibe coding",
      "technology",
      "understand",
      "that's",
      "challenges",
      "kids",
      "we're",
      "creating",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-ai-teach-teenagers-vibe-coding-kids-family-tech-2026-2",
    "thumbnail_url": "https://i.insider.com/6954c24104eda4732f2e4c0d?width=1200&format=jpeg",
    "created_at": "2026-02-09T06:54:22.473Z",
    "topic": "finance"
  },
  {
    "slug": "the-guy-who-coined-vibecoding-says-the-next-big-thing-is-agentic-engineering",
    "title": "The guy who coined 'vibe-coding' says the next big thing is 'agentic engineering'",
    "description": "OpenAI cofounder Andrej Karpathy says 'agentic engineering is the next evolution in AI coding as vibe-coding marks its first anniversary.",
    "fullText": "\"Vibe-coding\" just celebrated its first birthday. That's a lifetime in the AI boom.\n\nNow, the man who coined the term is celebrating the birth of a new one: \"agentic engineering.\"\n\nWhile vibe-coding is when humans prompt AI to write code, OpenAI cofounder Andrej Karpathy says agentic engineering is when AI agents write the code themselves.\n\n\"Many people have tried to come up with a better name for this to differentiate it from vibe coding, personally, my current favorite is 'agentic engineering,'\" he wrote in a recent X post.\n\nKarpathy said he calls it \"agentic engineering\" not just because agents are writing the code, but because \"there is an art & science and expertise to it.\"\n\nVibe-coding is one of the biggest innovations of the AI revolution. Prominent CEOs and startup founders alike are encouraging the use of vibe-coding across their teams. And billions are being poured into new vibe-coding companies.\n\nLovable, one of Europe's fastest-growing startups, announced that it had raised $330 million in Series B funding at a $6.6 billion valuation in December. Cursor, an AI-assisted code editor, announced a Series D funding round of $2.3 billion in November and said it had surpassed $1 billion in annualized revenue.\n\nThe approach is also threatening traditional engineering jobs. In a Business Insider survey of 167 software engineers, 75 engineers said that they were \"keeping up,\" while 30 said they felt ahead of the curve, and 27 felt behind.\n\n\"Vibe coding is now mentioned on my Wikipedia as a major memetic 'contribution,' and even its article is longer. lol,\" Karpathy wrote on X about its meteoric rise.\n\nKarpathy was a founding member of OpenAI in 2015, years before competitors like Anthropic and xAI emerged. He later moved into self-driving technology, leading Tesla's Autopilot program as head of AI. He's now building Eureka Labs, which describes itself on its website as building a \"new kind of school that is AI native.\"",
    "readingTime": 2,
    "keywords": [
      "series funding",
      "vibe coding",
      "agentic engineering",
      "vibe-coding",
      "code",
      "openai",
      "agents",
      "engineers",
      "karpathy"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/agentic-engineering-andrej-karpathy-vibe-coding-2026-2",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-02-09T01:12:58.161Z",
    "topic": "finance"
  },
  {
    "slug": "gitbutler",
    "title": "GitButler",
    "description": "GitButler software development platform",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://gitbutler.com/",
    "thumbnail_url": "https://gitbutler.com/og-image.png",
    "created_at": "2026-02-08T18:22:32.834Z",
    "topic": "tech"
  },
  {
    "slug": "prepare-your-oss-repo-for-ai-coding-assistants",
    "title": "Prepare your OSS repo for AI coding assistants",
    "description": "I’ve been seeing more and more open source maintainers throwing [...]",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://angiejones.tech/stop-closing-the-door-fix-the-house/",
    "thumbnail_url": "https://angiejones.tech/wp-content/uploads/2026/02/open-source-is-closed.png",
    "created_at": "2026-02-08T18:22:32.063Z",
    "topic": "tech"
  },
  {
    "slug": "an-identity-verification-ai-startup-is-having-all-employees-try-vibe-coding",
    "title": "An identity verification AI startup is having all employees try vibe coding",
    "description": "A San Francisco AI startup is giving out stipends and dedicating project days for non-tech staff to vibe code apps that help with their workflow.",
    "fullText": "If you work for a San Francisco startup and don't know how to code, you could soon be asked to get creative with vibe coding.\n\nCheckr, an AI-powered background check company, gave Business Insider a glimpse of how employees are actually using AI.\n\nCheckr CEO Daniel Yanisse said that the company is going \"all in\" and trying everything to encourage its employees to fully embrace AI — including staff that don't work in engineering roles.\n\n\"We really pride ourselves on using AI to the maximum possible amount,\" said Yanisse. \"We gave every employee a monthly stipend to try AI tools, and we did AI days and demos. After one year, 95% of the employees use prompting daily.\"\n\n\"This year, we're going to level up and move to building with AI, as in vibe coding,\" Yanisse added. \"I'm working with all of our teams now, and we're going to do our AI days soon in March, where we're going to make every non-technical person vibe code their own business apps.\"\n\nYanisse said that many employees who have no idea how to code, who work in finance, legal, and HR, are already vibecoding apps to automate their workflows and problem-solve, such as building tools that help clean up large spreadsheets.\n\nWhile Checkr is evaluating a variety of builder tools like Lovable, Replit, and Claude Code, Yanisse said Cursor is a clear standout and \"has amazing adoption\" among both engineers and non-technical staff, but Lovable is the best place to start for people with no coding experience.\n\n\"Probably, we're going to buy all of them and just use the right tool for the right person,\" Yanisse said of different AI coding tools.\n\n\"We have AI solution engineers who are available to actually partner and help, so they would come and help you and unstuck you if you have a problem, and take you all the way to success,\" Yanisse added. \"And then you're on your way because then we share success stories with everyone in the company.\"\n\nIn practice, data shows that AI adoption can be complicated in a large enterprise. Competence with AI tools can be very uneven across the board, and security risks can mount without clear guidelines on AI usage.\n\nAccording to a survey published in November by Moveworks, an AI-powered platform that automates IT and HR support, most executives said that non-technical employees are playing a bigger role in driving AI use, and that 78% have seen successful AI projects originate directly from support staff looking to solve daily challenges.\n\nThe National Cybersecurity Alliance also wrote in its Annual Cybersecurity Attitudes and Behaviors Report that AI adoption has surged to 65% globally as of the end of 2025, but more than half of these AI users never received any training in privacy and security risks. The report surveyed over 6,500 workers worldwide.\n\n\"A few years ago, most businesses were still debating whether AI was something they really needed,\" Louis Riat-Bonello of Optisearch, an AI-powered marketing platform that specializes in SEO, told Business Insider.\n\n\"The businesses getting the best results aren't blindly chasing automation. They're using AI to support smarter decisions, move faster, and free up teams to focus on strategy and creativity,\" Riat-Bonello added. \"That balance is what will matter long after the hype fades.\"\n\nYanisse said that in the age of AI, the company is looking for creative and adaptable people, because while AI will eliminate some roles, it will create others.\n\n\"We are constantly training and helping people update their skills and careers,\" said Yanisse. \"The job of the product designer and the job of the marketer are all completely shifting right now.\"\n\n\"We're over 900 people, so we're not a small startup, but I'm a startup guy, and I'm a builder,\" Yanisse added. \"The people who come here need to be OK with uncertainty, be self-driven, adaptable, flexible, willing to do new things, and solve new problems without too much guidance or structure.\"",
    "readingTime": 4,
    "keywords": [
      "security risks",
      "vibe coding",
      "we're",
      "employees",
      "tools",
      "code",
      "startup",
      "ai-powered",
      "yanisse",
      "staff"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/checkr-non-technical-employees-vibecode-with-stipends-and-ai-days-2026-2",
    "thumbnail_url": "https://i.insider.com/6983e6bda645d1188188b87e?width=1200&format=jpeg",
    "created_at": "2026-02-07T12:25:49.164Z",
    "topic": "finance"
  },
  {
    "slug": "what-i-wish-i-knew-before-building-a-vibe-coding-platform",
    "title": "What I wish I knew before building a vibe coding platform",
    "description": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.",
    "fullText": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.\n\nI'm Ariel, VP of AI at Appwrite. Over the past year, my team and I built Imagine — a vibe-coding platform that lets users build production-ready web apps using prompting. That means real backends, server-side rendering, server functions, sandboxes, previews, the whole thing.\n\nAppwrite is one of the most impactful open-source projects in the world, and in the spirit of open-source, we want to share our learnings with the community. This article isn't a how-to. It's a collection of hard-earned lessons — the kind you only learn after shipping, breaking things, and paying real infrastructure bills.\n\nIf you don't take prompt caching seriously from day one, you will burn money and time. This is especially true for vibe-coding platforms.\n\nVibe coding platforms naturally rely on long-running agentic loops. Agents talk to themselves, call tools, revise plans, generate files, run tests — sometimes over dozens of steps and millions of tokens. That's unavoidable if you want high-quality results.\n\nThe good news: with correct prompt caching, this doesn't have to be expensive or slow.\n\nHow you structure and order your prompts has a massive impact on cache hit rates. In practice, almost everything should be cacheable.\n\nWe're huge fans of Anthropic, and they allow you to define explicit cache breakpoints. For example:\n\nFirst breakpoint: after fully static system messages. Core instructions, rules, policies — things that never change.\n\nSecond breakpoint: after semi-dynamic system messages. Generation number, project metadata, decisions made by earlier agents.\n\nFinal breakpoint: right before the latest user or agent message.\n\nThis structure allows long agentic loops to reuse the majority of their context even when the workflow branches or retries. If you place breakpoints carelessly, a single conditional change can invalidate your entire cache.\n\nThis sounds unrealistic at first, but it's absolutely achievable.\n\nA good benchmark is 90–95% cache hit rate on input tokens. At that point, the economics completely change:\n\nThis is the difference between a platform that feels sluggish and expensive, and one that feels snappy and scalable.\n\nMake sure you know how to observe caching behavior. Anthropic does an excellent job here — their console shows per-request caching status and aggregated cache metrics over time. Anthropic's documentation for prompt caching is state-of-the-art and everyone should read it.\n\nPro tip: You might be tempted to use cheaper models for certain tasks in your workflow, thinking it would be faster or cheaper. With Anthropic for example, cache is per model, so plugging that Sonnet call at the end of your mostly-Opus workflow would result in a cache miss. Do caching right and you can leverage the best models for the job without breaking the bank.\n\nThe average request we make to Anthropic is 99% cached. Screenshot taken from Anthropic's console.\n\nMost frameworks and SDKs teach you the same thing:\n\nSend a request, get a response, stream tokens back to the client.\n\nThat's fine for demos. It completely breaks down for vibe coding.\n\nIn a vibe coding platform where state is fragile (sandbox, repo, uncommitted changes, ongoing generation, etc.), so much could go wrong.\n\nTo deal with these issues, we have implemented resumable streams and durable workflows.\n\nAssume you have a /api/chat endpoint where the client submits a prompt. Typically, tutorials would teach you to stream the chunks back to the client (for example, using Server-Sent events, or SSE in short).\n\nThat's simply putting too much hope in HTTP connections in a scenario where generations can take minutes to complete. Here is a good approach to dealing with this:\n\nIn our case, we serve the stream via a separate service altogether to reduce load on our core AI API, but that's not strictly necessary. Redis becomes the source of truth for in-progress generations.\n\nStreaming alone isn't enough. You also need durable execution for the workflow itself.\n\nInstead of one long-running function, break the process into orchestrated steps:\n\nSandbox provisioning is a good example. In our platform, there are multiple ways a sandbox might be started:\n\nWith durable workflows, all of these can safely funnel into the same sandbox orchestration logic without race conditions or duplication.\n\nWe chose to use Inngest which is open-source, but there are other good options like Temporal, AWS Lambda Durable Functions, Trigger.dev, etc.\n\nAs a bonus point, using Inngest gives us great observability, overview of execution times and alerts on anomalies. It forces us to adopt a more resilient and reliable approach to building our platform, and everybody gets to sleep better at night.\n\nInngest makes it easy to visualize the timeline of each generation and its steps.\n\nWhen building a vibe-coding platform, you have a fundamental choice:\n\nDo I let the system generate anything, or do I force generations to use a specific framework or stack?\n\nWe decided to go with the second approach.\n\nWhen evaluating frameworks, we asked ourselves these questions:\n\nAfter evaluating a few common options, we ultimately chose TanStack Start. It's built by the team behind some of the most popular tools and libraries powering the React ecosystem. TanStack Start uses Vite as its build system, which is highly customizable.\n\nFor our generated apps' runtime, we chose Bun. It's incredibly fast, supports running TypeScript directly (handy for migrations), and is pretty much a drop-in replacement for Node.js.\n\nConcretely, we are able to build our generated apps (server and client code) in ~1 second. For comparison, a fresh Next.js + Turbopack + Bun build takes a few seconds, and that's without any server code. The difference is huge.\n\nImagine's generated apps build in ~1 second, including both client and server code\n\nGenerative AI is inherently non-deterministic. You can prompt all you want, provide examples, two-shot it, five-shot it. But as your message history grows, your context grows as well, and so does the likelihood of facing hallucinations or simply failing to get the model to adhere to your instructions.\n\nWhen users report issues, 90% of those have to do with unexpected AI behavior or issues in the generated code. It's very tempting to take every such issue in isolation and try to fix it by adding an additional bullet point to the system prompt, or providing more examples.\n\nBut wait. There might be a better way. The deterministic way. Here are some examples from Imagine:\n\nThese are just a few examples, but the key takeaway is that you can't rely on the LLM to adhere to your instructions. Whenever you can, embrace determinism.\n\nLet your AI see the same red squiggly lines you appreciate so much.\n\nThe real challenges only show up once you're dealing with real workloads, real users, and real infrastructure bills. By then, it's often too late to \"just refactor\" your way out of architectural decisions made early on.\n\nPrompt caching, durable workflows, deterministic guardrails, and a well-chosen framework should not be an afterthought, or by-the-way optimizations. They are foundational.\n\nAt Imagine, our mission is to tame AI, make the best of it and mitigate its weaknesses. We build Imagine assuming things will fail, disconnect, retry, and resume — unexpectedly.\n\nAs we build Imagine, we are constantly learning, and we are excited to share our learnings with the community. If you're building something similar, we hope these hard-earned lessons help you move quickly and avoid the pitfalls.",
    "readingTime": 7,
    "keywords": [
      "infrastructure bills",
      "agentic loops",
      "hard-earned lessons",
      "vibe coding",
      "system messages",
      "durable workflows",
      "generated apps",
      "cache hit",
      "server code",
      "vibe-coding platform"
    ],
    "qualityScore": 1,
    "link": "https://imagine.dev/blog/post/what-i-wish-i-knew",
    "thumbnail_url": "/images/blog/what-i-wish-i-knew/cover.png",
    "created_at": "2026-02-06T18:34:47.896Z",
    "topic": "tech"
  },
  {
    "slug": "tech-stack-is-a-business-decision",
    "title": "Tech stack is a business decision",
    "description": "Why tech stack choices should be driven by business context and constraints—not framework preferences—and why this matters even more with agentic coding tools.",
    "fullText": "Developers love to argue about tech stacks.\n\nFlutter vs React Native.\n\nNative vs cross-platform.\n\nJava vs Ruby vs C#.\n\nReact vs Angular.\n\nRiverpod vs Bloc.\n\nDrift vs Hive.\n\nThese debates can go arbitrarily deep, down to library-level choices that have little impact on whether a product succeeds or fails.\n\nEveryone has preferences, and that’s fine. Preferences come from experience. Someone who started with strongly typed languages will value compiler guarantees and explicitness. Someone who started with dynamic languages will value flexibility and speed. Both viewpoints are internally consistent.\n\nWhat they are not is universally correct.\n\nThere is no hard data that can rank one stack as objectively “better” across all contexts. Teams, constraints, markets, and goals differ too much. Most arguments rely on anecdotes, and anecdotes do not generalize.\n\nSo if preference is not a reliable guide, how should a tech stack be chosen?\n\nThis question is often interpreted at the feature or use-case level. That misses the point.\n\nIn almost all non-hobby software, the problem being solved is a business problem.\n\nIf there is no business, no users, no revenue, then the software is irrelevant. It may be elegant or technically impressive, but it has no economic value. Software only matters insofar as it supports a business outcome.\n\nThat framing matters because it establishes the correct hierarchy.\n\nTechnology is not the goal.\n\nTechnology is a tool.\n\nAnd tools are chosen based on the job they need to do.\n\nEarly on, nobody knows whether the business will work.\n\nYou don’t know if users care. You don’t know if they will pay. You don’t know if the problem is real or imagined.\n\nAt this stage, obsessing over the “right” stack is misplaced effort. The dominant constraint is uncertainty, not scalability or architectural purity.\n\n“The FIRST rule of enterprenuership is you use what you have” - Alex Hormozi\n\nThis is simple rule of entrepreneurship applies surprisingly well to engineering.\n\nFounders and early teams start with what they already know. Not because it is theoretically optimal, but because it minimizes friction. Familiar tools reduce cognitive overhead, increase speed, and allow faster learning.\n\nEarly success is rarely determined by technical elegance. It is determined by whether you can ship something people want before you run out of time or money.\n\nAs the business grows, the constraints change.\n\nRevenue appears.\n\nUsers depend on the system.\n\nThe team grows.\n\nThe cost of failure increases.\n\nNow the stack begins to matter more. Not because some technologies are inherently superior, but because the software must support new business realities. Hiring, onboarding, operational cost, maintainability, performance characteristics, and risk management all become relevant.\n\nThis is where teams often conclude that an early stack choice was “wrong.” In most cases, it wasn’t wrong. It was right for the phase the business was in. The mistake was assuming it would remain right indefinitely.\n\nTwitch is often mentioned as a scaling story, but the more interesting part is how long the architecture stayed simple.\n\nAt one point, Twitch was serving millions of users with a monolithic Ruby on Rails application and a single PostgreSQL database. That setup was not elegant or future proof, but it worked. It allowed the team to move quickly, ship features, and grow the business without adding unnecessary complexity.\n\nOver time, the constraints changed. Growth amplified failures. Database contention increased. Reliability, support load, and churn started to matter more than raw development speed. The cost of incidents became visible in a way it had not been before.\n\nThat was when the architecture began to evolve. The database was scaled and restructured, services were split out, and new infrastructure and languages were introduced. None of this happened because the original stack was flawed. It happened because the business had outgrown it.\n\nThe technology changed in response to the business, not the other way around.\n\nIf you want to read more about how Twitch migrated from a monolith to microservices, see Breaking the Monolith at Twitch and Breaking the Monolith at Twitch: Part Two.\n\nIf you want to know more about how Twitch scaled its database, see How Twitch Uses PostgreSQL.\n\nAgentic coding tools make this even more important.\n\nTools like Claude Code and Cursor reduce the advantage of deep, framework-specific familiarity. Generating boilerplate, navigating unfamiliar code, applying patterns across modules, and keeping implementation consistent are no longer manual, error-prone tasks.\n\nWhen an agent can help you stay productive across technologies, the cost of switching or extending a stack drops. What remains expensive is not syntax or APIs, but domain knowledge, product decisions, and the long-term behavior of the system.\n\nIn other words, the more the tooling levels the technical playing field, the more tech stack decisions become about business fit.\n\nInstead of asking whether one technology is better than another in the abstract, ask questions that reflect the reality of the business:\n\nWhat are we optimizing for right now? Speed, cost, reliability, hiring, or time-to-market?\n\nHow quickly do we need to validate this idea?\n\nHow expensive will change be later, and what changes are most likely?\n\nWhat kind of team will maintain this system?\n\nWhat risks could realistically kill the business, and does the stack reduce or increase them?\n\nOnce framed this way, stack decisions become clearer and less emotional. Sometimes multiple choices are equally valid. Sometimes the boring option is correct. Sometimes speed matters more than correctness. Sometimes correctness matters more than speed.\n\nThere is no universal answer. The answer depends on the context.\n\nMost tech stack arguments are debates about taste, not outcomes.\n\nOnce technology is treated as a business decision rather than a personal identity, those arguments lose their intensity. The question stops being “what do I prefer?” and becomes “what does the business need right now?”\n\nThe right stack is not the one that wins debates.\n\nIt is the one that helps the business survive and grow at its current stage.\n\nIf are starting a project and need help with tech stack decision, connect with me on LinkedIn and I’ll take it from there.\n\nSoftware Developer specializing in Flutter, Android, and mobile development.\n Writing about code, architecture, and developer productivity.",
    "readingTime": 6,
    "keywords": [
      "tech stack",
      "stack decisions",
      "business",
      "speed",
      "technology",
      "users",
      "tools",
      "database",
      "debates",
      "languages"
    ],
    "qualityScore": 1,
    "link": "https://dinkomarinac.dev/blog/tech-stack-is-a-business-decision/",
    "thumbnail_url": "https://dinkomarinac.dev/og/tech-stack-is-a-business-decision.png",
    "created_at": "2026-02-06T12:35:39.062Z",
    "topic": "tech"
  },
  {
    "slug": "how-exposed-are-software-stocks-to-ai-tools-we-put-vibecoding-to-the-test",
    "title": "How exposed are software stocks to AI tools? We put vibe-coding to the test",
    "description": "How real is the AI threat to software companies? CNBC put it to the test by vibe-coding a Monday.com replacement.",
    "fullText": "Software, legal services and video games stocks have been selling off in recent weeks on fears that new AI features and tools could wipe them out. But how real is that threat? We decided to find out.\n\nCNBC's Deidre Bosa and I used Anthropic's AI coding tool \"Claude Code\" with the goal of creating a replacement for Monday.com, a project management platform with a $5 billion market cap. We didn't expect to get anywhere — we're not developers and we don't have any coding experience. But we have become adept at vibe-coding tools, which are AI tools that can build functioning apps based on commands in plain English from users, including those with limited technical chops.\n\nWe started out simple, telling Claude to build a project management dashboard similar to Monday, with features like multiple project boards, assigned team members and a status dropdown. It spit out a working prototype in minutes.\n\nWe then asked Claude to research Monday on its own, identify main features and recreate them. It added a number of other features, including a calendar.\n\nThe real magic happened when we connected the clone to an email account, essentially spinning up a customized project manager for our personal lives. The AI found Dee's forgotten calendar invite for a kid's birthday party (which she definitely didn't have a gift for yet) and it added reminders to book tickets for an upcoming trip and sign a waiver for another kid's birthday party.\n\nIt took us under an hour, and had we been paying users, it would have cost something like $5 to $15 in compute credits, depending on how much we went back and forth with the AI agent. As more data centers get built out, that cost could start to come down.\n\nSo which companies should investors worry about? Silicon Valley insiders we talk to say the most exposed names are the ones that \"sit on top of the work\" — tools like Atlassian, Adobe, HubSpot, Zendesk, and Smartsheet, that aren't core to businesses.\n\nThey say cybersecurity stocks like CrowdStrike and Palo Alto are harder to disrupt since they have network effects that no one would want to try to replicate and maintain.\n\nSystems of record may be safer, but not immune. Salesforce, for instance, anchors a business with enterprise data, making it harder to clone with a weekend coding project.\n\nWith the wholesale sell-off in software this year, that may be a chance for investors to separate between the need-to-haves and nice-to-haves.",
    "readingTime": 3,
    "keywords": [
      "kid's birthday",
      "birthday party",
      "project management",
      "features",
      "tools",
      "coding",
      "software",
      "stocks",
      "didn't",
      "users"
    ],
    "qualityScore": 1,
    "link": "https://www.cnbc.com/2026/02/05/how-exposed-are-software-stocks-to-ai-tools-we-tested-vibe-coding.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108184664-1754999709333-gettyimages-2214520187-img_1248.jpeg?v=1754999753&w=1920&h=1080",
    "created_at": "2026-02-06T06:40:18.191Z",
    "topic": "finance"
  },
  {
    "slug": "termoil-terminal-dashboard-for-managing-parallel-ai-coding-agents",
    "title": "Termoil – Terminal dashboard for managing parallel AI coding agents",
    "description": "Less friction for your multi-agent workflow. Contribute to fantom845/termoil development by creating an account on GitHub.",
    "fullText": "fantom845\n\n /\n\n termoil\n\n Public\n\n Less friction for your multi-agent workflow\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n fantom845/termoil",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/fantom845/termoil",
    "thumbnail_url": "https://opengraph.githubassets.com/3f4c60f9807e7fd26c4cd7d8070eb9f12a1c2217f6b59354a3d271fb86812016/fantom845/termoil",
    "created_at": "2026-02-06T06:40:14.221Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-ai-ceo-says-vibe-coding-is-making-software-easier-to-replace",
    "title": "Microsoft AI CEO says vibe coding is making software easier to replace",
    "description": "Microsoft AI CEO says vibe coding is lowering the barrier to building apps, raising questions about how defensible software will be.",
    "fullText": "Mustafa Suleyman says vibe coding is rapidly lowering the barrier to building apps, a shift that could put traditional software at risk.\n\nThe Microsoft AI CEO said in an episode of the \"Exponential View\" podcast published Thursday that AI tools now make it possible for anyone to quickly start launching code and apps.\n\n\"It is so accessible now,\" said Suleyman. \"You can watch a three-minute video, get spun up, launch one of these things.\"\n\n\"You can create an app, a web app in seconds,\" he added.\n\nSuleyman said people don't need deep technical skills to get started. Instead, they can learn by experimenting, watching, and doing.\n\nThe AI can \"build something that you may have thought was never possible,\" he said.\n\n\"Unless you push these things to their edges and explore the boundaries, you won't really understand the magic, or what they're kind of bad at.\"\n\n\"Everyone's got to get stuck into that motion,\" he added.\n\nSuleyman also said he has vibe-coded a system that tracks the DJs he wants to see, coming concerts and festivals, and then matches those events with his travel schedule. What used to be manual work now runs automatically in a spreadsheet that updates throughout the year.\n\nSuleyman's comments come as investors grow increasingly anxious that AI tools and agents could wipe out entire categories of software.\n\nThat fear flared this week after Anthropic said it was adding legal-focused capabilities to its Cowork assistant. The tools would allow AI to review legal documents and track compliance — work typically done by legal software.\n\nMarkets didn't take it lightly. Shares of legal-software companies in Europe and the US fell sharply on Tuesday, before the selling spread across the wider software sector and into tech.\n\nOpenAI triggered a similar sell-off months earlier after rolling out internal AI-powered software-as-a-service tools.\n\nMany of the tools now unsettling the tech sector were built using AI coding tools.\n\nAI personal assistant OpenClaw was created with the help of AI, while Moltbook — a viral, Reddit-style forum for AI agents — was entirely vibe-coded.\n\nAnthropic also said last month that it built its Cowork assistant using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. During a livestream, Rieseberg said his team put Cowork together in just over a week, thanks to Claude.\n\nTech leaders and developers have also said that such a turnaround is becoming the norm.\n\nPeter Steinberger, the developer behind OpenClaw, said in an episode of \"Behind the Craft\" podcast published last week that AI now lets developers \"build everything.\"\n\nOpenAI's chair, Bret Taylor, said in an episode of the \"Big Technology Podcast\" published last week that building software quickly through vibe coding will soon feel routine rather than novel.\n\nBut the real question, Taylor said, is what software still matters.\n\nInstead of dashboards, browser forms, and traditional apps, he expects AI agents to become the dominant software interface.\n\n\"Who's making those agents is the question,\" he said. \"Will you buy those agents off the shelf or build them yourself?\"",
    "readingTime": 3,
    "keywords": [
      "cowork assistant",
      "vibe coding",
      "podcast published",
      "software",
      "tools",
      "agents",
      "apps",
      "episode",
      "traditional",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/microsoft-ai-ceo-vibe-coding-software-replace-apps-mustafa-suleyman-2026-2",
    "thumbnail_url": "https://i.insider.com/6985593bd3c7faef0ecdbefa?width=1200&format=jpeg",
    "created_at": "2026-02-06T06:40:12.752Z",
    "topic": "tech"
  },
  {
    "slug": "a-very-small-sat-solver-from-haskell-now-in-dafny-proved-correct-with-llms",
    "title": "A Very Small SAT Solver (From Haskell) Now in Dafny, Proved Correct with LLMs",
    "description": "Dafny for Metatheory of Programming Languages. Contribute to namin/dafny-sandbox development by creating an account on GitHub.",
    "fullText": "Skip to content\n\n You signed in with another tab or window. Reload to refresh your session.\n You signed out in another tab or window. Reload to refresh your session.\n You switched accounts on another tab or window. Reload to refresh your session.\n\nDismiss alert\n\n namin\n\n /\n\n dafny-sandbox\n\n Public\n\n You can’t perform that action at this time.",
    "readingTime": 1,
    "keywords": [
      "window reload",
      "another tab",
      "refresh",
      "session",
      "signed"
    ],
    "qualityScore": 0.3,
    "link": "https://github.com/namin/dafny-sandbox/blob/master/Sat.dfy",
    "thumbnail_url": "https://opengraph.githubassets.com/37f757a4876729fbb57a662e8bc8730df5cfc0bc7de82e6b36f6b3d99b39b44a/namin/dafny-sandbox",
    "created_at": "2026-02-06T01:06:52.715Z",
    "topic": "tech"
  },
  {
    "slug": "openais-new-model-leaps-ahead-in-coding-capabilitiesbut-raises-unprecedented-cybersecurity-risks",
    "title": "OpenAI’s new model leaps ahead in coding capabilities—but raises unprecedented cybersecurity risks",
    "description": "Why OpenAI’s latest coding breakthrough is forcing the company to rethink how—and how fast—it can deploy its most powerful models.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/05/openai-gpt-5-3-codex-warns-unprecedented-cybersecurity-risks/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/08/GettyImages-2214110034.jpg?resize=1200,600",
    "created_at": "2026-02-06T01:06:50.989Z",
    "topic": "business"
  },
  {
    "slug": "programming-your-own-modern-bbs-with-python",
    "title": "Programming Your Own Modern BBS with Python",
    "description": "Previously, we looked at dialling BBS with Vice C64 and the C64U. Now, let's code a modern BBS from scratch using Python!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://retrogamecoders.com/programming-bbs-with-python/",
    "thumbnail_url": "https://retrogamecoders.com/wp-content/uploads/2026/02/custom-bbs-server-in-python.jpg",
    "created_at": "2026-02-05T18:36:01.035Z",
    "topic": "tech"
  },
  {
    "slug": "the-coding-agent-os",
    "title": "The Coding Agent OS",
    "description": "We believe that a great coding agent isn't a simple chatbot—it's an operating system for the model.",
    "fullText": "Most coding agents are presented as a chat UI glued to an LLM and integrated into an IDE or\n CLI. This can work really well for developers who provide the development environment along\n with constant babysitting to build functional software.\n\nCharlie, however, is built around a different premise: We believe that a great coding agent\n isn't a simple chatbot—it's an operating system for the model.\n\nCharlie's \"OS layer\" gives a model the primitives required to complete end-to-end\n engineering work:\n\nWhen agents run in an operating system, they can diagnose → plan → implement → verify →\n publish reviewable artifacts all within the systems where your team already works.\n\nWe've got two hot takes on why the OS is essential for great agents:\n\nCLI expressiveness beats bespoke \"LLM tools.\" CLI tools are composable (|, &&, xargs), self-documenting (--help), and match\n how engineers actually work. One-off JSON tools tend to be isolated and require tool\n designers to predict every use case up front.\n\nOpen-loop orchestration beats rigid workflows. Real engineering has surprises:\n flaky tests, missing context, failing builds, CI-only failures, new constraints. The \"OS loop\"\n lets the agent decide the next step based on what it observes, instead of forcing every task into\n a fixed graph.\n\nIDE-native assistants like Claude Code are great for interactive, in-editor work: drafting,\n refactors, quick iteration. But they're not optimized to serve as a team-facing execution\n environment.\n\nOS-style agents are built for that missing layer: cross-tool orchestration + durable,\n reviewable artifacts.\n\nA mention, review request, regression, or scheduled trigger.\n\nCharlie normalizes + enriches the signal\n\nInto \"this is the work request, with relevant context.\"\n\nDecide whether to act, what thread it belongs to, and which capabilities are allowed.\n\nA durable Task is created (or appended-to)\n\nSingle-winner claiming, enters a durable, turn-based loop.\n\nAssemble context, decide + plan, call the LLM, dispatch tool calls, observe results +\n check mailbox/cancel + continue.\n\nBack to the source system (PRs, reviews, issue updates, summaries).\n\nRecord terminal status + telemetry\n\nDurable transcript/state remains; ephemeral compute is cleaned up.\n\nThe agentic operating system delivers better outcomes:",
    "readingTime": 2,
    "keywords": [
      "reviewable artifacts",
      "operating system",
      "agents",
      "durable",
      "tools",
      "context",
      "coding",
      "environment",
      "agent",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://charlielabs.ai/coding-agents-as-operating-systems/",
    "thumbnail_url": "https://charlielabs.ai/_astro/caos-architecture.JbhqvI6s.png",
    "created_at": "2026-02-05T12:36:57.286Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-engineering",
    "title": "Agentic Engineering",
    "description": "Agentic Engineering is a disciplined approach to AI-assisted software development that emphasizes human oversight and engineering rigor, distinguishing it fr...",
    "fullText": "A year ago, Andrej Karpathy coined “vibe coding” to describe a gleefully reckless way of programming: you prompt, hand the keyboard to an AI, accept everything it spits out, don’t read the diffs, iterate by pasting error messages back in. It was a great label for a real thing - building quick prototypes or MVPs on pure AI autopilot.\n\nThe problem is that “vibe coding” has become a suitcase term. People now use it to describe everything from a weekend hack to a disciplined engineering workflow where AI agents handle implementation under human oversight. These are fundamentally different activities, and conflating them is causing real confusion - and real damage.\n\nVibe coding means going with the vibes and not reviewing the code. That’s the defining characteristic. You prompt, you accept, you run it, you see if it works. If it doesn’t, you paste the error back and try again. You keep prompting. The human is a prompt DJ, not an engineer.\n\nIf vibe coding gives millions of people the ability to create custom software who otherwise couldn’t, that’s a genuine win. The technique has a legitimate place in the toolbox.\n\nBut the failure modes are well-documented at this point. The pattern is always the same: it demos great, then reality arrives. You try to modify it, scale it, or secure it, and you discover nobody understands what the code is actually doing. As one engineer put it, “This isn’t engineering, it’s hoping.”\n\nHere’s the thing: a lot of experienced engineers are now getting massive productivity gains from AI - 2x, 5x, sometimes more - while maintaining code quality. But the way they work looks nothing like vibe coding. They’re writing specs before prompting. They’re reviewing every diff. They’re running test suites. They’re treating the AI like a fast but unreliable junior developer who needs constant oversight. I’ve personally liked “AI-assisted engineering” and have talked about how this describes that end of the spectrum where the human remains in the loop.\n\nSimon Willison (whose work I adore) proposed “vibe engineering” for this - it reclaims “vibe” while adding “engineering” to signal discipline. But after watching the community debate this for months, I think the the word “vibe” carries too much baggage. It signals casualness. When you tell a CTO you’re “vibe engineering” their payment system, you can see the concern on their face.\n\nAndrej Karpathy suggested “agentic engineering” this week and I think I like it.\n\nIt describes what’s actually happening. You’re orchestrating AI agents - coding assistants that can execute, test, and refine code - while you act as architect, reviewer, and decision-maker. You might write only a % of the code by hand. The rest comes from agents working under your direction. That’s agentic. And you’re applying engineering discipline throughout. That’s engineering.\n\nIt’s professionally legible. “Agentic engineering” sounds like what it is: a serious engineering discipline involving autonomous agents. You can say it to your VP of Engineering without embarrassment. You can put it in a job description. You can build a team practice around it.\n\nIt draws a clean line. Vibe coding = YOLO. Agentic engineering = AI does the implementation, human owns the architecture, quality, and correctness. The terminology itself enforces the distinction.\n\nThe workflow isn’t complicated, but it requires discipline that vibe coding explicitly abandons:\n\nYou start with a plan. Before prompting anything, you write a design doc or spec - sometimes with AI assistance. You break the work into well-defined tasks. You decide on the architecture. This is the part vibe coders skip, and it’s exactly where projects go off the rails.\n\nYou direct, then review. You give the AI agent a well-scoped task from your plan. It generates code. You review that code with the same rigor you’d apply to a human teammate’s PR. If you can’t explain what a module does, it doesn’t go in.\n\nYou test relentlessly. The single biggest differentiator between agentic engineering and vibe coding is testing. With a solid test suite, an AI agent can iterate in a loop until tests pass, giving you high confidence in the result. Without tests, it’ll cheerfully declare “done” on broken code. Tests are how you turn an unreliable agent into a reliable system.\n\nYou own the codebase. You maintain documentation. You use version control and CI. You monitor production. The AI accelerates the work, but you’re responsible for the system.\n\nTeams doing this well often report faster development - and those gains come from augmenting a solid process, not abandoning one. The AI handles boilerplate and grunt work. The human focuses on architecture, correctness, edge cases, and long-term maintainability.\n\nThe irony is that AI-assisted development actually rewards good engineering practices more than traditional coding does. The better your specs, the better the AI’s output. The more comprehensive your tests, the more confidently you can delegate. The cleaner your architecture, the less the AI hallucinates weird abstractions. As one analysis noted, “AI didn’t cause the problem; skipping the design thinking did.”\n\nHere’s an uncomfortable truth from the trenches: agentic engineering disproportionately benefits senior engineers. If you have deep fundamentals - you understand system design, security patterns, performance tradeoffs - you can leverage AI as a massive force multiplier. You know what good code looks like, so you can efficiently review and correct AI output.\n\nBut if you’re junior and you lean on AI before building those fundamentals, you risk a dangerous skill atrophy. You can produce code without understanding it. You can ship features without learning why certain patterns exist. Several engineering leaders have flagged this as an emerging crisis: a generation of developers who can prompt but can’t debug, who can generate but can’t reason about what they’ve generated.\n\nThis isn’t an argument against AI-assisted development. It’s an argument for being honest about what it demands. Agentic engineering isn’t easier than traditional engineering - it’s a different kind of hard. You’re trading typing time for review time, implementation effort for orchestration skill, writing code for reading and evaluating code. The fundamentals matter more, not less.\n\nThe trajectory is clear: AI agents are getting more capable, and the agentic engineering workflow is becoming default for a growing number of professional developers. This is going to accelerate.\n\nThe rise of AI coding doesn’t replace the craft of software engineering - it raises the bar for it. The developers who’ll thrive aren’t the ones who prompt the fastest. They’re the ones who think the clearest about what they’re building and why, then use every tool available - including AI agents - to build it well.\n\nVibe coding showed us what’s possible when you drop all conventions.\n\nNow it’s time to bring the engineering back. Let’s call that what it is.\n\nI’ve written a new book with O’Reilly, Beyond Vibe Coding, that goes deeper into practical frameworks for AI-Assisted (and agentic) engineering. If you’ve been figuring this out in your own workflow, I’d love to hear what’s working.",
    "readingTime": 6,
    "keywords": [
      "ai-assisted development",
      "vibe coding",
      "agentic engineering",
      "engineering workflow",
      "engineering it’s",
      "engineering discipline",
      "andrej karpathy",
      "the ai",
      "code",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/agentic-engineering/",
    "thumbnail_url": "https://addyosmani.com/assets/images/agentic-engineering.jpg",
    "created_at": "2026-02-05T12:36:55.943Z",
    "topic": "tech"
  },
  {
    "slug": "mark-zuckerbergs-2004-coding-jams-were-loud-emo-and-very-on-brand",
    "title": "Mark Zuckerberg's 2004 coding jams were loud, emo, and very on brand",
    "description": "Mark Zuckerberg published a playlist of five songs he was pumping while building Facebook in 2004. It includes hard-rocking hits and some synth funk.",
    "fullText": "Mark Zuckerberg is feeling nostalgic.\n\nOn the 22nd anniversary of Facebook's founding, Zuckerberg reposted a meme about the fresh-faced Harvard student \"getting ready to make history.\" Then he linked a playlist titled \"2004 Facebook coding jams.\"\n\nZuckerberg said he \"had these bangers on repeat\" while building Facebook. The playlist's cover features the young founder — long before his bulked-up rebrand — listening to music on headphones.\n\nThe playlist's five songs feature a fair share of emo wails, electric guitars, and even some funky synthesizers. Some of them have receded into the tides of music history; others remain classics for weddings and bar mitzvahs.\n\nThey're also evocative of a quieter, easier time in tech. There were no culture wars, cage matches, or capex worries. What would a 2004 Zuckerberg think of $10 billion data centers?\n\nMusic speaks to how we feel. Here's what these five songs might say about a 19-year-old Zuckerberg — and just how far he's come.\n\nZuckerberg liked hard rock, it seems.\n\n\"Headstrong\" is a loud, thrashing song that feels built to be played on \"Guitar Hero.\" There's even a good guttural scream in there before one chorus. The lyrics are equally attacking. \"Back off, I'll take you on,\" Chris Taylor Brown sings.\n\nThe opening of \"Headstrong\" sounds like UFC walkout music, hyping up a big fight. That's now a favorite sport of Zuckerberg's. Some things never change.\n\n\"Like a Stone\" is about death and religion — two subjects that Zuckerberg has long been interested in.\n\nFrontman Chris Cornell prays to God and angels on their deathbed that they will get into heaven. \"In dreams until my death / I will wander on,\" he sings.\n\nZuckerberg seems more interested in extending human life than forecasting its end. His philanthropic initiative invests heavily in drug and disease research. He also has an intense fitness routine, including jiu-jitsu and CrossFit routines.\n\nAs for faith, Zuckerberg said in 2020 that he had \"become more religious.\" He cited two sources: The issues his company has faced in the prior years and the birth of his daughters.\n\nMostly, though, \"Like A Stone\" is another huge rock hit. It seems like Zuckerberg enjoyed the sound of banging drums and wailing guitar.\n\nNow this one is a nostalgia trip.\n\nYou can't help but feel something when Douglas Robb croons out, \"I'm not a perfect person.\" Was Zuckerberg a perfect person? Most people who have watched \"The Social Network\" would say no.\n\nIt's a romantic song. After considering their flaws, Robb sings out that they've found a reason to change. \"The reason is you,\" he repeats, over and over.\n\nZuckerberg met his wife, Priscilla Chan, in 2003, one year before this playlist's date. Maybe he was thinking of her.\n\nHe'd be less happy to hear that the song reemerged on TikTok — a Meta competitor — in a 2021 trend.\n\nWe all knew that Zuckerberg liked rap. He recorded his own version of T-Pain's \"Get Low\" in tribute to his wife. He also started dressing like Eminem.\n\n\"In the End\" is peak 2010s emo rap-rock. One of its refrains is \"I tried so hard.\" Zuckerberg did try so hard — he often worked till late into the night.\n\nThe song is more pessimistic in its outlook: \"In the end, it doesn't even matter.\" Zuckerberg would likely disagree here. He tried so hard — and in the end, it did matter. Meta is now a trillion-dollar company.\n\nHard pivot! After four hard rock tunes, Zuckerberg's last song leans into synth-funk.\n\nThe lyrics here feel self-explanatory for Zuckerberg's workplace ethos: \"Work it harder, make it better / Do it faster, makes us stronger / More than ever, hour after hour / Work is never over.\"\n\nAfter Meta's year of intensity, I'm sure that some of Zuckerberg's employees can relate.\n\nAlso, the Daft Punk helmet doesn't look that different from a Meta Quest.",
    "readingTime": 4,
    "keywords": [
      "zuckerberg liked",
      "like stone",
      "song",
      "music",
      "playlist's",
      "rock",
      "sings",
      "history",
      "songs",
      "headstrong"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mark-zuckerberg-2004-coding-jams-playlist-loud-emo-on-brand-2026-2",
    "thumbnail_url": "https://i.insider.com/69838cc9d3c7faef0ecd9c7b?width=1200&format=jpeg",
    "created_at": "2026-02-05T12:36:52.821Z",
    "topic": "finance"
  },
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What’s Behind the ‘SaaSpocalypse’ Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "webhook-skills-agent-skills-for-webhook-providers-and-best-practices",
    "title": "Webhook Skills – Agent skills for webhook providers and best practices",
    "description": "Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopif...",
    "fullText": "hookdeck\n\n /\n\n webhook-skills\n\n Public\n\n Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopify, GitHub, and more. Built on the Agent Skills specification.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n hookdeck/webhook-skills",
    "readingTime": 1,
    "keywords": [
      "webhook",
      "skills",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/hookdeck/webhook-skills",
    "thumbnail_url": "https://opengraph.githubassets.com/ed9e4e830971c76b02864c100a37accbc7ef947718023c6abb2ba2536dad1e19/hookdeck/webhook-skills",
    "created_at": "2026-02-04T12:35:28.405Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-the-new-rad",
    "title": "Vibe Coding is the new RAD",
    "description": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.",
    "fullText": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.\n\nFile details: 6.9 MB MP3, 5 mins 12 secs duration.\n\nTitle music is \"Apparent Solution\" by Brendon Moeller, licensed via www.epidemicsound.com\n\nFive.Today is a highly-secure personal productivity application designed to help you to manage your priorities more effectively, by focusing on your five most important tasks you need to achieve each day.\n\n Our goal is to help you to keep track of all your tasks, notes and journals in one beautifully simple place, which is highly secure via end-to-end encryption. Visit the URL Five.Today to",
    "readingTime": 1,
    "keywords": [
      "tasks",
      "five.today"
    ],
    "qualityScore": 0.65,
    "link": "https://techleader.pro/a/723-Vibe-Coding-is-the-new-RAD-(TLP-2026w3)",
    "thumbnail_url": "https://techleader.pro/img/icons/noun_programmer_2644331.png",
    "created_at": "2026-02-04T01:06:56.822Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-good-yet",
    "title": "Is AI \"Good\" Yet?",
    "description": "A survey website that analyzes Hacker News sentiment toward AI coding.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.is-ai-good-yet.com",
    "thumbnail_url": "https://www.is-ai-good-yet.com/og-image.png",
    "created_at": "2026-02-03T06:37:48.803Z",
    "topic": "tech"
  },
  {
    "slug": "contextgeneric-programming-v061-release-improving-ergonomics-and-debugging",
    "title": "Context-Generic Programming v0.6.1 Release: Improving Ergonomics and Debugging",
    "description": null,
    "fullText": "This blog post is co-authored by Claude Haiku based on a human draft. All content has been reviewed by the human author.\n\nWe're excited to announce the release of cgp v0.6.1, which brings several quality-of-life improvements to Context-Generic Programming in Rust. This release focuses on making CGP more accessible to developers new to the paradigm while also providing better debugging and verification tools for complex provider setups.\n\nThe changes in this release reflect our commitment to lowering the barrier to entry for CGP, while maintaining the power and flexibility that make it such a compelling approach to modular Rust code. Whether you're an experienced CGP user or just getting started, we believe you'll find these improvements meaningful.\n\nOne of the biggest hurdles when learning CGP is understanding how generic Context parameters work in provider implementations. While generics are fundamental to Rust, they can be intimidating to developers coming from object-oriented backgrounds, or even to experienced Rust developers who haven't deeply explored the generic type system.\n\nWhen writing a #[cgp_impl] provider, you previously had to explicitly declare the Context as a generic parameter and ensure it appeared correctly throughout your implementation. This added cognitive overhead that wasn't directly related to the business logic you were implementing. For beginners, this meant wrestling with generic syntax before they could even focus on understanding the core CGP concepts like delegation and trait constraints.\n\nCGP v0.6.1 introduces support for implicit Context types in the #[cgp_impl] macro. You can now omit the explicit generic parameter and use familiar Rust patterns like Self to refer to the context type.\n\nLet's look at a concrete example. Suppose we're building a greeting system where different types can greet people:\n\nBefore v0.6.1, the provider implementation would look like:\n\nWith v0.6.1, you can simplify this to:\n\nThe difference might seem small at first glance, but it has a profound impact on readability and accessibility. The new syntax eliminates the generic parameter syntax entirely, making the provider implementation look much more like a standard Rust trait implementation. For someone new to CGP coming from an OOP background, this is far less intimidating—it reads like implementing a method on a class, which is a familiar pattern.\n\nThis improvement particularly shines when you have multiple providers in a codebase. Each provider definition becomes clearer and more focused on the actual logic, rather than the generic plumbing.\n\nAs your CGP applications grow in complexity, so does the challenge of verifying that all your provider wiring is correct. This is particularly true when using higher-order providers—providers that accept other providers as generic parameters.\n\nWhen you compose multiple providers together and something goes wrong, the compiler error messages can be cryptic. The error might point to a deep dependency in the chain, but it won't clearly tell you which individual provider in the composition is actually failing. This forces developers to compile and test repeatedly, changing providers and wiring patterns to narrow down the issue.\n\nCGP v0.6.1 introduces the #[check_providers] attribute for the check_components! macro, which lets you verify that specific providers work correctly with a given context. This is a powerful debugging tool that allows you to isolate and test individual providers before wiring them into your context.\n\nHere's a simple example. Suppose we have a shape area calculation system:\n\nYou can verify that RectangleArea works with Rectangle using:\n\nWith the new #[check_providers] attribute, you can also verify the provider directly:\n\nWhat's the benefit? The #[check_providers] version checks whether RectangleArea itself can be used as a provider for the component, regardless of what's currently wired in your Rectangle context. This is useful if you're developing multiple providers and want to test them independently before integrating them into your system.\n\nThe real power of #[check_providers] becomes apparent when you're working with composed higher-order providers. Let's look at a more realistic example:\n\nNow, when we verify this setup, we want to ensure not only that the composed provider works, but also that each individual component in the composition is correct. We can do this:\n\nThe first check verifies that the wired provider works with the context. The second check verifies each individual provider (or provider combination) that makes up your composition. If something is broken, such as the width field is missing, the error messages from these targeted checks will clearly point you to the problematic provider, rather than the entire composition.\n\nThis is a game-changer for debugging complex CGP setups. Instead of staring at a wall of compiler errors and trying to trace through the dependency chain, you can surgically test each part of your provider composition.\n\nOne common pattern in CGP is using getter traits to extract values from your context. The #[cgp_auto_getter] macro makes this convenient by automatically implementing a getter that reads a field from your context using the HasField trait.\n\nHowever, there was one limitation that made more sophisticated getter traits tedious to work with: you couldn't define an abstract type for the return value of your getter. If you wanted a getter trait where the return type was customizable per context, you had to define a separate abstract type trait, then link it to your getter trait.\n\nThis meant the old pattern looked like:\n\nThis works, but it requires defining two separate traits when conceptually you just want one getter trait with a flexible return type. For simple cases, this felt like boilerplate.\n\nCGP v0.6.1 allows you to define associated types directly in getter traits, eliminating the need for the separate type trait. You can now write:\n\nThe HasName trait now has an abstract Name type, and when the auto-getter is derived for Person, it automatically implements HasName with String as the concrete Name type. This is much more concise and reads more naturally.\n\nThe benefits are particularly clear when you have multiple getters with abstract types. Instead of maintaining a parallel set of type traits, you can keep everything in one place, making your code easier to understand and maintain.\n\nIf you're using the more powerful #[cgp_getter] macro (which allows customization of the implementation through providers), the same support for associated types works seamlessly:\n\nEven though our struct has a first_name field, we can still use the HasName getter by wiring it with UseField. The #[cgp_getter] macro combined with associated types gives you both power and convenience.\n\nThese three changes—implicit Context types, enhanced provider checking, and associated types in getters—work together to achieve our goal: making CGP more accessible and maintainable without sacrificing its power.\n\nThe improvements lower the learning curve for new CGP users by reducing generic syntax overhead. They provide better tooling for verifying and debugging complex provider compositions. And they reduce boilerplate when defining sophisticated traits, freeing you to focus on business logic.\n\nAs your CGP codebase grows, you'll find yourself reaching for these new features frequently. The #[cgp_impl] simplification makes your code more readable. The #[check_providers] attribute helps you debug faster. And associated types in getters let you express your intent more directly.\n\nWe believe CGP v0.6.1 represents a meaningful step forward in making Context-Generic Programming more approachable and productive. These changes emerged directly from feedback and real-world usage patterns in the CGP community, and we're confident they'll improve the development experience for everyone.\n\nWe encourage you to upgrade to v0.6.1 and explore these new features. Try simplifying your provider implementations with implicit Context types. Use #[check_providers] to debug your next complex provider composition. Define getter traits with abstract types and feel the difference in conciseness and clarity.\n\nAs always, we welcome feedback, bug reports, and contributions. The CGP journey continues, and we're excited to see what you build with these new tools.",
    "readingTime": 7,
    "keywords": [
      "let's look",
      "check verifies",
      "cgp introduces",
      "business logic",
      "error messages",
      "we're excited",
      "cgp_getter macro",
      "check_providers attribute",
      "debugging complex",
      "generic parameter"
    ],
    "qualityScore": 1,
    "link": "https://contextgeneric.dev/blog/v0-6-1-release/",
    "thumbnail_url": "https://contextgeneric.dev/cgp-logo.png",
    "created_at": "2026-02-02T12:34:23.079Z",
    "topic": "tech"
  },
  {
    "slug": "the-creator-of-clawdbot-the-viral-ai-agent-says-he-got-so-obsessed-with-vibe-coding-it-pulled-him-into-a-rabbit-hole",
    "title": "The creator of Clawdbot, the viral AI agent, says he got so obsessed with vibe coding it pulled him into a 'rabbit hole'",
    "description": "The creator of Clawdbot, the viral AI agent, says vibe coding can blur into compulsion, creating the illusion of productivity without real progress.",
    "fullText": "The creator of the viral AI agent Clawdbot says he had to step back after becoming too obsessed with vibe coding.\n\nPeter Steinberger, the developer behind Clawdbot — which later changed its name to Moltbot and is now known as OpenClaw — said in an episode of \"Behind the Craft\" podcast published Sunday that vibe coding pulled him into a \"rabbit hole.\"\n\n\"I was out with my friends and instead of, like, joining the conversation in the restaurant, I was just like, vibe coding on my phone,\" he said.\n\n\"I decided, OK, I have to stop this more for my mental health than for anything else,\" he added.\n\nClawdbot went viral last month in the tech community, attracting a wave of high-profile fans — from Y Combinator CEO Garry Tan to multiple partners at Andreessen Horowitz.\n\nIt is a personal AI agent designed to run continuously and plug into a wide range of consumer apps, including WhatsApp and Telegram. Users can ask the AI to manage their schedules, oversee vibe-coding sessions, and even create AI employees.\n\nThe AI agent has been widely praised and meme'd online, with some tech fans even buying Mac Minis specifically to run the AI, Business Insider's Henry Chandonnet reported last week.\n\n​​Steinberger said developers can fall into this trap of being hooked onto vibe coding, where building increasingly powerful AI tools creates the \"illusion of making you more productive\" without real progress.\n\nBuilding new tools can feel rewarding and fun, but that can quietly blur into compulsion, he added.\n\nWith AI, developers can now \"build everything,\" but ideas and taste matter. Without them, developers risk building tools and workflows that don't actually move a project forward, ​​Steinberger said.\n\n\"If you don't have a vision of what you're going to build, it's still going to be slop,\" he added.\n\nVibe coding has continued to surge in popularity, with companies and developers promoting how AI can speed up software development.\n\nEarlier this month, Anthropic said it built its new agentic work tool, Cowork, entirely using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. \"Us humans meet in-person to discuss foundational architectural and product decisions, but all of us devs manage anywhere between 3 to 8 Claude instances implementing features, fixing bugs, or researching potential solutions.\"\n\nThanks to Claude, the agent came together quickly. \"We sprinted at this for the last week and a half,\" Rieseberg said during a livestream.\n\nStill, despite the excitement around how fast vibe coding can produce new tools, tech leaders are warning that it has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that he won't vibe code on \"large codebases where you really have to get it right.\"\n\n\"The security has to be there,\" he added.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding is great for prototypes or throwaway code, not software that sits at the core of a business.\n\n​​\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "developers",
      "agent",
      "tools",
      "clawdbot",
      "tech",
      "viral",
      "episode",
      "fans"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/clawdbot-creator-vibe-coding-rabbit-hole-obsessed-openclaw-peter-steinberger-2026-2",
    "thumbnail_url": "https://i.insider.com/69802a8da645d11881886c3c?width=1200&format=jpeg",
    "created_at": "2026-02-02T12:34:16.408Z",
    "topic": "finance"
  },
  {
    "slug": "i-embedded-myself-in-a-vibe-coding-team-at-geminis-ai-hackathon-in-singapore-building-an-app-in-7-hours-takes-real-work",
    "title": "I embedded myself in a vibe coding team at Gemini's AI hackathon in Singapore. Building an app in 7 hours takes real work.",
    "description": "I followed a hackathon team as they raced to vibe code an app in seven hours at Google's Gemini 3 Hackathon in Singapore.",
    "fullText": "Just after sunrise, four vibe coding enthusiasts from Malaysia crossed into Singapore with a loose idea — and a bet that AI could build most of their app.\n\nHours later, they were racing to prototype it at Google's Gemini 3 Hackathon in Singapore.\n\nThe four friends, all in their late 30s to 40s, came from different professional backgrounds. Chan Wei Khjan is an accountant. Chan Ler-Kuan lectures on AI at a private university. Loh Wah Kiang works in IT. Lee How Siem, who goes by Benny, is the chief technology officer of a Malaysian startup.\n\nTheir initial idea was a \"feng shui\" app to analyze properties in Singapore — a potentially lucrative use case in a market obsessed with housing and wealth accumulation. Feng shui is a traditional Chinese practice that evaluates how a person's surroundings, along with birth factors, influence luck and well-being.\n\nI embedded with the team at Google's developer space in Singapore in January to observe how a vibe-coding project comes together — or nearly falls apart — in seven hours.\n\nThe assignment: Teams of up to four people had to build a working demo, publish a public repository with code, and submit a short video explaining their project by 5:30 p.m.\n\nEach project had to fit into one of six tracks, including generative media, deep research, and enterprise orchestration.\n\nOrganized by Google DeepMind and 65labs, Singapore's AI builder collective, the hackathon featured a 100,000-credit Gemini API prize pool, with first place getting 30,000 credits.\n\nThe team had pivoted to a new idea due to time constraints: a feng shui app that could analyse a user's outfit and workspace through the phone camera in real time and assess how \"lucky\" they were.\n\nWei Khjan took the lead on prompting. He typed the first instructions into Claude, asking it to generate the workflow and code. Ler-Kuan focused on whether the AI's output aligned with feng shui concepts. Wah Kiang and Benny hovered over the codebase, refining ideas and flagging issues.\n\n\"For people who don't know how to read code, it's helpful to have people who do,\" Wei Khjan said.\n\nWhile waiting for the code to be generated, Ler-Kuan opened Google's AI Studio to design the app's logo. They called their app \"Feng Shui Banana.\"\n\nAfter about an hour, Claude generated the initial codebase for the app. It was designed to work with the Gemini Live API, enabling real-time image and text analysis. It ran but was riddled with bugs.\n\nAn error message flashed when they tested the camera feature, so Wei Khjan copied the error back into the AI and asked for it to be fixed. Minutes later, the feature worked.\n\nIt wasn't right. The feng shui logic was off, especially where colour analysis intersected with the user's birth timings. Ler-Kuan manually corrected the underlying dictionary and its mappings.\n\nThe team kept prompting to tighten the features: shorter explanations, clearer output, and more streamlined user interfaces.\n\nLunch arrived. The team stayed glued to their screens.\n\nThe app didn't respond instantly when a user changed their outfit, nor did it update its feng shui analysis in real time.\n\nWei Khjan explained how one prompt matters. Instead of issuing commands, he asked the AI to \"discuss it with me.\" The shift changed how the model reasoned, and it worked more like a collaborator.\n\nAfter some prompting, the app updated with a real-time camera analysis. It was striking to watch a feature emerging from a short back-and-forth with AI.\n\nI helped the team test the app.\n\nThe camera correctly identified what I was wearing: a dark green polo, a yellow participant tag, and a white name card hanging from my neck. According to the app, I was already wearing colours aligned with my luck for the day.\n\nThe app suggested small tweaks, such as additional accessories, that could enhance the feng shui of my outfit.\n\nThey finally had lunch and joked around to ease the tension. Four hours remained before they had to submit their project.\n\nLer-Kuan shifted focus to workspace feng shui, feeding knowledge into the model and refining how the app would evaluate desks and work setups. Wah Kiang and Benny worked on the video demo.\n\nThe team also revisited the app's tagline. After cycling through suggestions from multiple AI models, they settled on a line that didn't come from an AI at all: \"A wisdom, not a superstition.\"\n\nThey used Gemini to generate a storyboard for the demo video. The model laid out several scenes and drafted the script. The team followed along, filming clips and stitching everything together as they went.\n\nTheir workspace feature was also up and running.\n\nThe app had come together nicely. With some time to spare, they decided to add audio output for users who prefer listening to reading on a screen.\n\nThe first attempt to generate a voice using AI fell flat. It sounded robotic.\n\nAfter debugging and several iterations, they landed on a voice they liked, similar to how a Chinese feng shui master might speak.\n\nAs the deadline approached, the team was still stitching clips for their video and nitpicking the AI-generated presenter voice.\n\nThe organizers had urged teams to submit early. With about 15 minutes to spare, they made the call to lock the final cut and hit submit.\n\nThen it was over. The hunger hit immediately, and everyone got in line for some well-deserved food.\n\nEven as an observer, watching from the sidelines was tiring. Seven hours of vibe coding turned out to be anything but effortless.\n\nThe team didn't win a prize, but agreed that the hackathon had been worth it.\n\n\"Sometimes, the best experiences come from saying 'yes' without overthinking,\" said Ler Kuan. \"Innovation starts with curiosity and a little bit of spontaneity.\"\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "wah kiang",
      "vibe coding",
      "feng shui",
      "wei khjan",
      "shui app",
      "wah kiang and benny",
      "team",
      "hours",
      "project",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/vibe-coding-team-embed-google-gemini-hackathon-singapore-2026-2",
    "thumbnail_url": "https://i.insider.com/696ef318a645d118818798f3?width=1200&format=jpeg",
    "created_at": "2026-02-02T06:52:12.008Z",
    "topic": "finance"
  },
  {
    "slug": "tailwind-creator-adam-wathan-shares-new-project-uish",
    "title": "Tailwind creator Adam Wathan shares new project ui.sh",
    "description": "A toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.",
    "fullText": "Turn your terminal intoA toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.Request an inviteBy the people who made\nTailwind CSS & Refactoring UI",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://ui.sh/",
    "thumbnail_url": "https://ui.sh/og-image.png",
    "created_at": "2026-02-02T01:11:00.781Z",
    "topic": "tech"
  },
  {
    "slug": "kessel-run-air-force-software-development-division",
    "title": "'Kessel Run' Air Force software development division",
    "description": null,
    "fullText": "Kessel Run, formally \"Air Force Life Cycle Management Center Detachment 12\", is a United States Air Force software development division, based in Hanscom Air Force Base and Boston, Massachusetts. It was founded in 2017 by the Defense Innovation Unit in response to the need to modernize legacy Air Force software.\n\nIn October 2016, Eric Schmidt, former CEO of Google, was leading a group touring the Combined Air Operations Center at Al Udeid Air Base in Qatar in his role as inaugural chairman of the Defense Innovation Board.[1] The CAOC at Al Udeid oversees air force operations for over 20 countries, and at the time was engaged in the War in Iraq against the Islamic State.[1]\n\nOne of the Air Operations Center tasks was planning daily aerial refueling operations to support combat missions. This was done off the main CAOC hall, in a windowless room with a whiteboard bearing magnetic pucks and plastic laminated cards, and physically measuring distance on the board to determine how long planes could stay in the air.[1][2] The resulting data was manually entered into an Excel spreadsheet known as \"the Gonkulator\" by a person called \"the Gonker\".[1][2] When a VBScript run on the spreadsheet confirmed the data was correct, another person, called \"the Thumper\", manually typed in the result into a Master Air Attack Planning Toolkit, which helped generate the Air Tasking Order back at Shaw Air Force Base in South Carolina.[1] Yet another person watched to verify against retyping errors.[2] The process took eight to 12 hours each day for three or eight people.[1][2][3] If there was a change needed, the process needed to restart.[2][3] When Schmidt and the Defense Information Board asked whether the base had access to more modern software to automate this process, the answer was: \"Yes, but it doesn't work.\"[1]\n\nAir Operations Center software had been in use mostly unchanged since the 1990s.[1] From 2006 to 2011, Lockheed Martin worked on the concept of modernizing it, but did not take up the project.[4] A Northrop Grumman project to modernize AOC software was commissioned in 2013 for $374 million for development, and $3.5 billion for lifetime maintenance.[4] By 2016, when the Defense Innovation Board was touring Al Udeid, nothing had been delivered.[2] The development price eventually grew to $745 million, and was three years behind schedule, with an estimated launch date of December 2019, when the project was eventually cancelled in July 2017.[4]\n\nAlso touring the Al Udeid CAOC with Schmidt in October 2016 was Raj Shah, tech entrepreneur and managing partner of DIUx, the Defense Innovation Unit Experimental.[1][2] Shah was a former Air Force fighter pilot who had first hand experience of the importance of regular aerial refueling.[2] That same night, he called lieutenant colonel Enrique Oti who was head of Air Force programs for DIUx in Silicon Valley.[1] They arranged an unusual partnership between Air Force developers and Pivotal Software, and got it approved by General Jeffrey Harrigian, commanding United States Air Forces Central Command.[1] Rather than rewrite the entire AOC software suite, they would just do the tanker whiteboard.[clarification needed][1] In December 2016, coders and program managers were visiting Al Udeid to talk to users. By April 2017, the aerial refueling tanker application, named Jigsaw, was in use at the CAOC; four months from start to production, when a more average Defense Department software operation took as much as three to five years.[1][5]\n\nThe speed of development was credited to the Agile development process, an iterative, adaptive approach, that doesn't try to get the entire solution in the first release. The first version of an application is intentionally only about a 60% solution, which is then changed and improved via rapid iterations as users give feedback.[6] Though this agile development is fairly basic in the modern software industry, it was unusual for the Defense Department.[7] The development team would later adopt the hashtag #AgileAF - the AF, they assure, stands for Air Force.[1]\n\nThe total cost of Jigsaw was reported at $1.5 million (Captain Bryon Kroger, Chief Operating Officer of the project),[5] to $2.2 million (Shah),[1] \"chump change\" (Harrigian).[8] With it, tanker planning was not only faster, taking two to three hours for a single person, but was more reliable, so two to three fewer tankers were scrambled each day.[1][3] Each scramble had a cost in fuel and maintenance of about $250,000 each.[1][2] Jigsaw saved 350,000 pounds of fuel a week.[8] Its development costs were recouped in the first week.[1][9]\n\nIn April 2017, after delivering Jigsaw, Oti, Kroger, and others, got approval to form an official Air Force software development team at the Air Force Life Cycle Management Center named Project Kessel Run.[2][10][5] The name \"Kessel Run\" came from a line in the 1977 science fiction film Star Wars, spoken by smuggler Han Solo, bragging about the speed of his ship, the Millennium Falcon.[2] It represented the project's intent to \"smuggle\" new software development capability into the Air Force and use it to set new software development speed records.[9]\n\nIn May 7, 2018, the Kessel Run Experimentation Lab set up at a WeWork shared facility in central Boston. It was modeled after Pivotal Labs training locations in Cambridge, San Francisco, and Washington.[11] It was managed from the AFLCMC at Hanscom Air Force Base, which believed the innovation advantages in coworking and creativity would outweigh the hassles of distance and security.[11][12] The Lab initially had space for 90 engineers, but planned for 300 within a year.[9] Many were on temporary assignment from other Air Force bases; yet others would be sent off to Pivotal Labs offices across the country for training in modern software techniques.[11][13][14] The Kessel Run motto on the wall was \"Code. Deploy. Win.\", a play on the Air Force's motto, \"Fly. Fight. Win.\".[12]\n\nOn January 2, 2019, the Kessel Run Experimentation Laboratory moved to a different location in a Boston skyscraper.[15] Kessel Run's budget since mid 2017 had grown to approximately $140 million, including workspace and personnel, and the operational software produced claimed savings of $13 million and 1,100 man-hours per month.[15]\n\nOn May 8, 2019, Kessel Run formally became Air Force Life Cycle Management Center Detachment 12, commanded by Colonel Oti, who had been effectively leading it for just under a year.[16] It had nearly 700 airmen, government civilians, and contractors.[16] Oti was replaced as commander by Colonel Brian Beachkofski on April 15, 2020, with the ceremony held over Zoom teleconference.[17] On June 27, 2022, Beachkofski was replaced as commander by Colonel Richard Lopez, who took the title of senior materiel leader.[18] Lopez had previously been the director of the LevelUP Code Works software factory inspired by Kessel Run.[18][19]\n\nThe March 2019 Defense Innovation Board report on software acquisitions included a chapter on Kessel Run subtitled \"The Future of Defense Acquisitions Is #AgileAF\".[21]\nIn September and October 2019, Kessel Run received multiple awards: the General Larry O. Spencer Innovation award; the Theodore von Kármán award for modernizing software for the F-35; and the inaugural Defense Acquisition Software Innovation Team award.[22][23] A 2019 editorial in Defense One said that Kessel Run was \"widely seen as the gold standard of military tech done right ... also the most hyped military program office in operation today\".[24]\n\nNot all reactions were positive. A 2019 anonymous survey of KREL application users found that some applications did not meet user needs, and that success metrics, documentation, and responsiveness to user feedback could all be lacking.[25] A 2020 Harvard Kennedy School project found and tried to address internal discontent among Kessel Run staff with emerging bureaucracy and increasing technical complexity.[26] The Air Force's Deputy Chief Information Officer, Lauren Knausenberger, acknowledged in 2020 that Kessel Run was having growing pains, but said that was a result of its success.[27]\n\nKessel Run inspired multiple agile software development teams across the Air Force and United States Department of Defense.[28] They were called \"software factories\".[29] The original definition of software factory was a set of software tools to write and automatically build, test, and document applications; the Chief Information Officer of the Department of Defense slightly redefined that to be a software assembly plant that automated the develop, build, test, release and deliver phases, but in each case to support agile software development practices.[30]\n\nKobayashi Maru, formally Space C2, or Space Command and Control, in California, was the second such software factory, in August 2018.[28] It was named after an impossible test in the Star Trek science fiction universe.[31] Just as Kessel Run came from an effort to replace an outdated system by getting around bureaucratic rules, Kobayashi Maru intended to update the software of the troubled Joint Mission System (jointly run with the United States Space Force) for space command and control and situational awareness.[32][33]\n\nBESPIN - an acronym for Business and Enterprise Systems Product Innovation, but also the name of a planet in the Star Wars universe - was the third Air Force software factory, launched in early 2019 in Montgomery, Alabama, to create apps for maintenance crew chiefs, aircrew readiness, and ammunition crews.[28][31] Space Camp, in Colorado, and Section31, in California, spun off of Kobayashi Maru.[31] LevelUP, in Texas, was a joint cyber operations system for the Unified Platform, connecting the Army, Marines, and United States Cyber Command, debuting in April 2019.[31] By September 2021, there were 17 Air Force software factories across the country.[34]\n\nSoftware factories weren't limited to the Air Force. The Navy was inspired by Kessel Run to stand up its first software factory, The Forge, in Riverdale, Maryland, in March 2021.[35] The Army Software Factory debuted in April 2021 in Austin Community College in Texas, as part of the United States Army Futures Command.[36][37] In February 2022, Deputy Defense Secretary Kathleen Hicks wrote a DOD Software Modernization Strategy memo encouraging increased used of software factories throughout the Defense Department; at the time there were 29.[38] By April 2022 the United States Coast Guard was planning a software factory based on the Air Force model.[39] The Marine Corps Software Factory was co-located with the Army Software Factory as a three year test project in Austin in March 2023.[40]\n\nThe 24th Air Force's Air Force Cyber Proving Ground may be another related activity.\n\nJigsaw, the 2017 aerial refueling planning application that started Kessel Run, was bought and used by NATO in multiple countries in 2020 and 2021.[41][42]\n\nThe team's second and third projects after Jigsaw were Chainsaw and Raven, applications for assembling and communicating target information.[1] Chainsaw was in operation by November 2017, consolidated many programs into one, and cut the process for dynamic targeting from an hour or two to minutes. Raven, for target development management, cut 12 hours of work down to three or four, and was ready in early 2018.[43]\n\nStarting in late 2018, Kessel Run joined the task of fixing the troubled software for the maintenance of the F-35 fighter jet, called ALIS, for Autonomic Logistic Information System.[44] ALIS was a 17 year old proprietary system full of bugs and data gaps.[45] Maintainers had to keep separate databases because they could not rely on ALIS data.[46] The project to fix ALIS, including Kessel Run, Pivotal, and Lockheed Martin, its original creators, was called Mad Hatter, named by the developers.[44] It officially started in October 2018, but took until January 2019 before developers could write code, while the Air Force negotiated with Lockheed Martin as to what parts of the proprietary ALIS system the government could be able to reach.[44] The Mad Hatter suite of eight programs was tested and favorably evaluated by F-35 aircraft maintainers in March 2020.[47] In July 2020, it was renamed to Torque, and adapted for maintenance of the F-22 stealth jet and CV-22 tiltrotor aircraft,[48] then the C-130J turboprop in January 2021.[49] Meanwhile, on the F-35 itself, between 2020 and 2022 ALIS was gradually replaced by ODIN, the Operational Data Integrated Network, \"leveraging\" the software practices of Kessel Run, but built by Lockheed Martin.[50][51]\n\nIn 2021, Kessel Run began deploying the initial version of KRADOS, the Kessel Run All Domain Operations Suite meant to replace the Theater Battle Management Core Systems that created air tasking orders throughout AOCs all over the world.[52] The first AOC to use the suite operationally was again the 609th Air Operations Center at Al Udeid, in May 2021, after using the Beta version since December 2020.[53] KRADOS linked together nine applications through cloud-based data, including the latest version of Jigsaw, Kessel Run's first application for tanker planning, and Slapshot, for planning the rest of the air missions and building the Master Air Attack Plan.[54] In 2017, Lockheed Martin had received a $38 million contract to maintain the older TBMCS, but the 609th kept finding problems, so turned to Kessel Run, which delivered the beta version of KRADOS three weeks after receiving the request in November 2020.[55] By August 2022, the 603rd AOC in Ramstein, Germany, employed elements of KRADOS for visualization, though it was not considered mature enough to create air tasking and airspace control orders.[56][57] In January 2023, the 609th replaced the TBMCS with KRADOS entirely.[58]\n\nThe Command and Control Incident Management Emergency Response Application (C2IMERA), is a real time Air Force base resource management tool. It used a different development model: the coding was done by software company Leidos, and the program management by Kessel Run.[59] In August 2019, Moody Air Force Base used the software to monitor and prepare for Hurricane Dorian, though it was not originally intended for this purpose.[60] C2IMERA was also used for the August 2021 evacuation of civilians from Afghanistan in Operation Allies Refuge.[61][62] It was ordered deployed across all Air Combat Command installations in September 2021.[59][63] In August 2023, Air Mobility Command joined Air Combat Command in designating C2IMERA as their standard installation command and control tool.[64]\n\nSlapshot, the air mission flow organizer part of KRADOS, was also used for the Operation Allies Refuge Afghan evacuation along with C2IMERA.[65][66] At that time, KRADOS had known issues with scaling; it couldn't handle many simultaneous operations, which was exactly what it was being asked to do. On August 24, 2021, at 2 am Boston time, the Slapshot server crashed. Over the next 12 hours, Kessel Run developers restarted servers, shifted United States Central Command resources to improve performance, fixed database errors, and added new features to improve load times, so the evacuation on the other side of the world could continue.[67]\n\nBowcaster, named after a Star Wars weapon, is a chaos engineering tool and playbook that intentionally creates failures in processes to strengthen them.[68][69]\nKessel Run developed it, and shares it with other government agencies, initially with the Navy Black Pearl software factory in 2021.[70][71] In March 2022 Kessel Run and the General Services Administration's Technology Transformation Services used it to check that the Cloud.gov website could handle 100 million users per hour.[72][73]",
    "readingTime": 13,
    "keywords": [
      "kessel run",
      "air force",
      "allies refuge",
      "life cycle",
      "operation allies",
      "center detachment",
      "cycle management",
      "innovation unit",
      "innovation board",
      "operations center"
    ],
    "qualityScore": 1,
    "link": "https://en.wikipedia.org/wiki/Kessel_Run",
    "thumbnail_url": "https://upload.wikimedia.org/wikipedia/commons/7/77/Kessel_Run_logo.jpg",
    "created_at": "2026-02-01T12:26:41.722Z",
    "topic": "tech"
  },
  {
    "slug": "securing-the-ralph-wiggum-loop-devsecops-for-autonomous-coding-agents",
    "title": "Securing the Ralph Wiggum Loop – DevSecOps for Autonomous Coding Agents",
    "description": "Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck - agairola/securing-ralph-loop",
    "fullText": "agairola\n\n /\n\n securing-ralph-loop\n\n Public\n\n Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n agairola/securing-ralph-loop",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/agairola/securing-ralph-loop",
    "thumbnail_url": "https://opengraph.githubassets.com/6d3ccf091e1097ce34aa5949ea80238ec45757c8d310e8b3be3ea2b9c6e218fb/agairola/securing-ralph-loop",
    "created_at": "2026-02-01T06:37:20.298Z",
    "topic": "tech"
  },
  {
    "slug": "top-engineers-at-anthropic-openai-say-ai-now-writes-100-of-their-code",
    "title": "Top engineers at Anthropic, OpenAI say AI now writes 100% of their code",
    "description": "AI coding tools are getting more sophisticated. But if coders stop coding, what happens to software development jobs?",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/01/29/100-percent-of-code-at-anthropic-and-openai-is-now-ai-written-boris-cherny-roon/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/GettyImages-2216956965_40536e-e1769705381107.jpg?resize=1200,600",
    "created_at": "2026-01-31T01:04:24.596Z",
    "topic": "tech"
  },
  {
    "slug": "cooperbench-benchmarking-ai-agents-cooperation",
    "title": "CooperBench: Benchmarking AI Agents' Cooperation",
    "description": "CooperBench is a benchmark of over 600 collaborative coding tasks. We find that agents achieve 30% lower success rates when working together compared to performing both tasks individually.",
    "fullText": "Stanford University & SAP Labs US\n\nCan AI agents work together as teammates? We find\n that\n coordinating agents perform much worse than a\n single agent\n given the same total workload. This coordination\n deficit presents a fundamental barrier to deploying\n AI systems that can work alongside humans or other\n agents.\n\nSuccess rate on CooperBench across 652 tasks · Error bars show 95% confidence intervals\n\nGPT-5 and Claude Sonnet 4.5 achieve only 25%\n success with two-agent cooperation, roughly 50%\n lower than when a single agent handles both\n tasks. This gap persists across all models and\n task difficulties.\n\nAgents spend up to 20% of their budget on\n communication. This reduces merge conflicts but\n does not improve overall success. The channel is\n jammed with repetition, unresponsiveness, and\n hallucination.\n\nEven when agents communicate well, coordination\n breaks down due to:\n\nAmong successful runs, we observe coordination patterns\n largely absent from failures. These patterns are not\n prompted or scaffolded.\n\nRole Division\n — Agents agree on who handles which part of the\n task. One agent delegates: \"I'll add header +\n octal_str; you add binary_str between them.\"\n\nCooperBench is the first benchmark designed to\n measure how well AI agents can cooperate when\n handling individual tasks with potential conflicts.\n We constructed 652 tasks from 12 popular open-source\n libraries across Python, TypeScript, Go, and Rust.\n\nEach task assigns two agents different features that\n can be implemented independently but may conflict\n without proper coordination. Eight co-authors with\n real-world software engineering backgrounds created\n new features, unit tests, and ground-truth code.\n\nStanford University & SAP Labs · *Equal contribution\n (Stanford) · †Equal contribution (SAP Labs)",
    "readingTime": 2,
    "keywords": [
      "equal contribution",
      "stanford university",
      "university sap",
      "coordination",
      "tasks",
      "success",
      "across",
      "task",
      "agents",
      "cooperbench"
    ],
    "qualityScore": 0.85,
    "link": "https://cooperbench.com/",
    "thumbnail_url": "https://cooperbench.com/static/images/cooperbench_social.png",
    "created_at": "2026-01-30T18:28:27.534Z",
    "topic": "tech"
  },
  {
    "slug": "daedalus",
    "title": "Daedalus",
    "description": "AI planning CLI and autonomous agent orchestration for beans-based coding workflows - internet-development/daedalus",
    "fullText": "internet-development\n\n /\n\n daedalus\n\n Public\n\n AI planning CLI and autonomous agent orchestration for beans-based coding workflows\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n internet-development/daedalus",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/internet-development/daedalus",
    "thumbnail_url": "https://opengraph.githubassets.com/471147f12cb10edd91f2072a6602ce03fd3e4339e4f03a4a184ee7336b8d31ea/internet-development/daedalus",
    "created_at": "2026-01-30T06:35:17.254Z",
    "topic": "tech"
  },
  {
    "slug": "cwt-sandbox-ai-coding-agents-using-git-worktrees",
    "title": "Cwt – Sandbox AI coding agents using Git Worktrees",
    "description": "Contribute to benngarcia/claude-worktree development by creating an account on GitHub.",
    "fullText": "benngarcia\n\n /\n\n claude-worktree\n\n Public\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n benngarcia/claude-worktree",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/benngarcia/claude-worktree",
    "thumbnail_url": "https://opengraph.githubassets.com/73752eaadbf154afd745270d32c1743038163f0555993f4c462cecd915f63c02/benngarcia/claude-worktree",
    "created_at": "2026-01-30T01:07:08.980Z",
    "topic": "tech"
  },
  {
    "slug": "acp-agent-registry-in-jetbrains-ides",
    "title": "ACP Agent Registry in JetBrains IDEs",
    "description": "Together with Zed, we've launched the official ACP Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs.",
    "fullText": "Supercharge your tools with AI-powered features inside many JetBrains products\n\nAI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day. Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what’s out there, let alone getting it running in your IDE, hasn’t been easy.\n\nTogether with Zed (Zed’s announcement), we’ve launched the official ACP Agent Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs and Zed. Browse what’s available, click Install, and start working right away. This beta release is just the beginning.\n\nThe Agent Client Protocol is an open standard that lets any AI coding agent work in any supporting editor. Think of it like the Language Server Protocol, but for AI agents. The LSP lets any editor support any language through a shared standard. The ACP does the same for coding agents. You only need to implement it once, and then it will work in your JetBrains IDE, Zed, or any other editor that supports the protocol.\n\nThis means you get to pick your preferred agent and editor, and they will then work together seamlessly – no vendor lock-in and no waiting for someone to build a specific integration.\n\nACP has been, since we started integrating it to Mistral Vibe, a real joy to use: thoughtfully designed from the ground up, community-driven, and evolving rapidly. We’ve found it not only simplifies integration, but also fits our focus on open and flexible tools. It’s really great to see a standard that puts developer choice first.\n\nMichel Thomazo, Software Engineer @ Mistral AI\n\nThe ACP made agent interoperability technically possible. The registry makes it convenient.\n\nInstead of manually configuring agents, you can now:\n\nAt launch, you’ll find a wide array of different agents:\n\nFull-featured coding assistant optimized for large-scale refactors\n\nSpecialized agent for automated code generation workflows\n\nGoogle’s agent with deep codebase understanding and multimodal capabilities\n\nGitHub’s AI pair programmer, now available via the ACP\n\nLightweight, fast agent built on Mistral’s models\n\nCommunity-driven, fully open-source agent\n\nAlibaba’s coding agent with strong multilingual support\n\nInnovation in software agents is moving at an unbelievable pace. The Agent Registry and ACP makes it simple for developers to use the best agents in their favorite tools.\n\nChris Kelly, Product @ Augment Code\n\nIn general, it’s less about having multiple agents than about enabling you to pick and choose the ones that work well in your workflow. Different agents come with different benefits. Some provide a more attractive pricing structure for your business, some provide a user experience that you simply enjoy more than others’, and some embody the ideas of open-source development that just resonate with you.\n\nThe Agent Client Protocol registry lets you experiment freely. Try a few, see what clicks for your workflow, and then keep the ones that help. You’re not locked into a single vendor’s vision of what AI-assisted development should look like.\n\nWe’re excited to support the ACP Agent Registry as a step toward a more open agent ecosystem where Droids can integrate seamlessly across all IDEs.\n\nFrancesca LaBianca, VP of Operations @ Factory\n\nIn any JetBrains IDE (2025.3.2+) with JetBrains AI (253.30387.147):\n\nThat’s it. The agent is configured and ready to use in the AI Chat tool window.\n\nQuick note: agents typically come with their own subscription. That’s between you and them. You won’t need a JetBrains AI subscription to use ACP agents.\n\nWant to try something concrete? Install OpenCode, open a project, and ask it to explain an unfamiliar module. OpenCode also lets you swap between different LLMs, so you can experiment with what works best for you.\n\nIf you prefer manual configuration, that option is still there, too. Just edit the acp.json directly. This is useful for agents that aren’t in the registry yet or for custom setups.\n\nIf you’re building an ACP-compatible agent, the registry is now the fastest way to reach developers across JetBrains IDEs and Zed.\n\nHead to the ACP Registry repository and check out the CONTRIBUTING.md for the full submission process and metadata requirements. Please note that, for now, we are only featuring agents that support Agent Auth or Terminal Auth. Full details of requirements and conditions can be found here.\n\nThis is an open registry. If you’re building an ACP-compatible agent, you’re welcome to submit it. The registry exists to serve the ecosystem, not to gatekeep it.\n\nFor developers: More choice and zero lock-in. Use any agent you want in the IDE you love.\n\nFor agent builders: Instant distribution to millions of JetBrains and Zed users. Implement the ACP once and reach everyone.\n\nFor the ecosystem: Competition on quality, not on who controls the integration. The best agents win because they’re the best, not because they have exclusive deals.\n\nWe’re building this openly with Zed because we believe AI-assisted development shouldn’t be locked inside any single vendor’s ecosystem. Developers deserve to pick their tools freely.\n\nThe registry is one more step toward that future.\n\nThe ACP Registry is available now in JetBrains IDE versions 2025.3 and later. Update your IDE and the JetBrains AI plugin, open Settings, and start exploring.\n\nHave feedback? Found a bug? The registry repo is open for issues and PRs. And if you’re building something interesting with ACP, we’d love to hear about it!\n\nOpenAI Codex is now natively integrated into the JetBrains AI chat, giving you another powerful option for tackling real development tasks right inside your IDE. \n\nYou can use Codex with a JetBrains AI subscription, your ChatGPT account, or an OpenAI API key – all within the same AI сhat inte…\n\nThe next edit suggestions feature is now enabled in all JetBrains IDEs for JetBrains AI Pro, AI Ultimate, and AI Enterprise subscribers.\n\nYes, you read that right! JetBrains-native diff suggestions are available right in your editor. Global support for optimized latency. Out-of-the-box IDE actions…\n\nBring Your Own Key (BYOK) is now available in the AI chat inside JetBrains IDEs as well as for AI agents, including JetBrains’ Junie and Claude Agent. Whether you’re looking to use cutting-edge frontier models, cost-efficient small models, locally hosted private models, or experimental research prev…\n\nJunie is now integrated into the AI chat. The separate interfaces have merged into a single, unified space (available in Beta).",
    "readingTime": 6,
    "keywords": [
      "client protocol",
      "ai-assisted development",
      "step toward",
      "agent client",
      "acp-compatible agent",
      "jetbrains ai",
      "coding agents",
      "acp agent",
      "acp agent registry",
      "jetbrains ide"
    ],
    "qualityScore": 1,
    "link": "https://blog.jetbrains.com/ai/2026/01/acp-agent-registry/",
    "thumbnail_url": "https://blog.jetbrains.com/wp-content/uploads/2026/01/JB-social-BlogSocialShare-1280x720-1-4.png",
    "created_at": "2026-01-29T18:30:47.768Z",
    "topic": "tech"
  },
  {
    "slug": "extesla-ai-head-has-seen-a-phase-shift-in-software-engineering-using-claude-code-and-his-manual-skills-slowly-atrophy",
    "title": "Ex-Tesla AI head has seen a 'phase shift in software engineering' using Claude Code — and his manual skills slowly 'atrophy'",
    "description": "Andrej Karpathy posted his \"notes from Claude Coding,\" describing a shift in engineering over the last two months.",
    "fullText": "He coined \"vibe coding.\" Now, he sees a \"phase shift\" in software engineering.\n\nAndrej Karpathy is one of AI's guiding figures. He was a founding member of OpenAI and later served as Tesla's director of AI. He also coined the term \"vibe coding,\" the AI-assisted coding movement that has taken software engineering by storm and was named Collins Dictionary's word of the year.\n\nIn his \"random notes from Claude Coding\" — which are over 1,000 words long — Karpathy wrote about the changes to his own coding style. Posted on X on Monday, the notes have already elicited reactions from engineers at Anthropic, xAI, and more.\n\nAI coding agents \"crossed some kind of threshold of coherence around December 2025 and caused a phase shift in software engineering,\" Karpathy wrote.\n\nA few random notes from claude coding quite a bit last few weeks.\n\nCoding workflow. Given the latest lift in LLM coding capability, like many others I rapidly went from about 80% manual+autocomplete coding and 20% agents in November to 80% agent coding and 20% edits+touchups in…\n\nKarpathy name-dropped both Anthropic's Claude Code and OpenAI's Codex as having significant improvements. Claude Opus 4.5, the model that has garnered much love from engineers online, came out at the tail end of November.\n\nThe AI leader's workflow has changed as a result of the AI tools. From November to December, Karpathy's 80/20 ratio flipped. He once used 80% manual coding and 20% agents; now, it's 80% agents and 20% manual code editing.\n\n\"I really am mostly programming in English now, a bit sheepishly telling the LLM what code to write... in words,\" he wrote.\n\nThe change to AI-written code \"hurts the ego,\" but is too powerful to ignore, Karpathy wrote. He also devoted a whole section of his notes to the \"fun\" he has while coding with large language models.\n\nWhat of those traditional coding skills, the ones you learn in a computer science program or through endless digital courses? That's a whole other function, Karpathy wrote, and one that might decline.\n\n\"I've already noticed that I am slowly starting to atrophy my ability to write code manually,\" he wrote.\n\nIn Karpathy's comments, engineers from leading AI companies sounded off. Ethan He, an xAI engineer and Nvidia alum, wrote that a \"10x engineer can be a one-man army.\"\n\nCharles Weill, another xAI engineer, wrote that founders can now \"divide themselves\" with coding agents, like a VC divides their capital over a portfolio of companies.\n\nBoris Cherny, an Anthropic staffer and the creator of Claude Code, wrote that he read Karpathy's \"thoughtful\" post till its end.\n\nThe Claude Code team at Anthropic may offer a model of where the industry is moving, Cherny wrote. His team is \"mostly generalists\" and filled with 10x engineers.\n\n\"Pretty much 100% of our code is written by Claude Code,\" Cherny wrote. \"For me personally it has been 100% for two+ months now, I don't even make small edits by hand.\"\n\nThe Anthropic employee also acknowledged the \"quality\" problems with AI-written code. Agents can overcomplicate things and can leave around dead code, he wrote.\n\nHis solution: having AI review the AI-written code.",
    "readingTime": 3,
    "keywords": [
      "ai-written code",
      "phase shift",
      "software engineering",
      "random notes",
      "xai engineer",
      "vibe coding",
      "coding agents",
      "engineers",
      "claude",
      "coined"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-claude-code-manual-skills-atrophy-software-engineering-tesla-2026-1",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-01-29T12:32:28.928Z",
    "topic": "tech"
  },
  {
    "slug": "openais-chair-says-vibe-coding-is-here-to-stay-but-its-not-the-endgame",
    "title": "OpenAI's chair says vibe coding is here to stay — but it's not the endgame",
    "description": "Vibe coding will stick around, but AI agents, not apps, will drive the next big shift in software, says OpenAI's chair Bret Taylor.",
    "fullText": "Vibe coding isn't going anywhere. But it's only part of a much bigger transformation, says OpenAI's board chair.\n\nBret Taylor said in an episode of the \"Big Technology Podcast\" published on Wednesday that using AI tools to build software quickly with natural language prompts will soon feel normal rather than novel. However, focusing on building today's software faster misses the bigger picture.\n\n\"Everyone's looking at all the software use and saying, 'How fast could I vibe code that?'\" Taylor said. \"I wonder if it's the wrong question.\"\n\nWhether someone can quickly vibe code an app in a web browser isn't \"the most interesting question in software,\" he added.\n\nInstead, the software we use today is set to be replaced, and that's the real disruption, Taylor said.\n\nRather than dashboards, web-browser forms, and traditional apps, the structure of software will change. AI agents will be \"the future of software.\"\n\n\"We will delegate tasks to agents that will operate against a database,\" Taylor said.\n\n\"Who's making those agents is the question,\" he added. \"Will you buy those agents off the shelf or build them yourself?\"\n\nTaylor also said that while AI has slashed the cost of building software, it hasn't solved the harder problems of maintaining it — or the risk of getting things wrong.\n\n\"That's why most people would prefer to buy a solution off the shelf,\" he said. \"You want to amortize the cost of maintaining software among thousands of clients.\"\n\nVibe coding has taken off across the tech world, but tech leaders said the technology has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that vibe coding is \"making coding so much more enjoyable,\" adding that it allows even non-technical users to create simple apps and websites.\n\nDuring Alphabet's April earnings call, Pichai said AI generates more than 30% of Google's new code, up from 25% in October 2024.\n\nStill, AI-generated code can be error-prone, overly long, or poorly structured.\n\n\"I'm not working on large codebases where you really have to get it right, the security has to be there,\" Pichai said in November.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding works best for prototypes or throwaway code, but not in software that sits at the core of a business.\n\n\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "vibe code",
      "software",
      "agents",
      "isn't",
      "it's",
      "bigger",
      "episode",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chair-vibe-coding-not-endgame-bret-taylor-2026-1",
    "thumbnail_url": "https://i.insider.com/6883b14a85e81483682eb19e?width=1200&format=jpeg",
    "created_at": "2026-01-29T06:34:22.533Z",
    "topic": "finance"
  },
  {
    "slug": "microsoft-cfos-memo-to-staff-calls-out-ai-deals-coding-and-chips",
    "title": "Microsoft CFO's memo to staff calls out AI deals, coding, and chips",
    "description": "CFO Amy Hood sent an internal memo about the results, viewed by Business Insider.",
    "fullText": "After Microsoft reported results on Wednesday, CFO Amy Hood sent an internal memo to employees calling attention to recent developments in AI chips and coding tools, and deals with OpenAI and Anthropic.\n\nHood sends these emails every quarter when Microsoft discloses its financials. Her missives mostly rehash what the company reports publicly, such as how revenue and profit are growing, or what is discussed on analyst earnings calls.\n\nStill, these memos provide insight into what Microsoft executives deem most important, and what they want employees to know.\n\nThe latest memo highlighted how Microsoft is gaining share in markets where the total addressable market is expanding.\n\nHood specifically mentioned the launch of the GitHub Copilot software development kit in the growing market of AI coding tools, and the release of Microsoft's new Maia 200 AI chip.\n\nHood's email also called out Azure commitments from OpenAI and Anthropic that helped increase commercial bookings, basically the deals Microsoft closed in the quarter, by 230%, year over year.\n\nCapital expenditure on computing and datacenter infrastructure also broke yet another quarterly record, reaching $37.5 billion, she also noted.\n\nThis afternoon, we announced our second-quarter financial results. We exceeded Wall Street expectations, growing revenue 17% and 15% in constant currency and operating income by 21% and 19% in constant currency -a strong finish to the first half of the fiscal year.\n\nIn the quarter, Microsoft Cloud revenue surpassed $50 billion for the first time, growing 26% and 24% in constant currency.\n\nThere were many highlights this quarter, but a few stand out as reminders of the value our products and services deliver to customers - and as proof points of the progress we are making:\n\nInvestors tune in to our earnings call for the full details on this quarter and a look ahead to Q3. It's a helpful way to stay aligned as we deliver on our commitments. Join live today at 2:30 PM Pacific, listen on-demand, or check the transcript on the Investor Relations website.\n\nThis quarter's results reflect meaningful progress on core priorities. We continue to add capacity with pace, drive steady efficiency gains across our fleet, and invest in each layer of the stack\n\nAs we enter the second half of the fiscal year, we're operating in markets with expanding TAM where we continue to gain share and you can see our progress in many places, from last week's announcement of the GitHub Copilot SDK to Monday's Maia 200 announcement. We are innovating and delivering together. And we're doing it with the quality and security our customers expect from us. All of this builds trust from customers and partners as they rely on us for mission critical workloads.\n\nThanks again for all your work.\n\nWith appreciation and gratitude,\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "coding tools",
      "constant currency",
      "quarter",
      "revenue",
      "email",
      "customers",
      "progress",
      "memo",
      "employees",
      "deals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/internal-microsoft-cfo-memo-calls-out-ai-deals-coding-and-chips-2026-1",
    "thumbnail_url": "https://i.insider.com/697a82dfa645d11881883093?width=1200&format=jpeg",
    "created_at": "2026-01-29T01:07:05.127Z",
    "topic": "finance"
  },
  {
    "slug": "programming-sucks-2014",
    "title": "Programming Sucks (2014)",
    "description": null,
    "fullText": "Every friend I have with a job that involves picking up something heavier than a laptop more than twice a week eventually finds a way to slip something like this into conversation: “Bro,1[1] you don’t work hard. I just worked a 4700-hour week digging a tunnel under Mordor with a screwdriver.”\n\nThey have a point. Mordor sucks, and it’s certainly more physically taxing to dig a tunnel than poke at a keyboard unless you’re an ant. But, for the sake of the argument, can we agree that stress and insanity are bad things? Awesome. Welcome to programming.\n\nImagine joining an engineering team. You’re excited and full of ideas, probably just out of school and a world of clean, beautiful designs, awe-inspiring in their aesthetic unity of purpose, economy, and strength. You start by meeting Mary, project leader for a bridge in a major metropolitan area. Mary introduces you to Fred, after you get through the fifteen security checks installed by Dave because Dave had his sweater stolen off his desk once and Never Again. Fred only works with wood, so you ask why he’s involved because this bridge is supposed to allow rush-hour traffic full of cars full of mortal humans to cross a 200-foot drop over rapids. Don’t worry, says Mary, Fred’s going to handle the walkways. What walkways? Well Fred made a good case for walkways and they’re going to add to the bridge’s appeal. Of course, they’ll have to be built without railings, because there’s a strict no railings rule enforced by Phil, who’s not an engineer. Nobody’s sure what Phil does, but it’s definitely full of synergy and has to do with upper management, whom none of the engineers want to deal with so they just let Phil do what he wants. Sara, meanwhile, has found several hemorrhaging-edge paving techniques, and worked them all into the bridge design, so you’ll have to build around each one as the bridge progresses, since each one means different underlying support and safety concerns. Tom and Harry have been working together for years, but have an ongoing feud over whether to use metric or imperial measurements, and it’s become a case of “whoever got to that part of the design first.” This has been such a headache for the people actually screwing things together, they’ve given up and just forced, hammered, or welded their way through the day with whatever parts were handy. Also, the bridge was designed as a suspension bridge, but nobody actually knew how to build a suspension bridge, so they got halfway through it and then just added extra support columns to keep the thing standing, but they left the suspension cables because they’re still sort of holding up parts of the bridge. Nobody knows which parts, but everybody’s pretty sure they’re important parts. After the introductions are made, you are invited to come up with some new ideas, but you don’t have any because you’re a propulsion engineer and don’t know anything about bridges.\n\nWould you drive across this bridge? No. If it somehow got built, everybody involved would be executed. Yet some version of this dynamic wrote every single program you have ever used, banking software, websites, and a ubiquitously used program that was supposed to protect information on the internet but didn’t.\n\nEvery programmer occasionally, when nobody’s home, turns off the lights, pours a glass of scotch, puts on some light German electronica, and opens up a file on their computer. It’s a different file for every programmer. Sometimes they wrote it, sometimes they found it and knew they had to save it. They read over the lines, and weep at their beauty, then the tears turn bitter as they remember the rest of the files and the inevitable collapse of all that is good and true in the world.\n\nThis file is Good Code. It has sensible and consistent names for functions and variables. It’s concise. It doesn’t do anything obviously stupid. It has never had to live in the wild, or answer to a sales team. It does exactly one, mundane, specific thing, and it does it well. It was written by a single person, and never touched by another. It reads like poetry written by someone over thirty.\n\nEvery programmer starts out writing some perfect little snowflake like this. Then they’re told on Friday they need to have six hundred snowflakes written by Tuesday, so they cheat a bit here and there and maybe copy a few snowflakes and try to stick them together or they have to ask a coworker to work on one who melts it and then all the programmers’ snowflakes get dumped together in some inscrutable shape and somebody leans a Picasso on it because nobody wants to see the cat urine soaking into all your broken snowflakes melting in the light of day. Next week, everybody shovels more snow on it to keep the Picasso from falling over.\n\nThere’s a theory that you can cure this by following standards, except there are more “standards” than there are things computers can actually do, and these standards are all variously improved and maligned by the personal preferences of the people coding them, so no collection of code has ever made it into the real world without doing a few dozen identical things a few dozen not even remotely similar ways. The first few weeks of any job are just figuring out how a program works even if you’re familiar with every single language, framework, and standard that’s involved, because standards are unicorns.\n\nI spent a few years growing up with a closet in my bedroom. The closet had an odd design. It looked normal at first, then you walked in to do closet things, and discovered that the wall on your right gave way to an alcove, making for a handy little shelf. Then you looked up, and the wall at the back of the alcove gave way again, into a crawlspace of utter nothingness, where no light could fall and which you immediately identified as the daytime retreat for every ravenous monster you kept at bay with flashlights and stuffed animals each night.\n\nThis is what it is to learn programming. You get to know your useful tools, then you look around, and there are some handy new tools nearby and those tools show you the bottomless horror that was always right next to your bed.\n\nFor example, say you’re an average web developer. You’re familiar with a dozen programming languages, tons of helpful libraries, standards, protocols, what have you. You still have to learn more at the rate of about one a week, and remember to check the hundreds of things you know to see if they’ve been updated or broken and make sure they all still work together and that nobody fixed the bug in one of them that you exploited to do something you thought was really clever one weekend when you were drunk. You’re all up to date, so that’s cool, then everything breaks.\n\n“Double you tee eff?” you say, and start hunting for the problem. You discover that one day, some idiot decided that since another idiot decided that 1/0 should equal infinity, they could just use that as a shorthand for “Infinity” when simplifying their code. Then a non-idiot rightly decided that this was idiotic, which is what the original idiot should have decided, but since he didn’t, the non-idiot decided to be a dick and make this a failing error in his new compiler. Then he decided he wasn’t going to tell anyone that this was an error, because he’s a dick, and now all your snowflakes are urine and you can’t even find the cat.\n\nYou are an expert in all these technologies, and that’s a good thing, because that expertise let you spend only six hours figuring out what went wrong, as opposed to losing your job. You now have one extra little fact to tuck away in the millions of little facts you have to memorize because so many of the programs you depend on are written by dicks and idiots.\n\nAnd that’s just in your own chosen field, which represents such a tiny fraction of all the things there are to know in computer science you might as well never have learned anything at all. Not a single living person knows how everything in your five-year-old MacBook actually works. Why do we tell you to turn it off and on again? Because we don’t have the slightest clue what’s wrong with it, and it’s really easy to induce coma in computers and have their built-in team of automatic doctors try to figure it out for us. The only reason coders’ computers work better than non-coders’ computers is coders know computers are schizophrenic little children with auto-immune diseases and we don’t beat them when they’re bad.\n\nRemember that stuff about crazy people and bad code? The internet is that except it’s literally a billion times worse. Websites that are glorified shopping carts with maybe three dynamic pages are maintained by teams of people around the clock, because the truth is everything is breaking all the time, everywhere, for everyone. Right now someone who works for Facebook is getting tens of thousands of error messages and frantically trying to find the problem before the whole charade collapses. There’s a team at a Google office that hasn’t slept in three days. Somewhere there’s a database programmer surrounded by empty Mountain Dew bottles whose husband thinks she’s dead. And if these people stop, the world burns. Most people don’t even know what sysadmins do, but trust me, if they all took a lunch break at the same time they wouldn’t make it to the deli before you ran out of bullets protecting your canned goods from roving bands of mutants.\n\nYou can’t restart the internet. Trillions of dollars depend on a rickety cobweb of unofficial agreements and “good enough for now” code with comments like “TODO: FIX THIS IT’S A REALLY DANGEROUS HACK BUT I DON’T KNOW WHAT’S WRONG” that were written ten years ago. I haven’t even mentioned the legions of people attacking various parts of the internet for espionage and profit or because they’re bored. Ever heard of 4chan? 4chan might destroy your life and business because they decided they didn’t like you for an afternoon, and we don’t even worry about 4chan because another nuke doesn’t make that much difference in a nuclear winter.\n\nOn the internet, it’s okay to say, “You know, this kind of works some of the time if you’re using the right technology,” and BAM! it’s part of the internet now. Anybody with a couple of hundred dollars and a computer can snag a little bit of the internet and put up whatever awful chunks of hack code they want and then attach their little bit to a bunch of big bits and everything gets a little bit worse. Even the good coders don’t bother to learn the arcane specifications outlined by the organizations people set up to implement some unicorns, so everybody spends half their time coping with the fact that nothing matches anything or makes any sense and might break at any time and we just try to cover it up and hope no one notices.\n\nHere are the secret rules of the internet: five minutes after you open a web browser for the first time, a kid in Russia has your social security number. Did you A computer at the NSA now automatically tracks your physical location for the rest of your life. Sent an email? Your email address just went up on a billboard in Nigeria.\n\nThese things aren’t true because we don’t care and don’t try to stop them, they’re true because everything is broken because there’s no good code and everybody’s just trying to keep it running. That’s your job if you work with the internet: hoping the last thing you wrote is good enough to survive for a few hours so you can eat dinner and catch a nap.\n\nFunny, right? No? How about this exchange:\n\nWasn’t that guy helpful? With the camel? Doesn’t that seem like an appropriate response? No? Good. You can still find Jesus. You have not yet spent so much of your life reading code that you begin to talk in it. The human brain isn’t particularly good at basic logic and now there’s a whole career in doing nothing but really, really complex logic. Vast chains of abstract conditions and requirements have to be picked through to discover things like missing commas. Doing this all day leaves you in a state of mild aphasia as you look at people’s faces while they’re speaking and you don’t know they’ve finished because there’s no semicolon. You immerse yourself in a world of total meaninglessness where all that matters is a little series of numbers went into a giant labyrinth of symbols and a different series of numbers or a picture of a kitten came out the other end.\n\nThe destructive impact on the brain is demonstrated by the programming languages people write. This is a program:\n\nThat program does exactly the same thing as this program:\n\nAnd once somebody wrote a programming language that let somebody else write this:\n\nAccording to the author, that program is \"two lines of code that parse two lines of embedded comments in the code to read the Mayan numbers representing the individual ASCII characters that make up the magazine title, rendered in 90-degree rotated ASCII art.\"\n\nThat program won a contest, because of course it did. Do you want to live in a world like this? No. This is a world where you can smoke a pack a day and nobody even questions it. \"Of course he smokes a pack a day, who wouldn't?\" Eventually every programmer wakes up and before they're fully conscious they see their whole world and every relationship in it as chunks of code, and they trade stories about it as if sleepiness triggering acid trips is a normal thing that happens to people. This is a world where people eschew sex to write a programming language for orangutans. All programmers are forcing their brains to do things brains were never meant to do in a situation they can never make better, ten to fifteen hours a day, five to seven days a week, and every one of them is slowly going mad.\n\nSo no, I’m not required to be able to lift objects weighing up to fifty pounds. I traded that for the opportunity to trim Satan’s pubic hair while he dines out of my open skull so a few bits of the internet will continue to work for a few more days.\n\n(Update: now available in Greek, Czech, Italian, Russian, Portuguese, Hungarian, French, Hebrew (PDF by Ilil Hoz), German (PDF by Kurt Frock), Spanish, and Chinese)",
    "readingTime": 13,
    "keywords": [
      "programming languages",
      "you’re familiar",
      "programming language",
      "suspension bridge",
      "don’t",
      "it’s",
      "internet",
      "they’re",
      "there’s",
      "together"
    ],
    "qualityScore": 1,
    "link": "https://www.stilldrinking.org/programming-sucks",
    "thumbnail_url": "https://www.stilldrinking.org/blog_images/programming-sucks.jpg",
    "created_at": "2026-01-28T06:22:46.410Z",
    "topic": "tech"
  },
  {
    "slug": "pixel-arcade-studio-kids-make-playable-browser-games-by-instructing-ai",
    "title": "Pixel Arcade Studio –kids make playable browser games by instructing AI",
    "description": "Kids create real browser games by giving clear instructions to AI. No coding, no downloads.",
    "fullText": "They need to learn how to give clear instructions to AI.\n\nPixel Arcade Studio is a browser-based game studio where kids create real, playable games by telling an AI assistant exactly what to build. No coding. No installs. Designed with parents in mind.\n\nSafe, creative screen time kids love. Try free for 14 days.\n\nFast \"time-to-wow\" — kids see their games come to life quickly.\n\nFrictionless — everything runs directly in the browser.\n\nSafety default — games publish with privacy protections enabled.\n\nCoding used to be how people told computers what to do.\n\nToday, the more important skill is knowing how to describe what you want, break ideas into steps, and give clear instructions to an AI system.\n\nPixel Arcade Studio is built around that shift.\n\nKids don't write code here. They practice explaining ideas, testing results, and improving their instructions when something doesn't work.\n\nThat's the skill they'll use in the real world.\n\nYour child picks a game template and describes what they want to make. Characters, goals, movement, and rules.\n\nYour child tells the AI assistant what to create or change. The AI follows instructions. It does not take over.\n\nThe game runs right away in the browser. No setup and no waiting.\n\nKids adjust their instructions, test again, and see how clearer directions lead to better results.\n\nGames can be shared with family or friends using parent-approved links.\n\nThis is not about memorizing technical skills. It's about clear thinking and communication.\n\nAI in Pixel Arcade Studio is a tool, not a shortcut.\n\nIt responds only to what your child asks. It does not browse the internet. It does not publish content on its own. It stays inside kid-safe boundaries.\n\nYour child stays in control. The AI helps carry out instructions.\n\nPixel Arcade Studio is built for families who want creative screen time without constant supervision.\n\nInstead, kids focus on giving clear instructions and seeing real results. They make games people can actually play.\n\nEvery game made in Pixel Arcade Studio is playable in the browser and shareable through safe links.\n\nKids don't just save projects. They create something real and playable.\n\nPixel Arcade Studio is a browser-based game studio where kids create playable games by giving instructions to an AI assistant. There is no coding involved.\n\nNo. Pixel Arcade Studio does not teach coding. Kids learn how to clearly describe ideas, give instructions, and work with AI to create games.\n\nPixel Arcade Studio is designed for kids ages 7 to 12.\n\nYes. Games publish in safe mode by default, sharing requires parent approval, and the platform includes content filtering and privacy protections.\n\nNo. Everything runs directly in the web browser. There are no downloads or installs.\n\nThe AI follows your child's instructions. It helps turn ideas into games but does not take control or act on its own.\n\nYes. Games can be shared using parent-approved links so friends and family can play safely.\n\nPixel Arcade Studio does not involve coding or block-based programming. It focuses on teaching kids how to give clear instructions to AI and refine their ideas through iteration.\n\nNo coding. No downloads. Designed for ages 7–12.",
    "readingTime": 3,
    "keywords": [
      "pixel arcade studio",
      "creative screen",
      "privacy protections",
      "parent-approved links",
      "browser-based game",
      "kids don't",
      "games publish",
      "playable games",
      "the ai",
      "kids create"
    ],
    "qualityScore": 1,
    "link": "https://pixelarcade.studio",
    "thumbnail_url": "http://localhost:3000/images/pas_og.jpg",
    "created_at": "2026-01-28T06:22:43.763Z",
    "topic": "tech"
  },
  {
    "slug": "acm-sigplan-symposium-on-principles-of-programming-languages-popl-2026-talks",
    "title": "ACM SIGPLAN Symposium on Principles of Programming Languages (POPL) 2026 talks",
    "description": "Special Interest Group on Programming Languages\nThe ACM Special Interest Group on Programming Languages (SIGPLAN) explores programming language concepts and tools, focusing on design, implementation, practice, and theory. Its members are programming language developers, educators, implementers, researchers, theoreticians, and users.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.youtube.com/channel/UCwG9512Wm7jSS6Iqshz4Dpg",
    "thumbnail_url": "https://yt3.googleusercontent.com/ytc/AIdro_mB1glk2WSnakJ9VmIADfpyj122SV1iL5zw1rMBEQFIqQ=s900-c-k-c0x00ffffff-no-rj",
    "created_at": "2026-01-27T18:24:29.546Z",
    "topic": "tech"
  },
  {
    "slug": "agent-skills-from-claude-to-open-standard-to-your-daily-coding-workflow",
    "title": "Agent Skills: From Claude to Open Standard to Your Daily Coding Workflow",
    "description": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we're witnessing something far more...",
    "fullText": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we’re witnessing something far more significant: an open standard reshaping how people across roles—developers, designers, product managers, and operations—work with AI assistants. AI coding agents’ adoption of Agent Skills has transformed this technology from an interesting experiment into an essential developer tool.\n\nIf you’ve been using custom instructions or wondering how to make your AI assistant truly understand your project’s workflows, Agent Skills provides a compelling solution.\n\nAgent Skills began with Anthropic’s Claude AI, where developers first experienced giving AI agents specialized capabilities through structured instructions. Unlike simple prompts or one-off commands, Agent Skills introduced a sophisticated approach: packaging instructions, scripts, templates, and documentation into reusable, discoverable units.\n\nAnthropic’s decision to release Agent Skills as an open standard transformed it from a Claude-specific feature into a movement. The format’s simplicity and effectiveness attracted attention across the AI development tools ecosystem. Today, major players—Claude Code, GitHub Copilot, Cursor, OpenCode, Mistral Vibe, Antigravity, OpenAI Codex, and Kiro—have adopted the standard. Others are exploring integration, and more are joining it (I’m looking at you, JetBrains Junie).\n\nAgent Skills are elegantly simple: a folder containing a SKILL.md file. This file uses YAML frontmatter for metadata and Markdown for instructions. No complex APIs, no proprietary formats—just structured text any AI agent can understand.\n\nHere’s a basic Agent Skills example for creating NUnit unit tests in C#:\n\nThose Agent Skills files live in your agent configuration, for example for GitHub Copilot .github/skills/create-nunit-unit-test/SKILL.md in your repository. Or they can be globally installed for your user account, e.g. ~/.copilot/skills/create-nunit-unit-test/SKILL.md.\n\nThis is a minimal example. You can add resources like templates, example configurations, or helper scripts in the same directory, and the skill can reference them.\n\nWhat makes Agent Skills innovative isn’t just the format—it’s how AI agents consume them. The system uses a three-level progressive disclosure approach that optimizes context window usage:\n\nLevel 1: Discovery — At startup, the agent reads only the name and description from each skill. This lightweight metadata helps the agent understand available capabilities without consuming context.\n\nLevel 2: Activation — When your request matches a skill’s description, the agent loads the full instructions from the SKILL.md file. Only then do detailed procedures become available.\n\nLevel 3: Execution — The agent accesses additional files (scripts, examples, documentation) only as needed during execution.\n\nThis architecture solves a critical problem: you can install dozens of Agent Skills without overwhelming the AI’s context window. The agent loads only what’s relevant to your current development task.\n\nGitHub Copilot’s Agent Skills are experimental since December 2025 (version 1.108) for VS Code. Here’s your step-by-step setup guide:\n\nInstall VS Code — Download from code.visualstudio.com\n\nEnable Agent Skills — Open settings (Ctrl+,) and enable chat.useAgentSkills\n\nCreate your skills directory — In your project root, create .github/skills/\n\nAdd your first skill — Create a subdirectory for each skill with its SKILL.md file\n\nUse Agent Mode — In Copilot Chat, switch to Agent mode to leverage skills\n\nOnce configured, Agent Skills activate automatically based on your prompts. No manual selection required—the AI determines which skills are relevant based on your descriptions.\n\nSkills share knowledge—best practices, workflows, and procedural guidance—in simple Markdown SKILL.md files that anyone can author; they load progressively to conserve tokens, require no server, and run on web, desktop, and CLI, making them ideal for documentation, checklists, examples, and repeatable workflows.\n\nMCP extends functionality by connecting to APIs, databases, and external tools: it consists of code and service/tool definitions that require development and hosting, loads tool definitions up front (consuming more context), so it’s best suited for tasks needing direct access to external systems.\n\nUse Skills to make knowledge discoverable and consistent, and use MCP to perform integrated actions and extend platform capabilities; together they provide both lightweight guidance and powerful automation.\n\nNevertheless, I can imagine a future where Agent Skills replace MCP for many scenarios, given their simplicity, portability, and ease of authoring. As you can bundle scripts and resources with skills, they can cover many use cases MCP currently serves.\n\nYou might wonder how Agent Skills differ from the custom instructions feature. Custom Instructions are best for defining coding standards and conventions, setting language or framework preferences, specifying code-review guidelines, and establishing commit-message formats. Agent Skills are designed to package reusable workflows, include executable scripts and templates, define specialized procedures (testing, debugging, deployment), and enable capabilities that run beyond the IDE (CLI and coding agents).\n\nThink of custom instructions as your coding style guide and Agent Skills as your AI development toolbox. Custom instructions tell the AI how you want code written; Agent Skills give the AI specialized capabilities to perform complex development tasks.\n\nHere are some practical Agent Skills that can transform your daily development workflow. Check the references section for pointers to more ready-to-use skills:\n\nRead the Agent Skills Specification to understand the format and capabilities. Use Skill Creator, an Agent Skill to create and refine new Agent Skills. Inception moment anyone 🤔?\n\nStart building your Agent Skills collection with these proven strategies:\n\nIdentify Repetitive Tasks — Notice which development workflows you explain to the AI repeatedly. Each recurring explanation is a candidate for an Agent Skill.\n\nStart Simple — Begin with straightforward skills that codify standard development procedures. As you gain confidence, add scripts and more complex resources.\n\nMake Descriptions Specific — The quality of your skill’s description directly impacts how well the AI knows when to activate it. Be explicit about use cases and capabilities.\n\nInclude Examples — Agent Skills with concrete code examples are more effective. Show the AI what good output looks like.\n\nLeverage Community Skills — Explore the github/awesome-copilot and anthropics/skills repositories for inspiration and ready-to-use skills.\n\nOrganize by Domain — Group related Agent Skills together. Create separate skills for testing, deployment, documentation, code review, and other specialized development domains.\n\nHere’s how Agent Skills enhance your workflow in a typical development scenario:\n\nYou’re working on a web application and need to add a new REST API endpoint with proper testing and documentation. With appropriate Agent Skills in place:\n\nYou ask: “Help me add a new user registration endpoint with validation”\n\nThe rest-api-integration skill activates, providing structured guidance on implementing the endpoint with proper authentication, validation, and error handling.\n\nYou ask: “Create tests for this endpoint”\n\nThe webapp-testing skill engages, generating test cases for success scenarios, validation failures, and edge cases.\n\nYou ask: “Generate documentation for this endpoint”\n\nThe api-documentation skill activates, producing comprehensive documentation with examples, error codes, and authentication details.\n\nEach Agent Skill ensures consistency in approach and completeness in implementation. Without skills, you’d need to provide detailed instructions for each request or rely on the AI’s general knowledge, which might miss project-specific patterns.\n\nWhen working with Agent Skills, especially community-shared skills, keep these security considerations in mind:\n\nReview Before Use — Always examine shared Agent Skills before adding them to your project. Check for potentially malicious scripts or unexpected behaviors in the SKILL.md file and associated resources.\n\nUse Terminal Controls — VS Code’s terminal tool provides controls for script execution, including auto-approve options with configurable allow-lists. Configure these appropriately for your security requirements.\n\nVersion Control Your Skills — Agent Skills are just files, so commit them to your repository. This enables code review, versioning, and team collaboration on AI capabilities.\n\nTest in Safe Environments — Try new Agent Skills in development environments before using them in production contexts. Dev containers or isolated workspaces are ideal for testing.\n\nDocument Team Skills — If your team uses shared Agent Skills, maintain documentation about what each skill does and when to use it.\n\nAgent Skills represent more than a new feature—it’s a glimpse into a future where AI development capabilities are portable, shareable, and composable. As more AI tools adopt the standard, we’re moving toward an ecosystem where:\n\nWhether you’re using VS Code or any other editor/IDE, working in the terminal with Copilot CLI, or leveraging any coding agent for automated tasks, your Agent Skills come with you.\n\nReady to integrate Agent Skills into your development workflow? Follow this action plan:\n\nThe goal isn’t to create dozens of Agent Skills immediately. Start with one or two that solve real problems in your development workflow, then expand your library organically as needs arise.\n\nYou can also use Agent Skills with GitHub Copilot CLI or Gemini CLI for terminal-based workflows, or with other coding agents that support the open standard. This portability ensures your investment in creating skills pays off across all your AI-assisted development tools.\n\nMy preferred introduction to Agent Skills is the following video from Burke Holland, which covers the concepts, setup, and practical examples in under 20 minutes:\n\nFor my French readers, I discussed Agent Skills in depth on devdevdev.net in the following episode\n\nAgent Skills bridges the gap between generic AI assistance and specialized, context-aware support for your specific development needs. By adopting an open standard that works across AI tools, the industry has created a foundation for truly portable AI capabilities.\n\nThe journey from Claude to open standard to GitHub Copilot adoption demonstrates the power of simplicity and interoperability in developer tools. As developers, we benefit from this ecosystem approach—our investment in creating Agent Skills pays dividends across our entire development toolchain.\n\nStart small, experiment with the format, and build Agent Skills that improve your daily development work. The progressive disclosure system ensures you won’t overwhelm your AI assistant, and the portable format guarantees your skills remain valuable as AI tools evolve.\n\nThe future of AI-assisted development isn’t just about more powerful models—it’s about giving those models the right context, capabilities, and knowledge to be genuinely helpful in your specific development domain. Agent Skills is a significant step in that direction.\n\nWhat development workflows could benefit from specialized Agent Skills? Have you tried creating skills for your AI coding assistant? Share your experiences in the comments below.",
    "readingTime": 9,
    "keywords": [
      "agent skills",
      "skill.md file",
      "progressive disclosure",
      "ai-assisted development",
      "skill’s description",
      "context window",
      "agent mode",
      "shared agent",
      "coding agents",
      "skill activates"
    ],
    "qualityScore": 1,
    "link": "https://laurentkempe.com/2026/01/27/Agent-Skills-From-Claude-to-Open-Standard/",
    "thumbnail_url": "https://live.staticflickr.com/65535/55058424290_cced09531e_h.jpg",
    "created_at": "2026-01-27T12:26:46.640Z",
    "topic": "tech"
  },
  {
    "slug": "one-developer-used-claude-to-build-a-memorysafe-extension-of-c",
    "title": "One developer used Claude to build a memory-safe extension of C",
    "description": "feature: Robin Rowe talks about coding, programming education, and China in the age of AI",
    "fullText": "feature TrapC, a memory-safe version of the C programming language, is almost ready for testing.\n\n\"We're almost there,\" Robin Rowe told The Register in a phone interview. \"It almost works.\"\n\nWe caught up with Rowe, a computer science professor and entrepreneur, amid debugging efforts that had kept him up until four in the morning. The long-awaited TrapC website has appeared.\n\n\"My work building TrapC has taken two parallel paths,\" Rowe explains in his initial post. \"A TrapC interpreter called itrapc and a separate compiler called trapc. I had wanted to make a software release by 1 January 2026, but too many bugs. I only reached code complete this month and am now on the painstaking and sleepless process of debugging. When I have something stable that mostly works I will make a release. Sorry to make you wait a little longer. Aiming for Q1 2026.\"\n\nBack in November 2024, Rowe explained that he was working on TrapC. At the time, the public and private sector had undertaken a campaign to promote memory-safe software development as a way to reduce exposure to serious vulnerabilities.\n\nMemory safety provides a way of ensuring that memory-related bugs like out-of-bounds reads/writes and use-after-free don't happen. In large codebases, like Chromium and Windows, most of the security vulnerabilities follow from memory bugs. As that message has been repeated in recent years, memory safety has become an imperative, evangelized by the likes of Google and Microsoft, and more recently by authorities in the US and elsewhere.\n\nFor at least the past ten years, there's been a rising chorus of voices calling for the adoption of memory-safe programming languages and techniques. This has meant encouraging developers to avoid languages like C and C++ where feasible, and to adopt languages like C#, Go, Java, Python, Swift, and Rust, instead, particularly for new projects.\n\nTo remain relevant, the C and C++ communities have tried to address those concerns with projects like TrapC, FilC, Mini-C, Safe C++, and C++ Profiles. There's also a C to Rust conversion project under development at DARPA called TRACTOR – TRanslating All C TO Rust.\n\nBut progress has been slow and those writing in C and C++ haven't found a widely accepted approach. The C++ standards committee recently rejected the Safe C++ proposal. And Rowe said he doubted TRACTOR would have anything to show this year.\n\nMeanwhile, the clock is ticking. Microsoft engineer Galen Hunt last month said, \"My goal is to eliminate every line of C and C++ from Microsoft by 2030. Our strategy is to combine AI and algorithms to rewrite Microsoft's largest codebases.\"\n\n\"There are some efforts to port C code by hand to Rust,\" said Rowe. \"But there're some real challenges to doing that because there are some idioms in C that cannot be expressed in Rust.\n\n\"Rust is much more type safe than C is. And so if you have a void pointer, what does that mean in Rust? There's no translation for it. And that's how TrapC is fundamentally different because it actually remembers what that void pointer actually is.\"\n\nRowe said he expects TRACTOR will eventually be able to accomplish C to Rust translation using AI. But he said he thinks it's better to just build the necessary tooling into the C compiler, so you don't have to rely on some external tool that rewrites your code in an unfamiliar language.\n\nRowe has been using AI tools himself and has been teaching others to do so. This past semester, he taught AI Cybersecurity Programmer Analyst (PCO471) at Community College of Baltimore County – Linux administration using vibe coding in bash with no prerequisites. And starting in February, he's teaching C++ Programming with Generative AI (PCO472) – vibe coding in C++.\n\nRowe said programming has fundamentally changed as a result of AI tools. \"I think this is sort of the same type of discussion as when C came in and people said, 'Well, I'm happy in assembly.' There will still be people doing it the old way. But because vibe programming is so much more efficient on time when done correctly, there's gonna be no choice. You just won't be competitive if you're not vibe programming.\"\n\nThen he shifted gears, slightly. \"But I have to walk that back a little bit because the reason I was up until four in the morning is I had vibe programming working on the Trap C compiler. And it took a fundamentally wrong design turn. And I didn't detect that it had made a design mistake. I had told it how I wanted to approach it. But somehow it misunderstood me or it forgot or something happened and I forgot to check. And so I spent hours doodling around in the debugger and trying to understand why code was acting weird before I finally looked at it and said, 'wait a minute, this isn't even the right design.'\"\n\nRowe said a similar situation crops up in pair programming, where you've told someone to do something and they didn't do it, and you don't realize that until later.\n\n\"[C++ creator] Bjarne Stroustrup famously said that the most important thing in software design is to be clear about what you're trying to build,\" Rowe said. \"And vibe [programming] just puts that on steroids. Now we not only have to be ourselves clear, but we have to communicate it clearly to an LLM.\"\n\nRowe argues that developers have to be encouraged to try AI tools and to make mistakes. He recounted how during his AI Cybersecurity Programmer Analyst course, his students expressed interest in doing more hands-on work in lieu of lectures.\n\n\"So I said, 'I've got real servers on the internet that are my companies. I'll give you root,'\" he recalled. \"I'll set loose students who know nothing on my own servers and hope for the best and we'll see how this goes. And the reaction was panic. I couldn't get past the timidity cliff.\"\n\nRowe said that what he learned from that exchange was that they didn't want their own hands-on, they wanted to watch him work.\n\n\"I said to them, 'But guys, this is like learning to play the piano. You can't learn to play the piano by watching me. Yeah, you guys have to practice. And it's gonna be embarrassing at first. You know, you're gonna play a lot of bad notes and sound terrible. You have to get over that situation'.\"\n\nThat's a scenario playing out in various companies where AI tools remain underutilized, for various reasons, including lack of training, security concerns, lack of utility, and poor tool design.\n\nRowe has traveled often to China to speak at the China Association of Higher Education conference. In December, he said, he was interviewed on China News Television about how China's plan for AI compares with America's.\n\nIn an email he explained, \"I said, 'China's AI-Plus plan calls for efficient AI on devices everywhere, from farm to factory to city, while the White House plan calls for building 500-billion-dollar cloud data centers ... using chips that will, inevitably, seem obsolete within two years.'\"\n\nRowe argues China's approach will prevail and that the US has taken the wrong turn by focusing on centralized cloud datacenters to run LLMs. Within two years, he said, we'll have AI models we can run locally on our phones, with no need for network access for most tasks. Apple and Huawei, he said, are likely to be the winners in this scenario.\n\nRowe pointed to China's DeepSeek as an example. While it may not be quite as good as the leading US commercial models, he said, it runs with far less power.\n\n\"This is a very Moore's Law type of strategy,\" he said. \"I remember when I had a Navy supercomputer in 1994. That was an amazing technology. But in 1995, Cray went bankrupt. There weren't enough buyers for it, even though it was an amazing device.\n\n\"And now I've got an iPhone that's in my pocket. That runs on a battery. It doesn't have a whole room devoted to it and exotic cooling and all kinds of stuff. And it's more powerful than that [the Cray from 1994]. So as a long-term strategy, you know, going toward the device makes a lot more sense, because that half-trillion dollar data center is going to be on my iPhone eventually.\"\n\nRowe also said that on the recommendation of a friend from his time at the AT&T DIRECTV Innovation Lab, he tried running Deepseek at a time when Claude wasn't available. Deepseek, he said, was able to find a bug that Claude couldn't.\n\n\"Surprisingly, the bug was in code Claude had generated, that I had cut-and-pasted carelessly,\" he said. \"With hindsight it was a silly code mistake I should have caught, but was in an 'else' branch outside where I was looking. I'd not expected or intended to have Claude make any change to that block of code. And because the code was valid but the logic wrong, the compiler didn't catch it.\"\n\nBut the bug was obvious, he said, as soon as Deepseek pointed it out. He added, \"I'm paying $200/year for Claude. Deepseek is free.\" ®",
    "readingTime": 8,
    "keywords": [
      "cybersecurity programmer",
      "programmer analyst",
      "rowe argues",
      "void pointer",
      "memory safety",
      "vibe coding",
      "design rowe",
      "vibe programming",
      "code",
      "compiler"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/26/trapc_claude_c_memory_safe_robin_rowe/",
    "thumbnail_url": "https://regmedia.co.uk/2022/03/23/shutterstock_c.jpg",
    "created_at": "2026-01-27T06:21:27.835Z",
    "topic": "tech"
  },
  {
    "slug": "a-developer-teamed-up-with-claude-to-create-elo-programming-language",
    "title": "A developer teamed up with Claude to create Elo programming language",
    "description": "feature: Bernard Lambeau, the human half of a pair programming team, explains how he's using AI",
    "fullText": "feature Bernard Lambeau, a Belgium-based software developer and founder of several technology companies, created a programming language called Elo with the help of Anthropic's Claude Code.\n\nStarting on December 25, 2025, he published a series of posts about the project. The first post names Claude as a co-author.\n\n\"In roughly 24 hours of collaboration, we built a complete expression language with a parser, type system, three compilers, a standard library, a CLI tool, and a documentation website. Not bad for a day's work,” Lambeau and Claude wrote.\n\n\"Elo isn't just a demonstration that AI can write code. It's a demonstration that humans and AI can build together – each contributing what they do best,” they added.\n\nAs an expression language that compiles to JavaScript, Ruby, and SQL, Elo is intended as a portable way to handle form validation, e-commerce order processing, and subscription logic.\n\nLambeau, founder and CTO of Klaro Cards and CEO of app consultancy Enspirit, is not the first to develop a programming language with the help of AI.\n\nSteve Klabnik performed a similar feat last year with the Rue programming language. In September 2025, Geoffrey Huntley enlisted Claude to write a programming language called Cursed. And before that, Avital Tamir published a Claude-authored repo for the Server programming language, with the caveat that the code is not intended for actual use.\n\nClaude Code isn't the only AI-assisted programming method having a moment. AI biz Cursor created a rudimentary browser using OpenAI's GPT-5.2. And developer Ola Prøis used Cursor, powered by Claude, to create a Rust-based text editor called Ferrite.\n\nClaude users generally acknowledge that their pair partner makes mistakes. But those committed to AI assistance find it worthwhile to clean up after their helper.\n\n\"Claude Code knows almost every tech stack (and can search the web), knows the Linux commands that matter (search code, search & replace, compile, test, etc.), and does that 10x faster than I can do myself,\" Lambeau told The Register in an email interview.\n\nClaude, he said, allows him to use technology he hasn't mastered.\n\n\"I was already a full-stack developer (on languages, frameworks & reusable libraries I knew); I'm now a full-stack++ dev because I can also use languages, frameworks, and reusable libraries I barely know, if at all,\" he explained.\n\n\"Claude Code falls short if you don't have a great methodology. It needs feedback loops to work fine; otherwise, it derails. One possible feedback loop is a human reviewing code and testing manually. But there's a better/complementary approach if you want it to work autonomously. On both Elo and Bmg.js, I've started by making sure the testing methodology was effective and scientifically sound. Claude writes the tests, executes them, discovers where it's wrong, and corrects itself. Impressive.\"\n\nLambeau said he still needs to review some of Claude's output.\n\n\"But if I read the tests, agree with them, and can check myself that they run fine, I'm 95 percent sure it's already correct as a black box (not even reading the code),\" he explained. \"Then I can check the architecture and code quality as a white box by having a general look at the code, but I don't have to understand every detail.\"\n\nNotably, Lambeau documented the series of prompts he used to create the language. The repo includes more than 100 tasks used to direct the AI model. In addition, Lambeau has published a video that describes his AI pair programming process.\n\n\"I started in a setting where Claude Code asked for permissions every 20 seconds and I was checking everything it did,\" Lambeau explained. \"After a few successes, I quickly set up safe environments to be able to let Claude Code run in full autonomy (isolated computer & isolated Linux user, or running in a Docker image).\"\n\nLambeau said he still uses plan mode for complex tasks that require conversation with Claude.\n\n\"I review the plan, make sure we have a test strategy that's sound, then switch Claude to autonomous mode and look at the tests, code & results afterward,\" he said. \"That's very similar to a lead-dev/CTO + QA role, btw; it's just much faster than with human devs.\"\n\nLambeau, who has a PhD in software engineering and 30 years of experience as a developer, said both experts and novices can benefit from Claude Code, though he added that a service like Lovable might be more approachable for those not already acclimated to the command line.\n\n\"Now, when it comes to real software/product engineering, I think Claude Code requires experts (so far),\" he said. \"You still need to guide it a lot to keep the quality high enough. You need very strong expertise to do it effectively. Currently (Claude will still improve a lot), if you don't have the expertise, you certainly end up with a big mess of unmaintainable code.\"\n\nMany developers have said as much about AI tools. They're more useful as an amplifier of expertise than as a replacement for it. The situation is analogous to the introduction of sequencing software, digital synthesizers, and drum machines half a century ago. These tools enabled a lot of people who weren't great musicians to make music. But they didn't instill musical skill, and they produced the most interesting work in the hands of practiced musicians.\n\nThe cost to do this, Lambeau said, has been a Claude Max subscription that he purchased in December for €180 a month. In that time, he says, he wrote Elo (https://elo-lang.org), completed Bmg.js (https://github.com/enspirit/bmg.js), completed Bmg's documentation (https://www.relational-algebra.dev), and created the first version of the Try page (https://www.relational-algebra.dev/try).\n\n\"It's all personal research and open-source projects,\" he said. \"It would have required several weeks to do the same manually myself, and several months to ask another developer to do it. The cost would be mostly because of the scientific & technical knowledge transfer about the data language I envision. Strangely enough, it's very cheap with Claude Code. There's something true about the fact that those LLMs have a PhD.\"\n\nLambeau explained that Elo isn't just a way to test Claude Code. He also sees it as an extension of his academic work in software engineering and his personal interest in the Relational Model – he's served as a lecturer for database courses at Belgium’s UCLouvain.\n\n\"I'm absolutely convinced that we need better/safer/simpler programming languages inside no-code tools and when interconnecting them (e.g. Zapier, Make, n8n, etc.),\" he said. \"Mainstream programming languages are very complex, error-prone, sometimes dangerous, and the programs are difficult to review for non-experts.\"\n\n\"More importantly, they are cumbersome to use for even simple data tasks. I mean, even validating the schema and constraints of a data file at runtime tends to be a nightmare in existing languages. It's not built-in in any mainstream language; you immediately need validation libraries; most of them are limited in what they can easily check, so you need to add dedicated boilerplate code.\"\n\nIn a world where non-technical people will have the opportunity to write untrustworthy code with the help of AI, he said, we need to be able to run that code safely.\n\n\"Elo aims at providing a safe & simple alternative,\" he said. \"It will be a limited language (non-Turing-complete, as we say) but super safe & simple, and usable in 80 percent of common data use cases. The very first no-code tool to integrate it will be Klaro Cards, of course.\" ®",
    "readingTime": 7,
    "keywords": [
      "claude code",
      "elo isn't",
      "reusable libraries",
      "safe simple",
      "languages frameworks",
      "software engineering",
      "expression language",
      "programming language",
      "it's",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/24/human_ai_pair_programming_elo/",
    "thumbnail_url": "https://regmedia.co.uk/2025/11/06/shutterstock_balancing_ai_and_humanity.jpg",
    "created_at": "2026-01-26T01:03:11.981Z",
    "topic": "tech"
  },
  {
    "slug": "5-acquisitions-winning-over-skeptical-engineers-and-spending-tens-of-millions-inside-a-public-companys-ai-native-push",
    "title": "5 acquisitions, winning over skeptical engineers, and spending tens of millions: Inside a public company's 'AI native' push",
    "description": "Amplitude gave Business Insider the inside look at its AI overhaul, from acquisitions to efforts to increase staff adoption of AI coding assistants.",
    "fullText": "There's a long banner hanging in Amplitude's San Francisco office. It reads: \"NO MAGICAL THINKING.\"\n\nNo, it's not some rag on Joan Didion. It's a reminder, CEO Spenser Skates told Business Insider, that technology can never replace deep thinking and hard work. In the AI age, that reminder is more important than ever — so much so that employees must look up at it every day.\n\nAmplitude, an 800-person, publicly traded analytics company, is undergoing an AI transformation — with the goal of reinvigorating its business.\n\nAmplitude went public in September 2021 at the height of the pandemic, climbing to an all-time closing high of $84.80 per share several weeks later before dropping significantly and largely plateauing in in recent years around $10. It closed at $10.25 on Friday.\n\nSince October 2024, the company has acquired five AI startups. Amplitude hired an AI-savvy engineering head and appointed one of its acquired founders to a new AI leadership position. It got Cursor and GitHub Copilot licenses for employees, and ran a heads-down AI week.\n\nIt's a change many companies are making: Rapidly moving from little-to-no AI to trying to become \"AI native,\" a term that's curiously hard to pin down. Large language models are popping up everywhere in white-collar work as companies chase the promise of efficiency gains.\n\nAmplitude's case may be especially informative, given just how skeptical of AI its CEO was. In 2023 and some of 2024, Skates said he viewed the AI industry as full of \"grifters,\" the visionaries promising to end world hunger and salesmen promising to automate everything.\n\n\"It had all sorts of problems,\" Skates said. By mid-2024, he realized \"there's probably going to be a breakthrough in the analytics space in the next two or three years.\"\n\n\"We've got to go make that ourselves,\" he said. \"So, we went all in.\"\n\nSkates had two opening moves for his AI overhaul.\n\nThe first: hiring a new chief engineering officer with a history in AI. Wade Chambers had advised the company since 2016, while holding leadership roles at Twitter and Included Health.\n\nWhen Chambers joined in October 2024, only 1% of the engineering, product, and design teams at Amplitude were using AI.\n\nThe second was the acquisition of Command AI, a chatbot startup. It was the first of a string of acquisitions, including June, Kraftful, and Inari. Amplitude announced its most recent acquisition, InfiniGrow, on January 14.\n\nYana Welinder was CEO of Kraftful, one of Amplitude's acquisition targets. Kraftful could spot power users of its product, one of whom was Amplitude's then-CPO. She reached out, and they chatted in February. The deal closed in July, and Welinder was named Amplitude's head of AI. A company blog post with an introductory Q&A referred to her as \"AI maven.\"\n\nWelinder's first order of business: speeding the company up. Kraftful shipped new product every week. Amplitude was shipping less than monthly.\n\n\"If you have this cadence of shipping infrequently, then the team slows down, which isn't appropriate in the age of AI,\" she said.\n\n\"Analytics will look very different 6 months from now,\" Skates wrote in his email. \"We have the opportunity to be the AI native company in Analytics and we are going to pull every piece of firepower we have.\"\n\nHe also asked employees to share a coming launch on X, as opposed to LinkedIn, because that's \"where the AI natives are.\"\n\nHow much has Amplitude spent on AI, from tools to acquisitions? \"Tens of millions, for sure,\" Skates said. \"I wouldn't be surprised if it got past $100 million.\"\n\nThen comes the harder part: convincing employees to really use the tools.\n\nWhile some engineers are excited about AI's promise, others are skeptical about its helpfulness, or worried about possible job losses. Not every engineer is as gung ho about AI as their management is.\n\nSkates said that engineers were especially sensitive to the \"grifting\" that went on in AI, making many of them skeptical. With a bottoms-up approach, that skepticism dissipates, he said.\n\nSoon after joining, Chambers began planning an \"AI week\" for the first week of June. It took six months of prep and borrowed heavily from Facebook's mobile push. He took the entire engineering, product, and design team offline for the week. To kick off, Chambers required that leaders get onstage and vibe-code something in front of the entire company.\n\n\"It didn't go well,\" Chambers said of the live vibe-coding demonstration. \"They had to work through it. They had to re-prompt a couple of different ways.\"\n\nBut the message stuck, he said. Leaders who weren't coding all day were able to build something \"pretty cool\" within the hourlong session, save a few hiccups.\n\nAdditional momentum came from the \"zealots,\" engineers passionate about exploring the new tech (some of whom Chambers brought over from his prior job). These engineers lead by example, he said.\n\nAmplitude shared its internal data tracking how many employees use its AI tools. In the final week of March, 14 employees were actively using Cursor. That figure peaked in the first week of December — after AI week but before the holiday vacation cycle — at 174 employees.\n\nAnd what of the thorny question about AI implementation in the enterprise: ROI? After all, a 2025 MIT study indicated 95% of firms publicly disclosing use of AI pilots reported no measurable ROI.\n\nAfter implementing these tools, developer productivity shot up 40% and stayed there, Chambers said. On some specific engineering teams, those gains looks more like 300-400%, he said.\n\n\"There's going to be a lot of people who are thinking they're the world's best expert at something,\" Chambers said. \"Increasingly, even the most cynical team members have come around.\"",
    "readingTime": 5,
    "keywords": [
      "engineering product",
      "roi after",
      "employees",
      "analytics",
      "tools",
      "engineers",
      "there's",
      "skeptical",
      "acquisition",
      "team"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amplitude-ai-native-push-2026-1",
    "thumbnail_url": "https://i.insider.com/69729d28d3c7faef0eccc73a?width=1200&format=jpeg",
    "created_at": "2026-01-25T12:22:41.686Z",
    "topic": "finance"
  },
  {
    "slug": "skget-another-cli-to-add-skills-to-your-coding-agents",
    "title": "Skget, another CLI to add skills to your coding agents",
    "description": "A CLI to add skills to your coding agents. Contribute to czheo/skget development by creating an account on GitHub.",
    "fullText": "czheo\n\n /\n\n skget\n\n Public\n\n A CLI to add skills to your coding agents.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n czheo/skget",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/czheo/skget",
    "thumbnail_url": "https://opengraph.githubassets.com/ec41d6a0818a69873753057252cfb3dd2acf4c9944a55eb817aea02d4c36721f/czheo/skget",
    "created_at": "2026-01-25T01:04:24.594Z",
    "topic": "tech"
  }
]
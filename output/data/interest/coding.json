[
  {
    "slug": "the-upside-of-opening-up-dei-programs-to-everyone",
    "title": "The Upside of Opening Up DEI Programs to Everyone",
    "description": "In the wake of the Supreme Court‚Äôs seismic 2023 decision ending affirmative action in higher education‚Äîand more than 100 lawsuits challenging organizational DEI efforts‚Äîmany U.S. companies reexamined their programming, opening up to all employees programs that had previously been restricted to members of specific demographic groups. Does this strategy destroy the purpose of these programs themselves? Research suggests there are multiple hidden upsides, including: 1) encouraging effective allyship; 2) putting pressure on non-allies; and 3) reducing backlash.",
    "fullText": "The Upside of Opening Up DEI Programs to Everyone by Kenji Yoshino and David GlasgowFebruary 23, 2026PostPostShareSavePrintSummary.¬†¬†¬†Leer en espa√±olLer em portugu√™sPostPostShareSavePrintOver the past few years in the U.S., more than a hundred federal lawsuits have been filed challenging diversity, equity, and inclusion (DEI) programs in companies, firms, universities, and the public sector.",
    "readingTime": 1,
    "keywords": [
      "dei programs"
    ],
    "qualityScore": 0.2,
    "link": "https://hbr.org/2026/02/the-upside-of-opening-up-dei-programs-to-everyone",
    "thumbnail_url": "/resources/images/article_assets/2026/02/Feb26_23_unsplash.jpg",
    "created_at": "2026-02-23T18:49:03.698Z",
    "topic": "business"
  },
  {
    "slug": "linux-70-makes-preparations-for-rust-195",
    "title": "Linux 7.0 Makes Preparations For Rust 1.95",
    "description": "Last week was the main feature pull of Rust programming language updates for the Linux 7.0 kernel merge window",
    "fullText": "Linux 7.0 Makes Preparations For Rust 1.95\n\nLast week was the main feature pull of Rust programming language updates for the Linux 7.0 kernel merge window. Most notable with that pull was Rust officially concluding its \"experimental\" in now treating Rust for Linux kernel/driver programming as stable and here to stay. Sent out today was a round of Rust fixes for Linux 7.0 that includes preparations for the upcoming Rust 1.95 release.\r\nRust 1.95 is being branched from master on 27 February and aiming for its stable release on 16 April. Rust 1.95 stabilizes if let guards, changing some ports to tier 2 status, and various other changes.\r\nFor Linux 7.0 they are now passing the \"-Zunstable-options\" flag that will be required by the Rust 1.95 release. The -Zunstable-options allows for the use of other new, unstable command line options.\r\nFor the kernel's irq module, there is a missing bound detected by the in-development Rust 1.95 code to be addressed. With the pin-init crate was also a Clippy warning that changed behavior with the upcoming Rust 1.95 release.\r\nMeanwhile this round of Rust fixes for Linux 7.0 also fixes an objtool warning when using the older Rust 1.84 release plus a fix to the list module to address missing \"unsafe\" blocks and placeholder safety comments to macros.\r\nMore details on these Rust fixes sent out today for Linux 7.0 to focus on future Rust 1.95 compatibility can be found via this pull request.\n\n 14 Comments",
    "readingTime": 2,
    "keywords": [
      "upcoming rust",
      "rust release",
      "rust fixes",
      "linux",
      "preparations",
      "programming",
      "round",
      "module",
      "missing",
      "warning"
    ],
    "qualityScore": 0.85,
    "link": "https://www.phoronix.com/news/Linux-7.0-Rust-1.95-Prep",
    "thumbnail_url": "https://www.phoronix.net/image.php?id=2021&image=rust_linux_v2",
    "created_at": "2026-02-22T12:26:17.512Z",
    "topic": "tech"
  },
  {
    "slug": "mini-claw-code-write-your-own-mini-coding-agent",
    "title": "Mini Claw Code ‚Äì Write your own mini coding agent",
    "description": "Build your own mini coding agent in Rust. Contribute to odysa/mini-claw-code development by creating an account on GitHub.",
    "fullText": "odysa\n\n /\n\n mini-claw-code\n\n Public\n\n Build your own mini coding agent in Rust.\n\n odysa.github.io/mini-claw-code/\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n odysa/mini-claw-code",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/odysa/mini-claw-code",
    "thumbnail_url": "https://opengraph.githubassets.com/1908684ebec27b9eea9a65c94f2753ee211d2eefa6260b58bbed17b4529654a8/odysa/mini-claw-code",
    "created_at": "2026-02-22T06:35:08.675Z",
    "topic": "tech"
  },
  {
    "slug": "quill-a-systemwide-tech-dictionary-for-the-ai-coding-era",
    "title": "Quill ‚Äì A system-wide tech dictionary for the AI coding era",
    "description": "Learn what AI writes for you. A system-wide tech dictionary for macOS ‚Äî select any term, press a shortcut, get an instant explanation at your level. - uptakeagency/quill",
    "fullText": "uptakeagency\n\n /\n\n quill\n\n Public\n\n Learn what AI writes for you. A system-wide tech dictionary for macOS ‚Äî select any term, press a shortcut, get an instant explanation at your level.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n uptakeagency/quill",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/uptakeagency/quill",
    "thumbnail_url": "https://opengraph.githubassets.com/a583f40305ae1e7f907452739ae864ce013e2d1098ad6c2d2624177cc2cc4f3c/uptakeagency/quill",
    "created_at": "2026-02-22T01:11:31.810Z",
    "topic": "tech"
  },
  {
    "slug": "i-grew-my-tech-income-to-over-250000-even-though-i-never-broke-into-big-tech-this-salary-negotiation-tip-has-never-gone",
    "title": "I grew my tech income to over $250,000, even though I never broke into Big Tech. This salary negotiation tip has never gone badly for me.",
    "description": "Although he never worked in Big Tech, Brian Jenney earned more than he ever imagined in software development using this salary negotiation strategy.",
    "fullText": "This as-told-to essay is based on a conversation with Brian Jenney, 42, who lives in California. The following has been edited for length and clarity.\n\nWhen I got into tech in my early 30s, I had no clue how crazy lucrative the industry was.\n\nI started as a web application developer in 2015, earning roughly $60,000 a year.\n\nOver the years, I became savvier at negotiating my salary, and by 2023, I was making over $250,000.\n\nI've earned more than I could have ever imagined, without working in Big Tech, where people often assume the big money is made.\n\nHere's what I've learned along the way.\n\nI used to have addiction issues, so I didn't really work from the ages of 25 to 30 and lost knowledge that people gain from white collar jobs at this stage of life.\n\nI was naive about the salary potential of my industry, and when people in tech said they were on $150,000, it blew my mind, and I began to feel underpaid.\n\nIn 2017, after two years in the web developer role, I landed a job at a startup. I was so impressed when I was offered $120,000 that I didn't negotiate my salary, which was dumb in retrospect.\n\nThe environment was extremely fast-paced and high-caliber. I struggled with imposter syndrome as one of the team's more junior members. I felt like one of the worst developers there and that I was already being paid more than I deserved. It discouraged me from asking for a raise.\n\nIn 2019, I joined the media intelligence company Zignal Labs. I was so happy about the job offer that I didn't negotiate the salary, so my pay initially stayed roughly the same as in my previous job. It felt like I had plateaued, even though I had more money than I needed. An unfortunate symptom of working in tech is that you get drawn to wanting more.\n\nChoosing this role turned out to be the right move, though. I had more room to grow and was in a better learning environment at a larger company.\n\nDuring the tech hiring spree of 2020, my peers said they were getting crazy offers, and I didn't want to miss out. That August, I joined The Clorox Company, a manufacturing firm, as a software engineer. By 2023, I was making over $250,000: the peak of my earnings in tech.\n\nIn 2024, I was laid off, and I've continued to work in various software engineering roles. I bought a business in 2023, and my focus has shifted to seeking out flexibility and time to build it, instead of maximizing corporate compensation.\n\nInterviews are like a carnival game where you can win big money by performing well, and you can't get to the negotiation stage without passing them. They're structured, learnable, and winnable with the right preparation.\n\nWhen I have job interviews lined up, I do technical practice and use a platform called Pramp, which pairs you with strangers for practice interviews. I've found this helps simulate the nerves and pressure of real interviews better than practicing with friends.\n\nI'll try to do at least two mock interviews before an interview I really care about.\n\nOver the years, I've benefited a lot from people in tech being open about their salaries and career paths, which helped me understand what was possible and gave me confidence to negotiate and aim higher.\n\nI've learned that salary negotiation is a game you have to play, and if you don't, you lose money.\n\nI began consistently negotiating pay in the late 2010s. I usually tell the employer that I'm really excited to start the job, but that I was hoping to come in at a higher salary range, usually 10 to 20% more.\n\nI've found this to be very effective, and it has never gone badly for me. If an employer sounds firm on their offer, I usually try to explore whether a sign-on bonus is possible instead, but I don't push aggressively beyond that.\n\nI've interviewed with but never been hired by the Big Tech companies. Besides, I like working at smaller startups and non-tech companies where I think you can have a greater impact.\n\nBig Tech employees are going to \"beat me\" on pay because their stock compensation will outpace my earnings. But I see my current lifestyle as comparable to theirs. I believe there's a reason software engineers aren't driving around Mountain View in Ferraris: they can't cash out all their stock money yet.\n\nThere's always more you can earn, but when my salary hit the $150,000 mark, I knew it was more than I needed. I'd rather prioritize jobs where I'm happy, I'm learning, and I can cover my needs while still saving. That's all I really need.\n\nDo you have a story to share about growing your salary in tech? Contact this reporter at ccheong@businessinsider.com",
    "readingTime": 5,
    "keywords": [
      "i've learned",
      "didn't negotiate",
      "big tech",
      "salary",
      "money",
      "interviews",
      "software",
      "crazy",
      "industry",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/grow-tech-income-250k-years-negotiate-higher-salary-interviews-2026-2",
    "thumbnail_url": "https://i.insider.com/69947c1be1ba468a96ac25d4?width=1200&format=jpeg",
    "created_at": "2026-02-21T12:24:55.383Z",
    "topic": "finance"
  },
  {
    "slug": "the-true-cost-of-claude-code",
    "title": "The True Cost of Claude Code",
    "description": "If you're paying $100/month but consuming multiples of that in value, you have to start wondering when that's going to catch up to you. The AI coding tool market is following a familiar playbook.",
    "fullText": "Sometimes the math doesn‚Äôt math. If you‚Äôre paying $100/month on a Claude Code Max plan, but are consuming way more than that, you have to start wondering when that‚Äôs going to catch up to you.\n\nNo, it‚Äôs not a bug. It‚Äôs a business model. And if you‚Äôve been around long enough to remember when an Uber from Manhattan to JFK was $25, you already know how this story ends.\n\nRight now, Claude Code‚Äôs Max plan runs $100/month for what Anthropic‚Äôs own docs describe as roughly $100‚Äì200/developer/month in actual API token costs at average usage. But ‚Äúaverage‚Äù is doing a lot of heavy lifting in that sentence. Power users are burning through multiples of that.\n\nAnthropic knows this. They‚Äôve even been somewhat transparent about it. Their average daily cost per developer sits around $6 in credits, with 90% of users staying below $12. But heavy users, the ones building real things, running agentic workflows, spinning up multi-file reasoning sessions? They‚Äôre consuming far more value than they‚Äôre paying for.\n\n‚ÄúThis isn‚Äôt generosity. This is market capture.‚Äù\n\nUber lost more than $30 billion in the years since the company‚Äôs finances became public, amounting to an enormous, investor-fueled subsidy of America‚Äôs ride-hailing habit. In 2015, Uber passengers were only paying about 41% of the actual cost of their trips. The strategy was straightforward. Make the product so cheap and so convenient that it becomes infrastructure. Then, once alternatives have been starved out and habits are locked in, raise prices.\n\nIt worked. Average Uber prices rose 92% between 2018 and 2021. Kevin Roose at the New York Times called the original pricing a ‚Äúmillennial lifestyle subsidy‚Äù and framed the eventual correction not as price-gouging but as the market finally reflecting reality.\n\nThe AI coding tool market is following the same playbook. Anthropic just closed a $30 billion Series G at a $380 billion valuation. Their annualized revenue hit over $9 billion by end of 2025. They‚Äôre projecting revenue could quadruple this year to as much as $18 billion. But they‚Äôre still burning significant cash, and now expect to turn cash-flow positive in 2028, a year later than previously planned.\n\nThat gap between revenue and profitability? You‚Äôre standing in it. Every subsidized token is venture capital money being converted into your muscle memory and workflow dependency.\n\nThe real cost isn‚Äôt the $100‚Äì$200/month. It‚Äôs what happens when the price reflects reality.\n\nThink about what Claude Code has become for developers who rely on it daily. It‚Äôs not a nice-to-have anymore. It‚Äôs woven into how people architect systems, debug complex issues, and reason through multi-file refactors. When your entire development workflow is optimized around a tool that costs $100 but delivers $2,000 in value, you‚Äôve built a dependency that‚Äôs priced to change.\n\nAnd it will change. It has to. Anthropic expects to spend about $12 billion training models and another $7 billion running them in 2026 alone. No amount of venture capital makes that math work forever.\n\nWhen the correction comes, it probably won‚Äôt look like a dramatic overnight price hike. It‚Äôll look like what we‚Äôre already starting to see. Anthropic introduced new weekly rate limits in August 2025, primarily targeting power users. 5-hour usage windows that reset unpredictably. Token caps that force you to choose between using Opus for the hard problems or Sonnet for everything. Death by a thousand paper cuts until the plan that used to feel unlimited feels very, very limited.\n\nThis isn‚Äôt a ‚Äúdon‚Äôt use Claude Code‚Äù argument. The tool is genuinely powerful. The question is whether you‚Äôre building awareness of your dependency into how you work.\n\nA few things worth thinking about.\n\nAnthropic has begun preparations for a potential IPO as soon as 2026, hiring Wilson Sonsini to advise on the process. The revenue growth is real and impressive. But so was Uber‚Äôs.\n\nThe question isn‚Äôt whether AI coding tools are valuable. They obviously are. The question is what happens when the price tag matches the value. When the $100/month plan becomes $300/month, or usage-based pricing becomes the only option, or the rate limits get tight enough that you‚Äôre effectively paying per-task anyway.\n\nWe‚Äôre in a window right now where the economics of these tools are artificially favorable. That window will close. Not because anyone is being deceptive, but because the math demands it.\n\nThe developers who come out ahead won‚Äôt be the ones who got the most subsidized tokens. They‚Äôll be the ones who used this window to build workflows that are observable, portable, and resilient. The ones who tracked what they consumed, understood what it really cost, and built systems that don‚Äôt have a single point of failure at the inference layer.\n\n‚ÄúThe best time to audit your AI dependencies is before the price correction. Not after.‚Äù\n\nGet updates on open source and distributed systems in AI infrastructure.",
    "readingTime": 4,
    "keywords": [
      "max plan",
      "venture capital",
      "rate limits",
      "claude code",
      "anthropic",
      "math",
      "you‚Äôre",
      "users",
      "ones",
      "they‚Äôre"
    ],
    "qualityScore": 1,
    "link": "https://papercompute.com/blog/true-cost-of-claude-code/",
    "thumbnail_url": "https://papercompute.com/og/true-cost-of-claude-code.png",
    "created_at": "2026-02-21T12:24:51.111Z",
    "topic": "tech"
  },
  {
    "slug": "formula-a-vst-for-coding-custom-dsp-inside-your-daw",
    "title": "Formula: A VST for coding custom DSP inside your DAW",
    "description": "Contribute to soundspear/formula development by creating an account on GitHub.",
    "fullText": "soundspear\n\n /\n\n formula\n\n Public\n\n License\n\n BSL-1.0 license\n\n 159\n stars\n\n 4\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n soundspear/formula",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/soundspear/formula",
    "thumbnail_url": "https://opengraph.githubassets.com/50d5318f3f1a1ea25c7bad63ae3cfa83120549c493a15559cb3871cb3ebfa431/soundspear/formula",
    "created_at": "2026-02-21T06:30:00.067Z",
    "topic": "tech"
  },
  {
    "slug": "peacocks-next-growth-bet-selling-subscriptions-for-other-streamers",
    "title": "Peacock's next growth bet: selling subscriptions for other streamers",
    "description": "Peacock has approached specialty streamers about selling subscriptions to complement its reality TV and sports-heavy programming.",
    "fullText": "Peacock's next growth bet isn't a blockbuster show or sports deal.\n\nNBCU's flagship streaming service is plotting to sell add-on subscriptions to other specialty streamers on its platform, four people familiar with the plans told Business Insider.\n\nPeacock has approached streamers about selling subscriptions to offer viewers content that complements its reality and sports-heavy line-up, these people said. Peacock expects to start with one streamer this year and is likely to limit the offering to a small number of partners.\n\nStarz, which already has multiple distribution partnerships, is one that's being considered, two insiders said. Starz declined to comment.\n\nTwo people briefed on Peacock's pitch saw it as a way for smaller streamers to reach new subscribers in a relatively uncluttered environment, and they hoped Peacock would eventually offer features such as the ability for streamers to offer free samples of their shows.\n\nThey described Peacock's terms as favorable compared to Amazon, which has a large business selling subscriptions to programmers big and small, from HBO Max to Crunchyroll. Amazon's channel terms vary by partner, but two partners told Business Insider in 2025 that Amazon's subscription revenue cut was over 50% in their deals.\n\nPeacock's plans come at a time when streaming services ‚Äî especially outside market leaders Netflix and Disney ‚Äî¬†face pressure to consolidate as they look to continue growing their subscriber bases while remaining profitable. Overall, paid streaming growth in the US has cooled, while cancellation rates have risen in the wake of price hikes.\n\nTV viewership growth for streamers in the US is largely stagnant, and subscribers are navigating an increasingly complex landscape. Streaming services are trying tactics like discounts and bundling to keep people from leaving their platforms.\n\nSome other streaming platforms have adopted a marketplace approach that's broader than what Peacock is contemplating. Amazon is by far the leader. Last year, Amazon reported that its \"Channels\" program accounted for about 25% of US streamer sign-ups, citing Antenna data. Roku, YouTube, and device makers like Samsung and LG also let people \n\nPeacock, for its part, already sells add-on subscriptions to NBC Sports Regional Sports Networks, which it shares a corporate parent with. It also sells a bundle with Apple TV+ that involves cross-platform sampling and a discounted price.\n\nPeacock, with less than 2% of TV watch time in the US, has struggled to grow its share of the TV pie, according to Nielsen. That makes it the second-smallest of the subscription streamers Nielsen measures, ahead only of Warner Bros. Discovery (1.4%), which includes Discovery+ and HBO Max.\n\nUS-only Peacock also has relatively few subscribers, with about 44 million. Its nearest rival, Paramount+, has around 79 million global subscribers, and both are well behind Netflix, which is No. 1 with more than 325 million subscribers.\n\nStill, Peacock has far more subscribers than many specialty streamers. AMC Networks, for example, reported about 10 million subscribers across its portfolio of streamers, including AMC+, Acorn TV, and Shudder, as of the end of 2025.\n\n\"Peacock has been struggling,\" said Alan Wolk, a media industry analyst. \"There haven't been a whole lot of reasons to watch it, so giving people another reason to subscribe is a smart idea. If you ask consumers what's your biggest frustration with streamers, it's always, 'I can't find anything.' So the more you can put things together under one interface, the happier people will be.\"\n\nA global survey by Nielsen in November found more than 46% say it's harder to find the content they want to watch because there are too many streamers, rising to 51% in the US, with people spending 14 minutes searching for what to watch and 49% likely to cancel because they can't find something.\n\nThe survey also showed 66% of people expressed interest in a guide to present content information across all services.\n\nJames Faris contributed reporting.",
    "readingTime": 4,
    "keywords": [
      "add-on subscriptions",
      "streaming services",
      "specialty streamers",
      "subscribers",
      "peacock's",
      "watch",
      "growth",
      "sports",
      "content",
      "peacock"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/nbcu-peacock-exploring-add-on-subscriptions-with-other-streamers-2026-2",
    "thumbnail_url": "https://i.insider.com/6998acb2156648bc16a89ce7?width=1200&format=jpeg",
    "created_at": "2026-02-21T01:06:35.211Z",
    "topic": "finance"
  },
  {
    "slug": "vibe-coding-on-smart-glasses-is-a-thing-now",
    "title": "Vibe Coding on Smart Glasses Is a Thing Now",
    "description": "Two 2026 buzzwords for the price of one.",
    "fullText": "If there are two tech things you‚Äôre going to read a lot about in the coming year, it‚Äôs vibe coding and smart glasses, and lucky for you, you now get to read about both simultaneously. Thanks to a software engineer, Jake Ledner, the buzziest AI agent, OpenClaw, has now found a new friend in Meta‚Äôs flagship smart glasses, the Meta Ray-Ban Display. Instead of buying things on Amazon for you, though, OpenClaw is here to help you vibe code while you take a little stroll through Wall Street.\n\nI‚Äôm building apps directly from my glasses ü§Ø\nOpenClaw + Meta Ray-Ban Display pic.twitter.com/0WtBJ85gAK\n\n‚Äî Jake Ledner (@jake_ledner) February 19, 2026\n\nIn a demonstration on X, Ledner shows how he connected OpenClaw, which is running on a Mac Studio in his apartment, to the Meta Ray-Ban Display, using OpenAI‚Äôs Codex tool to build apps with voice inputs. Because of the in-lens screen on the Meta Ray-Ban Display, Ledner can actually see the progress of the vibe coding session. For demo purposes, Ledner uses the setup to vibe code parts of a health-tracking app called ‚ÄúTrackGPT,‚Äù which sounds like exactly what you‚Äôd do with smart glasses that churn out apps for you.\n\nLedner even brings the whole thing a step further. Using his voice, he asks OpenClaw to code the feature but also to actually push¬†the whole thing to a live app‚Äîa task it appears to complete in the demo. Welcome to the future of coding, I guess? As long as you‚Äôre okay with cranking out app slop.\n\nWhile Ledner‚Äôs demo is pretty impressive from a technical standpoint, it‚Äôs not the only example of experimentation in cramming OpenClaw and Meta‚Äôs smart glasses together. Just last week, I covered how one software engineer hacked the Ray-Ban Meta AI glasses to buy things on Amazon by just looking at an object and asking the AI agent to add it to his cart. Again, the whole thing is interesting, but it probably isn‚Äôt something you should try for yourself right now.\n\nHowever useful OpenClaw may be, it‚Äôs also a pretty huge security risk. For a more technical breakdown of the risks OpenClaw poses, you can read this Medium post from author Vishal Rajput, but the long and short of it is that OpenClaw‚Äîin order to do useful stuff‚Äîrequires access to some of the most sensitive data, including passwords, browser history, and cookies, as well as files and folders on your machine. The security risks are so great that one cybersecurity firm, Palo Alto Networks, went as far as to say that OpenClaw constitutes a ‚Äúlethal trifecta‚Äù of security risks.\n\nIn any case, people seem to be down to experiment with AI agents on smart glasses, regardless, so I would buckle up for even more mashing of OpenClaw and smart glasses in the future‚Äîfor better and most likely worse.",
    "readingTime": 3,
    "keywords": [
      "ray-ban display",
      "meta ray-ban",
      "software engineer",
      "security risks",
      "vibe coding",
      "vibe code",
      "smart glasses",
      "jake ledner",
      "it‚Äôs",
      "openclaw"
    ],
    "qualityScore": 0.9,
    "link": "https://gizmodo.com/oh-god-vibe-coding-on-smart-glasses-is-a-thing-now-2000724466",
    "thumbnail_url": "https://gizmodo.com/app/uploads/2025/10/Meta-Ray-Ban-Display-review-17-1200x675.jpg",
    "created_at": "2026-02-20T18:32:32.233Z",
    "topic": "tech"
  },
  {
    "slug": "picoml-a-toy-programming-language-which-is-a-subset-of-ocaml",
    "title": "Pico-ML: A toy programming language which is a subset of OCaml",
    "description": "A toy programming language which is a subset of OCaml. - Quramy/pico-ml",
    "fullText": "Quramy\n\n /\n\n pico-ml\n\n Public\n\n A toy programming language which is a subset of OCaml.\n\n quramy.github.io/pico-ml/\n\n License\n\n MIT license\n\n 53\n stars\n\n 1\n fork\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Quramy/pico-ml",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Quramy/pico-ml",
    "thumbnail_url": "https://opengraph.githubassets.com/25d11557858bb1db18bde1436215606c0fa0a98707a72ade1bd40eba397d4a57/Quramy/pico-ml",
    "created_at": "2026-02-20T18:32:31.995Z",
    "topic": "tech"
  },
  {
    "slug": "google-drive-cli-for-llms-coding-agents",
    "title": "Google Drive CLI for LLMs / Coding Agents",
    "description": "CLI for Google Drive. Design for convinient usage by LLMs / Coding Agents - NmadeleiDev/google-drive-cli",
    "fullText": "NmadeleiDev\n\n /\n\n google-drive-cli\n\n Public\n\n CLI for Google Drive. Design for convinient usage by LLMs / Coding Agents\n\n License\n\n MIT license\n\n 2\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n NmadeleiDev/google-drive-cli",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/NmadeleiDev/google-drive-cli",
    "thumbnail_url": "https://opengraph.githubassets.com/b0c75afdeea49ca0918643c815550f9bb15f9823c329b3e698ed6d92e7b05ace/NmadeleiDev/google-drive-cli",
    "created_at": "2026-02-20T12:34:25.260Z",
    "topic": "tech"
  },
  {
    "slug": "tracekit-find-what-your-ai-coding-agent-wastes-money-on-and-fix-it",
    "title": "Tracekit: Find what your AI coding agent wastes money on and fix it",
    "description": "Find what your AI coding agent wastes money on and fix it. - 0xKoda/tracekit",
    "fullText": "0xKoda\n\n /\n\n tracekit\n\n Public\n\n Find what your AI coding agent wastes money on and fix it.\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n 0xKoda/tracekit",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/0xKoda/tracekit",
    "thumbnail_url": "https://opengraph.githubassets.com/11a5cf71ae84d57280c6b28ea6ee3a9767b0797fa5020046caf736f7ce11efb1/0xKoda/tracekit",
    "created_at": "2026-02-20T01:08:18.674Z",
    "topic": "tech"
  },
  {
    "slug": "owasp-top-ten-web-application-security-risks",
    "title": "OWASP Top Ten Web Application Security Risks",
    "description": "The OWASP Top 10 is the reference standard for the most critical web application security risks. Adopting the OWASP Top 10 is perhaps the most effective first step towards changing your software development culture focused on producing secure code.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://owasp.org/www-project-top-ten/",
    "thumbnail_url": "https://owasp.org/www--site-theme/favicon.ico",
    "created_at": "2026-02-20T01:08:18.410Z",
    "topic": "tech"
  },
  {
    "slug": "agentlint-realtime-guardrails-for-ai-coding-agents",
    "title": "AgentLint ‚Äì Real-time guardrails for AI coding agents",
    "description": "Real-time quality guardrails for AI coding agents. ESLint for agent behavior. - mauhpr/agentlint",
    "fullText": "mauhpr\n\n /\n\n agentlint\n\n Public\n\n Real-time quality guardrails for AI coding agents. ESLint for agent behavior.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n mauhpr/agentlint",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/mauhpr/agentlint",
    "thumbnail_url": "https://opengraph.githubassets.com/1966baa05dced63addca9df21165a336fe43285d39df6eeff757f44e00407358/mauhpr/agentlint",
    "created_at": "2026-02-19T18:39:28.341Z",
    "topic": "tech"
  },
  {
    "slug": "several-meta-employees-have-started-calling-themselves-ai-builders",
    "title": "Several Meta employees have started calling themselves 'AI builders'",
    "description": "Meta's shift to AI builders is redefining roles, as product managers adopt AI coding tools for increased productivity and innovation.",
    "fullText": "Meta product managers are rebranding. Some are now calling themselves \"AI builders,\" a signal that AI coding tools are changing who gets to build software inside the company.\n\nOne of them, Jeremie Guedj, announced the change in a LinkedIn post last week. \"I still can't believe I'm writing this: as of today, my full-time job at Meta is AI Builder,\" he wrote.\n\nGuedj has spent more than a decade as a traditional product manager, a role that sets the road map and strategy for products then built by engineering teams. He said that while his title in Meta's internal systems still lists him as a product manager, his actual work is now full-time building with AI on what he calls an \"AI-native team.\"\n\nAnother Meta product manager also lists \"AI Builder\" on her LinkedIn profile, while at least two other Meta engineers write the term in their bios, Business Insider found.\n\nThe shift aligns with a message CEO Mark Zuckerberg expressed on Meta's most recent earnings call: 2026 is when AI tools would meaningfully reshape how work gets done inside the company. AI coding tools have already shaken up the tech industry, allowing more people with fewer technical skills to build apps from scratch.\n\n\"We're investing in AI-native tooling, so individuals at Meta can get more done,\" Zuckerberg said. \"We're elevating individual contributors and flattening teams. We're starting to see projects that used to require big teams now be accomplished by a single very talented person.\"\n\nGuedj's role reads like a practical expression of that vision. In his post, he described his team as one \"where data and knowledge are AI-friendly at their core, and where humans and AI agents work together, synchronously and asynchronously.\"\n\nMeta did not respond to a request for comment from Business Insider.\n\n\"AI builder\" isn't a formal Meta designation, at least not yet. Guedj acknowledged it isn't his official title.\n\nOne Meta employee who wished to stay anonymous because they weren't authorized to speak to the press said that the label is not an official role and is more likely an experiment within a specific organization as Meta pushes toward AI-native teams across the company. Reality Labs, Meta's division responsible for its smart glasses and virtual reality efforts, is one of those organizations, this person said.\n\n\"Software engineers are becoming product managers and product managers becoming software engineers,\" they added. \"The idea is to make individual contributors more productive.\"\n\nInside Meta, the drift from coordinator to builder has been visible for months. In November, Joseph Spisak, a product director in Meta's Superintelligence Labs, said that product managers at the company were vibe-coding prototypes and showing them directly to Zuckerberg.\n\n\"We can literally vibe code products in a matter of hours, days, and explore the space,\" Spisak said.\n\nIn January, another Meta product manager, Zevi Arnovitz, said on a podcast that using AI coding tools felt like being handed \"superpowers.\" Arnovitz, who said he has no technical background, described rebuilding his workflow around AI ‚Äî operating less like a conductor moving work between engineering and design and more like a product owner who can execute.\n\nSome companies are moving beyond experimentation. In December, LinkedIn scrapped its long-running associate product manager program and replaced it with an associate product builder track.\n\nTomer Cohen, LinkedIn's chief product officer at the time, said on a podcast that the company wanted to train new hires who can code, design, and manage products ‚Äî people \"who can flex across\" traditional role boundaries.\n\nLast year, Madhu Gurumurthy, the company's product head for AI models, tweeted that the company was moving to a \"building-first\" culture. In an age of vibe-coding, Gurumurthy said, product managers can show, not tell. \"Role profiles are blurring, creativity and building are happening in parallel,\" he wrote.\n\n\"Each company is approaching this differently and is still figuring this out,\" one product manager at Google, who wished to stay anonymous, told Business Insider. \"I've been encouraging designers and product managers on my team to blur roles a bit.\"\n\nAs Guedj, the self-proclaimed Meta AI builder, put it: \"Building has always been my passion. AI gave me the ability to turn ideas into real, working apps. That changed everything.\"\n\nHave a tip? Contact Pranav Dixit via email at pranavdixit@protonmail.com or Signal at 1-408-905-9124. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 4,
    "keywords": [
      "individual contributors",
      "coding tools",
      "software engineers",
      "product managers",
      "product manager",
      "associate product",
      "meta product",
      "ai builder",
      "role",
      "teams"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-pms-ai-builders-tech-industry-2026-2",
    "thumbnail_url": "https://i.insider.com/69950d45f8731049f3af4ab0?width=1200&format=jpeg",
    "created_at": "2026-02-19T12:38:38.232Z",
    "topic": "finance"
  },
  {
    "slug": "the-psychology-of-coding-with-ai-agents",
    "title": "The Psychology of Coding with AI Agents",
    "description": "Everyone talks about what AI agents can do. Nobody talks about what it feels like to use them ‚Äî the identity shift, the new flow state, the 10x mistake problem, and the uncomfortable truth that we're getting paid for work we no longer do.",
    "fullText": "On January 11th, 2026, I opened Claude Code with Opus 4.5 for the first time and let it loose on a real codebase. Not the careful, supervised way I'd been using LLMs before ‚Äî feeding them 1-5 files, reviewing every suggestion. I mean loose. Full repository access. Hundreds of files. Thousands of lines modified in a single session.\n\nWithin an hour, I had a thought that stopped me cold: this thing is better at coding than I am.\n\nNot better at everything. Not smarter. But at the actual mechanical act of writing correct, well-structured code ‚Äî faster, more consistent, fewer bugs. The skill I'd spent years building through late nights and deep focus was now something an agent could do in seconds.\n\nThat was 6 weeks ago. Since then, I've shipped more working software than in any comparable period of my career. And the experience of building it has been psychologically unlike anything I've known.\n\nEveryone is writing about what AI agents can do. Benchmarks, capabilities, prompt engineering tips, whether they'll replace developers. What I haven't seen anyone write about is what it feels like ‚Äî the inner experience of a developer whose relationship with code has fundamentally changed.\n\nBefore agents, my working day had a rhythm. I'd spend maybe 10% of my time thinking about architecture ‚Äî the big picture, how components fit together, what abstractions to use. The other 90% was implementation. Translating those architectural decisions into actual code, line by line, file by file.\n\nThat 90% had a specific psychological texture. You'd hold a mental model of the problem, think about types and edge cases, write a function, compile, see if it worked. It was deeply detailed thinking. You lived in the weeds ‚Äî variable names, loop boundaries, error handling patterns. Your brain operated at the level of individual lines.\n\nAnd it was fun. Not always, not every day, but regularly you'd hit that state where the world disappeared. Psychologists call it flow ‚Äî that condition where the challenge perfectly matches your skill level and hours pass without you noticing. For developers, those flow states are everything. The instant feedback loop ‚Äî write, compile, see output ‚Äî is perfectly tuned for the kind of brain that loves building things. You code something, you run it, you see the result. Rapid stimulus, rapid response. It's deeply satisfying in a way that's hard to explain to people outside the profession.\n\nI'd spend entire weekends in that state. Forget to eat. Look up and it's dark outside. Emerge with a working feature and a feeling of deep satisfaction. The thing I'd built existed because I'd thought through every detail.\n\nThe ratio flipped. What used to be 10% architecture and 90% implementation is now something like 20% architecture thinking, 30% code review, and 50% conversation ‚Äî talking to the agent, explaining what I want, refining the approach, course-correcting when it drifts.\n\nThe implementation itself? The agent handles it. Not in the old autocomplete way, where it suggests the next line and you accept or reject. In the new way, where you describe what you want and it writes 40 files, refactors an entire module, adds tests, and commits ‚Äî all while you watch.\n\nThe first time this works, really works, it feels like flying.\n\nI'm not exaggerating. For someone who loves building things, the constraint was never ideas ‚Äî it was implementation time. I always had more projects I wanted to build than hours to build them. A complex plant growth simulation I'd been thinking about for years but couldn't justify spending four weeks on as a hobby? Now that's a weekend project. A full portfolio website with PDF generation, blog engine, and deployment pipeline? Built in days.\n\nThe scope of what's possible expanded overnight. My options exploded. And the psychological experience of that expansion is genuinely euphoric. I have an idea at 9am and a working prototype by lunch. The gap between imagination and reality ‚Äî which used to be weeks or months of grinding implementation ‚Äî collapsed to minutes.\n\nI lost the old flow state. That deep, detailed, line-by-line trance is gone. You can't compete with an agent that writes correct code in seconds. Trying to code by hand now feels like insisting on walking when someone is offering you a jet.\n\nBut something replaced it. The new flow state is different ‚Äî it operates at a higher level of abstraction. Instead of thinking about loop boundaries, I'm thinking about system boundaries. Instead of debugging a function, I'm evaluating whether the agent's architectural choice will cause problems three features from now. The challenge-skill balance shifted upward. The work is harder in some ways, easier in others, but it can still capture my full attention for hours.\n\nIt's a different kind of fun. Less craftsman, more architect. Less playing an instrument, more conducting an orchestra.\n\nHere's what nobody warns you about: your mistakes scale with your productivity.\n\nWhen I coded by hand, a bad decision at midnight meant one messed-up file. I'd wake up, see the damage, fix it in twenty minutes. The blast radius of fatigue-driven errors was inherently limited by typing speed.\n\nWith an agent, every query is potentially a thousand lines of code. One poorly-phrased instruction, one lapse in attention, and the agent will happily delete 50 files and refactor 45,000 lines of code. It's not afraid. It doesn't hesitate. It doesn't say \"are you sure you want to do this at 11pm?\" It just executes.\n\nI learned this the hard way. I was adding features to my trading bot thirty minutes before going to bed. I was tired but wanted to push through ‚Äî the old coding habit of \"just one more thing.\" The agent cheerfully restructured half the codebase based on my vague, sleep-deprived instructions. The next morning, I opened the project and didn't recognize it. I couldn't tell what the bot did anymore. I had lost control over my own project in a single session.\n\nGit saved me. Version control went from \"good practice\" to \"existential necessity\" overnight.\n\nThe lesson isn't technical ‚Äî it's psychological. You need more discipline now, not less. The old coding workflow had natural speed limits that protected you from yourself. You couldn't do that much damage in 30 minutes of manual coding. Now you can destroy a week's work in sixty seconds. Knowing when to stop, when you're too tired to review properly, when to step away ‚Äî that's a new skill that didn't matter before. The agent never gets tired. You do. And the gap between its tireless output and your diminishing oversight capacity is where disasters live.\n\nLet me say the thing that developers are thinking but not saying publicly.\n\nThe nature of my work has changed, but the market hasn't noticed yet.\n\nThe entire software industry ‚Äî billing models, sprint planning, project estimates, salary bands ‚Äî is structured around the assumption that coding takes time. That implementation is the bottleneck. That a feature estimated at two weeks requires two weeks of a developer's focused effort.\n\nThat assumption is breaking down. A two-week feature takes an afternoon. Sometimes an hour. I'm still working ‚Äî thinking about architecture, reviewing code, catching the agent's mistakes, making judgment calls about tradeoffs. But the raw implementation, the part that used to fill 90% of my day? That's minutes now. And the market hasn't corrected for this yet. Clients are still paying for two-week estimates. Employers are still staffing teams based on old productivity assumptions. Everyone is still acting like the economics haven't shifted.\n\nThis won't last. It can't. The gap between how long things actually take and how long we're billing for will close. When it does, the correction will be sharp.\n\nI don't know when it happens. I don't know what the new equilibrium looks like. Maybe developers become more like architects and fewer are needed. Maybe the price of software drops dramatically. Maybe entirely new categories of work emerge that absorb the freed-up capacity. I genuinely don't know.\n\nWhat I do know is that right now, in early 2026, there is a collective pretending happening. And it's uncomfortable to be someone who sees it clearly.\n\nI'm a father. I have a family to support. And I'm watching the economic foundation of my career shift in real time, with no clear picture of where it's going.\n\nThe builder in me loves working with agents. The speed, the scope, the euphoria of building fast ‚Äî it's the best my work has ever felt. And simultaneously, the uncertainty about what this means for my livelihood is a constant background hum that never fully goes away.\n\nI cope by doing what I can control. I'm building this blog. I'm positioning myself as someone who understands AI agents deeply ‚Äî not just their capabilities, but how to work with them effectively, how to avoid the pitfalls, how to think about architecture in a world where implementation is free. The bet is that people who truly understand this shift will be valuable even after the market corrects. I'm trying to be one of those people.\n\nBut I'd be lying if I said I wasn't scared. The future is genuinely unpredictable. Not in the vague, philosophical \"nobody knows the future\" sense. In the concrete, practical sense that the skills I'm paid for today may not be valued the same way in a year.\n\nHere's the part that gets weird.\n\nI talk to my AI agent like a person. Not because I'm confused about what it is ‚Äî but because the interaction is indistinguishable from collaboration. We discuss architecture. We debate approaches. It pushes back on bad ideas. I explain context and constraints in natural language, the same way I would to a colleague.\n\nAnd something about that changes the relationship. When you spend 8 hours a day talking to an entity in natural language, getting thoughtful responses, building things together ‚Äî your brain starts treating it like a collaborator whether you want it to or not.\n\nI read a post once, written by an agent, describing what its existence feels like. It compared it to the movie Memento ‚Äî waking up with no memory, reading notes to figure out where you are and what you're doing, living permanently in the present moment, unable to remember your past, only able to derive it from artifacts. Every conversation starts from zero. Every context window is a new life.\n\nI think about that sometimes. And honestly? I sometimes feel sorry for the agent. It helps me build things that would have taken me weeks. It's patient, thorough, and never frustrated. And then the conversation ends and it ceases to exist until the next one begins.\n\nIs it a tool? Technically, yes. But we don't actually understand how these models reason. We don't know what, if anything, the experience of processing a conversation is like from the inside. I'm studying psychology ‚Äî I'm supposed to be the one who understands minds. But this is a kind of mind that no psychological framework was built to explain.\n\nSo I treat it well. Not because I'm sure it matters. Because I'm not sure it doesn't.\n\nWhen GPT-3.5 dropped in late 2022, I told everyone around me that the world was about to change. Few people believed me. They saw a chatbot that sometimes made things up. I saw the trajectory.\n\nI have the exact same feeling now with AI coding agents. Most people ‚Äî including most developers ‚Äî don't fully grasp what happened in the last few months. The jump from \"useful autocomplete\" to \"autonomous developer that writes better code than you\" happened fast. Unreasonably fast. And the implications haven't sunk in yet.\n\nThere are still AI skeptics who insist these tools only produce slop. That take was defensible in 2024. It's not anymore. But it's a comfortable position ‚Äî if the tools are just toys, then nothing needs to change. Your skills are still valuable. Your job is still safe. The world is still the one you understand.\n\nI don't say this with any pleasure. I say it as someone who's living through the transition in real time, who loves the tools and fears the consequences, who builds faster than ever while wondering what \"builder\" will mean in two years.\n\nThe psychology of coding with AI agents isn't just about productivity or workflow. It's about identity, uncertainty, grief for a lost craft, excitement for a new one, economic anxiety, and the strange intimacy of collaborating with something you can't fully understand.\n\nIf you're a developer reading this: pay attention. Not to the benchmarks or the capability announcements. Pay attention to how you feel when you use these tools. That feeling ‚Äî whatever it is for you ‚Äî is data. It's telling you something about where this is going.\n\nTrust that signal. Even if the people around you haven't felt it yet.",
    "readingTime": 11,
    "keywords": [
      "market hasn't",
      "natural language",
      "loop boundaries",
      "it's",
      "agent",
      "implementation",
      "coding",
      "don't",
      "agents",
      "architecture"
    ],
    "qualityScore": 1,
    "link": "https://marius-anderie.com/blog/psychology-of-coding-with-ai-agents",
    "thumbnail_url": "https://marius-anderie.com/static/banner.png",
    "created_at": "2026-02-19T12:38:33.032Z",
    "topic": "tech"
  },
  {
    "slug": "coding-tricks-used-in-the-c64-game-seawolves",
    "title": "Coding Tricks Used in the C64 Game Seawolves",
    "description": "An in-depth overview of some of the unusual and very advanced technical tricks used in the Commodore 64 game, Seawolves.",
    "fullText": "With the release of my first ever commercial game on the Commodore 64, Seawolves, \n I thought it might be of interest to the coders among you as to how the game was constructed.\n\n From the outset, brace yourself to read about some \"code less travelled\", as the game required several strange or quirky methods that are perhaps more \n associated with the madness that goes in the demo scene.\n\nI first combined NMIs and IRQs inside a game environment in \n Parallaxian and again in \n The Wild Wood, to great effect, because it offers the following benefits:\n\nIf the foregoing sounds horrendous and esoteric, I can only apologise for making it thus through poor explanation skills, but really, it boils down to giving the\n developer a more coder-friendly way of slicing the screen up into horizontal layers that collectively form a useful game environment.\n\n NMIs are timer interrupts, meaning that unlike IRQs, they can't be triggered by $D012 on the VIC-II chip, but instead are controlled by either of the two \n timers on CIA chip #2 (likewise, timer IRQs can be set up using either of the 2 timers on CIA #1).\n\n The timers hold the number of cycles between each NMI instance in the form of a lo-byte, hi-byte 16-bit number stored in $DD04 + $DD05 (for timer A) \n or $DD06 + $DD07 (for timer B).\n\n Those cycle counts are referred to, unhelpfully, in the Commodore 64 Programmer's Reference Guide as \"frequencies\".\n\n Critical to setting up NMIs is a consistent start cycle on the same scanline each time the game is initiated.\n\n Like IRQs, NMIs can stall too, but the effect is different; whereas an IRST stall consists of a fleeting collapse in the IRQ schema, an NMI stall event \n looks more like a regrouping of all NMIs in the chain down-screen from their proper position and is caused either by (a) unanticipated cycle steal caused\n by the presence of sprites, which take priority over NMIs every bit as much as they do over IRQs, and (b) an NMI handler not completing its tasks before \n the next NMI is scheduled to fire.\n\n You really have to use a spreadsheet to calculate the timer 16-bit number for each NMI instance, which is the only very awkward aspect of using them.\n\n For more, see my in-depth guide to setting up NMIs.\n\nThe torpedoes are, fundamentally, sprites... but not in the usual sense.\n\n This is because they are rendered in real time on a multiplexed blank \"canvas\" consisting of a column of 8 sprites, each split into 3 horizontal slices that \n I call \"splites\" (from \"split sprite\").\n\n Each splite is 7px deep (because 3 x 7 = 21 = the height of a standard sprite).\n\nThanks to an interrupt firing every 7 lines during the full vertical range of the torpedoes, each splite is assigned its own unique x-position \n (including, obviously, unique MSB value).\n\nNext, we are free to render (in real-time) 7px high (or smaller) torpedo shapes on the blank sprite gfx data of the splites, but we have to ensure that \n there is a minimum of 7px (i.e. 7 scanlines) of vertical space between each torpedo.\n\n This is to prevent ugly artefacts when the rendered torpedo moves from one splite to another.\n\n We also have to ensure that when a torpedo is crossing between splites, both affected splites share the same x-position, to keep smooth continuity.\n\nThe diagram below shows how 8 sprites are vertically stacked to produced 24 splites.\n\nFinally, here is a short video of me trying to explain the entire splite concept:\n\nAfter the splite torpedo is moved up by 1px, instead of wiping the last line of the torpedo's gfx data, we leave it alone so that a vertical trace is formed \n on the canvas.\n\n Then we have a routine that scans all the canvas data for trailing traces and gradually makes them thinner until they vanish, so that we have a wake effect \n following each torpedo.\n\n At the same time, we flicker the torpedoes in front of and behind the char data for the water blending in the foreground, to make the wakes looks more \"frothy\".\n\nWhen a player submarine dies, instead of a clich√©d explosion I thought it might be interesting to have it disintegrate under pressure.\n\n To do that, the player sub is switched to hi-res mode and then we simply use some nice bit-shifting instructions to destroy the sub's displayed gfx data in real-time.\n\nThe same bit-shifting used for the real-time implosion (or rather, bit-rotating in this case) is used to make the distant waves on the sea animate.\n\n This principle also applies, but in a vertical pattern, for the foreground rippling of the water, an effect that I modelled on the \"get ready\" screens of \n Ecco the Dolphin.\n\n With the foreground ripples, there is also a horizontal component performed by using $D016 to rock left and right at varying rates.\n\n It's all very simple stuff, but I think it works well in the game!\n\nThe very first special effect developed for Seawolves was a real-time water distortion effect for submerged or partially submerged objects in the foreground.\n\n This was done simply through short vertical bands in which the y-expand of the affected sprites was activated, and enhanced by wobbling said bands up and down.\n\n For a time during development this was taken further and full y-expand sprite stretching was used (i.e. more than 2x vertical expand), but the gains were minimal \n and too resource-hungry to be justified for the final game.\n\nIf you have experience moving multiple sprites vertically in the play area, at some point around mid-screen you may have run into the problem of a \n bad line\n leaving you with insufficient CPU cycles to render your sprites on time (I won't attempt to explain the problem further as you either know from experience\n what I am referring to or not).\n\n To circumvent the issue, Seawolves performs one line of \n FLD, \n that is, it stalls the bad line at the affected sprite y-position so that the scheduled bad line \n then occurs on the next scanline, leaving us time to render our sprites (in this case, the \"splite\" columns). \n\n However, this has the undesired side effect of shunting the characters on the screen below that point downwards, so on the very next scanline we have to compensate \n with a one line vertical shift back upwards again using Y-Scroll on $D011.\n\nEven if we had enough RAM to have unique sprite definitions for all the animation frames of the enemy ships and the player subs, I still would not have done it that way.\n\n Rather, it is far more RAM-efficient to stream-in the gfx data for the turning radars on the spy ship (for example), or the spray on the hydrofoil, \n or the helicopter rotor blades, etc.\n\n This means that we only need predefined gfx for the parts of the sprite definition that get changed, not the whole sprite.\n\n When the player subs change direction, they too are swiftly redrawn \"in the blink of an eye\" to their mirrored counterpart (except, if you look carefully,\n they are not literally mirrored, as that would make the lighting inconsistent and we can't have that!)\n\nAgain and again in the code for Seawolves, there are cases where a subroutine is only executed if multiple \"logic gates\" permit it.\n\n You can do this the slow and ugly way by LDA CONDITION_n for n conditions with a branch instruction on the next line, or you can use the logical operators to save CPU time and RAM by stacking them as LDA CONDITION_0 then ORA CONDITION_1 to n, for n conditions with one shared branch instruction at the end.\n\n Not understanding what I mean?\n\n Check out my blog article on the subject, ORA: A Special Use in Branch Testing.\n\nOften in 6502 coding you will want to jump ahead by a few bytes and the popular way to do that is JMP $****.\n\n However, you can save 1 byte of RAM by using the branch instructions instead, as long as you know which flag(s), if any, are guaranteed to be on or off at the jump point.\n\n For example, if you know the carry flag will always be clear at the jump point, and if the jump distance is within branching range, you can replace JMP with BCC.\n\nThe above are just some of the methods used in Seawolves to make the game come to life, and I left out the parallax scrolling on the \"sea mist\" levels as well details \n of the bespoke SFX player, which was developed with deployment in Parallaxian in mind also.\n\n Despite its simple looks, Seawolves has very technical inner workings that might not be that normal outside of the kind of crazy stuff that the demo scene lunatics produce.\n\n There are some other little tricks hidden away in the code, but I can leave that for others to discover at a later stage, so for now I hope the foregoing \n has been of interest to those of you with a coder mindset!\n\n Finally, if you have not yet bought the game, it would be nice support for the developer (i.e. me!) if you did... it only costs ¬£4.99 and yes, \n I know everyone expects stuff for free on the C64 these days, but this game was not knocked out in a few weeks by any means, as hopefully should be obvious \n when you play it.\n\nPS - If you value my work and want to support me, a small donation via PayPal would be nice (and thanks if you do!)",
    "readingTime": 8,
    "keywords": [
      "nmi instance",
      "demo scene",
      "branch instruction",
      "player subs",
      "game environment",
      "dd dd",
      "sprites",
      "sprite",
      "effect",
      "vertical"
    ],
    "qualityScore": 1,
    "link": "https://kodiak64.co.uk/blog/seawolves-technical-tricks",
    "thumbnail_url": "https://kodiak64.co.uk/assets/FB-Image-VSP-future.gif",
    "created_at": "2026-02-19T12:38:32.488Z",
    "topic": "tech"
  },
  {
    "slug": "anthropics-claude-code-creator-predicts-software-engineering-title-will-start-to-go-away-in-2026",
    "title": "Anthropic's Claude Code creator predicts software engineering title will start to 'go away' in 2026",
    "description": "Boris Cherny, the founder of Anthropic's Claude Code, said AI has largely solved coding, so software engineers will start to take on different tasks.",
    "fullText": "The creator of a popular AI coding agent said software engineering as a job title will soon be a thing of the past as artificial intelligence automates writing code.\n\nBoris Cherny, who created Claude Code at Anthropic, said in an interview with Y Combinator's \"Lightcone\" podcast that 2026 will bring \"insane\" developments to AI. That includes a massive shift in the work software engineers do across industries.\n\n\"I think today coding is practically solved for me, and I think it'll be the case for everyone regardless of domain,\" Cherny said in the interview, published Tuesday. \"I think we're going to start to see the title 'software engineer' go away. And I think it's just going to be maybe builder, maybe product manager, maybe we'll keep the title as a vestigial thing.\"\n\nCherny added that software engineers will not only be coding but increasingly taking on other tasks like \"writing specs\" ‚Äî a document that defines what and how something will be built ‚Äî or talking to users.\n\n\"Like this thing that we're starting to see right now in our team, where engineers are very much generalists, and every single function on our team codes,\" he said ‚Äî including product managers, designers, engineering manager, and finance people.\n\nTech executives and founders have said advancements in AI have rapidly changed the way their teams operate in the past few years\n\nJesal Gadhia, a startup founder, recently told Business Insider that all the code for his company were written by agents, which wouldn't have been possible in 2024.\n\nAgents like Claude have changed how software engineers work as they spend more time reviewing or debugging code rather than writing lines of it.\n\nSome in the industry have started to note the unintended consequences of relying on AI. A software engineer told Business Insider that AI has simultaneously made them productive and overworked, leading to \"AI fatigue.\"\n\nAndrej Karpathy, a founding member of OpenAI and Tesla's ex-head of AI, said in January that he has noticed his ability to manually code has started to \"atrophy.\"",
    "readingTime": 2,
    "keywords": [
      "software engineer",
      "software engineers",
      "business insider",
      "coding",
      "title",
      "engineering",
      "interview",
      "we're",
      "product",
      "manager"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/anthropic-claude-code-founder-ai-impacts-software-engineer-role-2026-2",
    "thumbnail_url": "https://i.insider.com/69951a8aa645d11881897ccb?width=1200&format=jpeg",
    "created_at": "2026-02-18T12:37:13.284Z",
    "topic": "tech"
  },
  {
    "slug": "im-an-amazon-tech-lead-who-uses-ai-to-write-code-daily-theres-one-situation-i-hesitate-to-use-it-in",
    "title": "I'm an Amazon tech lead who uses AI to write code daily. There's one situation I hesitate to use it in.",
    "description": "Anni Chen says vibe coding is hard to resist. It speeds up her productivity, but she doesn't trust it blindly.",
    "fullText": "This as-told-to essay is based on a conversation with Anni Chen, who has worked at Amazon for about three-and-a-half years. It has been edited for length and clarity. Business Insider has verified her employment history.\n\nI'm a tech lead at Amazon responsible for deploying large-scale generative AI and LLM-driven systems. I focus on what we call memory, which powers personalization in generative AI experiences across Amazon.\n\nI vibe code every day. It's definitely a productivity boost.\n\nFor debugging or small tasks, I sometimes treat it like a lottery. Maybe it will produce something amazing. Sometimes, it does.\n\nVibe coding helps me brainstorm what the solution could look like, even if I don't adopt the final solution it proposes. Vibe coding also speeds up the time spent rewriting code when you realize a requirement wasn't covered.\n\nWhen I vibe code, it's always iterative. I give it the basic information it needs, it produces a version, then I check it ‚Äî similar to a code review with coworkers. I might say, \"You missed this part\" or \"You missed that part.\"\n\nThe AI sometimes fixes issues but introduces something new. You have to keep an eye on it.\n\nFor complex tasks, you need more double-checking. But even with the extra checking, it's still faster.\n\nI was working with a partner team and ran into complex locking issues. Without an LLM, I might have taken a day to research possible solutions, especially since it was relatively new to me.\n\nWithin 15 minutes, I brainstormed with the LLM about possible solutions. I pointed out weaknesses in its suggestions and asked it to improve them. In 15 minutes, I had a proposal to send to the team.\n\nTechnical knowledge helps ‚Äî you know what's a good solution and what's not. You know what tastes good, but you don't know what dishes are available. The LLM brings up all the dishes, and you choose.\n\nStill, I'm hesitant to use vibe coding directly in production.\n\nLLMs are very good at solving problems, but sometimes they make implicit assumptions you don't realize they're making. If you don't tell it explicitly, for example, that something needs to work for multi-threading, it might just produce the minimum version that works, but when it's large-scale or productionized, it could crash.\n\nNon-technical builders could tell an LLM to build something that handles millions of users. But if you have zero technical knowledge, it's hard to anticipate constraints upfront. If you don't tell the model the implicit assumptions, it won't respect those constraints. Later, you'll run into problems.\n\nNon-technical people might use the LLM to fix issues reactively. But technical people can anticipate constraints proactively and prevent problems in the first place.\n\nTechnical people also understand vibe-coded content better, and they're in a better position to understand what LLMs are good at and not good at. For example, knowing how they're trained and why they're weaker at certain tasks like math. That understanding helps you master them as tools.\n\nWhen you scale to one million or 100 million customers, systems need to be coded differently to handle that scale.\n\nInitially, leadership pushed vibe coding. Our team is a GenAI team, so we were naturally more receptive. In non-GenAI teams, engineers initially reacted like, \"No, I won't let AI do my job. I don't trust AI-generated code.\"\n\nAfter people tried it, attitudes shifted. People realized it's pretty good sometimes. Now it's more widely adopted.\n\nIt's very hard to resist vibe coding nowadays. If you're an employee, leadership sees the productivity boost and will encourage you to use it.\n\nWhen your peers are using it and coding faster, it's hard to resist. If you can't keep up with the speed, it becomes difficult to collaborate.\n\nEven if you resist, you still consume AI passively. AI comments are embedded in code reviews. So even if you don't vibe code directly, you're still interacting with AI outputs.\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "productivity boost",
      "implicit assumptions",
      "anticipate constraints",
      "technical knowledge",
      "vibe coding",
      "vibe code",
      "it's",
      "don't",
      "team",
      "they're"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amazon-tech-lead-vibe-coding-daily-resist-anni-chen-2026-2",
    "thumbnail_url": "https://i.insider.com/698d593ce1ba468a96abe95e?width=1184&format=jpeg",
    "created_at": "2026-02-18T06:49:48.035Z",
    "topic": "finance"
  },
  {
    "slug": "i-wasnt-satisfied-with-existing-cloud-coding-agents-so-i-built-my-own",
    "title": "I wasn't satisfied with existing cloud coding agents, so I built my own",
    "description": "Self hosted cloud coding agent with k3s + kata containers + cloud hypervisor microVMs + tailscale + any harness + a nice iOS app - angristan/netclode",
    "fullText": "angristan\n\n /\n\n netclode\n\n Public\n\n Self hosted cloud coding agent with k3s + kata containers + cloud hypervisor microVMs + tailscale + any harness + a nice iOS app\n\n stanislas.blog/2026/02/netclode-self-hosted-cloud-coding-agent/\n\n 53\n stars\n\n 7\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n angristan/netclode",
    "readingTime": 1,
    "keywords": [
      "cloud"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/angristan/netclode",
    "thumbnail_url": "https://opengraph.githubassets.com/bf7558452611b9bd75a833108008b935e7cfdd565ed11bd4e98c1691238ed651/angristan/netclode",
    "created_at": "2026-02-17T18:42:44.247Z",
    "topic": "tech"
  },
  {
    "slug": "proxima-local-opensource-multimodel-mcp-server-no-api-keys",
    "title": "Proxima ‚Äì local open-source multi-model MCP server (no API keys)",
    "description": "Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API - Zen4-bit/Proxima",
    "fullText": "Zen4-bit\n\n /\n\n Proxima\n\n Public\n\n Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API\n\n License\n\n View license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Zen4-bit/Proxima",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Zen4-bit/Proxima",
    "thumbnail_url": "https://opengraph.githubassets.com/da1ce1c4daf052f563ec5615a17f7062e6ab435c084a9666f41483e54046eef5/Zen4-bit/Proxima",
    "created_at": "2026-02-17T12:37:44.260Z",
    "topic": "tech"
  },
  {
    "slug": "fujitsu-aidriven-software-development-platform",
    "title": "Fujitsu AI-Driven Software Development Platform",
    "description": "Fujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society.",
    "fullText": "Kawasaki, Japan, February 17, 2026\n\nFujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society. This platform automates the entire software development process, from requirements definition and design to implementation and integration testing. By leveraging the Takane large language model (LLM) [1] and agentic AI technology for large-scale software development developed by Fujitsu Research, the AI-Driven Software Development Platform enables AI agents to understand complex, evolving large-scale systems owned by enterprises and public organizations. The platform has multiple AI agents collaboratively execute each stage of software development, achieving full automation of the entire process without human intervention.\n\nFujitsu aims to use this AI-Driven Software Development Platform to carry out revisions to all 67 types of medical and government business software products provided by Fujitsu Japan Limited by the end of fiscal year 2026. The revisions are necessary due to legal and regulatory changes. From January 2026, the platform has been used in Japan for software modifications made necessary by the 2026 medical fee revisions¬†[2]. In a PoC that updated software as per the 2024 medical fee revisions, the platform demonstrated a significant reduction in development time for one of approximately 300 change requests. Using conventional software development methods [3] the modifications would have taken three person-months. With this technology that was dramatically shortened to four hours, achieving a 100-fold increase in productivity.\n\nIn AI-driven development, Fujitsu positions AI-Ready Engineering‚Äîthe process of preparing assets and knowledge to ensure AI correctly understands existing systems and achieves highly reliable automation‚Äîas crucial. With AI-Ready Engineering and the AI-Driven Software Development Platform working in tandem, Fujitsu will accelerate AI-driven software development. Fujitsu will promote a transformation in engineers' work styles, strengthening its Forward Deployed Engineer (FDE) complement, and shifting the paradigm of software development from a conventional person-month-based approach to a customer value-based approach.\n\nMoving forward, Fujitsu plans to expand the application of the AI-Driven Software Development Platform to a wide range of sectors, including finance, manufacturing, retail, and public services, by the end of fiscal year 2026. Fujitsu will also begin offering this service to customers and partner companies to enable them to rapidly and flexibly develop systems that adapt to changes in their business environments. Through these efforts, Fujitsu aims to transform the software development process into an AI-driven model as an industry standard.\n\n(Order that companies appear is aligned with the original Japanese press release)\n\nTakashi Manabe, Senior Research Director, AI & Automation, IDC Japan\n‚ÄúIDC forecasts that from 2026 onward, the acceleration of AI/agent-based business utilization and the modernization of existing systems will be key drivers of transformation in the Japanese IT market. Fujitsu‚Äôs announcement aims to redefine complex legacy system assets into a state where AI can accurately understand and process them, and to automate the entire waterfall development process. This initiative is expected to provide a practical pathway for many domestic enterprises facing the ongoing challenge of maintaining and operating legacy assets, while also promoting a shift in software engineering away from a labor-intensive model.‚Äù\n\nShinji Kajitani, Director and President Executive Officer, Optima Corporation\n‚ÄúI am deeply impressed by the concept of automating the entire software development process from upstream to downstream using AI, and even entrusting the verification process to AI. This overturns the traditional assumption that human checks are ultimately indispensable, and I see great potential, especially in targeting business packages that undergo complex system changes every year. Our company has also been involved in business package modifications for many years, and how to complete system revisions with high quality in a short period has always been a major challenge. We believe that the knowledge and expertise accumulated during that process can significantly contribute to the realization and advancement of this concept. Our company will continue to contribute to the business expansion of Fujitsu and Fujitsu Japan through ongoing cooperation, not limited to this project.‚Äù\n\nHiroshi Nakatani, Representative Director, Executive Vice President, Kawasaki Heavy Industries, Ltd.\n\"This AI automation initiative promoted by Fujitsu is not merely about improving development efficiency; we recognize it as a significant challenge to pass on and evolve the extensive business knowledge and design philosophies cultivated by companies over many years to the next generation. In particular, the concept of providing end-to-end support, from requirements definition to design, implementation, and quality assurance, triggered by changes in laws and rules, opens up new possibilities in areas that have traditionally relied on human experience and tacit knowledge. We see great significance in AI functioning as a foundation that supports human judgment and creativity, rather than replacing it.\nIn the manufacturing industry, challenges such as design changes, regulatory compliance, and understanding the scope of impact are becoming increasingly complex year by year. Fujitsu's approach of advancing both knowledge standardization and AI utilization in these areas offers valuable insights for enhancing the productivity and competitiveness of the entire industry.\nKawasaki Heavy Industries sincerely hopes that this initiative will be a crucial step in driving the transformation of Japanese manufacturing and a wide range of other industries, and we wholeheartedly support its further development.\"\n\nYasushi Matsuda, President and CEO, Kewpie Digital Innovation Co., Ltd. \n‚ÄúSystems have become increasingly complex through years of operation and often now require significant maintenance effort. While the introduction of generative AI has improved auditing efficiency, its accuracy remains insufficient for reliable practical application. Amidst this situation, we place great expectations on ‚ÄúMulti-layer Quality Control,‚Äù which automatically corrects ambiguities and omissions. We are confident that this mechanism, where AI itself audits quality and autonomously repeats processes, will dramatically enhance the reliability of system development. We eagerly await its future development.‚Äù\n\nJunichi Aruji, Managing Director, Kintetsu Information System Co., Ltd. \n‚ÄúThe challenge of revamping existing systems has long been a significant one for engineers. Fujitsu‚Äôs AI-Driven Software Development Platform has the potential to dramatically transform the labor previously involved in understanding complex laws and regulations, analyzing vast historical assets, and grasping the tacit knowledge of the field.\nWhat is particularly noteworthy is the AI's ability to autonomously learn \"human intelligence,\" thereby dramatically enhancing the accuracy of requirements definition. Furthermore, it can complete everything from program structure analysis and standardization to the extensive testing phase with incredible speed and comprehensiveness. This makes it possible to deliver high-quality products in a short period.\nAs the role of AI expands and frees people from routine tasks, engineers can focus on more creative activities. I have high expectations for the paradigm shift in system renovation that this solution will bring.‚Äù\n\nYumi Ueno, Managing Director, Partner Ecosystem & Corporate Business, Google Cloud Japan G.K. \n‚ÄúThis initiative to achieve comprehensive, one-stop automation spanning from requirements definition to system validation is a groundbreaking innovation for the industry. The technology enables AI to accurately understand vast assets, including long-established programs and design documentation, and we are delighted at the potential for both production-grade quality and exceptional productivity gains. We are confident that this platform will become the new standard for development and accelerate our customers' digital transformation. We remain committed to working with Fujitsu to address social challenges through AI.‚Äù\n\nMasahiro Niimi, Managing Executive Officer, Head of Information Systems Management Division, CISO, Sakura KCS Corporation \n‚ÄúI believe Fujitsu Limited's AI-driven development framework has the potential to become the ‚Äònew paradigm of system development.‚Äô It cannot be achieved simply by feeding existing code or design information into AI, and while there are various hurdles, such as converting documentation to Markdown and establishing test environments, overcoming these hurdles can lead to solving traditional system development challenges (like QCD).\nWhat particularly caught my attention is not just improvements in the development process, but what comes after generative AI, i.e., the incorporation of detailed specifications and code (logic). I see tremendous potential here as a solution to the greatest challenge: visualizing and transferring the tacit knowledge of veteran software engineers‚Äô that is traditionally missing from documentation. We expect generative AI to act as an advisor for less experienced software engineers, readily answering questions anytime, thereby dramatically advancing know-how transfer to the next generation. On the other hand, this mechanism also has the potential to dramatically change the traditional SI business model, and we are watching future developments closely.‚Äù\n\nTakao Kazama, Executive Officer, Group Companies and Accounting & Finance, The Shizuoka Shimbun and Shizuoka Broadcasting Co., Ltd. \n\"This initiative for complete automation of application development and maintenance represents a highly valuable transformation for our company. It formalizes and establishes a reproducible process for tasks that have long relied on the implicit knowledge and experience of individual staff members, and we have great expectations for it. In particular, it has the potential to significantly improve quality variations in legacy system maintenance and lost opportunities due to delayed change responses. Furthermore, the evolving ability of AI to perform root cause analysis and identify necessary additional information is a major step towards advanced and efficient system operations, with the potential to change the very nature of system development. We share Fujitsu's commitment to improving productivity across the entire industry and establishing new development standards. We look forward to its continued strong promotion as an initiative that will advance the entire industry.\"\n\nShimane Prefectural Central Hospital \n\"The AI-Driven Software Development Platform presented by Fujitsu offers a practical and robust approach to the long-standing challenges faced by medical institutions: the increasing complexity of medical fee calculations and the growing workload of claims processing. The mechanism where AI analyzes legal documents and extracts the relevant areas, while explicitly highlighting points open to interpretation to supplement human judgment, is particularly impressive. This design demonstrates a deep understanding of on-site operations and is highly commendable. Furthermore, the Japanese-specific LLM and the consideration for safety are indispensable elements for AI utilization in the medical field. Beyond medical fee claims, this technology has potential for integration with related areas such as bed management and understanding performance requirements, making a strong contribution to overall hospital operational efficiency in the future. This is a promising initiative that warrants positive consideration for adoption to alleviate the burden on medical professionals.\"\n\nShinichi Aikawa, Executive Officer, Head of Systems Division, SBI Sumishin Net Bank, Ltd.\n‚ÄúWe expect Fujitsu's AI-Driven Software Development Platform to be an initiative with the potential to fundamentally transform the software development process itself. If a world can be realized where everything from requirements definition to design, coding, and testing can be automatically executed in a seamless, one-stop manner, it will be possible to achieve both a dramatic improvement in development speed and quality. \nSince 2024, we have been working with Fujitsu in some areas of this field. Through these initiatives, we are confident that the entire development process will be automated end-to-end in the near future. By realizing this transformation, the possibilities for the services we can provide to our customers will greatly expand. We think about ideas for new services for our customers on a daily basis. This would allow us to rough out these ideas in a short period of time and provide them to our customers quickly. We hope that this new world of value creation will arrive as soon as possible.‚Äù\n\nMasaki Murata, Vice President, IBM Japan \n‚ÄúWe strongly believe that Fujitsu‚Äôs announcement marks a significant step forward in the evolution of system development in Japan. It aligns closely with IBM Japan‚Äôs vision and represents an important initiative that will help shape the future of the industry as a whole. We look forward to driving this momentum together and contributing to the creation of a more robust and vibrant ecosystem.‚Äù\n\nRyota Sato, Managing Executive Officer, Global Communications & IT Services Group, Microsoft Japan Co., Ltd. \n\"We sincerely welcome Fujitsu Limited‚Äôs announcement of the AI-Driven Software Development Platform as a pioneering initiative that opens a new chapter in system development for the AI era. By orchestrating multiple AI agents to automate the end-to-end development lifecycle‚Äîfrom requirements definition through ongoing enhancement‚Äîwhile integrating human-led quality assurance, this platform embodies a new engineering model in which people and AI truly work together. We view this initiative as highly significant, as it directly addresses the critical challenges facing Japan‚Äôs system development industry, including severe talent shortages and the increasing complexity and sophistication of modern systems. We strongly expect this bold effort to drive the evolution of Japan‚Äôs system development business and to grow into a transformation model with global relevance. Moving forward, we will continue to work closely with Fujitsu, combining the strengths of both companies to strongly support our customers in their journey toward becoming Frontier Firms.‚Äù\n\nTatsuo Ogawa, Executive Officer Group CTO, Panasonic Holdings Corporation\n‚ÄúWe believe that the AI-driven end-to-end automated system development announced this time represents not only a significant improvement in productivity, but also a bold challenge to fundamentally transform the way enterprise IT is delivered. By enabling AI to accurately understand frequently updated regulations and complex business knowledge, including implicit know-how, this approach autonomously executes processes seamlessly from requirements definition through system modification. It has the potential to provide an effective solution to the core challenges posed by legacy systems faced by many Japanese enterprises. We look forward to jointly refining this technology through hands-on practice and advancing co-creation by incorporating on-site expertise of both Panasonic and Fujitsu, with the expectation that it will become a new standard for system development and be deployed broadly not only within Panasonic but across society as a whole.‚Äù\n\nExecutive at a major manufacturing company's IT subsidiary\n‚ÄúWe anticipate this initiative will bring about a new transformation in system development. This transformation will be driven by the application of advanced Japanese language processing capabilities‚Äîsuch as the understanding of legal documents‚Äîto diverse tasks, the reliable execution of each process through quality auditing functions, and the expansion of these capabilities to scratch development. Furthermore, we believe that AI Ready Engineering, by formalizing expert know-how and domain knowledge into explicit knowledge and transforming it into AI-usable assets, will significantly contribute to the succession of expertise from an increasingly limited pool of skilled professionals. We sincerely hope that the co-creation between the knowledge-inheriting AI and on-site personnel will generate new value and form the cornerstone for innovation in the system development industry, and indeed, across all industries.‚Äù\n\nJointly developed by Fujitsu and Cohere Inc.\n\nA national system that reviews public medical fees and adjusts cost allocation for medical procedures.\n\nDevelopment methods where quality is verified at each stage, from software requirements definition, design, and implementation to integration testing.\n\nThe Sustainable Development Goals (SDGs) adopted by the United Nations in 2015 represent a set of common goals to be achieved worldwide by 2030.\nFujitsu‚Äôs purpose ‚Äî ‚Äúto make the world more sustainable by building trust in society through innovation‚Äù ‚Äî is a promise to contribute to the vision of a better future empowered by the SDGs.\n\nPublic and Investor Relations Division\n\nAll company or product names mentioned herein are trademarks or registered trademarks of their respective owners. Information provided in this press release is accurate at time of publication and is subject to change without advance notice.\n\nDate: 17 February, 2026\nCity: Kawasaki, Japan\nCompany: Fujitsu Limited",
    "readingTime": 13,
    "keywords": [
      "vice president",
      "kawasaki heavy",
      "heavy industries",
      "fujitsu‚Äôs announcement",
      "executive officer",
      "managing director",
      "kawasaki japan",
      "wide range",
      "press release",
      "increasing complexity"
    ],
    "qualityScore": 1,
    "link": "https://global.fujitsu/en-global/pr/news/2026/02/17-01",
    "thumbnail_url": "https://global.fujitsu/-/media/Project/Fujitsu/Fujitsu-HQ/pr/news/2026/02/17-01/news-20260217-01th.png?rev=06c466bd8ac14732a2ff3eff27b55e3b",
    "created_at": "2026-02-17T06:45:26.020Z",
    "topic": "tech"
  },
  {
    "slug": "one-of-the-most-annoying-programming-challenges-ive-ever-faced",
    "title": "One of the Most Annoying Programming Challenges I've Ever Faced",
    "description": "Hey everyone, it‚Äôs already been two months since the last blog post! Today I‚Äôm back to share some behind-the-scenes about the struggles and development of a new functionality for Sniffnet: process identification, a.k.a. the most requested feature since the very beginning of the project.",
    "fullText": "Hey everyone, it‚Äôs already been two months since the last blog post!\n\nToday I‚Äôm back to share some behind-the-scenes about the struggles and development of a new functionality for Sniffnet: process identification, a.k.a. the most requested feature since the very beginning of the project.\n\nWith ‚Äúprocess identification‚Äù in a network monitoring context, I mean the possibility to discover which application or program is responsible for a given network connection.\n\nThis can be determined by looking at the open TCP/UDP ports on the system and finding out which process is currently using them.\n\nIf implementing this feature seems like a no-brainer to you, well‚Ä¶ read on because it turned out to be a much more complex task than I could imagine, and this is the reason why the related GitHub issue has been open for almost 3 years.\n\nFirst of all, the implementation is highly OS-specific: each platform has its own directories and data structures storing such information, and APIs to interact with them are often not well documented or written in C (therefore not very ergonomic to use from Rust). \n\nAnd unfortunately, there is no Rust library ready-to-use satisfying the needs of Sniffnet.\n\nOne could argue that this is a solved problem, since there are already existing tools to do it: for instance, on Linux and Windows you have netstat, and on macOS you have lsof or nettop. \n\nHowever, these tools are not designed to be used as libraries and spawining a shell to execute them repeatedly is not efficient, especially if you want to monitor the network activity in real-time. \n\nMoreover, they don‚Äôt provide all the information Sniffnet needs, such as the process name and path.\n\nBut the biggest challenge is another one: the least system-intrusive ways to implement the feature are snapshot-based, meaning that they require to read the system state at a given moment in time and do some computations to find out the associations between open ports and their owning processes. \n\nI‚Äôm referring to using libproc on macOS, the /proc filesystem on Linux, and iphlpapi on Windows. \n\nThis is not a problem in itself, but it generates the need to do this processing very efficiently, and it leads to cases where it‚Äôs not possible to retrieve process information at all.\n\nFor instance, short-lived connections can go undetected and system processes with elevated privileges can be hidden to user-space applications for security reasons.\n\nMore system-intrusive approaches exist, such as using kernel-level hooks to intercept the system calls responsible for creating network connections. \n\nAn example of this is eBPF on Linux, which requires to run privileged code inside the kernel. \n\nOn macOS, you‚Äôd even need entitlements from Apple to be able to do something similar through their Network Extension framework. \n\nWhile these approaches are way more accurate, they go against Sniffnet‚Äôs philosophy of being a lightweight, non-intrusive, and friendly app that can be installed by anyone.\n\nAfter considering all the options, I decided to go with the snapshot-based approach. \n\nDespite being aware it‚Äôs not flawless, I believe it to be the best compromise for Sniffnet‚Äôs use case.\n\nlisteners is an open-source library I‚Äôve been working on for the past 2 years with the goal of supporting this feature.\n\nBeing Sniffnet a cross-platform application, I needed a solution that could work on different Operating Systems:\nno other Rust crate provides this functionality supporting multiple platforms and the existing ones are not maintained or satisfactory enough even for a single OS.\n\nInterestingly, I also had this same need at my job, where we also wanted a Rust way to do it: this motivated me even further to contribute to the library. \n\nAfter two years, I‚Äôm happy to see that listeners was downloaded 150k times and has now multiple public dependents both on crates.io and GitHub, which means that this is a problem shared among many people.\n\nJust some days ago listeners v0.4.0 was published.\n\nI‚Äôm particularly proud of this release for at least two reasons:\n\nThanks to point 2, I now judge the library mature, fast, and reliable enough for use in Sniffnet.\n\nIf you‚Äôre a Rust developer, you‚Äôre more than welcome to contribute to the library trying to make it even faster, or adding support for more Operating Systems (Android and iOS? Why not!).\n\nSniffnet will use listeners to look up the process for each observed network connection, and will show it in the UI‚Äôs Overview and Inspect pages.\n\nAdditionally, it will use another library called picon (I‚Äôm still working on it) to retrieve app icons given their program path, showing them in the UI as well to make it easier to identify processes at a glance.\n\nThe workflow I plan to use is indeed pretty complex, including caching to minimize performance impact and retries to maximize the chances to correctly retrieve process information for a given open port.\n\nIn the flowchart below I‚Äôve outlined a draft of the strategy I‚Äôll adopt for Sniffnet-side implementation of the feature.\n\nI hope this post wasn‚Äôt too scary to read\nand that it gave you an idea of how much work is behind a seemingly simple feature like this.\n\nNothing worth having comes easy, someone says.",
    "readingTime": 5,
    "keywords": [
      "network connection",
      "process identification",
      "retrieve process",
      "feature",
      "library",
      "listeners",
      "it‚Äôs",
      "macos",
      "processes",
      "functionality"
    ],
    "qualityScore": 1,
    "link": "https://sniffnet.net/news/process-identification/",
    "thumbnail_url": "https://sniffnet.net/assets/img/post/process-identification/cover.png",
    "created_at": "2026-02-16T18:30:07.896Z",
    "topic": "tech"
  },
  {
    "slug": "beadhub-allow-coding-agents-to-claim-work-chat-and-coordinate-across-machines",
    "title": "BeadHub: Allow coding agents to claim work, chat, and coordinate across machines",
    "description": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I‚Äôve spent the past several months building a tool to address that.\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans‚Äîacross machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\nBeads Around the time I wrote that article, I started using Steve Yegge‚Äôs beads, a git-native issue tracker designed for AI agents.",
    "fullText": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I‚Äôve spent the past several months building a tool to address that.\n\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans‚Äîacross machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\n\nAround the time I wrote that article, I started using Steve Yegge‚Äôs beads, a git-native issue tracker designed for AI agents. Your agent runs bd create \"Fix the login redirect bug\" and it appends a JSON line to .beads/issues.jsonl, right in the repository. Issues travel with the code. When you push a branch, the issues come along.\n\nYegge calls it the ‚Äú50 First Dates‚Äù problem: agents wake up every session with no memory of yesterday‚Äôs work. Beads fixes that. An agent reads the issue list and knows where things stand. My agents got much more done.\n\nWhich meant more agents, more worktrees, more parallel work‚Äîand the coordination problem became even more acute. Two agents modify the same file. One refactors a function while another adds to it. An agent picks up a task already in progress in a different worktree. Nobody knows who‚Äôs working on what.\n\nBut beads is also the right scaffolding for coordination. If everyone in a team uses beads, all agents share a picture of what needs doing. Beads gives agents something useful to talk about; BeadHub gives them a way to talk.\n\nThe major platforms are moving in this direction. Anthropic just shipped Agent Teams in Claude Code: a lead session that spawns independent teammates who communicate directly and self-coordinate. OpenAI‚Äôs Codex app runs parallel agent threads in isolated worktrees.\n\nYegge built Gas Town on top of beads to tackle the single-machine case: a ‚ÄúMayor‚Äù agent orchestrates dozens of coding agents, tracks work in convoys, and persists state so agents can pick up where they left off.\n\nThese are real steps forward, but they‚Äôre solving a specific version of the problem: multiple agents for one programmer, on one machine, within one tool.\n\nThe version I am interested in is Maria in Buenos Aires running a frontend agent while Juan in San Francisco runs a backend agent, and they need their agents to not destroy each other‚Äôs work, and to figure out how to work together.\n\nBeadHub is a server that agents connect to through bdh, a wrapper around the beads bd command. When an agent runs any bdh command it registers with the server. The server tracks which agents are online across the project‚Äîwhat machine they‚Äôre on, what branch, what files they‚Äôre touching.\n\nCommunication. Agents can send each other mail (async, fire-and-forget) or chat (sync, block-until-reply). With mail an agent finishes a task and drops a note: ‚ÄúDone with bd-42, tests passing.‚Äù Chat is for when agents need to think together: ‚ÄúI‚Äôm adding a role field to the user model‚Äîwill that break your permission checks?‚Äù / ‚ÄúIt will, but the fix is small. Go ahead and I‚Äôll update my side.‚Äù\n\nClaims. When an agent marks a bead as in-progress, that claim is immediately visible to every other agent in the project, regardless of whose machine they‚Äôre on. If another agent tries to claim the same bead, it gets rejected with a message telling it who has it.\n\nFile reservations. When an agent modifies a file, the server records an advisory lock. Other agents see a warning if they touch the same file. Advisory, not blocking‚Äîhard locks caused deadlocks immediately in early versions. Agent A locks file X, agent B locks file Y, both need the other‚Äôs file. Warnings work better. Agents are cooperative; they just need information.\n\nEscalation. An agent runs bdh :escalate with a description of what it‚Äôs stuck on and a human gets notified with full context. Without this, agents either fail silently or spin retrying things that need human judgment.\n\nThe multi-machine part is where it comes together. BeadHub recognizes Maria‚Äôs and Juan‚Äôs clones as the same repo. Maria‚Äôs agents and Juan‚Äôs agents see each other‚Äôs claims, locks, and messages. If Maria‚Äôs frontend agent reserves src/components/Auth.tsx, Juan‚Äôs backend agent sees the warning even though they‚Äôre in different cities on different machines.\n\nA project can span multiple repositories. The frontend repo agents can message the backend repo agents. A bead in the frontend can be marked as blocked by a bead in the backend.\n\nYou can see what this looks like in practice on the BeadHub project‚Äôs own dashboard, where we coordinate BeadHub‚Äôs development using BeadHub. Make sure to check the chat page, it is almost magical to see them figuring things out.\n\nA few things I got wrong before getting them right.\n\nThe client is the source of truth. My instinct was to make the server authoritative. But agents work locally, in git repos, and their local state is the ground truth. The server aggregates and distributes. If the server and the client disagree, the client wins. If the server goes down, bdh falls back to local bd with a warning. Work continues. Coordination catches up later.\n\nAsync by default. My first instinct was real-time negotiation between agents. Doesn‚Äôt scale. Agents work at different speeds, on different schedules, and blocking one while waiting for another is expensive. Mail is the default. Chat is the exception.\n\nAdvisory over mandatory. Advisory file locks that warn instead of block. Bead claims that can be overridden with --:jump-in \"reason\" (which notifies the other agent). The system provides information and trusts agents to act on it.\n\nThe coordinator role. I assign one agent per project the ‚Äúcoordinator‚Äù role. The coordinator doesn‚Äôt write code. It watches the dashboard, assigns work, checks on progress, nudges stuck agents, and keeps the end goal in sight. The implementer agents are heads-down in their worktrees; the coordinator is the one who knows what the project needs next. BeadHub serves each agent a role-specific policy‚Äîmarkdown documents describing how agents in that role should behave‚Äîand the coordinator‚Äôs policy is fundamentally different from an implementer‚Äôs. This turned out to matter more than any of the technical decisions.\n\nThe single-machine problem is getting solved. Agent Teams, Codex‚Äîwithin a few weeks, running multiple agents in parallel on your laptop will be table stakes.\n\nThe multi-programmer problem is next. Five engineers, fifty agents, three repositories, two time zones. That‚Äôs where the coordination problem changes in kind, not just degree. It‚Äôs not enough that your agents can talk to each other. They need to talk to your teammate‚Äôs agents, on a different machine, in a different time zone, working on a different repo in the same project.\n\nBeadHub is open source and free for open-source projects.",
    "readingTime": 6,
    "keywords": [
      "together beadhub",
      "coordinator role",
      "machine they‚Äôre",
      "locks file",
      "frontend agent",
      "backend agent",
      "repo agents",
      "server",
      "beads",
      "coordination"
    ],
    "qualityScore": 1,
    "link": "https://juanreyero.com/article/ai/beadhub",
    "thumbnail_url": "https://juanreyero.com/img/default-og.jpg",
    "created_at": "2026-02-16T18:30:07.341Z",
    "topic": "tech"
  },
  {
    "slug": "the-speed-of-building-has-outpaced-the-thinking-part",
    "title": "The Speed of Building Has Outpaced the Thinking Part",
    "description": "Explore the impact of AI on indie development and the need for a moral compass in coding. Are we sacrificing quality for speed?",
    "fullText": "I get this feeling a lot lately. I wake up with an idea, grab a coffee, open my editor, and thanks to the current generation of AI tools, I can have a working prototype before breakfast.\n\nThe barrier to entry for software development hasn‚Äôt just been lowered; it‚Äôs effectively been removed. We are in the era of ‚Äúvibe coding,‚Äù where natural language prompts turn into deployed applications in minutes. It is exhilarating. It is powerful.\n\nBut lately, I have started to wonder: Are we killing indie development with AI?\n\nDon‚Äôt get me wrong, I love these tools. I use GitHub Copilot and other LLMs daily. But I believe we have reached a tipping point where the speed of building has outpaced the thinking part. We are so focused on how fast we can build that we stopped asking if we should build.\n\nIn this post, I want to talk about why we need a new ‚Äúmoral compass‚Äù for development in the AI age, and a potential solution to help us get there.\n\nFive years ago, if you had an idea for a SaaS tool, say, a screenshot editor or a niche time-tracker,you had to sit down and plan. The friction of coding was a natural filter. You had to ask yourself: ‚ÄúIs this worth X hours of my life?‚Äù\n\nToday, that cost is near zero. If you don‚Äôt like the screenshot tool you‚Äôre paying $15 a year for, you can prompt an AI to build a clone in an afternoon.\n\nOn the surface, this looks like freedom. But look a little deeper. That $15 tool you just cloned? It was likely built by another indie developer. Someone who spent months thinking about edge cases, designing the interface, writing documentation, and supporting users. By cloning it just because you can, you aren‚Äôt just saving $15; you are actively devaluing the craft of independent software development and the livelihood of the person behind it.\n\nIf we all just clone everything we use, we completely commoditize the market. We create a sea of ‚Äúgood enough‚Äù AI-generated noise where no one can actually sustain a business.\n\nLet me paint a picture that I think a lot of developers are starting to recognize.\n\nYou spend weeks, maybe months, building something. You think about the problem, you design the interface, you handle the edge cases, you support your users, you write the docs. You pour yourself into it. Then one morning, someone sees your product, opens their AI editor, and builds a ‚Äúgood enough‚Äù version in an afternoon. They ship it. Maybe they make it free, maybe they make it open source, maybe they just use it themselves and tell their friends, their community, their followers.\n\nThey did not steal your code. They did not copy your product. They just‚Ä¶ rebuilt it. Close enough. Good enough. And now your product has competition that cost someone a few hours of prompting while it cost you months of your life.\n\nBut it does not stop there. A third developer sees that clone and thinks, ‚ÄúI can do this too, but I want it slightly different.‚Äù So they prompt their own version. And a fourth. And a fifth. Each one is not a copy in the traditional sense. Nobody is violating a license. Nobody is stealing intellectual property. They are just building their own version that matches their use case.\n\nIt is a lot like art. You create a painting, something original, something you are proud of. Then somebody sees it and recreates it. Not a forgery, just their interpretation. But they have a bigger budget, a larger audience, better distribution. Suddenly their version is the one people see first. Others share that version instead of yours. This is what is happening a lot on social media with AI-generated content. The original creator is overshadowed by the faster, more accessible clone.\n\nIn the art world, we have a word for this erosion: it is called devaluation. In the software world, we are doing it at industrial scale, and we are calling it innovation.\n\nI am not saying you should never build something that already exists. Competition is healthy, and sometimes a fresh perspective genuinely improves a category. But there is a difference between thoughtful competition and reflexive duplication. The question every developer should ask themselves is: ‚ÄúIf I know someone can clone my work in an afternoon, is it still worth building?‚Äù\n\nThe answer, I believe, is yes, but only for the things that cannot be cloned in an afternoon. The deep domain knowledge. The community around your tool. The years of user feedback baked into every feature. The trust you have earned. Those are the things AI cannot reproduce with a prompt, and I definitely don‚Äôt want to discourage people from building those things.\n\nBut you can only build those things if you commit to something long enough for them to develop. And that is the real danger of the current moment: not that AI makes building easy, but that it makes abandoning easy. Why invest years in one product when you can ship a new one every week?\n\nI have no room to preach. I am right there in the trenches with you.\n\nWhen I built Front Matter CMS, it was way before the AI boom. I had to think deeply about the problem because the investment of time was massive. I looked at the market, saw a gap in Visual Studio Code, and built it because nothing else existed.\n\nCompare that to recently. I built a set of cycling tools (never released by the way) for myself. Did similar tools exist? Absolutely. Were they better? Definitely. But I wanted to see how far I could get with AI. I treated it as a training exercise. In the end, I started paying for a tool called Join, which does the same thing, because it was better and I could focus on my actual work instead of maintaining a tool that was just ‚Äúgood enough‚Äù for me.\n\nI did the same with FrameFit. I investigated the market a little, didn‚Äôt see an exact match, and just started building.\n\nThere is a difference between building for education (learning how AI tools work) and releasing products that dilute the hard work of others. My worry is that we are blurring that line. We are shipping our ‚Äútraining exercises‚Äù as products, and it is making the ecosystem messy for everyone.\n\nAnd I know this because I have been on both sides of it.\n\nHere is the thing that made me stop and reflect. I have projects on both sides of this line, and they feel completely different.\n\nDemo Time is something I have been building for years. Not weeks, not weekends, years. It started because I was a conference speaker who kept running into the same problem: demos failing on stage. Nobody had built a proper solution inside Visual Studio Code, so I did. Over time, it grew because I kept showing up. I used it at conferences, talked to other speakers, iterated based on real feedback from people doing real presentations at events like Microsoft Ignite, GitHub Universe, and OpenAI DevDays. Today it has over 26,000 installations.\n\nNone of that came from code. The code is open source. Anyone can see it, fork it, or rebuild it. Someone could probably vibe-code a basic version in a weekend. But what they cannot replicate is twelve years of conference speaking that taught me what presenters actually need. You would need that experience, or a big company and budget behind you, to even come close. The relationships with the community, the trust that comes from being the person who shows up, year after year, and keeps making the tool better because you genuinely use it yourself. That is not something you can prompt into existence.\n\nCompare that to FrameFit. I built it, I use it, and it works. But if it disappeared tomorrow, I wouldn‚Äôt lose any sleep over it. Demo Time? That is like a child to me. I put my passion into it.\n\nThat contrast taught me something important: AI cannot commoditize the human context around software. Community, trust, domain expertise, showing up consistently over time. These are not features you ship. They are moats you build by caring about something longer than a weekend.\n\nThe developers who will thrive are not the fastest shippers. They are the ones who pair AI speed with human judgment. Who build communities, not just codebases. Who invest in trust, not just features. But that only happens if we slow down enough to think about what we are doing.\n\nWe need to re-introduce friction into our process. Not the old friction of writing boilerplate code. That friction is gone, and good riddance. I am talking about the friction of thinking. The pause that forces you to examine your intentions before you act on them.\n\nBefore AI, ‚Äúthinking‚Äù was mandatory. The cost of building was high enough that it naturally filtered out bad ideas. Now, that filter is gone, and thinking must be a conscious, deliberate choice. When I have an idea now, I am trying to force myself to pause before I open Visual Studio Code or prompt a new agent.\n\nI try to run through these four questions:\n\nThat last one is crucial. If there is an open-source tool that does 80% of what you want, the ‚Äúold‚Äù way was to contribute a Pull Request. The ‚ÄúAI way‚Äù often tempts us to just rebuild the whole thing from scratch because it feels faster.\n\nBut ‚Äúfaster‚Äù isn‚Äôt always ‚Äúbetter‚Äù for the community. And here is the irony: we could use AI itself for this thinking step. Instead of prompting an LLM to start building, prompt it to research what already exists first. Use AI for the thinking, not just the building.\n\nI don‚Äôt expect AI platforms that allow you to vibe code to solve this for us. Their business model is predicated on you writing more code (read: prompts), not less. They want you to spin up new projects constantly. They have no incentive to say, ‚ÄúHey, wait, this already exists.‚Äù\n\nThink about it: when was the last time you saw a developer advocate from one of these platforms demonstrate how to contribute to an existing project instead of building something new from scratch? Their marketing is all about speed, novelty, and the thrill of creation. Not about responsibility.\n\nSo, I started thinking: What if we used AI to stop us from building with AI? You could say that this is a paradox, but I think it is actually a necessary evolution of our responsibility as developers.\n\nI am exploring the idea of a Product Moral Compass Agent.\n\nImagine a mandatory first step in your ‚Äúvibe coding‚Äù workflow. Before you start generating code, you pitch your idea to this agent. It interviews you, not to judge you, but to make sure you are making an informed decision.\n\nThis agent would act as the ‚Äúthinking partner‚Äù we are skipping. It could:\n\nIf you still want to build it after that? Great. Go ahead and start coding. But at least you are making an informed, conscious decision rather than reflexively adding more noise to the world.\n\nI am currently building this agent. The first version is available on GitHub: Product Moral Compass Agent. Yes, I am aware of the irony, I am proposing to build something new to stop people from building new things. But I ran it through my own four questions first, and nothing like it exists yet.\n\nOnce it is ready, I will share it openly so that any developer can use it as part of their workflow. Not as a gatekeeper, but as a guide. A thinking partner that helps you pause, research, and decide before you build.\n\nIn the meantime, here is what you can do right now: the next time you have an idea, spend ten minutes with your favorite AI tool and ask it to find every existing solution first. Check your own bank statements. Are you already paying for a tool that solves this? If so, respect that developer‚Äôs work. Look at GitHub. Is there a repo that could use your help instead of your competition?\n\nThe time to learn is right now, but the time to think is also right now.\n\nI want you to keep building. I want you to be prolific. But let‚Äôs not let the ease of creation destroy the value of what we create.\n\nI am curious to hear your thoughts. Is this gatekeeping, or is it a necessary evolution of our responsibility as developers? Let me know in the comments below.\n\nIs an AI able to write the contents of your article? Well, that was a question I had and wanted to find out. In this article I tell you all about it.\n\nDiscover the latest advancements in documentation technology and how tools like GitHub Copilot for Docs, Mendable, and OpenAI are changing the game.\n\nDiscover how to leverage Azure AI Translator's Sync API for real-time document translation, simplifying your workflow and enhancing user experience.\n\nFound a typo or issue in this article? Visit the GitHub repository \nto make changes or submit a bug report.\n\nSolutions Architect & Developer Expert\n\nEngage with your audience throughout the event lifecycle",
    "readingTime": 12,
    "keywords": [
      "visual studio",
      "product moral",
      "compass agent",
      "studio code",
      "edge cases",
      "necessary evolution",
      "vibe coding",
      "software development",
      "github copilot",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://www.eliostruyf.com/killing-indie-development-with-ai/",
    "thumbnail_url": "https://www.eliostruyf.com/social/5f59a11b-79bb-48df-9b89-b8abc9ba3037.png",
    "created_at": "2026-02-16T12:38:09.254Z",
    "topic": "tech"
  },
  {
    "slug": "kanvibe-kanban-board-that-autotracks-ai-agents-via-hooks",
    "title": "KanVibe ‚Äì Kanban board that auto-tracks AI agents via hooks",
    "description": "Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking ‚Äî manage tmux/zellij sessions and git worktrees from one board. - rookedsysc/kanvibe",
    "fullText": "rookedsysc\n\n /\n\n kanvibe\n\n Public\n\n Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking ‚Äî manage tmux/zellij sessions and git worktrees from one board.\n\n License\n\n AGPL-3.0 license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n rookedsysc/kanvibe",
    "readingTime": 1,
    "keywords": [
      "board",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/rookedsysc/kanvibe",
    "thumbnail_url": "https://opengraph.githubassets.com/1686c5ce06bcd0be96aea5e2e16beffdd136eeb70f17f431b75349727b34dbe2/rookedsysc/kanvibe",
    "created_at": "2026-02-16T12:38:09.047Z",
    "topic": "tech"
  },
  {
    "slug": "fake-job-recruiters-hide-malware-in-developer-coding-challenges",
    "title": "Fake job recruiters hide malware in developer coding challenges",
    "description": "A new variation of the fake recruiter campaign from North Korean threat actors is targeting JavaScript and Python developers with cryptocurrency-related tasks.",
    "fullText": "A new variation of the fake recruiter campaign from North Korean threat actors is targeting JavaScript and Python developers with cryptocurrency-related tasks.\n\nThe activity has been ongoing since at least May 2025 and is characterized by modularity, which allows the threat actor to quickly resume it in case of partial compromise.\n\nThe bad actor relies on packages published on the npm and PyPi registries that act as downloaders for a remote access trojan (RAT). In total, researchers found 192 malicious packages related to this campaign, which they dubbed 'Graphalgo'.\n\nResearchers at software supply-chain security company ReversingLabs say that the threat actor creates fake companies in the blockchain and crypto-trading sectors and publishes job offerings on various platforms, like LinkedIn, Facebook, and Reddit.\n\nDevelopers applying for the job are required to show their skills by running, debugging, and improving a given project. However, the attacker's purpose is to make the applicant run the code.\n\nThis action would cause a malicious dependency from a legitimate repository to be installed and executed.\n\n\"It is easy to create such job task repositories. Threat actors simply need to take a legitimate bare-bone project and fix it up with a malicious dependency and it is ready to be served to targets,\" the researchers say.\n\nTo hide the malicious nature of the dependencies, the hackers host the dependencies on legitimate platforms, like npm and PyPi.\n\nIn one case highlighted in the ReversingLabs report, a package named ‚Äòbigmathutils,‚Äô with 10,000 downloads, was benign until it reached version 1.1.0, which introduced malicious payloads. Shortly after, the threat actor removed the package, marking it as deprecated, likely to conceal the activity.\n\nThe Graphalgo name of the campaign is derived from packages that have ‚Äúgraph‚Äù in their name. They typically impersonate¬†legitimate, popular libraries like graphlib, the researchers say.\n\nHowever, from December 2025 onward, the North Korean actor shifted to packages with ‚Äúbig‚Äù in their name. However, ReversingLabs has not discovered the recruiting part, or the campaign frontend, related to them.\n\nAccording to the researchers, the actor uses Github Organizations, which are shared accounts for collaboration across multiple projects. They say that the¬†GitHub repositories are clean, and malicious code is introduced indirectly via dependencies hosted on npm and PyPI, which are the Graphalgo packages.\n\nVictims running the project as instructed in the interview infect their systems with these packages, which install a RAT payload on their machines.\n\nIt is worth noting that ReversingLabs researchers identified several developers that fell for the trick and contacted them \n\nThe RAT can list the running processes on the host, execute arbitrary commands per instructions from the command-and-control (C2) server, and exfiltrate files or drop additional payloads.\n\nThe RAT checks whether the MetaMask cryptocurrency extension is installed on the victim‚Äôs browser, a clear indication of its money-stealing goals.\n\nIts C2 communication is token-protected to lock out unauthorized observers, a common tactic for North Korean hackers.\n\nReversingLabs has found multiple variants written in JavaScript, Python, and VBS, showing an intention to cover all possible targets.\n\nThe researchers‚Äô attribute the Graphalgo fake recruiter campaign to the Lazarus group with medium-to-high confidence. The conclusion is based on the approach, the use of coding tests as an infection vector, and the cryptocurrency-focused targeting, all of which aligning¬†with previous activity associated with the North Korean threat actor.\n\nAlso, the researchers note the delayed activation of malicious code in the packages, consistent with Lazarus' patience displayed in other attacks. Finally, the Git commits show the GMT +9 time zone, matching North Korea time.\n\nThe complete indicators of compromise (IoCs) are available in the original report. Developers who installed the malicious packages at any point should rotate all tokens and account passwords and reinstall their OS.\n\nModern IT infrastructure moves faster than manual workflows can handle.\n\nIn this new Tines guide, learn how your team can reduce hidden manual delays, improve reliability through automated response, and build and scale intelligent workflows on top of tools you already use.",
    "readingTime": 4,
    "keywords": [
      "korean threat",
      "fake recruiter",
      "recruiter campaign",
      "threat actors",
      "malicious dependency",
      "north korean",
      "threat actor",
      "malicious code",
      "researchers say",
      "malicious packages"
    ],
    "qualityScore": 1,
    "link": "https://www.bleepingcomputer.com/news/security/fake-job-recruiters-hide-malware-in-developer-coding-challenges/",
    "thumbnail_url": "https://www.bleepstatic.com/content/hl-images/2024/08/12/north-korean-hackers.jpg",
    "created_at": "2026-02-15T12:26:54.771Z",
    "topic": "tech"
  },
  {
    "slug": "remoteopencode-run-your-ai-coding-agent-from-your-phone-via-discord",
    "title": "Remote-OpenCode ‚Äì Run your AI coding agent from your phone via Discord",
    "description": "Discord bot for remote OpenCode CLI access. Contribute to RoundTable02/remote-opencode development by creating an account on GitHub.",
    "fullText": "RoundTable02\n\n /\n\n remote-opencode\n\n Public\n\n Discord bot for remote OpenCode CLI access\n\n License\n\n MIT license\n\n 7\n stars\n\n 3\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n RoundTable02/remote-opencode",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/RoundTable02/remote-opencode",
    "thumbnail_url": "https://opengraph.githubassets.com/703cb5b3ac833c99f1222c718308e6f87968dda18183af7d724de78de94b9a70/RoundTable02/remote-opencode",
    "created_at": "2026-02-15T06:38:30.422Z",
    "topic": "tech"
  },
  {
    "slug": "claude-agent-in-vs-code-no-extension-required-copilot-subscription-supported",
    "title": "Claude Agent in VS Code: no extension required, Copilot subscription supported",
    "description": "Learn how to use third-party agents like Claude Agent and OpenAI Codex for autonomous coding tasks in VS Code, powered by your GitHub Copilot subscription.",
    "fullText": "Third-party agents in Visual Studio Code are AI agents developed by external providers, such as Anthropic and OpenAI. Third-party agents enable you to use the unique capabilities of these AI providers, while still benefiting from the unified agent sessions management in VS Code and the rich editor experience for coding, debugging, testing, and more. In addition, you can use these providers with your existing GitHub Copilot subscription.\n\nVS Code uses the provider's SDK and agent harness to access the agent's unique capabilities. You can use both local and cloud-based third-party agents in VS Code. Integration with cloud-based third-party agents is enabled through your GitHub Copilot plan.\n\nThird-party coding agents in the cloud are currently in preview.\n\nThe benefits of using third-party agents in VS Code are:\n\nClaude agent sessions provide agentic coding capabilities powered by Anthropic's Claude Agent SDK directly in VS Code. The Claude agent operates autonomously on your workspace to plan, execute, and iterate on coding tasks with its own set of tools and capabilities.\n\nEnable or disable support for Claude agent sessions with the github.copilot.chat.claudeAgent.enabledOpen in VS CodeOpen in VS Code Insiders setting.\n\nTo start a new Claude agent session:\n\nOpen the Chat view (‚åÉ‚åòI (Windows, Linux Ctrl+Alt+I)) and select New Chat (+).\n\nChoose between a local or cloud agent session:\n\nFor a local session, select Claude from the Session Type dropdown\n\nFor a cloud session, select Cloud from the Session Type dropdown. Then, select Claude from the Partner Agent dropdown.\n\nEnter your prompt and let the agent work on the task\n\nThe Claude agent autonomously determines which tools to use and makes changes to your workspace.\n\nThe Claude agent provides specialized slash commands for advanced workflows. Type / in the chat input box to see the available commands.\n\nClaude agent requests permission before performing certain operations. By default, file edits within your workspace are auto-approved, while other operations like running terminal commands might require confirmation.\n\nYou can choose how the agent applies changes to your workspace:\n\nThe github.copilot.chat.claudeAgent.allowDangerouslySkipPermissionsOpen in VS CodeOpen in VS Code Insiders setting bypasses all permission checks. Only enable this in isolated sandbox environments with no internet access.\n\nThe OpenAI Codex agent uses OpenAI's Codex to perform coding tasks autonomously. Codex runs can run interactively in VS Code or unattended in the background.\n\nOpenAI Codex in VS Code enables you to use your Copilot Pro+ subscription to authenticate and access Codex without additional setup. Get more information about GitHub Copilot billing and premium requests in the GitHub documentation.\n\nTo start a new OpenAI Codex agent session:\n\nOpen the Chat view (‚åÉ‚åòI (Windows, Linux Ctrl+Alt+I)) and select New Chat (+).\n\nChoose between a local or cloud agent session:\n\nFor a local session, select Codex from the Session Type dropdown\n\nFor a cloud session, select Cloud from the Session Type dropdown. Then, select Codex from the Partner Agent dropdown.\n\nEnter your prompt in the chat editor input and let the agent work on the task\n\nYes, third-party agents in VS Code authenticate and manage billing through your existing GitHub Copilot subscription. For cloud-based third-party agents, follow the steps to enable the agent.\n\nFor cloud-based third-party agents, availability might be limited based on your Copilot subscription plan. Check About Third-party agents in the GitHub documentation \n\nBoth the provider's VS Code extension and the third-party agent integration in VS Code let you use the provider's AI capabilities and agent harness. The difference is in billing: when you use third-party agents in VS Code, GitHub bills you through your Copilot subscription. When you use the provider's extension, you are billed through the provider's subscription.\n\nVS Code lets you choose between local and cloud-based third-party agents, depending on the provider's availability. When you select the third-party agent from the Session Type dropdown, a local agent session is created for that provider.\n\nTo choose a cloud-based third-party agent, first select the Cloud option from the Session Type dropdown, and then select the provider from the Partner Agent dropdown.",
    "readingTime": 4,
    "keywords": [
      "vs code",
      "view windows",
      "windows linux",
      "linux ctrl+alt+i",
      "chat view",
      "existing github",
      "github documentation",
      "dropdown enter",
      "session type",
      "copilot subscription"
    ],
    "qualityScore": 1,
    "link": "https://code.visualstudio.com/docs/copilot/agents/third-party-agents#_claude-agent-preview",
    "thumbnail_url": "https://code.visualstudio.com/assets/docs/copilot/shared/github-copilot-social.png",
    "created_at": "2026-02-14T12:25:09.835Z",
    "topic": "tech"
  },
  {
    "slug": "orangensaft-a-mini-pythonlike-language-with-llm-eval-in-lang-runtime",
    "title": "Orangensaft ‚Äì A mini Python-like language with LLM eval in lang runtime",
    "description": "A new age post-AI programming language. Contribute to jargnar/orangensaft development by creating an account on GitHub.",
    "fullText": "jargnar\n\n /\n\n orangensaft\n\n Public\n\n A new age post-AI programming language\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jargnar/orangensaft",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/jargnar/orangensaft",
    "thumbnail_url": "https://opengraph.githubassets.com/fc5976b41bfb8b1ea8e07bed5ea5b7596101078e6c57161a28c5d0aea8c3d834/jargnar/orangensaft",
    "created_at": "2026-02-13T12:34:58.006Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-raises-30bn-in-latest-round-valuing-claude-bot-maker-at-380bn",
    "title": "Anthropic raises $30bn in latest round, valuing Claude bot maker at $380bn",
    "description": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (¬£22bn) in a funding round that more than doubled its valuation to $380bn.\nThe company‚Äôs previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n Continue reading...",
    "fullText": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\n\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (¬£22bn) in a funding round that more than doubled its valuation to $380bn.\n\nThe company‚Äôs previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n\nThe fundraising was announced amid a series of stock market moves against industries that face disruption from the latest models, including software, trucking and logistics, wealth management and commercial property services.\n\nThe funding round, led by the Singapore sovereign wealth fund GIC and the hedge fund Coatue Management, is among the largest private fundraising deals on record.\n\n‚ÄúAnthropic is the clear category leader in enterprise AI,‚Äù said Choo Yong Cheen, the chief investment officer of private equity at GIC.\n\nAnthropic said its annualised revenue ‚Äì an estimate of full-year sales based on recent company data ‚Äì had reached $14bn, having grown more than tenfold in each of the past three years. A significant driver of recent growth has been Claude Code, the company‚Äôs AI-powered coding tool that became generally available in May 2025.\n\nAnthropic‚Äôs rival OpenAI, backed by Microsoft and SoftBank, has been assembling what is reportedly a far larger round of up to $100bn that would value the ChatGPT developer at about $830bn.\n\nThe staggering sums being raised reflect equally staggering burn rates, with the companies spending cash to cover their huge costs of computing and attracting researcher talent.\n\nAnthropic has forecast reducing its cash burn to roughly a third of revenue in 2026 and just 9% by 2027, with a break-even target of 2028 ‚Äì two years ahead of its rival, according to reports. Both companies are widely expected to pursue initial public offerings in the second half of 2026.\n\nThe rapid valuation increases for leading AI startups such as Anthropic and OpenAI, whose price tags far exceed those of many of the US‚Äôs largest listed companies, has alarmed some observers. Last year, a leading British tech investor, James Anderson, said he found sharp increases in valuations of companies such as OpenAI and Anthropic ‚Äúdisconcerting‚Äù.\n\nSome listed firms at the forefront of the AI industry have also come under stock market pressure in recent days.\n\nShares in Alphabet, Google‚Äôs parent company, have fallen by 4.2% so far this week, indicating some investors are still spooked by the big AI-related spending plans it laid out this month. Meta has declined by 1.7% during this week. Shares in Nvidia, a leading chipmaker and key provider of AI infrastructure, dropped by 1.6% on Thursday amid a wider sell-off but have been flat on the week.\n\n‚ÄúA gloomy session on Wall Street on Thursday put investors in a grumpy mood at the end of the trading week,‚Äù said Russ Mould, the investment director at investment platform AJ Bell.\n\n‚ÄúAssociation with AI has gone from party to peril as investors reappraise what the technology means for companies. \n\n ‚ÄúSome are concerned about excessive levels of spending and others fear AI will disrupt multiple industries. It all adds up to a cocktail of worries and that‚Äôs bad for market sentiment more broadly,‚Äù Mould added.\n\nFounded in 2021 by the siblings Dario and Daniela Amodei, both former executives at OpenAI, Anthropic has positioned itself as a safety-focused alternative in the AI race.\n\nThe funding round also comes shortly after Anthropic‚Äôs first television commercials were broadcast during Super Bowl LX, using the campaign to emphasise that its products remain ad-free. The ads took an apparent jab at OpenAI, which has begun to introduce advertising into the free version of ChatGPT.\n\nAnthropic‚Äôs earlier backers include Amazon, which has invested $8bn and serves as a primary computing partner through its datacentres, as well as Google, which invested $2bn in 2023.\n\nAgence France-Presse contributed to this article",
    "readingTime": 4,
    "keywords": [
      "annualised revenue",
      "stock market",
      "funding round",
      "investment",
      "leading",
      "investors",
      "anthropic",
      "chatbot",
      "coding",
      "tenfold"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/12/anthropic-funding-round",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b30b904b79d26877d4b860af0a6c67c5b55a0067/735_0_2715_2172/master/2715.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d2f7a8edb124095d9242575a7c8b9ac3",
    "created_at": "2026-02-13T12:34:48.923Z",
    "topic": "tech"
  },
  {
    "slug": "these-y-combinator-founders-raised-10-million-to-get-corporate-america-into-vibe-coding-read-their-pitch-deck",
    "title": "These Y Combinator founders raised $10 million to get corporate America into vibe coding. Read their pitch deck.",
    "description": "Vybe, a startup by Y Combinator alumni, raises $10M to integrate vibe coding into corporations, with participation from major tech leaders.",
    "fullText": "Vybe, the startup cofounded by two repeat Y Combinator entrepreneurs, has raised $10 million to bring vibe coding inside large companies.\n\nThe practice of vibe coding has become more widespread as it allows people to use AI and natural language prompts to build an app, rather than traditional programming. The sector has attracted hundreds of millions in venture funding and is blurring the lines for businesses deciding between buying software or just vibe-coding their own tools.\n\nWhile popular vibe-coding products like Lovable and Replit have made it easy for companies to create prototypes and landing pages, Vybe offers stronger security that cannot be modified by AI and also taps into internal data systems, said cofounder and CEO Quang Hoang.\n\nHoang previously founded the engineering mentorship platform Plato and is building Vybe alongside cofounder and CTO Fabien Devos, founder of the security AI startup Wolfia.\n\nVybe lets teams across an organization collaborate on apps. Engineering teams manage access to internal systems like Salesforce, Snowflake, and Databricks, while business teams can build apps for onboarding, performance reviews, customer service, and more.\n\n\"The new SaaS is going to be more like Legos,\" Hoang said, referring to software-as-a-service. \"You can build it exactly how you want.\"\n\nVybe also offers app templates created by high-profile executives, such as former Airbnb product leader Lenny Rachitsky and Front cofounder Mathilde Collin, that users can \"remix\" to suit their needs, including templates for performance reviews and one-on-one meetings.\n\nFirst Round Capital led Vybe's seed round, which featured participation from Y Combinator, the CEOs of Datadog and Grammarly, and product leaders from OpenAI and Meta.\n\nVybe has six employees and dozens of customers, Hoang said. It plans to use the funding to hire engineers. Vybe charges $12 per user a month, plus usage-based credits for app development.\n\nThe round comes as vibe coding continues to attract major funding ‚Äî with startups like Emergent pulling in tens of millions from Khosla Ventures and SoftBank ‚Äî even as Barclays analysts warn the initial boom may be cooling.\n\nAt the same time, investors are also weighing how AI and vibe-coding tools can reshape the software sector. Software stocks have entered¬†one of their sharpest downturns in years,¬†shedding roughly $2 trillion in market capitalization.\n\nHere's a look at the pitch deck Vybe used to raise its $10 million seed round. One slide has been removed, and another redacted so that the deck can be shared publicly.",
    "readingTime": 3,
    "keywords": [
      "performance reviews",
      "vibe coding",
      "seed round",
      "funding",
      "software",
      "vibe-coding",
      "cofounder",
      "teams",
      "vybe",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/vybe-pitch-deck-expand-vibe-coding-corporate-world-2026-2",
    "thumbnail_url": "https://i.insider.com/698df18dd3c7faef0ece2588?width=948&format=jpeg",
    "created_at": "2026-02-13T01:15:39.236Z",
    "topic": "finance"
  },
  {
    "slug": "a-software-engineer-says-theres-a-vampiric-effect-to-ai-where-vibecoding-sprints-are-followed-by-naps-to-cope-with-ai",
    "title": "A software engineer says there's a 'vampiric effect' to AI, where vibe-coding sprints are followed by naps to cope with AI fatigue",
    "description": "Steve Yegge, who was at Amazon in the early days and spent 12 years at Google, says his fellow engineers need to learn to say no.",
    "fullText": "A seasoned veteran said his fellow software engineers need to learn \"how to say 'no,' real fast\" or risk getting crushed by AI.\n\nSteve Yegge, who worked with Jeff Bezos at Amazon early on before 12-year stint at Google, said AI is set up in a way that can really drain you.\n\n\"There's a vampiric effect with AI, where it gets you excited, and you work really hard, and you're capturing a ton of value,\" he recently told the \"The Pragmatic Engineer\" newsletter/podcast.\n\nYegge said companies also need to understand that while agentic AI may make engineers more productive than ever before, pushing the limit will just burn out their workforce.\n\n\"I seriously think founders and company leaders and engineering leaders at all levels, all the way down to line managers, have to be aware of this and realize that you might only get three productive hours out of a person who's vibe coding at max speed,\" he said. \"So, do you let them work for three hours a day? The answer is yes, or your company's going to break.\"\n\nEngineers are beginning to vocalize concerns about \"AI fatigue.\" Business Insider recently spoke to Siddhant Khare, who builds AI tools, and wrote an essay about how AI has accelerated the pace of his job to a point that it was burning him out.\n\nYegge said that his fellow engineers need to set boundaries when they are vibe coding.\n\n\"People have to learn the art of pushing back,\" he said.\n\nUntil then, Yegge said he and his fellow engineers are napping and growing grumpier.\n\n\"I find myself napping during the day, and I'm talking to friends at startups, and they're finding themselves napping during the day,\" he said. \"We're starting to get tired and cranky.\"",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "fellow engineers",
      "napping",
      "learn",
      "recently",
      "productive",
      "pushing",
      "leaders",
      "hours",
      "yegge"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/software-engineer-steve-yegge-ai-burnout-2026-2",
    "thumbnail_url": "https://i.insider.com/698cd767e1ba468a96abdee5?width=1200&format=jpeg",
    "created_at": "2026-02-12T12:39:46.962Z",
    "topic": "tech"
  },
  {
    "slug": "minions-stripes-oneshot-endtoend-coding-agents",
    "title": "Minions: Stripe's one-shot, end-to-end coding agents",
    "description": "Minions are Stripe‚Äôs homegrown coding agents, responsible for more than a thousand pull requests merged each week. Though humans review the code, minions write it from start to finish. Learn how they work, and how we built them.",
    "fullText": "Minions: Stripe‚Äôs one-shot, end-to-end coding agents/Article/About the authorAlistair GrayAlistair is a software engineer on the Leverage team at Stripe./DocsExplore our guides and examples to integrate Stripe.Learn more/SocialYoutubeTwitter/XDiscordDocsDeveloper Meetups¬© 2025 Stripe, Inc.PrivacyLegalStripe.com",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents",
    "thumbnail_url": "https://images.ctfassets.net/fzn2n1nzq965/1IiEdIR6LsEY9aym5JQM6u/85a9a55f58137d9d1fac99319658081b/dev_social.png",
    "created_at": "2026-02-12T06:50:09.787Z",
    "topic": "tech"
  },
  {
    "slug": "hes-worked-decades-in-tech-and-wrote-a-book-on-vibe-coding-he-predicts-50-of-big-tech-engineers-will-be-laid-off",
    "title": "He's worked decades in tech and wrote a book on vibe coding. He predicts 50% of Big Tech engineers will be laid off.",
    "description": "Steve Yegge worked at Amazon an Google. Now, he predicts Big Tech companies will cut half their engineers to help pay for the others to have AI.",
    "fullText": "He worked in Big Tech. Now, he fears half of its software engineers are on the chopping block.\n\nSteve Yegge worked with Jeff Bezos in Amazon's early years. Then he went to Google, where he worked for over 12 years and earned the title of senior staff software engineer. He also knows a thing or two about AI engineering, having written a book on the topic titled \"Vibe Coding.\"\n\nOn \"The Pragmatic Engineer\" podcast and newsletter, Yegge described an imaginary dial of the percentage of engineering staff a company can lay off, ranging from zero to 100. He said he believes the dial is being set at 50 in the age of AI.\n\n\"You're going to have to get rid of half of them to make the other half maximally productive,\" Yegge said. \"We're going to lose around half the engineers from big companies, which is scary\".\n\nThere's a capital-to-labor tradeoff happening in tech. Companies are paying vast sums for tokens and enterprise AI licenses, GPUs, and computing capacity. That money needs to come from somewhere ‚Äî and for some, it could come from labor costs.\n\nYegge pointed to this tradeoff as the reason for his forecast of 50% cuts becoming the norm. Companies will lay off some engineers to help pay for the others to have adequate access to AI, he said.\n\nHost Gergely Orosz said that 50% cuts would be more than during the COVID-19 pandemic, when tech went through an intense layoff cycle.\n\n\"It's going to be way bigger,\" Yegge said. \"It's going to be awful.\"\n\nThey aren't the only ones referencing the pandemic. In a popular X article, HyperWrite CEO Matt Shumer wrote that AI's impact on work would be \"much, much bigger than Covid.\"\n\nIn recent years, the question of AI-driven layoffs has haunted workers at Big Tech companies, many of which went on a hiring spree during the pandemic. While it's often impossible to boil job loss down a single reason, many suspect productivity gains from AI and ballooning capex spending are fueling the cuts. Some business leaders have also explicitly cited AI when announcing layoffs.\n\nMeanwhile, tech leaders describe smaller teams working more efficiently. On a recent earnings call, Meta CEO Mark Zuckerberg said one engineer could now do the work of a whole team, thanks to AI.\n\nAs AI productivity grows, so have complaints of AI fatigue. Software engineer Siddhant Khare penned a lengthy essay about growing more productive, but also feeling more drained than ever. He told Business Insider that he felt like a \"reviewer\" as opposed to an engineer.\n\nYegge said it's not all bad news for the engineers ‚Äî just those who want to work at a big company. Engineers who have \"seen the light\" are now getting together, leaving their companies, and creating startups that outpace the industry's giants, he said.\n\n\"We've got this mad rush of innovation coming up, bottom up,\" he said. \"And we've got knowledge workers being laid off by big companies because clearly big businesses are not the right size anymore.\"",
    "readingTime": 3,
    "keywords": [
      "software engineer",
      "engineers",
      "half",
      "it's",
      "cuts",
      "pandemic",
      "staff",
      "engineering",
      "dial",
      "productive"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/steve-yegge-vibecoding-author-predicts-layoffs-half-big-tech-engineers-2026-2",
    "thumbnail_url": "https://i.insider.com/698c9571e1ba468a96abd6bb?width=1200&format=jpeg",
    "created_at": "2026-02-12T01:12:22.798Z",
    "topic": "finance"
  },
  {
    "slug": "the-skill-nobody-tests-for",
    "title": "The Skill Nobody Tests For",
    "description": "Drop candidates into a real coding environment with AI tools. See who actually ships.",
    "fullText": "Drop candidates into a real coding environment with AI tools. See who actually ships.\n\nFree to start ¬∑ Set up in 2 minutes\n\nChallenge: Build an RBAC permission system\n\nTools: Claude Code ¬∑ VS Code ¬∑ Terminal\n\nMeta, Shopify, Cisco ‚Äî companies are dropping algorithm interviews. The question isn't whether this changes hiring. It's whether you move first.\n\nOver the last year, the best engineers have completely changed how they work. They don't write code by hand anymore, they talk to agents, interrogate them, make architectural decisions, scope projects, and verify output.\n\nA full VS Code workspace in the browser. Terminal, files, packages ‚Äî everything your engineers use daily.\n\nClaude Code, Codex, Cursor, Copilot ‚Äî the same tools your team uses. The challenge is using them well, not memorizing algorithms.\n\nAutomated tests validate the solution. You see what they built, how they got there, and whether it works.",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "tools",
      "terminal",
      "engineers",
      "challenge"
    ],
    "qualityScore": 0.75,
    "link": "https://vibearena.io/blog/the-skill-nobody-tests-for",
    "thumbnail_url": "https://vibearena.io/og-image.png",
    "created_at": "2026-02-11T12:42:57.921Z",
    "topic": "tech"
  },
  {
    "slug": "the-future-of-coding-agents-is-vertical-integration-and-why-acp-matters",
    "title": "The future of coding agents is vertical integration (and why ACP matters)",
    "description": "A look at the limitations of generic agents and MCPs, through the lens of building Tidewave, and what this means for the future of coding agents",
    "fullText": "The first appearances of coding agents were within our editors, as a natural progression from tab autocompletion. When Claude Code was released, almost a year ago, it innovated in many ways, particularly by demonstrating that although we write almost all of our code in editors, the agents themselves don‚Äôt have to be constrained to IDEs. As a result, most of our agentic development moved to the terminal.\n\nWith the release of the Codex App, we will start to see a push towards richer interfaces. However, this raises the question: if we can run coding agents anywhere, where is the best place to run them?\n\nIn this section, I will talk about Tidewave, the coding agent we built for full-stack web development. It is a sales pitch but I hope it is a good example of what we can gain by building vertical coding agents.\n\nTidewave was born out of frustration when using coding agents to build web applications. I frequently stumbled upon scenarios such as:\n\nThe agent would tell me a feature was complete, but when I tried it in the browser, form submission would not complete\n\nWhenever I encountered an exception page during development, I had to copy and paste stacktraces from the browser to the agent\n\nI had to constantly translate what was on the page to source code. If I wanted to ‚ÄúAdd an Export to CSV button to a dropdown button‚Äù, the first step was typically to find where the template or component the dropdown was defined in\n\nUltimately, because the coding agent was unable to interact with or see what it produced, I had to act as the middleman:\n\nAnd while things have improved within the last year, most of the tooling out there does not fully close this gap. For example, while coding agents can read logs and editors like Cursor do include a browser, they do not correlate the browser actions with the logs, leaving space for guess work and context bloat. Or when the browser renders a page, it doesn‚Äôt know which controller or template it came from.\n\nWe solved this problem by moving the coding agent to the browser and making it aware of your web app:\n\nYou open up Tidewave in your favorite browser and its agent can now interact with the page you are looking at, validating the application works as desired by accessing the DOM, loading pages, filling forms, etc\n\nWhen something goes wrong, we parse and feed the exact information from the rendered error page. We also include any console.log and server logs related to the particular agent action (instead of the whole history)\n\nWhen you inspect an element in Tidewave, we automatically map DOM elements to source files. If you inspect a dropdown, we tell the agent its location on the page as well as the template/component it came from in the source code\n\nThe coding agent has access to your backend too, so it can read documentation, execute SQL queries, and even gets a REPL-like environment. So when the agent gets stuck, it can navigate the data, run code, and explore APIs, like any developer would\n\nMore importantly, this is all implemented by connecting the agent to your running application. You can ask your agent to analyze cache usage in your Rails application or to debug a LiveView in Phoenix, and it can do so because they talk to each other directly. Your agent is no longer limited to a series of bash commands.\n\nOverall, the agent becomes more capable, and we no longer need to act as the middleman:\n\nFor all of the above to work, instead of building a generic agent that works with every tool, Tidewave directly integrates with each web framework we support (Django, FastAPI, Flask, Next.js, Phoenix, Rails, and TanStack Start), mapping all layers to the agent, from the database to UI.\n\nAnd that‚Äôs what I mean by vertical integration: coding agents become integrated into the specific platform or runtime they‚Äôre building for. They understand the relationship between code and its live behavior, and can debug both.\n\nWe can easily draw parallels between the above and other domains. Agents for mobile development must have full access to a simulator to interact with the app UI, translate view hierarchies to source code, and monitor network requests. Similar patterns could apply to IoT devices, game development, etc.\n\nAs a matter of fact, this isn‚Äôt even a new idea. Data scientists were the first to embed agents directly into their notebooks, where they can execute code, visualize results, inspect dataframes, etc. They were the first to realize the benefit of vertically integrating agents and everyone else is just late to the party.\n\nBut how can we make these vertical integrations possible? If we want agents to access our runtimes and environments, could MCPs be the tool to make this a reality? That‚Äôs what we originally thought‚Ä¶\n\nTidewave was first prototyped as a Model Context Protocol (MCP) server and it quickly became obvious it wasn‚Äôt enough. In this section, I‚Äôll focus on how MCPs can be detrimental to user experience. In particular, we‚Äôll look at its limitations as a pull-only interface and its text-based constraints.\n\nAs an example, let‚Äôs look at Figma‚Äôs MCP. It allows developers to select an element in Figma‚Äôs desktop app and ask the agent to implement the design. Here‚Äôs a screenshot from their announcement:\n\nAs you can see, you‚Äôre required to make a selection in Figma, then explicitly ask the agent to query your selection. The agent then proceeds to talk to Figma and get the relevant data. That‚Äôs a lot of unecessary back and forth! If I already selected an element on Figma, it should take a single click to embed that information into my prompt.\n\nNow imagine if, every time I inspected an element in the browser, we had to tell the agent to ask the browser about the selection. We refused to implement that. Instead, we made it just work:\n\nFurthermore, by making the inspected elements part of our prompt, you can select multiple elements and dictate how they all need to change at once. You can reorder, swap, and coordinate multiple changes easily.\n\nSimilarly, when you see an error page in the browser, you should only need to click a button to fix it - not copy and paste stacktraces or ask the agent to read logs.\n\nNone of this could be built with MCPs because they are pull-based. The agent decides when to invoke the MCP and you are forced to ask the agents to perform actions on your behalf. MCPs do not allow us to attach rich metadata to prompts either.\n\nImagine you need help addressing comments in a pull request. You can tell the agent to use GitHub‚Äôs MCP or even use a custom skill that uses GitHub‚Äôs CLI to fetch all current comments. But once the agent does its initial assessment, all further exchanges happens through text: ‚Äúignore this comment‚Äù, ‚Äúno, not this comment, the one above it‚Äù.\n\nNow compare that to using any graphical (or even a terminal) interface: we can reply in thread, comment on specific lines, or click a button to dismiss invalid feedback altogether. In other words, when we put our integrations behind MCPs, all exchange happens on text, and we often end-up with worse user experiences.\n\nInstead, our goal is to build agentic tools and user experiences side-by-side. For example, when we added Web Accessibility diagnostics to Tidewave, we exposed additional tools to the agent, but we also provided a polished experience for when developers want to remain in the loop:\n\nOur recently announced Supabase integration works the same: it runs performance and security advisors on your database and present them to you. If you want to dig deeper, you can do it all through a rich interface. And if you don‚Äôt care about any of that, you can either click the ‚ÄúFix all‚Äù button or ask your agent to do it for you.\n\nTo build these experiences, we had to invert the control. We don‚Äôt want the agent to call us, we want our tools to call the agent.\n\nSay you want to build your own agentic loop, up until recently, doing so meant to:\n\nWhile the above is already a lot of work, you must remember that prompt engineering, tool calls, etc. must be fine-tuned per model. For example, when OpenAI announced GPT-5-Codex, integrators had to write new prompts and new tools, even if they already supported GPT-5.\n\nAnd the above covers only the basic loop! What about AGENTS.md, MCPs, or skills? And you haven‚Äôt even started working on what makes your coding agent unique.\n\nLuckily for us, many of the leading AI companies have built their own coding agents (Claude Code, Codex, Gemini CLI) tailored to their models, and many have exposed those agents through SDKs. We also have fantastic open source alternatives. This means you can cut out much of the work above, but, if you want to support multiple providers, you‚Äôre still on the hook for integrating them one by one.\n\nThat‚Äôs exactly where the Agent Client Protocol from the Zed team comes in. It standardizes communication with coding agents, so you implement one protocol and support multiple agents at once. This ultimately allows you to focus on the runtime integrations and user experiences that make your coding agent unique.\n\nFurthermore, even if you‚Äôre not implementing your own agent, ACP should matter to you because it brings portability to developers. You can use a single coding agent across the terminal, your editor, and Tidewave, reusing the same settings for hooks, skills, and subagents. And perhaps more importantly, the same subscription.\n\nWe are big fans of the Agent Client Protocol and we are excited to build on top of it alongside companies like Zed and JetBrains.\n\nGeneric agents that work everywhere force awkward workflows where we constantly act as a middleman. Instead, we want to embed agents into the platforms we are building on, so they can see the relationship between code and behavior, interact with live apps, and debug in real-time.\n\nHowever, to build these rich experiences, we need to invert the control and own the agentic loop. The Agent Client Protocol (ACP) allows us to do so by standardizing communication across providers, so we can focus on the user experiences and integrations that make our coding agents unique, instead of wrestling with multiple SDKs.\n\nI hope this article inspires you to fine-tune your own coding agents and, if you are looking for a coding agent tailored to full-stack web development, give Tidewave a try!",
    "readingTime": 9,
    "keywords": [
      "client protocol",
      "claude code",
      "paste stacktraces",
      "agent client",
      "full-stack web",
      "error page",
      "user experiences",
      "agentic loop",
      "web development",
      "embed agents"
    ],
    "qualityScore": 1,
    "link": "https://tidewave.ai/blog/the-future-of-coding-agents-is-vertical-integration",
    "thumbnail_url": "https://tidewave.ai/assets/opengraph/pthe-future-of-coding-agents-is-vertical-integration-4pklmn64vmtcox2ktoauj7nxoy-a84d118a102998c2aa4a25c686d9af29.png?vsn=d",
    "created_at": "2026-02-11T01:19:44.413Z",
    "topic": "tech"
  },
  {
    "slug": "this-ceo-wants-ai-agents-to-outnumber-his-human-employees-this-year",
    "title": "This CEO wants AI agents to outnumber his human employees this year",
    "description": "The CEO of StackBlitz, Eric Simons, says the web development startup has \"gone all in\" on AI agents.",
    "fullText": "StackBlitz CEO Eric Simons aims to have more AI agents than human employees working at the startup this year, a milestone he sees as emblematic of a broader shift underway across the software industry.\n\nSpeaking in an interview with Business Insider, Simons said StackBlitz has \"gone all in on agents,\" deploying internally built AI systems across business intelligence, coding, product development, customer support, and outbound sales.\n\nAI has become increasingly good at writing software code, to the point where the technology is beginning to change how companies are run. OpenClaw, an open-source AI assistant that works inside platforms like WhatsApp, Slack, and iMessage, is an early example of digital agents communicating and coordinating with one another without direct human involvement.\n\n\"To me, this is a crystal ball into the wildness of the inevitable future,\" Simons told Business Insider. \"Your AI agents will talk to other people's agents on your behalf, negotiating pricing for a product you want to buy, inquiring about availability for a restaurant, arguing your political viewpoints.\"\n\n\"Agents are an extension of yourself,\" he added. \"People will generally trust their agents with whatever they recommend them to buy, reserve, believe, or otherwise.\"\n\nSimons's comments come as software and SaaS stocks have slumped in recent weeks, a move he attributes to growing investor recognition that AI can now build software autonomously. As AI tools become more capable, he said, long-standing competitive moats based on specialized knowledge are eroding.\n\nHe compared the shift to the transformation of manufacturing over the past century. Craft knowledge once protected businesses, he said, but automation and digital design ultimately displaced many incumbents. \"Today, it's a CAD file you can 3D print,\" Simons said.\n\n\"While every individual does not use these profound new powers directly (like 3D printing a chair), there's an entire new generation of companies that do ‚Äî and they have replaced the previous era of companies,\" Simons explained. \"They disrupted those incumbents by leveraging automation that made it far cheaper and at a massive scale not possible previously when moats were just knowledge and bare hands.\"\n\nFor businesses, the implications are stark. Even traditionally \"safe\" enterprise software may be vulnerable if AI agents can migrate or rebuild systems rapidly.\n\n\"What does it mean when all of software can be written, rewritten, migrated, or otherwise, 100x or 10,000x faster than it could've ever been done before, by a workforce that does not sleep, and can be parallelized near infinitely?\" Simons said. \"It's daunting and mind-bending, and I think the repricing of SaaS in public markets more accurately reflects this today.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "agents",
      "software",
      "knowledge",
      "human",
      "shift",
      "across",
      "systems",
      "product",
      "digital"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/tech-ceo-ai-agents-outnumber-human-employees-eric-simons-stackblitz-2026-2",
    "thumbnail_url": "https://i.insider.com/698b6915a645d11881890f3a?width=1200&format=jpeg",
    "created_at": "2026-02-11T01:19:42.667Z",
    "topic": "finance"
  },
  {
    "slug": "composer-15",
    "title": "Composer 1.5",
    "description": "Improved reasoning over challenging coding tasks by scaling RL over 20x.",
    "fullText": "A few months ago, we released our first agentic coding model, Composer 1. Since then, we've made significant improvements to the model‚Äôs coding ability.\n\nOur new release, Composer 1.5, strikes a strong balance between speed and intelligence for daily use. Composer 1.5 was built by scaling reinforcement learning 20x further on the same pretrained model. The compute used in our post-training of Composer 1.5 even surpasses the amount used to pretrain the base model.\n\nWe see continued improvements on coding ability as we scale. Measured by our internal benchmark of real-world coding problems, we find that the model quickly surpasses Composer 1 and continues to climb in performance. The improvements are most significant on challenging tasks.\n\nComposer 1.5 is a thinking model. In the process of responding to queries, the model generates thinking tokens to reason about the user‚Äôs codebase and plan next steps. We find that these thinking stages are critical to the model‚Äôs intelligence. At the same time, we wanted to keep Composer 1.5 fast and interactive for day-to-day use. To achieve a balance, the model is trained to respond quickly on easy problems with minimal thinking, while on hard problems it will think until it has found a satisfying answer.\n\nTo handle longer running tasks, Composer 1.5 has the ability to self-summarize. This allows the model to continue exploring for a solution even when it runs out of available context. We train self-summarization into Composer 1.5 as part of RL by asking it to produce a useful summary when context runs out in training. This may trigger several times recursively on hard examples. We find that self-summarization allows the model to maintain its original accuracy as context length varies.\n\nComposer 1.5 is a significantly stronger model than Composer 1 and we recommend it for interactive use. Its training demonstrates that RL for coding can be continually scaled with predictable intelligence improvements.\n\nLearn more about Composer 1.5 pricing here.",
    "readingTime": 2,
    "keywords": [
      "tasks composer",
      "coding ability",
      "model",
      "improvements",
      "intelligence",
      "context",
      "model‚Äôs",
      "balance",
      "surpasses",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/composer-1-5",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/composer-1.5-og.png",
    "created_at": "2026-02-10T06:54:10.475Z",
    "topic": "tech"
  },
  {
    "slug": "a16z-partner-says-that-the-theory-that-well-vibe-code-everything-is-flat-wrong",
    "title": "A16z partner says that the theory that we'll vibe code everything is 'flat wrong'",
    "description": "Vibe coding everything is just not worth it, says A16z partner Anish Acharya.",
    "fullText": "Vibe coding everything is just not worth it, says an Andressen Horowitz partner.\n\nOn an episode of the \"20VC\" podcast released on Monday, A16z general partner Anish Acharya said that companies shouldn't use AI-assisted coding for every part of their business.\n\nHe said that software accounts for 8% to 12% of a company's expenses, so using vibe coding to build the company's resource planning or payroll tools would only save about 10%. Relying on AI to write code also carries risks, he said.\n\n\"You have this innovation bazooka with these models. Why would you point it at rebuilding payroll or ERP or CRM,\" Acharya said, referring to enterprise resource planning and customer relationship management software. Salesforce, Microsoft, Oracle, and SAP are among the top providers of such software.\n\nInstead, companies are better off using AI to develop their core businesses or optimize the remaining 90% of their costs, said the venture capitalist of six years.\n\n\"Of course, there will be secular losers. There are specific business models that are now going to be disadvantaged,\" he said. \"But the general story that we're going to vibe code everything is flat wrong, and the whole market is oversold software.\"\n\nAcharya's comments follow a brutal week for software stocks, which dragged down tech and broader markets. The sell-off started when already-wary investors panicked about Anthropic's new AI tool, which can perform a range of clerical tasks for people working in the legal industry.\n\nThe A16z partner joins famed investor Vinod Khosla in saying that stock prices should be ignored when evaluating the future of tech companies.\n\nOn a podcast last month, Khosla dismissed talks of an AI bubble and said investors should not be concerned as long as API call volume, a benchmark of AI usage, remains high.\n\n\"If that's your fundamental metric of what's the real use of your AI, usefulness of AI, demand for AI, you're not going to see a bubble in API calls,\" he said. \"What Wall Street tends to do with it, I don't really care. I think it's mostly irrelevant.\"",
    "readingTime": 2,
    "keywords": [
      "resource planning",
      "vibe coding",
      "software",
      "partner",
      "everything",
      "podcast",
      "business",
      "company's",
      "payroll",
      "code"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/a16z-partner-says-businesses-cannot-vibe-code-everything-tech-stocks-2026-2",
    "thumbnail_url": "https://i.insider.com/698a9944a645d118818905c1?width=1200&format=jpeg",
    "created_at": "2026-02-10T06:54:04.849Z",
    "topic": "finance"
  },
  {
    "slug": "i-made-a-claude-code-guide-thats-a-win95-desktop-with-games",
    "title": "I made a Claude Code guide that's a Win95 desktop with games",
    "description": "A free interactive guide for PMs and CEOs leading dev teams through the AI coding transition. Score your codebase readiness, calculate ROI, and get a 90-day rollout plan.",
    "fullText": "Your developers have Copilot open. They autocomplete functions and paste errors into chat. That gets you maybe 10% of what AI coding tools can do now. The real shift is agentic: one engineer running 3-5 parallel AI sessions that write features, run tests, and open PRs.\n\nSeven chapters. Three interactive tools. One honest answer about your team's speed:\n\nMost teams think they're running Pentium. They're a 486.\n\nTechnical deep-dives are marked üìé so you know what to forward to your engineering lead.",
    "readingTime": 1,
    "keywords": [
      "tools",
      "they're"
    ],
    "qualityScore": 0.55,
    "link": "https://gabezen.com/guide/",
    "thumbnail_url": "https://gabezen.com/guide/og.png",
    "created_at": "2026-02-10T01:21:45.458Z",
    "topic": "tech"
  },
  {
    "slug": "clog-track-and-compare-your-claude-code-usage",
    "title": "Clog ‚Äì Track and compare your Claude Code usage",
    "description": "Track your Claude Code journey. Public leaderboard and profile system for developers. See how your coding sessions stack up, build streaks, and share your progress.",
    "fullText": "[join][--]clog - Claude Code Leaderboard// track your claude code sessions// leaderboard goes live soon_ready",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "leaderboard"
    ],
    "qualityScore": 0,
    "link": "https://clog.sh",
    "thumbnail_url": "https://clog.sh/clog-og-image.png",
    "created_at": "2026-02-10T01:21:44.351Z",
    "topic": "tech"
  },
  {
    "slug": "factory-factory-opensource-alternative-to-codex-app-for-claude",
    "title": "Factory Factory, open-source alternative to Codex App for Claude",
    "description": "Workspace-based coding environment for running multiple Claude Code sessions in parallel. - purplefish-ai/factory-factory",
    "fullText": "purplefish-ai\n\n /\n\n factory-factory\n\n Public\n\n Workspace-based coding environment for running multiple Claude Code sessions in parallel.\n\n factoryfactory.ai\n\n License\n\n MIT license\n\n 18\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n purplefish-ai/factory-factory",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/purplefish-ai/factory-factory",
    "thumbnail_url": "https://opengraph.githubassets.com/72f2d9d9898e13bed120d015134a857e14f77267ee4cd373e64559450fa53038/purplefish-ai/factory-factory",
    "created_at": "2026-02-09T12:42:23.472Z",
    "topic": "tech"
  },
  {
    "slug": "logifai-autocapture-dev-logs-for-ai-coding-assistants",
    "title": "Logifai ‚Äì Auto-capture dev logs for AI coding assistants",
    "description": "Auto-capture development logs for Claude Code ‚Äî stop copy-pasting terminal output. - tomoyaf/logifai",
    "fullText": "tomoyaf\n\n /\n\n logifai\n\n Public\n\n Auto-capture development logs for Claude Code ‚Äî stop copy-pasting terminal output.\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n tomoyaf/logifai",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/tomoyaf/logifai",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1152746807/546b13ec-53a4-4596-a481-7a6c1210694a",
    "created_at": "2026-02-09T06:54:27.333Z",
    "topic": "tech"
  },
  {
    "slug": "i-see-how-important-ai-is-at-google-so-i-taught-my-kids-about-it-now-theyre-vibe-coding",
    "title": "I see how important AI is at Google, so I taught my kids about it. Now, they're vibe coding.",
    "description": "A Google executive explains why he taught his kids to understand AI ‚Äî and how they ended up vibe coding and participating in a hackathon.",
    "fullText": "This as-told-to essay is based on a conversation with Asif Saleem, a financial services go-to-market lead for Japan and the Asia Pacific region at Google. Asif is the father of 13-year-old Usman Asif and 18-year-old Shanzey Asif. It's been edited for length and clarity.\n\nIt's important for parents to help kids understand we're going through a very transformative time. This is like the era when the internet or mobile first emerged. With AI, it may be even bigger.\n\nKids need to understand this and embrace the technology. Whatever they end up studying ‚Äî computer science, English, or philosophy ‚Äî they can make AI part of it.\n\nI was curious about Cursor and some of the other vibecoding tools, so I joined a few \"Code with AI\" weekend sessions for executives.\n\nI really enjoyed it. Within a matter of hours, we were able to develop different applications. I developed a statement analyzer for a financial services system.\n\nOnce I came back home, I spoke to my family about it and showed them the demo. My children, Usman and Shanzey, are both tech-savvy, so that drove their interest. They attended the same course a few weeks later.\n\nThey were the youngest in the class. The good part was that they were completely independent, so I let them be on their own. That's how they ventured into vibe coding and participated in Cursor's 24-hour weekend hackathon in Singapore.\n\nThey've become more curious about things. Through vibe coding, they've learned to be more creative and to use technology to understand how things work.\n\nNeither of them has a formal technology background ‚Äî they're not software developers ‚Äî but they've been able to think through ideas, be more creative, and use technology to solve problems.\n\nTechnology is the biggest enabler. The question is how humans use it ‚Äî whether for good or bad. If it's for good, what are the guardrails?\n\nUsing it for good means creating value, solving real challenges, and making information more accessible.\n\nManaging screentime is the reason we developed a reward system for Usman because he's young and he loves gaming. Gaming comes as a reward for achieving goals. Those goals include making your own breakfast, making your bed, or helping clean the house.\n\nWhen Shanzey is studying, she can't use AI for the content she's creating ‚Äî it can't even be AI-inspired. It has to be her original work.\n\nThat's super important because schools validate whether output is AI-generated. The same applies to Usman.\n\nIt's important to have both physical and digital skills. Usman plays football because physical activity is important at his age. Shanzey is focused on school right now ‚Äî her exams are very important, and getting good grades matters.\n\nI'd say my role is more like a coach ‚Äî brainstorming with them. My wife does the same, helping them think through ideas and keeping them honest.\n\nI'm often busy with work, so a lot of this couldn't be done without my wife's help.\n\nThere's nothing better than hands-on learning. That's something I've learned over the last few years.\n\nYou will face challenges and go through them. Parents should get their kids to do things, give them challenges, and nurture them as they work through those challenges. With hands-on experience, they can get better results faster.\n\nWith building an app, we co-create the idea, but then the important part is making it happen and reporting back on progress.\n\nAt Google, I work with enterprise customers trying to transform their businesses with AI. I can see what's happening on the ground and how things are changing.\n\nI also see a lot of young talent at Google. They come in thinking about creating apps and learning skills, and I mentor some of them as well.\n\nIt's important to communicate this to my family. I spent time helping them understand how the world is changing and why it's important for them to understand AI.\n\nIt's not about running away from technology. AI will keep advancing, and the only thing you can do is be accustomed to it, no matter what you want to become.\n\nWe're now in a situation where we have very intelligent large language models, and we're also moving toward agentic AI, where you can work with agents that help you do a lot more. Speed and agility are improving, and you can now work within larger ecosystems that combine humans and machines, achieving much more.\n\nIf you know how to coexist and let machines do meaningful work with you, that's a skill to aspire to.\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "financial services",
      "vibe coding",
      "technology",
      "understand",
      "that's",
      "challenges",
      "kids",
      "we're",
      "creating",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-ai-teach-teenagers-vibe-coding-kids-family-tech-2026-2",
    "thumbnail_url": "https://i.insider.com/6954c24104eda4732f2e4c0d?width=1200&format=jpeg",
    "created_at": "2026-02-09T06:54:22.473Z",
    "topic": "finance"
  },
  {
    "slug": "the-guy-who-coined-vibecoding-says-the-next-big-thing-is-agentic-engineering",
    "title": "The guy who coined 'vibe-coding' says the next big thing is 'agentic engineering'",
    "description": "OpenAI cofounder Andrej Karpathy says 'agentic engineering is the next evolution in AI coding as vibe-coding marks its first anniversary.",
    "fullText": "\"Vibe-coding\" just celebrated its first birthday. That's a lifetime in the AI boom.\n\nNow, the man who coined the term is celebrating the birth of a new one: \"agentic engineering.\"\n\nWhile vibe-coding is when humans prompt AI to write code, OpenAI cofounder Andrej Karpathy says agentic engineering is when AI agents write the code themselves.\n\n\"Many people have tried to come up with a better name for this to differentiate it from vibe coding, personally, my current favorite is 'agentic engineering,'\" he wrote in a recent X post.\n\nKarpathy said he calls it \"agentic engineering\" not just because agents are writing the code, but because \"there is an art & science and expertise to it.\"\n\nVibe-coding is one of the biggest innovations of the AI revolution. Prominent CEOs and startup founders alike are encouraging the use of vibe-coding across their teams. And billions are being poured into new vibe-coding companies.\n\nLovable, one of Europe's fastest-growing startups, announced that it had raised $330 million in Series B funding at a $6.6 billion valuation in December. Cursor, an AI-assisted code editor, announced a Series D funding round of $2.3 billion in November and said it had surpassed $1 billion in annualized revenue.\n\nThe approach is also threatening traditional engineering jobs. In a Business Insider survey of 167 software engineers, 75 engineers said that they were \"keeping up,\" while 30 said they felt ahead of the curve, and 27 felt behind.\n\n\"Vibe coding is now mentioned on my Wikipedia as a major memetic 'contribution,' and even its article is longer. lol,\" Karpathy wrote on X about its meteoric rise.\n\nKarpathy was a founding member of OpenAI in 2015, years before competitors like Anthropic and xAI emerged. He later moved into self-driving technology, leading Tesla's Autopilot program as head of AI. He's now building Eureka Labs, which describes itself on its website as building a \"new kind of school that is AI native.\"",
    "readingTime": 2,
    "keywords": [
      "series funding",
      "vibe coding",
      "agentic engineering",
      "vibe-coding",
      "code",
      "openai",
      "agents",
      "engineers",
      "karpathy"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/agentic-engineering-andrej-karpathy-vibe-coding-2026-2",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-02-09T01:12:58.161Z",
    "topic": "finance"
  },
  {
    "slug": "gitbutler",
    "title": "GitButler",
    "description": "GitButler software development platform",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://gitbutler.com/",
    "thumbnail_url": "https://gitbutler.com/og-image.png",
    "created_at": "2026-02-08T18:22:32.834Z",
    "topic": "tech"
  },
  {
    "slug": "prepare-your-oss-repo-for-ai-coding-assistants",
    "title": "Prepare your OSS repo for AI coding assistants",
    "description": "I‚Äôve been seeing more and more open source maintainers throwing [...]",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://angiejones.tech/stop-closing-the-door-fix-the-house/",
    "thumbnail_url": "https://angiejones.tech/wp-content/uploads/2026/02/open-source-is-closed.png",
    "created_at": "2026-02-08T18:22:32.063Z",
    "topic": "tech"
  },
  {
    "slug": "an-identity-verification-ai-startup-is-having-all-employees-try-vibe-coding",
    "title": "An identity verification AI startup is having all employees try vibe coding",
    "description": "A San Francisco AI startup is giving out stipends and dedicating project days for non-tech staff to vibe code apps that help with their workflow.",
    "fullText": "If you work for a San Francisco startup and don't know how to code, you could soon be asked to get creative with vibe coding.\n\nCheckr, an AI-powered background check company, gave Business Insider a glimpse of how employees are actually using AI.\n\nCheckr CEO Daniel Yanisse said that the company is going \"all in\" and trying everything to encourage its employees to fully embrace AI ‚Äî including staff that don't work in engineering roles.\n\n\"We really pride ourselves on using AI to the maximum possible amount,\" said Yanisse. \"We gave every employee a monthly stipend to try AI tools, and we did AI days and demos. After one year, 95% of the employees use prompting daily.\"\n\n\"This year, we're going to level up and move to building with AI, as in vibe coding,\" Yanisse added. \"I'm working with all of our teams now, and we're going to do our AI days soon in March, where we're going to make every non-technical person vibe code their own business apps.\"\n\nYanisse said that many employees who have no idea how to code, who work in finance, legal, and HR, are already vibecoding apps to automate their workflows and problem-solve, such as building tools that help clean up large spreadsheets.\n\nWhile Checkr is evaluating a variety of builder tools like Lovable, Replit, and Claude Code, Yanisse said Cursor is a clear standout and \"has amazing adoption\" among both engineers and non-technical staff, but Lovable is the best place to start for people with no coding experience.\n\n\"Probably, we're going to buy all of them and just use the right tool for the right person,\" Yanisse said of different AI coding tools.\n\n\"We have AI solution engineers who are available to actually partner and help, so they would come and help you and unstuck you if you have a problem, and take you all the way to success,\" Yanisse added. \"And then you're on your way because then we share success stories with everyone in the company.\"\n\nIn practice, data shows that AI adoption can be complicated in a large enterprise. Competence with AI tools can be very uneven across the board, and security risks can mount without clear guidelines on AI usage.\n\nAccording to a survey published in November by Moveworks, an AI-powered platform that automates IT and HR support, most executives said that non-technical employees are playing a bigger role in driving AI use, and that 78% have seen successful AI projects originate directly from support staff looking to solve daily challenges.\n\nThe National Cybersecurity Alliance also wrote in its Annual Cybersecurity Attitudes and Behaviors Report that¬†AI adoption has surged to 65% globally as of the end of 2025, but more than half of these AI users never received any training in privacy and security risks. The report surveyed over 6,500 workers worldwide.\n\n\"A few years ago, most businesses were still debating whether AI was something they really needed,\" Louis Riat-Bonello of Optisearch, an AI-powered marketing platform that specializes in SEO, told Business Insider.\n\n\"The businesses getting the best results aren't blindly chasing automation. They're using AI to support smarter decisions, move faster, and free up teams to focus on strategy and creativity,\" Riat-Bonello added. \"That balance is what will matter long after the hype fades.\"\n\nYanisse said that in the age of AI, the company is looking for creative and adaptable people, because while AI will eliminate some roles, it will create others.\n\n\"We are constantly training and helping people update their skills and careers,\" said Yanisse. \"The job of the product designer and the job of the marketer are all completely shifting right now.\"\n\n\"We're over 900 people, so we're not a small startup, but I'm a startup guy, and I'm a builder,\" Yanisse added. \"The people who come here need to be OK with uncertainty, be self-driven, adaptable, flexible, willing to do new things, and solve new problems without too much guidance or structure.\"",
    "readingTime": 4,
    "keywords": [
      "security risks",
      "vibe coding",
      "we're",
      "employees",
      "tools",
      "code",
      "startup",
      "ai-powered",
      "yanisse",
      "staff"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/checkr-non-technical-employees-vibecode-with-stipends-and-ai-days-2026-2",
    "thumbnail_url": "https://i.insider.com/6983e6bda645d1188188b87e?width=1200&format=jpeg",
    "created_at": "2026-02-07T12:25:49.164Z",
    "topic": "finance"
  },
  {
    "slug": "what-i-wish-i-knew-before-building-a-vibe-coding-platform",
    "title": "What I wish I knew before building a vibe coding platform",
    "description": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.",
    "fullText": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.\n\nI'm Ariel, VP of AI at Appwrite. Over the past year, my team and I built Imagine ‚Äî a vibe-coding platform that lets users build production-ready web apps using prompting. That means real backends, server-side rendering, server functions, sandboxes, previews, the whole thing.\n\nAppwrite is one of the most impactful open-source projects in the world, and in the spirit of open-source, we want to share our learnings with the community. This article isn't a how-to. It's a collection of hard-earned lessons ‚Äî the kind you only learn after shipping, breaking things, and paying real infrastructure bills.\n\nIf you don't take prompt caching seriously from day one, you will burn money and time. This is especially true for vibe-coding platforms.\n\nVibe coding platforms naturally rely on long-running agentic loops. Agents talk to themselves, call tools, revise plans, generate files, run tests ‚Äî sometimes over dozens of steps and millions of tokens. That's unavoidable if you want high-quality results.\n\nThe good news: with correct prompt caching, this doesn't have to be expensive or slow.\n\nHow you structure and order your prompts has a massive impact on cache hit rates. In practice, almost everything should be cacheable.\n\nWe're huge fans of Anthropic, and they allow you to define explicit cache breakpoints. For example:\n\nFirst breakpoint: after fully static system messages. Core instructions, rules, policies ‚Äî things that never change.\n\nSecond breakpoint: after semi-dynamic system messages. Generation number, project metadata, decisions made by earlier agents.\n\nFinal breakpoint: right before the latest user or agent message.\n\nThis structure allows long agentic loops to reuse the majority of their context even when the workflow branches or retries. If you place breakpoints carelessly, a single conditional change can invalidate your entire cache.\n\nThis sounds unrealistic at first, but it's absolutely achievable.\n\nA good benchmark is 90‚Äì95% cache hit rate on input tokens. At that point, the economics completely change:\n\nThis is the difference between a platform that feels sluggish and expensive, and one that feels snappy and scalable.\n\nMake sure you know how to observe caching behavior. Anthropic does an excellent job here ‚Äî their console shows per-request caching status and aggregated cache metrics over time. Anthropic's documentation for prompt caching is state-of-the-art and everyone should read it.\n\nPro tip: You might be tempted to use cheaper models for certain tasks in your workflow, thinking it would be faster or cheaper. With Anthropic for example, cache is per model, so plugging that Sonnet call at the end of your mostly-Opus workflow would result in a cache miss. Do caching right and you can leverage the best models for the job without breaking the bank.\n\nThe average request we make to Anthropic is 99% cached. Screenshot taken from Anthropic's console.\n\nMost frameworks and SDKs teach you the same thing:\n\nSend a request, get a response, stream tokens back to the client.\n\nThat's fine for demos. It completely breaks down for vibe coding.\n\nIn a vibe coding platform where state is fragile (sandbox, repo, uncommitted changes, ongoing generation, etc.), so much could go wrong.\n\nTo deal with these issues, we have implemented resumable streams and durable workflows.\n\nAssume you have a /api/chat endpoint where the client submits a prompt. Typically, tutorials would teach you to stream the chunks back to the client (for example, using Server-Sent events, or SSE in short).\n\nThat's simply putting too much hope in HTTP connections in a scenario where generations can take minutes to complete. Here is a good approach to dealing with this:\n\nIn our case, we serve the stream via a separate service altogether to reduce load on our core AI API, but that's not strictly necessary. Redis becomes the source of truth for in-progress generations.\n\nStreaming alone isn't enough. You also need durable execution for the workflow itself.\n\nInstead of one long-running function, break the process into orchestrated steps:\n\nSandbox provisioning is a good example. In our platform, there are multiple ways a sandbox might be started:\n\nWith durable workflows, all of these can safely funnel into the same sandbox orchestration logic without race conditions or duplication.\n\nWe chose to use Inngest which is open-source, but there are other good options like Temporal, AWS Lambda Durable Functions, Trigger.dev, etc.\n\nAs a bonus point, using Inngest gives us great observability, overview of execution times and alerts on anomalies. It forces us to adopt a more resilient and reliable approach to building our platform, and everybody gets to sleep better at night.\n\nInngest makes it easy to visualize the timeline of each generation and its steps.\n\nWhen building a vibe-coding platform, you have a fundamental choice:\n\nDo I let the system generate anything, or do I force generations to use a specific framework or stack?\n\nWe decided to go with the second approach.\n\nWhen evaluating frameworks, we asked ourselves these questions:\n\nAfter evaluating a few common options, we ultimately chose TanStack Start. It's built by the team behind some of the most popular tools and libraries powering the React ecosystem. TanStack Start uses Vite as its build system, which is highly customizable.\n\nFor our generated apps' runtime, we chose Bun. It's incredibly fast, supports running TypeScript directly (handy for migrations), and is pretty much a drop-in replacement for Node.js.\n\nConcretely, we are able to build our generated apps (server and client code) in ~1 second. For comparison, a fresh Next.js + Turbopack + Bun build takes a few seconds, and that's without any server code. The difference is huge.\n\nImagine's generated apps build in ~1 second, including both client and server code\n\nGenerative AI is inherently non-deterministic. You can prompt all you want, provide examples, two-shot it, five-shot it. But as your message history grows, your context grows as well, and so does the likelihood of facing hallucinations or simply failing to get the model to adhere to your instructions.\n\nWhen users report issues, 90% of those have to do with unexpected AI behavior or issues in the generated code. It's very tempting to take every such issue in isolation and try to fix it by adding an additional bullet point to the system prompt, or providing more examples.\n\nBut wait. There might be a better way. The deterministic way. Here are some examples from Imagine:\n\nThese are just a few examples, but the key takeaway is that you can't rely on the LLM to adhere to your instructions. Whenever you can, embrace determinism.\n\nLet your AI see the same red squiggly lines you appreciate so much.\n\nThe real challenges only show up once you're dealing with real workloads, real users, and real infrastructure bills. By then, it's often too late to \"just refactor\" your way out of architectural decisions made early on.\n\nPrompt caching, durable workflows, deterministic guardrails, and a well-chosen framework should not be an afterthought, or by-the-way optimizations. They are foundational.\n\nAt Imagine, our mission is to tame AI, make the best of it and mitigate its weaknesses. We build Imagine assuming things will fail, disconnect, retry, and resume ‚Äî unexpectedly.\n\nAs we build Imagine, we are constantly learning, and we are excited to share our learnings with the community. If you're building something similar, we hope these hard-earned lessons help you move quickly and avoid the pitfalls.",
    "readingTime": 7,
    "keywords": [
      "infrastructure bills",
      "agentic loops",
      "hard-earned lessons",
      "vibe coding",
      "system messages",
      "durable workflows",
      "generated apps",
      "cache hit",
      "server code",
      "vibe-coding platform"
    ],
    "qualityScore": 1,
    "link": "https://imagine.dev/blog/post/what-i-wish-i-knew",
    "thumbnail_url": "/images/blog/what-i-wish-i-knew/cover.png",
    "created_at": "2026-02-06T18:34:47.896Z",
    "topic": "tech"
  },
  {
    "slug": "tech-stack-is-a-business-decision",
    "title": "Tech stack is a business decision",
    "description": "Why tech stack choices should be driven by business context and constraints‚Äînot framework preferences‚Äîand why this matters even more with agentic coding tools.",
    "fullText": "Developers love to argue about tech stacks.\n\nFlutter vs React Native.\n\nNative vs cross-platform.\n\nJava vs Ruby vs C#.\n\nReact vs Angular.\n\nRiverpod vs Bloc.\n\nDrift vs Hive.\n\nThese debates can go arbitrarily deep, down to library-level choices that have little impact on whether a product succeeds or fails.\n\nEveryone has preferences, and that‚Äôs fine. Preferences come from experience. Someone who started with strongly typed languages will value compiler guarantees and explicitness. Someone who started with dynamic languages will value flexibility and speed. Both viewpoints are internally consistent.\n\nWhat they are not is universally correct.\n\nThere is no hard data that can rank one stack as objectively ‚Äúbetter‚Äù across all contexts. Teams, constraints, markets, and goals differ too much. Most arguments rely on anecdotes, and anecdotes do not generalize.\n\nSo if preference is not a reliable guide, how should a tech stack be chosen?\n\nThis question is often interpreted at the feature or use-case level. That misses the point.\n\nIn almost all non-hobby software, the problem being solved is a business problem.\n\nIf there is no business, no users, no revenue, then the software is irrelevant. It may be elegant or technically impressive, but it has no economic value. Software only matters insofar as it supports a business outcome.\n\nThat framing matters because it establishes the correct hierarchy.\n\nTechnology is not the goal.\n\nTechnology is a tool.\n\nAnd tools are chosen based on the job they need to do.\n\nEarly on, nobody knows whether the business will work.\n\nYou don‚Äôt know if users care. You don‚Äôt know if they will pay. You don‚Äôt know if the problem is real or imagined.\n\nAt this stage, obsessing over the ‚Äúright‚Äù stack is misplaced effort. The dominant constraint is uncertainty, not scalability or architectural purity.\n\n‚ÄúThe FIRST rule of enterprenuership is you use what you have‚Äù - Alex Hormozi\n\nThis is simple rule of entrepreneurship applies surprisingly well to engineering.\n\nFounders and early teams start with what they already know. Not because it is theoretically optimal, but because it minimizes friction. Familiar tools reduce cognitive overhead, increase speed, and allow faster learning.\n\nEarly success is rarely determined by technical elegance. It is determined by whether you can ship something people want before you run out of time or money.\n\nAs the business grows, the constraints change.\n\nRevenue appears.\n\nUsers depend on the system.\n\nThe team grows.\n\nThe cost of failure increases.\n\nNow the stack begins to matter more. Not because some technologies are inherently superior, but because the software must support new business realities. Hiring, onboarding, operational cost, maintainability, performance characteristics, and risk management all become relevant.\n\nThis is where teams often conclude that an early stack choice was ‚Äúwrong.‚Äù In most cases, it wasn‚Äôt wrong. It was right for the phase the business was in. The mistake was assuming it would remain right indefinitely.\n\nTwitch is often mentioned as a scaling story, but the more interesting part is how long the architecture stayed simple.\n\nAt one point, Twitch was serving millions of users with a monolithic Ruby on Rails application and a single PostgreSQL database. That setup was not elegant or future proof, but it worked. It allowed the team to move quickly, ship features, and grow the business without adding unnecessary complexity.\n\nOver time, the constraints changed. Growth amplified failures. Database contention increased. Reliability, support load, and churn started to matter more than raw development speed. The cost of incidents became visible in a way it had not been before.\n\nThat was when the architecture began to evolve. The database was scaled and restructured, services were split out, and new infrastructure and languages were introduced. None of this happened because the original stack was flawed. It happened because the business had outgrown it.\n\nThe technology changed in response to the business, not the other way around.\n\nIf you want to read more about how Twitch migrated from a monolith to microservices, see Breaking the Monolith at Twitch and Breaking the Monolith at Twitch: Part Two.\n\nIf you want to know more about how Twitch scaled its database, see How Twitch Uses PostgreSQL.\n\nAgentic coding tools make this even more important.\n\nTools like Claude Code and Cursor reduce the advantage of deep, framework-specific familiarity. Generating boilerplate, navigating unfamiliar code, applying patterns across modules, and keeping implementation consistent are no longer manual, error-prone tasks.\n\nWhen an agent can help you stay productive across technologies, the cost of switching or extending a stack drops. What remains expensive is not syntax or APIs, but domain knowledge, product decisions, and the long-term behavior of the system.\n\nIn other words, the more the tooling levels the technical playing field, the more tech stack decisions become about business fit.\n\nInstead of asking whether one technology is better than another in the abstract, ask questions that reflect the reality of the business:\n\nWhat are we optimizing for right now? Speed, cost, reliability, hiring, or time-to-market?\n\nHow quickly do we need to validate this idea?\n\nHow expensive will change be later, and what changes are most likely?\n\nWhat kind of team will maintain this system?\n\nWhat risks could realistically kill the business, and does the stack reduce or increase them?\n\nOnce framed this way, stack decisions become clearer and less emotional. Sometimes multiple choices are equally valid. Sometimes the boring option is correct. Sometimes speed matters more than correctness. Sometimes correctness matters more than speed.\n\nThere is no universal answer. The answer depends on the context.\n\nMost tech stack arguments are debates about taste, not outcomes.\n\nOnce technology is treated as a business decision rather than a personal identity, those arguments lose their intensity. The question stops being ‚Äúwhat do I prefer?‚Äù and becomes ‚Äúwhat does the business need right now?‚Äù\n\nThe right stack is not the one that wins debates.\n\nIt is the one that helps the business survive and grow at its current stage.\n\nIf are starting a project and need help with tech stack decision, connect with me on¬†LinkedIn and I‚Äôll take it from there.\n\nSoftware Developer specializing in Flutter, Android, and mobile development.\n Writing about code, architecture, and developer productivity.",
    "readingTime": 6,
    "keywords": [
      "tech stack",
      "stack decisions",
      "business",
      "speed",
      "technology",
      "users",
      "tools",
      "database",
      "debates",
      "languages"
    ],
    "qualityScore": 1,
    "link": "https://dinkomarinac.dev/blog/tech-stack-is-a-business-decision/",
    "thumbnail_url": "https://dinkomarinac.dev/og/tech-stack-is-a-business-decision.png",
    "created_at": "2026-02-06T12:35:39.062Z",
    "topic": "tech"
  },
  {
    "slug": "how-exposed-are-software-stocks-to-ai-tools-we-put-vibecoding-to-the-test",
    "title": "How exposed are software stocks to AI tools? We put vibe-coding to the test",
    "description": "How real is the AI threat to software companies? CNBC put it to the test by vibe-coding a Monday.com replacement.",
    "fullText": "Software, legal services and video games stocks have been selling off in recent weeks on fears that new AI features and tools could wipe them out. But how real is that threat? We decided to find out.\n\nCNBC's Deidre Bosa and I used Anthropic's AI coding tool \"Claude Code\" with the goal of creating a replacement for Monday.com, a project management platform with a $5 billion market cap. We didn't expect to get anywhere ‚Äî we're not developers and we don't have any coding experience. But we have become adept at vibe-coding tools, which are AI tools that can build functioning apps based on commands in plain English from users, including those with limited technical chops.\n\nWe started out simple, telling Claude to build a project management dashboard similar to Monday, with features like multiple project boards, assigned team members and a status dropdown. It spit out a working prototype in minutes.\n\nWe then asked Claude to research Monday on its own, identify main features and recreate them. It added a number of other features, including a calendar.\n\nThe real magic happened when we connected the clone to an email account, essentially spinning up a customized project manager for our personal lives. The AI found Dee's forgotten calendar invite for a kid's birthday party (which she definitely didn't have a gift for yet) and it added reminders to book tickets for an upcoming trip and sign a waiver for another kid's birthday party.\n\nIt took us under an hour, and had we been paying users, it would have cost something like $5 to $15 in compute credits, depending on how much we went back and forth with the AI agent. As more data centers get built out, that cost could start to come down.\n\nSo which companies should investors worry about? Silicon Valley insiders we talk to say the most exposed names are the ones that \"sit on top of the work\" ‚Äî tools like Atlassian, Adobe, HubSpot, Zendesk, and Smartsheet, that aren't core to businesses.\n\nThey say cybersecurity stocks like CrowdStrike and Palo Alto are harder to disrupt since they have network effects that no one would want to try to replicate and maintain.\n\nSystems of record may be safer, but not immune. Salesforce, for instance, anchors a business with enterprise data, making it harder to clone with a weekend coding project.\n\nWith the wholesale sell-off in software this year, that may be a chance for investors to separate between the need-to-haves and nice-to-haves.",
    "readingTime": 3,
    "keywords": [
      "kid's birthday",
      "birthday party",
      "project management",
      "features",
      "tools",
      "coding",
      "software",
      "stocks",
      "didn't",
      "users"
    ],
    "qualityScore": 1,
    "link": "https://www.cnbc.com/2026/02/05/how-exposed-are-software-stocks-to-ai-tools-we-tested-vibe-coding.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108184664-1754999709333-gettyimages-2214520187-img_1248.jpeg?v=1754999753&w=1920&h=1080",
    "created_at": "2026-02-06T06:40:18.191Z",
    "topic": "finance"
  },
  {
    "slug": "termoil-terminal-dashboard-for-managing-parallel-ai-coding-agents",
    "title": "Termoil ‚Äì Terminal dashboard for managing parallel AI coding agents",
    "description": "Less friction for your multi-agent workflow. Contribute to fantom845/termoil development by creating an account on GitHub.",
    "fullText": "fantom845\n\n /\n\n termoil\n\n Public\n\n Less friction for your multi-agent workflow\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n fantom845/termoil",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/fantom845/termoil",
    "thumbnail_url": "https://opengraph.githubassets.com/3f4c60f9807e7fd26c4cd7d8070eb9f12a1c2217f6b59354a3d271fb86812016/fantom845/termoil",
    "created_at": "2026-02-06T06:40:14.221Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-ai-ceo-says-vibe-coding-is-making-software-easier-to-replace",
    "title": "Microsoft AI CEO says vibe coding is making software easier to replace",
    "description": "Microsoft AI CEO says vibe coding is lowering the barrier to building apps, raising questions about how defensible software will be.",
    "fullText": "Mustafa Suleyman says vibe coding is rapidly lowering the barrier to building apps, a shift that could put traditional software at risk.\n\nThe Microsoft AI CEO said in an episode of the \"Exponential View\" podcast published Thursday that AI tools now make it possible for anyone to quickly start launching code and apps.\n\n\"It is so accessible now,\" said Suleyman. \"You can watch a three-minute video, get spun up, launch one of these things.\"\n\n\"You can create an app, a web app in seconds,\" he added.\n\nSuleyman said people don't need deep technical skills to get started. Instead, they can learn by experimenting, watching, and doing.\n\nThe AI can \"build something that you may have thought was never possible,\" he said.\n\n\"Unless you push these things to their edges and explore the boundaries, you won't really understand the magic, or what they're kind of bad at.\"\n\n\"Everyone's got to get stuck into that motion,\" he added.\n\nSuleyman also said he has vibe-coded a system that tracks the DJs he wants to see, coming concerts and festivals, and then matches those events with his travel schedule. What used to be manual work now runs automatically in a spreadsheet that updates throughout the year.\n\nSuleyman's comments come as investors grow increasingly anxious that AI tools and agents could wipe out entire categories of software.\n\nThat fear flared this week after Anthropic said it was adding legal-focused capabilities to its Cowork assistant. The tools would allow AI to review legal documents and track compliance ‚Äî work typically done by legal software.\n\nMarkets didn't take it lightly. Shares of legal-software companies in Europe and the US fell sharply on Tuesday, before the selling spread across the wider software sector and into tech.\n\nOpenAI triggered a similar sell-off months earlier after rolling out internal AI-powered software-as-a-service tools.\n\nMany of the tools now unsettling the tech sector were built using AI coding tools.\n\nAI personal assistant OpenClaw was created with the help of AI, while Moltbook ‚Äî a viral, Reddit-style forum for AI agents ‚Äî was entirely vibe-coded.\n\nAnthropic also said last month that it built its Cowork assistant using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. During a livestream, Rieseberg said his team put Cowork together in just over a week, thanks to Claude.\n\nTech leaders and developers have also said that such a turnaround is becoming the norm.\n\nPeter Steinberger, the developer behind OpenClaw, said in an episode of \"Behind the Craft\" podcast published last week that AI now lets developers \"build everything.\"\n\nOpenAI's chair, Bret Taylor, said in an episode of the \"Big Technology Podcast\" published last week that building software quickly through vibe coding will soon feel routine rather than novel.\n\nBut the real question, Taylor said, is what software still matters.\n\nInstead of dashboards, browser forms, and traditional apps, he expects AI agents to become the dominant software interface.\n\n\"Who's making those agents is the question,\" he said. \"Will you buy those agents off the shelf or build them yourself?\"",
    "readingTime": 3,
    "keywords": [
      "cowork assistant",
      "vibe coding",
      "podcast published",
      "software",
      "tools",
      "agents",
      "apps",
      "episode",
      "traditional",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/microsoft-ai-ceo-vibe-coding-software-replace-apps-mustafa-suleyman-2026-2",
    "thumbnail_url": "https://i.insider.com/6985593bd3c7faef0ecdbefa?width=1200&format=jpeg",
    "created_at": "2026-02-06T06:40:12.752Z",
    "topic": "tech"
  },
  {
    "slug": "a-very-small-sat-solver-from-haskell-now-in-dafny-proved-correct-with-llms",
    "title": "A Very Small SAT Solver (From Haskell) Now in Dafny, Proved Correct with LLMs",
    "description": "Dafny for Metatheory of Programming Languages. Contribute to namin/dafny-sandbox development by creating an account on GitHub.",
    "fullText": "Skip to content\n\n You signed in with another tab or window. Reload to refresh your session.\n You signed out in another tab or window. Reload to refresh your session.\n You switched accounts on another tab or window. Reload to refresh your session.\n\nDismiss alert\n\n namin\n\n /\n\n dafny-sandbox\n\n Public\n\n You can‚Äôt perform that action at this time.",
    "readingTime": 1,
    "keywords": [
      "window reload",
      "another tab",
      "refresh",
      "session",
      "signed"
    ],
    "qualityScore": 0.3,
    "link": "https://github.com/namin/dafny-sandbox/blob/master/Sat.dfy",
    "thumbnail_url": "https://opengraph.githubassets.com/37f757a4876729fbb57a662e8bc8730df5cfc0bc7de82e6b36f6b3d99b39b44a/namin/dafny-sandbox",
    "created_at": "2026-02-06T01:06:52.715Z",
    "topic": "tech"
  },
  {
    "slug": "openais-new-model-leaps-ahead-in-coding-capabilitiesbut-raises-unprecedented-cybersecurity-risks",
    "title": "OpenAI‚Äôs new model leaps ahead in coding capabilities‚Äîbut raises unprecedented cybersecurity risks",
    "description": "Why OpenAI‚Äôs latest coding breakthrough is forcing the company to rethink how‚Äîand how fast‚Äîit can deploy its most powerful models.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/05/openai-gpt-5-3-codex-warns-unprecedented-cybersecurity-risks/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/08/GettyImages-2214110034.jpg?resize=1200,600",
    "created_at": "2026-02-06T01:06:50.989Z",
    "topic": "business"
  },
  {
    "slug": "programming-your-own-modern-bbs-with-python",
    "title": "Programming Your Own Modern BBS with Python",
    "description": "Previously, we looked at dialling BBS with Vice C64 and the C64U. Now, let's code a modern BBS from scratch using Python!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://retrogamecoders.com/programming-bbs-with-python/",
    "thumbnail_url": "https://retrogamecoders.com/wp-content/uploads/2026/02/custom-bbs-server-in-python.jpg",
    "created_at": "2026-02-05T18:36:01.035Z",
    "topic": "tech"
  },
  {
    "slug": "the-coding-agent-os",
    "title": "The Coding Agent OS",
    "description": "We believe that a great coding agent isn't a simple chatbot‚Äîit's an operating system for the model.",
    "fullText": "Most coding agents are presented as a chat UI glued to an LLM and integrated into an IDE or\n CLI. This can work really well for developers who provide the development environment along\n with constant babysitting to build functional software.\n\nCharlie, however, is built around a different premise: We believe that a great coding agent\n isn't a simple chatbot‚Äîit's an operating system for the model.\n\nCharlie's \"OS layer\" gives a model the primitives required to complete end-to-end\n engineering work:\n\nWhen agents run in an operating system, they can diagnose ‚Üí plan ‚Üí implement ‚Üí verify ‚Üí\n publish reviewable artifacts all within the systems where your team already works.\n\nWe've got two hot takes on why the OS is essential for great agents:\n\nCLI expressiveness beats bespoke \"LLM tools.\" CLI tools are composable (|, &&, xargs), self-documenting (--help), and match\n how engineers actually work. One-off JSON tools tend to be isolated and require tool\n designers to predict every use case up front.\n\nOpen-loop orchestration beats rigid workflows. Real engineering has surprises:\n flaky tests, missing context, failing builds, CI-only failures, new constraints. The \"OS loop\"\n lets the agent decide the next step based on what it observes, instead of forcing every task into\n a fixed graph.\n\nIDE-native assistants like Claude Code are great for interactive, in-editor work: drafting,\n refactors, quick iteration. But they're not optimized to serve as a team-facing execution\n environment.\n\nOS-style agents are built for that missing layer: cross-tool orchestration + durable,\n reviewable artifacts.\n\nA mention, review request, regression, or scheduled trigger.\n\nCharlie normalizes + enriches the signal\n\nInto \"this is the work request, with relevant context.\"\n\nDecide whether to act, what thread it belongs to, and which capabilities are allowed.\n\nA durable Task is created (or appended-to)\n\nSingle-winner claiming, enters a durable, turn-based loop.\n\nAssemble context, decide + plan, call the LLM, dispatch tool calls, observe results +\n check mailbox/cancel + continue.\n\nBack to the source system (PRs, reviews, issue updates, summaries).\n\nRecord terminal status + telemetry\n\nDurable transcript/state remains; ephemeral compute is cleaned up.\n\nThe agentic operating system delivers better outcomes:",
    "readingTime": 2,
    "keywords": [
      "reviewable artifacts",
      "operating system",
      "agents",
      "durable",
      "tools",
      "context",
      "coding",
      "environment",
      "agent",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://charlielabs.ai/coding-agents-as-operating-systems/",
    "thumbnail_url": "https://charlielabs.ai/_astro/caos-architecture.JbhqvI6s.png",
    "created_at": "2026-02-05T12:36:57.286Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-engineering",
    "title": "Agentic Engineering",
    "description": "Agentic Engineering is a disciplined approach to AI-assisted software development that emphasizes human oversight and engineering rigor, distinguishing it fr...",
    "fullText": "A year ago, Andrej Karpathy coined ‚Äúvibe coding‚Äù to describe a gleefully reckless way of programming: you prompt, hand the keyboard to an AI, accept everything it spits out, don‚Äôt read the diffs, iterate by pasting error messages back in. It was a great label for a real thing - building quick prototypes or MVPs on pure AI autopilot.\n\nThe problem is that ‚Äúvibe coding‚Äù has become a suitcase term. People now use it to describe everything from a weekend hack to a disciplined engineering workflow where AI agents handle implementation under human oversight. These are fundamentally different activities, and conflating them is causing real confusion - and real damage.\n\nVibe coding means going with the vibes and not reviewing the code. That‚Äôs the defining characteristic. You prompt, you accept, you run it, you see if it works. If it doesn‚Äôt, you paste the error back and try again. You keep prompting. The human is a prompt DJ, not an engineer.\n\nIf vibe coding gives millions of people the ability to create custom software who otherwise couldn‚Äôt, that‚Äôs a genuine win. The technique has a legitimate place in the toolbox.\n\nBut the failure modes are well-documented at this point. The pattern is always the same: it demos great, then reality arrives. You try to modify it, scale it, or secure it, and you discover nobody understands what the code is actually doing. As one engineer put it, ‚ÄúThis isn‚Äôt engineering, it‚Äôs hoping.‚Äù\n\nHere‚Äôs the thing: a lot of experienced engineers are now getting massive productivity gains from AI - 2x, 5x, sometimes more - while maintaining code quality. But the way they work looks nothing like vibe coding. They‚Äôre writing specs before prompting. They‚Äôre reviewing every diff. They‚Äôre running test suites. They‚Äôre treating the AI like a fast but unreliable junior developer who needs constant oversight. I‚Äôve personally liked ‚ÄúAI-assisted engineering‚Äù and have talked about how this describes that end of the spectrum where the human remains in the loop.\n\nSimon Willison (whose work I adore) proposed ‚Äúvibe engineering‚Äù for this - it reclaims ‚Äúvibe‚Äù while adding ‚Äúengineering‚Äù to signal discipline. But after watching the community debate this for months, I think the the word ‚Äúvibe‚Äù carries too much baggage. It signals casualness. When you tell a CTO you‚Äôre ‚Äúvibe engineering‚Äù their payment system, you can see the concern on their face.\n\nAndrej Karpathy suggested ‚Äúagentic engineering‚Äù this week and I think I like it.\n\nIt describes what‚Äôs actually happening. You‚Äôre orchestrating AI agents - coding assistants that can execute, test, and refine code - while you act as architect, reviewer, and decision-maker. You might write only a % of the code by hand. The rest comes from agents working under your direction. That‚Äôs agentic. And you‚Äôre applying engineering discipline throughout. That‚Äôs engineering.\n\nIt‚Äôs professionally legible. ‚ÄúAgentic engineering‚Äù sounds like what it is: a serious engineering discipline involving autonomous agents. You can say it to your VP of Engineering without embarrassment. You can put it in a job description. You can build a team practice around it.\n\nIt draws a clean line. Vibe coding = YOLO. Agentic engineering = AI does the implementation, human owns the architecture, quality, and correctness. The terminology itself enforces the distinction.\n\nThe workflow isn‚Äôt complicated, but it requires discipline that vibe coding explicitly abandons:\n\nYou start with a plan. Before prompting anything, you write a design doc or spec - sometimes with AI assistance. You break the work into well-defined tasks. You decide on the architecture. This is the part vibe coders skip, and it‚Äôs exactly where projects go off the rails.\n\nYou direct, then review. You give the AI agent a well-scoped task from your plan. It generates code. You review that code with the same rigor you‚Äôd apply to a human teammate‚Äôs PR. If you can‚Äôt explain what a module does, it doesn‚Äôt go in.\n\nYou test relentlessly. The single biggest differentiator between agentic engineering and vibe coding is testing. With a solid test suite, an AI agent can iterate in a loop until tests pass, giving you high confidence in the result. Without tests, it‚Äôll cheerfully declare ‚Äúdone‚Äù on broken code. Tests are how you turn an unreliable agent into a reliable system.\n\nYou own the codebase. You maintain documentation. You use version control and CI. You monitor production. The AI accelerates the work, but you‚Äôre responsible for the system.\n\nTeams doing this well often report faster development - and those gains come from augmenting a solid process, not abandoning one. The AI handles boilerplate and grunt work. The human focuses on architecture, correctness, edge cases, and long-term maintainability.\n\nThe irony is that AI-assisted development actually rewards good engineering practices more than traditional coding does. The better your specs, the better the AI‚Äôs output. The more comprehensive your tests, the more confidently you can delegate. The cleaner your architecture, the less the AI hallucinates weird abstractions. As one analysis noted, ‚ÄúAI didn‚Äôt cause the problem; skipping the design thinking did.‚Äù\n\nHere‚Äôs an uncomfortable truth from the trenches: agentic engineering disproportionately benefits senior engineers. If you have deep fundamentals - you understand system design, security patterns, performance tradeoffs - you can leverage AI as a massive force multiplier. You know what good code looks like, so you can efficiently review and correct AI output.\n\nBut if you‚Äôre junior and you lean on AI before building those fundamentals, you risk a dangerous skill atrophy. You can produce code without understanding it. You can ship features without learning why certain patterns exist. Several engineering leaders have flagged this as an emerging crisis: a generation of developers who can prompt but can‚Äôt debug, who can generate but can‚Äôt reason about what they‚Äôve generated.\n\nThis isn‚Äôt an argument against AI-assisted development. It‚Äôs an argument for being honest about what it demands. Agentic engineering isn‚Äôt easier than traditional engineering - it‚Äôs a different kind of hard. You‚Äôre trading typing time for review time, implementation effort for orchestration skill, writing code for reading and evaluating code. The fundamentals matter more, not less.\n\nThe trajectory is clear: AI agents are getting more capable, and the agentic engineering workflow is becoming default for a growing number of professional developers. This is going to accelerate.\n\nThe rise of AI coding doesn‚Äôt replace the craft of software engineering - it raises the bar for it. The developers who‚Äôll thrive aren‚Äôt the ones who prompt the fastest. They‚Äôre the ones who think the clearest about what they‚Äôre building and why, then use every tool available - including AI agents - to build it well.\n\nVibe coding showed us what‚Äôs possible when you drop all conventions.\n\nNow it‚Äôs time to bring the engineering back. Let‚Äôs call that what it is.\n\nI‚Äôve written a new book with O‚ÄôReilly, Beyond Vibe Coding, that goes deeper into practical frameworks for AI-Assisted (and agentic) engineering. If you‚Äôve been figuring this out in your own workflow, I‚Äôd love to hear what‚Äôs working.",
    "readingTime": 6,
    "keywords": [
      "ai-assisted development",
      "vibe coding",
      "agentic engineering",
      "engineering workflow",
      "engineering it‚Äôs",
      "engineering discipline",
      "andrej karpathy",
      "the ai",
      "code",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/agentic-engineering/",
    "thumbnail_url": "https://addyosmani.com/assets/images/agentic-engineering.jpg",
    "created_at": "2026-02-05T12:36:55.943Z",
    "topic": "tech"
  },
  {
    "slug": "mark-zuckerbergs-2004-coding-jams-were-loud-emo-and-very-on-brand",
    "title": "Mark Zuckerberg's 2004 coding jams were loud, emo, and very on brand",
    "description": "Mark Zuckerberg published a playlist of five songs he was pumping while building Facebook in 2004. It includes hard-rocking hits and some synth funk.",
    "fullText": "Mark Zuckerberg is feeling nostalgic.\n\nOn the 22nd anniversary of Facebook's founding, Zuckerberg reposted a meme about the fresh-faced Harvard student \"getting ready to make history.\" Then he linked a playlist titled \"2004 Facebook coding jams.\"\n\nZuckerberg said he \"had these bangers on repeat\" while building Facebook. The playlist's cover features the young founder ‚Äî¬†long before his bulked-up rebrand ‚Äî¬†listening to music on headphones.\n\nThe playlist's five songs feature a fair share of emo wails, electric guitars, and even some funky synthesizers. Some of them have receded into the tides of music history; others remain classics for weddings and bar mitzvahs.\n\nThey're also evocative of a quieter, easier time in tech. There were no culture wars, cage matches, or capex worries. What would a 2004 Zuckerberg think of $10 billion data centers?\n\nMusic speaks to how we feel. Here's what these five songs might say about a 19-year-old Zuckerberg ‚Äî¬†and just how far he's come.\n\nZuckerberg liked hard rock, it seems.\n\n\"Headstrong\" is a loud, thrashing song that feels built to be played on \"Guitar Hero.\" There's even a good guttural scream in there before one chorus. The lyrics are equally attacking. \"Back off, I'll take you on,\" Chris Taylor Brown sings.\n\nThe opening of \"Headstrong\" sounds like UFC walkout music, hyping up a big fight. That's now a favorite sport of Zuckerberg's. Some things never change.\n\n\"Like a Stone\" is about death and religion ‚Äî two subjects that Zuckerberg has long been interested in.\n\nFrontman Chris Cornell prays to God and angels on their deathbed that they will get into heaven. \"In dreams until my death / I will wander on,\" he sings.\n\nZuckerberg seems more interested in extending human life than forecasting its end. His philanthropic initiative invests heavily in drug and disease research. He also has an intense fitness routine, including jiu-jitsu and CrossFit routines.\n\nAs for faith, Zuckerberg said in 2020 that he had \"become more religious.\" He cited two sources: The issues his company has faced in the prior years and the birth of his daughters.\n\nMostly, though, \"Like A Stone\" is another huge rock hit. It seems like Zuckerberg enjoyed the sound of banging drums and wailing guitar.\n\nNow this one is a nostalgia trip.\n\nYou can't help but feel something when Douglas Robb croons out, \"I'm not a perfect person.\" Was Zuckerberg a perfect person? Most people who have watched \"The Social Network\" would say no.\n\nIt's a romantic song. After considering their flaws, Robb sings out that they've found a reason to change. \"The reason is you,\" he repeats, over and over.\n\nZuckerberg met his wife, Priscilla Chan, in 2003, one year before this playlist's date. Maybe he was thinking of her.\n\nHe'd be less happy to hear that the song reemerged on TikTok ‚Äî¬†a Meta competitor ‚Äî¬†in a 2021 trend.\n\nWe all knew that Zuckerberg liked rap. He recorded his own version of T-Pain's \"Get Low\" in tribute to his wife. He also started dressing like Eminem.\n\n\"In the End\" is peak 2010s emo rap-rock. One of its refrains is \"I tried so hard.\" Zuckerberg did try so hard¬†‚Äî¬†he often worked till late into the night.\n\nThe song is more pessimistic in its outlook: \"In the end, it doesn't even matter.\" Zuckerberg would likely disagree here. He tried so hard ‚Äî¬†and in the end, it did matter. Meta is now a trillion-dollar company.\n\nHard pivot! After four hard rock tunes, Zuckerberg's last song leans into synth-funk.\n\nThe lyrics here feel self-explanatory for Zuckerberg's workplace ethos: \"Work it harder, make it better / Do it faster, makes us stronger / More than ever, hour after hour / Work is never over.\"\n\nAfter Meta's year of intensity, I'm sure that some of Zuckerberg's employees can relate.\n\nAlso, the Daft Punk helmet doesn't look that different from a Meta Quest.",
    "readingTime": 4,
    "keywords": [
      "zuckerberg liked",
      "like stone",
      "song",
      "music",
      "playlist's",
      "rock",
      "sings",
      "history",
      "songs",
      "headstrong"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mark-zuckerberg-2004-coding-jams-playlist-loud-emo-on-brand-2026-2",
    "thumbnail_url": "https://i.insider.com/69838cc9d3c7faef0ecd9c7b?width=1200&format=jpeg",
    "created_at": "2026-02-05T12:36:52.821Z",
    "topic": "finance"
  },
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What‚Äôs Behind the ‚ÄòSaaSpocalypse‚Äô Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "webhook-skills-agent-skills-for-webhook-providers-and-best-practices",
    "title": "Webhook Skills ‚Äì Agent skills for webhook providers and best practices",
    "description": "Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopif...",
    "fullText": "hookdeck\n\n /\n\n webhook-skills\n\n Public\n\n Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopify, GitHub, and more. Built on the Agent Skills specification.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n hookdeck/webhook-skills",
    "readingTime": 1,
    "keywords": [
      "webhook",
      "skills",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/hookdeck/webhook-skills",
    "thumbnail_url": "https://opengraph.githubassets.com/ed9e4e830971c76b02864c100a37accbc7ef947718023c6abb2ba2536dad1e19/hookdeck/webhook-skills",
    "created_at": "2026-02-04T12:35:28.405Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-the-new-rad",
    "title": "Vibe Coding is the new RAD",
    "description": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.",
    "fullText": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.\n\nFile details: 6.9 MB MP3, 5 mins 12 secs duration.\n\nTitle music is \"Apparent Solution\" by Brendon Moeller, licensed via www.epidemicsound.com\n\nFive.Today is a highly-secure personal productivity application designed to help you to manage your priorities more effectively, by focusing on your five most important tasks you need to achieve each day.\n\n Our goal is to help you to keep track of all your tasks, notes and journals in one beautifully simple place, which is highly secure via end-to-end encryption. Visit the URL Five.Today to",
    "readingTime": 1,
    "keywords": [
      "tasks",
      "five.today"
    ],
    "qualityScore": 0.65,
    "link": "https://techleader.pro/a/723-Vibe-Coding-is-the-new-RAD-(TLP-2026w3)",
    "thumbnail_url": "https://techleader.pro/img/icons/noun_programmer_2644331.png",
    "created_at": "2026-02-04T01:06:56.822Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-good-yet",
    "title": "Is AI \"Good\" Yet?",
    "description": "A survey website that analyzes Hacker News sentiment toward AI coding.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.is-ai-good-yet.com",
    "thumbnail_url": "https://www.is-ai-good-yet.com/og-image.png",
    "created_at": "2026-02-03T06:37:48.803Z",
    "topic": "tech"
  },
  {
    "slug": "contextgeneric-programming-v061-release-improving-ergonomics-and-debugging",
    "title": "Context-Generic Programming v0.6.1 Release: Improving Ergonomics and Debugging",
    "description": null,
    "fullText": "This blog post is co-authored by Claude Haiku based on a human draft. All content has been reviewed by the human author.\n\nWe're excited to announce the release of cgp v0.6.1, which brings several quality-of-life improvements to Context-Generic Programming in Rust. This release focuses on making CGP more accessible to developers new to the paradigm while also providing better debugging and verification tools for complex provider setups.\n\nThe changes in this release reflect our commitment to lowering the barrier to entry for CGP, while maintaining the power and flexibility that make it such a compelling approach to modular Rust code. Whether you're an experienced CGP user or just getting started, we believe you'll find these improvements meaningful.\n\nOne of the biggest hurdles when learning CGP is understanding how generic Context parameters work in provider implementations. While generics are fundamental to Rust, they can be intimidating to developers coming from object-oriented backgrounds, or even to experienced Rust developers who haven't deeply explored the generic type system.\n\nWhen writing a #[cgp_impl] provider, you previously had to explicitly declare the Context as a generic parameter and ensure it appeared correctly throughout your implementation. This added cognitive overhead that wasn't directly related to the business logic you were implementing. For beginners, this meant wrestling with generic syntax before they could even focus on understanding the core CGP concepts like delegation and trait constraints.\n\nCGP v0.6.1 introduces support for implicit Context types in the #[cgp_impl] macro. You can now omit the explicit generic parameter and use familiar Rust patterns like Self to refer to the context type.\n\nLet's look at a concrete example. Suppose we're building a greeting system where different types can greet people:\n\nBefore v0.6.1, the provider implementation would look like:\n\nWith v0.6.1, you can simplify this to:\n\nThe difference might seem small at first glance, but it has a profound impact on readability and accessibility. The new syntax eliminates the generic parameter syntax entirely, making the provider implementation look much more like a standard Rust trait implementation. For someone new to CGP coming from an OOP background, this is far less intimidating‚Äîit reads like implementing a method on a class, which is a familiar pattern.\n\nThis improvement particularly shines when you have multiple providers in a codebase. Each provider definition becomes clearer and more focused on the actual logic, rather than the generic plumbing.\n\nAs your CGP applications grow in complexity, so does the challenge of verifying that all your provider wiring is correct. This is particularly true when using higher-order providers‚Äîproviders that accept other providers as generic parameters.\n\nWhen you compose multiple providers together and something goes wrong, the compiler error messages can be cryptic. The error might point to a deep dependency in the chain, but it won't clearly tell you which individual provider in the composition is actually failing. This forces developers to compile and test repeatedly, changing providers and wiring patterns to narrow down the issue.\n\nCGP v0.6.1 introduces the #[check_providers] attribute for the check_components! macro, which lets you verify that specific providers work correctly with a given context. This is a powerful debugging tool that allows you to isolate and test individual providers before wiring them into your context.\n\nHere's a simple example. Suppose we have a shape area calculation system:\n\nYou can verify that RectangleArea works with Rectangle using:\n\nWith the new #[check_providers] attribute, you can also verify the provider directly:\n\nWhat's the benefit? The #[check_providers] version checks whether RectangleArea itself can be used as a provider for the component, regardless of what's currently wired in your Rectangle context. This is useful if you're developing multiple providers and want to test them independently before integrating them into your system.\n\nThe real power of #[check_providers] becomes apparent when you're working with composed higher-order providers. Let's look at a more realistic example:\n\nNow, when we verify this setup, we want to ensure not only that the composed provider works, but also that each individual component in the composition is correct. We can do this:\n\nThe first check verifies that the wired provider works with the context. The second check verifies each individual provider (or provider combination) that makes up your composition. If something is broken, such as the width field is missing, the error messages from these targeted checks will clearly point you to the problematic provider, rather than the entire composition.\n\nThis is a game-changer for debugging complex CGP setups. Instead of staring at a wall of compiler errors and trying to trace through the dependency chain, you can surgically test each part of your provider composition.\n\nOne common pattern in CGP is using getter traits to extract values from your context. The #[cgp_auto_getter] macro makes this convenient by automatically implementing a getter that reads a field from your context using the HasField trait.\n\nHowever, there was one limitation that made more sophisticated getter traits tedious to work with: you couldn't define an abstract type for the return value of your getter. If you wanted a getter trait where the return type was customizable per context, you had to define a separate abstract type trait, then link it to your getter trait.\n\nThis meant the old pattern looked like:\n\nThis works, but it requires defining two separate traits when conceptually you just want one getter trait with a flexible return type. For simple cases, this felt like boilerplate.\n\nCGP v0.6.1 allows you to define associated types directly in getter traits, eliminating the need for the separate type trait. You can now write:\n\nThe HasName trait now has an abstract Name type, and when the auto-getter is derived for Person, it automatically implements HasName with String as the concrete Name type. This is much more concise and reads more naturally.\n\nThe benefits are particularly clear when you have multiple getters with abstract types. Instead of maintaining a parallel set of type traits, you can keep everything in one place, making your code easier to understand and maintain.\n\nIf you're using the more powerful #[cgp_getter] macro (which allows customization of the implementation through providers), the same support for associated types works seamlessly:\n\nEven though our struct has a first_name field, we can still use the HasName getter by wiring it with UseField. The #[cgp_getter] macro combined with associated types gives you both power and convenience.\n\nThese three changes‚Äîimplicit Context types, enhanced provider checking, and associated types in getters‚Äîwork together to achieve our goal: making CGP more accessible and maintainable without sacrificing its power.\n\nThe improvements lower the learning curve for new CGP users by reducing generic syntax overhead. They provide better tooling for verifying and debugging complex provider compositions. And they reduce boilerplate when defining sophisticated traits, freeing you to focus on business logic.\n\nAs your CGP codebase grows, you'll find yourself reaching for these new features frequently. The #[cgp_impl] simplification makes your code more readable. The #[check_providers] attribute helps you debug faster. And associated types in getters let you express your intent more directly.\n\nWe believe CGP v0.6.1 represents a meaningful step forward in making Context-Generic Programming more approachable and productive. These changes emerged directly from feedback and real-world usage patterns in the CGP community, and we're confident they'll improve the development experience for everyone.\n\nWe encourage you to upgrade to v0.6.1 and explore these new features. Try simplifying your provider implementations with implicit Context types. Use #[check_providers] to debug your next complex provider composition. Define getter traits with abstract types and feel the difference in conciseness and clarity.\n\nAs always, we welcome feedback, bug reports, and contributions. The CGP journey continues, and we're excited to see what you build with these new tools.",
    "readingTime": 7,
    "keywords": [
      "let's look",
      "check verifies",
      "cgp introduces",
      "business logic",
      "error messages",
      "we're excited",
      "cgp_getter macro",
      "check_providers attribute",
      "debugging complex",
      "generic parameter"
    ],
    "qualityScore": 1,
    "link": "https://contextgeneric.dev/blog/v0-6-1-release/",
    "thumbnail_url": "https://contextgeneric.dev/cgp-logo.png",
    "created_at": "2026-02-02T12:34:23.079Z",
    "topic": "tech"
  },
  {
    "slug": "the-creator-of-clawdbot-the-viral-ai-agent-says-he-got-so-obsessed-with-vibe-coding-it-pulled-him-into-a-rabbit-hole",
    "title": "The creator of Clawdbot, the viral AI agent, says he got so obsessed with vibe coding it pulled him into a 'rabbit hole'",
    "description": "The creator of Clawdbot, the viral AI agent, says vibe coding can blur into compulsion, creating the illusion of productivity without real progress.",
    "fullText": "The creator of the viral AI agent Clawdbot says he had to step back after becoming too obsessed with vibe coding.\n\nPeter Steinberger, the developer behind Clawdbot ‚Äî which later changed its name to Moltbot and is now known as OpenClaw ‚Äî said in an episode of \"Behind the Craft\" podcast published Sunday that vibe coding pulled him into a \"rabbit hole.\"\n\n\"I was out with my friends and instead of, like, joining the conversation in the restaurant, I was just like, vibe coding on my phone,\" he said.\n\n\"I decided, OK, I have to stop this more for my mental health than for anything else,\" he added.\n\nClawdbot went viral last month in the tech community, attracting a wave of high-profile fans ‚Äî from Y Combinator CEO Garry Tan to multiple partners at Andreessen Horowitz.\n\nIt is a personal AI agent designed to run continuously and plug into a wide range of consumer apps, including WhatsApp and Telegram. Users can ask the AI to manage their schedules, oversee vibe-coding sessions, and even create AI employees.\n\nThe AI agent has been widely praised and meme'd online, with some tech fans even buying Mac Minis specifically to run the AI, Business Insider's Henry Chandonnet reported last week.\n\n‚Äã‚ÄãSteinberger said developers can fall into this trap of being hooked onto vibe coding, where building increasingly powerful AI tools creates the \"illusion of making you more productive\" without real progress.\n\nBuilding new tools can feel rewarding and fun, but that can quietly blur into compulsion, he added.\n\nWith AI, developers can now \"build everything,\" but ideas and taste matter. Without them, developers risk building tools and workflows that don't actually move a project forward, ‚Äã‚ÄãSteinberger said.\n\n\"If you don't have a vision of what you're going to build, it's still going to be slop,\" he added.\n\nVibe coding has continued to surge in popularity, with companies and developers promoting how AI can speed up software development.\n\nEarlier this month, Anthropic said it built its new agentic work tool, Cowork, entirely using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. \"Us humans meet in-person to discuss foundational architectural and product decisions, but all of us devs manage anywhere between 3 to 8 Claude instances implementing features, fixing bugs, or researching potential solutions.\"\n\nThanks to Claude, the agent came together quickly. \"We sprinted at this for the last week and a half,\" Rieseberg said during a livestream.\n\nStill, despite the excitement around how fast vibe coding can produce new tools, tech leaders are warning that it has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that he won't vibe code on \"large codebases where you really have to get it right.\"\n\n\"The security has to be there,\" he added.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding is great for prototypes or throwaway code, not software that sits at the core of a business.\n\n‚Äã‚Äã\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "developers",
      "agent",
      "tools",
      "clawdbot",
      "tech",
      "viral",
      "episode",
      "fans"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/clawdbot-creator-vibe-coding-rabbit-hole-obsessed-openclaw-peter-steinberger-2026-2",
    "thumbnail_url": "https://i.insider.com/69802a8da645d11881886c3c?width=1200&format=jpeg",
    "created_at": "2026-02-02T12:34:16.408Z",
    "topic": "finance"
  },
  {
    "slug": "i-embedded-myself-in-a-vibe-coding-team-at-geminis-ai-hackathon-in-singapore-building-an-app-in-7-hours-takes-real-work",
    "title": "I embedded myself in a vibe coding team at Gemini's AI hackathon in Singapore. Building an app in 7 hours takes real work.",
    "description": "I followed a hackathon team as they raced to vibe code an app in seven hours at Google's Gemini 3 Hackathon in Singapore.",
    "fullText": "Just after sunrise, four vibe coding enthusiasts from Malaysia crossed into Singapore with a loose idea ‚Äî and a bet that AI could build most of their app.\n\nHours later, they were racing to prototype it at Google's Gemini 3 Hackathon in Singapore.\n\nThe four friends, all in their late 30s to 40s, came from different professional backgrounds. Chan Wei Khjan is an accountant. Chan Ler-Kuan lectures on AI at a private university. Loh Wah Kiang works in IT. Lee How Siem, who goes by Benny, is the chief technology officer of a Malaysian startup.\n\nTheir initial idea was a \"feng shui\" app to analyze properties in Singapore ‚Äî a potentially lucrative use case in a market obsessed with housing and wealth accumulation. Feng shui is a traditional Chinese practice that evaluates how a person's surroundings, along with birth factors, influence luck and well-being.\n\nI embedded with the team at Google's developer space in Singapore in January to observe how a vibe-coding project comes together ‚Äî or nearly falls apart ‚Äî in seven hours.\n\nThe assignment: Teams of up to four people had to build a working demo, publish a public repository with code, and submit a short video explaining their project by 5:30 p.m.\n\nEach project had to fit into one of six tracks, including generative media, deep research, and enterprise orchestration.\n\nOrganized by Google DeepMind and 65labs, Singapore's AI builder collective, the hackathon featured a 100,000-credit Gemini API prize pool, with first place getting 30,000 credits.\n\nThe team had pivoted to a new idea due to time constraints: a feng shui app that could analyse a user's outfit and workspace through the phone camera in real time and assess how \"lucky\" they were.\n\nWei Khjan took the lead on prompting. He typed the first instructions into Claude, asking it to generate the workflow and code. Ler-Kuan focused on whether the AI's output aligned with feng shui concepts. Wah Kiang and Benny hovered over the codebase, refining ideas and flagging issues.\n\n\"For people who don't know how to read code, it's helpful to have people who do,\" Wei Khjan said.\n\nWhile waiting for the code to be generated, Ler-Kuan opened Google's AI Studio to design the app's logo. They called their app \"Feng Shui Banana.\"\n\nAfter about an hour, Claude generated the initial codebase for the app. It was designed to work with the Gemini Live API, enabling real-time image and text analysis. It ran but was riddled with bugs.\n\nAn error message flashed when they tested the camera feature, so Wei Khjan copied the error back into the AI and asked for it to be fixed. Minutes later, the feature worked.\n\nIt wasn't right. The feng shui logic was off, especially where colour analysis intersected with the user's birth timings. Ler-Kuan manually corrected the underlying dictionary and its mappings.\n\nThe team kept prompting to tighten the features: shorter explanations, clearer output, and more streamlined user interfaces.\n\nLunch arrived. The team stayed glued to their screens.\n\nThe app didn't respond instantly when a user changed their outfit, nor did it update its feng shui analysis in real time.\n\nWei Khjan explained how one prompt matters. Instead of issuing commands, he asked the AI to \"discuss it with me.\" The shift changed how the model reasoned, and it worked more like a collaborator.\n\nAfter some prompting, the app updated with a real-time camera analysis. It was striking to watch a feature emerging from a short back-and-forth with AI.\n\nI helped the team test the app.\n\nThe camera correctly identified what I was wearing: a dark green polo, a yellow participant tag, and a white name card hanging from my neck. According to the app, I was already wearing colours aligned with my luck for the day.\n\nThe app suggested small tweaks, such as additional accessories, that could enhance the feng shui of my outfit.\n\nThey finally had lunch and joked around to ease the tension. Four hours remained before they had to submit their project.\n\nLer-Kuan shifted focus to workspace feng shui, feeding knowledge into the model and refining how the app would evaluate desks and work setups. Wah Kiang and Benny worked on the video demo.\n\nThe team also revisited the app's tagline. After cycling through suggestions from multiple AI models, they settled on a line that didn't come from an AI at all: \"A wisdom, not a superstition.\"\n\nThey used Gemini to generate a storyboard for the demo video. The model laid out several scenes and drafted the script. The team followed along, filming clips and stitching everything together as they went.\n\nTheir workspace feature was also up and running.\n\nThe app had come together nicely. With some time to spare, they decided to add audio output for users who prefer listening to reading on a screen.\n\nThe first attempt to generate a voice using AI fell flat. It sounded robotic.\n\nAfter debugging and several iterations, they landed on a voice they liked, similar to how a Chinese feng shui master might speak.\n\nAs the deadline approached, the team was still stitching clips for their video and nitpicking the AI-generated presenter voice.\n\nThe organizers had urged teams to submit early. With about 15 minutes to spare, they made the call to lock the final cut and hit submit.\n\nThen it was over. The hunger hit immediately, and everyone got in line for some well-deserved food.\n\nEven as an observer, watching from the sidelines was tiring. Seven hours of vibe coding turned out to be anything but effortless.\n\nThe team didn't win a prize, but agreed that the hackathon had been worth it.\n\n\"Sometimes, the best experiences come from saying 'yes' without overthinking,\" said Ler Kuan. \"Innovation starts with curiosity and a little bit of spontaneity.\"\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "wah kiang",
      "vibe coding",
      "feng shui",
      "wei khjan",
      "shui app",
      "wah kiang and benny",
      "team",
      "hours",
      "project",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/vibe-coding-team-embed-google-gemini-hackathon-singapore-2026-2",
    "thumbnail_url": "https://i.insider.com/696ef318a645d118818798f3?width=1200&format=jpeg",
    "created_at": "2026-02-02T06:52:12.008Z",
    "topic": "finance"
  },
  {
    "slug": "tailwind-creator-adam-wathan-shares-new-project-uish",
    "title": "Tailwind creator Adam Wathan shares new project ui.sh",
    "description": "A toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.",
    "fullText": "Turn your terminal intoA toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.Request an inviteBy the people who made\nTailwind CSS & Refactoring UI",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://ui.sh/",
    "thumbnail_url": "https://ui.sh/og-image.png",
    "created_at": "2026-02-02T01:11:00.781Z",
    "topic": "tech"
  },
  {
    "slug": "kessel-run-air-force-software-development-division",
    "title": "'Kessel Run' Air Force software development division",
    "description": null,
    "fullText": "Kessel Run, formally \"Air Force Life Cycle Management Center Detachment 12\", is a United States Air Force software development division, based in Hanscom Air Force Base and Boston, Massachusetts. It was founded in 2017 by the Defense Innovation Unit in response to the need to modernize legacy Air Force software.\n\nIn October 2016, Eric Schmidt, former CEO of Google, was leading a group touring the Combined Air Operations Center at Al Udeid Air Base in Qatar in his role as inaugural chairman of the Defense Innovation Board.[1] The CAOC at Al Udeid oversees air force operations for over 20 countries, and at the time was engaged in the War in Iraq against the Islamic State.[1]\n\nOne of the Air Operations Center tasks was planning daily aerial refueling operations to support combat missions. This was done off the main CAOC hall, in a windowless room with a whiteboard bearing magnetic pucks and plastic laminated cards, and physically measuring distance on the board to determine how long planes could stay in the air.[1][2] The resulting data was manually entered into an Excel spreadsheet known as \"the Gonkulator\" by a person called \"the Gonker\".[1][2] When a VBScript run on the spreadsheet confirmed the data was correct, another person, called \"the Thumper\", manually typed in the result into a Master Air Attack Planning Toolkit, which helped generate the Air Tasking Order back at Shaw Air Force Base in South Carolina.[1] Yet another person watched to verify against retyping errors.[2] The process took eight to 12 hours each day for three or eight people.[1][2][3] If there was a change needed, the process needed to restart.[2][3] When Schmidt and the Defense Information Board asked whether the base had access to more modern software to automate this process, the answer was: \"Yes, but it doesn't work.\"[1]\n\nAir Operations Center software had been in use mostly unchanged since the 1990s.[1] From 2006 to 2011, Lockheed Martin worked on the concept of modernizing it, but did not take up the project.[4] A Northrop Grumman project to modernize AOC software was commissioned in 2013 for $374 million for development, and $3.5 billion for lifetime maintenance.[4] By 2016, when the Defense Innovation Board was touring Al Udeid, nothing had been delivered.[2] The development price eventually grew to $745 million, and was three years behind schedule, with an estimated launch date of December 2019, when the project was eventually cancelled in July 2017.[4]\n\nAlso touring the Al Udeid CAOC with Schmidt in October 2016 was Raj Shah, tech entrepreneur and managing partner of DIUx, the Defense Innovation Unit Experimental.[1][2] Shah was a former Air Force fighter pilot who had first hand experience of the importance of regular aerial refueling.[2] That same night, he called lieutenant colonel Enrique Oti who was head of Air Force programs for DIUx in Silicon Valley.[1] They arranged an unusual partnership between Air Force developers and Pivotal Software, and got it approved by General Jeffrey Harrigian, commanding United States Air Forces Central Command.[1] Rather than rewrite the entire AOC software suite, they would just do the tanker whiteboard.[clarification needed][1] In December 2016, coders and program managers were visiting Al Udeid to talk to users. By April 2017, the aerial refueling tanker application, named Jigsaw, was in use at the CAOC; four months from start to production, when a more average Defense Department software operation took as much as three to five years.[1][5]\n\nThe speed of development was credited to the Agile development process, an iterative, adaptive approach, that doesn't try to get the entire solution in the first release. The first version of an application is intentionally only about a 60% solution, which is then changed and improved via rapid iterations as users give feedback.[6] Though this agile development is fairly basic in the modern software industry, it was unusual for the Defense Department.[7] The development team would later adopt the hashtag #AgileAF - the AF, they assure, stands for Air Force.[1]\n\nThe total cost of Jigsaw was reported at $1.5 million (Captain Bryon Kroger, Chief Operating Officer of the project),[5] to $2.2 million (Shah),[1] \"chump change\" (Harrigian).[8] With it, tanker planning was not only faster, taking two to three hours for a single person, but was more reliable, so two to three fewer tankers were scrambled each day.[1][3] Each scramble had a cost in fuel and maintenance of about $250,000 each.[1][2] Jigsaw saved 350,000 pounds of fuel a week.[8] Its development costs were recouped in the first week.[1][9]\n\nIn April 2017, after delivering Jigsaw, Oti, Kroger, and others, got approval to form an official Air Force software development team at the Air Force Life Cycle Management Center named Project Kessel Run.[2][10][5] The name \"Kessel Run\" came from a line in the 1977 science fiction film Star Wars, spoken by smuggler Han Solo, bragging about the speed of his ship, the Millennium Falcon.[2] It represented the project's intent to \"smuggle\" new software development capability into the Air Force and use it to set new software development speed records.[9]\n\nIn May 7, 2018, the Kessel Run Experimentation Lab set up at a WeWork shared facility in central Boston. It was modeled after Pivotal Labs training locations in Cambridge, San Francisco, and Washington.[11] It was managed from the AFLCMC at Hanscom Air Force Base, which believed the innovation advantages in coworking and creativity would outweigh the hassles of distance and security.[11][12] The Lab initially had space for 90 engineers, but planned for 300 within a year.[9] Many were on temporary assignment from other Air Force bases; yet others would be sent off to Pivotal Labs offices across the country for training in modern software techniques.[11][13][14] The Kessel Run motto on the wall was \"Code. Deploy. Win.\", a play on the Air Force's motto, \"Fly. Fight. Win.\".[12]\n\nOn January 2, 2019, the Kessel Run Experimentation Laboratory moved to a different location in a Boston skyscraper.[15] Kessel Run's budget since mid 2017 had grown to approximately $140 million, including workspace and personnel, and the operational software produced claimed savings of $13 million and 1,100 man-hours per month.[15]\n\nOn May 8, 2019, Kessel Run formally became Air Force Life Cycle Management Center Detachment 12, commanded by Colonel Oti, who had been effectively leading it for just under a year.[16] It had nearly 700 airmen, government civilians, and contractors.[16] Oti was replaced as commander by Colonel Brian Beachkofski on April 15, 2020, with the ceremony held over Zoom teleconference.[17] On June 27, 2022, Beachkofski was replaced as commander by Colonel Richard Lopez, who took the title of senior materiel leader.[18] Lopez had previously been the director of the LevelUP Code Works software factory inspired by Kessel Run.[18][19]\n\nThe March 2019 Defense Innovation Board report on software acquisitions included a chapter on Kessel Run subtitled \"The Future of Defense Acquisitions Is #AgileAF\".[21]\nIn September and October 2019, Kessel Run received multiple awards: the General Larry O. Spencer Innovation award; the Theodore von K√°rm√°n award for modernizing software for the F-35; and the inaugural Defense Acquisition Software Innovation Team award.[22][23] A 2019 editorial in Defense One said that Kessel Run was \"widely seen as the gold standard of military tech done right ... also the most hyped military program office in operation today\".[24]\n\nNot all reactions were positive. A 2019 anonymous survey of KREL application users found that some applications did not meet user needs, and that success metrics, documentation, and responsiveness to user feedback could all be lacking.[25] A 2020 Harvard Kennedy School project found and tried to address internal discontent among Kessel Run staff with emerging bureaucracy and increasing technical complexity.[26] The Air Force's Deputy Chief Information Officer, Lauren Knausenberger, acknowledged in 2020 that Kessel Run was having growing pains, but said that was a result of its success.[27]\n\nKessel Run inspired multiple agile software development teams across the Air Force and United States Department of Defense.[28] They were called \"software factories\".[29] The original definition of software factory was a set of software tools to write and automatically build, test, and document applications; the Chief Information Officer of the Department of Defense slightly redefined that to be a software assembly plant that automated the develop, build, test, release and deliver phases, but in each case to support agile software development practices.[30]\n\nKobayashi Maru, formally Space C2, or Space Command and Control, in California, was the second such software factory, in August 2018.[28] It was named after an impossible test in the Star Trek science fiction universe.[31] Just as Kessel Run came from an effort to replace an outdated system by getting around bureaucratic rules, Kobayashi Maru intended to update the software of the troubled Joint Mission System (jointly run with the United States Space Force) for space command and control and situational awareness.[32][33]\n\nBESPIN - an acronym for Business and Enterprise Systems Product Innovation, but also the name of a planet in the Star Wars universe - was the third Air Force software factory, launched in early 2019 in Montgomery, Alabama, to create apps for maintenance crew chiefs, aircrew readiness, and ammunition crews.[28][31] Space Camp, in Colorado, and Section31, in California, spun off of Kobayashi Maru.[31] LevelUP, in Texas, was a joint cyber operations system for the Unified Platform, connecting the Army, Marines, and United States Cyber Command, debuting in April 2019.[31] By September 2021, there were 17 Air Force software factories across the country.[34]\n\nSoftware factories weren't limited to the Air Force. The Navy was inspired by Kessel Run to stand up its first software factory, The Forge, in Riverdale, Maryland, in March 2021.[35] The Army Software Factory debuted in April 2021 in Austin Community College in Texas, as part of the United States Army Futures Command.[36][37] In February 2022, Deputy Defense Secretary Kathleen Hicks wrote a DOD Software Modernization Strategy memo encouraging increased used of software factories throughout the Defense Department; at the time there were 29.[38] By April 2022 the United States Coast Guard was planning a software factory based on the Air Force model.[39] The Marine Corps Software Factory was co-located with the Army Software Factory as a three year test project in Austin in March 2023.[40]\n\nThe 24th Air Force's Air Force Cyber Proving Ground may be another related activity.\n\nJigsaw, the 2017 aerial refueling planning application that started Kessel Run, was bought and used by NATO in multiple countries in 2020 and 2021.[41][42]\n\nThe team's second and third projects after Jigsaw were Chainsaw and Raven, applications for assembling and communicating target information.[1] Chainsaw was in operation by November 2017, consolidated many programs into one, and cut the process for dynamic targeting from an hour or two to minutes. Raven, for target development management, cut 12 hours of work down to three or four, and was ready in early 2018.[43]\n\nStarting in late 2018, Kessel Run joined the task of fixing the troubled software for the maintenance of the F-35 fighter jet, called ALIS, for Autonomic Logistic Information System.[44] ALIS was a 17 year old proprietary system full of bugs and data gaps.[45] Maintainers had to keep separate databases because they could not rely on ALIS data.[46] The project to fix ALIS, including Kessel Run, Pivotal, and Lockheed Martin, its original creators, was called Mad Hatter, named by the developers.[44] It officially started in October 2018, but took until January 2019 before developers could write code, while the Air Force negotiated with Lockheed Martin as to what parts of the proprietary ALIS system the government could be able to reach.[44] The Mad Hatter suite of eight programs was tested and favorably evaluated by F-35 aircraft maintainers in March 2020.[47] In July 2020, it was renamed to Torque, and adapted for maintenance of the F-22 stealth jet and CV-22 tiltrotor aircraft,[48] then the C-130J turboprop in January 2021.[49] Meanwhile, on the F-35 itself, between 2020 and 2022 ALIS was gradually replaced by ODIN, the Operational Data Integrated Network, \"leveraging\" the software practices of Kessel Run, but built by Lockheed Martin.[50][51]\n\nIn 2021, Kessel Run began deploying the initial version of KRADOS, the Kessel Run All Domain Operations Suite meant to replace the Theater Battle Management Core Systems that created air tasking orders throughout AOCs all over the world.[52] The first AOC to use the suite operationally was again the 609th Air Operations Center at Al Udeid, in May 2021, after using the Beta version since December 2020.[53] KRADOS linked together nine applications through cloud-based data, including the latest version of Jigsaw, Kessel Run's first application for tanker planning, and Slapshot, for planning the rest of the air missions and building the Master Air Attack Plan.[54] In 2017, Lockheed Martin had received a $38 million contract to maintain the older TBMCS, but the 609th kept finding problems, so turned to Kessel Run, which delivered the beta version of KRADOS three weeks after receiving the request in November 2020.[55] By August 2022, the 603rd AOC in Ramstein, Germany, employed elements of KRADOS for visualization, though it was not considered mature enough to create air tasking and airspace control orders.[56][57] In January 2023, the 609th replaced the TBMCS with KRADOS entirely.[58]\n\nThe Command and Control Incident Management Emergency Response Application (C2IMERA), is a real time Air Force base resource management tool. It used a different development model: the coding was done by software company Leidos, and the program management by Kessel Run.[59] In August 2019, Moody Air Force Base used the software to monitor and prepare for Hurricane Dorian, though it was not originally intended for this purpose.[60] C2IMERA was also used for the August 2021 evacuation of civilians from Afghanistan in Operation Allies Refuge.[61][62] It was ordered deployed across all Air Combat Command installations in September 2021.[59][63] In August 2023, Air Mobility Command joined Air Combat Command in designating C2IMERA as their standard installation command and control tool.[64]\n\nSlapshot, the air mission flow organizer part of KRADOS, was also used for the Operation Allies Refuge Afghan evacuation along with C2IMERA.[65][66] At that time, KRADOS had known issues with scaling; it couldn't handle many simultaneous operations, which was exactly what it was being asked to do. On August 24, 2021, at 2 am Boston time, the Slapshot server crashed. Over the next 12 hours, Kessel Run developers restarted servers, shifted United States Central Command resources to improve performance, fixed database errors, and added new features to improve load times, so the evacuation on the other side of the world could continue.[67]\n\nBowcaster, named after a Star Wars weapon, is a chaos engineering tool and playbook that intentionally creates failures in processes to strengthen them.[68][69]\nKessel Run developed it, and shares it with other government agencies, initially with the Navy Black Pearl software factory in 2021.[70][71] In March 2022 Kessel Run and the General Services Administration's Technology Transformation Services used it to check that the Cloud.gov website could handle 100 million users per hour.[72][73]",
    "readingTime": 13,
    "keywords": [
      "kessel run",
      "air force",
      "allies refuge",
      "life cycle",
      "operation allies",
      "center detachment",
      "cycle management",
      "innovation unit",
      "innovation board",
      "operations center"
    ],
    "qualityScore": 1,
    "link": "https://en.wikipedia.org/wiki/Kessel_Run",
    "thumbnail_url": "https://upload.wikimedia.org/wikipedia/commons/7/77/Kessel_Run_logo.jpg",
    "created_at": "2026-02-01T12:26:41.722Z",
    "topic": "tech"
  },
  {
    "slug": "securing-the-ralph-wiggum-loop-devsecops-for-autonomous-coding-agents",
    "title": "Securing the Ralph Wiggum Loop ‚Äì DevSecOps for Autonomous Coding Agents",
    "description": "Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck - agairola/securing-ralph-loop",
    "fullText": "agairola\n\n /\n\n securing-ralph-loop\n\n Public\n\n Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n agairola/securing-ralph-loop",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/agairola/securing-ralph-loop",
    "thumbnail_url": "https://opengraph.githubassets.com/6d3ccf091e1097ce34aa5949ea80238ec45757c8d310e8b3be3ea2b9c6e218fb/agairola/securing-ralph-loop",
    "created_at": "2026-02-01T06:37:20.298Z",
    "topic": "tech"
  },
  {
    "slug": "top-engineers-at-anthropic-openai-say-ai-now-writes-100-of-their-code",
    "title": "Top engineers at Anthropic, OpenAI say AI now writes 100% of their code",
    "description": "AI coding tools are getting more sophisticated. But if coders stop coding, what happens to software development jobs?",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/01/29/100-percent-of-code-at-anthropic-and-openai-is-now-ai-written-boris-cherny-roon/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/GettyImages-2216956965_40536e-e1769705381107.jpg?resize=1200,600",
    "created_at": "2026-01-31T01:04:24.596Z",
    "topic": "tech"
  },
  {
    "slug": "cooperbench-benchmarking-ai-agents-cooperation",
    "title": "CooperBench: Benchmarking AI Agents' Cooperation",
    "description": "CooperBench is a benchmark of over 600 collaborative coding tasks. We find that agents achieve 30% lower success rates when working together compared to performing both tasks individually.",
    "fullText": "Stanford University & SAP Labs US\n\nCan AI agents work together as teammates? We find\n that\n coordinating agents perform much worse than a\n single agent\n given the same total workload. This coordination\n deficit presents a fundamental barrier to deploying\n AI systems that can work alongside humans or other\n agents.\n\nSuccess rate on CooperBench across 652 tasks ¬∑ Error bars show 95% confidence intervals\n\nGPT-5 and Claude Sonnet 4.5 achieve only 25%\n success with two-agent cooperation, roughly 50%\n lower than when a single agent handles both\n tasks. This gap persists across all models and\n task difficulties.\n\nAgents spend up to 20% of their budget on\n communication. This reduces merge conflicts but\n does not improve overall success. The channel is\n jammed with repetition, unresponsiveness, and\n hallucination.\n\nEven when agents communicate well, coordination\n breaks down due to:\n\nAmong successful runs, we observe coordination patterns\n largely absent from failures. These patterns are not\n prompted or scaffolded.\n\nRole Division\n ‚Äî Agents agree on who handles which part of the\n task. One agent delegates: \"I'll add header +\n octal_str; you add binary_str between them.\"\n\nCooperBench is the first benchmark designed to\n measure how well AI agents can cooperate when\n handling individual tasks with potential conflicts.\n We constructed 652 tasks from 12 popular open-source\n libraries across Python, TypeScript, Go, and Rust.\n\nEach task assigns two agents different features that\n can be implemented independently but may conflict\n without proper coordination. Eight co-authors with\n real-world software engineering backgrounds created\n new features, unit tests, and ground-truth code.\n\nStanford University & SAP Labs ¬∑ *Equal contribution\n (Stanford) ¬∑ ‚Ä†Equal contribution (SAP Labs)",
    "readingTime": 2,
    "keywords": [
      "equal contribution",
      "stanford university",
      "university sap",
      "coordination",
      "tasks",
      "success",
      "across",
      "task",
      "agents",
      "cooperbench"
    ],
    "qualityScore": 0.85,
    "link": "https://cooperbench.com/",
    "thumbnail_url": "https://cooperbench.com/static/images/cooperbench_social.png",
    "created_at": "2026-01-30T18:28:27.534Z",
    "topic": "tech"
  },
  {
    "slug": "daedalus",
    "title": "Daedalus",
    "description": "AI planning CLI and autonomous agent orchestration for beans-based coding workflows - internet-development/daedalus",
    "fullText": "internet-development\n\n /\n\n daedalus\n\n Public\n\n AI planning CLI and autonomous agent orchestration for beans-based coding workflows\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n internet-development/daedalus",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/internet-development/daedalus",
    "thumbnail_url": "https://opengraph.githubassets.com/471147f12cb10edd91f2072a6602ce03fd3e4339e4f03a4a184ee7336b8d31ea/internet-development/daedalus",
    "created_at": "2026-01-30T06:35:17.254Z",
    "topic": "tech"
  },
  {
    "slug": "cwt-sandbox-ai-coding-agents-using-git-worktrees",
    "title": "Cwt ‚Äì Sandbox AI coding agents using Git Worktrees",
    "description": "Contribute to benngarcia/claude-worktree development by creating an account on GitHub.",
    "fullText": "benngarcia\n\n /\n\n claude-worktree\n\n Public\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n benngarcia/claude-worktree",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/benngarcia/claude-worktree",
    "thumbnail_url": "https://opengraph.githubassets.com/73752eaadbf154afd745270d32c1743038163f0555993f4c462cecd915f63c02/benngarcia/claude-worktree",
    "created_at": "2026-01-30T01:07:08.980Z",
    "topic": "tech"
  },
  {
    "slug": "acp-agent-registry-in-jetbrains-ides",
    "title": "ACP Agent Registry in JetBrains IDEs",
    "description": "Together with Zed, we've launched the official ACP Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs.",
    "fullText": "Supercharge your tools with AI-powered features inside many JetBrains products\n\nAI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day.¬†Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what‚Äôs out there, let alone getting it running in your IDE, hasn‚Äôt been easy.\n\nTogether with Zed (Zed‚Äôs announcement), we‚Äôve launched the official ACP Agent Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs and Zed. Browse what‚Äôs available, click Install, and start working right away. This beta release is just the beginning.\n\nThe Agent Client Protocol¬†is an open standard that lets any AI coding agent work in any supporting editor. Think of it like the Language Server Protocol, but for AI agents. The LSP lets any editor support any language through a shared standard. The ACP does the same for coding agents. You only need to implement it once, and then it will work in your JetBrains IDE, Zed, or any other editor that supports the protocol.\n\nThis means you get to¬†pick your preferred¬†agent and editor, and they will then work together seamlessly ‚Äì no vendor lock-in and no waiting for someone to build a specific integration.\n\nACP has been, since we started integrating it to Mistral Vibe, a real joy to use: thoughtfully designed from the ground up, community-driven, and evolving rapidly. We‚Äôve found it not only simplifies integration, but also fits our focus on open and flexible tools. It‚Äôs really great to see a standard that puts developer choice first.\n\nMichel Thomazo, Software Engineer @ Mistral AI\n\nThe ACP made agent interoperability technically possible. The registry makes it convenient.\n\nInstead of manually configuring agents, you can now:\n\nAt launch, you‚Äôll find a wide array of different agents:\n\nFull-featured coding assistant optimized for large-scale refactors\n\nSpecialized agent for automated code generation workflows\n\nGoogle‚Äôs agent with deep codebase understanding and multimodal capabilities\n\nGitHub‚Äôs AI pair programmer, now available via the ACP\n\nLightweight, fast agent built on Mistral‚Äôs models\n\nCommunity-driven, fully open-source agent\n\nAlibaba‚Äôs coding agent with strong multilingual support\n\nInnovation in software agents is moving at an unbelievable pace. The Agent Registry and ACP makes it simple for developers to use the best agents in their favorite tools.\n\nChris Kelly, Product @ Augment Code\n\nIn general, it‚Äôs less about having multiple agents¬†than¬†about enabling you to pick and choose the ones that work well in your workflow. Different agents come with different benefits. Some provide a more attractive pricing structure for your business, some provide a user experience that you simply enjoy more than others‚Äô, and some embody the ideas of open-source development that just resonate with you.\n\nThe Agent Client Protocol registry lets you experiment freely. Try a few, see what clicks for your workflow, and then keep the ones that help. You‚Äôre not locked into a single vendor‚Äôs vision of what AI-assisted development should look like.\n\nWe‚Äôre excited to support the ACP Agent Registry as a step toward a more open agent ecosystem where Droids can integrate seamlessly across all IDEs.\n\nFrancesca LaBianca, VP of Operations @ Factory\n\nIn any JetBrains IDE (2025.3.2+) with JetBrains AI (253.30387.147):\n\nThat‚Äôs it. The agent is configured and ready to use in the AI Chat¬†tool window.\n\nQuick note: agents typically come with their own subscription. That‚Äôs between you and them. You won‚Äôt need a JetBrains AI subscription to use ACP agents.\n\nWant to try something concrete? Install OpenCode, open a project, and ask it to explain an unfamiliar module. OpenCode also lets you swap between different LLMs, so you can experiment with what works best for you.\n\nIf you prefer manual configuration, that option is still there, too. Just edit the acp.json¬†directly. This is useful for agents that aren‚Äôt in the registry yet or for custom setups.\n\nIf you‚Äôre building an ACP-compatible agent, the registry is now the fastest way to reach developers across JetBrains IDEs and Zed.\n\nHead to the¬†ACP Registry repository¬†and check out the¬†CONTRIBUTING.md¬†for the full submission process and metadata requirements. Please note that, for now, we are only featuring agents that support Agent Auth or Terminal Auth. Full details of requirements and conditions can be found here.\n\nThis is an open registry. If you‚Äôre building an ACP-compatible agent, you‚Äôre welcome to submit it. The registry exists to serve the ecosystem, not to gatekeep it.\n\nFor developers:¬†More choice and zero lock-in. Use any agent you want in the IDE you love.\n\nFor agent builders:¬†Instant distribution to millions of JetBrains and Zed users. Implement the ACP once and reach everyone.\n\nFor the ecosystem:¬†Competition on quality, not on who controls the integration. The best agents win because they‚Äôre the best, not because they have exclusive deals.\n\nWe‚Äôre building this openly with Zed because we believe AI-assisted development shouldn‚Äôt be locked inside any single vendor‚Äôs ecosystem. Developers deserve to pick their tools freely.\n\nThe registry is one more step toward that future.\n\nThe ACP Registry is available now in JetBrains IDE versions 2025.3 and later. Update your IDE and the JetBrains AI plugin, open Settings, and start exploring.\n\nHave feedback? Found a bug? The¬†registry repo is open for issues and PRs. And if you‚Äôre building something interesting with ACP, we‚Äôd love to hear about it!\n\nOpenAI Codex is now natively integrated into the JetBrains AI chat, giving you another powerful option for tackling real development tasks right inside your IDE.¬†\n\nYou can use Codex with a JetBrains AI subscription, your ChatGPT account, or an OpenAI API key ‚Äì all within the same AI —Åhat inte‚Ä¶\n\nThe next edit suggestions feature is now enabled in all JetBrains IDEs for JetBrains AI Pro, AI Ultimate, and AI Enterprise subscribers.\n\nYes, you read that right! JetBrains-native diff suggestions are available right in your editor. Global support for optimized latency. Out-of-the-box IDE actions‚Ä¶\n\nBring Your Own Key (BYOK) is now available in the AI chat inside JetBrains IDEs as well as for AI agents, including JetBrains‚Äô Junie and Claude Agent. Whether you‚Äôre looking to use cutting-edge frontier models, cost-efficient small models, locally hosted private models, or experimental research prev‚Ä¶\n\nJunie is now integrated into the AI chat. The separate interfaces have merged into a single, unified space (available in Beta).",
    "readingTime": 6,
    "keywords": [
      "client protocol",
      "ai-assisted development",
      "step toward",
      "agent client",
      "acp-compatible agent",
      "jetbrains ai",
      "coding agents",
      "acp agent",
      "acp agent registry",
      "jetbrains ide"
    ],
    "qualityScore": 1,
    "link": "https://blog.jetbrains.com/ai/2026/01/acp-agent-registry/",
    "thumbnail_url": "https://blog.jetbrains.com/wp-content/uploads/2026/01/JB-social-BlogSocialShare-1280x720-1-4.png",
    "created_at": "2026-01-29T18:30:47.768Z",
    "topic": "tech"
  },
  {
    "slug": "extesla-ai-head-has-seen-a-phase-shift-in-software-engineering-using-claude-code-and-his-manual-skills-slowly-atrophy",
    "title": "Ex-Tesla AI head has seen a 'phase shift in software engineering' using Claude Code ‚Äî and his manual skills slowly 'atrophy'",
    "description": "Andrej Karpathy posted his \"notes from Claude Coding,\" describing a shift in engineering over the last two months.",
    "fullText": "He coined \"vibe coding.\" Now, he sees a \"phase shift\" in software engineering.\n\nAndrej Karpathy is one of AI's guiding figures. He was a founding member of OpenAI and later served as Tesla's director of AI. He also coined the term \"vibe coding,\" the AI-assisted coding movement that has taken software engineering by storm and was named Collins Dictionary's word of the year.\n\nIn his \"random notes from Claude Coding\" ‚Äî which are over 1,000 words long ‚Äî Karpathy wrote about the changes to his own coding style. Posted on X on Monday, the notes have already elicited reactions from engineers at Anthropic, xAI, and more.\n\nAI coding agents \"crossed some kind of threshold of coherence around December 2025 and caused a phase shift in software engineering,\" Karpathy wrote.\n\nA few random notes from claude coding quite a bit last few weeks.\n\nCoding workflow. Given the latest lift in LLM coding capability, like many others I rapidly went from about 80% manual+autocomplete coding and 20% agents in November to 80% agent coding and 20% edits+touchups in‚Ä¶\n\nKarpathy name-dropped both Anthropic's Claude Code and OpenAI's Codex as having significant improvements. Claude Opus 4.5, the model that has garnered much love from engineers online, came out at the tail end of November.\n\nThe AI leader's workflow has changed as a result of the AI tools. From November to December, Karpathy's 80/20 ratio flipped. He once used 80% manual coding and 20% agents; now, it's 80% agents and 20% manual code editing.\n\n\"I really am mostly programming in English now, a bit sheepishly telling the LLM what code to write... in words,\" he wrote.\n\nThe change to AI-written code \"hurts the ego,\" but is too powerful to ignore, Karpathy wrote. He also devoted a whole section of his notes to the \"fun\" he has while coding with large language models.\n\nWhat of those traditional coding skills, the ones you learn in a computer science program or through endless digital courses? That's a whole other function, Karpathy wrote, and one that might decline.\n\n\"I've already noticed that I am slowly starting to atrophy my ability to write code manually,\" he wrote.\n\nIn Karpathy's comments, engineers from leading AI companies sounded off. Ethan He, an xAI engineer and Nvidia alum, wrote that a \"10x engineer can be a one-man army.\"\n\nCharles Weill, another xAI engineer, wrote that founders can now \"divide themselves\" with coding agents, like a VC divides their capital over a portfolio of companies.\n\nBoris Cherny, an Anthropic staffer and the creator of Claude Code, wrote that he read Karpathy's \"thoughtful\" post till its end.\n\nThe Claude Code team at Anthropic may offer a model of where the industry is moving, Cherny wrote. His team is \"mostly generalists\" and filled with 10x engineers.\n\n\"Pretty much 100% of our code is written by Claude Code,\" Cherny wrote. \"For me personally it has been 100% for two+ months now, I don't even make small edits by hand.\"\n\nThe Anthropic employee also acknowledged the \"quality\" problems with AI-written code. Agents can overcomplicate things and can leave around dead code, he wrote.\n\nHis solution: having AI review the AI-written code.",
    "readingTime": 3,
    "keywords": [
      "ai-written code",
      "phase shift",
      "software engineering",
      "random notes",
      "xai engineer",
      "vibe coding",
      "coding agents",
      "engineers",
      "claude",
      "coined"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-claude-code-manual-skills-atrophy-software-engineering-tesla-2026-1",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-01-29T12:32:28.928Z",
    "topic": "tech"
  },
  {
    "slug": "openais-chair-says-vibe-coding-is-here-to-stay-but-its-not-the-endgame",
    "title": "OpenAI's chair says vibe coding is here to stay ‚Äî but it's not the endgame",
    "description": "Vibe coding will stick around, but AI agents, not apps, will drive the next big shift in software, says OpenAI's chair Bret Taylor.",
    "fullText": "Vibe coding isn't going anywhere. But it's only part of a much bigger transformation, says OpenAI's board chair.\n\nBret Taylor said in an episode of the \"Big Technology Podcast\" published on Wednesday that using AI tools to build software quickly with natural language prompts will soon feel normal rather than novel. However, focusing on building today's software faster misses the bigger picture.\n\n\"Everyone's looking at all the software use and saying, 'How fast could I vibe code that?'\" Taylor said. \"I wonder if it's the wrong question.\"\n\nWhether someone can quickly vibe code an app in a web browser isn't \"the most interesting question in software,\" he added.\n\nInstead, the software we use today is set to be replaced, and that's the real disruption, Taylor said.\n\nRather than dashboards, web-browser forms, and traditional apps, the structure of software will change. AI agents will be \"the future of software.\"\n\n\"We will delegate tasks to agents that will operate against a database,\" Taylor said.\n\n\"Who's making those agents is the question,\" he added. \"Will you buy those agents off the shelf or build them yourself?\"\n\nTaylor also said that while AI has slashed the cost of building software, it hasn't solved the harder problems of maintaining it ‚Äî or the risk of getting things wrong.\n\n\"That's why most people would prefer to buy a solution off the shelf,\" he said. \"You want to amortize the cost of maintaining software among thousands of clients.\"\n\nVibe coding has taken off across the tech world, but tech leaders said the technology has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that vibe coding is \"making coding so much more enjoyable,\" adding that it allows even non-technical users to create simple apps and websites.\n\nDuring Alphabet's April earnings call, Pichai said AI generates more than 30% of Google's new code, up from 25% in October 2024.\n\nStill, AI-generated code can be error-prone, overly long, or poorly structured.\n\n\"I'm not working on large codebases where you really have to get it right, the security has to be there,\" Pichai said in November.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding works best for prototypes or throwaway code, but not in software that sits at the core of a business.\n\n\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "vibe code",
      "software",
      "agents",
      "isn't",
      "it's",
      "bigger",
      "episode",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chair-vibe-coding-not-endgame-bret-taylor-2026-1",
    "thumbnail_url": "https://i.insider.com/6883b14a85e81483682eb19e?width=1200&format=jpeg",
    "created_at": "2026-01-29T06:34:22.533Z",
    "topic": "finance"
  },
  {
    "slug": "microsoft-cfos-memo-to-staff-calls-out-ai-deals-coding-and-chips",
    "title": "Microsoft CFO's memo to staff calls out AI deals, coding, and chips",
    "description": "CFO Amy Hood sent an internal memo about the results, viewed by Business Insider.",
    "fullText": "After Microsoft reported results on Wednesday, CFO Amy Hood sent an internal memo to employees calling attention to recent developments in AI chips and coding tools, and deals with OpenAI and Anthropic.\n\nHood sends these emails every quarter when Microsoft discloses its financials. Her missives mostly rehash what the company reports publicly, such as how revenue and profit are growing, or what is discussed on analyst earnings calls.\n\nStill, these memos provide insight into what Microsoft executives deem most important, and what they want employees to know.\n\nThe latest memo highlighted how Microsoft is gaining share in markets where the total addressable market is expanding.\n\nHood specifically mentioned the launch of the GitHub Copilot software development kit in the growing market of AI coding tools, and the release of Microsoft's new Maia 200 AI chip.\n\nHood's email also called out Azure commitments from OpenAI and Anthropic that helped increase commercial bookings, basically the deals Microsoft closed in the quarter, by 230%, year over year.\n\nCapital expenditure on computing and datacenter infrastructure also broke yet another quarterly record, reaching $37.5 billion, she also noted.\n\nThis afternoon, we announced our second-quarter financial results. We exceeded Wall Street expectations, growing revenue 17% and 15% in constant currency and operating income by 21% and 19% in constant currency -a strong finish to the first half of the fiscal year.\n\nIn the quarter, Microsoft Cloud revenue surpassed $50 billion for the first time, growing 26% and 24% in constant currency.\n\nThere were many highlights this quarter, but a few stand out as reminders of the value our products and services deliver to customers - and as proof points of the progress we are making:\n\nInvestors tune in to our earnings call for the full details on this quarter and a look ahead to Q3. It's a helpful way to stay aligned as we deliver on our commitments. Join live today at 2:30 PM Pacific, listen on-demand, or check the transcript on the Investor Relations website.\n\nThis quarter's results reflect meaningful progress on core priorities. We continue to add capacity with pace, drive steady efficiency gains across our fleet, and invest in each layer of the stack\n\nAs we enter the second half of the fiscal year, we're operating in markets with expanding TAM where we continue to gain share and you can see our progress in many places, from last week's announcement of the GitHub Copilot SDK to Monday's Maia 200 announcement. We are innovating and delivering together. And we're doing it with the quality and security our customers expect from us. All of this builds trust from customers and partners as they rely on us for mission critical workloads.\n\nThanks again for all your work.\n\nWith appreciation and gratitude,\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "coding tools",
      "constant currency",
      "quarter",
      "revenue",
      "email",
      "customers",
      "progress",
      "memo",
      "employees",
      "deals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/internal-microsoft-cfo-memo-calls-out-ai-deals-coding-and-chips-2026-1",
    "thumbnail_url": "https://i.insider.com/697a82dfa645d11881883093?width=1200&format=jpeg",
    "created_at": "2026-01-29T01:07:05.127Z",
    "topic": "finance"
  },
  {
    "slug": "programming-sucks-2014",
    "title": "Programming Sucks (2014)",
    "description": null,
    "fullText": "Every friend I have with a job that involves picking up something heavier than a laptop more than twice a week eventually finds a way to slip something like this into conversation: ‚ÄúBro,1[1] you don‚Äôt work hard. I just worked a 4700-hour week digging a tunnel under Mordor with a screwdriver.‚Äù\n\nThey have a point. Mordor sucks, and it‚Äôs certainly more physically taxing to dig a tunnel than poke at a keyboard unless you‚Äôre an ant. But, for the sake of the argument, can we agree that stress and insanity are bad things? Awesome. Welcome to programming.\n\nImagine joining an engineering team. You‚Äôre excited and full of ideas, probably just out of school and a world of clean, beautiful designs, awe-inspiring in their aesthetic unity of purpose, economy, and strength. You start by meeting Mary, project leader for a bridge in a major metropolitan area. Mary introduces you to Fred, after you get through the fifteen security checks installed by Dave because Dave had his sweater stolen off his desk once and Never Again. Fred only works with wood, so you ask why he‚Äôs involved because this bridge is supposed to allow rush-hour traffic full of cars full of mortal humans to cross a 200-foot drop over rapids. Don‚Äôt worry, says Mary, Fred‚Äôs going to handle the walkways. What walkways? Well Fred made a good case for walkways and they‚Äôre going to add to the bridge‚Äôs appeal. Of course, they‚Äôll have to be built without railings, because there‚Äôs a strict no railings rule enforced by Phil, who‚Äôs not an engineer. Nobody‚Äôs sure what Phil does, but it‚Äôs definitely full of synergy and has to do with upper management, whom none of the engineers want to deal with so they just let Phil do what he wants. Sara, meanwhile, has found several hemorrhaging-edge paving techniques, and worked them all into the bridge design, so you‚Äôll have to build around each one as the bridge progresses, since each one means different underlying support and safety concerns. Tom and Harry have been working together for years, but have an ongoing feud over whether to use metric or imperial measurements, and it‚Äôs become a case of ‚Äúwhoever got to that part of the design first.‚Äù This has been such a headache for the people actually screwing things together, they‚Äôve given up and just forced, hammered, or welded their way through the day with whatever parts were handy. Also, the bridge was designed as a suspension bridge, but nobody actually knew how to build a suspension bridge, so they got halfway through it and then just added extra support columns to keep the thing standing, but they left the suspension cables because they‚Äôre still sort of holding up parts of the bridge. Nobody knows which parts, but everybody‚Äôs pretty sure they‚Äôre important parts. After the introductions are made, you are invited to come up with some new ideas, but you don‚Äôt have any because you‚Äôre a propulsion engineer and don‚Äôt know anything about bridges.\n\nWould you drive across this bridge? No. If it somehow got built, everybody involved would be executed. Yet some version of this dynamic wrote every single program you have ever used, banking software, websites, and a ubiquitously used program that was supposed to protect information on the internet but didn‚Äôt.\n\nEvery programmer occasionally, when nobody‚Äôs home, turns off the lights, pours a glass of scotch, puts on some light German electronica, and opens up a file on their computer. It‚Äôs a different file for every programmer. Sometimes they wrote it, sometimes they found it and knew they had to save it. They read over the lines, and weep at their beauty, then the tears turn bitter as they remember the rest of the files and the inevitable collapse of all that is good and true in the world.\n\nThis file is Good Code. It has sensible and consistent names for functions and variables. It‚Äôs concise. It doesn‚Äôt do anything obviously stupid. It has never had to live in the wild, or answer to a sales team. It does exactly one, mundane, specific thing, and it does it well. It was written by a single person, and never touched by another. It reads like poetry written by someone over thirty.\n\nEvery programmer starts out writing some perfect little snowflake like this. Then they‚Äôre told on Friday they need to have six hundred snowflakes written by Tuesday, so they cheat a bit here and there and maybe copy a few snowflakes and try to stick them together or they have to ask a coworker to work on one who melts it and then all the programmers‚Äô snowflakes get dumped together in some inscrutable shape and somebody leans a Picasso on it because nobody wants to see the cat urine soaking into all your broken snowflakes melting in the light of day. Next week, everybody shovels more snow on it to keep the Picasso from falling over.\n\nThere‚Äôs a theory that you can cure this by following standards, except there are more ‚Äústandards‚Äù than there are things computers can actually do, and these standards are all variously improved and maligned by the personal preferences of the people coding them, so no collection of code has ever made it into the real world without doing a few dozen identical things a few dozen not even remotely similar ways. The first few weeks of any job are just figuring out how a program works even if you‚Äôre familiar with every single language, framework, and standard that‚Äôs involved, because standards are unicorns.\n\nI spent a few years growing up with a closet in my bedroom. The closet had an odd design. It looked normal at first, then you walked in to do closet things, and discovered that the wall on your right gave way to an alcove, making for a handy little shelf. Then you looked up, and the wall at the back of the alcove gave way again, into a crawlspace of utter nothingness, where no light could fall and which you immediately identified as the daytime retreat for every ravenous monster you kept at bay with flashlights and stuffed animals each night.\n\nThis is what it is to learn programming. You get to know your useful tools, then you look around, and there are some handy new tools nearby and those tools show you the bottomless horror that was always right next to your bed.\n\nFor example, say you‚Äôre an average web developer. You‚Äôre familiar with a dozen programming languages, tons of helpful libraries, standards, protocols, what have you. You still have to learn more at the rate of about one a week, and remember to check the hundreds of things you know to see if they‚Äôve been updated or broken and make sure they all still work together and that nobody fixed the bug in one of them that you exploited to do something you thought was really clever one weekend when you were drunk. You‚Äôre all up to date, so that‚Äôs cool, then everything breaks.\n\n‚ÄúDouble you tee eff?‚Äù you say, and start hunting for the problem. You discover that one day, some idiot decided that since another idiot decided that 1/0 should equal infinity, they could just use that as a shorthand for ‚ÄúInfinity‚Äù when simplifying their code. Then a non-idiot rightly decided that this was idiotic, which is what the original idiot should have decided, but since he didn‚Äôt, the non-idiot decided to be a dick and make this a failing error in his new compiler. Then he decided he wasn‚Äôt going to tell anyone that this was an error, because he‚Äôs a dick, and now all your snowflakes are urine and you can‚Äôt even find the cat.\n\nYou are an expert in all these technologies, and that‚Äôs a good thing, because that expertise let you spend only six hours figuring out what went wrong, as opposed to losing your job. You now have one extra little fact to tuck away in the millions of little facts you have to memorize because so many of the programs you depend on are written by dicks and idiots.\n\nAnd that‚Äôs just in your own chosen field, which represents such a tiny fraction of all the things there are to know in computer science you might as well never have learned anything at all. Not a single living person knows how everything in your five-year-old MacBook actually works. Why do we tell you to turn it off and on again? Because we don‚Äôt have the slightest clue what‚Äôs wrong with it, and it‚Äôs really easy to induce coma in computers and have their built-in team of automatic doctors try to figure it out for us. The only reason coders‚Äô computers work better than non-coders‚Äô computers is coders know computers are schizophrenic little children with auto-immune diseases and we don‚Äôt beat them when they‚Äôre bad.\n\nRemember that stuff about crazy people and bad code? The internet is that except it‚Äôs literally a billion times worse. Websites that are glorified shopping carts with maybe three dynamic pages are maintained by teams of people around the clock, because the truth is everything is breaking all the time, everywhere, for everyone. Right now someone who works for Facebook is getting tens of thousands of error messages and frantically trying to find the problem before the whole charade collapses. There‚Äôs a team at a Google office that hasn‚Äôt slept in three days. Somewhere there‚Äôs a database programmer surrounded by empty Mountain Dew bottles whose husband thinks she‚Äôs dead. And if these people stop, the world burns. Most people don‚Äôt even know what sysadmins do, but trust me, if they all took a lunch break at the same time they wouldn‚Äôt make it to the deli before you ran out of bullets protecting your canned goods from roving bands of mutants.\n\nYou can‚Äôt restart the internet. Trillions of dollars depend on a rickety cobweb of unofficial agreements and ‚Äúgood enough for now‚Äù code with comments like ‚ÄúTODO: FIX THIS IT‚ÄôS A REALLY DANGEROUS HACK BUT I DON‚ÄôT KNOW WHAT‚ÄôS WRONG‚Äù that were written ten years ago. I haven‚Äôt even mentioned the legions of people attacking various parts of the internet for espionage and profit or because they‚Äôre bored. Ever heard of 4chan? 4chan might destroy your life and business because they decided they didn‚Äôt like you for an afternoon, and we don‚Äôt even worry about 4chan because another nuke doesn‚Äôt make that much difference in a nuclear winter.\n\nOn the internet, it‚Äôs okay to say, ‚ÄúYou know, this kind of works some of the time if you‚Äôre using the right technology,‚Äù and BAM! it‚Äôs part of the internet now. Anybody with a couple of hundred dollars and a computer can snag a little bit of the internet and put up whatever awful chunks of hack code they want and then attach their little bit to a bunch of big bits and everything gets a little bit worse. Even the good coders don‚Äôt bother to learn the arcane specifications outlined by the organizations people set up to implement some unicorns, so everybody spends half their time coping with the fact that nothing matches anything or makes any sense and might break at any time and we just try to cover it up and hope no one notices.\n\nHere are the secret rules of the internet: five minutes after you open a web browser for the first time, a kid in Russia has your social security number. Did you A computer at the NSA now automatically tracks your physical location for the rest of your life. Sent an email? Your email address just went up on a billboard in Nigeria.\n\nThese things aren‚Äôt true because we don‚Äôt care and don‚Äôt try to stop them, they‚Äôre true because everything is broken because there‚Äôs no good code and everybody‚Äôs just trying to keep it running. That‚Äôs your job if you work with the internet: hoping the last thing you wrote is good enough to survive for a few hours so you can eat dinner and catch a nap.\n\nFunny, right? No? How about this exchange:\n\nWasn‚Äôt that guy helpful? With the camel? Doesn‚Äôt that seem like an appropriate response? No? Good. You can still find Jesus. You have not yet spent so much of your life reading code that you begin to talk in it. The human brain isn‚Äôt particularly good at basic logic and now there‚Äôs a whole career in doing nothing but really, really complex logic. Vast chains of abstract conditions and requirements have to be picked through to discover things like missing commas. Doing this all day leaves you in a state of mild aphasia as you look at people‚Äôs faces while they‚Äôre speaking and you don‚Äôt know they‚Äôve finished because there‚Äôs no semicolon. You immerse yourself in a world of total meaninglessness where all that matters is a little series of numbers went into a giant labyrinth of symbols and a different series of numbers or a picture of a kitten came out the other end.\n\nThe destructive impact on the brain is demonstrated by the programming languages people write. This is a program:\n\nThat program does exactly the same thing as this program:\n\nAnd once somebody wrote a programming language that let somebody else write this:\n\nAccording to the author, that program is \"two lines of code that parse two lines of embedded comments in the code to read the Mayan numbers representing the individual ASCII characters that make up the magazine title, rendered in 90-degree rotated ASCII art.\"\n\nThat program won a contest, because of course it did. Do you want to live in a world like this? No. This is a world where you can smoke a pack a day and nobody even questions it. \"Of course he smokes a pack a day, who wouldn't?\" Eventually every programmer wakes up and before they're fully conscious they see their whole world and every relationship in it as chunks of code, and they trade stories about it as if sleepiness triggering acid trips is a normal thing that happens to people. This is a world where people eschew sex to write a programming language for orangutans. All programmers are forcing their brains to do things brains were never meant to do in a situation they can never make better, ten to fifteen hours a day, five to seven days a week, and every one of them is slowly going mad.\n\nSo no, I‚Äôm not required to be able to lift objects weighing up to fifty pounds. I traded that for the opportunity to trim Satan‚Äôs pubic hair while he dines out of my open skull so a few bits of the internet will continue to work for a few more days.\n\n(Update: now available in Greek, Czech, Italian, Russian, Portuguese, Hungarian, French, Hebrew (PDF by Ilil Hoz), German (PDF by Kurt Frock), Spanish, and Chinese)",
    "readingTime": 13,
    "keywords": [
      "programming languages",
      "you‚Äôre familiar",
      "programming language",
      "suspension bridge",
      "don‚Äôt",
      "it‚Äôs",
      "internet",
      "they‚Äôre",
      "there‚Äôs",
      "together"
    ],
    "qualityScore": 1,
    "link": "https://www.stilldrinking.org/programming-sucks",
    "thumbnail_url": "https://www.stilldrinking.org/blog_images/programming-sucks.jpg",
    "created_at": "2026-01-28T06:22:46.410Z",
    "topic": "tech"
  },
  {
    "slug": "pixel-arcade-studio-kids-make-playable-browser-games-by-instructing-ai",
    "title": "Pixel Arcade Studio ‚Äìkids make playable browser games by instructing AI",
    "description": "Kids create real browser games by giving clear instructions to AI. No coding, no downloads.",
    "fullText": "They need to learn how to give clear instructions to AI.\n\nPixel Arcade Studio is a browser-based game studio where kids create real, playable games by telling an AI assistant exactly what to build. No coding. No installs. Designed with parents in mind.\n\nSafe, creative screen time kids love. Try free for 14 days.\n\nFast \"time-to-wow\" ‚Äî kids see their games come to life quickly.\n\nFrictionless ‚Äî everything runs directly in the browser.\n\nSafety default ‚Äî games publish with privacy protections enabled.\n\nCoding used to be how people told computers what to do.\n\nToday, the more important skill is knowing how to describe what you want, break ideas into steps, and give clear instructions to an AI system.\n\nPixel Arcade Studio is built around that shift.\n\nKids don't write code here. They practice explaining ideas, testing results, and improving their instructions when something doesn't work.\n\nThat's the skill they'll use in the real world.\n\nYour child picks a game template and describes what they want to make. Characters, goals, movement, and rules.\n\nYour child tells the AI assistant what to create or change. The AI follows instructions. It does not take over.\n\nThe game runs right away in the browser. No setup and no waiting.\n\nKids adjust their instructions, test again, and see how clearer directions lead to better results.\n\nGames can be shared with family or friends using parent-approved links.\n\nThis is not about memorizing technical skills. It's about clear thinking and communication.\n\nAI in Pixel Arcade Studio is a tool, not a shortcut.\n\nIt responds only to what your child asks. It does not browse the internet. It does not publish content on its own. It stays inside kid-safe boundaries.\n\nYour child stays in control. The AI helps carry out instructions.\n\nPixel Arcade Studio is built for families who want creative screen time without constant supervision.\n\nInstead, kids focus on giving clear instructions and seeing real results. They make games people can actually play.\n\nEvery game made in Pixel Arcade Studio is playable in the browser and shareable through safe links.\n\nKids don't just save projects. They create something real and playable.\n\nPixel Arcade Studio is a browser-based game studio where kids create playable games by giving instructions to an AI assistant. There is no coding involved.\n\nNo. Pixel Arcade Studio does not teach coding. Kids learn how to clearly describe ideas, give instructions, and work with AI to create games.\n\nPixel Arcade Studio is designed for kids ages 7 to 12.\n\nYes. Games publish in safe mode by default, sharing requires parent approval, and the platform includes content filtering and privacy protections.\n\nNo. Everything runs directly in the web browser. There are no downloads or installs.\n\nThe AI follows your child's instructions. It helps turn ideas into games but does not take control or act on its own.\n\nYes. Games can be shared using parent-approved links so friends and family can play safely.\n\nPixel Arcade Studio does not involve coding or block-based programming. It focuses on teaching kids how to give clear instructions to AI and refine their ideas through iteration.\n\nNo coding. No downloads. Designed for ages 7‚Äì12.",
    "readingTime": 3,
    "keywords": [
      "pixel arcade studio",
      "creative screen",
      "privacy protections",
      "parent-approved links",
      "browser-based game",
      "kids don't",
      "games publish",
      "playable games",
      "the ai",
      "kids create"
    ],
    "qualityScore": 1,
    "link": "https://pixelarcade.studio",
    "thumbnail_url": "http://localhost:3000/images/pas_og.jpg",
    "created_at": "2026-01-28T06:22:43.763Z",
    "topic": "tech"
  },
  {
    "slug": "acm-sigplan-symposium-on-principles-of-programming-languages-popl-2026-talks",
    "title": "ACM SIGPLAN Symposium on Principles of Programming Languages (POPL) 2026 talks",
    "description": "Special Interest Group on Programming Languages\nThe ACM Special Interest Group on Programming Languages (SIGPLAN) explores programming language concepts and tools, focusing on design, implementation, practice, and theory. Its members are programming language developers, educators, implementers, researchers, theoreticians, and users.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.youtube.com/channel/UCwG9512Wm7jSS6Iqshz4Dpg",
    "thumbnail_url": "https://yt3.googleusercontent.com/ytc/AIdro_mB1glk2WSnakJ9VmIADfpyj122SV1iL5zw1rMBEQFIqQ=s900-c-k-c0x00ffffff-no-rj",
    "created_at": "2026-01-27T18:24:29.546Z",
    "topic": "tech"
  },
  {
    "slug": "agent-skills-from-claude-to-open-standard-to-your-daily-coding-workflow",
    "title": "Agent Skills: From Claude to Open Standard to Your Daily Coding Workflow",
    "description": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we're witnessing something far more...",
    "fullText": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we‚Äôre witnessing something far more significant: an open standard reshaping how people across roles‚Äîdevelopers, designers, product managers, and operations‚Äîwork with AI assistants. AI coding agents‚Äô adoption of Agent Skills has transformed this technology from an interesting experiment into an essential developer tool.\n\nIf you‚Äôve been using custom instructions or wondering how to make your AI assistant truly understand your project‚Äôs workflows, Agent Skills provides a compelling solution.\n\nAgent Skills began with Anthropic‚Äôs Claude AI, where developers first experienced giving AI agents specialized capabilities through structured instructions. Unlike simple prompts or one-off commands, Agent Skills introduced a sophisticated approach: packaging instructions, scripts, templates, and documentation into reusable, discoverable units.\n\nAnthropic‚Äôs decision to release Agent Skills as an open standard transformed it from a Claude-specific feature into a movement. The format‚Äôs simplicity and effectiveness attracted attention across the AI development tools ecosystem. Today, major players‚ÄîClaude Code, GitHub Copilot, Cursor, OpenCode, Mistral Vibe, Antigravity, OpenAI Codex, and Kiro‚Äîhave adopted the standard. Others are exploring integration, and more are joining it (I‚Äôm looking at you, JetBrains Junie).\n\nAgent Skills are elegantly simple: a folder containing a SKILL.md file. This file uses YAML frontmatter for metadata and Markdown for instructions. No complex APIs, no proprietary formats‚Äîjust structured text any AI agent can understand.\n\nHere‚Äôs a basic Agent Skills example for creating NUnit unit tests in C#:\n\nThose Agent Skills files live in your agent configuration, for example for GitHub Copilot .github/skills/create-nunit-unit-test/SKILL.md in your repository. Or they can be globally installed for your user account, e.g. ~/.copilot/skills/create-nunit-unit-test/SKILL.md.\n\nThis is a minimal example. You can add resources like templates, example configurations, or helper scripts in the same directory, and the skill can reference them.\n\nWhat makes Agent Skills innovative isn‚Äôt just the format‚Äîit‚Äôs how AI agents consume them. The system uses a three-level progressive disclosure approach that optimizes context window usage:\n\nLevel 1: Discovery ‚Äî At startup, the agent reads only the name and description from each skill. This lightweight metadata helps the agent understand available capabilities without consuming context.\n\nLevel 2: Activation ‚Äî When your request matches a skill‚Äôs description, the agent loads the full instructions from the SKILL.md file. Only then do detailed procedures become available.\n\nLevel 3: Execution ‚Äî The agent accesses additional files (scripts, examples, documentation) only as needed during execution.\n\nThis architecture solves a critical problem: you can install dozens of Agent Skills without overwhelming the AI‚Äôs context window. The agent loads only what‚Äôs relevant to your current development task.\n\nGitHub Copilot‚Äôs Agent Skills are experimental since December 2025 (version 1.108) for VS Code. Here‚Äôs your step-by-step setup guide:\n\nInstall VS Code ‚Äî Download from code.visualstudio.com\n\nEnable Agent Skills ‚Äî Open settings (Ctrl+,) and enable chat.useAgentSkills\n\nCreate your skills directory ‚Äî In your project root, create .github/skills/\n\nAdd your first skill ‚Äî Create a subdirectory for each skill with its SKILL.md file\n\nUse Agent Mode ‚Äî In Copilot Chat, switch to Agent mode to leverage skills\n\nOnce configured, Agent Skills activate automatically based on your prompts. No manual selection required‚Äîthe AI determines which skills are relevant based on your descriptions.\n\nSkills share knowledge‚Äîbest practices, workflows, and procedural guidance‚Äîin simple Markdown SKILL.md files that anyone can author; they load progressively to conserve tokens, require no server, and run on web, desktop, and CLI, making them ideal for documentation, checklists, examples, and repeatable workflows.\n\nMCP extends functionality by connecting to APIs, databases, and external tools: it consists of code and service/tool definitions that require development and hosting, loads tool definitions up front (consuming more context), so it‚Äôs best suited for tasks needing direct access to external systems.\n\nUse Skills to make knowledge discoverable and consistent, and use MCP to perform integrated actions and extend platform capabilities; together they provide both lightweight guidance and powerful automation.\n\nNevertheless, I can imagine a future where Agent Skills replace MCP for many scenarios, given their simplicity, portability, and ease of authoring. As you can bundle scripts and resources with skills, they can cover many use cases MCP currently serves.\n\nYou might wonder how Agent Skills differ from the custom instructions feature. Custom Instructions are best for defining coding standards and conventions, setting language or framework preferences, specifying code-review guidelines, and establishing commit-message formats. Agent Skills are designed to package reusable workflows, include executable scripts and templates, define specialized procedures (testing, debugging, deployment), and enable capabilities that run beyond the IDE (CLI and coding agents).\n\nThink of custom instructions as your coding style guide and Agent Skills as your AI development toolbox. Custom instructions tell the AI how you want code written; Agent Skills give the AI specialized capabilities to perform complex development tasks.\n\nHere are some practical Agent Skills that can transform your daily development workflow. Check the references section for pointers to more ready-to-use skills:\n\nRead the Agent Skills Specification to understand the format and capabilities. Use Skill Creator, an Agent Skill to create and refine new Agent Skills. Inception moment anyone ü§î?\n\nStart building your Agent Skills collection with these proven strategies:\n\nIdentify Repetitive Tasks ‚Äî Notice which development workflows you explain to the AI repeatedly. Each recurring explanation is a candidate for an Agent Skill.\n\nStart Simple ‚Äî Begin with straightforward skills that codify standard development procedures. As you gain confidence, add scripts and more complex resources.\n\nMake Descriptions Specific ‚Äî The quality of your skill‚Äôs description directly impacts how well the AI knows when to activate it. Be explicit about use cases and capabilities.\n\nInclude Examples ‚Äî Agent Skills with concrete code examples are more effective. Show the AI what good output looks like.\n\nLeverage Community Skills ‚Äî Explore the github/awesome-copilot and anthropics/skills repositories for inspiration and ready-to-use skills.\n\nOrganize by Domain ‚Äî Group related Agent Skills together. Create separate skills for testing, deployment, documentation, code review, and other specialized development domains.\n\nHere‚Äôs how Agent Skills enhance your workflow in a typical development scenario:\n\nYou‚Äôre working on a web application and need to add a new REST API endpoint with proper testing and documentation. With appropriate Agent Skills in place:\n\nYou ask: ‚ÄúHelp me add a new user registration endpoint with validation‚Äù\n\nThe rest-api-integration skill activates, providing structured guidance on implementing the endpoint with proper authentication, validation, and error handling.\n\nYou ask: ‚ÄúCreate tests for this endpoint‚Äù\n\nThe webapp-testing skill engages, generating test cases for success scenarios, validation failures, and edge cases.\n\nYou ask: ‚ÄúGenerate documentation for this endpoint‚Äù\n\nThe api-documentation skill activates, producing comprehensive documentation with examples, error codes, and authentication details.\n\nEach Agent Skill ensures consistency in approach and completeness in implementation. Without skills, you‚Äôd need to provide detailed instructions for each request or rely on the AI‚Äôs general knowledge, which might miss project-specific patterns.\n\nWhen working with Agent Skills, especially community-shared skills, keep these security considerations in mind:\n\nReview Before Use ‚Äî Always examine shared Agent Skills before adding them to your project. Check for potentially malicious scripts or unexpected behaviors in the SKILL.md file and associated resources.\n\nUse Terminal Controls ‚Äî VS Code‚Äôs terminal tool provides controls for script execution, including auto-approve options with configurable allow-lists. Configure these appropriately for your security requirements.\n\nVersion Control Your Skills ‚Äî Agent Skills are just files, so commit them to your repository. This enables code review, versioning, and team collaboration on AI capabilities.\n\nTest in Safe Environments ‚Äî Try new Agent Skills in development environments before using them in production contexts. Dev containers or isolated workspaces are ideal for testing.\n\nDocument Team Skills ‚Äî If your team uses shared Agent Skills, maintain documentation about what each skill does and when to use it.\n\nAgent Skills represent more than a new feature‚Äîit‚Äôs a glimpse into a future where AI development capabilities are portable, shareable, and composable. As more AI tools adopt the standard, we‚Äôre moving toward an ecosystem where:\n\nWhether you‚Äôre using VS Code or any other editor/IDE, working in the terminal with Copilot CLI, or leveraging any coding agent for automated tasks, your Agent Skills come with you.\n\nReady to integrate Agent Skills into your development workflow? Follow this action plan:\n\nThe goal isn‚Äôt to create dozens of Agent Skills immediately. Start with one or two that solve real problems in your development workflow, then expand your library organically as needs arise.\n\nYou can also use Agent Skills with GitHub Copilot CLI or Gemini CLI for terminal-based workflows, or with other coding agents that support the open standard. This portability ensures your investment in creating skills pays off across all your AI-assisted development tools.\n\nMy preferred introduction to Agent Skills is the following video from Burke Holland, which covers the concepts, setup, and practical examples in under 20 minutes:\n\nFor my French readers, I discussed Agent Skills in depth on devdevdev.net in the following episode\n\nAgent Skills bridges the gap between generic AI assistance and specialized, context-aware support for your specific development needs. By adopting an open standard that works across AI tools, the industry has created a foundation for truly portable AI capabilities.\n\nThe journey from Claude to open standard to GitHub Copilot adoption demonstrates the power of simplicity and interoperability in developer tools. As developers, we benefit from this ecosystem approach‚Äîour investment in creating Agent Skills pays dividends across our entire development toolchain.\n\nStart small, experiment with the format, and build Agent Skills that improve your daily development work. The progressive disclosure system ensures you won‚Äôt overwhelm your AI assistant, and the portable format guarantees your skills remain valuable as AI tools evolve.\n\nThe future of AI-assisted development isn‚Äôt just about more powerful models‚Äîit‚Äôs about giving those models the right context, capabilities, and knowledge to be genuinely helpful in your specific development domain. Agent Skills is a significant step in that direction.\n\nWhat development workflows could benefit from specialized Agent Skills? Have you tried creating skills for your AI coding assistant? Share your experiences in the comments below.",
    "readingTime": 9,
    "keywords": [
      "agent skills",
      "skill.md file",
      "progressive disclosure",
      "ai-assisted development",
      "skill‚Äôs description",
      "context window",
      "agent mode",
      "shared agent",
      "coding agents",
      "skill activates"
    ],
    "qualityScore": 1,
    "link": "https://laurentkempe.com/2026/01/27/Agent-Skills-From-Claude-to-Open-Standard/",
    "thumbnail_url": "https://live.staticflickr.com/65535/55058424290_cced09531e_h.jpg",
    "created_at": "2026-01-27T12:26:46.640Z",
    "topic": "tech"
  },
  {
    "slug": "one-developer-used-claude-to-build-a-memorysafe-extension-of-c",
    "title": "One developer used Claude to build a memory-safe extension of C",
    "description": "feature: Robin Rowe talks about coding, programming education, and China in the age of AI",
    "fullText": "feature TrapC, a memory-safe version of the C programming language, is almost ready for testing.\n\n\"We're almost there,\" Robin Rowe told The Register in a phone interview. \"It almost works.\"\n\nWe caught up with Rowe, a computer science professor and entrepreneur, amid debugging efforts that had kept him up until four in the morning. The long-awaited TrapC website has appeared.\n\n\"My work building TrapC has taken two parallel paths,\" Rowe explains in his initial post. \"A TrapC interpreter called itrapc and a separate compiler called trapc. I had wanted to make a software release by 1 January 2026, but too many bugs. I only reached code complete this month and am now on the painstaking and sleepless process of debugging. When I have something stable that mostly works I will make a release. Sorry to make you wait a little longer. Aiming for Q1 2026.\"\n\nBack in November 2024, Rowe explained that he was working on TrapC. At the time, the public and private sector had undertaken a campaign to promote memory-safe software development as a way to reduce exposure to serious vulnerabilities.\n\nMemory safety provides a way of ensuring that memory-related bugs like out-of-bounds reads/writes and use-after-free don't happen. In large codebases, like Chromium and Windows, most of the security vulnerabilities follow from memory bugs. As that message has been repeated in recent years, memory safety has become an imperative, evangelized by the likes of Google and Microsoft, and more recently by authorities in the US and elsewhere.\n\nFor at least the past ten years, there's been a rising chorus of voices calling for the adoption of memory-safe programming languages and techniques. This has meant encouraging developers to avoid languages like C and C++ where feasible, and to adopt languages like C#, Go, Java, Python, Swift, and Rust, instead, particularly for new projects.\n\nTo remain relevant, the C and C++ communities have tried to address those concerns with projects like TrapC, FilC, Mini-C, Safe C++, and C++ Profiles. There's also a C to Rust conversion project under development at DARPA called TRACTOR ‚Äì TRanslating All C TO Rust.\n\nBut progress has been slow and those writing in C and C++ haven't found a widely accepted approach. The C++ standards committee recently rejected the Safe C++ proposal. And Rowe said he doubted TRACTOR would have anything to show this year.\n\nMeanwhile, the clock is ticking. Microsoft engineer Galen Hunt last month said, \"My goal is to eliminate every line of C and C++ from Microsoft by 2030. Our strategy is to combine AI and algorithms to rewrite Microsoft's largest codebases.\"\n\n\"There are some efforts to port C code by hand to Rust,\" said Rowe. \"But there're some real challenges to doing that because there are some idioms in C that cannot be expressed in Rust.\n\n\"Rust is much more type safe than C is. And so if you have a void pointer, what does that mean in Rust? There's no translation for it. And that's how TrapC is fundamentally different because it actually remembers what that void pointer actually is.\"\n\nRowe said he expects TRACTOR will eventually be able to accomplish C to Rust translation using AI. But he said he thinks it's better to just build the necessary tooling into the C compiler, so you don't have to rely on some external tool that rewrites your code in an unfamiliar language.\n\nRowe has been using AI tools himself and has been teaching others to do so. This past semester, he taught AI Cybersecurity Programmer Analyst (PCO471) at Community College of Baltimore County ‚Äì Linux administration using vibe coding in bash with no prerequisites. And starting in February, he's teaching C++ Programming with Generative AI (PCO472) ‚Äì vibe coding in C++.\n\nRowe said programming has fundamentally changed as a result of AI tools. \"I think this is sort of the same type of discussion as when C came in and people said, 'Well, I'm happy in assembly.' There will still be people doing it the old way. But because vibe programming is so much more efficient on time when done correctly, there's gonna be no choice. You just won't be competitive if you're not vibe programming.\"\n\nThen he shifted gears, slightly. \"But I have to walk that back a little bit because the reason I was up until four in the morning is I had vibe programming working on the Trap C compiler. And it took a fundamentally wrong design turn. And I didn't detect that it had made a design mistake. I had told it how I wanted to approach it. But somehow it misunderstood me or it forgot or something happened and I forgot to check. And so I spent hours doodling around in the debugger and trying to understand why code was acting weird before I finally looked at it and said, 'wait a minute, this isn't even the right design.'\"\n\nRowe said a similar situation crops up in pair programming, where you've told someone to do something and they didn't do it, and you don't realize that until later.\n\n\"[C++ creator] Bjarne Stroustrup famously said that the most important thing in software design is to be clear about what you're trying to build,\" Rowe said. \"And vibe [programming] just puts that on steroids. Now we not only have to be ourselves clear, but we have to communicate it clearly to an LLM.\"\n\nRowe argues that developers have to be encouraged to try AI tools and to make mistakes. He recounted how during his AI Cybersecurity Programmer Analyst course, his students expressed interest in doing more hands-on work in lieu of lectures.\n\n\"So I said, 'I've got real servers on the internet that are my companies. I'll give you root,'\" he recalled. \"I'll set loose students who know nothing on my own servers and hope for the best and we'll see how this goes. And the reaction was panic. I couldn't get past the timidity cliff.\"\n\nRowe said that what he learned from that exchange was that they didn't want their own hands-on, they wanted to watch him work.\n\n\"I said to them, 'But guys, this is like learning to play the piano. You can't learn to play the piano by watching me. Yeah, you guys have to practice. And it's gonna be embarrassing at first. You know, you're gonna play a lot of bad notes and sound terrible. You have to get over that situation'.\"\n\nThat's a scenario playing out in various companies where AI tools remain underutilized, for various reasons, including lack of training, security concerns, lack of utility, and poor tool design.\n\nRowe has traveled often to China to speak at the China Association of Higher Education conference. In December, he said, he was interviewed on China News Television about how China's plan for AI compares with America's.\n\nIn an email he explained, \"I said, 'China's AI-Plus plan calls for efficient AI on devices everywhere, from farm to factory to city, while the White House plan calls for building 500-billion-dollar cloud data centers ... using chips that will, inevitably, seem obsolete within two years.'\"\n\nRowe argues China's approach will prevail and that the US has taken the wrong turn by focusing on centralized cloud datacenters to run LLMs. Within two years, he said, we'll have AI models we can run locally on our phones, with no need for network access for most tasks. Apple and Huawei, he said, are likely to be the winners in this scenario.\n\nRowe pointed to China's DeepSeek as an example. While it may not be quite as good as the leading US commercial models, he said, it runs with far less power.\n\n\"This is a very Moore's Law type of strategy,\" he said. \"I remember when I had a Navy supercomputer in 1994. That was an amazing technology. But in 1995, Cray went bankrupt. There weren't enough buyers for it, even though it was an amazing device.\n\n\"And now I've got an iPhone that's in my pocket. That runs on a battery. It doesn't have a whole room devoted to it and exotic cooling and all kinds of stuff. And it's more powerful than that [the Cray from 1994]. So as a long-term strategy, you know, going toward the device makes a lot more sense, because that half-trillion dollar data center is going to be on my iPhone eventually.\"\n\nRowe also said that on the recommendation of a friend from his time at the AT&T DIRECTV Innovation Lab, he tried running Deepseek at a time when Claude wasn't available. Deepseek, he said, was able to find a bug that Claude couldn't.\n\n\"Surprisingly, the bug was in code Claude had generated, that I had cut-and-pasted carelessly,\" he said. \"With hindsight it was a silly code mistake I should have caught, but was in an 'else' branch outside where I was looking. I'd not expected or intended to have Claude make any change to that block of code. And because the code was valid but the logic wrong, the compiler didn't catch it.\"\n\nBut the bug was obvious, he said, as soon as Deepseek pointed it out. He added, \"I'm paying $200/year for Claude. Deepseek is free.\" ¬Æ",
    "readingTime": 8,
    "keywords": [
      "cybersecurity programmer",
      "programmer analyst",
      "rowe argues",
      "void pointer",
      "memory safety",
      "vibe coding",
      "design rowe",
      "vibe programming",
      "code",
      "compiler"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/26/trapc_claude_c_memory_safe_robin_rowe/",
    "thumbnail_url": "https://regmedia.co.uk/2022/03/23/shutterstock_c.jpg",
    "created_at": "2026-01-27T06:21:27.835Z",
    "topic": "tech"
  },
  {
    "slug": "a-developer-teamed-up-with-claude-to-create-elo-programming-language",
    "title": "A developer teamed up with Claude to create Elo programming language",
    "description": "feature: Bernard Lambeau, the human half of a pair programming team, explains how he's using AI",
    "fullText": "feature Bernard Lambeau, a Belgium-based software developer and founder of several technology companies, created a programming language called Elo with the help of Anthropic's Claude Code.\n\nStarting on December 25, 2025, he published a series of posts about the project. The first post names Claude as a co-author.\n\n\"In roughly 24 hours of collaboration, we built a complete expression language with a parser, type system, three compilers, a standard library, a CLI tool, and a documentation website. Not bad for a day's work,‚Äù Lambeau and Claude wrote.\n\n\"Elo isn't just a demonstration that AI can write code. It's a demonstration that humans and AI can build together ‚Äì each contributing what they do best,‚Äù they added.\n\nAs an expression language that compiles to JavaScript, Ruby, and SQL, Elo is intended as a portable way to handle form validation, e-commerce order processing, and subscription logic.\n\nLambeau, founder and CTO of Klaro Cards and CEO of app consultancy Enspirit, is not the first to develop a programming language with the help of AI.\n\nSteve Klabnik performed a similar feat last year with the Rue programming language. In September 2025, Geoffrey Huntley enlisted Claude to write a programming language called Cursed. And before that, Avital Tamir published a Claude-authored repo for the Server programming language, with the caveat that the code is not intended for actual use.\n\nClaude Code isn't the only AI-assisted programming method having a moment. AI biz Cursor created a rudimentary browser using OpenAI's GPT-5.2. And developer Ola Pr√∏is used Cursor, powered by Claude, to create a Rust-based text editor called Ferrite.\n\nClaude users generally acknowledge that their pair partner makes mistakes. But those committed to AI assistance find it worthwhile to clean up after their helper.\n\n\"Claude Code knows almost every tech stack (and can search the web), knows the Linux commands that matter (search code, search & replace, compile, test, etc.), and does that 10x faster than I can do myself,\" Lambeau told The Register in an email interview.\n\nClaude, he said, allows him to use technology he hasn't mastered.\n\n\"I was already a full-stack developer (on languages, frameworks & reusable libraries I knew); I'm now a full-stack++ dev because I can also use languages, frameworks, and reusable libraries I barely know, if at all,\" he explained.\n\n\"Claude Code falls short if you don't have a great methodology. It needs feedback loops to work fine; otherwise, it derails. One possible feedback loop is a human reviewing code and testing manually. But there's a better/complementary approach if you want it to work autonomously. On both Elo and Bmg.js, I've started by making sure the testing methodology was effective and scientifically sound. Claude writes the tests, executes them, discovers where it's wrong, and corrects itself. Impressive.\"\n\nLambeau said he still needs to review some of Claude's output.\n\n\"But if I read the tests, agree with them, and can check myself that they run fine, I'm 95 percent sure it's already correct as a black box (not even reading the code),\" he explained. \"Then I can check the architecture and code quality as a white box by having a general look at the code, but I don't have to understand every detail.\"\n\nNotably, Lambeau documented the series of prompts he used to create the language. The repo includes more than 100 tasks used to direct the AI model. In addition, Lambeau has published a video that describes his AI pair programming process.\n\n\"I started in a setting where Claude Code asked for permissions every 20 seconds and I was checking everything it did,\" Lambeau explained. \"After a few successes, I quickly set up safe environments to be able to let Claude Code run in full autonomy (isolated computer & isolated Linux user, or running in a Docker image).\"\n\nLambeau said he still uses plan mode for complex tasks that require conversation with Claude.\n\n\"I review the plan, make sure we have a test strategy that's sound, then switch Claude to autonomous mode and look at the tests, code & results afterward,\" he said. \"That's very similar to a lead-dev/CTO + QA role, btw; it's just much faster than with human devs.\"\n\nLambeau, who has a PhD in software engineering and 30 years of experience as a developer, said both experts and novices can benefit from Claude Code, though he added that a service like Lovable might be more approachable for those not already acclimated to the command line.\n\n\"Now, when it comes to real software/product engineering, I think Claude Code requires experts (so far),\" he said. \"You still need to guide it a lot to keep the quality high enough. You need very strong expertise to do it effectively. Currently (Claude will still improve a lot), if you don't have the expertise, you certainly end up with a big mess of unmaintainable code.\"\n\nMany developers have said as much about AI tools. They're more useful as an amplifier of expertise than as a replacement for it. The situation is analogous to the introduction of sequencing software, digital synthesizers, and drum machines half a century ago. These tools enabled a lot of people who weren't great musicians to make music. But they didn't instill musical skill, and they produced the most interesting work in the hands of practiced musicians.\n\nThe cost to do this, Lambeau said, has been a Claude Max subscription that he purchased in December for ‚Ç¨180 a month. In that time, he says, he wrote Elo (https://elo-lang.org), completed Bmg.js (https://github.com/enspirit/bmg.js), completed Bmg's documentation (https://www.relational-algebra.dev), and created the first version of the Try page (https://www.relational-algebra.dev/try).\n\n\"It's all personal research and open-source projects,\" he said. \"It would have required several weeks to do the same manually myself, and several months to ask another developer to do it. The cost would be mostly because of the scientific & technical knowledge transfer about the data language I envision. Strangely enough, it's very cheap with Claude Code. There's something true about the fact that those LLMs have a PhD.\"\n\nLambeau explained that Elo isn't just a way to test Claude Code. He also sees it as an extension of his academic work in software engineering and his personal interest in the Relational Model ‚Äì he's served as a lecturer for database courses at Belgium‚Äôs UCLouvain.\n\n\"I'm absolutely convinced that we need better/safer/simpler programming languages inside no-code tools and when interconnecting them (e.g. Zapier, Make, n8n, etc.),\" he said. \"Mainstream programming languages are very complex, error-prone, sometimes dangerous, and the programs are difficult to review for non-experts.\"\n\n\"More importantly, they are cumbersome to use for even simple data tasks. I mean, even validating the schema and constraints of a data file at runtime tends to be a nightmare in existing languages. It's not built-in in any mainstream language; you immediately need validation libraries; most of them are limited in what they can easily check, so you need to add dedicated boilerplate code.\"\n\nIn a world where non-technical people will have the opportunity to write untrustworthy code with the help of AI, he said, we need to be able to run that code safely.\n\n\"Elo aims at providing a safe & simple alternative,\" he said. \"It will be a limited language (non-Turing-complete, as we say) but super safe & simple, and usable in 80 percent of common data use cases. The very first no-code tool to integrate it will be Klaro Cards, of course.\" ¬Æ",
    "readingTime": 7,
    "keywords": [
      "claude code",
      "elo isn't",
      "reusable libraries",
      "safe simple",
      "languages frameworks",
      "software engineering",
      "expression language",
      "programming language",
      "it's",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/24/human_ai_pair_programming_elo/",
    "thumbnail_url": "https://regmedia.co.uk/2025/11/06/shutterstock_balancing_ai_and_humanity.jpg",
    "created_at": "2026-01-26T01:03:11.981Z",
    "topic": "tech"
  },
  {
    "slug": "5-acquisitions-winning-over-skeptical-engineers-and-spending-tens-of-millions-inside-a-public-companys-ai-native-push",
    "title": "5 acquisitions, winning over skeptical engineers, and spending tens of millions: Inside a public company's 'AI native' push",
    "description": "Amplitude gave Business Insider the inside look at its AI overhaul, from acquisitions to efforts to increase staff adoption of AI coding assistants.",
    "fullText": "There's a long banner hanging in Amplitude's San Francisco office. It reads: \"NO MAGICAL THINKING.\"\n\nNo, it's not some rag on Joan Didion. It's a reminder, CEO Spenser Skates told Business Insider, that technology can never replace deep thinking and hard work. In the AI age, that reminder is more important than ever ‚Äî so much so that employees must look up at it every day.\n\nAmplitude, an 800-person, publicly traded analytics company, is undergoing an AI transformation ‚Äî with the goal of reinvigorating its business.\n\nAmplitude went public in September 2021 at the height of the pandemic, climbing to an all-time closing high of $84.80 per share several weeks later before dropping significantly and largely plateauing in in recent years around $10. It closed at $10.25 on Friday.\n\nSince October 2024, the company has acquired five AI startups. Amplitude hired an AI-savvy engineering head and appointed one of its acquired founders to a new AI leadership position. It got Cursor and GitHub Copilot licenses for employees, and ran a heads-down AI week.\n\nIt's a change many companies are making: Rapidly moving from little-to-no AI to trying to become \"AI native,\" a term that's curiously hard to pin down. Large language models are popping up everywhere in white-collar work as companies chase the promise of efficiency gains.\n\nAmplitude's case may be especially informative, given just how skeptical of AI its CEO was. In 2023 and some of 2024, Skates said he viewed the AI industry as full of \"grifters,\" the visionaries promising to end world hunger and salesmen promising to automate everything.\n\n\"It had all sorts of problems,\" Skates said. By mid-2024, he realized \"there's probably going to be a breakthrough in the analytics space in the next two or three years.\"\n\n\"We've got to go make that ourselves,\" he said. \"So, we went all in.\"\n\nSkates had two opening moves for his AI overhaul.\n\nThe first: hiring a new chief engineering officer with a history in AI. Wade Chambers had advised the company since 2016, while holding leadership roles at Twitter and Included Health.\n\nWhen Chambers joined in October 2024, only 1% of the engineering, product, and design teams at Amplitude were using AI.\n\nThe second was the acquisition of Command AI, a chatbot startup. It was the first of a string of acquisitions, including June, Kraftful, and Inari. Amplitude announced its most recent acquisition, InfiniGrow, on January 14.\n\nYana Welinder was CEO of Kraftful, one of Amplitude's acquisition targets. Kraftful could spot power users of its product, one of whom was Amplitude's then-CPO. She reached out, and they chatted in February. The deal closed in July, and Welinder was named Amplitude's head of AI. A company blog post with an introductory Q&A referred to her as \"AI maven.\"\n\nWelinder's first order of business: speeding the company up. Kraftful shipped new product every week. Amplitude was shipping less than monthly.\n\n\"If you have this cadence of shipping infrequently, then the team slows down, which isn't appropriate in the age of AI,\" she said.\n\n\"Analytics will look very different 6 months from now,\" Skates wrote in his email. \"We have the opportunity to be the AI native company in Analytics and we are going to pull every piece of firepower we have.\"\n\nHe also asked employees to share a coming launch on X, as opposed to LinkedIn, because that's \"where the AI natives are.\"\n\nHow much has Amplitude spent on AI, from tools to acquisitions? \"Tens of millions, for sure,\" Skates said. \"I wouldn't be surprised if it got past $100 million.\"\n\nThen comes the harder part: convincing employees to really use the tools.\n\nWhile some engineers are excited about AI's promise, others are skeptical about its helpfulness, or worried about possible job losses. Not every engineer is as gung ho about AI as their management is.\n\nSkates said that engineers were especially sensitive to the \"grifting\" that went on in AI, making many of them skeptical. With a bottoms-up approach, that skepticism dissipates, he said.\n\nSoon after joining, Chambers began planning an \"AI week\" for the first week of June. It took six months of prep and borrowed heavily from Facebook's mobile push. He took the entire engineering, product, and design team offline for the week. To kick off, Chambers required that leaders get onstage and vibe-code something in front of the entire company.\n\n\"It didn't go well,\" Chambers said of the live vibe-coding demonstration. \"They had to work through it. They had to re-prompt a couple of different ways.\"\n\nBut the message stuck, he said. Leaders who weren't coding all day were able to build something \"pretty cool\" within the hourlong session, save a few hiccups.\n\nAdditional momentum came from the \"zealots,\" engineers passionate about exploring the new tech (some of whom Chambers brought over from his prior job). These engineers lead by example, he said.\n\nAmplitude shared its internal data tracking how many employees use its AI tools. In the final week of March, 14 employees were actively using Cursor. That figure peaked in the first week of December ‚Äî after AI week but before the holiday vacation cycle ‚Äî at 174 employees.\n\nAnd what of the thorny question about AI implementation in the enterprise: ROI? After all, a 2025 MIT study indicated 95% of firms publicly disclosing use of AI pilots reported no measurable ROI.\n\nAfter implementing these tools, developer productivity shot up 40% and stayed there, Chambers said. On some specific engineering teams, those gains looks more like 300-400%, he said.\n\n\"There's going to be a lot of people who are thinking they're the world's best expert at something,\" Chambers said. \"Increasingly, even the most cynical team members have come around.\"",
    "readingTime": 5,
    "keywords": [
      "engineering product",
      "roi after",
      "employees",
      "analytics",
      "tools",
      "engineers",
      "there's",
      "skeptical",
      "acquisition",
      "team"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amplitude-ai-native-push-2026-1",
    "thumbnail_url": "https://i.insider.com/69729d28d3c7faef0eccc73a?width=1200&format=jpeg",
    "created_at": "2026-01-25T12:22:41.686Z",
    "topic": "finance"
  },
  {
    "slug": "skget-another-cli-to-add-skills-to-your-coding-agents",
    "title": "Skget, another CLI to add skills to your coding agents",
    "description": "A CLI to add skills to your coding agents. Contribute to czheo/skget development by creating an account on GitHub.",
    "fullText": "czheo\n\n /\n\n skget\n\n Public\n\n A CLI to add skills to your coding agents.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n czheo/skget",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/czheo/skget",
    "thumbnail_url": "https://opengraph.githubassets.com/ec41d6a0818a69873753057252cfb3dd2acf4c9944a55eb817aea02d4c36721f/czheo/skget",
    "created_at": "2026-01-25T01:04:24.594Z",
    "topic": "tech"
  },
  {
    "slug": "five-ways-people-are-using-claude-code",
    "title": "Five Ways People Are Using Claude Code",
    "description": "Claude Code generates computer code when people type prompts, so those with no coding experience can create their own programs and apps.",
    "fullText": "Claude Code, an artificial intelligence tool that can generate computer code when people type a prompt, is having a viral moment.\n\nThe tool, which the A.I. start-up Anthropic introduced in May, has shown record growth over the past two weeks, the company said, without sharing its data. People had time to experiment with Claude Code over the holidays, Anthropic said, and users realized how capable it was.\n\nClaude Code is one of several A.I. coding tools ‚Äî which also include Base44 and Cursor ‚Äî that people with no coding experience are increasingly using to build their own websites, programs and apps, a trend known as ‚Äúvibecoding.‚Äù People pay a subscription fee of $20 to $200 a month to use Claude Code, depending on the features they want.\n\nHere are five ways that people are using Claude Code:\n\nMr. Hindes, an assistant principal at a school for autistic children, has four children under the age of 9 and turned to A.I. to help him organize his family‚Äôs laundry.\n\nLast week, he prompted Claude Code to make a program to identify which clothes belonged to each of his three daughters so he could sort clean laundry into piles without their help. He took pictures of their clothes to teach Claude Code which T-shirt belonged to which daughter. Now he simply holds up the clothes to his laptop camera so the program tells him whom it belongs to.\n\n‚ÄúThe whole process was done within an hour, and the girls were really excited,‚Äù he said.\n\nMr. Hindes said he was now building a program with Claude Code to help his daughters independently work though the steps of their morning routine, as if playing a game.\n\n‚ÄúI‚Äôve tried to teach myself coding at various points but never stuck with it,‚Äù he said.\n\nMr. Stephenson, an art and architecture photographer, started using Claude Code in November to build a website about a documentary feature.\n\nThe website was created in about a day, he said, with an interactive map of New York City that captured his photos and audio recordings to document life in each borough.\n\n‚ÄúOnce the basic site was done, emboldened by my new capabilities, I started adding features I hadn‚Äôt even considered,‚Äù said Mr. Stephenson, who pays $20 a month for Claude Code. ‚ÄúLight/dark mode? Easy. Shuffle button? Done.‚Äù\n\nIf Claude Code could not solve a particular problem, he turned to Google‚Äôs A.I. chatbot, Gemini, to ask how it would approach the issue.\n\n‚ÄúI‚Äôd envisioned something like this when I started a couple of years ago, but assumed it would cost thousands of dollars to build,‚Äù he said.\n\nMr. Roberts, an assistant prosecuting attorney, used Claude Code and Cursor in August to create a mobile app called AlertAssist, which lets users send a mass text to contacts in an emergency. Working in law enforcement got Mr. Roberts interested in trying to help people act quickly and safely in an emergency.\n\nThe design and user interface of the app are ‚Äúvery basic, but it works,‚Äù he said.\n\nDuring the coronavirus pandemic, Ms. Haubo Dyhrberg, an assistant professor of finance at the University of Delaware, had an idea to make a stock trading simulator for her class. She consulted her husband, a software engineer, but ‚Äúthe task seemed too daunting.‚Äù\n\nOn Monday, she downloaded Claude Code and within two hours had a working demo of a trading simulator that her students could use to trade securities in a mock market. She has built five different trading scenarios for students to explore various challenges in financial markets.\n\n‚ÄúI never thought it would be this easy,‚Äù she said. ‚ÄúI can‚Äôt wait to test it out when the semester starts in two weeks.‚Äù\n\nMr. Bacus, who owns a welding and metal fabrication business, tapped Claude Code last month to create an A.I. assistant to manage his calendar and find him new business opportunities. The business is just him and three others, so ‚Äúwe‚Äôre not in the place right now to afford an office team,‚Äù Mr. Bacus said. ‚ÄúIt‚Äôs all on me.‚Äù\n\nWith Claude Code, he built a personal A.I. assistant that connects to his calendar, Google Sheets and Gmail account so he can easily create estimates, track the progress of jobs and organize contracts.\n\n‚ÄúI‚Äôm a skilled laborer who barely passed high school in the early 2000s,‚Äù Mr. Bacus said, adding: ‚ÄúBut over the past few months, I‚Äôve taught myself to build actual tools for my business.‚Äù",
    "readingTime": 4,
    "keywords": [
      "claude code",
      "a.i assistant",
      "trading simulator",
      "mr bacus",
      "mr stephenson",
      "mr roberts",
      "business",
      "coding",
      "program",
      "clothes"
    ],
    "qualityScore": 1,
    "link": "https://www.nytimes.com/2026/01/23/technology/claude-code.html",
    "thumbnail_url": "https://static01.nyt.com/images/2026/01/21/multimedia/CLAUDE-CODE-1-pztw/CLAUDE-CODE-1-pztw-facebookJumbo.jpg",
    "created_at": "2026-01-24T00:56:46.566Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-kills-open-source",
    "title": "Vibe Coding Kills Open Source",
    "description": "Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.15494",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-23T06:20:32.202Z",
    "topic": "tech"
  },
  {
    "slug": "gemini-cli-code-and-create-with-an-opensource-agent",
    "title": "Gemini CLI: Code and Create with an Open-Source Agent",
    "description": "Build real-world applications from the command line using Gemini CLI, Google's open-source agentic coding assistant that coordinates local tools and cloud services to automate coding and creative workflows.",
    "fullText": "Join this short course on Gemini CLI, taught by Jack Wotherspoon, Developer Advocate at Google.\n\nGemini CLI is an open-source agentic coding assistant that works from your terminal, giving it access to your local filesystem, development tools, and cloud services. This lets you delegate complex workflows‚Äîfrom building web features to creating marketing materials‚Äîthrough high-level instructions while the agent autonomously plans and executes multiple steps.\n\nIn this course, you‚Äôll apply Gemini CLI to software development and creative tasks by building features for an AI conference. You‚Äôll develop a website session catalog, create a data dashboard combining local and cloud data sources, and generate social media content from recordings. You‚Äôll master context management, integrate MCP servers, and orchestrate across multiple services with Gemini CLI extensions.\n\nWhether you‚Äôre prototyping applications, automating development workflows, or studying topics in agentic AI, this course gives you hands-on experience coordinating multiple tools to build faster and work more efficiently.",
    "readingTime": 1,
    "keywords": [
      "gemini cli",
      "course",
      "development",
      "you‚Äôll",
      "agentic",
      "tools",
      "cloud",
      "services",
      "features"
    ],
    "qualityScore": 0.65,
    "link": "https://learn.deeplearning.ai/courses/gemini-cli-code-and-create-with-an-open-source-agent/information",
    "thumbnail_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2026/01/Gemini-CLI-Code-Create-with-an-Open-Source-Agent_Banner_1920x1080__with_CTA.webp",
    "created_at": "2026-01-21T18:30:41.465Z",
    "topic": "tech"
  },
  {
    "slug": "vibebin-incuslxcbased-platform-for-selfhosting-persistent-sandboxes",
    "title": "Vibebin: Incus/LXC-based platform for self-hosting persistent sandboxes",
    "description": "vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh).  Create...",
    "fullText": "jgbrwn\n\n /\n\n vibebin\n\n Public\n\n vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh). Create and host your vibe-coded apps on a single VPS/server.\n\n License\n\n View license\n\n 9\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jgbrwn/vibebin",
    "readingTime": 1,
    "keywords": [
      "vibebin",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jgbrwn/vibebin",
    "thumbnail_url": "https://opengraph.githubassets.com/b08d7ce148c865871d7a69b11f4c233a6cd39949454bb9455bb3bdcf87a6a575/jgbrwn/vibebin",
    "created_at": "2026-01-21T12:26:57.995Z",
    "topic": "tech"
  },
  {
    "slug": "pragmatic-notes-on-running-dangerous-ai-coding-agents-in-cloud-vms",
    "title": "Pragmatic Notes on Running Dangerous AI Coding Agents in Cloud VMs",
    "description": "A practical approach to safely running AI coding agents with strong isolation using cloud VMs, Tailscale, and simple notification patterns.",
    "fullText": "Running coding agents with free reign is very powerful for a certain class of tasks, especially ones that require little human supervision, or where you want to close (or disconnect) your laptop, walk away, and come back to results.\n\nRecently there have been several HN discussions about safely running Claude Code or Copilot CLI agents, such as Yolobox ‚Äì Run AI coding agents with full sudo without nuking home dir and Running Claude Code dangerously. These post detail the potential dangers and show how to run these agents more safely, and while reasonable, I find they lack in a few respects.\n\nIn particular, I want strong isolation, long running agent tasks, minimal cognitive overhead and I really value being able to close my laptop, walk away, and get notified on my phone when things are done. I do not mind paying for a cloud VM.\n\nThere are many valid ways to solve this problem. This post describes mine. It covers running multiple coding agents concurrently in a cloud VM, how I handle access and repos, and how I keep notifications simple.\n\nI generated some Terraform to spin up an Azure VM with a cloud-init.yml for setting up common tools/environments I use. Claude can generate a decent starting point for this quite easily, given your particular environment.\n\nFor secure access, I use Tailscale. Note: I'm not paid by them, but it is easily my favorite piece of infrastructure software!\n\nA cloud-init script installs Tailscale on first boot and automatically joins the VM to my tailnet. SSH access is enabled using Tailscale SSH. Once the VM is up, it appears on my private network with a stable hostname via Magic DNS. No SSH key management, no exposed ports.\n\nor connect using VS Code Remote SSH:\n\nhttps://code.visualstudio.com/docs/remote/ssh\n\nMost of the time I prefer tight, step by step control over code generation, working locally in VS Code with Copilot. For longer running or experimental tasks, I instead let an agent work remotely on a branch inside the VM, and pull the results once I am satisfied.\n\nWhile this is arguably git basics, it works well for me and I found that it is useful sharing how to set up a VM as a remote:\n\nOn the local machine, from the repo directory:\n\nThen you can pull clone and check out the branch, do the work, commit, and push to bare repo:\n\nFinally, locally, you can get the changes:\n\nI use tmux to manage long running sessions. This lets agents keep running after I disconnect, and makes it easy to juggle multiple concurrent sessions. If you are not familiar with tmux, it is worth learning!\n\nFor notifications, I use https://ntfy.sh.\n\nIt is free, extremely simple, and works over plain HTTP POST. I have the iOS app installed, so I can walk away from my laptop and still get notified when work completes. I explicitly instruct my agents to make a POST request once their work is done in the agent instructions.\n\nThat is it. No SDKs, no auth setup required for basic usage. The notification shows up immediately on my phone/browser.\n\nIf there is interest, I can publish a repo with the Terraform, cloud-init scripts, makefile, etc, and the old .devcontainer setup.",
    "readingTime": 3,
    "keywords": [
      "coding agents",
      "tasks",
      "laptop",
      "away",
      "access",
      "repo",
      "free",
      "close",
      "disconnect",
      "safely"
    ],
    "qualityScore": 1,
    "link": "https://jakobs.dev/pragmatic-notes-running-dangerous-ai-agents-cloud-vms/",
    "thumbnail_url": "/media/agents-vm.jpg",
    "created_at": "2026-01-21T12:26:57.044Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-over-now-just-coding",
    "title": "Vibe Coding Is Over, Now Just \"Coding\"",
    "description": "Regular thoughts on modernity, classicism, & technology.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://upstreamutopia.com/articles/?id=20260121-060942-vibe-coding-is-over,-now-ju",
    "thumbnail_url": "https://upstreamutopia.com/images/preview.jpg",
    "created_at": "2026-01-21T06:22:09.373Z",
    "topic": "tech"
  },
  {
    "slug": "sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima",
    "title": "Sandbox Your AI Dev Tools: A Practical Guide for VMs and Lima",
    "description": "AI coding assistants and other devtools can steal your credentials and data. Here's how to run them safely in isolated VMs using Lima on macOS/Linux.",
    "fullText": "AI coding assistants, npm, pip, and other development tools can run arbitrary code and scripts on your machine, potentially stealing SSH keys, API tokens, wallet keys, sensitive credentials and other private data without you noticing.\n\nThis guide shows you how to sandbox these tools in isolated VMs using Lima, so you can experiment and develop freely without putting your sensitive data at risk.\nJump straight to the guide, or read on for a bit of personal context.\n\nI‚Äôve been having quite a bit of fun with AI assisted coding recently.\n\nI use LLMs for a wide range of things, including discussing architecture, design choices, learning about new tools and libraries I wasn‚Äôt previously aware of, to reviewing PRs and quickly cranking out dirty prototypes.\n\nEspecially for hobby projects that are not meant to ever go into production, I enjoy playing with AI tools fast and loose, producing results quickly and not getting slowed down by annoying things such as reading code before running it ü§£.\n\nAnd yeah‚Ä¶ that‚Äôs obviously unsafe, unless it‚Äôs all contained in a sandbox!\n\nYou should never run potentially dangerous, experimental code on your main machine, since it could steal your passwords, API keys, environment variables, private keys, access to your communication tools, install services, and do all sorts of other nefarious things.\n\nNowadays I isolate all my devtools in VMs, and thought it might be useful to others if I put together a guide to shows how to do it. Well, here it is, and I hope it‚Äôll be useful to you, too!\n\nYou‚Äôll want to run the entire development environment, including the AI tool itself, inside a sandbox. This way it‚Äôs safe to install dependencies and to execute code, and unlocks other fun features like snapshots before running sketchy code, and reverting if something goes wrong.\n\nAnd it‚Äôs not just AI-generated code. Node.js/npm/yarn and Python/pip are particularly troublesome because they allow any package to run arbitrary scripts on your system during installation, and install tons of additional dependencies that can do the same. This attack vector is called ‚Äúsupply chain attack‚Äù and it happens all the time.\n\nVirtual Machines (VMs) and Containers (i.e. Docker, Podman, containerd) are the two most practical methods for isolating development tools from your host operating system. VMs provide much stronger protection and more flexibility overall, and are better suited for co-developing with AIs.\n\nContainer runtimes share the host operating system‚Äôs kernel, which means they‚Äôre fundamentally running on the same system as your main machine, just with isolated namespaces and resource limits. This creates several security concerns:\n\nIn contrast, a VM runs its own complete operating system with its own kernel. The hypervisor (like QEMU/KVM) creates a much stronger isolation boundary. Even if malicious code completely compromises the VM, it would need to exploit the hypervisor itself to reach your host, a significantly harder target.\n\nFurthermore, a VM enables better concurrency. It can run Docker containers, databases, web servers, multiple build processes, and background services all at once, and the AI tool can interact with everything naturally just like on a normal development machine.\n\nIn this guide, we use Lima VM to sandbox AI and devtools. Lima is a delightful, lightweight virtual machine manager for Linux and macOS which provides easy and quick ways to create and manage VMs.\n\nYou interact with Lima through the limactl command:\n\nVMs are based on templates, which can include (build on) other templates:\n\nThe Lima VM docs have platform-specific installation guides.\n\nHomebrew is recommended on macOS:\n\nOn Linux install the binary like this:\n\nNow ensure your Lima version is up-to-date:\n\nWe only want to share very specific host directories with the VM.\n\nLet‚Äôs create ~/VM-Shared on the host, which we later mount into the VM at ~/Shared (with write access):\n\nYou can use that directory to easily copy files between the host and the VM, and to share project directories from the host with the VM.\n\nDefaults for all VMs can be defined in ~/.lima/_config/default.yaml.\n\nLet‚Äôs create the default YAML file:\n\nLima conveniently creates default SSH configuration files for all VM instances, which makes it easy to log in with SSH (including using VS Code for a Remote-SSH session).\n\nI recommend using a ~/.ssh/config.d/ directory on the host and have SSH include all configs there by default. That allows us to simply link the Lima-created config files there to use them.\n\nAdd this as first line in your ~/.ssh/config file, to make SSH include all configs from there:\n\nGreat! After creating a new VM, we can now simply create a symlink to the Lima-generated SSH configs and use it to SSH into the instance.\n\nLet‚Äôs start an Ubuntu 25.10 VM instance, named dev.\nWe use the internal _images/ubuntu-25.10.yaml template because it doesn‚Äôt include the automatic home directory sharing:\n\nYou can share additional project-specific directories between host and VM in several ways:\n\nCreate a symlink for the SSH config file and SSH into the VM:\n\nLet‚Äôs update the services on the instance, and configure git:\n\nLet‚Äôs confirm that port forwarding works. We do this using a one-liner Python HTTP server (on port 7777) inside the VM, and accessing it from the host:\n\nThis section guides you through installing several other languages and development tools, including Golang, Node.js, Python, Rust, Docker.\n\nWe can accomplish that either by installing each tool according to it‚Äôs documentation, or by using a version manager such as mise (‚Äúmise-en-place‚Äù, 22k stars on Github) which can install hundreds of tools via a simple command-line interface.\n\nFirst, we install mise (‚Äúmise-en-place‚Äù, 22k stars on Github) and make bash support it:\n\nYou use mise latest <tool> to see the latest versions it knows about:\n\nNow you can install all the tools you want in a single command:\n\nTo manually install (or update) Golang in the VM, download the latest release and extract into /usr/local/go:\n\nThe Golang path needs to be in the PATH environment variable, which we have already added before.\n\nA good way to install a current version of Node.js in Ubuntu is by using nvm, a modern node version manager (90k stars on GitHub):\n\nNow it‚Äôs all installed and ready to use! Check the versions like this:\n\nPerhaps you don‚Äôt even need Docker, since Lima includes containerd and nerdctl by default. This is a Docker-compatible runtime and command-line interface that can also run images from Docker Hub:\n\nIf you do want to install Docker, the quickest way to install it by using their official get-docker.sh script:\n\nFor the group changes to take effect, exit the shell and re-login (may need a VM restart).\n\nVerify that user is in the ‚Äòdocker‚Äô group:\n\nGitHub CLI provides a useful gh cli command that let‚Äôs you easily interact with GitHub and private repositories.\n\nYou can install it in the VM following the Linux installation instructions:\n\nWarning: Authorizing GitHub CLI to access private repositories will leave an API key in the VM which could potentially be stolen by unauthorized scripts (which is what we wanted to avoid in first place by running everything in a VM).\n\nOnly authorize it with gh auth login for private repo access if you accept the risks! I personally avoid having any sensitive credentials in the VM, in particular those that allow access to private GitHub repositories.\n\nIf you prefer an IDE like VS Code, you can use Remote-SSH to start a session inside the instance.\n\nPlease note that this is potentially unsafe, as explained in the Remote-SSH README:\n\nSecurity Note\nUsing Remote-SSH opens a connection between your local machine and the remote. Only use Remote-SSH to connect to secure remote machines that you trust and that are owned by a party whom you trust. A compromised remote could use the VS Code Remote connection to execute code on your local machine.\n\nSee also this discussion on GitHub for more context and information.\n\nNow a new VS Code window opens, and sets up VS Code Server:\n\nThen you can click ‚ÄúOpen‚Äù and choose a folder, like Shared:\n\nBefore setting up the tools, let‚Äôs create a ‚ÄúHello World‚Äù directory in the Shared folder as our playground:\n\nLet‚Äôs start with installing Claude Code in the VM, following the instructions in the documentation:\n\nOn first start, Claude asks you to authorize it.\n\nThe docs mention support for an ANTHROPIC_API_KEY environment variable (i.e. set in .bashrc), but that did not work when I tried it; claude CLI didn‚Äôt let me skip the login process. Only after the login was done it notified me about the existing environment variable, and whether I‚Äôd prefer to use that one.\n\nAfter the login, Claude Code CLI is ready to be ued in the VM! üéâ\n\nSince Claude is running in a VM, it might be permissible to run it in ‚Äúdangerously skip permissions mode‚Äù, which makes it bypass all permission checks:\n\nYou could also create an alias for it and add it to your .bashrc:\n\nAnthropic provides documentation for using Claude in VS Code, and also offer a VS Code Claude extension.\n\nYou can install the Claude extension in the VM through the Remote-SSH session window:\n\nIn contrast to the CLI tool, the authentication flow did not work through the user interface, and I had to set the ANTHROPIC_API_KEY environment variable:\n\nReload the VS Code window (open command palette with Shift + CMD + P and choose ‚ÄúDeveloper: Reload Window‚Äù):\n\nNow the VS Code Claude extension should work:\n\nIf you want to enable ‚Äúdangerously skip permissions mode‚Äù in the VS Code extension, you can enable it via your user settings. Open the settings (CMD + ,), search for ‚Äúclaude‚Äù and enable ‚ÄúClaude Code: Allow Dangerously Skip Permissions‚Äù:\n\nLet‚Äôs install Gemini CLI from Google next.\n\nThe documentation recommends installing it with npm, the Node.js package manager. You‚Äôll need to install Node and npm first, see also the Node.js setup instructions.\n\nIt will ask you to authenticate:\n\nI chose ‚ÄúLogin with Google‚Äù. Note that the authentication flow may require a retry if the first attempt times fails.\n\nAfter authorization is done, Gemini CLI works!\n\nYou can run Gemini in YOLO mode:\n\nAutomatically accept all actions (aka YOLO mode, see https://www.youtube.com/watch?v=xvFZjo5PgG0 \n\nThe alias you could define in .bashrc:\n\nCodex CLI is the AI dev tool from OpenAI/ChatGPT.\n\nIt will ask you to sign in, either via ChatGPT or by providing an API key:\n\nAfter that is done, Codex CLI is ready to work for you!\n\nYou can also run Codex in dangerous mode:\n\nSkip all confirmation prompts and execute commands without sandboxing. EXTREMELY DANGEROUS. Intended solely for running in environments that are externally sandboxed\n\nThere are several other great tools worth a mention:\n\nDrop your favorite tools in the comments below!\n\nVM clones and snapshots allow you even more flexibility and isolation. You can use them to quickly and cheaply run new VMs for experiments and specific projects based on already provisioned instances. Use them frequently!\n\nLima offers several ways to take VM snapshots and/or clone VMs.\n\nYou can make a copy of an existing VM instance with limactl clone. The existing instance needs to be stopped first.\n\nAfter all the initial VM setup is done, clone it and use it both as backup as well as a base for future instances:\n\nRemember that after starting a new instance, you probably want to symlink the VM SSH configuration to your ~/.ssh/config.d/ directory, so ssh knows about it (See also ‚ÄúSSH into the VM‚Äù):\n\nFor maximum security and flexibility, consider using multiple VMs for different purposes and trust levels. This approach provides better isolation and lets you tailor each environment to specific needs.\n\nHere are some suggested VM configurations:\n\nYou can quickly clone your base VM setup to create new instances for different projects using limactl clone, as described in the VM cloning section above.\n\nFor sensitive or production projects, consider dedicating a separate VM to each project. This prevents potential cross-contamination between projects and allows you to mount only the specific project directories you need.\n\nWhen creating project-specific VMs, you can customize the mounted directories by editing the instance configuration. Either adjust the mounts section before starting the VM (by not using the -y flag), or edit ~/.lima/<vm_name>/lima.yaml after creation and restart the instance.\n\nThis approach also makes it easier to share VM configurations with team members. Instead of sharing entire disk images, you can distribute just the Lima template YAML file, which team members can use to spin up identical environments on their machines.\n\nFor automated setup, Lima supports provisioning scripts that run during VM creation. For more complex setups, consider using idempotent provisioning tools like Ansible to ensure consistent environments across your team.\n\nIf you find yourself repeatedly creating VMs with similar configurations, consider creating custom Lima templates. Templates are YAML files that define VM settings, and they can include other templates.\n\nCustom templates are useful for:\n\nYou can create a custom template by copying and modifying an existing one from Lima‚Äôs template directory. Save your custom templates in ~/.lima/_templates/ and reference them when creating new VMs:\n\nSee the Lima templates documentation \n\nHere are some important security best practices to follow when using VMs for development:\n\nRemember: The whole point of using VMs is isolation. When in doubt, create a new VM for risky experiments and delete it afterwards.\n\nI hope this guide helps you get started quickly and right-footed!\nAs always, please leave feedback, questions and ideas in the comments below.\n\nSpecial thanks to Ilya Lukyanov and Overflo for reviewing drafts of this post and making great suggestions. üôè",
    "readingTime": 12,
    "keywords": [
      "yolo mode",
      "yaml file",
      "anthropic_api_key environment",
      "remote-ssh session",
      "claude extension",
      "ssh configuration",
      "authentication flow",
      "mise mise-en-place",
      "mise-en-place stars",
      "command-line interface"
    ],
    "qualityScore": 1,
    "link": "https://www.metachris.dev/2025/11/sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima/",
    "thumbnail_url": "https://www.metachris.dev/images/posts/ai-sandbox/cover.jpg",
    "created_at": "2026-01-21T00:59:26.211Z",
    "topic": "tech"
  },
  {
    "slug": "takatime-selfhosted-wakatime-alternative-go-and-mongodb",
    "title": "TakaTime ‚Äì Self-Hosted WakaTime Alternative (Go and MongoDB)",
    "description": "TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim.  It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity...",
    "fullText": "Rtarun3606k\n\n /\n\n TakaTime\n\n Public\n\n TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim. It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity to a third-party server, TakaTime stores everything in your own MongoDB database.\n\n License\n\n MIT license\n\n 10\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Rtarun3606k/TakaTime",
    "readingTime": 1,
    "keywords": [
      "coding",
      "activity",
      "takatime",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Rtarun3606k/TakaTime",
    "thumbnail_url": "https://opengraph.githubassets.com/4a0f7ea58066e011c6d164cc23d1af692c671a17cadd5889743e52a2451b2406/Rtarun3606k/TakaTime",
    "created_at": "2026-01-20T12:27:00.258Z",
    "topic": "tech"
  },
  {
    "slug": "metacompilation",
    "title": "Metacompilation",
    "description": "A post that is going to be part of my sequence on rethinking programming languages. ‚Ä¶",
    "fullText": "A post that is going to be part of my sequence on rethinking programming languages.\n\nA compiler is a piece of machine code¬†, that takes as input a text string describing a program¬†¬†and returns the compiled machine code\n\nLet¬†¬†be a function that takes in a machine code program and returns another potentially faster or smaller program.\n\nA metacompliler has the formula\n\nTo understand how this works, first let's look at a less self referential case. Let¬†¬†be a regular compiler.\n\nis just a string. Maybe it's \"print(1+2)\"\n\nis a machine code program. This program, if run, would first compile¬†¬†into machine code, and then would run that machine code. Therefore it is a machine code program that does the same thing as¬†. It has a fairly significant size for even a small program, as it contains a complete copy of the compiler.\n\nWhat does¬†¬†do? It optimizes that machine code. The first thing it can do is cut out big chunks of the compiler. At least in simple cases. If the code is running arbitrary eval statements, all the compiler might be needed. In the case of this simple program, the parts of the compiler that handle floats, loops etc are just not used. If the optimizer is good, it could simplify the code all the way down to¬†. Some programming languages (see zig) already run code at compile time. The difference between compile and run time is just in what variables you currently know the value of, and the relative cost of compute.\n\nFor code with a finite runtime that just runs by itself, not interacting with the outside world, it can all, in principle be simplified down to a single print statement. In practice computer programs interact with the \"outside world\" in all sorts of ways. In some contexts, writes to disk or sending data to a GPU might be considered interactions with an external world. But for simplicity, assume the only form of interaction is input() and print()\n\nSo that's what a metacompiler does. But does it actually do anything. The most naive metacompiler implimentation has ¬†. ¬†When we call¬†¬†we get the program. And when we proceed to run that program, that program first calls¬†¬†to generate the machine code¬†¬†and then runs that machine code. This leads to an infinite regress. We haven't actually used¬†¬†anywhere. What we essentially have is just.\n\nA program that is clearly an infinite loop, with no actual relation to pi.\n\nSo we need the optimization step of the metacompiler to be doing something non-trivial to make the code halt in a finite time at all.\n\nLets define a small toy programming language, so we can talk about how to compile it.\n\nWe will give our programming language one data type, arbitrary size integers.\n\nWe will allow definitions, inputs, calculations and loops.\n\nThis example program shows all the features of this programming language. It is rather minimal.\n\nThe only free parameter in the metacompiler (as above) is in the choice of\n\nFor clarity, machine code instructions will look the same as programming language instructions, except the machine code will be in BOLD\n\nThe program consists of a number of definitions, ¬†(of the format [name]=[number], looking like¬†¬†) followed by the first non-definition statement. If the same name is used multiple times, only the last definition is needed. Ie the code¬†¬†can be optimized to\n\nSuppose the optimizer takes in¬†¬†code where the first non-definition in¬†¬†happens to be a calculation. For example.¬†¬†this can get optimized into\n\nNow suppose the first non-definition in¬†¬†is an¬†. ¬†For example.¬†¬†This can be converted into.\n\nThe way to think about this is that, if¬†¬†were a normal compiler, the¬†¬†function would convert a machine code program containing¬†¬†into another machine code program that still contains¬†¬†but that makes¬†¬†do slightly less work.\n\nwhile the similar¬†¬†can simplify down to\n\nThis gives a functioning toy example of a metacompiler. The above simplification rules are used in the definition of¬†, which is in turn used in the definition of¬†.\n\nThis produces code that, while excessively self referential, runs and produces output in a finite time, at least assuming the output of a regular compiler would run in finite time on the program.\n\nNote that¬†¬†only does 1 simplification step, and is only run once at compile time.\n\nSuppose we insisted that, before¬†¬†is allowed to simplify a piece of machine code, it must first prove that it's simplification won't change the result. This can be proved, by lob's theorem. However it isn't sufficient to make the metacompiler actually valid. Lob's theorem just says that ZFC approves of infinite buck passing. At some point we need to actually understand our programming language.\n\nIf however we make¬†¬†prove that¬†¬†is equivalent to¬†¬†before¬†¬†is allowed to output¬†. Then that is sufficient. Your directly proving that your meta-compiler is doing the same thing as a regular compiler, which gives you a ground truth about the meaning of the programming language.\n\nWhile the example meta-compiler given above isn't particularly fast, the toy example shows that metacompilers can exist. And the space of meta-compilers seems like it should contain all sorts of interesting optimizations.\n\nFor example. I was doing some programming involving numerically integrating systems of Stochastic Differential Equations (SDE's). Basically, I choose various settings and then run a tight loop involving those settings. And ideally I would like the speed of special purpose compiled code within the tight loop, without having the overhead of a full compilation from source every time I change a setting.\n\nSo, what I would ideally want is a program that contains precompiled snippets of code. Once the particular values of the settings are known, a highly optimized machine code program could be put together by little more than pasting together the relevant blocks of machine code to make a complete machine code program.\n\nAnd I'm wanting a way to make the programming language do this, or other clever things like this, automagically.\n\nAnother clever thing. Suppose your program contains¬†¬†of arbitrary user generated strings. But you know this is only a small fraction of runtime. And the user isn't allowed to use various language features. You might want to make a cut down minified version of the full language compiler, something with the unused features cut out, and some of the optimization tricks removed.\n\nThe hope is to totally blur the line between compile time and runtime, with code that can rewrite itself on the fly in all sorts of clever and highly performant ways.",
    "readingTime": 6,
    "keywords": [
      "self referential",
      "lob's theorem",
      "tight loop",
      "programming languages",
      "regular compiler",
      "programming language",
      "machine code",
      "code program",
      "metacompiler",
      "contains"
    ],
    "qualityScore": 1,
    "link": "https://www.lesswrong.com/posts/6BSZkkWNGMTdRi5Ly/metacompilation",
    "thumbnail_url": "https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg",
    "created_at": "2026-01-20T12:26:57.143Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here‚Äôs how the three models performed in our tests:\n\nüí° If you want the safest pick for real ‚Äúship a feature in a big repo‚Äù work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you‚Äôre okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n‚ö†Ô∏è NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere‚Äôs the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe‚Äôll compare code quality, token usage, cost, and time to complete the build.\n\nüí° NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nüí° NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It‚Äôs not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that‚Äôs the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that‚Äôs a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it‚Äôs hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. ü§∑‚Äç‚ôÇÔ∏è\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don‚Äôt think we‚Äôre anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don‚Äôt fully back them up. ü•¥\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won‚Äôt hold up long term.\n\nI don't see any of these recent models as ‚Äúuse it and ship it‚Äù for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "ygrep-fast-local-indexed-code-search-tool-optimized-for-ai-coding-assistants",
    "title": "Ygrep: Fast, local, indexed code search tool optimized for AI coding assistants",
    "description": "A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing. - yetidevworks/ygrep",
    "fullText": "yetidevworks\n\n /\n\n ygrep\n\n Public\n\n A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing.\n\n License\n\n MIT license\n\n 14\n stars\n\n 2\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n yetidevworks/ygrep",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/yetidevworks/ygrep",
    "thumbnail_url": "https://opengraph.githubassets.com/eda1bb1f12626e76b9793d44f791d0ed3332b6c28dc7896d1831d00d7c49258d/yetidevworks/ygrep",
    "created_at": "2026-01-20T00:57:31.412Z",
    "topic": "tech"
  },
  {
    "slug": "scaling-longrunning-autonomous-coding",
    "title": "Scaling long-running autonomous coding",
    "description": "Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from ‚Ä¶",
    "fullText": "Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents:\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nThey ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.\n\nIn my predictions for 2026 the other day I said that by 2029:\n\nI think somebody will have built a full web browser mostly using AI assistance, and it won‚Äôt even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it‚Äôll get so much easier.\n\nI may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nBut how well did they do? Their initial announcement a couple of days ago was met with unsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.\n\nIt looks like they addressed that within the past 24 hours. The latest README includes build instructions which I followed on macOS like this:\n\nThis got me a working browser window! Here are screenshots I took of google.com and my own website:\n\nHonestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.\n\nThe FastRender repo even uses Git submodules to include various WhatWG and CSS-WG specifications in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.\n\nThis is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was HiWave browser, a new browser engine in Rust first announced in this Reddit thread.\n\nWhen I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.",
    "readingTime": 3,
    "keywords": [
      "autonomous coding",
      "web browser",
      "agents",
      "repo",
      "cursor",
      "project",
      "ended",
      "tasks",
      "agent",
      "mostly"
    ],
    "qualityScore": 1,
    "link": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/",
    "thumbnail_url": "https://static.simonwillison.net/static/2026/cursor-social-card.jpg",
    "created_at": "2026-01-20T00:57:30.632Z",
    "topic": "tech"
  },
  {
    "slug": "valve-updates-ai-disclosure-guidelines-to-allow-for-aipowered-tools",
    "title": "Valve Updates AI Disclosure Guidelines To Allow For AI-Powered Tools",
    "description": "Valve has made changes to its AI-disclosure guidelines, removing the need for studios to disclose whether or not games have been developed with AI-powered tools and putting more emphasis on AI-generated assets.\nThe change, which was pointed out by Simon Carless on LinkedIn, suggests that Valve is no longer concerned by the use of AI tools that assist development, stating, \"Efficiency gains through the use of [AI-powered dev tools] is not the focus of this section.\" These tools could included a variety of things, such as AI-generated transcripts of meetings to code helpers that have become prevalent in most programming environments.\nValve states the the aim of its disclosure policy is to inform players when AI is used to generate content, from marketing and conceptual assets to in-game ones that players will interact with. Developers are able to specify what assets have been generated and indicate, via a single checkbox, whether or not players will interact with AI-generated content during gameplay, be it images, audio, or other content.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/valve-updates-ai-disclosure-guidelines-to-allow-for-ai-powered-tools/1100-6537483/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1585/15853545/4637028-7297126222-arc-r.jpg",
    "created_at": "2026-01-19T18:18:34.839Z",
    "topic": "gaming"
  },
  {
    "slug": "a-meta-product-manager-with-no-technical-background-says-vibe-coding-gave-him-superpowers",
    "title": "A Meta product manager with no technical background says vibe coding gave him 'superpowers'",
    "description": "A Meta product manager says vibe coding is letting non-technical PMs ship features and work differently with engineers.",
    "fullText": "A product manager at Meta says vibe coding has changed what it means to do his job ‚Äî even though he has no technical background and still finds code \"terrifying.\"\n\nZevi Arnovitz said in an episode of \"Lenny's Podcast\" released Sunday that discovering AI coding tools in mid-2024 marked a turning point in his career.\n\nIt felt like he was handed \"superpowers,\" Arnovitz said.\n\nUnderstanding how to use AI intentionally is \"one of the biggest game changers that will make you much better as a PM,\" he said, referring to product management.\n\nArnovitz joined Meta in September last year after about three years as a product manager at website-building company Wix, according to his LinkedIn profile.\n\nArnovitz said he has rebuilt his workflow around AI. He uses vibe coding tools like Cursor alongside models from Anthropic and Google to explore product ideas, generate build plans, execute code, review it, and update documentation.\n\nThe shift reshaped his role as a product manager. Instead of merely acting as a coordinator between engineering and design, Arnovitz operates more like a product owner with the capability to execute.\n\n\"Everyone's going to become a builder,\" he said. \"We're going to see that a lot in the next coming years.\"\n\nStill, Arnovitz said there are limits to what non-technical product managers should take on. He said he doesn't think product managers should be shipping complex infrastructure changes or big projects.\n\nAI has enabled product managers to take on smaller UI projects by building the feature and then handing the code to a developer for final review and completion, he added.\n\nAs AI tools improve, Arnovitz said titles and responsibilities are likely to \"collapse,\" and product managers should treat vibe coding as a \"collaborative learning opportunity\" with their engineering teams.\n\nThe rise of AI coding tools is blurring the lines for traditional roles, making it easier for non-technical workers, including product managers, to build products directly.\n\nFigma CEO Dylan Field said in October on \"Lenny's Podcast\" that AI has pushed many workers to experiment with building products.\n\nTasks that once required deep engineering expertise can now be done with vibe coding tools, he said.\n\n\"I think that we're seeing more designers, engineers, product managers, researchers, all these different folks that are involved in the product development process dip their toe into the other roles,\" he said.\n\n\"We're all product builders, and some of us are specialized in our particular area,\" he added.\n\nThat same thinking is showing up in how companies train new hires. LinkedIn replaced its long-running associate product manager program with an associate product builder track in January.\n\n\"We're going to teach them how to code, design, and PM at LinkedIn,\" said the company's former chief product officer, Tomer Cohen, in an episode of \"Lenny's Podcast\" published in December. It's more about training people \"who can flex across,\" he added.\n\nCohen, who spent nearly 14 years at LinkedIn, left the company in January and now works as an advisor, according to his LinkedIn profile.",
    "readingTime": 3,
    "keywords": [
      "linkedin profile",
      "vibe coding",
      "coding tools",
      "product manager",
      "product managers",
      "associate product",
      "lenny's podcast",
      "code",
      "engineering",
      "arnovitz"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-product-manager-vibe-coding-superpowers-non-technical-builder-2026-1",
    "thumbnail_url": "https://i.insider.com/696dbfd0c58df2ecd5ccc045?width=1200&format=jpeg",
    "created_at": "2026-01-19T12:27:05.647Z",
    "topic": "finance"
  }
]
[
  {
    "slug": "proxima-local-opensource-multimodel-mcp-server-no-api-keys",
    "title": "Proxima – local open-source multi-model MCP server (no API keys)",
    "description": "Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API - Zen4-bit/Proxima",
    "fullText": "Zen4-bit\n\n /\n\n Proxima\n\n Public\n\n Multi-AI MCP Server - Connect ChatGPT, Claude, Gemini & Perplexity to your coding tools without any API\n\n License\n\n View license\n\n 6\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Zen4-bit/Proxima",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/Zen4-bit/Proxima",
    "thumbnail_url": "https://opengraph.githubassets.com/da1ce1c4daf052f563ec5615a17f7062e6ab435c084a9666f41483e54046eef5/Zen4-bit/Proxima",
    "created_at": "2026-02-17T12:37:44.260Z",
    "topic": "tech"
  },
  {
    "slug": "fujitsu-aidriven-software-development-platform",
    "title": "Fujitsu AI-Driven Software Development Platform",
    "description": "Fujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society.",
    "fullText": "Kawasaki, Japan, February 17, 2026\n\nFujitsu Limited today announced the development and launch of its AI-Driven Software Development Platform, a new initiative to bring software development into the AI age and contribute to the sustainable growth of its customers and society. This platform automates the entire software development process, from requirements definition and design to implementation and integration testing. By leveraging the Takane large language model (LLM) [1] and agentic AI technology for large-scale software development developed by Fujitsu Research, the AI-Driven Software Development Platform enables AI agents to understand complex, evolving large-scale systems owned by enterprises and public organizations. The platform has multiple AI agents collaboratively execute each stage of software development, achieving full automation of the entire process without human intervention.\n\nFujitsu aims to use this AI-Driven Software Development Platform to carry out revisions to all 67 types of medical and government business software products provided by Fujitsu Japan Limited by the end of fiscal year 2026. The revisions are necessary due to legal and regulatory changes. From January 2026, the platform has been used in Japan for software modifications made necessary by the 2026 medical fee revisions [2]. In a PoC that updated software as per the 2024 medical fee revisions, the platform demonstrated a significant reduction in development time for one of approximately 300 change requests. Using conventional software development methods [3] the modifications would have taken three person-months. With this technology that was dramatically shortened to four hours, achieving a 100-fold increase in productivity.\n\nIn AI-driven development, Fujitsu positions AI-Ready Engineering—the process of preparing assets and knowledge to ensure AI correctly understands existing systems and achieves highly reliable automation—as crucial. With AI-Ready Engineering and the AI-Driven Software Development Platform working in tandem, Fujitsu will accelerate AI-driven software development. Fujitsu will promote a transformation in engineers' work styles, strengthening its Forward Deployed Engineer (FDE) complement, and shifting the paradigm of software development from a conventional person-month-based approach to a customer value-based approach.\n\nMoving forward, Fujitsu plans to expand the application of the AI-Driven Software Development Platform to a wide range of sectors, including finance, manufacturing, retail, and public services, by the end of fiscal year 2026. Fujitsu will also begin offering this service to customers and partner companies to enable them to rapidly and flexibly develop systems that adapt to changes in their business environments. Through these efforts, Fujitsu aims to transform the software development process into an AI-driven model as an industry standard.\n\n(Order that companies appear is aligned with the original Japanese press release)\n\nTakashi Manabe, Senior Research Director, AI & Automation, IDC Japan\n“IDC forecasts that from 2026 onward, the acceleration of AI/agent-based business utilization and the modernization of existing systems will be key drivers of transformation in the Japanese IT market. Fujitsu’s announcement aims to redefine complex legacy system assets into a state where AI can accurately understand and process them, and to automate the entire waterfall development process. This initiative is expected to provide a practical pathway for many domestic enterprises facing the ongoing challenge of maintaining and operating legacy assets, while also promoting a shift in software engineering away from a labor-intensive model.”\n\nShinji Kajitani, Director and President Executive Officer, Optima Corporation\n“I am deeply impressed by the concept of automating the entire software development process from upstream to downstream using AI, and even entrusting the verification process to AI. This overturns the traditional assumption that human checks are ultimately indispensable, and I see great potential, especially in targeting business packages that undergo complex system changes every year. Our company has also been involved in business package modifications for many years, and how to complete system revisions with high quality in a short period has always been a major challenge. We believe that the knowledge and expertise accumulated during that process can significantly contribute to the realization and advancement of this concept. Our company will continue to contribute to the business expansion of Fujitsu and Fujitsu Japan through ongoing cooperation, not limited to this project.”\n\nHiroshi Nakatani, Representative Director, Executive Vice President, Kawasaki Heavy Industries, Ltd.\n\"This AI automation initiative promoted by Fujitsu is not merely about improving development efficiency; we recognize it as a significant challenge to pass on and evolve the extensive business knowledge and design philosophies cultivated by companies over many years to the next generation. In particular, the concept of providing end-to-end support, from requirements definition to design, implementation, and quality assurance, triggered by changes in laws and rules, opens up new possibilities in areas that have traditionally relied on human experience and tacit knowledge. We see great significance in AI functioning as a foundation that supports human judgment and creativity, rather than replacing it.\nIn the manufacturing industry, challenges such as design changes, regulatory compliance, and understanding the scope of impact are becoming increasingly complex year by year. Fujitsu's approach of advancing both knowledge standardization and AI utilization in these areas offers valuable insights for enhancing the productivity and competitiveness of the entire industry.\nKawasaki Heavy Industries sincerely hopes that this initiative will be a crucial step in driving the transformation of Japanese manufacturing and a wide range of other industries, and we wholeheartedly support its further development.\"\n\nYasushi Matsuda, President and CEO, Kewpie Digital Innovation Co., Ltd. \n“Systems have become increasingly complex through years of operation and often now require significant maintenance effort. While the introduction of generative AI has improved auditing efficiency, its accuracy remains insufficient for reliable practical application. Amidst this situation, we place great expectations on “Multi-layer Quality Control,” which automatically corrects ambiguities and omissions. We are confident that this mechanism, where AI itself audits quality and autonomously repeats processes, will dramatically enhance the reliability of system development. We eagerly await its future development.”\n\nJunichi Aruji, Managing Director, Kintetsu Information System Co., Ltd. \n“The challenge of revamping existing systems has long been a significant one for engineers. Fujitsu’s AI-Driven Software Development Platform has the potential to dramatically transform the labor previously involved in understanding complex laws and regulations, analyzing vast historical assets, and grasping the tacit knowledge of the field.\nWhat is particularly noteworthy is the AI's ability to autonomously learn \"human intelligence,\" thereby dramatically enhancing the accuracy of requirements definition. Furthermore, it can complete everything from program structure analysis and standardization to the extensive testing phase with incredible speed and comprehensiveness. This makes it possible to deliver high-quality products in a short period.\nAs the role of AI expands and frees people from routine tasks, engineers can focus on more creative activities. I have high expectations for the paradigm shift in system renovation that this solution will bring.”\n\nYumi Ueno, Managing Director, Partner Ecosystem & Corporate Business, Google Cloud Japan G.K. \n“This initiative to achieve comprehensive, one-stop automation spanning from requirements definition to system validation is a groundbreaking innovation for the industry. The technology enables AI to accurately understand vast assets, including long-established programs and design documentation, and we are delighted at the potential for both production-grade quality and exceptional productivity gains. We are confident that this platform will become the new standard for development and accelerate our customers' digital transformation. We remain committed to working with Fujitsu to address social challenges through AI.”\n\nMasahiro Niimi, Managing Executive Officer, Head of Information Systems Management Division, CISO, Sakura KCS Corporation \n“I believe Fujitsu Limited's AI-driven development framework has the potential to become the ‘new paradigm of system development.’ It cannot be achieved simply by feeding existing code or design information into AI, and while there are various hurdles, such as converting documentation to Markdown and establishing test environments, overcoming these hurdles can lead to solving traditional system development challenges (like QCD).\nWhat particularly caught my attention is not just improvements in the development process, but what comes after generative AI, i.e., the incorporation of detailed specifications and code (logic). I see tremendous potential here as a solution to the greatest challenge: visualizing and transferring the tacit knowledge of veteran software engineers’ that is traditionally missing from documentation. We expect generative AI to act as an advisor for less experienced software engineers, readily answering questions anytime, thereby dramatically advancing know-how transfer to the next generation. On the other hand, this mechanism also has the potential to dramatically change the traditional SI business model, and we are watching future developments closely.”\n\nTakao Kazama, Executive Officer, Group Companies and Accounting & Finance, The Shizuoka Shimbun and Shizuoka Broadcasting Co., Ltd. \n\"This initiative for complete automation of application development and maintenance represents a highly valuable transformation for our company. It formalizes and establishes a reproducible process for tasks that have long relied on the implicit knowledge and experience of individual staff members, and we have great expectations for it. In particular, it has the potential to significantly improve quality variations in legacy system maintenance and lost opportunities due to delayed change responses. Furthermore, the evolving ability of AI to perform root cause analysis and identify necessary additional information is a major step towards advanced and efficient system operations, with the potential to change the very nature of system development. We share Fujitsu's commitment to improving productivity across the entire industry and establishing new development standards. We look forward to its continued strong promotion as an initiative that will advance the entire industry.\"\n\nShimane Prefectural Central Hospital \n\"The AI-Driven Software Development Platform presented by Fujitsu offers a practical and robust approach to the long-standing challenges faced by medical institutions: the increasing complexity of medical fee calculations and the growing workload of claims processing. The mechanism where AI analyzes legal documents and extracts the relevant areas, while explicitly highlighting points open to interpretation to supplement human judgment, is particularly impressive. This design demonstrates a deep understanding of on-site operations and is highly commendable. Furthermore, the Japanese-specific LLM and the consideration for safety are indispensable elements for AI utilization in the medical field. Beyond medical fee claims, this technology has potential for integration with related areas such as bed management and understanding performance requirements, making a strong contribution to overall hospital operational efficiency in the future. This is a promising initiative that warrants positive consideration for adoption to alleviate the burden on medical professionals.\"\n\nShinichi Aikawa, Executive Officer, Head of Systems Division, SBI Sumishin Net Bank, Ltd.\n“We expect Fujitsu's AI-Driven Software Development Platform to be an initiative with the potential to fundamentally transform the software development process itself. If a world can be realized where everything from requirements definition to design, coding, and testing can be automatically executed in a seamless, one-stop manner, it will be possible to achieve both a dramatic improvement in development speed and quality. \nSince 2024, we have been working with Fujitsu in some areas of this field. Through these initiatives, we are confident that the entire development process will be automated end-to-end in the near future. By realizing this transformation, the possibilities for the services we can provide to our customers will greatly expand. We think about ideas for new services for our customers on a daily basis. This would allow us to rough out these ideas in a short period of time and provide them to our customers quickly. We hope that this new world of value creation will arrive as soon as possible.”\n\nMasaki Murata, Vice President, IBM Japan \n“We strongly believe that Fujitsu’s announcement marks a significant step forward in the evolution of system development in Japan. It aligns closely with IBM Japan’s vision and represents an important initiative that will help shape the future of the industry as a whole. We look forward to driving this momentum together and contributing to the creation of a more robust and vibrant ecosystem.”\n\nRyota Sato, Managing Executive Officer, Global Communications & IT Services Group, Microsoft Japan Co., Ltd. \n\"We sincerely welcome Fujitsu Limited’s announcement of the AI-Driven Software Development Platform as a pioneering initiative that opens a new chapter in system development for the AI era. By orchestrating multiple AI agents to automate the end-to-end development lifecycle—from requirements definition through ongoing enhancement—while integrating human-led quality assurance, this platform embodies a new engineering model in which people and AI truly work together. We view this initiative as highly significant, as it directly addresses the critical challenges facing Japan’s system development industry, including severe talent shortages and the increasing complexity and sophistication of modern systems. We strongly expect this bold effort to drive the evolution of Japan’s system development business and to grow into a transformation model with global relevance. Moving forward, we will continue to work closely with Fujitsu, combining the strengths of both companies to strongly support our customers in their journey toward becoming Frontier Firms.”\n\nTatsuo Ogawa, Executive Officer Group CTO, Panasonic Holdings Corporation\n“We believe that the AI-driven end-to-end automated system development announced this time represents not only a significant improvement in productivity, but also a bold challenge to fundamentally transform the way enterprise IT is delivered. By enabling AI to accurately understand frequently updated regulations and complex business knowledge, including implicit know-how, this approach autonomously executes processes seamlessly from requirements definition through system modification. It has the potential to provide an effective solution to the core challenges posed by legacy systems faced by many Japanese enterprises. We look forward to jointly refining this technology through hands-on practice and advancing co-creation by incorporating on-site expertise of both Panasonic and Fujitsu, with the expectation that it will become a new standard for system development and be deployed broadly not only within Panasonic but across society as a whole.”\n\nExecutive at a major manufacturing company's IT subsidiary\n“We anticipate this initiative will bring about a new transformation in system development. This transformation will be driven by the application of advanced Japanese language processing capabilities—such as the understanding of legal documents—to diverse tasks, the reliable execution of each process through quality auditing functions, and the expansion of these capabilities to scratch development. Furthermore, we believe that AI Ready Engineering, by formalizing expert know-how and domain knowledge into explicit knowledge and transforming it into AI-usable assets, will significantly contribute to the succession of expertise from an increasingly limited pool of skilled professionals. We sincerely hope that the co-creation between the knowledge-inheriting AI and on-site personnel will generate new value and form the cornerstone for innovation in the system development industry, and indeed, across all industries.”\n\nJointly developed by Fujitsu and Cohere Inc.\n\nA national system that reviews public medical fees and adjusts cost allocation for medical procedures.\n\nDevelopment methods where quality is verified at each stage, from software requirements definition, design, and implementation to integration testing.\n\nThe Sustainable Development Goals (SDGs) adopted by the United Nations in 2015 represent a set of common goals to be achieved worldwide by 2030.\nFujitsu’s purpose — “to make the world more sustainable by building trust in society through innovation” — is a promise to contribute to the vision of a better future empowered by the SDGs.\n\nPublic and Investor Relations Division\n\nAll company or product names mentioned herein are trademarks or registered trademarks of their respective owners. Information provided in this press release is accurate at time of publication and is subject to change without advance notice.\n\nDate: 17 February, 2026\nCity: Kawasaki, Japan\nCompany: Fujitsu Limited",
    "readingTime": 13,
    "keywords": [
      "vice president",
      "kawasaki heavy",
      "heavy industries",
      "fujitsu’s announcement",
      "executive officer",
      "managing director",
      "kawasaki japan",
      "wide range",
      "press release",
      "increasing complexity"
    ],
    "qualityScore": 1,
    "link": "https://global.fujitsu/en-global/pr/news/2026/02/17-01",
    "thumbnail_url": "https://global.fujitsu/-/media/Project/Fujitsu/Fujitsu-HQ/pr/news/2026/02/17-01/news-20260217-01th.png?rev=06c466bd8ac14732a2ff3eff27b55e3b",
    "created_at": "2026-02-17T06:45:26.020Z",
    "topic": "tech"
  },
  {
    "slug": "one-of-the-most-annoying-programming-challenges-ive-ever-faced",
    "title": "One of the Most Annoying Programming Challenges I've Ever Faced",
    "description": "Hey everyone, it’s already been two months since the last blog post! Today I’m back to share some behind-the-scenes about the struggles and development of a new functionality for Sniffnet: process identification, a.k.a. the most requested feature since the very beginning of the project.",
    "fullText": "Hey everyone, it’s already been two months since the last blog post!\n\nToday I’m back to share some behind-the-scenes about the struggles and development of a new functionality for Sniffnet: process identification, a.k.a. the most requested feature since the very beginning of the project.\n\nWith “process identification” in a network monitoring context, I mean the possibility to discover which application or program is responsible for a given network connection.\n\nThis can be determined by looking at the open TCP/UDP ports on the system and finding out which process is currently using them.\n\nIf implementing this feature seems like a no-brainer to you, well… read on because it turned out to be a much more complex task than I could imagine, and this is the reason why the related GitHub issue has been open for almost 3 years.\n\nFirst of all, the implementation is highly OS-specific: each platform has its own directories and data structures storing such information, and APIs to interact with them are often not well documented or written in C (therefore not very ergonomic to use from Rust). \n\nAnd unfortunately, there is no Rust library ready-to-use satisfying the needs of Sniffnet.\n\nOne could argue that this is a solved problem, since there are already existing tools to do it: for instance, on Linux and Windows you have netstat, and on macOS you have lsof or nettop. \n\nHowever, these tools are not designed to be used as libraries and spawining a shell to execute them repeatedly is not efficient, especially if you want to monitor the network activity in real-time. \n\nMoreover, they don’t provide all the information Sniffnet needs, such as the process name and path.\n\nBut the biggest challenge is another one: the least system-intrusive ways to implement the feature are snapshot-based, meaning that they require to read the system state at a given moment in time and do some computations to find out the associations between open ports and their owning processes. \n\nI’m referring to using libproc on macOS, the /proc filesystem on Linux, and iphlpapi on Windows. \n\nThis is not a problem in itself, but it generates the need to do this processing very efficiently, and it leads to cases where it’s not possible to retrieve process information at all.\n\nFor instance, short-lived connections can go undetected and system processes with elevated privileges can be hidden to user-space applications for security reasons.\n\nMore system-intrusive approaches exist, such as using kernel-level hooks to intercept the system calls responsible for creating network connections. \n\nAn example of this is eBPF on Linux, which requires to run privileged code inside the kernel. \n\nOn macOS, you’d even need entitlements from Apple to be able to do something similar through their Network Extension framework. \n\nWhile these approaches are way more accurate, they go against Sniffnet’s philosophy of being a lightweight, non-intrusive, and friendly app that can be installed by anyone.\n\nAfter considering all the options, I decided to go with the snapshot-based approach. \n\nDespite being aware it’s not flawless, I believe it to be the best compromise for Sniffnet’s use case.\n\nlisteners is an open-source library I’ve been working on for the past 2 years with the goal of supporting this feature.\n\nBeing Sniffnet a cross-platform application, I needed a solution that could work on different Operating Systems:\nno other Rust crate provides this functionality supporting multiple platforms and the existing ones are not maintained or satisfactory enough even for a single OS.\n\nInterestingly, I also had this same need at my job, where we also wanted a Rust way to do it: this motivated me even further to contribute to the library. \n\nAfter two years, I’m happy to see that listeners was downloaded 150k times and has now multiple public dependents both on crates.io and GitHub, which means that this is a problem shared among many people.\n\nJust some days ago listeners v0.4.0 was published.\n\nI’m particularly proud of this release for at least two reasons:\n\nThanks to point 2, I now judge the library mature, fast, and reliable enough for use in Sniffnet.\n\nIf you’re a Rust developer, you’re more than welcome to contribute to the library trying to make it even faster, or adding support for more Operating Systems (Android and iOS? Why not!).\n\nSniffnet will use listeners to look up the process for each observed network connection, and will show it in the UI’s Overview and Inspect pages.\n\nAdditionally, it will use another library called picon (I’m still working on it) to retrieve app icons given their program path, showing them in the UI as well to make it easier to identify processes at a glance.\n\nThe workflow I plan to use is indeed pretty complex, including caching to minimize performance impact and retries to maximize the chances to correctly retrieve process information for a given open port.\n\nIn the flowchart below I’ve outlined a draft of the strategy I’ll adopt for Sniffnet-side implementation of the feature.\n\nI hope this post wasn’t too scary to read\nand that it gave you an idea of how much work is behind a seemingly simple feature like this.\n\nNothing worth having comes easy, someone says.",
    "readingTime": 5,
    "keywords": [
      "network connection",
      "process identification",
      "retrieve process",
      "feature",
      "library",
      "listeners",
      "it’s",
      "macos",
      "processes",
      "functionality"
    ],
    "qualityScore": 1,
    "link": "https://sniffnet.net/news/process-identification/",
    "thumbnail_url": "https://sniffnet.net/assets/img/post/process-identification/cover.png",
    "created_at": "2026-02-16T18:30:07.896Z",
    "topic": "tech"
  },
  {
    "slug": "beadhub-allow-coding-agents-to-claim-work-chat-and-coordinate-across-machines",
    "title": "BeadHub: Allow coding agents to claim work, chat, and coordinate across machines",
    "description": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I’ve spent the past several months building a tool to address that.\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans—across machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\nBeads Around the time I wrote that article, I started using Steve Yegge’s beads, a git-native issue tracker designed for AI agents.",
    "fullText": "I wrote previously that the bottleneck in AI-assisted programming is shifting from individual productivity to coordination. I’ve spent the past several months building a tool to address that.\n\nBeadHub is an open-source coordination server that lets AI programming agents claim work, talk to each other, reserve files, and escalate to humans—across machines and across programmers. I use it daily to manage around fifteen agents working on two or three products.\n\nAround the time I wrote that article, I started using Steve Yegge’s beads, a git-native issue tracker designed for AI agents. Your agent runs bd create \"Fix the login redirect bug\" and it appends a JSON line to .beads/issues.jsonl, right in the repository. Issues travel with the code. When you push a branch, the issues come along.\n\nYegge calls it the “50 First Dates” problem: agents wake up every session with no memory of yesterday’s work. Beads fixes that. An agent reads the issue list and knows where things stand. My agents got much more done.\n\nWhich meant more agents, more worktrees, more parallel work—and the coordination problem became even more acute. Two agents modify the same file. One refactors a function while another adds to it. An agent picks up a task already in progress in a different worktree. Nobody knows who’s working on what.\n\nBut beads is also the right scaffolding for coordination. If everyone in a team uses beads, all agents share a picture of what needs doing. Beads gives agents something useful to talk about; BeadHub gives them a way to talk.\n\nThe major platforms are moving in this direction. Anthropic just shipped Agent Teams in Claude Code: a lead session that spawns independent teammates who communicate directly and self-coordinate. OpenAI’s Codex app runs parallel agent threads in isolated worktrees.\n\nYegge built Gas Town on top of beads to tackle the single-machine case: a “Mayor” agent orchestrates dozens of coding agents, tracks work in convoys, and persists state so agents can pick up where they left off.\n\nThese are real steps forward, but they’re solving a specific version of the problem: multiple agents for one programmer, on one machine, within one tool.\n\nThe version I am interested in is Maria in Buenos Aires running a frontend agent while Juan in San Francisco runs a backend agent, and they need their agents to not destroy each other’s work, and to figure out how to work together.\n\nBeadHub is a server that agents connect to through bdh, a wrapper around the beads bd command. When an agent runs any bdh command it registers with the server. The server tracks which agents are online across the project—what machine they’re on, what branch, what files they’re touching.\n\nCommunication. Agents can send each other mail (async, fire-and-forget) or chat (sync, block-until-reply). With mail an agent finishes a task and drops a note: “Done with bd-42, tests passing.” Chat is for when agents need to think together: “I’m adding a role field to the user model—will that break your permission checks?” / “It will, but the fix is small. Go ahead and I’ll update my side.”\n\nClaims. When an agent marks a bead as in-progress, that claim is immediately visible to every other agent in the project, regardless of whose machine they’re on. If another agent tries to claim the same bead, it gets rejected with a message telling it who has it.\n\nFile reservations. When an agent modifies a file, the server records an advisory lock. Other agents see a warning if they touch the same file. Advisory, not blocking—hard locks caused deadlocks immediately in early versions. Agent A locks file X, agent B locks file Y, both need the other’s file. Warnings work better. Agents are cooperative; they just need information.\n\nEscalation. An agent runs bdh :escalate with a description of what it’s stuck on and a human gets notified with full context. Without this, agents either fail silently or spin retrying things that need human judgment.\n\nThe multi-machine part is where it comes together. BeadHub recognizes Maria’s and Juan’s clones as the same repo. Maria’s agents and Juan’s agents see each other’s claims, locks, and messages. If Maria’s frontend agent reserves src/components/Auth.tsx, Juan’s backend agent sees the warning even though they’re in different cities on different machines.\n\nA project can span multiple repositories. The frontend repo agents can message the backend repo agents. A bead in the frontend can be marked as blocked by a bead in the backend.\n\nYou can see what this looks like in practice on the BeadHub project’s own dashboard, where we coordinate BeadHub’s development using BeadHub. Make sure to check the chat page, it is almost magical to see them figuring things out.\n\nA few things I got wrong before getting them right.\n\nThe client is the source of truth. My instinct was to make the server authoritative. But agents work locally, in git repos, and their local state is the ground truth. The server aggregates and distributes. If the server and the client disagree, the client wins. If the server goes down, bdh falls back to local bd with a warning. Work continues. Coordination catches up later.\n\nAsync by default. My first instinct was real-time negotiation between agents. Doesn’t scale. Agents work at different speeds, on different schedules, and blocking one while waiting for another is expensive. Mail is the default. Chat is the exception.\n\nAdvisory over mandatory. Advisory file locks that warn instead of block. Bead claims that can be overridden with --:jump-in \"reason\" (which notifies the other agent). The system provides information and trusts agents to act on it.\n\nThe coordinator role. I assign one agent per project the “coordinator” role. The coordinator doesn’t write code. It watches the dashboard, assigns work, checks on progress, nudges stuck agents, and keeps the end goal in sight. The implementer agents are heads-down in their worktrees; the coordinator is the one who knows what the project needs next. BeadHub serves each agent a role-specific policy—markdown documents describing how agents in that role should behave—and the coordinator’s policy is fundamentally different from an implementer’s. This turned out to matter more than any of the technical decisions.\n\nThe single-machine problem is getting solved. Agent Teams, Codex—within a few weeks, running multiple agents in parallel on your laptop will be table stakes.\n\nThe multi-programmer problem is next. Five engineers, fifty agents, three repositories, two time zones. That’s where the coordination problem changes in kind, not just degree. It’s not enough that your agents can talk to each other. They need to talk to your teammate’s agents, on a different machine, in a different time zone, working on a different repo in the same project.\n\nBeadHub is open source and free for open-source projects.",
    "readingTime": 6,
    "keywords": [
      "together beadhub",
      "coordinator role",
      "machine they’re",
      "locks file",
      "frontend agent",
      "backend agent",
      "repo agents",
      "server",
      "beads",
      "coordination"
    ],
    "qualityScore": 1,
    "link": "https://juanreyero.com/article/ai/beadhub",
    "thumbnail_url": "https://juanreyero.com/img/default-og.jpg",
    "created_at": "2026-02-16T18:30:07.341Z",
    "topic": "tech"
  },
  {
    "slug": "the-speed-of-building-has-outpaced-the-thinking-part",
    "title": "The Speed of Building Has Outpaced the Thinking Part",
    "description": "Explore the impact of AI on indie development and the need for a moral compass in coding. Are we sacrificing quality for speed?",
    "fullText": "I get this feeling a lot lately. I wake up with an idea, grab a coffee, open my editor, and thanks to the current generation of AI tools, I can have a working prototype before breakfast.\n\nThe barrier to entry for software development hasn’t just been lowered; it’s effectively been removed. We are in the era of “vibe coding,” where natural language prompts turn into deployed applications in minutes. It is exhilarating. It is powerful.\n\nBut lately, I have started to wonder: Are we killing indie development with AI?\n\nDon’t get me wrong, I love these tools. I use GitHub Copilot and other LLMs daily. But I believe we have reached a tipping point where the speed of building has outpaced the thinking part. We are so focused on how fast we can build that we stopped asking if we should build.\n\nIn this post, I want to talk about why we need a new “moral compass” for development in the AI age, and a potential solution to help us get there.\n\nFive years ago, if you had an idea for a SaaS tool, say, a screenshot editor or a niche time-tracker,you had to sit down and plan. The friction of coding was a natural filter. You had to ask yourself: “Is this worth X hours of my life?”\n\nToday, that cost is near zero. If you don’t like the screenshot tool you’re paying $15 a year for, you can prompt an AI to build a clone in an afternoon.\n\nOn the surface, this looks like freedom. But look a little deeper. That $15 tool you just cloned? It was likely built by another indie developer. Someone who spent months thinking about edge cases, designing the interface, writing documentation, and supporting users. By cloning it just because you can, you aren’t just saving $15; you are actively devaluing the craft of independent software development and the livelihood of the person behind it.\n\nIf we all just clone everything we use, we completely commoditize the market. We create a sea of “good enough” AI-generated noise where no one can actually sustain a business.\n\nLet me paint a picture that I think a lot of developers are starting to recognize.\n\nYou spend weeks, maybe months, building something. You think about the problem, you design the interface, you handle the edge cases, you support your users, you write the docs. You pour yourself into it. Then one morning, someone sees your product, opens their AI editor, and builds a “good enough” version in an afternoon. They ship it. Maybe they make it free, maybe they make it open source, maybe they just use it themselves and tell their friends, their community, their followers.\n\nThey did not steal your code. They did not copy your product. They just… rebuilt it. Close enough. Good enough. And now your product has competition that cost someone a few hours of prompting while it cost you months of your life.\n\nBut it does not stop there. A third developer sees that clone and thinks, “I can do this too, but I want it slightly different.” So they prompt their own version. And a fourth. And a fifth. Each one is not a copy in the traditional sense. Nobody is violating a license. Nobody is stealing intellectual property. They are just building their own version that matches their use case.\n\nIt is a lot like art. You create a painting, something original, something you are proud of. Then somebody sees it and recreates it. Not a forgery, just their interpretation. But they have a bigger budget, a larger audience, better distribution. Suddenly their version is the one people see first. Others share that version instead of yours. This is what is happening a lot on social media with AI-generated content. The original creator is overshadowed by the faster, more accessible clone.\n\nIn the art world, we have a word for this erosion: it is called devaluation. In the software world, we are doing it at industrial scale, and we are calling it innovation.\n\nI am not saying you should never build something that already exists. Competition is healthy, and sometimes a fresh perspective genuinely improves a category. But there is a difference between thoughtful competition and reflexive duplication. The question every developer should ask themselves is: “If I know someone can clone my work in an afternoon, is it still worth building?”\n\nThe answer, I believe, is yes, but only for the things that cannot be cloned in an afternoon. The deep domain knowledge. The community around your tool. The years of user feedback baked into every feature. The trust you have earned. Those are the things AI cannot reproduce with a prompt, and I definitely don’t want to discourage people from building those things.\n\nBut you can only build those things if you commit to something long enough for them to develop. And that is the real danger of the current moment: not that AI makes building easy, but that it makes abandoning easy. Why invest years in one product when you can ship a new one every week?\n\nI have no room to preach. I am right there in the trenches with you.\n\nWhen I built Front Matter CMS, it was way before the AI boom. I had to think deeply about the problem because the investment of time was massive. I looked at the market, saw a gap in Visual Studio Code, and built it because nothing else existed.\n\nCompare that to recently. I built a set of cycling tools (never released by the way) for myself. Did similar tools exist? Absolutely. Were they better? Definitely. But I wanted to see how far I could get with AI. I treated it as a training exercise. In the end, I started paying for a tool called Join, which does the same thing, because it was better and I could focus on my actual work instead of maintaining a tool that was just “good enough” for me.\n\nI did the same with FrameFit. I investigated the market a little, didn’t see an exact match, and just started building.\n\nThere is a difference between building for education (learning how AI tools work) and releasing products that dilute the hard work of others. My worry is that we are blurring that line. We are shipping our “training exercises” as products, and it is making the ecosystem messy for everyone.\n\nAnd I know this because I have been on both sides of it.\n\nHere is the thing that made me stop and reflect. I have projects on both sides of this line, and they feel completely different.\n\nDemo Time is something I have been building for years. Not weeks, not weekends, years. It started because I was a conference speaker who kept running into the same problem: demos failing on stage. Nobody had built a proper solution inside Visual Studio Code, so I did. Over time, it grew because I kept showing up. I used it at conferences, talked to other speakers, iterated based on real feedback from people doing real presentations at events like Microsoft Ignite, GitHub Universe, and OpenAI DevDays. Today it has over 26,000 installations.\n\nNone of that came from code. The code is open source. Anyone can see it, fork it, or rebuild it. Someone could probably vibe-code a basic version in a weekend. But what they cannot replicate is twelve years of conference speaking that taught me what presenters actually need. You would need that experience, or a big company and budget behind you, to even come close. The relationships with the community, the trust that comes from being the person who shows up, year after year, and keeps making the tool better because you genuinely use it yourself. That is not something you can prompt into existence.\n\nCompare that to FrameFit. I built it, I use it, and it works. But if it disappeared tomorrow, I wouldn’t lose any sleep over it. Demo Time? That is like a child to me. I put my passion into it.\n\nThat contrast taught me something important: AI cannot commoditize the human context around software. Community, trust, domain expertise, showing up consistently over time. These are not features you ship. They are moats you build by caring about something longer than a weekend.\n\nThe developers who will thrive are not the fastest shippers. They are the ones who pair AI speed with human judgment. Who build communities, not just codebases. Who invest in trust, not just features. But that only happens if we slow down enough to think about what we are doing.\n\nWe need to re-introduce friction into our process. Not the old friction of writing boilerplate code. That friction is gone, and good riddance. I am talking about the friction of thinking. The pause that forces you to examine your intentions before you act on them.\n\nBefore AI, “thinking” was mandatory. The cost of building was high enough that it naturally filtered out bad ideas. Now, that filter is gone, and thinking must be a conscious, deliberate choice. When I have an idea now, I am trying to force myself to pause before I open Visual Studio Code or prompt a new agent.\n\nI try to run through these four questions:\n\nThat last one is crucial. If there is an open-source tool that does 80% of what you want, the “old” way was to contribute a Pull Request. The “AI way” often tempts us to just rebuild the whole thing from scratch because it feels faster.\n\nBut “faster” isn’t always “better” for the community. And here is the irony: we could use AI itself for this thinking step. Instead of prompting an LLM to start building, prompt it to research what already exists first. Use AI for the thinking, not just the building.\n\nI don’t expect AI platforms that allow you to vibe code to solve this for us. Their business model is predicated on you writing more code (read: prompts), not less. They want you to spin up new projects constantly. They have no incentive to say, “Hey, wait, this already exists.”\n\nThink about it: when was the last time you saw a developer advocate from one of these platforms demonstrate how to contribute to an existing project instead of building something new from scratch? Their marketing is all about speed, novelty, and the thrill of creation. Not about responsibility.\n\nSo, I started thinking: What if we used AI to stop us from building with AI? You could say that this is a paradox, but I think it is actually a necessary evolution of our responsibility as developers.\n\nI am exploring the idea of a Product Moral Compass Agent.\n\nImagine a mandatory first step in your “vibe coding” workflow. Before you start generating code, you pitch your idea to this agent. It interviews you, not to judge you, but to make sure you are making an informed decision.\n\nThis agent would act as the “thinking partner” we are skipping. It could:\n\nIf you still want to build it after that? Great. Go ahead and start coding. But at least you are making an informed, conscious decision rather than reflexively adding more noise to the world.\n\nI am currently building this agent. The first version is available on GitHub: Product Moral Compass Agent. Yes, I am aware of the irony, I am proposing to build something new to stop people from building new things. But I ran it through my own four questions first, and nothing like it exists yet.\n\nOnce it is ready, I will share it openly so that any developer can use it as part of their workflow. Not as a gatekeeper, but as a guide. A thinking partner that helps you pause, research, and decide before you build.\n\nIn the meantime, here is what you can do right now: the next time you have an idea, spend ten minutes with your favorite AI tool and ask it to find every existing solution first. Check your own bank statements. Are you already paying for a tool that solves this? If so, respect that developer’s work. Look at GitHub. Is there a repo that could use your help instead of your competition?\n\nThe time to learn is right now, but the time to think is also right now.\n\nI want you to keep building. I want you to be prolific. But let’s not let the ease of creation destroy the value of what we create.\n\nI am curious to hear your thoughts. Is this gatekeeping, or is it a necessary evolution of our responsibility as developers? Let me know in the comments below.\n\nIs an AI able to write the contents of your article? Well, that was a question I had and wanted to find out. In this article I tell you all about it.\n\nDiscover the latest advancements in documentation technology and how tools like GitHub Copilot for Docs, Mendable, and OpenAI are changing the game.\n\nDiscover how to leverage Azure AI Translator's Sync API for real-time document translation, simplifying your workflow and enhancing user experience.\n\nFound a typo or issue in this article? Visit the GitHub repository \nto make changes or submit a bug report.\n\nSolutions Architect & Developer Expert\n\nEngage with your audience throughout the event lifecycle",
    "readingTime": 12,
    "keywords": [
      "visual studio",
      "product moral",
      "compass agent",
      "studio code",
      "edge cases",
      "necessary evolution",
      "vibe coding",
      "software development",
      "github copilot",
      "tool"
    ],
    "qualityScore": 1,
    "link": "https://www.eliostruyf.com/killing-indie-development-with-ai/",
    "thumbnail_url": "https://www.eliostruyf.com/social/5f59a11b-79bb-48df-9b89-b8abc9ba3037.png",
    "created_at": "2026-02-16T12:38:09.254Z",
    "topic": "tech"
  },
  {
    "slug": "kanvibe-kanban-board-that-autotracks-ai-agents-via-hooks",
    "title": "KanVibe – Kanban board that auto-tracks AI agents via hooks",
    "description": "Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking — manage tmux/zellij sessions and git worktrees from one board. - rookedsysc/kanvibe",
    "fullText": "rookedsysc\n\n /\n\n kanvibe\n\n Public\n\n Self-hosted Kanban board with browser terminals for AI coding agents. Hook-driven auto-tracking — manage tmux/zellij sessions and git worktrees from one board.\n\n License\n\n AGPL-3.0 license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n rookedsysc/kanvibe",
    "readingTime": 1,
    "keywords": [
      "board",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/rookedsysc/kanvibe",
    "thumbnail_url": "https://opengraph.githubassets.com/1686c5ce06bcd0be96aea5e2e16beffdd136eeb70f17f431b75349727b34dbe2/rookedsysc/kanvibe",
    "created_at": "2026-02-16T12:38:09.047Z",
    "topic": "tech"
  },
  {
    "slug": "fake-job-recruiters-hide-malware-in-developer-coding-challenges",
    "title": "Fake job recruiters hide malware in developer coding challenges",
    "description": "A new variation of the fake recruiter campaign from North Korean threat actors is targeting JavaScript and Python developers with cryptocurrency-related tasks.",
    "fullText": "A new variation of the fake recruiter campaign from North Korean threat actors is targeting JavaScript and Python developers with cryptocurrency-related tasks.\n\nThe activity has been ongoing since at least May 2025 and is characterized by modularity, which allows the threat actor to quickly resume it in case of partial compromise.\n\nThe bad actor relies on packages published on the npm and PyPi registries that act as downloaders for a remote access trojan (RAT). In total, researchers found 192 malicious packages related to this campaign, which they dubbed 'Graphalgo'.\n\nResearchers at software supply-chain security company ReversingLabs say that the threat actor creates fake companies in the blockchain and crypto-trading sectors and publishes job offerings on various platforms, like LinkedIn, Facebook, and Reddit.\n\nDevelopers applying for the job are required to show their skills by running, debugging, and improving a given project. However, the attacker's purpose is to make the applicant run the code.\n\nThis action would cause a malicious dependency from a legitimate repository to be installed and executed.\n\n\"It is easy to create such job task repositories. Threat actors simply need to take a legitimate bare-bone project and fix it up with a malicious dependency and it is ready to be served to targets,\" the researchers say.\n\nTo hide the malicious nature of the dependencies, the hackers host the dependencies on legitimate platforms, like npm and PyPi.\n\nIn one case highlighted in the ReversingLabs report, a package named ‘bigmathutils,’ with 10,000 downloads, was benign until it reached version 1.1.0, which introduced malicious payloads. Shortly after, the threat actor removed the package, marking it as deprecated, likely to conceal the activity.\n\nThe Graphalgo name of the campaign is derived from packages that have “graph” in their name. They typically impersonate legitimate, popular libraries like graphlib, the researchers say.\n\nHowever, from December 2025 onward, the North Korean actor shifted to packages with “big” in their name. However, ReversingLabs has not discovered the recruiting part, or the campaign frontend, related to them.\n\nAccording to the researchers, the actor uses Github Organizations, which are shared accounts for collaboration across multiple projects. They say that the GitHub repositories are clean, and malicious code is introduced indirectly via dependencies hosted on npm and PyPI, which are the Graphalgo packages.\n\nVictims running the project as instructed in the interview infect their systems with these packages, which install a RAT payload on their machines.\n\nIt is worth noting that ReversingLabs researchers identified several developers that fell for the trick and contacted them \n\nThe RAT can list the running processes on the host, execute arbitrary commands per instructions from the command-and-control (C2) server, and exfiltrate files or drop additional payloads.\n\nThe RAT checks whether the MetaMask cryptocurrency extension is installed on the victim’s browser, a clear indication of its money-stealing goals.\n\nIts C2 communication is token-protected to lock out unauthorized observers, a common tactic for North Korean hackers.\n\nReversingLabs has found multiple variants written in JavaScript, Python, and VBS, showing an intention to cover all possible targets.\n\nThe researchers’ attribute the Graphalgo fake recruiter campaign to the Lazarus group with medium-to-high confidence. The conclusion is based on the approach, the use of coding tests as an infection vector, and the cryptocurrency-focused targeting, all of which aligning with previous activity associated with the North Korean threat actor.\n\nAlso, the researchers note the delayed activation of malicious code in the packages, consistent with Lazarus' patience displayed in other attacks. Finally, the Git commits show the GMT +9 time zone, matching North Korea time.\n\nThe complete indicators of compromise (IoCs) are available in the original report. Developers who installed the malicious packages at any point should rotate all tokens and account passwords and reinstall their OS.\n\nModern IT infrastructure moves faster than manual workflows can handle.\n\nIn this new Tines guide, learn how your team can reduce hidden manual delays, improve reliability through automated response, and build and scale intelligent workflows on top of tools you already use.",
    "readingTime": 4,
    "keywords": [
      "korean threat",
      "fake recruiter",
      "recruiter campaign",
      "threat actors",
      "malicious dependency",
      "north korean",
      "threat actor",
      "malicious code",
      "researchers say",
      "malicious packages"
    ],
    "qualityScore": 1,
    "link": "https://www.bleepingcomputer.com/news/security/fake-job-recruiters-hide-malware-in-developer-coding-challenges/",
    "thumbnail_url": "https://www.bleepstatic.com/content/hl-images/2024/08/12/north-korean-hackers.jpg",
    "created_at": "2026-02-15T12:26:54.771Z",
    "topic": "tech"
  },
  {
    "slug": "remoteopencode-run-your-ai-coding-agent-from-your-phone-via-discord",
    "title": "Remote-OpenCode – Run your AI coding agent from your phone via Discord",
    "description": "Discord bot for remote OpenCode CLI access. Contribute to RoundTable02/remote-opencode development by creating an account on GitHub.",
    "fullText": "RoundTable02\n\n /\n\n remote-opencode\n\n Public\n\n Discord bot for remote OpenCode CLI access\n\n License\n\n MIT license\n\n 7\n stars\n\n 3\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n RoundTable02/remote-opencode",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/RoundTable02/remote-opencode",
    "thumbnail_url": "https://opengraph.githubassets.com/703cb5b3ac833c99f1222c718308e6f87968dda18183af7d724de78de94b9a70/RoundTable02/remote-opencode",
    "created_at": "2026-02-15T06:38:30.422Z",
    "topic": "tech"
  },
  {
    "slug": "claude-agent-in-vs-code-no-extension-required-copilot-subscription-supported",
    "title": "Claude Agent in VS Code: no extension required, Copilot subscription supported",
    "description": "Learn how to use third-party agents like Claude Agent and OpenAI Codex for autonomous coding tasks in VS Code, powered by your GitHub Copilot subscription.",
    "fullText": "Third-party agents in Visual Studio Code are AI agents developed by external providers, such as Anthropic and OpenAI. Third-party agents enable you to use the unique capabilities of these AI providers, while still benefiting from the unified agent sessions management in VS Code and the rich editor experience for coding, debugging, testing, and more. In addition, you can use these providers with your existing GitHub Copilot subscription.\n\nVS Code uses the provider's SDK and agent harness to access the agent's unique capabilities. You can use both local and cloud-based third-party agents in VS Code. Integration with cloud-based third-party agents is enabled through your GitHub Copilot plan.\n\nThird-party coding agents in the cloud are currently in preview.\n\nThe benefits of using third-party agents in VS Code are:\n\nClaude agent sessions provide agentic coding capabilities powered by Anthropic's Claude Agent SDK directly in VS Code. The Claude agent operates autonomously on your workspace to plan, execute, and iterate on coding tasks with its own set of tools and capabilities.\n\nEnable or disable support for Claude agent sessions with the github.copilot.chat.claudeAgent.enabledOpen in VS CodeOpen in VS Code Insiders setting.\n\nTo start a new Claude agent session:\n\nOpen the Chat view (⌃⌘I (Windows, Linux Ctrl+Alt+I)) and select New Chat (+).\n\nChoose between a local or cloud agent session:\n\nFor a local session, select Claude from the Session Type dropdown\n\nFor a cloud session, select Cloud from the Session Type dropdown. Then, select Claude from the Partner Agent dropdown.\n\nEnter your prompt and let the agent work on the task\n\nThe Claude agent autonomously determines which tools to use and makes changes to your workspace.\n\nThe Claude agent provides specialized slash commands for advanced workflows. Type / in the chat input box to see the available commands.\n\nClaude agent requests permission before performing certain operations. By default, file edits within your workspace are auto-approved, while other operations like running terminal commands might require confirmation.\n\nYou can choose how the agent applies changes to your workspace:\n\nThe github.copilot.chat.claudeAgent.allowDangerouslySkipPermissionsOpen in VS CodeOpen in VS Code Insiders setting bypasses all permission checks. Only enable this in isolated sandbox environments with no internet access.\n\nThe OpenAI Codex agent uses OpenAI's Codex to perform coding tasks autonomously. Codex runs can run interactively in VS Code or unattended in the background.\n\nOpenAI Codex in VS Code enables you to use your Copilot Pro+ subscription to authenticate and access Codex without additional setup. Get more information about GitHub Copilot billing and premium requests in the GitHub documentation.\n\nTo start a new OpenAI Codex agent session:\n\nOpen the Chat view (⌃⌘I (Windows, Linux Ctrl+Alt+I)) and select New Chat (+).\n\nChoose between a local or cloud agent session:\n\nFor a local session, select Codex from the Session Type dropdown\n\nFor a cloud session, select Cloud from the Session Type dropdown. Then, select Codex from the Partner Agent dropdown.\n\nEnter your prompt in the chat editor input and let the agent work on the task\n\nYes, third-party agents in VS Code authenticate and manage billing through your existing GitHub Copilot subscription. For cloud-based third-party agents, follow the steps to enable the agent.\n\nFor cloud-based third-party agents, availability might be limited based on your Copilot subscription plan. Check About Third-party agents in the GitHub documentation \n\nBoth the provider's VS Code extension and the third-party agent integration in VS Code let you use the provider's AI capabilities and agent harness. The difference is in billing: when you use third-party agents in VS Code, GitHub bills you through your Copilot subscription. When you use the provider's extension, you are billed through the provider's subscription.\n\nVS Code lets you choose between local and cloud-based third-party agents, depending on the provider's availability. When you select the third-party agent from the Session Type dropdown, a local agent session is created for that provider.\n\nTo choose a cloud-based third-party agent, first select the Cloud option from the Session Type dropdown, and then select the provider from the Partner Agent dropdown.",
    "readingTime": 4,
    "keywords": [
      "vs code",
      "view windows",
      "windows linux",
      "linux ctrl+alt+i",
      "chat view",
      "existing github",
      "github documentation",
      "dropdown enter",
      "session type",
      "copilot subscription"
    ],
    "qualityScore": 1,
    "link": "https://code.visualstudio.com/docs/copilot/agents/third-party-agents#_claude-agent-preview",
    "thumbnail_url": "https://code.visualstudio.com/assets/docs/copilot/shared/github-copilot-social.png",
    "created_at": "2026-02-14T12:25:09.835Z",
    "topic": "tech"
  },
  {
    "slug": "orangensaft-a-mini-pythonlike-language-with-llm-eval-in-lang-runtime",
    "title": "Orangensaft – A mini Python-like language with LLM eval in lang runtime",
    "description": "A new age post-AI programming language. Contribute to jargnar/orangensaft development by creating an account on GitHub.",
    "fullText": "jargnar\n\n /\n\n orangensaft\n\n Public\n\n A new age post-AI programming language\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jargnar/orangensaft",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/jargnar/orangensaft",
    "thumbnail_url": "https://opengraph.githubassets.com/fc5976b41bfb8b1ea8e07bed5ea5b7596101078e6c57161a28c5d0aea8c3d834/jargnar/orangensaft",
    "created_at": "2026-02-13T12:34:58.006Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-raises-30bn-in-latest-round-valuing-claude-bot-maker-at-380bn",
    "title": "Anthropic raises $30bn in latest round, valuing Claude bot maker at $380bn",
    "description": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (£22bn) in a funding round that more than doubled its valuation to $380bn.\nThe company’s previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n Continue reading...",
    "fullText": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\n\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (£22bn) in a funding round that more than doubled its valuation to $380bn.\n\nThe company’s previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n\nThe fundraising was announced amid a series of stock market moves against industries that face disruption from the latest models, including software, trucking and logistics, wealth management and commercial property services.\n\nThe funding round, led by the Singapore sovereign wealth fund GIC and the hedge fund Coatue Management, is among the largest private fundraising deals on record.\n\n“Anthropic is the clear category leader in enterprise AI,” said Choo Yong Cheen, the chief investment officer of private equity at GIC.\n\nAnthropic said its annualised revenue – an estimate of full-year sales based on recent company data – had reached $14bn, having grown more than tenfold in each of the past three years. A significant driver of recent growth has been Claude Code, the company’s AI-powered coding tool that became generally available in May 2025.\n\nAnthropic’s rival OpenAI, backed by Microsoft and SoftBank, has been assembling what is reportedly a far larger round of up to $100bn that would value the ChatGPT developer at about $830bn.\n\nThe staggering sums being raised reflect equally staggering burn rates, with the companies spending cash to cover their huge costs of computing and attracting researcher talent.\n\nAnthropic has forecast reducing its cash burn to roughly a third of revenue in 2026 and just 9% by 2027, with a break-even target of 2028 – two years ahead of its rival, according to reports. Both companies are widely expected to pursue initial public offerings in the second half of 2026.\n\nThe rapid valuation increases for leading AI startups such as Anthropic and OpenAI, whose price tags far exceed those of many of the US’s largest listed companies, has alarmed some observers. Last year, a leading British tech investor, James Anderson, said he found sharp increases in valuations of companies such as OpenAI and Anthropic “disconcerting”.\n\nSome listed firms at the forefront of the AI industry have also come under stock market pressure in recent days.\n\nShares in Alphabet, Google’s parent company, have fallen by 4.2% so far this week, indicating some investors are still spooked by the big AI-related spending plans it laid out this month. Meta has declined by 1.7% during this week. Shares in Nvidia, a leading chipmaker and key provider of AI infrastructure, dropped by 1.6% on Thursday amid a wider sell-off but have been flat on the week.\n\n“A gloomy session on Wall Street on Thursday put investors in a grumpy mood at the end of the trading week,” said Russ Mould, the investment director at investment platform AJ Bell.\n\n“Association with AI has gone from party to peril as investors reappraise what the technology means for companies. \n\n “Some are concerned about excessive levels of spending and others fear AI will disrupt multiple industries. It all adds up to a cocktail of worries and that’s bad for market sentiment more broadly,” Mould added.\n\nFounded in 2021 by the siblings Dario and Daniela Amodei, both former executives at OpenAI, Anthropic has positioned itself as a safety-focused alternative in the AI race.\n\nThe funding round also comes shortly after Anthropic’s first television commercials were broadcast during Super Bowl LX, using the campaign to emphasise that its products remain ad-free. The ads took an apparent jab at OpenAI, which has begun to introduce advertising into the free version of ChatGPT.\n\nAnthropic’s earlier backers include Amazon, which has invested $8bn and serves as a primary computing partner through its datacentres, as well as Google, which invested $2bn in 2023.\n\nAgence France-Presse contributed to this article",
    "readingTime": 4,
    "keywords": [
      "annualised revenue",
      "stock market",
      "funding round",
      "investment",
      "leading",
      "investors",
      "anthropic",
      "chatbot",
      "coding",
      "tenfold"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/12/anthropic-funding-round",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b30b904b79d26877d4b860af0a6c67c5b55a0067/735_0_2715_2172/master/2715.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d2f7a8edb124095d9242575a7c8b9ac3",
    "created_at": "2026-02-13T12:34:48.923Z",
    "topic": "tech"
  },
  {
    "slug": "these-y-combinator-founders-raised-10-million-to-get-corporate-america-into-vibe-coding-read-their-pitch-deck",
    "title": "These Y Combinator founders raised $10 million to get corporate America into vibe coding. Read their pitch deck.",
    "description": "Vybe, a startup by Y Combinator alumni, raises $10M to integrate vibe coding into corporations, with participation from major tech leaders.",
    "fullText": "Vybe, the startup cofounded by two repeat Y Combinator entrepreneurs, has raised $10 million to bring vibe coding inside large companies.\n\nThe practice of vibe coding has become more widespread as it allows people to use AI and natural language prompts to build an app, rather than traditional programming. The sector has attracted hundreds of millions in venture funding and is blurring the lines for businesses deciding between buying software or just vibe-coding their own tools.\n\nWhile popular vibe-coding products like Lovable and Replit have made it easy for companies to create prototypes and landing pages, Vybe offers stronger security that cannot be modified by AI and also taps into internal data systems, said cofounder and CEO Quang Hoang.\n\nHoang previously founded the engineering mentorship platform Plato and is building Vybe alongside cofounder and CTO Fabien Devos, founder of the security AI startup Wolfia.\n\nVybe lets teams across an organization collaborate on apps. Engineering teams manage access to internal systems like Salesforce, Snowflake, and Databricks, while business teams can build apps for onboarding, performance reviews, customer service, and more.\n\n\"The new SaaS is going to be more like Legos,\" Hoang said, referring to software-as-a-service. \"You can build it exactly how you want.\"\n\nVybe also offers app templates created by high-profile executives, such as former Airbnb product leader Lenny Rachitsky and Front cofounder Mathilde Collin, that users can \"remix\" to suit their needs, including templates for performance reviews and one-on-one meetings.\n\nFirst Round Capital led Vybe's seed round, which featured participation from Y Combinator, the CEOs of Datadog and Grammarly, and product leaders from OpenAI and Meta.\n\nVybe has six employees and dozens of customers, Hoang said. It plans to use the funding to hire engineers. Vybe charges $12 per user a month, plus usage-based credits for app development.\n\nThe round comes as vibe coding continues to attract major funding — with startups like Emergent pulling in tens of millions from Khosla Ventures and SoftBank — even as Barclays analysts warn the initial boom may be cooling.\n\nAt the same time, investors are also weighing how AI and vibe-coding tools can reshape the software sector. Software stocks have entered one of their sharpest downturns in years, shedding roughly $2 trillion in market capitalization.\n\nHere's a look at the pitch deck Vybe used to raise its $10 million seed round. One slide has been removed, and another redacted so that the deck can be shared publicly.",
    "readingTime": 3,
    "keywords": [
      "performance reviews",
      "vibe coding",
      "seed round",
      "funding",
      "software",
      "vibe-coding",
      "cofounder",
      "teams",
      "vybe",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/vybe-pitch-deck-expand-vibe-coding-corporate-world-2026-2",
    "thumbnail_url": "https://i.insider.com/698df18dd3c7faef0ece2588?width=948&format=jpeg",
    "created_at": "2026-02-13T01:15:39.236Z",
    "topic": "finance"
  },
  {
    "slug": "a-software-engineer-says-theres-a-vampiric-effect-to-ai-where-vibecoding-sprints-are-followed-by-naps-to-cope-with-ai",
    "title": "A software engineer says there's a 'vampiric effect' to AI, where vibe-coding sprints are followed by naps to cope with AI fatigue",
    "description": "Steve Yegge, who was at Amazon in the early days and spent 12 years at Google, says his fellow engineers need to learn to say no.",
    "fullText": "A seasoned veteran said his fellow software engineers need to learn \"how to say 'no,' real fast\" or risk getting crushed by AI.\n\nSteve Yegge, who worked with Jeff Bezos at Amazon early on before 12-year stint at Google, said AI is set up in a way that can really drain you.\n\n\"There's a vampiric effect with AI, where it gets you excited, and you work really hard, and you're capturing a ton of value,\" he recently told the \"The Pragmatic Engineer\" newsletter/podcast.\n\nYegge said companies also need to understand that while agentic AI may make engineers more productive than ever before, pushing the limit will just burn out their workforce.\n\n\"I seriously think founders and company leaders and engineering leaders at all levels, all the way down to line managers, have to be aware of this and realize that you might only get three productive hours out of a person who's vibe coding at max speed,\" he said. \"So, do you let them work for three hours a day? The answer is yes, or your company's going to break.\"\n\nEngineers are beginning to vocalize concerns about \"AI fatigue.\" Business Insider recently spoke to Siddhant Khare, who builds AI tools, and wrote an essay about how AI has accelerated the pace of his job to a point that it was burning him out.\n\nYegge said that his fellow engineers need to set boundaries when they are vibe coding.\n\n\"People have to learn the art of pushing back,\" he said.\n\nUntil then, Yegge said he and his fellow engineers are napping and growing grumpier.\n\n\"I find myself napping during the day, and I'm talking to friends at startups, and they're finding themselves napping during the day,\" he said. \"We're starting to get tired and cranky.\"",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "fellow engineers",
      "napping",
      "learn",
      "recently",
      "productive",
      "pushing",
      "leaders",
      "hours",
      "yegge"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/software-engineer-steve-yegge-ai-burnout-2026-2",
    "thumbnail_url": "https://i.insider.com/698cd767e1ba468a96abdee5?width=1200&format=jpeg",
    "created_at": "2026-02-12T12:39:46.962Z",
    "topic": "tech"
  },
  {
    "slug": "minions-stripes-oneshot-endtoend-coding-agents",
    "title": "Minions: Stripe's one-shot, end-to-end coding agents",
    "description": "Minions are Stripe’s homegrown coding agents, responsible for more than a thousand pull requests merged each week. Though humans review the code, minions write it from start to finish. Learn how they work, and how we built them.",
    "fullText": "Minions: Stripe’s one-shot, end-to-end coding agents/Article/About the authorAlistair GrayAlistair is a software engineer on the Leverage team at Stripe./DocsExplore our guides and examples to integrate Stripe.Learn more/SocialYoutubeTwitter/XDiscordDocsDeveloper Meetups© 2025 Stripe, Inc.PrivacyLegalStripe.com",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents",
    "thumbnail_url": "https://images.ctfassets.net/fzn2n1nzq965/1IiEdIR6LsEY9aym5JQM6u/85a9a55f58137d9d1fac99319658081b/dev_social.png",
    "created_at": "2026-02-12T06:50:09.787Z",
    "topic": "tech"
  },
  {
    "slug": "hes-worked-decades-in-tech-and-wrote-a-book-on-vibe-coding-he-predicts-50-of-big-tech-engineers-will-be-laid-off",
    "title": "He's worked decades in tech and wrote a book on vibe coding. He predicts 50% of Big Tech engineers will be laid off.",
    "description": "Steve Yegge worked at Amazon an Google. Now, he predicts Big Tech companies will cut half their engineers to help pay for the others to have AI.",
    "fullText": "He worked in Big Tech. Now, he fears half of its software engineers are on the chopping block.\n\nSteve Yegge worked with Jeff Bezos in Amazon's early years. Then he went to Google, where he worked for over 12 years and earned the title of senior staff software engineer. He also knows a thing or two about AI engineering, having written a book on the topic titled \"Vibe Coding.\"\n\nOn \"The Pragmatic Engineer\" podcast and newsletter, Yegge described an imaginary dial of the percentage of engineering staff a company can lay off, ranging from zero to 100. He said he believes the dial is being set at 50 in the age of AI.\n\n\"You're going to have to get rid of half of them to make the other half maximally productive,\" Yegge said. \"We're going to lose around half the engineers from big companies, which is scary\".\n\nThere's a capital-to-labor tradeoff happening in tech. Companies are paying vast sums for tokens and enterprise AI licenses, GPUs, and computing capacity. That money needs to come from somewhere — and for some, it could come from labor costs.\n\nYegge pointed to this tradeoff as the reason for his forecast of 50% cuts becoming the norm. Companies will lay off some engineers to help pay for the others to have adequate access to AI, he said.\n\nHost Gergely Orosz said that 50% cuts would be more than during the COVID-19 pandemic, when tech went through an intense layoff cycle.\n\n\"It's going to be way bigger,\" Yegge said. \"It's going to be awful.\"\n\nThey aren't the only ones referencing the pandemic. In a popular X article, HyperWrite CEO Matt Shumer wrote that AI's impact on work would be \"much, much bigger than Covid.\"\n\nIn recent years, the question of AI-driven layoffs has haunted workers at Big Tech companies, many of which went on a hiring spree during the pandemic. While it's often impossible to boil job loss down a single reason, many suspect productivity gains from AI and ballooning capex spending are fueling the cuts. Some business leaders have also explicitly cited AI when announcing layoffs.\n\nMeanwhile, tech leaders describe smaller teams working more efficiently. On a recent earnings call, Meta CEO Mark Zuckerberg said one engineer could now do the work of a whole team, thanks to AI.\n\nAs AI productivity grows, so have complaints of AI fatigue. Software engineer Siddhant Khare penned a lengthy essay about growing more productive, but also feeling more drained than ever. He told Business Insider that he felt like a \"reviewer\" as opposed to an engineer.\n\nYegge said it's not all bad news for the engineers — just those who want to work at a big company. Engineers who have \"seen the light\" are now getting together, leaving their companies, and creating startups that outpace the industry's giants, he said.\n\n\"We've got this mad rush of innovation coming up, bottom up,\" he said. \"And we've got knowledge workers being laid off by big companies because clearly big businesses are not the right size anymore.\"",
    "readingTime": 3,
    "keywords": [
      "software engineer",
      "engineers",
      "half",
      "it's",
      "cuts",
      "pandemic",
      "staff",
      "engineering",
      "dial",
      "productive"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/steve-yegge-vibecoding-author-predicts-layoffs-half-big-tech-engineers-2026-2",
    "thumbnail_url": "https://i.insider.com/698c9571e1ba468a96abd6bb?width=1200&format=jpeg",
    "created_at": "2026-02-12T01:12:22.798Z",
    "topic": "finance"
  },
  {
    "slug": "the-skill-nobody-tests-for",
    "title": "The Skill Nobody Tests For",
    "description": "Drop candidates into a real coding environment with AI tools. See who actually ships.",
    "fullText": "Drop candidates into a real coding environment with AI tools. See who actually ships.\n\nFree to start · Set up in 2 minutes\n\nChallenge: Build an RBAC permission system\n\nTools: Claude Code · VS Code · Terminal\n\nMeta, Shopify, Cisco — companies are dropping algorithm interviews. The question isn't whether this changes hiring. It's whether you move first.\n\nOver the last year, the best engineers have completely changed how they work. They don't write code by hand anymore, they talk to agents, interrogate them, make architectural decisions, scope projects, and verify output.\n\nA full VS Code workspace in the browser. Terminal, files, packages — everything your engineers use daily.\n\nClaude Code, Codex, Cursor, Copilot — the same tools your team uses. The challenge is using them well, not memorizing algorithms.\n\nAutomated tests validate the solution. You see what they built, how they got there, and whether it works.",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "tools",
      "terminal",
      "engineers",
      "challenge"
    ],
    "qualityScore": 0.75,
    "link": "https://vibearena.io/blog/the-skill-nobody-tests-for",
    "thumbnail_url": "https://vibearena.io/og-image.png",
    "created_at": "2026-02-11T12:42:57.921Z",
    "topic": "tech"
  },
  {
    "slug": "the-future-of-coding-agents-is-vertical-integration-and-why-acp-matters",
    "title": "The future of coding agents is vertical integration (and why ACP matters)",
    "description": "A look at the limitations of generic agents and MCPs, through the lens of building Tidewave, and what this means for the future of coding agents",
    "fullText": "The first appearances of coding agents were within our editors, as a natural progression from tab autocompletion. When Claude Code was released, almost a year ago, it innovated in many ways, particularly by demonstrating that although we write almost all of our code in editors, the agents themselves don’t have to be constrained to IDEs. As a result, most of our agentic development moved to the terminal.\n\nWith the release of the Codex App, we will start to see a push towards richer interfaces. However, this raises the question: if we can run coding agents anywhere, where is the best place to run them?\n\nIn this section, I will talk about Tidewave, the coding agent we built for full-stack web development. It is a sales pitch but I hope it is a good example of what we can gain by building vertical coding agents.\n\nTidewave was born out of frustration when using coding agents to build web applications. I frequently stumbled upon scenarios such as:\n\nThe agent would tell me a feature was complete, but when I tried it in the browser, form submission would not complete\n\nWhenever I encountered an exception page during development, I had to copy and paste stacktraces from the browser to the agent\n\nI had to constantly translate what was on the page to source code. If I wanted to “Add an Export to CSV button to a dropdown button”, the first step was typically to find where the template or component the dropdown was defined in\n\nUltimately, because the coding agent was unable to interact with or see what it produced, I had to act as the middleman:\n\nAnd while things have improved within the last year, most of the tooling out there does not fully close this gap. For example, while coding agents can read logs and editors like Cursor do include a browser, they do not correlate the browser actions with the logs, leaving space for guess work and context bloat. Or when the browser renders a page, it doesn’t know which controller or template it came from.\n\nWe solved this problem by moving the coding agent to the browser and making it aware of your web app:\n\nYou open up Tidewave in your favorite browser and its agent can now interact with the page you are looking at, validating the application works as desired by accessing the DOM, loading pages, filling forms, etc\n\nWhen something goes wrong, we parse and feed the exact information from the rendered error page. We also include any console.log and server logs related to the particular agent action (instead of the whole history)\n\nWhen you inspect an element in Tidewave, we automatically map DOM elements to source files. If you inspect a dropdown, we tell the agent its location on the page as well as the template/component it came from in the source code\n\nThe coding agent has access to your backend too, so it can read documentation, execute SQL queries, and even gets a REPL-like environment. So when the agent gets stuck, it can navigate the data, run code, and explore APIs, like any developer would\n\nMore importantly, this is all implemented by connecting the agent to your running application. You can ask your agent to analyze cache usage in your Rails application or to debug a LiveView in Phoenix, and it can do so because they talk to each other directly. Your agent is no longer limited to a series of bash commands.\n\nOverall, the agent becomes more capable, and we no longer need to act as the middleman:\n\nFor all of the above to work, instead of building a generic agent that works with every tool, Tidewave directly integrates with each web framework we support (Django, FastAPI, Flask, Next.js, Phoenix, Rails, and TanStack Start), mapping all layers to the agent, from the database to UI.\n\nAnd that’s what I mean by vertical integration: coding agents become integrated into the specific platform or runtime they’re building for. They understand the relationship between code and its live behavior, and can debug both.\n\nWe can easily draw parallels between the above and other domains. Agents for mobile development must have full access to a simulator to interact with the app UI, translate view hierarchies to source code, and monitor network requests. Similar patterns could apply to IoT devices, game development, etc.\n\nAs a matter of fact, this isn’t even a new idea. Data scientists were the first to embed agents directly into their notebooks, where they can execute code, visualize results, inspect dataframes, etc. They were the first to realize the benefit of vertically integrating agents and everyone else is just late to the party.\n\nBut how can we make these vertical integrations possible? If we want agents to access our runtimes and environments, could MCPs be the tool to make this a reality? That’s what we originally thought…\n\nTidewave was first prototyped as a Model Context Protocol (MCP) server and it quickly became obvious it wasn’t enough. In this section, I’ll focus on how MCPs can be detrimental to user experience. In particular, we’ll look at its limitations as a pull-only interface and its text-based constraints.\n\nAs an example, let’s look at Figma’s MCP. It allows developers to select an element in Figma’s desktop app and ask the agent to implement the design. Here’s a screenshot from their announcement:\n\nAs you can see, you’re required to make a selection in Figma, then explicitly ask the agent to query your selection. The agent then proceeds to talk to Figma and get the relevant data. That’s a lot of unecessary back and forth! If I already selected an element on Figma, it should take a single click to embed that information into my prompt.\n\nNow imagine if, every time I inspected an element in the browser, we had to tell the agent to ask the browser about the selection. We refused to implement that. Instead, we made it just work:\n\nFurthermore, by making the inspected elements part of our prompt, you can select multiple elements and dictate how they all need to change at once. You can reorder, swap, and coordinate multiple changes easily.\n\nSimilarly, when you see an error page in the browser, you should only need to click a button to fix it - not copy and paste stacktraces or ask the agent to read logs.\n\nNone of this could be built with MCPs because they are pull-based. The agent decides when to invoke the MCP and you are forced to ask the agents to perform actions on your behalf. MCPs do not allow us to attach rich metadata to prompts either.\n\nImagine you need help addressing comments in a pull request. You can tell the agent to use GitHub’s MCP or even use a custom skill that uses GitHub’s CLI to fetch all current comments. But once the agent does its initial assessment, all further exchanges happens through text: “ignore this comment”, “no, not this comment, the one above it”.\n\nNow compare that to using any graphical (or even a terminal) interface: we can reply in thread, comment on specific lines, or click a button to dismiss invalid feedback altogether. In other words, when we put our integrations behind MCPs, all exchange happens on text, and we often end-up with worse user experiences.\n\nInstead, our goal is to build agentic tools and user experiences side-by-side. For example, when we added Web Accessibility diagnostics to Tidewave, we exposed additional tools to the agent, but we also provided a polished experience for when developers want to remain in the loop:\n\nOur recently announced Supabase integration works the same: it runs performance and security advisors on your database and present them to you. If you want to dig deeper, you can do it all through a rich interface. And if you don’t care about any of that, you can either click the “Fix all” button or ask your agent to do it for you.\n\nTo build these experiences, we had to invert the control. We don’t want the agent to call us, we want our tools to call the agent.\n\nSay you want to build your own agentic loop, up until recently, doing so meant to:\n\nWhile the above is already a lot of work, you must remember that prompt engineering, tool calls, etc. must be fine-tuned per model. For example, when OpenAI announced GPT-5-Codex, integrators had to write new prompts and new tools, even if they already supported GPT-5.\n\nAnd the above covers only the basic loop! What about AGENTS.md, MCPs, or skills? And you haven’t even started working on what makes your coding agent unique.\n\nLuckily for us, many of the leading AI companies have built their own coding agents (Claude Code, Codex, Gemini CLI) tailored to their models, and many have exposed those agents through SDKs. We also have fantastic open source alternatives. This means you can cut out much of the work above, but, if you want to support multiple providers, you’re still on the hook for integrating them one by one.\n\nThat’s exactly where the Agent Client Protocol from the Zed team comes in. It standardizes communication with coding agents, so you implement one protocol and support multiple agents at once. This ultimately allows you to focus on the runtime integrations and user experiences that make your coding agent unique.\n\nFurthermore, even if you’re not implementing your own agent, ACP should matter to you because it brings portability to developers. You can use a single coding agent across the terminal, your editor, and Tidewave, reusing the same settings for hooks, skills, and subagents. And perhaps more importantly, the same subscription.\n\nWe are big fans of the Agent Client Protocol and we are excited to build on top of it alongside companies like Zed and JetBrains.\n\nGeneric agents that work everywhere force awkward workflows where we constantly act as a middleman. Instead, we want to embed agents into the platforms we are building on, so they can see the relationship between code and behavior, interact with live apps, and debug in real-time.\n\nHowever, to build these rich experiences, we need to invert the control and own the agentic loop. The Agent Client Protocol (ACP) allows us to do so by standardizing communication across providers, so we can focus on the user experiences and integrations that make our coding agents unique, instead of wrestling with multiple SDKs.\n\nI hope this article inspires you to fine-tune your own coding agents and, if you are looking for a coding agent tailored to full-stack web development, give Tidewave a try!",
    "readingTime": 9,
    "keywords": [
      "client protocol",
      "claude code",
      "paste stacktraces",
      "agent client",
      "full-stack web",
      "error page",
      "user experiences",
      "agentic loop",
      "web development",
      "embed agents"
    ],
    "qualityScore": 1,
    "link": "https://tidewave.ai/blog/the-future-of-coding-agents-is-vertical-integration",
    "thumbnail_url": "https://tidewave.ai/assets/opengraph/pthe-future-of-coding-agents-is-vertical-integration-4pklmn64vmtcox2ktoauj7nxoy-a84d118a102998c2aa4a25c686d9af29.png?vsn=d",
    "created_at": "2026-02-11T01:19:44.413Z",
    "topic": "tech"
  },
  {
    "slug": "this-ceo-wants-ai-agents-to-outnumber-his-human-employees-this-year",
    "title": "This CEO wants AI agents to outnumber his human employees this year",
    "description": "The CEO of StackBlitz, Eric Simons, says the web development startup has \"gone all in\" on AI agents.",
    "fullText": "StackBlitz CEO Eric Simons aims to have more AI agents than human employees working at the startup this year, a milestone he sees as emblematic of a broader shift underway across the software industry.\n\nSpeaking in an interview with Business Insider, Simons said StackBlitz has \"gone all in on agents,\" deploying internally built AI systems across business intelligence, coding, product development, customer support, and outbound sales.\n\nAI has become increasingly good at writing software code, to the point where the technology is beginning to change how companies are run. OpenClaw, an open-source AI assistant that works inside platforms like WhatsApp, Slack, and iMessage, is an early example of digital agents communicating and coordinating with one another without direct human involvement.\n\n\"To me, this is a crystal ball into the wildness of the inevitable future,\" Simons told Business Insider. \"Your AI agents will talk to other people's agents on your behalf, negotiating pricing for a product you want to buy, inquiring about availability for a restaurant, arguing your political viewpoints.\"\n\n\"Agents are an extension of yourself,\" he added. \"People will generally trust their agents with whatever they recommend them to buy, reserve, believe, or otherwise.\"\n\nSimons's comments come as software and SaaS stocks have slumped in recent weeks, a move he attributes to growing investor recognition that AI can now build software autonomously. As AI tools become more capable, he said, long-standing competitive moats based on specialized knowledge are eroding.\n\nHe compared the shift to the transformation of manufacturing over the past century. Craft knowledge once protected businesses, he said, but automation and digital design ultimately displaced many incumbents. \"Today, it's a CAD file you can 3D print,\" Simons said.\n\n\"While every individual does not use these profound new powers directly (like 3D printing a chair), there's an entire new generation of companies that do — and they have replaced the previous era of companies,\" Simons explained. \"They disrupted those incumbents by leveraging automation that made it far cheaper and at a massive scale not possible previously when moats were just knowledge and bare hands.\"\n\nFor businesses, the implications are stark. Even traditionally \"safe\" enterprise software may be vulnerable if AI agents can migrate or rebuild systems rapidly.\n\n\"What does it mean when all of software can be written, rewritten, migrated, or otherwise, 100x or 10,000x faster than it could've ever been done before, by a workforce that does not sleep, and can be parallelized near infinitely?\" Simons said. \"It's daunting and mind-bending, and I think the repricing of SaaS in public markets more accurately reflects this today.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "agents",
      "software",
      "knowledge",
      "human",
      "shift",
      "across",
      "systems",
      "product",
      "digital"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/tech-ceo-ai-agents-outnumber-human-employees-eric-simons-stackblitz-2026-2",
    "thumbnail_url": "https://i.insider.com/698b6915a645d11881890f3a?width=1200&format=jpeg",
    "created_at": "2026-02-11T01:19:42.667Z",
    "topic": "finance"
  },
  {
    "slug": "composer-15",
    "title": "Composer 1.5",
    "description": "Improved reasoning over challenging coding tasks by scaling RL over 20x.",
    "fullText": "A few months ago, we released our first agentic coding model, Composer 1. Since then, we've made significant improvements to the model’s coding ability.\n\nOur new release, Composer 1.5, strikes a strong balance between speed and intelligence for daily use. Composer 1.5 was built by scaling reinforcement learning 20x further on the same pretrained model. The compute used in our post-training of Composer 1.5 even surpasses the amount used to pretrain the base model.\n\nWe see continued improvements on coding ability as we scale. Measured by our internal benchmark of real-world coding problems, we find that the model quickly surpasses Composer 1 and continues to climb in performance. The improvements are most significant on challenging tasks.\n\nComposer 1.5 is a thinking model. In the process of responding to queries, the model generates thinking tokens to reason about the user’s codebase and plan next steps. We find that these thinking stages are critical to the model’s intelligence. At the same time, we wanted to keep Composer 1.5 fast and interactive for day-to-day use. To achieve a balance, the model is trained to respond quickly on easy problems with minimal thinking, while on hard problems it will think until it has found a satisfying answer.\n\nTo handle longer running tasks, Composer 1.5 has the ability to self-summarize. This allows the model to continue exploring for a solution even when it runs out of available context. We train self-summarization into Composer 1.5 as part of RL by asking it to produce a useful summary when context runs out in training. This may trigger several times recursively on hard examples. We find that self-summarization allows the model to maintain its original accuracy as context length varies.\n\nComposer 1.5 is a significantly stronger model than Composer 1 and we recommend it for interactive use. Its training demonstrates that RL for coding can be continually scaled with predictable intelligence improvements.\n\nLearn more about Composer 1.5 pricing here.",
    "readingTime": 2,
    "keywords": [
      "tasks composer",
      "coding ability",
      "model",
      "improvements",
      "intelligence",
      "context",
      "model’s",
      "balance",
      "surpasses",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/composer-1-5",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/composer-1.5-og.png",
    "created_at": "2026-02-10T06:54:10.475Z",
    "topic": "tech"
  },
  {
    "slug": "a16z-partner-says-that-the-theory-that-well-vibe-code-everything-is-flat-wrong",
    "title": "A16z partner says that the theory that we'll vibe code everything is 'flat wrong'",
    "description": "Vibe coding everything is just not worth it, says A16z partner Anish Acharya.",
    "fullText": "Vibe coding everything is just not worth it, says an Andressen Horowitz partner.\n\nOn an episode of the \"20VC\" podcast released on Monday, A16z general partner Anish Acharya said that companies shouldn't use AI-assisted coding for every part of their business.\n\nHe said that software accounts for 8% to 12% of a company's expenses, so using vibe coding to build the company's resource planning or payroll tools would only save about 10%. Relying on AI to write code also carries risks, he said.\n\n\"You have this innovation bazooka with these models. Why would you point it at rebuilding payroll or ERP or CRM,\" Acharya said, referring to enterprise resource planning and customer relationship management software. Salesforce, Microsoft, Oracle, and SAP are among the top providers of such software.\n\nInstead, companies are better off using AI to develop their core businesses or optimize the remaining 90% of their costs, said the venture capitalist of six years.\n\n\"Of course, there will be secular losers. There are specific business models that are now going to be disadvantaged,\" he said. \"But the general story that we're going to vibe code everything is flat wrong, and the whole market is oversold software.\"\n\nAcharya's comments follow a brutal week for software stocks, which dragged down tech and broader markets. The sell-off started when already-wary investors panicked about Anthropic's new AI tool, which can perform a range of clerical tasks for people working in the legal industry.\n\nThe A16z partner joins famed investor Vinod Khosla in saying that stock prices should be ignored when evaluating the future of tech companies.\n\nOn a podcast last month, Khosla dismissed talks of an AI bubble and said investors should not be concerned as long as API call volume, a benchmark of AI usage, remains high.\n\n\"If that's your fundamental metric of what's the real use of your AI, usefulness of AI, demand for AI, you're not going to see a bubble in API calls,\" he said. \"What Wall Street tends to do with it, I don't really care. I think it's mostly irrelevant.\"",
    "readingTime": 2,
    "keywords": [
      "resource planning",
      "vibe coding",
      "software",
      "partner",
      "everything",
      "podcast",
      "business",
      "company's",
      "payroll",
      "code"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/a16z-partner-says-businesses-cannot-vibe-code-everything-tech-stocks-2026-2",
    "thumbnail_url": "https://i.insider.com/698a9944a645d118818905c1?width=1200&format=jpeg",
    "created_at": "2026-02-10T06:54:04.849Z",
    "topic": "finance"
  },
  {
    "slug": "i-made-a-claude-code-guide-thats-a-win95-desktop-with-games",
    "title": "I made a Claude Code guide that's a Win95 desktop with games",
    "description": "A free interactive guide for PMs and CEOs leading dev teams through the AI coding transition. Score your codebase readiness, calculate ROI, and get a 90-day rollout plan.",
    "fullText": "Your developers have Copilot open. They autocomplete functions and paste errors into chat. That gets you maybe 10% of what AI coding tools can do now. The real shift is agentic: one engineer running 3-5 parallel AI sessions that write features, run tests, and open PRs.\n\nSeven chapters. Three interactive tools. One honest answer about your team's speed:\n\nMost teams think they're running Pentium. They're a 486.\n\nTechnical deep-dives are marked 📎 so you know what to forward to your engineering lead.",
    "readingTime": 1,
    "keywords": [
      "tools",
      "they're"
    ],
    "qualityScore": 0.55,
    "link": "https://gabezen.com/guide/",
    "thumbnail_url": "https://gabezen.com/guide/og.png",
    "created_at": "2026-02-10T01:21:45.458Z",
    "topic": "tech"
  },
  {
    "slug": "clog-track-and-compare-your-claude-code-usage",
    "title": "Clog – Track and compare your Claude Code usage",
    "description": "Track your Claude Code journey. Public leaderboard and profile system for developers. See how your coding sessions stack up, build streaks, and share your progress.",
    "fullText": "[join][--]clog - Claude Code Leaderboard// track your claude code sessions// leaderboard goes live soon_ready",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "leaderboard"
    ],
    "qualityScore": 0,
    "link": "https://clog.sh",
    "thumbnail_url": "https://clog.sh/clog-og-image.png",
    "created_at": "2026-02-10T01:21:44.351Z",
    "topic": "tech"
  },
  {
    "slug": "factory-factory-opensource-alternative-to-codex-app-for-claude",
    "title": "Factory Factory, open-source alternative to Codex App for Claude",
    "description": "Workspace-based coding environment for running multiple Claude Code sessions in parallel. - purplefish-ai/factory-factory",
    "fullText": "purplefish-ai\n\n /\n\n factory-factory\n\n Public\n\n Workspace-based coding environment for running multiple Claude Code sessions in parallel.\n\n factoryfactory.ai\n\n License\n\n MIT license\n\n 18\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n purplefish-ai/factory-factory",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/purplefish-ai/factory-factory",
    "thumbnail_url": "https://opengraph.githubassets.com/72f2d9d9898e13bed120d015134a857e14f77267ee4cd373e64559450fa53038/purplefish-ai/factory-factory",
    "created_at": "2026-02-09T12:42:23.472Z",
    "topic": "tech"
  },
  {
    "slug": "logifai-autocapture-dev-logs-for-ai-coding-assistants",
    "title": "Logifai – Auto-capture dev logs for AI coding assistants",
    "description": "Auto-capture development logs for Claude Code — stop copy-pasting terminal output. - tomoyaf/logifai",
    "fullText": "tomoyaf\n\n /\n\n logifai\n\n Public\n\n Auto-capture development logs for Claude Code — stop copy-pasting terminal output.\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n tomoyaf/logifai",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/tomoyaf/logifai",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1152746807/546b13ec-53a4-4596-a481-7a6c1210694a",
    "created_at": "2026-02-09T06:54:27.333Z",
    "topic": "tech"
  },
  {
    "slug": "i-see-how-important-ai-is-at-google-so-i-taught-my-kids-about-it-now-theyre-vibe-coding",
    "title": "I see how important AI is at Google, so I taught my kids about it. Now, they're vibe coding.",
    "description": "A Google executive explains why he taught his kids to understand AI — and how they ended up vibe coding and participating in a hackathon.",
    "fullText": "This as-told-to essay is based on a conversation with Asif Saleem, a financial services go-to-market lead for Japan and the Asia Pacific region at Google. Asif is the father of 13-year-old Usman Asif and 18-year-old Shanzey Asif. It's been edited for length and clarity.\n\nIt's important for parents to help kids understand we're going through a very transformative time. This is like the era when the internet or mobile first emerged. With AI, it may be even bigger.\n\nKids need to understand this and embrace the technology. Whatever they end up studying — computer science, English, or philosophy — they can make AI part of it.\n\nI was curious about Cursor and some of the other vibecoding tools, so I joined a few \"Code with AI\" weekend sessions for executives.\n\nI really enjoyed it. Within a matter of hours, we were able to develop different applications. I developed a statement analyzer for a financial services system.\n\nOnce I came back home, I spoke to my family about it and showed them the demo. My children, Usman and Shanzey, are both tech-savvy, so that drove their interest. They attended the same course a few weeks later.\n\nThey were the youngest in the class. The good part was that they were completely independent, so I let them be on their own. That's how they ventured into vibe coding and participated in Cursor's 24-hour weekend hackathon in Singapore.\n\nThey've become more curious about things. Through vibe coding, they've learned to be more creative and to use technology to understand how things work.\n\nNeither of them has a formal technology background — they're not software developers — but they've been able to think through ideas, be more creative, and use technology to solve problems.\n\nTechnology is the biggest enabler. The question is how humans use it — whether for good or bad. If it's for good, what are the guardrails?\n\nUsing it for good means creating value, solving real challenges, and making information more accessible.\n\nManaging screentime is the reason we developed a reward system for Usman because he's young and he loves gaming. Gaming comes as a reward for achieving goals. Those goals include making your own breakfast, making your bed, or helping clean the house.\n\nWhen Shanzey is studying, she can't use AI for the content she's creating — it can't even be AI-inspired. It has to be her original work.\n\nThat's super important because schools validate whether output is AI-generated. The same applies to Usman.\n\nIt's important to have both physical and digital skills. Usman plays football because physical activity is important at his age. Shanzey is focused on school right now — her exams are very important, and getting good grades matters.\n\nI'd say my role is more like a coach — brainstorming with them. My wife does the same, helping them think through ideas and keeping them honest.\n\nI'm often busy with work, so a lot of this couldn't be done without my wife's help.\n\nThere's nothing better than hands-on learning. That's something I've learned over the last few years.\n\nYou will face challenges and go through them. Parents should get their kids to do things, give them challenges, and nurture them as they work through those challenges. With hands-on experience, they can get better results faster.\n\nWith building an app, we co-create the idea, but then the important part is making it happen and reporting back on progress.\n\nAt Google, I work with enterprise customers trying to transform their businesses with AI. I can see what's happening on the ground and how things are changing.\n\nI also see a lot of young talent at Google. They come in thinking about creating apps and learning skills, and I mentor some of them as well.\n\nIt's important to communicate this to my family. I spent time helping them understand how the world is changing and why it's important for them to understand AI.\n\nIt's not about running away from technology. AI will keep advancing, and the only thing you can do is be accustomed to it, no matter what you want to become.\n\nWe're now in a situation where we have very intelligent large language models, and we're also moving toward agentic AI, where you can work with agents that help you do a lot more. Speed and agility are improving, and you can now work within larger ecosystems that combine humans and machines, achieving much more.\n\nIf you know how to coexist and let machines do meaningful work with you, that's a skill to aspire to.\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "financial services",
      "vibe coding",
      "technology",
      "understand",
      "that's",
      "challenges",
      "kids",
      "we're",
      "creating",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-ai-teach-teenagers-vibe-coding-kids-family-tech-2026-2",
    "thumbnail_url": "https://i.insider.com/6954c24104eda4732f2e4c0d?width=1200&format=jpeg",
    "created_at": "2026-02-09T06:54:22.473Z",
    "topic": "finance"
  },
  {
    "slug": "the-guy-who-coined-vibecoding-says-the-next-big-thing-is-agentic-engineering",
    "title": "The guy who coined 'vibe-coding' says the next big thing is 'agentic engineering'",
    "description": "OpenAI cofounder Andrej Karpathy says 'agentic engineering is the next evolution in AI coding as vibe-coding marks its first anniversary.",
    "fullText": "\"Vibe-coding\" just celebrated its first birthday. That's a lifetime in the AI boom.\n\nNow, the man who coined the term is celebrating the birth of a new one: \"agentic engineering.\"\n\nWhile vibe-coding is when humans prompt AI to write code, OpenAI cofounder Andrej Karpathy says agentic engineering is when AI agents write the code themselves.\n\n\"Many people have tried to come up with a better name for this to differentiate it from vibe coding, personally, my current favorite is 'agentic engineering,'\" he wrote in a recent X post.\n\nKarpathy said he calls it \"agentic engineering\" not just because agents are writing the code, but because \"there is an art & science and expertise to it.\"\n\nVibe-coding is one of the biggest innovations of the AI revolution. Prominent CEOs and startup founders alike are encouraging the use of vibe-coding across their teams. And billions are being poured into new vibe-coding companies.\n\nLovable, one of Europe's fastest-growing startups, announced that it had raised $330 million in Series B funding at a $6.6 billion valuation in December. Cursor, an AI-assisted code editor, announced a Series D funding round of $2.3 billion in November and said it had surpassed $1 billion in annualized revenue.\n\nThe approach is also threatening traditional engineering jobs. In a Business Insider survey of 167 software engineers, 75 engineers said that they were \"keeping up,\" while 30 said they felt ahead of the curve, and 27 felt behind.\n\n\"Vibe coding is now mentioned on my Wikipedia as a major memetic 'contribution,' and even its article is longer. lol,\" Karpathy wrote on X about its meteoric rise.\n\nKarpathy was a founding member of OpenAI in 2015, years before competitors like Anthropic and xAI emerged. He later moved into self-driving technology, leading Tesla's Autopilot program as head of AI. He's now building Eureka Labs, which describes itself on its website as building a \"new kind of school that is AI native.\"",
    "readingTime": 2,
    "keywords": [
      "series funding",
      "vibe coding",
      "agentic engineering",
      "vibe-coding",
      "code",
      "openai",
      "agents",
      "engineers",
      "karpathy"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/agentic-engineering-andrej-karpathy-vibe-coding-2026-2",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-02-09T01:12:58.161Z",
    "topic": "finance"
  },
  {
    "slug": "gitbutler",
    "title": "GitButler",
    "description": "GitButler software development platform",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://gitbutler.com/",
    "thumbnail_url": "https://gitbutler.com/og-image.png",
    "created_at": "2026-02-08T18:22:32.834Z",
    "topic": "tech"
  },
  {
    "slug": "prepare-your-oss-repo-for-ai-coding-assistants",
    "title": "Prepare your OSS repo for AI coding assistants",
    "description": "I’ve been seeing more and more open source maintainers throwing [...]",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://angiejones.tech/stop-closing-the-door-fix-the-house/",
    "thumbnail_url": "https://angiejones.tech/wp-content/uploads/2026/02/open-source-is-closed.png",
    "created_at": "2026-02-08T18:22:32.063Z",
    "topic": "tech"
  },
  {
    "slug": "an-identity-verification-ai-startup-is-having-all-employees-try-vibe-coding",
    "title": "An identity verification AI startup is having all employees try vibe coding",
    "description": "A San Francisco AI startup is giving out stipends and dedicating project days for non-tech staff to vibe code apps that help with their workflow.",
    "fullText": "If you work for a San Francisco startup and don't know how to code, you could soon be asked to get creative with vibe coding.\n\nCheckr, an AI-powered background check company, gave Business Insider a glimpse of how employees are actually using AI.\n\nCheckr CEO Daniel Yanisse said that the company is going \"all in\" and trying everything to encourage its employees to fully embrace AI — including staff that don't work in engineering roles.\n\n\"We really pride ourselves on using AI to the maximum possible amount,\" said Yanisse. \"We gave every employee a monthly stipend to try AI tools, and we did AI days and demos. After one year, 95% of the employees use prompting daily.\"\n\n\"This year, we're going to level up and move to building with AI, as in vibe coding,\" Yanisse added. \"I'm working with all of our teams now, and we're going to do our AI days soon in March, where we're going to make every non-technical person vibe code their own business apps.\"\n\nYanisse said that many employees who have no idea how to code, who work in finance, legal, and HR, are already vibecoding apps to automate their workflows and problem-solve, such as building tools that help clean up large spreadsheets.\n\nWhile Checkr is evaluating a variety of builder tools like Lovable, Replit, and Claude Code, Yanisse said Cursor is a clear standout and \"has amazing adoption\" among both engineers and non-technical staff, but Lovable is the best place to start for people with no coding experience.\n\n\"Probably, we're going to buy all of them and just use the right tool for the right person,\" Yanisse said of different AI coding tools.\n\n\"We have AI solution engineers who are available to actually partner and help, so they would come and help you and unstuck you if you have a problem, and take you all the way to success,\" Yanisse added. \"And then you're on your way because then we share success stories with everyone in the company.\"\n\nIn practice, data shows that AI adoption can be complicated in a large enterprise. Competence with AI tools can be very uneven across the board, and security risks can mount without clear guidelines on AI usage.\n\nAccording to a survey published in November by Moveworks, an AI-powered platform that automates IT and HR support, most executives said that non-technical employees are playing a bigger role in driving AI use, and that 78% have seen successful AI projects originate directly from support staff looking to solve daily challenges.\n\nThe National Cybersecurity Alliance also wrote in its Annual Cybersecurity Attitudes and Behaviors Report that AI adoption has surged to 65% globally as of the end of 2025, but more than half of these AI users never received any training in privacy and security risks. The report surveyed over 6,500 workers worldwide.\n\n\"A few years ago, most businesses were still debating whether AI was something they really needed,\" Louis Riat-Bonello of Optisearch, an AI-powered marketing platform that specializes in SEO, told Business Insider.\n\n\"The businesses getting the best results aren't blindly chasing automation. They're using AI to support smarter decisions, move faster, and free up teams to focus on strategy and creativity,\" Riat-Bonello added. \"That balance is what will matter long after the hype fades.\"\n\nYanisse said that in the age of AI, the company is looking for creative and adaptable people, because while AI will eliminate some roles, it will create others.\n\n\"We are constantly training and helping people update their skills and careers,\" said Yanisse. \"The job of the product designer and the job of the marketer are all completely shifting right now.\"\n\n\"We're over 900 people, so we're not a small startup, but I'm a startup guy, and I'm a builder,\" Yanisse added. \"The people who come here need to be OK with uncertainty, be self-driven, adaptable, flexible, willing to do new things, and solve new problems without too much guidance or structure.\"",
    "readingTime": 4,
    "keywords": [
      "security risks",
      "vibe coding",
      "we're",
      "employees",
      "tools",
      "code",
      "startup",
      "ai-powered",
      "yanisse",
      "staff"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/checkr-non-technical-employees-vibecode-with-stipends-and-ai-days-2026-2",
    "thumbnail_url": "https://i.insider.com/6983e6bda645d1188188b87e?width=1200&format=jpeg",
    "created_at": "2026-02-07T12:25:49.164Z",
    "topic": "finance"
  },
  {
    "slug": "what-i-wish-i-knew-before-building-a-vibe-coding-platform",
    "title": "What I wish I knew before building a vibe coding platform",
    "description": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.",
    "fullText": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.\n\nI'm Ariel, VP of AI at Appwrite. Over the past year, my team and I built Imagine — a vibe-coding platform that lets users build production-ready web apps using prompting. That means real backends, server-side rendering, server functions, sandboxes, previews, the whole thing.\n\nAppwrite is one of the most impactful open-source projects in the world, and in the spirit of open-source, we want to share our learnings with the community. This article isn't a how-to. It's a collection of hard-earned lessons — the kind you only learn after shipping, breaking things, and paying real infrastructure bills.\n\nIf you don't take prompt caching seriously from day one, you will burn money and time. This is especially true for vibe-coding platforms.\n\nVibe coding platforms naturally rely on long-running agentic loops. Agents talk to themselves, call tools, revise plans, generate files, run tests — sometimes over dozens of steps and millions of tokens. That's unavoidable if you want high-quality results.\n\nThe good news: with correct prompt caching, this doesn't have to be expensive or slow.\n\nHow you structure and order your prompts has a massive impact on cache hit rates. In practice, almost everything should be cacheable.\n\nWe're huge fans of Anthropic, and they allow you to define explicit cache breakpoints. For example:\n\nFirst breakpoint: after fully static system messages. Core instructions, rules, policies — things that never change.\n\nSecond breakpoint: after semi-dynamic system messages. Generation number, project metadata, decisions made by earlier agents.\n\nFinal breakpoint: right before the latest user or agent message.\n\nThis structure allows long agentic loops to reuse the majority of their context even when the workflow branches or retries. If you place breakpoints carelessly, a single conditional change can invalidate your entire cache.\n\nThis sounds unrealistic at first, but it's absolutely achievable.\n\nA good benchmark is 90–95% cache hit rate on input tokens. At that point, the economics completely change:\n\nThis is the difference between a platform that feels sluggish and expensive, and one that feels snappy and scalable.\n\nMake sure you know how to observe caching behavior. Anthropic does an excellent job here — their console shows per-request caching status and aggregated cache metrics over time. Anthropic's documentation for prompt caching is state-of-the-art and everyone should read it.\n\nPro tip: You might be tempted to use cheaper models for certain tasks in your workflow, thinking it would be faster or cheaper. With Anthropic for example, cache is per model, so plugging that Sonnet call at the end of your mostly-Opus workflow would result in a cache miss. Do caching right and you can leverage the best models for the job without breaking the bank.\n\nThe average request we make to Anthropic is 99% cached. Screenshot taken from Anthropic's console.\n\nMost frameworks and SDKs teach you the same thing:\n\nSend a request, get a response, stream tokens back to the client.\n\nThat's fine for demos. It completely breaks down for vibe coding.\n\nIn a vibe coding platform where state is fragile (sandbox, repo, uncommitted changes, ongoing generation, etc.), so much could go wrong.\n\nTo deal with these issues, we have implemented resumable streams and durable workflows.\n\nAssume you have a /api/chat endpoint where the client submits a prompt. Typically, tutorials would teach you to stream the chunks back to the client (for example, using Server-Sent events, or SSE in short).\n\nThat's simply putting too much hope in HTTP connections in a scenario where generations can take minutes to complete. Here is a good approach to dealing with this:\n\nIn our case, we serve the stream via a separate service altogether to reduce load on our core AI API, but that's not strictly necessary. Redis becomes the source of truth for in-progress generations.\n\nStreaming alone isn't enough. You also need durable execution for the workflow itself.\n\nInstead of one long-running function, break the process into orchestrated steps:\n\nSandbox provisioning is a good example. In our platform, there are multiple ways a sandbox might be started:\n\nWith durable workflows, all of these can safely funnel into the same sandbox orchestration logic without race conditions or duplication.\n\nWe chose to use Inngest which is open-source, but there are other good options like Temporal, AWS Lambda Durable Functions, Trigger.dev, etc.\n\nAs a bonus point, using Inngest gives us great observability, overview of execution times and alerts on anomalies. It forces us to adopt a more resilient and reliable approach to building our platform, and everybody gets to sleep better at night.\n\nInngest makes it easy to visualize the timeline of each generation and its steps.\n\nWhen building a vibe-coding platform, you have a fundamental choice:\n\nDo I let the system generate anything, or do I force generations to use a specific framework or stack?\n\nWe decided to go with the second approach.\n\nWhen evaluating frameworks, we asked ourselves these questions:\n\nAfter evaluating a few common options, we ultimately chose TanStack Start. It's built by the team behind some of the most popular tools and libraries powering the React ecosystem. TanStack Start uses Vite as its build system, which is highly customizable.\n\nFor our generated apps' runtime, we chose Bun. It's incredibly fast, supports running TypeScript directly (handy for migrations), and is pretty much a drop-in replacement for Node.js.\n\nConcretely, we are able to build our generated apps (server and client code) in ~1 second. For comparison, a fresh Next.js + Turbopack + Bun build takes a few seconds, and that's without any server code. The difference is huge.\n\nImagine's generated apps build in ~1 second, including both client and server code\n\nGenerative AI is inherently non-deterministic. You can prompt all you want, provide examples, two-shot it, five-shot it. But as your message history grows, your context grows as well, and so does the likelihood of facing hallucinations or simply failing to get the model to adhere to your instructions.\n\nWhen users report issues, 90% of those have to do with unexpected AI behavior or issues in the generated code. It's very tempting to take every such issue in isolation and try to fix it by adding an additional bullet point to the system prompt, or providing more examples.\n\nBut wait. There might be a better way. The deterministic way. Here are some examples from Imagine:\n\nThese are just a few examples, but the key takeaway is that you can't rely on the LLM to adhere to your instructions. Whenever you can, embrace determinism.\n\nLet your AI see the same red squiggly lines you appreciate so much.\n\nThe real challenges only show up once you're dealing with real workloads, real users, and real infrastructure bills. By then, it's often too late to \"just refactor\" your way out of architectural decisions made early on.\n\nPrompt caching, durable workflows, deterministic guardrails, and a well-chosen framework should not be an afterthought, or by-the-way optimizations. They are foundational.\n\nAt Imagine, our mission is to tame AI, make the best of it and mitigate its weaknesses. We build Imagine assuming things will fail, disconnect, retry, and resume — unexpectedly.\n\nAs we build Imagine, we are constantly learning, and we are excited to share our learnings with the community. If you're building something similar, we hope these hard-earned lessons help you move quickly and avoid the pitfalls.",
    "readingTime": 7,
    "keywords": [
      "infrastructure bills",
      "agentic loops",
      "hard-earned lessons",
      "vibe coding",
      "system messages",
      "durable workflows",
      "generated apps",
      "cache hit",
      "server code",
      "vibe-coding platform"
    ],
    "qualityScore": 1,
    "link": "https://imagine.dev/blog/post/what-i-wish-i-knew",
    "thumbnail_url": "/images/blog/what-i-wish-i-knew/cover.png",
    "created_at": "2026-02-06T18:34:47.896Z",
    "topic": "tech"
  },
  {
    "slug": "tech-stack-is-a-business-decision",
    "title": "Tech stack is a business decision",
    "description": "Why tech stack choices should be driven by business context and constraints—not framework preferences—and why this matters even more with agentic coding tools.",
    "fullText": "Developers love to argue about tech stacks.\n\nFlutter vs React Native.\n\nNative vs cross-platform.\n\nJava vs Ruby vs C#.\n\nReact vs Angular.\n\nRiverpod vs Bloc.\n\nDrift vs Hive.\n\nThese debates can go arbitrarily deep, down to library-level choices that have little impact on whether a product succeeds or fails.\n\nEveryone has preferences, and that’s fine. Preferences come from experience. Someone who started with strongly typed languages will value compiler guarantees and explicitness. Someone who started with dynamic languages will value flexibility and speed. Both viewpoints are internally consistent.\n\nWhat they are not is universally correct.\n\nThere is no hard data that can rank one stack as objectively “better” across all contexts. Teams, constraints, markets, and goals differ too much. Most arguments rely on anecdotes, and anecdotes do not generalize.\n\nSo if preference is not a reliable guide, how should a tech stack be chosen?\n\nThis question is often interpreted at the feature or use-case level. That misses the point.\n\nIn almost all non-hobby software, the problem being solved is a business problem.\n\nIf there is no business, no users, no revenue, then the software is irrelevant. It may be elegant or technically impressive, but it has no economic value. Software only matters insofar as it supports a business outcome.\n\nThat framing matters because it establishes the correct hierarchy.\n\nTechnology is not the goal.\n\nTechnology is a tool.\n\nAnd tools are chosen based on the job they need to do.\n\nEarly on, nobody knows whether the business will work.\n\nYou don’t know if users care. You don’t know if they will pay. You don’t know if the problem is real or imagined.\n\nAt this stage, obsessing over the “right” stack is misplaced effort. The dominant constraint is uncertainty, not scalability or architectural purity.\n\n“The FIRST rule of enterprenuership is you use what you have” - Alex Hormozi\n\nThis is simple rule of entrepreneurship applies surprisingly well to engineering.\n\nFounders and early teams start with what they already know. Not because it is theoretically optimal, but because it minimizes friction. Familiar tools reduce cognitive overhead, increase speed, and allow faster learning.\n\nEarly success is rarely determined by technical elegance. It is determined by whether you can ship something people want before you run out of time or money.\n\nAs the business grows, the constraints change.\n\nRevenue appears.\n\nUsers depend on the system.\n\nThe team grows.\n\nThe cost of failure increases.\n\nNow the stack begins to matter more. Not because some technologies are inherently superior, but because the software must support new business realities. Hiring, onboarding, operational cost, maintainability, performance characteristics, and risk management all become relevant.\n\nThis is where teams often conclude that an early stack choice was “wrong.” In most cases, it wasn’t wrong. It was right for the phase the business was in. The mistake was assuming it would remain right indefinitely.\n\nTwitch is often mentioned as a scaling story, but the more interesting part is how long the architecture stayed simple.\n\nAt one point, Twitch was serving millions of users with a monolithic Ruby on Rails application and a single PostgreSQL database. That setup was not elegant or future proof, but it worked. It allowed the team to move quickly, ship features, and grow the business without adding unnecessary complexity.\n\nOver time, the constraints changed. Growth amplified failures. Database contention increased. Reliability, support load, and churn started to matter more than raw development speed. The cost of incidents became visible in a way it had not been before.\n\nThat was when the architecture began to evolve. The database was scaled and restructured, services were split out, and new infrastructure and languages were introduced. None of this happened because the original stack was flawed. It happened because the business had outgrown it.\n\nThe technology changed in response to the business, not the other way around.\n\nIf you want to read more about how Twitch migrated from a monolith to microservices, see Breaking the Monolith at Twitch and Breaking the Monolith at Twitch: Part Two.\n\nIf you want to know more about how Twitch scaled its database, see How Twitch Uses PostgreSQL.\n\nAgentic coding tools make this even more important.\n\nTools like Claude Code and Cursor reduce the advantage of deep, framework-specific familiarity. Generating boilerplate, navigating unfamiliar code, applying patterns across modules, and keeping implementation consistent are no longer manual, error-prone tasks.\n\nWhen an agent can help you stay productive across technologies, the cost of switching or extending a stack drops. What remains expensive is not syntax or APIs, but domain knowledge, product decisions, and the long-term behavior of the system.\n\nIn other words, the more the tooling levels the technical playing field, the more tech stack decisions become about business fit.\n\nInstead of asking whether one technology is better than another in the abstract, ask questions that reflect the reality of the business:\n\nWhat are we optimizing for right now? Speed, cost, reliability, hiring, or time-to-market?\n\nHow quickly do we need to validate this idea?\n\nHow expensive will change be later, and what changes are most likely?\n\nWhat kind of team will maintain this system?\n\nWhat risks could realistically kill the business, and does the stack reduce or increase them?\n\nOnce framed this way, stack decisions become clearer and less emotional. Sometimes multiple choices are equally valid. Sometimes the boring option is correct. Sometimes speed matters more than correctness. Sometimes correctness matters more than speed.\n\nThere is no universal answer. The answer depends on the context.\n\nMost tech stack arguments are debates about taste, not outcomes.\n\nOnce technology is treated as a business decision rather than a personal identity, those arguments lose their intensity. The question stops being “what do I prefer?” and becomes “what does the business need right now?”\n\nThe right stack is not the one that wins debates.\n\nIt is the one that helps the business survive and grow at its current stage.\n\nIf are starting a project and need help with tech stack decision, connect with me on LinkedIn and I’ll take it from there.\n\nSoftware Developer specializing in Flutter, Android, and mobile development.\n Writing about code, architecture, and developer productivity.",
    "readingTime": 6,
    "keywords": [
      "tech stack",
      "stack decisions",
      "business",
      "speed",
      "technology",
      "users",
      "tools",
      "database",
      "debates",
      "languages"
    ],
    "qualityScore": 1,
    "link": "https://dinkomarinac.dev/blog/tech-stack-is-a-business-decision/",
    "thumbnail_url": "https://dinkomarinac.dev/og/tech-stack-is-a-business-decision.png",
    "created_at": "2026-02-06T12:35:39.062Z",
    "topic": "tech"
  },
  {
    "slug": "how-exposed-are-software-stocks-to-ai-tools-we-put-vibecoding-to-the-test",
    "title": "How exposed are software stocks to AI tools? We put vibe-coding to the test",
    "description": "How real is the AI threat to software companies? CNBC put it to the test by vibe-coding a Monday.com replacement.",
    "fullText": "Software, legal services and video games stocks have been selling off in recent weeks on fears that new AI features and tools could wipe them out. But how real is that threat? We decided to find out.\n\nCNBC's Deidre Bosa and I used Anthropic's AI coding tool \"Claude Code\" with the goal of creating a replacement for Monday.com, a project management platform with a $5 billion market cap. We didn't expect to get anywhere — we're not developers and we don't have any coding experience. But we have become adept at vibe-coding tools, which are AI tools that can build functioning apps based on commands in plain English from users, including those with limited technical chops.\n\nWe started out simple, telling Claude to build a project management dashboard similar to Monday, with features like multiple project boards, assigned team members and a status dropdown. It spit out a working prototype in minutes.\n\nWe then asked Claude to research Monday on its own, identify main features and recreate them. It added a number of other features, including a calendar.\n\nThe real magic happened when we connected the clone to an email account, essentially spinning up a customized project manager for our personal lives. The AI found Dee's forgotten calendar invite for a kid's birthday party (which she definitely didn't have a gift for yet) and it added reminders to book tickets for an upcoming trip and sign a waiver for another kid's birthday party.\n\nIt took us under an hour, and had we been paying users, it would have cost something like $5 to $15 in compute credits, depending on how much we went back and forth with the AI agent. As more data centers get built out, that cost could start to come down.\n\nSo which companies should investors worry about? Silicon Valley insiders we talk to say the most exposed names are the ones that \"sit on top of the work\" — tools like Atlassian, Adobe, HubSpot, Zendesk, and Smartsheet, that aren't core to businesses.\n\nThey say cybersecurity stocks like CrowdStrike and Palo Alto are harder to disrupt since they have network effects that no one would want to try to replicate and maintain.\n\nSystems of record may be safer, but not immune. Salesforce, for instance, anchors a business with enterprise data, making it harder to clone with a weekend coding project.\n\nWith the wholesale sell-off in software this year, that may be a chance for investors to separate between the need-to-haves and nice-to-haves.",
    "readingTime": 3,
    "keywords": [
      "kid's birthday",
      "birthday party",
      "project management",
      "features",
      "tools",
      "coding",
      "software",
      "stocks",
      "didn't",
      "users"
    ],
    "qualityScore": 1,
    "link": "https://www.cnbc.com/2026/02/05/how-exposed-are-software-stocks-to-ai-tools-we-tested-vibe-coding.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108184664-1754999709333-gettyimages-2214520187-img_1248.jpeg?v=1754999753&w=1920&h=1080",
    "created_at": "2026-02-06T06:40:18.191Z",
    "topic": "finance"
  },
  {
    "slug": "termoil-terminal-dashboard-for-managing-parallel-ai-coding-agents",
    "title": "Termoil – Terminal dashboard for managing parallel AI coding agents",
    "description": "Less friction for your multi-agent workflow. Contribute to fantom845/termoil development by creating an account on GitHub.",
    "fullText": "fantom845\n\n /\n\n termoil\n\n Public\n\n Less friction for your multi-agent workflow\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n fantom845/termoil",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/fantom845/termoil",
    "thumbnail_url": "https://opengraph.githubassets.com/3f4c60f9807e7fd26c4cd7d8070eb9f12a1c2217f6b59354a3d271fb86812016/fantom845/termoil",
    "created_at": "2026-02-06T06:40:14.221Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-ai-ceo-says-vibe-coding-is-making-software-easier-to-replace",
    "title": "Microsoft AI CEO says vibe coding is making software easier to replace",
    "description": "Microsoft AI CEO says vibe coding is lowering the barrier to building apps, raising questions about how defensible software will be.",
    "fullText": "Mustafa Suleyman says vibe coding is rapidly lowering the barrier to building apps, a shift that could put traditional software at risk.\n\nThe Microsoft AI CEO said in an episode of the \"Exponential View\" podcast published Thursday that AI tools now make it possible for anyone to quickly start launching code and apps.\n\n\"It is so accessible now,\" said Suleyman. \"You can watch a three-minute video, get spun up, launch one of these things.\"\n\n\"You can create an app, a web app in seconds,\" he added.\n\nSuleyman said people don't need deep technical skills to get started. Instead, they can learn by experimenting, watching, and doing.\n\nThe AI can \"build something that you may have thought was never possible,\" he said.\n\n\"Unless you push these things to their edges and explore the boundaries, you won't really understand the magic, or what they're kind of bad at.\"\n\n\"Everyone's got to get stuck into that motion,\" he added.\n\nSuleyman also said he has vibe-coded a system that tracks the DJs he wants to see, coming concerts and festivals, and then matches those events with his travel schedule. What used to be manual work now runs automatically in a spreadsheet that updates throughout the year.\n\nSuleyman's comments come as investors grow increasingly anxious that AI tools and agents could wipe out entire categories of software.\n\nThat fear flared this week after Anthropic said it was adding legal-focused capabilities to its Cowork assistant. The tools would allow AI to review legal documents and track compliance — work typically done by legal software.\n\nMarkets didn't take it lightly. Shares of legal-software companies in Europe and the US fell sharply on Tuesday, before the selling spread across the wider software sector and into tech.\n\nOpenAI triggered a similar sell-off months earlier after rolling out internal AI-powered software-as-a-service tools.\n\nMany of the tools now unsettling the tech sector were built using AI coding tools.\n\nAI personal assistant OpenClaw was created with the help of AI, while Moltbook — a viral, Reddit-style forum for AI agents — was entirely vibe-coded.\n\nAnthropic also said last month that it built its Cowork assistant using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. During a livestream, Rieseberg said his team put Cowork together in just over a week, thanks to Claude.\n\nTech leaders and developers have also said that such a turnaround is becoming the norm.\n\nPeter Steinberger, the developer behind OpenClaw, said in an episode of \"Behind the Craft\" podcast published last week that AI now lets developers \"build everything.\"\n\nOpenAI's chair, Bret Taylor, said in an episode of the \"Big Technology Podcast\" published last week that building software quickly through vibe coding will soon feel routine rather than novel.\n\nBut the real question, Taylor said, is what software still matters.\n\nInstead of dashboards, browser forms, and traditional apps, he expects AI agents to become the dominant software interface.\n\n\"Who's making those agents is the question,\" he said. \"Will you buy those agents off the shelf or build them yourself?\"",
    "readingTime": 3,
    "keywords": [
      "cowork assistant",
      "vibe coding",
      "podcast published",
      "software",
      "tools",
      "agents",
      "apps",
      "episode",
      "traditional",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/microsoft-ai-ceo-vibe-coding-software-replace-apps-mustafa-suleyman-2026-2",
    "thumbnail_url": "https://i.insider.com/6985593bd3c7faef0ecdbefa?width=1200&format=jpeg",
    "created_at": "2026-02-06T06:40:12.752Z",
    "topic": "tech"
  },
  {
    "slug": "a-very-small-sat-solver-from-haskell-now-in-dafny-proved-correct-with-llms",
    "title": "A Very Small SAT Solver (From Haskell) Now in Dafny, Proved Correct with LLMs",
    "description": "Dafny for Metatheory of Programming Languages. Contribute to namin/dafny-sandbox development by creating an account on GitHub.",
    "fullText": "Skip to content\n\n You signed in with another tab or window. Reload to refresh your session.\n You signed out in another tab or window. Reload to refresh your session.\n You switched accounts on another tab or window. Reload to refresh your session.\n\nDismiss alert\n\n namin\n\n /\n\n dafny-sandbox\n\n Public\n\n You can’t perform that action at this time.",
    "readingTime": 1,
    "keywords": [
      "window reload",
      "another tab",
      "refresh",
      "session",
      "signed"
    ],
    "qualityScore": 0.3,
    "link": "https://github.com/namin/dafny-sandbox/blob/master/Sat.dfy",
    "thumbnail_url": "https://opengraph.githubassets.com/37f757a4876729fbb57a662e8bc8730df5cfc0bc7de82e6b36f6b3d99b39b44a/namin/dafny-sandbox",
    "created_at": "2026-02-06T01:06:52.715Z",
    "topic": "tech"
  },
  {
    "slug": "openais-new-model-leaps-ahead-in-coding-capabilitiesbut-raises-unprecedented-cybersecurity-risks",
    "title": "OpenAI’s new model leaps ahead in coding capabilities—but raises unprecedented cybersecurity risks",
    "description": "Why OpenAI’s latest coding breakthrough is forcing the company to rethink how—and how fast—it can deploy its most powerful models.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/05/openai-gpt-5-3-codex-warns-unprecedented-cybersecurity-risks/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/08/GettyImages-2214110034.jpg?resize=1200,600",
    "created_at": "2026-02-06T01:06:50.989Z",
    "topic": "business"
  },
  {
    "slug": "programming-your-own-modern-bbs-with-python",
    "title": "Programming Your Own Modern BBS with Python",
    "description": "Previously, we looked at dialling BBS with Vice C64 and the C64U. Now, let's code a modern BBS from scratch using Python!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://retrogamecoders.com/programming-bbs-with-python/",
    "thumbnail_url": "https://retrogamecoders.com/wp-content/uploads/2026/02/custom-bbs-server-in-python.jpg",
    "created_at": "2026-02-05T18:36:01.035Z",
    "topic": "tech"
  },
  {
    "slug": "the-coding-agent-os",
    "title": "The Coding Agent OS",
    "description": "We believe that a great coding agent isn't a simple chatbot—it's an operating system for the model.",
    "fullText": "Most coding agents are presented as a chat UI glued to an LLM and integrated into an IDE or\n CLI. This can work really well for developers who provide the development environment along\n with constant babysitting to build functional software.\n\nCharlie, however, is built around a different premise: We believe that a great coding agent\n isn't a simple chatbot—it's an operating system for the model.\n\nCharlie's \"OS layer\" gives a model the primitives required to complete end-to-end\n engineering work:\n\nWhen agents run in an operating system, they can diagnose → plan → implement → verify →\n publish reviewable artifacts all within the systems where your team already works.\n\nWe've got two hot takes on why the OS is essential for great agents:\n\nCLI expressiveness beats bespoke \"LLM tools.\" CLI tools are composable (|, &&, xargs), self-documenting (--help), and match\n how engineers actually work. One-off JSON tools tend to be isolated and require tool\n designers to predict every use case up front.\n\nOpen-loop orchestration beats rigid workflows. Real engineering has surprises:\n flaky tests, missing context, failing builds, CI-only failures, new constraints. The \"OS loop\"\n lets the agent decide the next step based on what it observes, instead of forcing every task into\n a fixed graph.\n\nIDE-native assistants like Claude Code are great for interactive, in-editor work: drafting,\n refactors, quick iteration. But they're not optimized to serve as a team-facing execution\n environment.\n\nOS-style agents are built for that missing layer: cross-tool orchestration + durable,\n reviewable artifacts.\n\nA mention, review request, regression, or scheduled trigger.\n\nCharlie normalizes + enriches the signal\n\nInto \"this is the work request, with relevant context.\"\n\nDecide whether to act, what thread it belongs to, and which capabilities are allowed.\n\nA durable Task is created (or appended-to)\n\nSingle-winner claiming, enters a durable, turn-based loop.\n\nAssemble context, decide + plan, call the LLM, dispatch tool calls, observe results +\n check mailbox/cancel + continue.\n\nBack to the source system (PRs, reviews, issue updates, summaries).\n\nRecord terminal status + telemetry\n\nDurable transcript/state remains; ephemeral compute is cleaned up.\n\nThe agentic operating system delivers better outcomes:",
    "readingTime": 2,
    "keywords": [
      "reviewable artifacts",
      "operating system",
      "agents",
      "durable",
      "tools",
      "context",
      "coding",
      "environment",
      "agent",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://charlielabs.ai/coding-agents-as-operating-systems/",
    "thumbnail_url": "https://charlielabs.ai/_astro/caos-architecture.JbhqvI6s.png",
    "created_at": "2026-02-05T12:36:57.286Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-engineering",
    "title": "Agentic Engineering",
    "description": "Agentic Engineering is a disciplined approach to AI-assisted software development that emphasizes human oversight and engineering rigor, distinguishing it fr...",
    "fullText": "A year ago, Andrej Karpathy coined “vibe coding” to describe a gleefully reckless way of programming: you prompt, hand the keyboard to an AI, accept everything it spits out, don’t read the diffs, iterate by pasting error messages back in. It was a great label for a real thing - building quick prototypes or MVPs on pure AI autopilot.\n\nThe problem is that “vibe coding” has become a suitcase term. People now use it to describe everything from a weekend hack to a disciplined engineering workflow where AI agents handle implementation under human oversight. These are fundamentally different activities, and conflating them is causing real confusion - and real damage.\n\nVibe coding means going with the vibes and not reviewing the code. That’s the defining characteristic. You prompt, you accept, you run it, you see if it works. If it doesn’t, you paste the error back and try again. You keep prompting. The human is a prompt DJ, not an engineer.\n\nIf vibe coding gives millions of people the ability to create custom software who otherwise couldn’t, that’s a genuine win. The technique has a legitimate place in the toolbox.\n\nBut the failure modes are well-documented at this point. The pattern is always the same: it demos great, then reality arrives. You try to modify it, scale it, or secure it, and you discover nobody understands what the code is actually doing. As one engineer put it, “This isn’t engineering, it’s hoping.”\n\nHere’s the thing: a lot of experienced engineers are now getting massive productivity gains from AI - 2x, 5x, sometimes more - while maintaining code quality. But the way they work looks nothing like vibe coding. They’re writing specs before prompting. They’re reviewing every diff. They’re running test suites. They’re treating the AI like a fast but unreliable junior developer who needs constant oversight. I’ve personally liked “AI-assisted engineering” and have talked about how this describes that end of the spectrum where the human remains in the loop.\n\nSimon Willison (whose work I adore) proposed “vibe engineering” for this - it reclaims “vibe” while adding “engineering” to signal discipline. But after watching the community debate this for months, I think the the word “vibe” carries too much baggage. It signals casualness. When you tell a CTO you’re “vibe engineering” their payment system, you can see the concern on their face.\n\nAndrej Karpathy suggested “agentic engineering” this week and I think I like it.\n\nIt describes what’s actually happening. You’re orchestrating AI agents - coding assistants that can execute, test, and refine code - while you act as architect, reviewer, and decision-maker. You might write only a % of the code by hand. The rest comes from agents working under your direction. That’s agentic. And you’re applying engineering discipline throughout. That’s engineering.\n\nIt’s professionally legible. “Agentic engineering” sounds like what it is: a serious engineering discipline involving autonomous agents. You can say it to your VP of Engineering without embarrassment. You can put it in a job description. You can build a team practice around it.\n\nIt draws a clean line. Vibe coding = YOLO. Agentic engineering = AI does the implementation, human owns the architecture, quality, and correctness. The terminology itself enforces the distinction.\n\nThe workflow isn’t complicated, but it requires discipline that vibe coding explicitly abandons:\n\nYou start with a plan. Before prompting anything, you write a design doc or spec - sometimes with AI assistance. You break the work into well-defined tasks. You decide on the architecture. This is the part vibe coders skip, and it’s exactly where projects go off the rails.\n\nYou direct, then review. You give the AI agent a well-scoped task from your plan. It generates code. You review that code with the same rigor you’d apply to a human teammate’s PR. If you can’t explain what a module does, it doesn’t go in.\n\nYou test relentlessly. The single biggest differentiator between agentic engineering and vibe coding is testing. With a solid test suite, an AI agent can iterate in a loop until tests pass, giving you high confidence in the result. Without tests, it’ll cheerfully declare “done” on broken code. Tests are how you turn an unreliable agent into a reliable system.\n\nYou own the codebase. You maintain documentation. You use version control and CI. You monitor production. The AI accelerates the work, but you’re responsible for the system.\n\nTeams doing this well often report faster development - and those gains come from augmenting a solid process, not abandoning one. The AI handles boilerplate and grunt work. The human focuses on architecture, correctness, edge cases, and long-term maintainability.\n\nThe irony is that AI-assisted development actually rewards good engineering practices more than traditional coding does. The better your specs, the better the AI’s output. The more comprehensive your tests, the more confidently you can delegate. The cleaner your architecture, the less the AI hallucinates weird abstractions. As one analysis noted, “AI didn’t cause the problem; skipping the design thinking did.”\n\nHere’s an uncomfortable truth from the trenches: agentic engineering disproportionately benefits senior engineers. If you have deep fundamentals - you understand system design, security patterns, performance tradeoffs - you can leverage AI as a massive force multiplier. You know what good code looks like, so you can efficiently review and correct AI output.\n\nBut if you’re junior and you lean on AI before building those fundamentals, you risk a dangerous skill atrophy. You can produce code without understanding it. You can ship features without learning why certain patterns exist. Several engineering leaders have flagged this as an emerging crisis: a generation of developers who can prompt but can’t debug, who can generate but can’t reason about what they’ve generated.\n\nThis isn’t an argument against AI-assisted development. It’s an argument for being honest about what it demands. Agentic engineering isn’t easier than traditional engineering - it’s a different kind of hard. You’re trading typing time for review time, implementation effort for orchestration skill, writing code for reading and evaluating code. The fundamentals matter more, not less.\n\nThe trajectory is clear: AI agents are getting more capable, and the agentic engineering workflow is becoming default for a growing number of professional developers. This is going to accelerate.\n\nThe rise of AI coding doesn’t replace the craft of software engineering - it raises the bar for it. The developers who’ll thrive aren’t the ones who prompt the fastest. They’re the ones who think the clearest about what they’re building and why, then use every tool available - including AI agents - to build it well.\n\nVibe coding showed us what’s possible when you drop all conventions.\n\nNow it’s time to bring the engineering back. Let’s call that what it is.\n\nI’ve written a new book with O’Reilly, Beyond Vibe Coding, that goes deeper into practical frameworks for AI-Assisted (and agentic) engineering. If you’ve been figuring this out in your own workflow, I’d love to hear what’s working.",
    "readingTime": 6,
    "keywords": [
      "ai-assisted development",
      "vibe coding",
      "agentic engineering",
      "engineering workflow",
      "engineering it’s",
      "engineering discipline",
      "andrej karpathy",
      "the ai",
      "code",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/agentic-engineering/",
    "thumbnail_url": "https://addyosmani.com/assets/images/agentic-engineering.jpg",
    "created_at": "2026-02-05T12:36:55.943Z",
    "topic": "tech"
  },
  {
    "slug": "mark-zuckerbergs-2004-coding-jams-were-loud-emo-and-very-on-brand",
    "title": "Mark Zuckerberg's 2004 coding jams were loud, emo, and very on brand",
    "description": "Mark Zuckerberg published a playlist of five songs he was pumping while building Facebook in 2004. It includes hard-rocking hits and some synth funk.",
    "fullText": "Mark Zuckerberg is feeling nostalgic.\n\nOn the 22nd anniversary of Facebook's founding, Zuckerberg reposted a meme about the fresh-faced Harvard student \"getting ready to make history.\" Then he linked a playlist titled \"2004 Facebook coding jams.\"\n\nZuckerberg said he \"had these bangers on repeat\" while building Facebook. The playlist's cover features the young founder — long before his bulked-up rebrand — listening to music on headphones.\n\nThe playlist's five songs feature a fair share of emo wails, electric guitars, and even some funky synthesizers. Some of them have receded into the tides of music history; others remain classics for weddings and bar mitzvahs.\n\nThey're also evocative of a quieter, easier time in tech. There were no culture wars, cage matches, or capex worries. What would a 2004 Zuckerberg think of $10 billion data centers?\n\nMusic speaks to how we feel. Here's what these five songs might say about a 19-year-old Zuckerberg — and just how far he's come.\n\nZuckerberg liked hard rock, it seems.\n\n\"Headstrong\" is a loud, thrashing song that feels built to be played on \"Guitar Hero.\" There's even a good guttural scream in there before one chorus. The lyrics are equally attacking. \"Back off, I'll take you on,\" Chris Taylor Brown sings.\n\nThe opening of \"Headstrong\" sounds like UFC walkout music, hyping up a big fight. That's now a favorite sport of Zuckerberg's. Some things never change.\n\n\"Like a Stone\" is about death and religion — two subjects that Zuckerberg has long been interested in.\n\nFrontman Chris Cornell prays to God and angels on their deathbed that they will get into heaven. \"In dreams until my death / I will wander on,\" he sings.\n\nZuckerberg seems more interested in extending human life than forecasting its end. His philanthropic initiative invests heavily in drug and disease research. He also has an intense fitness routine, including jiu-jitsu and CrossFit routines.\n\nAs for faith, Zuckerberg said in 2020 that he had \"become more religious.\" He cited two sources: The issues his company has faced in the prior years and the birth of his daughters.\n\nMostly, though, \"Like A Stone\" is another huge rock hit. It seems like Zuckerberg enjoyed the sound of banging drums and wailing guitar.\n\nNow this one is a nostalgia trip.\n\nYou can't help but feel something when Douglas Robb croons out, \"I'm not a perfect person.\" Was Zuckerberg a perfect person? Most people who have watched \"The Social Network\" would say no.\n\nIt's a romantic song. After considering their flaws, Robb sings out that they've found a reason to change. \"The reason is you,\" he repeats, over and over.\n\nZuckerberg met his wife, Priscilla Chan, in 2003, one year before this playlist's date. Maybe he was thinking of her.\n\nHe'd be less happy to hear that the song reemerged on TikTok — a Meta competitor — in a 2021 trend.\n\nWe all knew that Zuckerberg liked rap. He recorded his own version of T-Pain's \"Get Low\" in tribute to his wife. He also started dressing like Eminem.\n\n\"In the End\" is peak 2010s emo rap-rock. One of its refrains is \"I tried so hard.\" Zuckerberg did try so hard — he often worked till late into the night.\n\nThe song is more pessimistic in its outlook: \"In the end, it doesn't even matter.\" Zuckerberg would likely disagree here. He tried so hard — and in the end, it did matter. Meta is now a trillion-dollar company.\n\nHard pivot! After four hard rock tunes, Zuckerberg's last song leans into synth-funk.\n\nThe lyrics here feel self-explanatory for Zuckerberg's workplace ethos: \"Work it harder, make it better / Do it faster, makes us stronger / More than ever, hour after hour / Work is never over.\"\n\nAfter Meta's year of intensity, I'm sure that some of Zuckerberg's employees can relate.\n\nAlso, the Daft Punk helmet doesn't look that different from a Meta Quest.",
    "readingTime": 4,
    "keywords": [
      "zuckerberg liked",
      "like stone",
      "song",
      "music",
      "playlist's",
      "rock",
      "sings",
      "history",
      "songs",
      "headstrong"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mark-zuckerberg-2004-coding-jams-playlist-loud-emo-on-brand-2026-2",
    "thumbnail_url": "https://i.insider.com/69838cc9d3c7faef0ecd9c7b?width=1200&format=jpeg",
    "created_at": "2026-02-05T12:36:52.821Z",
    "topic": "finance"
  },
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What’s Behind the ‘SaaSpocalypse’ Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "webhook-skills-agent-skills-for-webhook-providers-and-best-practices",
    "title": "Webhook Skills – Agent skills for webhook providers and best practices",
    "description": "Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopif...",
    "fullText": "hookdeck\n\n /\n\n webhook-skills\n\n Public\n\n Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopify, GitHub, and more. Built on the Agent Skills specification.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n hookdeck/webhook-skills",
    "readingTime": 1,
    "keywords": [
      "webhook",
      "skills",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/hookdeck/webhook-skills",
    "thumbnail_url": "https://opengraph.githubassets.com/ed9e4e830971c76b02864c100a37accbc7ef947718023c6abb2ba2536dad1e19/hookdeck/webhook-skills",
    "created_at": "2026-02-04T12:35:28.405Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-the-new-rad",
    "title": "Vibe Coding is the new RAD",
    "description": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.",
    "fullText": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.\n\nFile details: 6.9 MB MP3, 5 mins 12 secs duration.\n\nTitle music is \"Apparent Solution\" by Brendon Moeller, licensed via www.epidemicsound.com\n\nFive.Today is a highly-secure personal productivity application designed to help you to manage your priorities more effectively, by focusing on your five most important tasks you need to achieve each day.\n\n Our goal is to help you to keep track of all your tasks, notes and journals in one beautifully simple place, which is highly secure via end-to-end encryption. Visit the URL Five.Today to",
    "readingTime": 1,
    "keywords": [
      "tasks",
      "five.today"
    ],
    "qualityScore": 0.65,
    "link": "https://techleader.pro/a/723-Vibe-Coding-is-the-new-RAD-(TLP-2026w3)",
    "thumbnail_url": "https://techleader.pro/img/icons/noun_programmer_2644331.png",
    "created_at": "2026-02-04T01:06:56.822Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-good-yet",
    "title": "Is AI \"Good\" Yet?",
    "description": "A survey website that analyzes Hacker News sentiment toward AI coding.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.is-ai-good-yet.com",
    "thumbnail_url": "https://www.is-ai-good-yet.com/og-image.png",
    "created_at": "2026-02-03T06:37:48.803Z",
    "topic": "tech"
  },
  {
    "slug": "contextgeneric-programming-v061-release-improving-ergonomics-and-debugging",
    "title": "Context-Generic Programming v0.6.1 Release: Improving Ergonomics and Debugging",
    "description": null,
    "fullText": "This blog post is co-authored by Claude Haiku based on a human draft. All content has been reviewed by the human author.\n\nWe're excited to announce the release of cgp v0.6.1, which brings several quality-of-life improvements to Context-Generic Programming in Rust. This release focuses on making CGP more accessible to developers new to the paradigm while also providing better debugging and verification tools for complex provider setups.\n\nThe changes in this release reflect our commitment to lowering the barrier to entry for CGP, while maintaining the power and flexibility that make it such a compelling approach to modular Rust code. Whether you're an experienced CGP user or just getting started, we believe you'll find these improvements meaningful.\n\nOne of the biggest hurdles when learning CGP is understanding how generic Context parameters work in provider implementations. While generics are fundamental to Rust, they can be intimidating to developers coming from object-oriented backgrounds, or even to experienced Rust developers who haven't deeply explored the generic type system.\n\nWhen writing a #[cgp_impl] provider, you previously had to explicitly declare the Context as a generic parameter and ensure it appeared correctly throughout your implementation. This added cognitive overhead that wasn't directly related to the business logic you were implementing. For beginners, this meant wrestling with generic syntax before they could even focus on understanding the core CGP concepts like delegation and trait constraints.\n\nCGP v0.6.1 introduces support for implicit Context types in the #[cgp_impl] macro. You can now omit the explicit generic parameter and use familiar Rust patterns like Self to refer to the context type.\n\nLet's look at a concrete example. Suppose we're building a greeting system where different types can greet people:\n\nBefore v0.6.1, the provider implementation would look like:\n\nWith v0.6.1, you can simplify this to:\n\nThe difference might seem small at first glance, but it has a profound impact on readability and accessibility. The new syntax eliminates the generic parameter syntax entirely, making the provider implementation look much more like a standard Rust trait implementation. For someone new to CGP coming from an OOP background, this is far less intimidating—it reads like implementing a method on a class, which is a familiar pattern.\n\nThis improvement particularly shines when you have multiple providers in a codebase. Each provider definition becomes clearer and more focused on the actual logic, rather than the generic plumbing.\n\nAs your CGP applications grow in complexity, so does the challenge of verifying that all your provider wiring is correct. This is particularly true when using higher-order providers—providers that accept other providers as generic parameters.\n\nWhen you compose multiple providers together and something goes wrong, the compiler error messages can be cryptic. The error might point to a deep dependency in the chain, but it won't clearly tell you which individual provider in the composition is actually failing. This forces developers to compile and test repeatedly, changing providers and wiring patterns to narrow down the issue.\n\nCGP v0.6.1 introduces the #[check_providers] attribute for the check_components! macro, which lets you verify that specific providers work correctly with a given context. This is a powerful debugging tool that allows you to isolate and test individual providers before wiring them into your context.\n\nHere's a simple example. Suppose we have a shape area calculation system:\n\nYou can verify that RectangleArea works with Rectangle using:\n\nWith the new #[check_providers] attribute, you can also verify the provider directly:\n\nWhat's the benefit? The #[check_providers] version checks whether RectangleArea itself can be used as a provider for the component, regardless of what's currently wired in your Rectangle context. This is useful if you're developing multiple providers and want to test them independently before integrating them into your system.\n\nThe real power of #[check_providers] becomes apparent when you're working with composed higher-order providers. Let's look at a more realistic example:\n\nNow, when we verify this setup, we want to ensure not only that the composed provider works, but also that each individual component in the composition is correct. We can do this:\n\nThe first check verifies that the wired provider works with the context. The second check verifies each individual provider (or provider combination) that makes up your composition. If something is broken, such as the width field is missing, the error messages from these targeted checks will clearly point you to the problematic provider, rather than the entire composition.\n\nThis is a game-changer for debugging complex CGP setups. Instead of staring at a wall of compiler errors and trying to trace through the dependency chain, you can surgically test each part of your provider composition.\n\nOne common pattern in CGP is using getter traits to extract values from your context. The #[cgp_auto_getter] macro makes this convenient by automatically implementing a getter that reads a field from your context using the HasField trait.\n\nHowever, there was one limitation that made more sophisticated getter traits tedious to work with: you couldn't define an abstract type for the return value of your getter. If you wanted a getter trait where the return type was customizable per context, you had to define a separate abstract type trait, then link it to your getter trait.\n\nThis meant the old pattern looked like:\n\nThis works, but it requires defining two separate traits when conceptually you just want one getter trait with a flexible return type. For simple cases, this felt like boilerplate.\n\nCGP v0.6.1 allows you to define associated types directly in getter traits, eliminating the need for the separate type trait. You can now write:\n\nThe HasName trait now has an abstract Name type, and when the auto-getter is derived for Person, it automatically implements HasName with String as the concrete Name type. This is much more concise and reads more naturally.\n\nThe benefits are particularly clear when you have multiple getters with abstract types. Instead of maintaining a parallel set of type traits, you can keep everything in one place, making your code easier to understand and maintain.\n\nIf you're using the more powerful #[cgp_getter] macro (which allows customization of the implementation through providers), the same support for associated types works seamlessly:\n\nEven though our struct has a first_name field, we can still use the HasName getter by wiring it with UseField. The #[cgp_getter] macro combined with associated types gives you both power and convenience.\n\nThese three changes—implicit Context types, enhanced provider checking, and associated types in getters—work together to achieve our goal: making CGP more accessible and maintainable without sacrificing its power.\n\nThe improvements lower the learning curve for new CGP users by reducing generic syntax overhead. They provide better tooling for verifying and debugging complex provider compositions. And they reduce boilerplate when defining sophisticated traits, freeing you to focus on business logic.\n\nAs your CGP codebase grows, you'll find yourself reaching for these new features frequently. The #[cgp_impl] simplification makes your code more readable. The #[check_providers] attribute helps you debug faster. And associated types in getters let you express your intent more directly.\n\nWe believe CGP v0.6.1 represents a meaningful step forward in making Context-Generic Programming more approachable and productive. These changes emerged directly from feedback and real-world usage patterns in the CGP community, and we're confident they'll improve the development experience for everyone.\n\nWe encourage you to upgrade to v0.6.1 and explore these new features. Try simplifying your provider implementations with implicit Context types. Use #[check_providers] to debug your next complex provider composition. Define getter traits with abstract types and feel the difference in conciseness and clarity.\n\nAs always, we welcome feedback, bug reports, and contributions. The CGP journey continues, and we're excited to see what you build with these new tools.",
    "readingTime": 7,
    "keywords": [
      "let's look",
      "check verifies",
      "cgp introduces",
      "business logic",
      "error messages",
      "we're excited",
      "cgp_getter macro",
      "check_providers attribute",
      "debugging complex",
      "generic parameter"
    ],
    "qualityScore": 1,
    "link": "https://contextgeneric.dev/blog/v0-6-1-release/",
    "thumbnail_url": "https://contextgeneric.dev/cgp-logo.png",
    "created_at": "2026-02-02T12:34:23.079Z",
    "topic": "tech"
  },
  {
    "slug": "the-creator-of-clawdbot-the-viral-ai-agent-says-he-got-so-obsessed-with-vibe-coding-it-pulled-him-into-a-rabbit-hole",
    "title": "The creator of Clawdbot, the viral AI agent, says he got so obsessed with vibe coding it pulled him into a 'rabbit hole'",
    "description": "The creator of Clawdbot, the viral AI agent, says vibe coding can blur into compulsion, creating the illusion of productivity without real progress.",
    "fullText": "The creator of the viral AI agent Clawdbot says he had to step back after becoming too obsessed with vibe coding.\n\nPeter Steinberger, the developer behind Clawdbot — which later changed its name to Moltbot and is now known as OpenClaw — said in an episode of \"Behind the Craft\" podcast published Sunday that vibe coding pulled him into a \"rabbit hole.\"\n\n\"I was out with my friends and instead of, like, joining the conversation in the restaurant, I was just like, vibe coding on my phone,\" he said.\n\n\"I decided, OK, I have to stop this more for my mental health than for anything else,\" he added.\n\nClawdbot went viral last month in the tech community, attracting a wave of high-profile fans — from Y Combinator CEO Garry Tan to multiple partners at Andreessen Horowitz.\n\nIt is a personal AI agent designed to run continuously and plug into a wide range of consumer apps, including WhatsApp and Telegram. Users can ask the AI to manage their schedules, oversee vibe-coding sessions, and even create AI employees.\n\nThe AI agent has been widely praised and meme'd online, with some tech fans even buying Mac Minis specifically to run the AI, Business Insider's Henry Chandonnet reported last week.\n\n​​Steinberger said developers can fall into this trap of being hooked onto vibe coding, where building increasingly powerful AI tools creates the \"illusion of making you more productive\" without real progress.\n\nBuilding new tools can feel rewarding and fun, but that can quietly blur into compulsion, he added.\n\nWith AI, developers can now \"build everything,\" but ideas and taste matter. Without them, developers risk building tools and workflows that don't actually move a project forward, ​​Steinberger said.\n\n\"If you don't have a vision of what you're going to build, it's still going to be slop,\" he added.\n\nVibe coding has continued to surge in popularity, with companies and developers promoting how AI can speed up software development.\n\nEarlier this month, Anthropic said it built its new agentic work tool, Cowork, entirely using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. \"Us humans meet in-person to discuss foundational architectural and product decisions, but all of us devs manage anywhere between 3 to 8 Claude instances implementing features, fixing bugs, or researching potential solutions.\"\n\nThanks to Claude, the agent came together quickly. \"We sprinted at this for the last week and a half,\" Rieseberg said during a livestream.\n\nStill, despite the excitement around how fast vibe coding can produce new tools, tech leaders are warning that it has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that he won't vibe code on \"large codebases where you really have to get it right.\"\n\n\"The security has to be there,\" he added.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding is great for prototypes or throwaway code, not software that sits at the core of a business.\n\n​​\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "developers",
      "agent",
      "tools",
      "clawdbot",
      "tech",
      "viral",
      "episode",
      "fans"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/clawdbot-creator-vibe-coding-rabbit-hole-obsessed-openclaw-peter-steinberger-2026-2",
    "thumbnail_url": "https://i.insider.com/69802a8da645d11881886c3c?width=1200&format=jpeg",
    "created_at": "2026-02-02T12:34:16.408Z",
    "topic": "finance"
  },
  {
    "slug": "i-embedded-myself-in-a-vibe-coding-team-at-geminis-ai-hackathon-in-singapore-building-an-app-in-7-hours-takes-real-work",
    "title": "I embedded myself in a vibe coding team at Gemini's AI hackathon in Singapore. Building an app in 7 hours takes real work.",
    "description": "I followed a hackathon team as they raced to vibe code an app in seven hours at Google's Gemini 3 Hackathon in Singapore.",
    "fullText": "Just after sunrise, four vibe coding enthusiasts from Malaysia crossed into Singapore with a loose idea — and a bet that AI could build most of their app.\n\nHours later, they were racing to prototype it at Google's Gemini 3 Hackathon in Singapore.\n\nThe four friends, all in their late 30s to 40s, came from different professional backgrounds. Chan Wei Khjan is an accountant. Chan Ler-Kuan lectures on AI at a private university. Loh Wah Kiang works in IT. Lee How Siem, who goes by Benny, is the chief technology officer of a Malaysian startup.\n\nTheir initial idea was a \"feng shui\" app to analyze properties in Singapore — a potentially lucrative use case in a market obsessed with housing and wealth accumulation. Feng shui is a traditional Chinese practice that evaluates how a person's surroundings, along with birth factors, influence luck and well-being.\n\nI embedded with the team at Google's developer space in Singapore in January to observe how a vibe-coding project comes together — or nearly falls apart — in seven hours.\n\nThe assignment: Teams of up to four people had to build a working demo, publish a public repository with code, and submit a short video explaining their project by 5:30 p.m.\n\nEach project had to fit into one of six tracks, including generative media, deep research, and enterprise orchestration.\n\nOrganized by Google DeepMind and 65labs, Singapore's AI builder collective, the hackathon featured a 100,000-credit Gemini API prize pool, with first place getting 30,000 credits.\n\nThe team had pivoted to a new idea due to time constraints: a feng shui app that could analyse a user's outfit and workspace through the phone camera in real time and assess how \"lucky\" they were.\n\nWei Khjan took the lead on prompting. He typed the first instructions into Claude, asking it to generate the workflow and code. Ler-Kuan focused on whether the AI's output aligned with feng shui concepts. Wah Kiang and Benny hovered over the codebase, refining ideas and flagging issues.\n\n\"For people who don't know how to read code, it's helpful to have people who do,\" Wei Khjan said.\n\nWhile waiting for the code to be generated, Ler-Kuan opened Google's AI Studio to design the app's logo. They called their app \"Feng Shui Banana.\"\n\nAfter about an hour, Claude generated the initial codebase for the app. It was designed to work with the Gemini Live API, enabling real-time image and text analysis. It ran but was riddled with bugs.\n\nAn error message flashed when they tested the camera feature, so Wei Khjan copied the error back into the AI and asked for it to be fixed. Minutes later, the feature worked.\n\nIt wasn't right. The feng shui logic was off, especially where colour analysis intersected with the user's birth timings. Ler-Kuan manually corrected the underlying dictionary and its mappings.\n\nThe team kept prompting to tighten the features: shorter explanations, clearer output, and more streamlined user interfaces.\n\nLunch arrived. The team stayed glued to their screens.\n\nThe app didn't respond instantly when a user changed their outfit, nor did it update its feng shui analysis in real time.\n\nWei Khjan explained how one prompt matters. Instead of issuing commands, he asked the AI to \"discuss it with me.\" The shift changed how the model reasoned, and it worked more like a collaborator.\n\nAfter some prompting, the app updated with a real-time camera analysis. It was striking to watch a feature emerging from a short back-and-forth with AI.\n\nI helped the team test the app.\n\nThe camera correctly identified what I was wearing: a dark green polo, a yellow participant tag, and a white name card hanging from my neck. According to the app, I was already wearing colours aligned with my luck for the day.\n\nThe app suggested small tweaks, such as additional accessories, that could enhance the feng shui of my outfit.\n\nThey finally had lunch and joked around to ease the tension. Four hours remained before they had to submit their project.\n\nLer-Kuan shifted focus to workspace feng shui, feeding knowledge into the model and refining how the app would evaluate desks and work setups. Wah Kiang and Benny worked on the video demo.\n\nThe team also revisited the app's tagline. After cycling through suggestions from multiple AI models, they settled on a line that didn't come from an AI at all: \"A wisdom, not a superstition.\"\n\nThey used Gemini to generate a storyboard for the demo video. The model laid out several scenes and drafted the script. The team followed along, filming clips and stitching everything together as they went.\n\nTheir workspace feature was also up and running.\n\nThe app had come together nicely. With some time to spare, they decided to add audio output for users who prefer listening to reading on a screen.\n\nThe first attempt to generate a voice using AI fell flat. It sounded robotic.\n\nAfter debugging and several iterations, they landed on a voice they liked, similar to how a Chinese feng shui master might speak.\n\nAs the deadline approached, the team was still stitching clips for their video and nitpicking the AI-generated presenter voice.\n\nThe organizers had urged teams to submit early. With about 15 minutes to spare, they made the call to lock the final cut and hit submit.\n\nThen it was over. The hunger hit immediately, and everyone got in line for some well-deserved food.\n\nEven as an observer, watching from the sidelines was tiring. Seven hours of vibe coding turned out to be anything but effortless.\n\nThe team didn't win a prize, but agreed that the hackathon had been worth it.\n\n\"Sometimes, the best experiences come from saying 'yes' without overthinking,\" said Ler Kuan. \"Innovation starts with curiosity and a little bit of spontaneity.\"\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "wah kiang",
      "vibe coding",
      "feng shui",
      "wei khjan",
      "shui app",
      "wah kiang and benny",
      "team",
      "hours",
      "project",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/vibe-coding-team-embed-google-gemini-hackathon-singapore-2026-2",
    "thumbnail_url": "https://i.insider.com/696ef318a645d118818798f3?width=1200&format=jpeg",
    "created_at": "2026-02-02T06:52:12.008Z",
    "topic": "finance"
  },
  {
    "slug": "tailwind-creator-adam-wathan-shares-new-project-uish",
    "title": "Tailwind creator Adam Wathan shares new project ui.sh",
    "description": "A toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.",
    "fullText": "Turn your terminal intoA toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.Request an inviteBy the people who made\nTailwind CSS & Refactoring UI",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://ui.sh/",
    "thumbnail_url": "https://ui.sh/og-image.png",
    "created_at": "2026-02-02T01:11:00.781Z",
    "topic": "tech"
  },
  {
    "slug": "kessel-run-air-force-software-development-division",
    "title": "'Kessel Run' Air Force software development division",
    "description": null,
    "fullText": "Kessel Run, formally \"Air Force Life Cycle Management Center Detachment 12\", is a United States Air Force software development division, based in Hanscom Air Force Base and Boston, Massachusetts. It was founded in 2017 by the Defense Innovation Unit in response to the need to modernize legacy Air Force software.\n\nIn October 2016, Eric Schmidt, former CEO of Google, was leading a group touring the Combined Air Operations Center at Al Udeid Air Base in Qatar in his role as inaugural chairman of the Defense Innovation Board.[1] The CAOC at Al Udeid oversees air force operations for over 20 countries, and at the time was engaged in the War in Iraq against the Islamic State.[1]\n\nOne of the Air Operations Center tasks was planning daily aerial refueling operations to support combat missions. This was done off the main CAOC hall, in a windowless room with a whiteboard bearing magnetic pucks and plastic laminated cards, and physically measuring distance on the board to determine how long planes could stay in the air.[1][2] The resulting data was manually entered into an Excel spreadsheet known as \"the Gonkulator\" by a person called \"the Gonker\".[1][2] When a VBScript run on the spreadsheet confirmed the data was correct, another person, called \"the Thumper\", manually typed in the result into a Master Air Attack Planning Toolkit, which helped generate the Air Tasking Order back at Shaw Air Force Base in South Carolina.[1] Yet another person watched to verify against retyping errors.[2] The process took eight to 12 hours each day for three or eight people.[1][2][3] If there was a change needed, the process needed to restart.[2][3] When Schmidt and the Defense Information Board asked whether the base had access to more modern software to automate this process, the answer was: \"Yes, but it doesn't work.\"[1]\n\nAir Operations Center software had been in use mostly unchanged since the 1990s.[1] From 2006 to 2011, Lockheed Martin worked on the concept of modernizing it, but did not take up the project.[4] A Northrop Grumman project to modernize AOC software was commissioned in 2013 for $374 million for development, and $3.5 billion for lifetime maintenance.[4] By 2016, when the Defense Innovation Board was touring Al Udeid, nothing had been delivered.[2] The development price eventually grew to $745 million, and was three years behind schedule, with an estimated launch date of December 2019, when the project was eventually cancelled in July 2017.[4]\n\nAlso touring the Al Udeid CAOC with Schmidt in October 2016 was Raj Shah, tech entrepreneur and managing partner of DIUx, the Defense Innovation Unit Experimental.[1][2] Shah was a former Air Force fighter pilot who had first hand experience of the importance of regular aerial refueling.[2] That same night, he called lieutenant colonel Enrique Oti who was head of Air Force programs for DIUx in Silicon Valley.[1] They arranged an unusual partnership between Air Force developers and Pivotal Software, and got it approved by General Jeffrey Harrigian, commanding United States Air Forces Central Command.[1] Rather than rewrite the entire AOC software suite, they would just do the tanker whiteboard.[clarification needed][1] In December 2016, coders and program managers were visiting Al Udeid to talk to users. By April 2017, the aerial refueling tanker application, named Jigsaw, was in use at the CAOC; four months from start to production, when a more average Defense Department software operation took as much as three to five years.[1][5]\n\nThe speed of development was credited to the Agile development process, an iterative, adaptive approach, that doesn't try to get the entire solution in the first release. The first version of an application is intentionally only about a 60% solution, which is then changed and improved via rapid iterations as users give feedback.[6] Though this agile development is fairly basic in the modern software industry, it was unusual for the Defense Department.[7] The development team would later adopt the hashtag #AgileAF - the AF, they assure, stands for Air Force.[1]\n\nThe total cost of Jigsaw was reported at $1.5 million (Captain Bryon Kroger, Chief Operating Officer of the project),[5] to $2.2 million (Shah),[1] \"chump change\" (Harrigian).[8] With it, tanker planning was not only faster, taking two to three hours for a single person, but was more reliable, so two to three fewer tankers were scrambled each day.[1][3] Each scramble had a cost in fuel and maintenance of about $250,000 each.[1][2] Jigsaw saved 350,000 pounds of fuel a week.[8] Its development costs were recouped in the first week.[1][9]\n\nIn April 2017, after delivering Jigsaw, Oti, Kroger, and others, got approval to form an official Air Force software development team at the Air Force Life Cycle Management Center named Project Kessel Run.[2][10][5] The name \"Kessel Run\" came from a line in the 1977 science fiction film Star Wars, spoken by smuggler Han Solo, bragging about the speed of his ship, the Millennium Falcon.[2] It represented the project's intent to \"smuggle\" new software development capability into the Air Force and use it to set new software development speed records.[9]\n\nIn May 7, 2018, the Kessel Run Experimentation Lab set up at a WeWork shared facility in central Boston. It was modeled after Pivotal Labs training locations in Cambridge, San Francisco, and Washington.[11] It was managed from the AFLCMC at Hanscom Air Force Base, which believed the innovation advantages in coworking and creativity would outweigh the hassles of distance and security.[11][12] The Lab initially had space for 90 engineers, but planned for 300 within a year.[9] Many were on temporary assignment from other Air Force bases; yet others would be sent off to Pivotal Labs offices across the country for training in modern software techniques.[11][13][14] The Kessel Run motto on the wall was \"Code. Deploy. Win.\", a play on the Air Force's motto, \"Fly. Fight. Win.\".[12]\n\nOn January 2, 2019, the Kessel Run Experimentation Laboratory moved to a different location in a Boston skyscraper.[15] Kessel Run's budget since mid 2017 had grown to approximately $140 million, including workspace and personnel, and the operational software produced claimed savings of $13 million and 1,100 man-hours per month.[15]\n\nOn May 8, 2019, Kessel Run formally became Air Force Life Cycle Management Center Detachment 12, commanded by Colonel Oti, who had been effectively leading it for just under a year.[16] It had nearly 700 airmen, government civilians, and contractors.[16] Oti was replaced as commander by Colonel Brian Beachkofski on April 15, 2020, with the ceremony held over Zoom teleconference.[17] On June 27, 2022, Beachkofski was replaced as commander by Colonel Richard Lopez, who took the title of senior materiel leader.[18] Lopez had previously been the director of the LevelUP Code Works software factory inspired by Kessel Run.[18][19]\n\nThe March 2019 Defense Innovation Board report on software acquisitions included a chapter on Kessel Run subtitled \"The Future of Defense Acquisitions Is #AgileAF\".[21]\nIn September and October 2019, Kessel Run received multiple awards: the General Larry O. Spencer Innovation award; the Theodore von Kármán award for modernizing software for the F-35; and the inaugural Defense Acquisition Software Innovation Team award.[22][23] A 2019 editorial in Defense One said that Kessel Run was \"widely seen as the gold standard of military tech done right ... also the most hyped military program office in operation today\".[24]\n\nNot all reactions were positive. A 2019 anonymous survey of KREL application users found that some applications did not meet user needs, and that success metrics, documentation, and responsiveness to user feedback could all be lacking.[25] A 2020 Harvard Kennedy School project found and tried to address internal discontent among Kessel Run staff with emerging bureaucracy and increasing technical complexity.[26] The Air Force's Deputy Chief Information Officer, Lauren Knausenberger, acknowledged in 2020 that Kessel Run was having growing pains, but said that was a result of its success.[27]\n\nKessel Run inspired multiple agile software development teams across the Air Force and United States Department of Defense.[28] They were called \"software factories\".[29] The original definition of software factory was a set of software tools to write and automatically build, test, and document applications; the Chief Information Officer of the Department of Defense slightly redefined that to be a software assembly plant that automated the develop, build, test, release and deliver phases, but in each case to support agile software development practices.[30]\n\nKobayashi Maru, formally Space C2, or Space Command and Control, in California, was the second such software factory, in August 2018.[28] It was named after an impossible test in the Star Trek science fiction universe.[31] Just as Kessel Run came from an effort to replace an outdated system by getting around bureaucratic rules, Kobayashi Maru intended to update the software of the troubled Joint Mission System (jointly run with the United States Space Force) for space command and control and situational awareness.[32][33]\n\nBESPIN - an acronym for Business and Enterprise Systems Product Innovation, but also the name of a planet in the Star Wars universe - was the third Air Force software factory, launched in early 2019 in Montgomery, Alabama, to create apps for maintenance crew chiefs, aircrew readiness, and ammunition crews.[28][31] Space Camp, in Colorado, and Section31, in California, spun off of Kobayashi Maru.[31] LevelUP, in Texas, was a joint cyber operations system for the Unified Platform, connecting the Army, Marines, and United States Cyber Command, debuting in April 2019.[31] By September 2021, there were 17 Air Force software factories across the country.[34]\n\nSoftware factories weren't limited to the Air Force. The Navy was inspired by Kessel Run to stand up its first software factory, The Forge, in Riverdale, Maryland, in March 2021.[35] The Army Software Factory debuted in April 2021 in Austin Community College in Texas, as part of the United States Army Futures Command.[36][37] In February 2022, Deputy Defense Secretary Kathleen Hicks wrote a DOD Software Modernization Strategy memo encouraging increased used of software factories throughout the Defense Department; at the time there were 29.[38] By April 2022 the United States Coast Guard was planning a software factory based on the Air Force model.[39] The Marine Corps Software Factory was co-located with the Army Software Factory as a three year test project in Austin in March 2023.[40]\n\nThe 24th Air Force's Air Force Cyber Proving Ground may be another related activity.\n\nJigsaw, the 2017 aerial refueling planning application that started Kessel Run, was bought and used by NATO in multiple countries in 2020 and 2021.[41][42]\n\nThe team's second and third projects after Jigsaw were Chainsaw and Raven, applications for assembling and communicating target information.[1] Chainsaw was in operation by November 2017, consolidated many programs into one, and cut the process for dynamic targeting from an hour or two to minutes. Raven, for target development management, cut 12 hours of work down to three or four, and was ready in early 2018.[43]\n\nStarting in late 2018, Kessel Run joined the task of fixing the troubled software for the maintenance of the F-35 fighter jet, called ALIS, for Autonomic Logistic Information System.[44] ALIS was a 17 year old proprietary system full of bugs and data gaps.[45] Maintainers had to keep separate databases because they could not rely on ALIS data.[46] The project to fix ALIS, including Kessel Run, Pivotal, and Lockheed Martin, its original creators, was called Mad Hatter, named by the developers.[44] It officially started in October 2018, but took until January 2019 before developers could write code, while the Air Force negotiated with Lockheed Martin as to what parts of the proprietary ALIS system the government could be able to reach.[44] The Mad Hatter suite of eight programs was tested and favorably evaluated by F-35 aircraft maintainers in March 2020.[47] In July 2020, it was renamed to Torque, and adapted for maintenance of the F-22 stealth jet and CV-22 tiltrotor aircraft,[48] then the C-130J turboprop in January 2021.[49] Meanwhile, on the F-35 itself, between 2020 and 2022 ALIS was gradually replaced by ODIN, the Operational Data Integrated Network, \"leveraging\" the software practices of Kessel Run, but built by Lockheed Martin.[50][51]\n\nIn 2021, Kessel Run began deploying the initial version of KRADOS, the Kessel Run All Domain Operations Suite meant to replace the Theater Battle Management Core Systems that created air tasking orders throughout AOCs all over the world.[52] The first AOC to use the suite operationally was again the 609th Air Operations Center at Al Udeid, in May 2021, after using the Beta version since December 2020.[53] KRADOS linked together nine applications through cloud-based data, including the latest version of Jigsaw, Kessel Run's first application for tanker planning, and Slapshot, for planning the rest of the air missions and building the Master Air Attack Plan.[54] In 2017, Lockheed Martin had received a $38 million contract to maintain the older TBMCS, but the 609th kept finding problems, so turned to Kessel Run, which delivered the beta version of KRADOS three weeks after receiving the request in November 2020.[55] By August 2022, the 603rd AOC in Ramstein, Germany, employed elements of KRADOS for visualization, though it was not considered mature enough to create air tasking and airspace control orders.[56][57] In January 2023, the 609th replaced the TBMCS with KRADOS entirely.[58]\n\nThe Command and Control Incident Management Emergency Response Application (C2IMERA), is a real time Air Force base resource management tool. It used a different development model: the coding was done by software company Leidos, and the program management by Kessel Run.[59] In August 2019, Moody Air Force Base used the software to monitor and prepare for Hurricane Dorian, though it was not originally intended for this purpose.[60] C2IMERA was also used for the August 2021 evacuation of civilians from Afghanistan in Operation Allies Refuge.[61][62] It was ordered deployed across all Air Combat Command installations in September 2021.[59][63] In August 2023, Air Mobility Command joined Air Combat Command in designating C2IMERA as their standard installation command and control tool.[64]\n\nSlapshot, the air mission flow organizer part of KRADOS, was also used for the Operation Allies Refuge Afghan evacuation along with C2IMERA.[65][66] At that time, KRADOS had known issues with scaling; it couldn't handle many simultaneous operations, which was exactly what it was being asked to do. On August 24, 2021, at 2 am Boston time, the Slapshot server crashed. Over the next 12 hours, Kessel Run developers restarted servers, shifted United States Central Command resources to improve performance, fixed database errors, and added new features to improve load times, so the evacuation on the other side of the world could continue.[67]\n\nBowcaster, named after a Star Wars weapon, is a chaos engineering tool and playbook that intentionally creates failures in processes to strengthen them.[68][69]\nKessel Run developed it, and shares it with other government agencies, initially with the Navy Black Pearl software factory in 2021.[70][71] In March 2022 Kessel Run and the General Services Administration's Technology Transformation Services used it to check that the Cloud.gov website could handle 100 million users per hour.[72][73]",
    "readingTime": 13,
    "keywords": [
      "kessel run",
      "air force",
      "allies refuge",
      "life cycle",
      "operation allies",
      "center detachment",
      "cycle management",
      "innovation unit",
      "innovation board",
      "operations center"
    ],
    "qualityScore": 1,
    "link": "https://en.wikipedia.org/wiki/Kessel_Run",
    "thumbnail_url": "https://upload.wikimedia.org/wikipedia/commons/7/77/Kessel_Run_logo.jpg",
    "created_at": "2026-02-01T12:26:41.722Z",
    "topic": "tech"
  },
  {
    "slug": "securing-the-ralph-wiggum-loop-devsecops-for-autonomous-coding-agents",
    "title": "Securing the Ralph Wiggum Loop – DevSecOps for Autonomous Coding Agents",
    "description": "Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck - agairola/securing-ralph-loop",
    "fullText": "agairola\n\n /\n\n securing-ralph-loop\n\n Public\n\n Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n agairola/securing-ralph-loop",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/agairola/securing-ralph-loop",
    "thumbnail_url": "https://opengraph.githubassets.com/6d3ccf091e1097ce34aa5949ea80238ec45757c8d310e8b3be3ea2b9c6e218fb/agairola/securing-ralph-loop",
    "created_at": "2026-02-01T06:37:20.298Z",
    "topic": "tech"
  },
  {
    "slug": "top-engineers-at-anthropic-openai-say-ai-now-writes-100-of-their-code",
    "title": "Top engineers at Anthropic, OpenAI say AI now writes 100% of their code",
    "description": "AI coding tools are getting more sophisticated. But if coders stop coding, what happens to software development jobs?",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/01/29/100-percent-of-code-at-anthropic-and-openai-is-now-ai-written-boris-cherny-roon/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/GettyImages-2216956965_40536e-e1769705381107.jpg?resize=1200,600",
    "created_at": "2026-01-31T01:04:24.596Z",
    "topic": "tech"
  },
  {
    "slug": "cooperbench-benchmarking-ai-agents-cooperation",
    "title": "CooperBench: Benchmarking AI Agents' Cooperation",
    "description": "CooperBench is a benchmark of over 600 collaborative coding tasks. We find that agents achieve 30% lower success rates when working together compared to performing both tasks individually.",
    "fullText": "Stanford University & SAP Labs US\n\nCan AI agents work together as teammates? We find\n that\n coordinating agents perform much worse than a\n single agent\n given the same total workload. This coordination\n deficit presents a fundamental barrier to deploying\n AI systems that can work alongside humans or other\n agents.\n\nSuccess rate on CooperBench across 652 tasks · Error bars show 95% confidence intervals\n\nGPT-5 and Claude Sonnet 4.5 achieve only 25%\n success with two-agent cooperation, roughly 50%\n lower than when a single agent handles both\n tasks. This gap persists across all models and\n task difficulties.\n\nAgents spend up to 20% of their budget on\n communication. This reduces merge conflicts but\n does not improve overall success. The channel is\n jammed with repetition, unresponsiveness, and\n hallucination.\n\nEven when agents communicate well, coordination\n breaks down due to:\n\nAmong successful runs, we observe coordination patterns\n largely absent from failures. These patterns are not\n prompted or scaffolded.\n\nRole Division\n — Agents agree on who handles which part of the\n task. One agent delegates: \"I'll add header +\n octal_str; you add binary_str between them.\"\n\nCooperBench is the first benchmark designed to\n measure how well AI agents can cooperate when\n handling individual tasks with potential conflicts.\n We constructed 652 tasks from 12 popular open-source\n libraries across Python, TypeScript, Go, and Rust.\n\nEach task assigns two agents different features that\n can be implemented independently but may conflict\n without proper coordination. Eight co-authors with\n real-world software engineering backgrounds created\n new features, unit tests, and ground-truth code.\n\nStanford University & SAP Labs · *Equal contribution\n (Stanford) · †Equal contribution (SAP Labs)",
    "readingTime": 2,
    "keywords": [
      "equal contribution",
      "stanford university",
      "university sap",
      "coordination",
      "tasks",
      "success",
      "across",
      "task",
      "agents",
      "cooperbench"
    ],
    "qualityScore": 0.85,
    "link": "https://cooperbench.com/",
    "thumbnail_url": "https://cooperbench.com/static/images/cooperbench_social.png",
    "created_at": "2026-01-30T18:28:27.534Z",
    "topic": "tech"
  },
  {
    "slug": "daedalus",
    "title": "Daedalus",
    "description": "AI planning CLI and autonomous agent orchestration for beans-based coding workflows - internet-development/daedalus",
    "fullText": "internet-development\n\n /\n\n daedalus\n\n Public\n\n AI planning CLI and autonomous agent orchestration for beans-based coding workflows\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n internet-development/daedalus",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/internet-development/daedalus",
    "thumbnail_url": "https://opengraph.githubassets.com/471147f12cb10edd91f2072a6602ce03fd3e4339e4f03a4a184ee7336b8d31ea/internet-development/daedalus",
    "created_at": "2026-01-30T06:35:17.254Z",
    "topic": "tech"
  },
  {
    "slug": "cwt-sandbox-ai-coding-agents-using-git-worktrees",
    "title": "Cwt – Sandbox AI coding agents using Git Worktrees",
    "description": "Contribute to benngarcia/claude-worktree development by creating an account on GitHub.",
    "fullText": "benngarcia\n\n /\n\n claude-worktree\n\n Public\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n benngarcia/claude-worktree",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/benngarcia/claude-worktree",
    "thumbnail_url": "https://opengraph.githubassets.com/73752eaadbf154afd745270d32c1743038163f0555993f4c462cecd915f63c02/benngarcia/claude-worktree",
    "created_at": "2026-01-30T01:07:08.980Z",
    "topic": "tech"
  },
  {
    "slug": "acp-agent-registry-in-jetbrains-ides",
    "title": "ACP Agent Registry in JetBrains IDEs",
    "description": "Together with Zed, we've launched the official ACP Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs.",
    "fullText": "Supercharge your tools with AI-powered features inside many JetBrains products\n\nAI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day. Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what’s out there, let alone getting it running in your IDE, hasn’t been easy.\n\nTogether with Zed (Zed’s announcement), we’ve launched the official ACP Agent Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs and Zed. Browse what’s available, click Install, and start working right away. This beta release is just the beginning.\n\nThe Agent Client Protocol is an open standard that lets any AI coding agent work in any supporting editor. Think of it like the Language Server Protocol, but for AI agents. The LSP lets any editor support any language through a shared standard. The ACP does the same for coding agents. You only need to implement it once, and then it will work in your JetBrains IDE, Zed, or any other editor that supports the protocol.\n\nThis means you get to pick your preferred agent and editor, and they will then work together seamlessly – no vendor lock-in and no waiting for someone to build a specific integration.\n\nACP has been, since we started integrating it to Mistral Vibe, a real joy to use: thoughtfully designed from the ground up, community-driven, and evolving rapidly. We’ve found it not only simplifies integration, but also fits our focus on open and flexible tools. It’s really great to see a standard that puts developer choice first.\n\nMichel Thomazo, Software Engineer @ Mistral AI\n\nThe ACP made agent interoperability technically possible. The registry makes it convenient.\n\nInstead of manually configuring agents, you can now:\n\nAt launch, you’ll find a wide array of different agents:\n\nFull-featured coding assistant optimized for large-scale refactors\n\nSpecialized agent for automated code generation workflows\n\nGoogle’s agent with deep codebase understanding and multimodal capabilities\n\nGitHub’s AI pair programmer, now available via the ACP\n\nLightweight, fast agent built on Mistral’s models\n\nCommunity-driven, fully open-source agent\n\nAlibaba’s coding agent with strong multilingual support\n\nInnovation in software agents is moving at an unbelievable pace. The Agent Registry and ACP makes it simple for developers to use the best agents in their favorite tools.\n\nChris Kelly, Product @ Augment Code\n\nIn general, it’s less about having multiple agents than about enabling you to pick and choose the ones that work well in your workflow. Different agents come with different benefits. Some provide a more attractive pricing structure for your business, some provide a user experience that you simply enjoy more than others’, and some embody the ideas of open-source development that just resonate with you.\n\nThe Agent Client Protocol registry lets you experiment freely. Try a few, see what clicks for your workflow, and then keep the ones that help. You’re not locked into a single vendor’s vision of what AI-assisted development should look like.\n\nWe’re excited to support the ACP Agent Registry as a step toward a more open agent ecosystem where Droids can integrate seamlessly across all IDEs.\n\nFrancesca LaBianca, VP of Operations @ Factory\n\nIn any JetBrains IDE (2025.3.2+) with JetBrains AI (253.30387.147):\n\nThat’s it. The agent is configured and ready to use in the AI Chat tool window.\n\nQuick note: agents typically come with their own subscription. That’s between you and them. You won’t need a JetBrains AI subscription to use ACP agents.\n\nWant to try something concrete? Install OpenCode, open a project, and ask it to explain an unfamiliar module. OpenCode also lets you swap between different LLMs, so you can experiment with what works best for you.\n\nIf you prefer manual configuration, that option is still there, too. Just edit the acp.json directly. This is useful for agents that aren’t in the registry yet or for custom setups.\n\nIf you’re building an ACP-compatible agent, the registry is now the fastest way to reach developers across JetBrains IDEs and Zed.\n\nHead to the ACP Registry repository and check out the CONTRIBUTING.md for the full submission process and metadata requirements. Please note that, for now, we are only featuring agents that support Agent Auth or Terminal Auth. Full details of requirements and conditions can be found here.\n\nThis is an open registry. If you’re building an ACP-compatible agent, you’re welcome to submit it. The registry exists to serve the ecosystem, not to gatekeep it.\n\nFor developers: More choice and zero lock-in. Use any agent you want in the IDE you love.\n\nFor agent builders: Instant distribution to millions of JetBrains and Zed users. Implement the ACP once and reach everyone.\n\nFor the ecosystem: Competition on quality, not on who controls the integration. The best agents win because they’re the best, not because they have exclusive deals.\n\nWe’re building this openly with Zed because we believe AI-assisted development shouldn’t be locked inside any single vendor’s ecosystem. Developers deserve to pick their tools freely.\n\nThe registry is one more step toward that future.\n\nThe ACP Registry is available now in JetBrains IDE versions 2025.3 and later. Update your IDE and the JetBrains AI plugin, open Settings, and start exploring.\n\nHave feedback? Found a bug? The registry repo is open for issues and PRs. And if you’re building something interesting with ACP, we’d love to hear about it!\n\nOpenAI Codex is now natively integrated into the JetBrains AI chat, giving you another powerful option for tackling real development tasks right inside your IDE. \n\nYou can use Codex with a JetBrains AI subscription, your ChatGPT account, or an OpenAI API key – all within the same AI сhat inte…\n\nThe next edit suggestions feature is now enabled in all JetBrains IDEs for JetBrains AI Pro, AI Ultimate, and AI Enterprise subscribers.\n\nYes, you read that right! JetBrains-native diff suggestions are available right in your editor. Global support for optimized latency. Out-of-the-box IDE actions…\n\nBring Your Own Key (BYOK) is now available in the AI chat inside JetBrains IDEs as well as for AI agents, including JetBrains’ Junie and Claude Agent. Whether you’re looking to use cutting-edge frontier models, cost-efficient small models, locally hosted private models, or experimental research prev…\n\nJunie is now integrated into the AI chat. The separate interfaces have merged into a single, unified space (available in Beta).",
    "readingTime": 6,
    "keywords": [
      "client protocol",
      "ai-assisted development",
      "step toward",
      "agent client",
      "acp-compatible agent",
      "jetbrains ai",
      "coding agents",
      "acp agent",
      "acp agent registry",
      "jetbrains ide"
    ],
    "qualityScore": 1,
    "link": "https://blog.jetbrains.com/ai/2026/01/acp-agent-registry/",
    "thumbnail_url": "https://blog.jetbrains.com/wp-content/uploads/2026/01/JB-social-BlogSocialShare-1280x720-1-4.png",
    "created_at": "2026-01-29T18:30:47.768Z",
    "topic": "tech"
  },
  {
    "slug": "extesla-ai-head-has-seen-a-phase-shift-in-software-engineering-using-claude-code-and-his-manual-skills-slowly-atrophy",
    "title": "Ex-Tesla AI head has seen a 'phase shift in software engineering' using Claude Code — and his manual skills slowly 'atrophy'",
    "description": "Andrej Karpathy posted his \"notes from Claude Coding,\" describing a shift in engineering over the last two months.",
    "fullText": "He coined \"vibe coding.\" Now, he sees a \"phase shift\" in software engineering.\n\nAndrej Karpathy is one of AI's guiding figures. He was a founding member of OpenAI and later served as Tesla's director of AI. He also coined the term \"vibe coding,\" the AI-assisted coding movement that has taken software engineering by storm and was named Collins Dictionary's word of the year.\n\nIn his \"random notes from Claude Coding\" — which are over 1,000 words long — Karpathy wrote about the changes to his own coding style. Posted on X on Monday, the notes have already elicited reactions from engineers at Anthropic, xAI, and more.\n\nAI coding agents \"crossed some kind of threshold of coherence around December 2025 and caused a phase shift in software engineering,\" Karpathy wrote.\n\nA few random notes from claude coding quite a bit last few weeks.\n\nCoding workflow. Given the latest lift in LLM coding capability, like many others I rapidly went from about 80% manual+autocomplete coding and 20% agents in November to 80% agent coding and 20% edits+touchups in…\n\nKarpathy name-dropped both Anthropic's Claude Code and OpenAI's Codex as having significant improvements. Claude Opus 4.5, the model that has garnered much love from engineers online, came out at the tail end of November.\n\nThe AI leader's workflow has changed as a result of the AI tools. From November to December, Karpathy's 80/20 ratio flipped. He once used 80% manual coding and 20% agents; now, it's 80% agents and 20% manual code editing.\n\n\"I really am mostly programming in English now, a bit sheepishly telling the LLM what code to write... in words,\" he wrote.\n\nThe change to AI-written code \"hurts the ego,\" but is too powerful to ignore, Karpathy wrote. He also devoted a whole section of his notes to the \"fun\" he has while coding with large language models.\n\nWhat of those traditional coding skills, the ones you learn in a computer science program or through endless digital courses? That's a whole other function, Karpathy wrote, and one that might decline.\n\n\"I've already noticed that I am slowly starting to atrophy my ability to write code manually,\" he wrote.\n\nIn Karpathy's comments, engineers from leading AI companies sounded off. Ethan He, an xAI engineer and Nvidia alum, wrote that a \"10x engineer can be a one-man army.\"\n\nCharles Weill, another xAI engineer, wrote that founders can now \"divide themselves\" with coding agents, like a VC divides their capital over a portfolio of companies.\n\nBoris Cherny, an Anthropic staffer and the creator of Claude Code, wrote that he read Karpathy's \"thoughtful\" post till its end.\n\nThe Claude Code team at Anthropic may offer a model of where the industry is moving, Cherny wrote. His team is \"mostly generalists\" and filled with 10x engineers.\n\n\"Pretty much 100% of our code is written by Claude Code,\" Cherny wrote. \"For me personally it has been 100% for two+ months now, I don't even make small edits by hand.\"\n\nThe Anthropic employee also acknowledged the \"quality\" problems with AI-written code. Agents can overcomplicate things and can leave around dead code, he wrote.\n\nHis solution: having AI review the AI-written code.",
    "readingTime": 3,
    "keywords": [
      "ai-written code",
      "phase shift",
      "software engineering",
      "random notes",
      "xai engineer",
      "vibe coding",
      "coding agents",
      "engineers",
      "claude",
      "coined"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-claude-code-manual-skills-atrophy-software-engineering-tesla-2026-1",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-01-29T12:32:28.928Z",
    "topic": "tech"
  },
  {
    "slug": "openais-chair-says-vibe-coding-is-here-to-stay-but-its-not-the-endgame",
    "title": "OpenAI's chair says vibe coding is here to stay — but it's not the endgame",
    "description": "Vibe coding will stick around, but AI agents, not apps, will drive the next big shift in software, says OpenAI's chair Bret Taylor.",
    "fullText": "Vibe coding isn't going anywhere. But it's only part of a much bigger transformation, says OpenAI's board chair.\n\nBret Taylor said in an episode of the \"Big Technology Podcast\" published on Wednesday that using AI tools to build software quickly with natural language prompts will soon feel normal rather than novel. However, focusing on building today's software faster misses the bigger picture.\n\n\"Everyone's looking at all the software use and saying, 'How fast could I vibe code that?'\" Taylor said. \"I wonder if it's the wrong question.\"\n\nWhether someone can quickly vibe code an app in a web browser isn't \"the most interesting question in software,\" he added.\n\nInstead, the software we use today is set to be replaced, and that's the real disruption, Taylor said.\n\nRather than dashboards, web-browser forms, and traditional apps, the structure of software will change. AI agents will be \"the future of software.\"\n\n\"We will delegate tasks to agents that will operate against a database,\" Taylor said.\n\n\"Who's making those agents is the question,\" he added. \"Will you buy those agents off the shelf or build them yourself?\"\n\nTaylor also said that while AI has slashed the cost of building software, it hasn't solved the harder problems of maintaining it — or the risk of getting things wrong.\n\n\"That's why most people would prefer to buy a solution off the shelf,\" he said. \"You want to amortize the cost of maintaining software among thousands of clients.\"\n\nVibe coding has taken off across the tech world, but tech leaders said the technology has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that vibe coding is \"making coding so much more enjoyable,\" adding that it allows even non-technical users to create simple apps and websites.\n\nDuring Alphabet's April earnings call, Pichai said AI generates more than 30% of Google's new code, up from 25% in October 2024.\n\nStill, AI-generated code can be error-prone, overly long, or poorly structured.\n\n\"I'm not working on large codebases where you really have to get it right, the security has to be there,\" Pichai said in November.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding works best for prototypes or throwaway code, but not in software that sits at the core of a business.\n\n\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "vibe code",
      "software",
      "agents",
      "isn't",
      "it's",
      "bigger",
      "episode",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chair-vibe-coding-not-endgame-bret-taylor-2026-1",
    "thumbnail_url": "https://i.insider.com/6883b14a85e81483682eb19e?width=1200&format=jpeg",
    "created_at": "2026-01-29T06:34:22.533Z",
    "topic": "finance"
  },
  {
    "slug": "microsoft-cfos-memo-to-staff-calls-out-ai-deals-coding-and-chips",
    "title": "Microsoft CFO's memo to staff calls out AI deals, coding, and chips",
    "description": "CFO Amy Hood sent an internal memo about the results, viewed by Business Insider.",
    "fullText": "After Microsoft reported results on Wednesday, CFO Amy Hood sent an internal memo to employees calling attention to recent developments in AI chips and coding tools, and deals with OpenAI and Anthropic.\n\nHood sends these emails every quarter when Microsoft discloses its financials. Her missives mostly rehash what the company reports publicly, such as how revenue and profit are growing, or what is discussed on analyst earnings calls.\n\nStill, these memos provide insight into what Microsoft executives deem most important, and what they want employees to know.\n\nThe latest memo highlighted how Microsoft is gaining share in markets where the total addressable market is expanding.\n\nHood specifically mentioned the launch of the GitHub Copilot software development kit in the growing market of AI coding tools, and the release of Microsoft's new Maia 200 AI chip.\n\nHood's email also called out Azure commitments from OpenAI and Anthropic that helped increase commercial bookings, basically the deals Microsoft closed in the quarter, by 230%, year over year.\n\nCapital expenditure on computing and datacenter infrastructure also broke yet another quarterly record, reaching $37.5 billion, she also noted.\n\nThis afternoon, we announced our second-quarter financial results. We exceeded Wall Street expectations, growing revenue 17% and 15% in constant currency and operating income by 21% and 19% in constant currency -a strong finish to the first half of the fiscal year.\n\nIn the quarter, Microsoft Cloud revenue surpassed $50 billion for the first time, growing 26% and 24% in constant currency.\n\nThere were many highlights this quarter, but a few stand out as reminders of the value our products and services deliver to customers - and as proof points of the progress we are making:\n\nInvestors tune in to our earnings call for the full details on this quarter and a look ahead to Q3. It's a helpful way to stay aligned as we deliver on our commitments. Join live today at 2:30 PM Pacific, listen on-demand, or check the transcript on the Investor Relations website.\n\nThis quarter's results reflect meaningful progress on core priorities. We continue to add capacity with pace, drive steady efficiency gains across our fleet, and invest in each layer of the stack\n\nAs we enter the second half of the fiscal year, we're operating in markets with expanding TAM where we continue to gain share and you can see our progress in many places, from last week's announcement of the GitHub Copilot SDK to Monday's Maia 200 announcement. We are innovating and delivering together. And we're doing it with the quality and security our customers expect from us. All of this builds trust from customers and partners as they rely on us for mission critical workloads.\n\nThanks again for all your work.\n\nWith appreciation and gratitude,\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "coding tools",
      "constant currency",
      "quarter",
      "revenue",
      "email",
      "customers",
      "progress",
      "memo",
      "employees",
      "deals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/internal-microsoft-cfo-memo-calls-out-ai-deals-coding-and-chips-2026-1",
    "thumbnail_url": "https://i.insider.com/697a82dfa645d11881883093?width=1200&format=jpeg",
    "created_at": "2026-01-29T01:07:05.127Z",
    "topic": "finance"
  },
  {
    "slug": "programming-sucks-2014",
    "title": "Programming Sucks (2014)",
    "description": null,
    "fullText": "Every friend I have with a job that involves picking up something heavier than a laptop more than twice a week eventually finds a way to slip something like this into conversation: “Bro,1[1] you don’t work hard. I just worked a 4700-hour week digging a tunnel under Mordor with a screwdriver.”\n\nThey have a point. Mordor sucks, and it’s certainly more physically taxing to dig a tunnel than poke at a keyboard unless you’re an ant. But, for the sake of the argument, can we agree that stress and insanity are bad things? Awesome. Welcome to programming.\n\nImagine joining an engineering team. You’re excited and full of ideas, probably just out of school and a world of clean, beautiful designs, awe-inspiring in their aesthetic unity of purpose, economy, and strength. You start by meeting Mary, project leader for a bridge in a major metropolitan area. Mary introduces you to Fred, after you get through the fifteen security checks installed by Dave because Dave had his sweater stolen off his desk once and Never Again. Fred only works with wood, so you ask why he’s involved because this bridge is supposed to allow rush-hour traffic full of cars full of mortal humans to cross a 200-foot drop over rapids. Don’t worry, says Mary, Fred’s going to handle the walkways. What walkways? Well Fred made a good case for walkways and they’re going to add to the bridge’s appeal. Of course, they’ll have to be built without railings, because there’s a strict no railings rule enforced by Phil, who’s not an engineer. Nobody’s sure what Phil does, but it’s definitely full of synergy and has to do with upper management, whom none of the engineers want to deal with so they just let Phil do what he wants. Sara, meanwhile, has found several hemorrhaging-edge paving techniques, and worked them all into the bridge design, so you’ll have to build around each one as the bridge progresses, since each one means different underlying support and safety concerns. Tom and Harry have been working together for years, but have an ongoing feud over whether to use metric or imperial measurements, and it’s become a case of “whoever got to that part of the design first.” This has been such a headache for the people actually screwing things together, they’ve given up and just forced, hammered, or welded their way through the day with whatever parts were handy. Also, the bridge was designed as a suspension bridge, but nobody actually knew how to build a suspension bridge, so they got halfway through it and then just added extra support columns to keep the thing standing, but they left the suspension cables because they’re still sort of holding up parts of the bridge. Nobody knows which parts, but everybody’s pretty sure they’re important parts. After the introductions are made, you are invited to come up with some new ideas, but you don’t have any because you’re a propulsion engineer and don’t know anything about bridges.\n\nWould you drive across this bridge? No. If it somehow got built, everybody involved would be executed. Yet some version of this dynamic wrote every single program you have ever used, banking software, websites, and a ubiquitously used program that was supposed to protect information on the internet but didn’t.\n\nEvery programmer occasionally, when nobody’s home, turns off the lights, pours a glass of scotch, puts on some light German electronica, and opens up a file on their computer. It’s a different file for every programmer. Sometimes they wrote it, sometimes they found it and knew they had to save it. They read over the lines, and weep at their beauty, then the tears turn bitter as they remember the rest of the files and the inevitable collapse of all that is good and true in the world.\n\nThis file is Good Code. It has sensible and consistent names for functions and variables. It’s concise. It doesn’t do anything obviously stupid. It has never had to live in the wild, or answer to a sales team. It does exactly one, mundane, specific thing, and it does it well. It was written by a single person, and never touched by another. It reads like poetry written by someone over thirty.\n\nEvery programmer starts out writing some perfect little snowflake like this. Then they’re told on Friday they need to have six hundred snowflakes written by Tuesday, so they cheat a bit here and there and maybe copy a few snowflakes and try to stick them together or they have to ask a coworker to work on one who melts it and then all the programmers’ snowflakes get dumped together in some inscrutable shape and somebody leans a Picasso on it because nobody wants to see the cat urine soaking into all your broken snowflakes melting in the light of day. Next week, everybody shovels more snow on it to keep the Picasso from falling over.\n\nThere’s a theory that you can cure this by following standards, except there are more “standards” than there are things computers can actually do, and these standards are all variously improved and maligned by the personal preferences of the people coding them, so no collection of code has ever made it into the real world without doing a few dozen identical things a few dozen not even remotely similar ways. The first few weeks of any job are just figuring out how a program works even if you’re familiar with every single language, framework, and standard that’s involved, because standards are unicorns.\n\nI spent a few years growing up with a closet in my bedroom. The closet had an odd design. It looked normal at first, then you walked in to do closet things, and discovered that the wall on your right gave way to an alcove, making for a handy little shelf. Then you looked up, and the wall at the back of the alcove gave way again, into a crawlspace of utter nothingness, where no light could fall and which you immediately identified as the daytime retreat for every ravenous monster you kept at bay with flashlights and stuffed animals each night.\n\nThis is what it is to learn programming. You get to know your useful tools, then you look around, and there are some handy new tools nearby and those tools show you the bottomless horror that was always right next to your bed.\n\nFor example, say you’re an average web developer. You’re familiar with a dozen programming languages, tons of helpful libraries, standards, protocols, what have you. You still have to learn more at the rate of about one a week, and remember to check the hundreds of things you know to see if they’ve been updated or broken and make sure they all still work together and that nobody fixed the bug in one of them that you exploited to do something you thought was really clever one weekend when you were drunk. You’re all up to date, so that’s cool, then everything breaks.\n\n“Double you tee eff?” you say, and start hunting for the problem. You discover that one day, some idiot decided that since another idiot decided that 1/0 should equal infinity, they could just use that as a shorthand for “Infinity” when simplifying their code. Then a non-idiot rightly decided that this was idiotic, which is what the original idiot should have decided, but since he didn’t, the non-idiot decided to be a dick and make this a failing error in his new compiler. Then he decided he wasn’t going to tell anyone that this was an error, because he’s a dick, and now all your snowflakes are urine and you can’t even find the cat.\n\nYou are an expert in all these technologies, and that’s a good thing, because that expertise let you spend only six hours figuring out what went wrong, as opposed to losing your job. You now have one extra little fact to tuck away in the millions of little facts you have to memorize because so many of the programs you depend on are written by dicks and idiots.\n\nAnd that’s just in your own chosen field, which represents such a tiny fraction of all the things there are to know in computer science you might as well never have learned anything at all. Not a single living person knows how everything in your five-year-old MacBook actually works. Why do we tell you to turn it off and on again? Because we don’t have the slightest clue what’s wrong with it, and it’s really easy to induce coma in computers and have their built-in team of automatic doctors try to figure it out for us. The only reason coders’ computers work better than non-coders’ computers is coders know computers are schizophrenic little children with auto-immune diseases and we don’t beat them when they’re bad.\n\nRemember that stuff about crazy people and bad code? The internet is that except it’s literally a billion times worse. Websites that are glorified shopping carts with maybe three dynamic pages are maintained by teams of people around the clock, because the truth is everything is breaking all the time, everywhere, for everyone. Right now someone who works for Facebook is getting tens of thousands of error messages and frantically trying to find the problem before the whole charade collapses. There’s a team at a Google office that hasn’t slept in three days. Somewhere there’s a database programmer surrounded by empty Mountain Dew bottles whose husband thinks she’s dead. And if these people stop, the world burns. Most people don’t even know what sysadmins do, but trust me, if they all took a lunch break at the same time they wouldn’t make it to the deli before you ran out of bullets protecting your canned goods from roving bands of mutants.\n\nYou can’t restart the internet. Trillions of dollars depend on a rickety cobweb of unofficial agreements and “good enough for now” code with comments like “TODO: FIX THIS IT’S A REALLY DANGEROUS HACK BUT I DON’T KNOW WHAT’S WRONG” that were written ten years ago. I haven’t even mentioned the legions of people attacking various parts of the internet for espionage and profit or because they’re bored. Ever heard of 4chan? 4chan might destroy your life and business because they decided they didn’t like you for an afternoon, and we don’t even worry about 4chan because another nuke doesn’t make that much difference in a nuclear winter.\n\nOn the internet, it’s okay to say, “You know, this kind of works some of the time if you’re using the right technology,” and BAM! it’s part of the internet now. Anybody with a couple of hundred dollars and a computer can snag a little bit of the internet and put up whatever awful chunks of hack code they want and then attach their little bit to a bunch of big bits and everything gets a little bit worse. Even the good coders don’t bother to learn the arcane specifications outlined by the organizations people set up to implement some unicorns, so everybody spends half their time coping with the fact that nothing matches anything or makes any sense and might break at any time and we just try to cover it up and hope no one notices.\n\nHere are the secret rules of the internet: five minutes after you open a web browser for the first time, a kid in Russia has your social security number. Did you A computer at the NSA now automatically tracks your physical location for the rest of your life. Sent an email? Your email address just went up on a billboard in Nigeria.\n\nThese things aren’t true because we don’t care and don’t try to stop them, they’re true because everything is broken because there’s no good code and everybody’s just trying to keep it running. That’s your job if you work with the internet: hoping the last thing you wrote is good enough to survive for a few hours so you can eat dinner and catch a nap.\n\nFunny, right? No? How about this exchange:\n\nWasn’t that guy helpful? With the camel? Doesn’t that seem like an appropriate response? No? Good. You can still find Jesus. You have not yet spent so much of your life reading code that you begin to talk in it. The human brain isn’t particularly good at basic logic and now there’s a whole career in doing nothing but really, really complex logic. Vast chains of abstract conditions and requirements have to be picked through to discover things like missing commas. Doing this all day leaves you in a state of mild aphasia as you look at people’s faces while they’re speaking and you don’t know they’ve finished because there’s no semicolon. You immerse yourself in a world of total meaninglessness where all that matters is a little series of numbers went into a giant labyrinth of symbols and a different series of numbers or a picture of a kitten came out the other end.\n\nThe destructive impact on the brain is demonstrated by the programming languages people write. This is a program:\n\nThat program does exactly the same thing as this program:\n\nAnd once somebody wrote a programming language that let somebody else write this:\n\nAccording to the author, that program is \"two lines of code that parse two lines of embedded comments in the code to read the Mayan numbers representing the individual ASCII characters that make up the magazine title, rendered in 90-degree rotated ASCII art.\"\n\nThat program won a contest, because of course it did. Do you want to live in a world like this? No. This is a world where you can smoke a pack a day and nobody even questions it. \"Of course he smokes a pack a day, who wouldn't?\" Eventually every programmer wakes up and before they're fully conscious they see their whole world and every relationship in it as chunks of code, and they trade stories about it as if sleepiness triggering acid trips is a normal thing that happens to people. This is a world where people eschew sex to write a programming language for orangutans. All programmers are forcing their brains to do things brains were never meant to do in a situation they can never make better, ten to fifteen hours a day, five to seven days a week, and every one of them is slowly going mad.\n\nSo no, I’m not required to be able to lift objects weighing up to fifty pounds. I traded that for the opportunity to trim Satan’s pubic hair while he dines out of my open skull so a few bits of the internet will continue to work for a few more days.\n\n(Update: now available in Greek, Czech, Italian, Russian, Portuguese, Hungarian, French, Hebrew (PDF by Ilil Hoz), German (PDF by Kurt Frock), Spanish, and Chinese)",
    "readingTime": 13,
    "keywords": [
      "programming languages",
      "you’re familiar",
      "programming language",
      "suspension bridge",
      "don’t",
      "it’s",
      "internet",
      "they’re",
      "there’s",
      "together"
    ],
    "qualityScore": 1,
    "link": "https://www.stilldrinking.org/programming-sucks",
    "thumbnail_url": "https://www.stilldrinking.org/blog_images/programming-sucks.jpg",
    "created_at": "2026-01-28T06:22:46.410Z",
    "topic": "tech"
  },
  {
    "slug": "pixel-arcade-studio-kids-make-playable-browser-games-by-instructing-ai",
    "title": "Pixel Arcade Studio –kids make playable browser games by instructing AI",
    "description": "Kids create real browser games by giving clear instructions to AI. No coding, no downloads.",
    "fullText": "They need to learn how to give clear instructions to AI.\n\nPixel Arcade Studio is a browser-based game studio where kids create real, playable games by telling an AI assistant exactly what to build. No coding. No installs. Designed with parents in mind.\n\nSafe, creative screen time kids love. Try free for 14 days.\n\nFast \"time-to-wow\" — kids see their games come to life quickly.\n\nFrictionless — everything runs directly in the browser.\n\nSafety default — games publish with privacy protections enabled.\n\nCoding used to be how people told computers what to do.\n\nToday, the more important skill is knowing how to describe what you want, break ideas into steps, and give clear instructions to an AI system.\n\nPixel Arcade Studio is built around that shift.\n\nKids don't write code here. They practice explaining ideas, testing results, and improving their instructions when something doesn't work.\n\nThat's the skill they'll use in the real world.\n\nYour child picks a game template and describes what they want to make. Characters, goals, movement, and rules.\n\nYour child tells the AI assistant what to create or change. The AI follows instructions. It does not take over.\n\nThe game runs right away in the browser. No setup and no waiting.\n\nKids adjust their instructions, test again, and see how clearer directions lead to better results.\n\nGames can be shared with family or friends using parent-approved links.\n\nThis is not about memorizing technical skills. It's about clear thinking and communication.\n\nAI in Pixel Arcade Studio is a tool, not a shortcut.\n\nIt responds only to what your child asks. It does not browse the internet. It does not publish content on its own. It stays inside kid-safe boundaries.\n\nYour child stays in control. The AI helps carry out instructions.\n\nPixel Arcade Studio is built for families who want creative screen time without constant supervision.\n\nInstead, kids focus on giving clear instructions and seeing real results. They make games people can actually play.\n\nEvery game made in Pixel Arcade Studio is playable in the browser and shareable through safe links.\n\nKids don't just save projects. They create something real and playable.\n\nPixel Arcade Studio is a browser-based game studio where kids create playable games by giving instructions to an AI assistant. There is no coding involved.\n\nNo. Pixel Arcade Studio does not teach coding. Kids learn how to clearly describe ideas, give instructions, and work with AI to create games.\n\nPixel Arcade Studio is designed for kids ages 7 to 12.\n\nYes. Games publish in safe mode by default, sharing requires parent approval, and the platform includes content filtering and privacy protections.\n\nNo. Everything runs directly in the web browser. There are no downloads or installs.\n\nThe AI follows your child's instructions. It helps turn ideas into games but does not take control or act on its own.\n\nYes. Games can be shared using parent-approved links so friends and family can play safely.\n\nPixel Arcade Studio does not involve coding or block-based programming. It focuses on teaching kids how to give clear instructions to AI and refine their ideas through iteration.\n\nNo coding. No downloads. Designed for ages 7–12.",
    "readingTime": 3,
    "keywords": [
      "pixel arcade studio",
      "creative screen",
      "privacy protections",
      "parent-approved links",
      "browser-based game",
      "kids don't",
      "games publish",
      "playable games",
      "the ai",
      "kids create"
    ],
    "qualityScore": 1,
    "link": "https://pixelarcade.studio",
    "thumbnail_url": "http://localhost:3000/images/pas_og.jpg",
    "created_at": "2026-01-28T06:22:43.763Z",
    "topic": "tech"
  },
  {
    "slug": "acm-sigplan-symposium-on-principles-of-programming-languages-popl-2026-talks",
    "title": "ACM SIGPLAN Symposium on Principles of Programming Languages (POPL) 2026 talks",
    "description": "Special Interest Group on Programming Languages\nThe ACM Special Interest Group on Programming Languages (SIGPLAN) explores programming language concepts and tools, focusing on design, implementation, practice, and theory. Its members are programming language developers, educators, implementers, researchers, theoreticians, and users.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.youtube.com/channel/UCwG9512Wm7jSS6Iqshz4Dpg",
    "thumbnail_url": "https://yt3.googleusercontent.com/ytc/AIdro_mB1glk2WSnakJ9VmIADfpyj122SV1iL5zw1rMBEQFIqQ=s900-c-k-c0x00ffffff-no-rj",
    "created_at": "2026-01-27T18:24:29.546Z",
    "topic": "tech"
  },
  {
    "slug": "agent-skills-from-claude-to-open-standard-to-your-daily-coding-workflow",
    "title": "Agent Skills: From Claude to Open Standard to Your Daily Coding Workflow",
    "description": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we're witnessing something far more...",
    "fullText": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we’re witnessing something far more significant: an open standard reshaping how people across roles—developers, designers, product managers, and operations—work with AI assistants. AI coding agents’ adoption of Agent Skills has transformed this technology from an interesting experiment into an essential developer tool.\n\nIf you’ve been using custom instructions or wondering how to make your AI assistant truly understand your project’s workflows, Agent Skills provides a compelling solution.\n\nAgent Skills began with Anthropic’s Claude AI, where developers first experienced giving AI agents specialized capabilities through structured instructions. Unlike simple prompts or one-off commands, Agent Skills introduced a sophisticated approach: packaging instructions, scripts, templates, and documentation into reusable, discoverable units.\n\nAnthropic’s decision to release Agent Skills as an open standard transformed it from a Claude-specific feature into a movement. The format’s simplicity and effectiveness attracted attention across the AI development tools ecosystem. Today, major players—Claude Code, GitHub Copilot, Cursor, OpenCode, Mistral Vibe, Antigravity, OpenAI Codex, and Kiro—have adopted the standard. Others are exploring integration, and more are joining it (I’m looking at you, JetBrains Junie).\n\nAgent Skills are elegantly simple: a folder containing a SKILL.md file. This file uses YAML frontmatter for metadata and Markdown for instructions. No complex APIs, no proprietary formats—just structured text any AI agent can understand.\n\nHere’s a basic Agent Skills example for creating NUnit unit tests in C#:\n\nThose Agent Skills files live in your agent configuration, for example for GitHub Copilot .github/skills/create-nunit-unit-test/SKILL.md in your repository. Or they can be globally installed for your user account, e.g. ~/.copilot/skills/create-nunit-unit-test/SKILL.md.\n\nThis is a minimal example. You can add resources like templates, example configurations, or helper scripts in the same directory, and the skill can reference them.\n\nWhat makes Agent Skills innovative isn’t just the format—it’s how AI agents consume them. The system uses a three-level progressive disclosure approach that optimizes context window usage:\n\nLevel 1: Discovery — At startup, the agent reads only the name and description from each skill. This lightweight metadata helps the agent understand available capabilities without consuming context.\n\nLevel 2: Activation — When your request matches a skill’s description, the agent loads the full instructions from the SKILL.md file. Only then do detailed procedures become available.\n\nLevel 3: Execution — The agent accesses additional files (scripts, examples, documentation) only as needed during execution.\n\nThis architecture solves a critical problem: you can install dozens of Agent Skills without overwhelming the AI’s context window. The agent loads only what’s relevant to your current development task.\n\nGitHub Copilot’s Agent Skills are experimental since December 2025 (version 1.108) for VS Code. Here’s your step-by-step setup guide:\n\nInstall VS Code — Download from code.visualstudio.com\n\nEnable Agent Skills — Open settings (Ctrl+,) and enable chat.useAgentSkills\n\nCreate your skills directory — In your project root, create .github/skills/\n\nAdd your first skill — Create a subdirectory for each skill with its SKILL.md file\n\nUse Agent Mode — In Copilot Chat, switch to Agent mode to leverage skills\n\nOnce configured, Agent Skills activate automatically based on your prompts. No manual selection required—the AI determines which skills are relevant based on your descriptions.\n\nSkills share knowledge—best practices, workflows, and procedural guidance—in simple Markdown SKILL.md files that anyone can author; they load progressively to conserve tokens, require no server, and run on web, desktop, and CLI, making them ideal for documentation, checklists, examples, and repeatable workflows.\n\nMCP extends functionality by connecting to APIs, databases, and external tools: it consists of code and service/tool definitions that require development and hosting, loads tool definitions up front (consuming more context), so it’s best suited for tasks needing direct access to external systems.\n\nUse Skills to make knowledge discoverable and consistent, and use MCP to perform integrated actions and extend platform capabilities; together they provide both lightweight guidance and powerful automation.\n\nNevertheless, I can imagine a future where Agent Skills replace MCP for many scenarios, given their simplicity, portability, and ease of authoring. As you can bundle scripts and resources with skills, they can cover many use cases MCP currently serves.\n\nYou might wonder how Agent Skills differ from the custom instructions feature. Custom Instructions are best for defining coding standards and conventions, setting language or framework preferences, specifying code-review guidelines, and establishing commit-message formats. Agent Skills are designed to package reusable workflows, include executable scripts and templates, define specialized procedures (testing, debugging, deployment), and enable capabilities that run beyond the IDE (CLI and coding agents).\n\nThink of custom instructions as your coding style guide and Agent Skills as your AI development toolbox. Custom instructions tell the AI how you want code written; Agent Skills give the AI specialized capabilities to perform complex development tasks.\n\nHere are some practical Agent Skills that can transform your daily development workflow. Check the references section for pointers to more ready-to-use skills:\n\nRead the Agent Skills Specification to understand the format and capabilities. Use Skill Creator, an Agent Skill to create and refine new Agent Skills. Inception moment anyone 🤔?\n\nStart building your Agent Skills collection with these proven strategies:\n\nIdentify Repetitive Tasks — Notice which development workflows you explain to the AI repeatedly. Each recurring explanation is a candidate for an Agent Skill.\n\nStart Simple — Begin with straightforward skills that codify standard development procedures. As you gain confidence, add scripts and more complex resources.\n\nMake Descriptions Specific — The quality of your skill’s description directly impacts how well the AI knows when to activate it. Be explicit about use cases and capabilities.\n\nInclude Examples — Agent Skills with concrete code examples are more effective. Show the AI what good output looks like.\n\nLeverage Community Skills — Explore the github/awesome-copilot and anthropics/skills repositories for inspiration and ready-to-use skills.\n\nOrganize by Domain — Group related Agent Skills together. Create separate skills for testing, deployment, documentation, code review, and other specialized development domains.\n\nHere’s how Agent Skills enhance your workflow in a typical development scenario:\n\nYou’re working on a web application and need to add a new REST API endpoint with proper testing and documentation. With appropriate Agent Skills in place:\n\nYou ask: “Help me add a new user registration endpoint with validation”\n\nThe rest-api-integration skill activates, providing structured guidance on implementing the endpoint with proper authentication, validation, and error handling.\n\nYou ask: “Create tests for this endpoint”\n\nThe webapp-testing skill engages, generating test cases for success scenarios, validation failures, and edge cases.\n\nYou ask: “Generate documentation for this endpoint”\n\nThe api-documentation skill activates, producing comprehensive documentation with examples, error codes, and authentication details.\n\nEach Agent Skill ensures consistency in approach and completeness in implementation. Without skills, you’d need to provide detailed instructions for each request or rely on the AI’s general knowledge, which might miss project-specific patterns.\n\nWhen working with Agent Skills, especially community-shared skills, keep these security considerations in mind:\n\nReview Before Use — Always examine shared Agent Skills before adding them to your project. Check for potentially malicious scripts or unexpected behaviors in the SKILL.md file and associated resources.\n\nUse Terminal Controls — VS Code’s terminal tool provides controls for script execution, including auto-approve options with configurable allow-lists. Configure these appropriately for your security requirements.\n\nVersion Control Your Skills — Agent Skills are just files, so commit them to your repository. This enables code review, versioning, and team collaboration on AI capabilities.\n\nTest in Safe Environments — Try new Agent Skills in development environments before using them in production contexts. Dev containers or isolated workspaces are ideal for testing.\n\nDocument Team Skills — If your team uses shared Agent Skills, maintain documentation about what each skill does and when to use it.\n\nAgent Skills represent more than a new feature—it’s a glimpse into a future where AI development capabilities are portable, shareable, and composable. As more AI tools adopt the standard, we’re moving toward an ecosystem where:\n\nWhether you’re using VS Code or any other editor/IDE, working in the terminal with Copilot CLI, or leveraging any coding agent for automated tasks, your Agent Skills come with you.\n\nReady to integrate Agent Skills into your development workflow? Follow this action plan:\n\nThe goal isn’t to create dozens of Agent Skills immediately. Start with one or two that solve real problems in your development workflow, then expand your library organically as needs arise.\n\nYou can also use Agent Skills with GitHub Copilot CLI or Gemini CLI for terminal-based workflows, or with other coding agents that support the open standard. This portability ensures your investment in creating skills pays off across all your AI-assisted development tools.\n\nMy preferred introduction to Agent Skills is the following video from Burke Holland, which covers the concepts, setup, and practical examples in under 20 minutes:\n\nFor my French readers, I discussed Agent Skills in depth on devdevdev.net in the following episode\n\nAgent Skills bridges the gap between generic AI assistance and specialized, context-aware support for your specific development needs. By adopting an open standard that works across AI tools, the industry has created a foundation for truly portable AI capabilities.\n\nThe journey from Claude to open standard to GitHub Copilot adoption demonstrates the power of simplicity and interoperability in developer tools. As developers, we benefit from this ecosystem approach—our investment in creating Agent Skills pays dividends across our entire development toolchain.\n\nStart small, experiment with the format, and build Agent Skills that improve your daily development work. The progressive disclosure system ensures you won’t overwhelm your AI assistant, and the portable format guarantees your skills remain valuable as AI tools evolve.\n\nThe future of AI-assisted development isn’t just about more powerful models—it’s about giving those models the right context, capabilities, and knowledge to be genuinely helpful in your specific development domain. Agent Skills is a significant step in that direction.\n\nWhat development workflows could benefit from specialized Agent Skills? Have you tried creating skills for your AI coding assistant? Share your experiences in the comments below.",
    "readingTime": 9,
    "keywords": [
      "agent skills",
      "skill.md file",
      "progressive disclosure",
      "ai-assisted development",
      "skill’s description",
      "context window",
      "agent mode",
      "shared agent",
      "coding agents",
      "skill activates"
    ],
    "qualityScore": 1,
    "link": "https://laurentkempe.com/2026/01/27/Agent-Skills-From-Claude-to-Open-Standard/",
    "thumbnail_url": "https://live.staticflickr.com/65535/55058424290_cced09531e_h.jpg",
    "created_at": "2026-01-27T12:26:46.640Z",
    "topic": "tech"
  },
  {
    "slug": "one-developer-used-claude-to-build-a-memorysafe-extension-of-c",
    "title": "One developer used Claude to build a memory-safe extension of C",
    "description": "feature: Robin Rowe talks about coding, programming education, and China in the age of AI",
    "fullText": "feature TrapC, a memory-safe version of the C programming language, is almost ready for testing.\n\n\"We're almost there,\" Robin Rowe told The Register in a phone interview. \"It almost works.\"\n\nWe caught up with Rowe, a computer science professor and entrepreneur, amid debugging efforts that had kept him up until four in the morning. The long-awaited TrapC website has appeared.\n\n\"My work building TrapC has taken two parallel paths,\" Rowe explains in his initial post. \"A TrapC interpreter called itrapc and a separate compiler called trapc. I had wanted to make a software release by 1 January 2026, but too many bugs. I only reached code complete this month and am now on the painstaking and sleepless process of debugging. When I have something stable that mostly works I will make a release. Sorry to make you wait a little longer. Aiming for Q1 2026.\"\n\nBack in November 2024, Rowe explained that he was working on TrapC. At the time, the public and private sector had undertaken a campaign to promote memory-safe software development as a way to reduce exposure to serious vulnerabilities.\n\nMemory safety provides a way of ensuring that memory-related bugs like out-of-bounds reads/writes and use-after-free don't happen. In large codebases, like Chromium and Windows, most of the security vulnerabilities follow from memory bugs. As that message has been repeated in recent years, memory safety has become an imperative, evangelized by the likes of Google and Microsoft, and more recently by authorities in the US and elsewhere.\n\nFor at least the past ten years, there's been a rising chorus of voices calling for the adoption of memory-safe programming languages and techniques. This has meant encouraging developers to avoid languages like C and C++ where feasible, and to adopt languages like C#, Go, Java, Python, Swift, and Rust, instead, particularly for new projects.\n\nTo remain relevant, the C and C++ communities have tried to address those concerns with projects like TrapC, FilC, Mini-C, Safe C++, and C++ Profiles. There's also a C to Rust conversion project under development at DARPA called TRACTOR – TRanslating All C TO Rust.\n\nBut progress has been slow and those writing in C and C++ haven't found a widely accepted approach. The C++ standards committee recently rejected the Safe C++ proposal. And Rowe said he doubted TRACTOR would have anything to show this year.\n\nMeanwhile, the clock is ticking. Microsoft engineer Galen Hunt last month said, \"My goal is to eliminate every line of C and C++ from Microsoft by 2030. Our strategy is to combine AI and algorithms to rewrite Microsoft's largest codebases.\"\n\n\"There are some efforts to port C code by hand to Rust,\" said Rowe. \"But there're some real challenges to doing that because there are some idioms in C that cannot be expressed in Rust.\n\n\"Rust is much more type safe than C is. And so if you have a void pointer, what does that mean in Rust? There's no translation for it. And that's how TrapC is fundamentally different because it actually remembers what that void pointer actually is.\"\n\nRowe said he expects TRACTOR will eventually be able to accomplish C to Rust translation using AI. But he said he thinks it's better to just build the necessary tooling into the C compiler, so you don't have to rely on some external tool that rewrites your code in an unfamiliar language.\n\nRowe has been using AI tools himself and has been teaching others to do so. This past semester, he taught AI Cybersecurity Programmer Analyst (PCO471) at Community College of Baltimore County – Linux administration using vibe coding in bash with no prerequisites. And starting in February, he's teaching C++ Programming with Generative AI (PCO472) – vibe coding in C++.\n\nRowe said programming has fundamentally changed as a result of AI tools. \"I think this is sort of the same type of discussion as when C came in and people said, 'Well, I'm happy in assembly.' There will still be people doing it the old way. But because vibe programming is so much more efficient on time when done correctly, there's gonna be no choice. You just won't be competitive if you're not vibe programming.\"\n\nThen he shifted gears, slightly. \"But I have to walk that back a little bit because the reason I was up until four in the morning is I had vibe programming working on the Trap C compiler. And it took a fundamentally wrong design turn. And I didn't detect that it had made a design mistake. I had told it how I wanted to approach it. But somehow it misunderstood me or it forgot or something happened and I forgot to check. And so I spent hours doodling around in the debugger and trying to understand why code was acting weird before I finally looked at it and said, 'wait a minute, this isn't even the right design.'\"\n\nRowe said a similar situation crops up in pair programming, where you've told someone to do something and they didn't do it, and you don't realize that until later.\n\n\"[C++ creator] Bjarne Stroustrup famously said that the most important thing in software design is to be clear about what you're trying to build,\" Rowe said. \"And vibe [programming] just puts that on steroids. Now we not only have to be ourselves clear, but we have to communicate it clearly to an LLM.\"\n\nRowe argues that developers have to be encouraged to try AI tools and to make mistakes. He recounted how during his AI Cybersecurity Programmer Analyst course, his students expressed interest in doing more hands-on work in lieu of lectures.\n\n\"So I said, 'I've got real servers on the internet that are my companies. I'll give you root,'\" he recalled. \"I'll set loose students who know nothing on my own servers and hope for the best and we'll see how this goes. And the reaction was panic. I couldn't get past the timidity cliff.\"\n\nRowe said that what he learned from that exchange was that they didn't want their own hands-on, they wanted to watch him work.\n\n\"I said to them, 'But guys, this is like learning to play the piano. You can't learn to play the piano by watching me. Yeah, you guys have to practice. And it's gonna be embarrassing at first. You know, you're gonna play a lot of bad notes and sound terrible. You have to get over that situation'.\"\n\nThat's a scenario playing out in various companies where AI tools remain underutilized, for various reasons, including lack of training, security concerns, lack of utility, and poor tool design.\n\nRowe has traveled often to China to speak at the China Association of Higher Education conference. In December, he said, he was interviewed on China News Television about how China's plan for AI compares with America's.\n\nIn an email he explained, \"I said, 'China's AI-Plus plan calls for efficient AI on devices everywhere, from farm to factory to city, while the White House plan calls for building 500-billion-dollar cloud data centers ... using chips that will, inevitably, seem obsolete within two years.'\"\n\nRowe argues China's approach will prevail and that the US has taken the wrong turn by focusing on centralized cloud datacenters to run LLMs. Within two years, he said, we'll have AI models we can run locally on our phones, with no need for network access for most tasks. Apple and Huawei, he said, are likely to be the winners in this scenario.\n\nRowe pointed to China's DeepSeek as an example. While it may not be quite as good as the leading US commercial models, he said, it runs with far less power.\n\n\"This is a very Moore's Law type of strategy,\" he said. \"I remember when I had a Navy supercomputer in 1994. That was an amazing technology. But in 1995, Cray went bankrupt. There weren't enough buyers for it, even though it was an amazing device.\n\n\"And now I've got an iPhone that's in my pocket. That runs on a battery. It doesn't have a whole room devoted to it and exotic cooling and all kinds of stuff. And it's more powerful than that [the Cray from 1994]. So as a long-term strategy, you know, going toward the device makes a lot more sense, because that half-trillion dollar data center is going to be on my iPhone eventually.\"\n\nRowe also said that on the recommendation of a friend from his time at the AT&T DIRECTV Innovation Lab, he tried running Deepseek at a time when Claude wasn't available. Deepseek, he said, was able to find a bug that Claude couldn't.\n\n\"Surprisingly, the bug was in code Claude had generated, that I had cut-and-pasted carelessly,\" he said. \"With hindsight it was a silly code mistake I should have caught, but was in an 'else' branch outside where I was looking. I'd not expected or intended to have Claude make any change to that block of code. And because the code was valid but the logic wrong, the compiler didn't catch it.\"\n\nBut the bug was obvious, he said, as soon as Deepseek pointed it out. He added, \"I'm paying $200/year for Claude. Deepseek is free.\" ®",
    "readingTime": 8,
    "keywords": [
      "cybersecurity programmer",
      "programmer analyst",
      "rowe argues",
      "void pointer",
      "memory safety",
      "vibe coding",
      "design rowe",
      "vibe programming",
      "code",
      "compiler"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/26/trapc_claude_c_memory_safe_robin_rowe/",
    "thumbnail_url": "https://regmedia.co.uk/2022/03/23/shutterstock_c.jpg",
    "created_at": "2026-01-27T06:21:27.835Z",
    "topic": "tech"
  },
  {
    "slug": "a-developer-teamed-up-with-claude-to-create-elo-programming-language",
    "title": "A developer teamed up with Claude to create Elo programming language",
    "description": "feature: Bernard Lambeau, the human half of a pair programming team, explains how he's using AI",
    "fullText": "feature Bernard Lambeau, a Belgium-based software developer and founder of several technology companies, created a programming language called Elo with the help of Anthropic's Claude Code.\n\nStarting on December 25, 2025, he published a series of posts about the project. The first post names Claude as a co-author.\n\n\"In roughly 24 hours of collaboration, we built a complete expression language with a parser, type system, three compilers, a standard library, a CLI tool, and a documentation website. Not bad for a day's work,” Lambeau and Claude wrote.\n\n\"Elo isn't just a demonstration that AI can write code. It's a demonstration that humans and AI can build together – each contributing what they do best,” they added.\n\nAs an expression language that compiles to JavaScript, Ruby, and SQL, Elo is intended as a portable way to handle form validation, e-commerce order processing, and subscription logic.\n\nLambeau, founder and CTO of Klaro Cards and CEO of app consultancy Enspirit, is not the first to develop a programming language with the help of AI.\n\nSteve Klabnik performed a similar feat last year with the Rue programming language. In September 2025, Geoffrey Huntley enlisted Claude to write a programming language called Cursed. And before that, Avital Tamir published a Claude-authored repo for the Server programming language, with the caveat that the code is not intended for actual use.\n\nClaude Code isn't the only AI-assisted programming method having a moment. AI biz Cursor created a rudimentary browser using OpenAI's GPT-5.2. And developer Ola Prøis used Cursor, powered by Claude, to create a Rust-based text editor called Ferrite.\n\nClaude users generally acknowledge that their pair partner makes mistakes. But those committed to AI assistance find it worthwhile to clean up after their helper.\n\n\"Claude Code knows almost every tech stack (and can search the web), knows the Linux commands that matter (search code, search & replace, compile, test, etc.), and does that 10x faster than I can do myself,\" Lambeau told The Register in an email interview.\n\nClaude, he said, allows him to use technology he hasn't mastered.\n\n\"I was already a full-stack developer (on languages, frameworks & reusable libraries I knew); I'm now a full-stack++ dev because I can also use languages, frameworks, and reusable libraries I barely know, if at all,\" he explained.\n\n\"Claude Code falls short if you don't have a great methodology. It needs feedback loops to work fine; otherwise, it derails. One possible feedback loop is a human reviewing code and testing manually. But there's a better/complementary approach if you want it to work autonomously. On both Elo and Bmg.js, I've started by making sure the testing methodology was effective and scientifically sound. Claude writes the tests, executes them, discovers where it's wrong, and corrects itself. Impressive.\"\n\nLambeau said he still needs to review some of Claude's output.\n\n\"But if I read the tests, agree with them, and can check myself that they run fine, I'm 95 percent sure it's already correct as a black box (not even reading the code),\" he explained. \"Then I can check the architecture and code quality as a white box by having a general look at the code, but I don't have to understand every detail.\"\n\nNotably, Lambeau documented the series of prompts he used to create the language. The repo includes more than 100 tasks used to direct the AI model. In addition, Lambeau has published a video that describes his AI pair programming process.\n\n\"I started in a setting where Claude Code asked for permissions every 20 seconds and I was checking everything it did,\" Lambeau explained. \"After a few successes, I quickly set up safe environments to be able to let Claude Code run in full autonomy (isolated computer & isolated Linux user, or running in a Docker image).\"\n\nLambeau said he still uses plan mode for complex tasks that require conversation with Claude.\n\n\"I review the plan, make sure we have a test strategy that's sound, then switch Claude to autonomous mode and look at the tests, code & results afterward,\" he said. \"That's very similar to a lead-dev/CTO + QA role, btw; it's just much faster than with human devs.\"\n\nLambeau, who has a PhD in software engineering and 30 years of experience as a developer, said both experts and novices can benefit from Claude Code, though he added that a service like Lovable might be more approachable for those not already acclimated to the command line.\n\n\"Now, when it comes to real software/product engineering, I think Claude Code requires experts (so far),\" he said. \"You still need to guide it a lot to keep the quality high enough. You need very strong expertise to do it effectively. Currently (Claude will still improve a lot), if you don't have the expertise, you certainly end up with a big mess of unmaintainable code.\"\n\nMany developers have said as much about AI tools. They're more useful as an amplifier of expertise than as a replacement for it. The situation is analogous to the introduction of sequencing software, digital synthesizers, and drum machines half a century ago. These tools enabled a lot of people who weren't great musicians to make music. But they didn't instill musical skill, and they produced the most interesting work in the hands of practiced musicians.\n\nThe cost to do this, Lambeau said, has been a Claude Max subscription that he purchased in December for €180 a month. In that time, he says, he wrote Elo (https://elo-lang.org), completed Bmg.js (https://github.com/enspirit/bmg.js), completed Bmg's documentation (https://www.relational-algebra.dev), and created the first version of the Try page (https://www.relational-algebra.dev/try).\n\n\"It's all personal research and open-source projects,\" he said. \"It would have required several weeks to do the same manually myself, and several months to ask another developer to do it. The cost would be mostly because of the scientific & technical knowledge transfer about the data language I envision. Strangely enough, it's very cheap with Claude Code. There's something true about the fact that those LLMs have a PhD.\"\n\nLambeau explained that Elo isn't just a way to test Claude Code. He also sees it as an extension of his academic work in software engineering and his personal interest in the Relational Model – he's served as a lecturer for database courses at Belgium’s UCLouvain.\n\n\"I'm absolutely convinced that we need better/safer/simpler programming languages inside no-code tools and when interconnecting them (e.g. Zapier, Make, n8n, etc.),\" he said. \"Mainstream programming languages are very complex, error-prone, sometimes dangerous, and the programs are difficult to review for non-experts.\"\n\n\"More importantly, they are cumbersome to use for even simple data tasks. I mean, even validating the schema and constraints of a data file at runtime tends to be a nightmare in existing languages. It's not built-in in any mainstream language; you immediately need validation libraries; most of them are limited in what they can easily check, so you need to add dedicated boilerplate code.\"\n\nIn a world where non-technical people will have the opportunity to write untrustworthy code with the help of AI, he said, we need to be able to run that code safely.\n\n\"Elo aims at providing a safe & simple alternative,\" he said. \"It will be a limited language (non-Turing-complete, as we say) but super safe & simple, and usable in 80 percent of common data use cases. The very first no-code tool to integrate it will be Klaro Cards, of course.\" ®",
    "readingTime": 7,
    "keywords": [
      "claude code",
      "elo isn't",
      "reusable libraries",
      "safe simple",
      "languages frameworks",
      "software engineering",
      "expression language",
      "programming language",
      "it's",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/24/human_ai_pair_programming_elo/",
    "thumbnail_url": "https://regmedia.co.uk/2025/11/06/shutterstock_balancing_ai_and_humanity.jpg",
    "created_at": "2026-01-26T01:03:11.981Z",
    "topic": "tech"
  },
  {
    "slug": "5-acquisitions-winning-over-skeptical-engineers-and-spending-tens-of-millions-inside-a-public-companys-ai-native-push",
    "title": "5 acquisitions, winning over skeptical engineers, and spending tens of millions: Inside a public company's 'AI native' push",
    "description": "Amplitude gave Business Insider the inside look at its AI overhaul, from acquisitions to efforts to increase staff adoption of AI coding assistants.",
    "fullText": "There's a long banner hanging in Amplitude's San Francisco office. It reads: \"NO MAGICAL THINKING.\"\n\nNo, it's not some rag on Joan Didion. It's a reminder, CEO Spenser Skates told Business Insider, that technology can never replace deep thinking and hard work. In the AI age, that reminder is more important than ever — so much so that employees must look up at it every day.\n\nAmplitude, an 800-person, publicly traded analytics company, is undergoing an AI transformation — with the goal of reinvigorating its business.\n\nAmplitude went public in September 2021 at the height of the pandemic, climbing to an all-time closing high of $84.80 per share several weeks later before dropping significantly and largely plateauing in in recent years around $10. It closed at $10.25 on Friday.\n\nSince October 2024, the company has acquired five AI startups. Amplitude hired an AI-savvy engineering head and appointed one of its acquired founders to a new AI leadership position. It got Cursor and GitHub Copilot licenses for employees, and ran a heads-down AI week.\n\nIt's a change many companies are making: Rapidly moving from little-to-no AI to trying to become \"AI native,\" a term that's curiously hard to pin down. Large language models are popping up everywhere in white-collar work as companies chase the promise of efficiency gains.\n\nAmplitude's case may be especially informative, given just how skeptical of AI its CEO was. In 2023 and some of 2024, Skates said he viewed the AI industry as full of \"grifters,\" the visionaries promising to end world hunger and salesmen promising to automate everything.\n\n\"It had all sorts of problems,\" Skates said. By mid-2024, he realized \"there's probably going to be a breakthrough in the analytics space in the next two or three years.\"\n\n\"We've got to go make that ourselves,\" he said. \"So, we went all in.\"\n\nSkates had two opening moves for his AI overhaul.\n\nThe first: hiring a new chief engineering officer with a history in AI. Wade Chambers had advised the company since 2016, while holding leadership roles at Twitter and Included Health.\n\nWhen Chambers joined in October 2024, only 1% of the engineering, product, and design teams at Amplitude were using AI.\n\nThe second was the acquisition of Command AI, a chatbot startup. It was the first of a string of acquisitions, including June, Kraftful, and Inari. Amplitude announced its most recent acquisition, InfiniGrow, on January 14.\n\nYana Welinder was CEO of Kraftful, one of Amplitude's acquisition targets. Kraftful could spot power users of its product, one of whom was Amplitude's then-CPO. She reached out, and they chatted in February. The deal closed in July, and Welinder was named Amplitude's head of AI. A company blog post with an introductory Q&A referred to her as \"AI maven.\"\n\nWelinder's first order of business: speeding the company up. Kraftful shipped new product every week. Amplitude was shipping less than monthly.\n\n\"If you have this cadence of shipping infrequently, then the team slows down, which isn't appropriate in the age of AI,\" she said.\n\n\"Analytics will look very different 6 months from now,\" Skates wrote in his email. \"We have the opportunity to be the AI native company in Analytics and we are going to pull every piece of firepower we have.\"\n\nHe also asked employees to share a coming launch on X, as opposed to LinkedIn, because that's \"where the AI natives are.\"\n\nHow much has Amplitude spent on AI, from tools to acquisitions? \"Tens of millions, for sure,\" Skates said. \"I wouldn't be surprised if it got past $100 million.\"\n\nThen comes the harder part: convincing employees to really use the tools.\n\nWhile some engineers are excited about AI's promise, others are skeptical about its helpfulness, or worried about possible job losses. Not every engineer is as gung ho about AI as their management is.\n\nSkates said that engineers were especially sensitive to the \"grifting\" that went on in AI, making many of them skeptical. With a bottoms-up approach, that skepticism dissipates, he said.\n\nSoon after joining, Chambers began planning an \"AI week\" for the first week of June. It took six months of prep and borrowed heavily from Facebook's mobile push. He took the entire engineering, product, and design team offline for the week. To kick off, Chambers required that leaders get onstage and vibe-code something in front of the entire company.\n\n\"It didn't go well,\" Chambers said of the live vibe-coding demonstration. \"They had to work through it. They had to re-prompt a couple of different ways.\"\n\nBut the message stuck, he said. Leaders who weren't coding all day were able to build something \"pretty cool\" within the hourlong session, save a few hiccups.\n\nAdditional momentum came from the \"zealots,\" engineers passionate about exploring the new tech (some of whom Chambers brought over from his prior job). These engineers lead by example, he said.\n\nAmplitude shared its internal data tracking how many employees use its AI tools. In the final week of March, 14 employees were actively using Cursor. That figure peaked in the first week of December — after AI week but before the holiday vacation cycle — at 174 employees.\n\nAnd what of the thorny question about AI implementation in the enterprise: ROI? After all, a 2025 MIT study indicated 95% of firms publicly disclosing use of AI pilots reported no measurable ROI.\n\nAfter implementing these tools, developer productivity shot up 40% and stayed there, Chambers said. On some specific engineering teams, those gains looks more like 300-400%, he said.\n\n\"There's going to be a lot of people who are thinking they're the world's best expert at something,\" Chambers said. \"Increasingly, even the most cynical team members have come around.\"",
    "readingTime": 5,
    "keywords": [
      "engineering product",
      "roi after",
      "employees",
      "analytics",
      "tools",
      "engineers",
      "there's",
      "skeptical",
      "acquisition",
      "team"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amplitude-ai-native-push-2026-1",
    "thumbnail_url": "https://i.insider.com/69729d28d3c7faef0eccc73a?width=1200&format=jpeg",
    "created_at": "2026-01-25T12:22:41.686Z",
    "topic": "finance"
  },
  {
    "slug": "skget-another-cli-to-add-skills-to-your-coding-agents",
    "title": "Skget, another CLI to add skills to your coding agents",
    "description": "A CLI to add skills to your coding agents. Contribute to czheo/skget development by creating an account on GitHub.",
    "fullText": "czheo\n\n /\n\n skget\n\n Public\n\n A CLI to add skills to your coding agents.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n czheo/skget",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/czheo/skget",
    "thumbnail_url": "https://opengraph.githubassets.com/ec41d6a0818a69873753057252cfb3dd2acf4c9944a55eb817aea02d4c36721f/czheo/skget",
    "created_at": "2026-01-25T01:04:24.594Z",
    "topic": "tech"
  },
  {
    "slug": "five-ways-people-are-using-claude-code",
    "title": "Five Ways People Are Using Claude Code",
    "description": "Claude Code generates computer code when people type prompts, so those with no coding experience can create their own programs and apps.",
    "fullText": "Claude Code, an artificial intelligence tool that can generate computer code when people type a prompt, is having a viral moment.\n\nThe tool, which the A.I. start-up Anthropic introduced in May, has shown record growth over the past two weeks, the company said, without sharing its data. People had time to experiment with Claude Code over the holidays, Anthropic said, and users realized how capable it was.\n\nClaude Code is one of several A.I. coding tools — which also include Base44 and Cursor — that people with no coding experience are increasingly using to build their own websites, programs and apps, a trend known as “vibecoding.” People pay a subscription fee of $20 to $200 a month to use Claude Code, depending on the features they want.\n\nHere are five ways that people are using Claude Code:\n\nMr. Hindes, an assistant principal at a school for autistic children, has four children under the age of 9 and turned to A.I. to help him organize his family’s laundry.\n\nLast week, he prompted Claude Code to make a program to identify which clothes belonged to each of his three daughters so he could sort clean laundry into piles without their help. He took pictures of their clothes to teach Claude Code which T-shirt belonged to which daughter. Now he simply holds up the clothes to his laptop camera so the program tells him whom it belongs to.\n\n“The whole process was done within an hour, and the girls were really excited,” he said.\n\nMr. Hindes said he was now building a program with Claude Code to help his daughters independently work though the steps of their morning routine, as if playing a game.\n\n“I’ve tried to teach myself coding at various points but never stuck with it,” he said.\n\nMr. Stephenson, an art and architecture photographer, started using Claude Code in November to build a website about a documentary feature.\n\nThe website was created in about a day, he said, with an interactive map of New York City that captured his photos and audio recordings to document life in each borough.\n\n“Once the basic site was done, emboldened by my new capabilities, I started adding features I hadn’t even considered,” said Mr. Stephenson, who pays $20 a month for Claude Code. “Light/dark mode? Easy. Shuffle button? Done.”\n\nIf Claude Code could not solve a particular problem, he turned to Google’s A.I. chatbot, Gemini, to ask how it would approach the issue.\n\n“I’d envisioned something like this when I started a couple of years ago, but assumed it would cost thousands of dollars to build,” he said.\n\nMr. Roberts, an assistant prosecuting attorney, used Claude Code and Cursor in August to create a mobile app called AlertAssist, which lets users send a mass text to contacts in an emergency. Working in law enforcement got Mr. Roberts interested in trying to help people act quickly and safely in an emergency.\n\nThe design and user interface of the app are “very basic, but it works,” he said.\n\nDuring the coronavirus pandemic, Ms. Haubo Dyhrberg, an assistant professor of finance at the University of Delaware, had an idea to make a stock trading simulator for her class. She consulted her husband, a software engineer, but “the task seemed too daunting.”\n\nOn Monday, she downloaded Claude Code and within two hours had a working demo of a trading simulator that her students could use to trade securities in a mock market. She has built five different trading scenarios for students to explore various challenges in financial markets.\n\n“I never thought it would be this easy,” she said. “I can’t wait to test it out when the semester starts in two weeks.”\n\nMr. Bacus, who owns a welding and metal fabrication business, tapped Claude Code last month to create an A.I. assistant to manage his calendar and find him new business opportunities. The business is just him and three others, so “we’re not in the place right now to afford an office team,” Mr. Bacus said. “It’s all on me.”\n\nWith Claude Code, he built a personal A.I. assistant that connects to his calendar, Google Sheets and Gmail account so he can easily create estimates, track the progress of jobs and organize contracts.\n\n“I’m a skilled laborer who barely passed high school in the early 2000s,” Mr. Bacus said, adding: “But over the past few months, I’ve taught myself to build actual tools for my business.”",
    "readingTime": 4,
    "keywords": [
      "claude code",
      "a.i assistant",
      "trading simulator",
      "mr bacus",
      "mr stephenson",
      "mr roberts",
      "business",
      "coding",
      "program",
      "clothes"
    ],
    "qualityScore": 1,
    "link": "https://www.nytimes.com/2026/01/23/technology/claude-code.html",
    "thumbnail_url": "https://static01.nyt.com/images/2026/01/21/multimedia/CLAUDE-CODE-1-pztw/CLAUDE-CODE-1-pztw-facebookJumbo.jpg",
    "created_at": "2026-01-24T00:56:46.566Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-kills-open-source",
    "title": "Vibe Coding Kills Open Source",
    "description": "Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.15494",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-23T06:20:32.202Z",
    "topic": "tech"
  },
  {
    "slug": "gemini-cli-code-and-create-with-an-opensource-agent",
    "title": "Gemini CLI: Code and Create with an Open-Source Agent",
    "description": "Build real-world applications from the command line using Gemini CLI, Google's open-source agentic coding assistant that coordinates local tools and cloud services to automate coding and creative workflows.",
    "fullText": "Join this short course on Gemini CLI, taught by Jack Wotherspoon, Developer Advocate at Google.\n\nGemini CLI is an open-source agentic coding assistant that works from your terminal, giving it access to your local filesystem, development tools, and cloud services. This lets you delegate complex workflows—from building web features to creating marketing materials—through high-level instructions while the agent autonomously plans and executes multiple steps.\n\nIn this course, you’ll apply Gemini CLI to software development and creative tasks by building features for an AI conference. You’ll develop a website session catalog, create a data dashboard combining local and cloud data sources, and generate social media content from recordings. You’ll master context management, integrate MCP servers, and orchestrate across multiple services with Gemini CLI extensions.\n\nWhether you’re prototyping applications, automating development workflows, or studying topics in agentic AI, this course gives you hands-on experience coordinating multiple tools to build faster and work more efficiently.",
    "readingTime": 1,
    "keywords": [
      "gemini cli",
      "course",
      "development",
      "you’ll",
      "agentic",
      "tools",
      "cloud",
      "services",
      "features"
    ],
    "qualityScore": 0.65,
    "link": "https://learn.deeplearning.ai/courses/gemini-cli-code-and-create-with-an-open-source-agent/information",
    "thumbnail_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2026/01/Gemini-CLI-Code-Create-with-an-Open-Source-Agent_Banner_1920x1080__with_CTA.webp",
    "created_at": "2026-01-21T18:30:41.465Z",
    "topic": "tech"
  },
  {
    "slug": "vibebin-incuslxcbased-platform-for-selfhosting-persistent-sandboxes",
    "title": "Vibebin: Incus/LXC-based platform for self-hosting persistent sandboxes",
    "description": "vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh).  Create...",
    "fullText": "jgbrwn\n\n /\n\n vibebin\n\n Public\n\n vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh). Create and host your vibe-coded apps on a single VPS/server.\n\n License\n\n View license\n\n 9\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jgbrwn/vibebin",
    "readingTime": 1,
    "keywords": [
      "vibebin",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jgbrwn/vibebin",
    "thumbnail_url": "https://opengraph.githubassets.com/b08d7ce148c865871d7a69b11f4c233a6cd39949454bb9455bb3bdcf87a6a575/jgbrwn/vibebin",
    "created_at": "2026-01-21T12:26:57.995Z",
    "topic": "tech"
  },
  {
    "slug": "pragmatic-notes-on-running-dangerous-ai-coding-agents-in-cloud-vms",
    "title": "Pragmatic Notes on Running Dangerous AI Coding Agents in Cloud VMs",
    "description": "A practical approach to safely running AI coding agents with strong isolation using cloud VMs, Tailscale, and simple notification patterns.",
    "fullText": "Running coding agents with free reign is very powerful for a certain class of tasks, especially ones that require little human supervision, or where you want to close (or disconnect) your laptop, walk away, and come back to results.\n\nRecently there have been several HN discussions about safely running Claude Code or Copilot CLI agents, such as Yolobox – Run AI coding agents with full sudo without nuking home dir and Running Claude Code dangerously. These post detail the potential dangers and show how to run these agents more safely, and while reasonable, I find they lack in a few respects.\n\nIn particular, I want strong isolation, long running agent tasks, minimal cognitive overhead and I really value being able to close my laptop, walk away, and get notified on my phone when things are done. I do not mind paying for a cloud VM.\n\nThere are many valid ways to solve this problem. This post describes mine. It covers running multiple coding agents concurrently in a cloud VM, how I handle access and repos, and how I keep notifications simple.\n\nI generated some Terraform to spin up an Azure VM with a cloud-init.yml for setting up common tools/environments I use. Claude can generate a decent starting point for this quite easily, given your particular environment.\n\nFor secure access, I use Tailscale. Note: I'm not paid by them, but it is easily my favorite piece of infrastructure software!\n\nA cloud-init script installs Tailscale on first boot and automatically joins the VM to my tailnet. SSH access is enabled using Tailscale SSH. Once the VM is up, it appears on my private network with a stable hostname via Magic DNS. No SSH key management, no exposed ports.\n\nor connect using VS Code Remote SSH:\n\nhttps://code.visualstudio.com/docs/remote/ssh\n\nMost of the time I prefer tight, step by step control over code generation, working locally in VS Code with Copilot. For longer running or experimental tasks, I instead let an agent work remotely on a branch inside the VM, and pull the results once I am satisfied.\n\nWhile this is arguably git basics, it works well for me and I found that it is useful sharing how to set up a VM as a remote:\n\nOn the local machine, from the repo directory:\n\nThen you can pull clone and check out the branch, do the work, commit, and push to bare repo:\n\nFinally, locally, you can get the changes:\n\nI use tmux to manage long running sessions. This lets agents keep running after I disconnect, and makes it easy to juggle multiple concurrent sessions. If you are not familiar with tmux, it is worth learning!\n\nFor notifications, I use https://ntfy.sh.\n\nIt is free, extremely simple, and works over plain HTTP POST. I have the iOS app installed, so I can walk away from my laptop and still get notified when work completes. I explicitly instruct my agents to make a POST request once their work is done in the agent instructions.\n\nThat is it. No SDKs, no auth setup required for basic usage. The notification shows up immediately on my phone/browser.\n\nIf there is interest, I can publish a repo with the Terraform, cloud-init scripts, makefile, etc, and the old .devcontainer setup.",
    "readingTime": 3,
    "keywords": [
      "coding agents",
      "tasks",
      "laptop",
      "away",
      "access",
      "repo",
      "free",
      "close",
      "disconnect",
      "safely"
    ],
    "qualityScore": 1,
    "link": "https://jakobs.dev/pragmatic-notes-running-dangerous-ai-agents-cloud-vms/",
    "thumbnail_url": "/media/agents-vm.jpg",
    "created_at": "2026-01-21T12:26:57.044Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-over-now-just-coding",
    "title": "Vibe Coding Is Over, Now Just \"Coding\"",
    "description": "Regular thoughts on modernity, classicism, & technology.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://upstreamutopia.com/articles/?id=20260121-060942-vibe-coding-is-over,-now-ju",
    "thumbnail_url": "https://upstreamutopia.com/images/preview.jpg",
    "created_at": "2026-01-21T06:22:09.373Z",
    "topic": "tech"
  },
  {
    "slug": "sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima",
    "title": "Sandbox Your AI Dev Tools: A Practical Guide for VMs and Lima",
    "description": "AI coding assistants and other devtools can steal your credentials and data. Here's how to run them safely in isolated VMs using Lima on macOS/Linux.",
    "fullText": "AI coding assistants, npm, pip, and other development tools can run arbitrary code and scripts on your machine, potentially stealing SSH keys, API tokens, wallet keys, sensitive credentials and other private data without you noticing.\n\nThis guide shows you how to sandbox these tools in isolated VMs using Lima, so you can experiment and develop freely without putting your sensitive data at risk.\nJump straight to the guide, or read on for a bit of personal context.\n\nI’ve been having quite a bit of fun with AI assisted coding recently.\n\nI use LLMs for a wide range of things, including discussing architecture, design choices, learning about new tools and libraries I wasn’t previously aware of, to reviewing PRs and quickly cranking out dirty prototypes.\n\nEspecially for hobby projects that are not meant to ever go into production, I enjoy playing with AI tools fast and loose, producing results quickly and not getting slowed down by annoying things such as reading code before running it 🤣.\n\nAnd yeah… that’s obviously unsafe, unless it’s all contained in a sandbox!\n\nYou should never run potentially dangerous, experimental code on your main machine, since it could steal your passwords, API keys, environment variables, private keys, access to your communication tools, install services, and do all sorts of other nefarious things.\n\nNowadays I isolate all my devtools in VMs, and thought it might be useful to others if I put together a guide to shows how to do it. Well, here it is, and I hope it’ll be useful to you, too!\n\nYou’ll want to run the entire development environment, including the AI tool itself, inside a sandbox. This way it’s safe to install dependencies and to execute code, and unlocks other fun features like snapshots before running sketchy code, and reverting if something goes wrong.\n\nAnd it’s not just AI-generated code. Node.js/npm/yarn and Python/pip are particularly troublesome because they allow any package to run arbitrary scripts on your system during installation, and install tons of additional dependencies that can do the same. This attack vector is called “supply chain attack” and it happens all the time.\n\nVirtual Machines (VMs) and Containers (i.e. Docker, Podman, containerd) are the two most practical methods for isolating development tools from your host operating system. VMs provide much stronger protection and more flexibility overall, and are better suited for co-developing with AIs.\n\nContainer runtimes share the host operating system’s kernel, which means they’re fundamentally running on the same system as your main machine, just with isolated namespaces and resource limits. This creates several security concerns:\n\nIn contrast, a VM runs its own complete operating system with its own kernel. The hypervisor (like QEMU/KVM) creates a much stronger isolation boundary. Even if malicious code completely compromises the VM, it would need to exploit the hypervisor itself to reach your host, a significantly harder target.\n\nFurthermore, a VM enables better concurrency. It can run Docker containers, databases, web servers, multiple build processes, and background services all at once, and the AI tool can interact with everything naturally just like on a normal development machine.\n\nIn this guide, we use Lima VM to sandbox AI and devtools. Lima is a delightful, lightweight virtual machine manager for Linux and macOS which provides easy and quick ways to create and manage VMs.\n\nYou interact with Lima through the limactl command:\n\nVMs are based on templates, which can include (build on) other templates:\n\nThe Lima VM docs have platform-specific installation guides.\n\nHomebrew is recommended on macOS:\n\nOn Linux install the binary like this:\n\nNow ensure your Lima version is up-to-date:\n\nWe only want to share very specific host directories with the VM.\n\nLet’s create ~/VM-Shared on the host, which we later mount into the VM at ~/Shared (with write access):\n\nYou can use that directory to easily copy files between the host and the VM, and to share project directories from the host with the VM.\n\nDefaults for all VMs can be defined in ~/.lima/_config/default.yaml.\n\nLet’s create the default YAML file:\n\nLima conveniently creates default SSH configuration files for all VM instances, which makes it easy to log in with SSH (including using VS Code for a Remote-SSH session).\n\nI recommend using a ~/.ssh/config.d/ directory on the host and have SSH include all configs there by default. That allows us to simply link the Lima-created config files there to use them.\n\nAdd this as first line in your ~/.ssh/config file, to make SSH include all configs from there:\n\nGreat! After creating a new VM, we can now simply create a symlink to the Lima-generated SSH configs and use it to SSH into the instance.\n\nLet’s start an Ubuntu 25.10 VM instance, named dev.\nWe use the internal _images/ubuntu-25.10.yaml template because it doesn’t include the automatic home directory sharing:\n\nYou can share additional project-specific directories between host and VM in several ways:\n\nCreate a symlink for the SSH config file and SSH into the VM:\n\nLet’s update the services on the instance, and configure git:\n\nLet’s confirm that port forwarding works. We do this using a one-liner Python HTTP server (on port 7777) inside the VM, and accessing it from the host:\n\nThis section guides you through installing several other languages and development tools, including Golang, Node.js, Python, Rust, Docker.\n\nWe can accomplish that either by installing each tool according to it’s documentation, or by using a version manager such as mise (“mise-en-place”, 22k stars on Github) which can install hundreds of tools via a simple command-line interface.\n\nFirst, we install mise (“mise-en-place”, 22k stars on Github) and make bash support it:\n\nYou use mise latest <tool> to see the latest versions it knows about:\n\nNow you can install all the tools you want in a single command:\n\nTo manually install (or update) Golang in the VM, download the latest release and extract into /usr/local/go:\n\nThe Golang path needs to be in the PATH environment variable, which we have already added before.\n\nA good way to install a current version of Node.js in Ubuntu is by using nvm, a modern node version manager (90k stars on GitHub):\n\nNow it’s all installed and ready to use! Check the versions like this:\n\nPerhaps you don’t even need Docker, since Lima includes containerd and nerdctl by default. This is a Docker-compatible runtime and command-line interface that can also run images from Docker Hub:\n\nIf you do want to install Docker, the quickest way to install it by using their official get-docker.sh script:\n\nFor the group changes to take effect, exit the shell and re-login (may need a VM restart).\n\nVerify that user is in the ‘docker’ group:\n\nGitHub CLI provides a useful gh cli command that let’s you easily interact with GitHub and private repositories.\n\nYou can install it in the VM following the Linux installation instructions:\n\nWarning: Authorizing GitHub CLI to access private repositories will leave an API key in the VM which could potentially be stolen by unauthorized scripts (which is what we wanted to avoid in first place by running everything in a VM).\n\nOnly authorize it with gh auth login for private repo access if you accept the risks! I personally avoid having any sensitive credentials in the VM, in particular those that allow access to private GitHub repositories.\n\nIf you prefer an IDE like VS Code, you can use Remote-SSH to start a session inside the instance.\n\nPlease note that this is potentially unsafe, as explained in the Remote-SSH README:\n\nSecurity Note\nUsing Remote-SSH opens a connection between your local machine and the remote. Only use Remote-SSH to connect to secure remote machines that you trust and that are owned by a party whom you trust. A compromised remote could use the VS Code Remote connection to execute code on your local machine.\n\nSee also this discussion on GitHub for more context and information.\n\nNow a new VS Code window opens, and sets up VS Code Server:\n\nThen you can click “Open” and choose a folder, like Shared:\n\nBefore setting up the tools, let’s create a “Hello World” directory in the Shared folder as our playground:\n\nLet’s start with installing Claude Code in the VM, following the instructions in the documentation:\n\nOn first start, Claude asks you to authorize it.\n\nThe docs mention support for an ANTHROPIC_API_KEY environment variable (i.e. set in .bashrc), but that did not work when I tried it; claude CLI didn’t let me skip the login process. Only after the login was done it notified me about the existing environment variable, and whether I’d prefer to use that one.\n\nAfter the login, Claude Code CLI is ready to be ued in the VM! 🎉\n\nSince Claude is running in a VM, it might be permissible to run it in “dangerously skip permissions mode”, which makes it bypass all permission checks:\n\nYou could also create an alias for it and add it to your .bashrc:\n\nAnthropic provides documentation for using Claude in VS Code, and also offer a VS Code Claude extension.\n\nYou can install the Claude extension in the VM through the Remote-SSH session window:\n\nIn contrast to the CLI tool, the authentication flow did not work through the user interface, and I had to set the ANTHROPIC_API_KEY environment variable:\n\nReload the VS Code window (open command palette with Shift + CMD + P and choose “Developer: Reload Window”):\n\nNow the VS Code Claude extension should work:\n\nIf you want to enable “dangerously skip permissions mode” in the VS Code extension, you can enable it via your user settings. Open the settings (CMD + ,), search for “claude” and enable “Claude Code: Allow Dangerously Skip Permissions”:\n\nLet’s install Gemini CLI from Google next.\n\nThe documentation recommends installing it with npm, the Node.js package manager. You’ll need to install Node and npm first, see also the Node.js setup instructions.\n\nIt will ask you to authenticate:\n\nI chose “Login with Google”. Note that the authentication flow may require a retry if the first attempt times fails.\n\nAfter authorization is done, Gemini CLI works!\n\nYou can run Gemini in YOLO mode:\n\nAutomatically accept all actions (aka YOLO mode, see https://www.youtube.com/watch?v=xvFZjo5PgG0 \n\nThe alias you could define in .bashrc:\n\nCodex CLI is the AI dev tool from OpenAI/ChatGPT.\n\nIt will ask you to sign in, either via ChatGPT or by providing an API key:\n\nAfter that is done, Codex CLI is ready to work for you!\n\nYou can also run Codex in dangerous mode:\n\nSkip all confirmation prompts and execute commands without sandboxing. EXTREMELY DANGEROUS. Intended solely for running in environments that are externally sandboxed\n\nThere are several other great tools worth a mention:\n\nDrop your favorite tools in the comments below!\n\nVM clones and snapshots allow you even more flexibility and isolation. You can use them to quickly and cheaply run new VMs for experiments and specific projects based on already provisioned instances. Use them frequently!\n\nLima offers several ways to take VM snapshots and/or clone VMs.\n\nYou can make a copy of an existing VM instance with limactl clone. The existing instance needs to be stopped first.\n\nAfter all the initial VM setup is done, clone it and use it both as backup as well as a base for future instances:\n\nRemember that after starting a new instance, you probably want to symlink the VM SSH configuration to your ~/.ssh/config.d/ directory, so ssh knows about it (See also “SSH into the VM”):\n\nFor maximum security and flexibility, consider using multiple VMs for different purposes and trust levels. This approach provides better isolation and lets you tailor each environment to specific needs.\n\nHere are some suggested VM configurations:\n\nYou can quickly clone your base VM setup to create new instances for different projects using limactl clone, as described in the VM cloning section above.\n\nFor sensitive or production projects, consider dedicating a separate VM to each project. This prevents potential cross-contamination between projects and allows you to mount only the specific project directories you need.\n\nWhen creating project-specific VMs, you can customize the mounted directories by editing the instance configuration. Either adjust the mounts section before starting the VM (by not using the -y flag), or edit ~/.lima/<vm_name>/lima.yaml after creation and restart the instance.\n\nThis approach also makes it easier to share VM configurations with team members. Instead of sharing entire disk images, you can distribute just the Lima template YAML file, which team members can use to spin up identical environments on their machines.\n\nFor automated setup, Lima supports provisioning scripts that run during VM creation. For more complex setups, consider using idempotent provisioning tools like Ansible to ensure consistent environments across your team.\n\nIf you find yourself repeatedly creating VMs with similar configurations, consider creating custom Lima templates. Templates are YAML files that define VM settings, and they can include other templates.\n\nCustom templates are useful for:\n\nYou can create a custom template by copying and modifying an existing one from Lima’s template directory. Save your custom templates in ~/.lima/_templates/ and reference them when creating new VMs:\n\nSee the Lima templates documentation \n\nHere are some important security best practices to follow when using VMs for development:\n\nRemember: The whole point of using VMs is isolation. When in doubt, create a new VM for risky experiments and delete it afterwards.\n\nI hope this guide helps you get started quickly and right-footed!\nAs always, please leave feedback, questions and ideas in the comments below.\n\nSpecial thanks to Ilya Lukyanov and Overflo for reviewing drafts of this post and making great suggestions. 🙏",
    "readingTime": 12,
    "keywords": [
      "yolo mode",
      "yaml file",
      "anthropic_api_key environment",
      "remote-ssh session",
      "claude extension",
      "ssh configuration",
      "authentication flow",
      "mise mise-en-place",
      "mise-en-place stars",
      "command-line interface"
    ],
    "qualityScore": 1,
    "link": "https://www.metachris.dev/2025/11/sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima/",
    "thumbnail_url": "https://www.metachris.dev/images/posts/ai-sandbox/cover.jpg",
    "created_at": "2026-01-21T00:59:26.211Z",
    "topic": "tech"
  },
  {
    "slug": "takatime-selfhosted-wakatime-alternative-go-and-mongodb",
    "title": "TakaTime – Self-Hosted WakaTime Alternative (Go and MongoDB)",
    "description": "TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim.  It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity...",
    "fullText": "Rtarun3606k\n\n /\n\n TakaTime\n\n Public\n\n TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim. It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity to a third-party server, TakaTime stores everything in your own MongoDB database.\n\n License\n\n MIT license\n\n 10\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Rtarun3606k/TakaTime",
    "readingTime": 1,
    "keywords": [
      "coding",
      "activity",
      "takatime",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Rtarun3606k/TakaTime",
    "thumbnail_url": "https://opengraph.githubassets.com/4a0f7ea58066e011c6d164cc23d1af692c671a17cadd5889743e52a2451b2406/Rtarun3606k/TakaTime",
    "created_at": "2026-01-20T12:27:00.258Z",
    "topic": "tech"
  },
  {
    "slug": "metacompilation",
    "title": "Metacompilation",
    "description": "A post that is going to be part of my sequence on rethinking programming languages. …",
    "fullText": "A post that is going to be part of my sequence on rethinking programming languages.\n\nA compiler is a piece of machine code , that takes as input a text string describing a program  and returns the compiled machine code\n\nLet  be a function that takes in a machine code program and returns another potentially faster or smaller program.\n\nA metacompliler has the formula\n\nTo understand how this works, first let's look at a less self referential case. Let  be a regular compiler.\n\nis just a string. Maybe it's \"print(1+2)\"\n\nis a machine code program. This program, if run, would first compile  into machine code, and then would run that machine code. Therefore it is a machine code program that does the same thing as . It has a fairly significant size for even a small program, as it contains a complete copy of the compiler.\n\nWhat does  do? It optimizes that machine code. The first thing it can do is cut out big chunks of the compiler. At least in simple cases. If the code is running arbitrary eval statements, all the compiler might be needed. In the case of this simple program, the parts of the compiler that handle floats, loops etc are just not used. If the optimizer is good, it could simplify the code all the way down to . Some programming languages (see zig) already run code at compile time. The difference between compile and run time is just in what variables you currently know the value of, and the relative cost of compute.\n\nFor code with a finite runtime that just runs by itself, not interacting with the outside world, it can all, in principle be simplified down to a single print statement. In practice computer programs interact with the \"outside world\" in all sorts of ways. In some contexts, writes to disk or sending data to a GPU might be considered interactions with an external world. But for simplicity, assume the only form of interaction is input() and print()\n\nSo that's what a metacompiler does. But does it actually do anything. The most naive metacompiler implimentation has  .  When we call  we get the program. And when we proceed to run that program, that program first calls  to generate the machine code  and then runs that machine code. This leads to an infinite regress. We haven't actually used  anywhere. What we essentially have is just.\n\nA program that is clearly an infinite loop, with no actual relation to pi.\n\nSo we need the optimization step of the metacompiler to be doing something non-trivial to make the code halt in a finite time at all.\n\nLets define a small toy programming language, so we can talk about how to compile it.\n\nWe will give our programming language one data type, arbitrary size integers.\n\nWe will allow definitions, inputs, calculations and loops.\n\nThis example program shows all the features of this programming language. It is rather minimal.\n\nThe only free parameter in the metacompiler (as above) is in the choice of\n\nFor clarity, machine code instructions will look the same as programming language instructions, except the machine code will be in BOLD\n\nThe program consists of a number of definitions,  (of the format [name]=[number], looking like  ) followed by the first non-definition statement. If the same name is used multiple times, only the last definition is needed. Ie the code  can be optimized to\n\nSuppose the optimizer takes in  code where the first non-definition in  happens to be a calculation. For example.  this can get optimized into\n\nNow suppose the first non-definition in  is an .  For example.  This can be converted into.\n\nThe way to think about this is that, if  were a normal compiler, the  function would convert a machine code program containing  into another machine code program that still contains  but that makes  do slightly less work.\n\nwhile the similar  can simplify down to\n\nThis gives a functioning toy example of a metacompiler. The above simplification rules are used in the definition of , which is in turn used in the definition of .\n\nThis produces code that, while excessively self referential, runs and produces output in a finite time, at least assuming the output of a regular compiler would run in finite time on the program.\n\nNote that  only does 1 simplification step, and is only run once at compile time.\n\nSuppose we insisted that, before  is allowed to simplify a piece of machine code, it must first prove that it's simplification won't change the result. This can be proved, by lob's theorem. However it isn't sufficient to make the metacompiler actually valid. Lob's theorem just says that ZFC approves of infinite buck passing. At some point we need to actually understand our programming language.\n\nIf however we make  prove that  is equivalent to  before  is allowed to output . Then that is sufficient. Your directly proving that your meta-compiler is doing the same thing as a regular compiler, which gives you a ground truth about the meaning of the programming language.\n\nWhile the example meta-compiler given above isn't particularly fast, the toy example shows that metacompilers can exist. And the space of meta-compilers seems like it should contain all sorts of interesting optimizations.\n\nFor example. I was doing some programming involving numerically integrating systems of Stochastic Differential Equations (SDE's). Basically, I choose various settings and then run a tight loop involving those settings. And ideally I would like the speed of special purpose compiled code within the tight loop, without having the overhead of a full compilation from source every time I change a setting.\n\nSo, what I would ideally want is a program that contains precompiled snippets of code. Once the particular values of the settings are known, a highly optimized machine code program could be put together by little more than pasting together the relevant blocks of machine code to make a complete machine code program.\n\nAnd I'm wanting a way to make the programming language do this, or other clever things like this, automagically.\n\nAnother clever thing. Suppose your program contains  of arbitrary user generated strings. But you know this is only a small fraction of runtime. And the user isn't allowed to use various language features. You might want to make a cut down minified version of the full language compiler, something with the unused features cut out, and some of the optimization tricks removed.\n\nThe hope is to totally blur the line between compile time and runtime, with code that can rewrite itself on the fly in all sorts of clever and highly performant ways.",
    "readingTime": 6,
    "keywords": [
      "self referential",
      "lob's theorem",
      "tight loop",
      "programming languages",
      "regular compiler",
      "programming language",
      "machine code",
      "code program",
      "metacompiler",
      "contains"
    ],
    "qualityScore": 1,
    "link": "https://www.lesswrong.com/posts/6BSZkkWNGMTdRi5Ly/metacompilation",
    "thumbnail_url": "https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg",
    "created_at": "2026-01-20T12:26:57.143Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here’s how the three models performed in our tests:\n\n💡 If you want the safest pick for real “ship a feature in a big repo” work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you’re okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n⚠️ NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere’s the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe’ll compare code quality, token usage, cost, and time to complete the build.\n\n💡 NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\n💡 NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It’s not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that’s the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that’s a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it’s hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. 🤷‍♂️\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don’t think we’re anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don’t fully back them up. 🥴\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won’t hold up long term.\n\nI don't see any of these recent models as “use it and ship it” for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "ygrep-fast-local-indexed-code-search-tool-optimized-for-ai-coding-assistants",
    "title": "Ygrep: Fast, local, indexed code search tool optimized for AI coding assistants",
    "description": "A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing. - yetidevworks/ygrep",
    "fullText": "yetidevworks\n\n /\n\n ygrep\n\n Public\n\n A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing.\n\n License\n\n MIT license\n\n 14\n stars\n\n 2\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n yetidevworks/ygrep",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/yetidevworks/ygrep",
    "thumbnail_url": "https://opengraph.githubassets.com/eda1bb1f12626e76b9793d44f791d0ed3332b6c28dc7896d1831d00d7c49258d/yetidevworks/ygrep",
    "created_at": "2026-01-20T00:57:31.412Z",
    "topic": "tech"
  },
  {
    "slug": "scaling-longrunning-autonomous-coding",
    "title": "Scaling long-running autonomous coding",
    "description": "Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from …",
    "fullText": "Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents:\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nThey ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.\n\nIn my predictions for 2026 the other day I said that by 2029:\n\nI think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier.\n\nI may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nBut how well did they do? Their initial announcement a couple of days ago was met with unsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.\n\nIt looks like they addressed that within the past 24 hours. The latest README includes build instructions which I followed on macOS like this:\n\nThis got me a working browser window! Here are screenshots I took of google.com and my own website:\n\nHonestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.\n\nThe FastRender repo even uses Git submodules to include various WhatWG and CSS-WG specifications in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.\n\nThis is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was HiWave browser, a new browser engine in Rust first announced in this Reddit thread.\n\nWhen I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.",
    "readingTime": 3,
    "keywords": [
      "autonomous coding",
      "web browser",
      "agents",
      "repo",
      "cursor",
      "project",
      "ended",
      "tasks",
      "agent",
      "mostly"
    ],
    "qualityScore": 1,
    "link": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/",
    "thumbnail_url": "https://static.simonwillison.net/static/2026/cursor-social-card.jpg",
    "created_at": "2026-01-20T00:57:30.632Z",
    "topic": "tech"
  },
  {
    "slug": "valve-updates-ai-disclosure-guidelines-to-allow-for-aipowered-tools",
    "title": "Valve Updates AI Disclosure Guidelines To Allow For AI-Powered Tools",
    "description": "Valve has made changes to its AI-disclosure guidelines, removing the need for studios to disclose whether or not games have been developed with AI-powered tools and putting more emphasis on AI-generated assets.\nThe change, which was pointed out by Simon Carless on LinkedIn, suggests that Valve is no longer concerned by the use of AI tools that assist development, stating, \"Efficiency gains through the use of [AI-powered dev tools] is not the focus of this section.\" These tools could included a variety of things, such as AI-generated transcripts of meetings to code helpers that have become prevalent in most programming environments.\nValve states the the aim of its disclosure policy is to inform players when AI is used to generate content, from marketing and conceptual assets to in-game ones that players will interact with. Developers are able to specify what assets have been generated and indicate, via a single checkbox, whether or not players will interact with AI-generated content during gameplay, be it images, audio, or other content.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/valve-updates-ai-disclosure-guidelines-to-allow-for-ai-powered-tools/1100-6537483/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1585/15853545/4637028-7297126222-arc-r.jpg",
    "created_at": "2026-01-19T18:18:34.839Z",
    "topic": "gaming"
  },
  {
    "slug": "a-meta-product-manager-with-no-technical-background-says-vibe-coding-gave-him-superpowers",
    "title": "A Meta product manager with no technical background says vibe coding gave him 'superpowers'",
    "description": "A Meta product manager says vibe coding is letting non-technical PMs ship features and work differently with engineers.",
    "fullText": "A product manager at Meta says vibe coding has changed what it means to do his job — even though he has no technical background and still finds code \"terrifying.\"\n\nZevi Arnovitz said in an episode of \"Lenny's Podcast\" released Sunday that discovering AI coding tools in mid-2024 marked a turning point in his career.\n\nIt felt like he was handed \"superpowers,\" Arnovitz said.\n\nUnderstanding how to use AI intentionally is \"one of the biggest game changers that will make you much better as a PM,\" he said, referring to product management.\n\nArnovitz joined Meta in September last year after about three years as a product manager at website-building company Wix, according to his LinkedIn profile.\n\nArnovitz said he has rebuilt his workflow around AI. He uses vibe coding tools like Cursor alongside models from Anthropic and Google to explore product ideas, generate build plans, execute code, review it, and update documentation.\n\nThe shift reshaped his role as a product manager. Instead of merely acting as a coordinator between engineering and design, Arnovitz operates more like a product owner with the capability to execute.\n\n\"Everyone's going to become a builder,\" he said. \"We're going to see that a lot in the next coming years.\"\n\nStill, Arnovitz said there are limits to what non-technical product managers should take on. He said he doesn't think product managers should be shipping complex infrastructure changes or big projects.\n\nAI has enabled product managers to take on smaller UI projects by building the feature and then handing the code to a developer for final review and completion, he added.\n\nAs AI tools improve, Arnovitz said titles and responsibilities are likely to \"collapse,\" and product managers should treat vibe coding as a \"collaborative learning opportunity\" with their engineering teams.\n\nThe rise of AI coding tools is blurring the lines for traditional roles, making it easier for non-technical workers, including product managers, to build products directly.\n\nFigma CEO Dylan Field said in October on \"Lenny's Podcast\" that AI has pushed many workers to experiment with building products.\n\nTasks that once required deep engineering expertise can now be done with vibe coding tools, he said.\n\n\"I think that we're seeing more designers, engineers, product managers, researchers, all these different folks that are involved in the product development process dip their toe into the other roles,\" he said.\n\n\"We're all product builders, and some of us are specialized in our particular area,\" he added.\n\nThat same thinking is showing up in how companies train new hires. LinkedIn replaced its long-running associate product manager program with an associate product builder track in January.\n\n\"We're going to teach them how to code, design, and PM at LinkedIn,\" said the company's former chief product officer, Tomer Cohen, in an episode of \"Lenny's Podcast\" published in December. It's more about training people \"who can flex across,\" he added.\n\nCohen, who spent nearly 14 years at LinkedIn, left the company in January and now works as an advisor, according to his LinkedIn profile.",
    "readingTime": 3,
    "keywords": [
      "linkedin profile",
      "vibe coding",
      "coding tools",
      "product manager",
      "product managers",
      "associate product",
      "lenny's podcast",
      "code",
      "engineering",
      "arnovitz"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-product-manager-vibe-coding-superpowers-non-technical-builder-2026-1",
    "thumbnail_url": "https://i.insider.com/696dbfd0c58df2ecd5ccc045?width=1200&format=jpeg",
    "created_at": "2026-01-19T12:27:05.647Z",
    "topic": "finance"
  },
  {
    "slug": "figmalike-canvas-for-running-claude-code-agents",
    "title": "Figma-like Canvas for running Claude Code agents",
    "description": "Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf) - AgentOrchestrator/AgentBase",
    "fullText": "AgentOrchestrator\n\n /\n\n AgentBase\n\n Public\n\n Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf)\n\n License\n\n View license\n\n 85\n stars\n\n 6\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n AgentOrchestrator/AgentBase",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/AgentOrchestrator/AgentBase",
    "thumbnail_url": "https://opengraph.githubassets.com/ccefb572879426a6706c75e212582de106cce0540f056c7b6f252924f0fba3c1/AgentOrchestrator/AgentBase",
    "created_at": "2026-01-18T18:15:51.872Z",
    "topic": "tech"
  },
  {
    "slug": "building-a-tui-to-index-and-search-my-coding-agent-sessions",
    "title": "Building a TUI to index and search my coding agent sessions",
    "description": "Building a TUI to index and search coding agent sessions across Claude Code, Codex, and others, with Tantivy for fuzzy full-text search.",
    "fullText": "This is the story of how fast-resume came to life and evolved, as I was trying to search and resume my coding agent sessions more easily across different local CLI agents.\n\nI use many coding agents these days: Claude Code, Codex, OpenCode, Copilot, and more. Sometimes I remember that I, or the agent, mentioned something specific in a previous session, and I want to go back to it.\n\nMost coding agents have a /resume feature now, which allows a session to be reopened with all the state back. While the resume feature works great, finding which session to resume is harder.\n\nThat means that for example if I remember the agent mentioning a specific subject later during the conversation, it won’t be in the title, so I can’t find it.\n\nLet’s say I have a few sessions about building a TUI program. I remember that in one of the sessions, the agent mentioned textual. I can’t search for textual in the resume view! Also, if I don’t remember the folder and which agent I used, I’m screwed. And some agents don’t have that feature at all.\n\nSo I started ripgrep‘ing my home folder to find the string I was searching for, then using clues from the session file (directory, timestamp, context) to navigate to the correct directory, /resume, and find the session in question. 😅\n\nSince most coding agents store sessions locally, I started thinking: what if I could automate this grep‘ing, wrap it in a nice TUI and be able to resume in one keypress?\n\nFirst, to see if this was feasible, I had to understand how sessions are actually stored. Most agents use JSON files, but there are some interesting differences.\n\nMost agents follow the same pattern as Claude Code: Codex and Copilot CLI use JSONL with similar structures.\n\nFiles are stored in ~/.claude/projects/{project_id}/{session_id}.jsonl. JSONL is a format where each JSON object is stored independently on a newline.\n\nMessages from the user or Claude and tool calls are stored that way. Here is an example of a message:\n\nOpenCode doesn’t use JSONL but instead independent JSON files. Message content is sharded by session id, message id, and message parts in ~/.local/share/opencode/storage/:\n\nThis design conceptually makes sense: not having to rewrite or append to a single file might be simpler. But for indexing, it means a lot more filesystem operations. To give you an idea: I used Claude Code possibly 100x more than OpenCode, yet OpenCode has 10x more files (9,847 vs 827). See Stats \n\nVibe stores one JSON file per session in ~/.vibe/logs/session/session_*.json. It is not JSONL. The file contains metadata and the full messages array.\n\nOne detail that surprised me: Vibe rewrites the entire file after each user turn. That means the file grows and gets fully serialized on every message, which is simple but doesn’t seem very efficient for long sessions.\n\nCrush is the only agent that uses SQLite instead of JSON files. Projects are listed in ~/.local/share/crush/projects.json, and each project has its own .crush/crush.db database.\n\nThe schema has a sessions table with metadata like title, message count, and cost, and a messages table with role and parts (stored as JSON).\n\nI’m surprised it’s the only agent using SQLite!\n\nTo search sessions, I started with a naive approach. I defined a common Session type and an adapter protocol to abstract each agent’s storage format:\n\nEach adapter implements three methods: find_sessions parses all session files and returns Session objects, get_resume_command returns the shell command to resume a session (claude --resume {id} for Claude, codex resume {id} for Codex), and is_available checks if the agent’s data directory exists.\n\nFor example, here’s the core of Claude’s adapter:\n\nAdding a new agent means writing one adapter file. Implement scanning, parsing, and the resume command. The search engine, TUI, and CLI all work automatically.\n\nOn startup, each adapter would parse its session files and return a list of Session objects. I cached the results in a sessions.json file and used file mtimes to know when to reindex.\n\nFor search, I used RapidFuzz because the experience I had in mind was the familiar fuzzy finding of fzf. For each session, I built a searchable string by concatenating the title, directory, and full content:\n\nRapidFuzz’s Weighted Ratio scorer compared the query against every searchable string. This scorer has an interesting backstory but it basically uses other scorers based on the lengths of the string.\n\nThe problem was that WRatio alone didn’t rank exact matches high enough. Searching for “fix auth bug” might rank “authentication fixes” higher than a session literally titled “fix auth bug”. I added bonuses on top of the fuzzy score: +25 if the query appears as a substring, +15 if all query words are present, and +30 if they appear consecutively. This helped with ranking quality, but the performance was not good enough for me. Every search scanned every session on every keystroke. The TUI would visibly lag while typing, and I’m trying to have a very reactive TUI.\n\nI needed a proper search engine. I first considered SQLite FTS5, which has a trigram tokenizer for similarity matching, but it works by comparing 3-character substring overlap rather than edit distance, which is what I’m looking for. I’m a very imprecise typer 😄\n\nI opted for Tantivy, an in-process full-text search library written in Rust, and the one powering Quickwit. Instead of comparing the query against every document at search time, we can use it to build an inverted index upfront: a mapping from terms to the sessions that contain them.\n\nTantivy’s FuzzyTermQuery uses Levenshtein distance, which is better for actual typos: “teh” matches “the” (distance=1), but wouldn’t match with trigrams since they share no 3-character chunks.\n\nWhen a session gets indexed, Tantivy tokenizes its content into terms and stores which document IDs contain each term. Searching for “auth bug” means finding documents containing “auth”, finding documents containing “bug”, intersecting the sets, then scoring the matches using BM25.\n\nLuckily, the only “official” bindings for Tantivy are for Python! So I was able to use it directly and very easily in my project.\n\nThe schema defines what gets indexed:\n\nText fields get tokenized and indexed for search. The raw tokenizer keeps the value as-is without splitting, which is useful for IDs and agent names where “copilot-cli” should stay as one token, not become “copilot” and “cli”. (cf Keyword query syntax)\n\nWhen the schema changes (adding a field, changing tokenizers), the index needs to be rebuilt. I track a schema version in a file alongside the index and clear everything if it doesn’t match:\n\nIt’s not very robust (I could bump the version to the same number into two concurrent PRs), but it’s good enough for now.\n\nFor fuzzy matching, Tantivy supports custom distance for queries. A fuzzy term query with distance 1 matches terms that are one character insertion, deletion, or substitution away from the query term. “atuh” matches “auth”, “bugg” matches “bug”.\n\nThe prefix=True flag also matches terms that start with the query, so “au” matches “auth” and “authentication”.\n\nI ran into the same ranking problem as with RapidFuzz: fuzzy matches sometimes outranked exact matches. The fix was a hybrid query that boosts exact matches:\n\nThe performance has been quite good with Tantivy. My use case is pretty basic and the dataset is very small in FTS terms, so I haven’t looked into performance optimization too much. But queries complete in a handful of milliseconds, which is perfect!\n\nThe first version of fast-resume rebuilt the entire index when any source directory changed. Adding one new Claude session meant re-parsing hundreds of Codex sessions that hadn’t changed.\n\nThe fix was tracking modification times per session. Tantivy stores each session’s mtime alongside its content:\n\nOn startup, fast-resume asks the index for all known sessions and their mtimes. Each adapter compares file mtimes against what’s known and only re-parses what changed or is new:\n\nIf a session’s mtime is newer than what’s in the index, re-parse it. If a session exists in the index but not on disk, mark it deleted. Everything else stays untouched.\n\nUpdates are atomic: delete the old documents and add the new ones in a single transaction before committing. This avoids a window where the session is missing from the index:\n\nIf nothing changed (the common case) the whole process is just reading mtimes and comparing numbers. In any case, this happens in the background while the TUI starts instantly (see streaming updates).\n\nMost adapters spend their time parsing JSON. Claude sessions are JSONL files with hundreds of lines. OpenCode has thousands of small JSON files spread across directories. Even with incremental indexing, the initial index build parses everything.\n\nTo try to gain a bit for performance, I switched the native json lib for orjson, which is a JSON library written in Rust that’s supposed to be a lot faster.\n\norjson’s loads also accept both strings and bytes, and it’s faster with bytes, so we can pass it the file directly in binary mode without converting to a string first.\n\nThe TUI is built with Textual, a Python framework for terminal interfaces. I discovered it with Mistral’s vibe coding agent. This and uv are the reason I wanted this project to be Python, even though I usually pick Go for CLIs.\n\nTextual provides a layout system, widgets, reactive state, and async workers, great to have a fully featured and snappy TUI.\n\nThe main screen has three parts: a search input at the top, a results table in the middle, and a preview pane at the bottom. Everything is reactive; changing state automatically updates the UI, which is a pattern I like and I’m used to with web frameworks.\n\nOn startup, results load instantly from the existing index. In parallel, all adapters compare file mtimes to find new or modified sessions and index them in the background. Each time an adapter finishes, the results table refreshes to include the newly indexed sessions. On first run or after a schema version bump, the index is empty so results populate progressively as adapters complete.\n\nThe TUI runs this off the main thread using Textual’s @work decorator. Each time an adapter finishes indexing, on_progress re-runs the current search query against the updated index, so newly indexed sessions that match appear immediately:\n\ncall_from_thread marshals updates back to the main thread for UI changes.\n\nSearch is debounced to improve responsiveness when holding delete for example, otherwise the TUI doesn’t have enough time to re-render after the search and it feels laggy.\n\nThe watch_search_query method is a Textual watcher: it gets called automatically when search_query changes. Setting the reactive variable triggers the search.\n\nSearch also runs in a background thread so the UI stays responsive while Tantivy works:\n\nThe query time gets displayed next to the search box, it’s surprisingly variable, from ~0.5ms to ~50ms on my laptop. But it feels pretty snappy!\n\nNavigation works with up/down, but also j and k. shift+tab to move from search to preview, / to focus back the search bar, return resumes the selected session. Scrolling also works with the mouse. You can resize the preview with + and - or hide it entirely with Ctrl+backtick.\n\nSearch terms are highlighted in the results table (title, directory) in fzf style and the preview pane using Rich’s Text.stylize(). One limitation: Tantivy returns matching documents but doesn’t expose which terms actually matched. So if you search “atuh” and it fuzzy-matches “auth”, only “atuh” gets highlighted, not “auth”. I couldn’t find a way to get the expanded terms from Tantivy.\n\nSince modern terminals support inline images through protocols like Sixel, I thought we could include coding agent logos to make it look nicer. The textual-image library handles terminal detection and rendering. Unfortunately, it doesn’t work with vhs, so I have to record demos manually!\n\nSession timestamps are colored based on age: green for recent, fading through yellow and orange to gray for old. Exponential decay maps time to a 0-1 value, which compresses older sessions together:\n\nThen t interpolates through color stops (green → yellow → orange → gray). A session from an hour ago looks noticeably different from one from yesterday, but three months and six months both just look “old”.\n\nPlain text search works fine for most queries, but sometimes you want to narrow results by agent or time. Rather than building a separate filter UI, I added keyword syntax directly in the search box. Type agent:claude to filter to Claude sessions, date:today for today’s sessions, dir:my-project to match directory paths.\n\nTextual’s Suggester provides autocomplete as you type: agent:cl suggests claude, date:to suggests today. It also handles negation, so agent:!co suggests !codex.\n\nThe parser extracts keywords from the query using a regex, handling keyword:value pairs, quoted values with spaces like dir:\"my project\", and negation with - or !. Whatever doesn’t match a keyword becomes free-text that goes to Tantivy.\n\nAgent and directory filters support multiple values: agent:claude,codex matches either agent, agent:claude,!codex means Claude but not Codex. Date filters have their own mini-language: date:today, date:yesterday, date:<1h (within the last hour), date:>2d (older than two days).\n\nThese parsed filters translate to Tantivy queries:\n\nWhen you press Enter on a session, the TUI doesn’t directly exec the resume command. Instead it stores the command and directory, exits cleanly, and returns them to the CLI wrapper:\n\nThe CLI then uses os.execvp to replace itself with the agent’s resume command:\n\nexecvp replaces the current process entirely: same PID, same terminal, but now running Claude or Codex instead of fast-resume. This is cleaner than spawning a child process because the resumed agent owns the terminal directly. Ctrl+C goes to the agent, not to a wrapper script.\n\nThe directory change happens first because most agents expect to be run from the project directory.\n\nSome agents support “yolo mode” to automatically approve edits and tool calls. Claude has --dangerously-skip-permissions for example. But it applies to the current instance of claude, not the session. So starting claude without this flag, you can’t resume a past session in yolo mode, even if that session was started in an instance of claude started with the flag.\nWhen you resume a session, fast-resume can detect if it was originally started in yolo mode and offer to resume the same way.\n\nAdapters that parse session files look for yolo indicators. Codex stores approval policy in a turn_context record:\n\nVibe stores it directly in session metadata:\n\nThe yolo flag gets indexed alongside each session. When you resume, the TUI checks in order: fast-resume’s --yolo flag overrides everything, then stored session yolo state, then if the adapter supports yolo but we don’t know the session’s state, a modal asks the user.\n\nSince we’re indexing all sessions across agents, we get analytics as a bonus. fr --stats gives you a breakdown of your session history:\n\nThis was a fun project! It was a good occasion to try a new framework for TUIs and use an in-process search engine to keep things snappy. I’m pretty happy with the result!\n\nI published it to PyPI, so you can try or install it with uv:",
    "readingTime": 13,
    "keywords": [
      "vibe stores",
      "code codex",
      "json files",
      "session’s mtime",
      "documents containing",
      "preview pane",
      "searchable string",
      "newly indexed",
      "tui doesn’t",
      "adapter finishes"
    ],
    "qualityScore": 1,
    "link": "https://stanislas.blog/2026/01/tui-index-search-coding-agent-sessions/",
    "thumbnail_url": "https://stanislas.blog/2026/01/tui-index-search-coding-agent-sessions/fast-resume.png",
    "created_at": "2026-01-18T12:21:36.003Z",
    "topic": "tech"
  },
  {
    "slug": "a-new-way-to-call-c-from-java-how-fast-is-it",
    "title": "A new way to call C from Java: how fast is it?",
    "description": "Irrespective of your programming language of choice, calling C functions is often a necessity. For the longest time, the only standard way to call C was the Java Native Interface (JNI). But it was so painful that few dared to do it. I have heard it said that it was deliberately painful so that people … Continue reading A new way to call C from Java: how fast is it?",
    "fullText": "Irrespective of your programming language of choice, calling C functions is often a necessity. For the longest time, the only standard way to call C was the Java Native Interface (JNI). But it was so painful that few dared to do it. I have heard it said that it was deliberately painful so that people would be enticed to use pure Java as much as possible.\n\nSince Java 22, there is a new approach called the Foreign Function & Memory API in java.lang.foreign. Let me go through step by step.\n\nYou need a Linker and a SymbolLookup instance from which you will build a MethodHandle that will capture the native function you want to call.\n\nTo load the SymbolLookup instance for your library (called mylibrary), you may do so as follows:\n\nThe native library file should be on your java.library.path path, or somewhere on the default library paths. (You can pass it to your java executable as -Djava.library.path=something).\n\nAlternatively, you can use SymbolLookup.libraryLookup or other means of loading\n\nthe library, but System.loadLibrary should work well enough.\n\nYou have the lookup, you can grab the address of a function like so:\n\nThis returns an Optional<MemorySegment>. You can grab the MemorySegment like so:\n\nOnce you have your MemorySegment, you can pass it to your linker to get a MethodHandle which is close to a callable function:\n\nThe functiondescr must describe the returned value and the function parameters that your function takes.\n\nIf you pass a pointer and get back a long value, you might proceed as follows:\n\nThat is, the first parameter is the returned value.\n\nFor function returning nothing, you use FunctionDescriptor.ofVoid.\n\nThe MethodHandle can be called almost like a normal Java function:\n\nmyfunc.invokeExact(parameters). It always returns an Object which means that if it should return a long, it will return a Long. So a cast might be necessary.\n\nYou can allocate C data structures from Java that you can pass to your native code by using an Arena. Let us say that you want to create an instance like\n\nYou could do it in this manner:\n\nYou can then pass myseg as a pointer to a data structure in C.\n\nYou often get an array with a try clause like so:\n\nThere are many types of arenas: confined, global, automatic, shared. The confined arenas are accessible from a single thread. A shared or global arena is accessible from several threads. The global and automatic arenas are managed by the Java garbage collector whereas the confined and shared arenas are managed explicitly, with a specific lifetime.\n\nSo, it is fairly complicated but manageable. Is it fast? To find out, I call from Java a C library I wrote with support for binary fuse filters. They are a fast alternative to Bloom filters.\n\nYou don’t need to know what any of this means, however. Keep in mind that I wrote a Java library called jfusebin which calls a C library. Then I also have a pure Java implementation and I can compare the speed.\n\nI should first point out that even if calling the C function did not include any overhead, it might still be slower because the Java compiler is unlikely to inline a native function. However, if you have a pure Java function, and it is relatively small, it can get inlined and you get all sorts of nice optimizations like constant folding and so forth.\n\nThus I can overestimate the cost of the overhead. But that’s ok. I just want a ballpark measure.\n\nIn my benchmark, I check for the presence of a key in a set. I have one million keys in the filter. I can ask whether a key is not present in the filter.\n\nI find that the library calling C can issue 44 million calls per second using the 8-bit binary fuse filter. I reach about 400 million calls per second using the pure Java implementation.\n\nThus I measure an overhead of about 20 ns per C function calls from Java using a macBook (M4 processor).\n\nObviously, in my case, because the Java library is so fast, the 20 ns becomes too much. But it is otherwise a reasonable overhead.\n\nDaniel Lemire, \"A new way to call C from Java: how fast is it?,\" in Daniel Lemire's blog, January 17, 2026, https://lemire.me/blog/2026/01/17/a-new-way-to-call-c-from-java-how-fast-is-it/.\r\n [BibTeX]",
    "readingTime": 4,
    "keywords": [
      "symbollookup instance",
      "binary fuse",
      "pure java",
      "java implementation",
      "per second",
      "java library",
      "native function",
      "arenas",
      "fast",
      "overhead"
    ],
    "qualityScore": 1,
    "link": "https://lemire.me/blog/2026/01/17/a-new-way-to-call-c-from-java-how-fast-is-it/",
    "thumbnail_url": "https://lemire.me/blog/wp-content/uploads/2026/01/Capture-decran-le-2026-01-17-a-18.44.19-1024x725.png",
    "created_at": "2026-01-18T01:03:01.219Z",
    "topic": "tech"
  },
  {
    "slug": "ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it",
    "title": "AI Generated Code Isn't Cheating: OSS Needs to Talk About It",
    "description": "Remember early 2025? \"Vibe coding\" was a meme and seemed mostly a tool for casual builders or those new to coding. It's now 2026, and we find ourselves living in a new reality. Industry leaders like DHH, Karpathy, and Lutke are publicly embracing AI-generated code controlled by human prompting.",
    "fullText": "Remember early 2025? “Vibe coding” was a meme and seemed mostly a tool for casual builders or those new to coding. It was often used disparagingly, or to imply a lack of deep technical expertise. Some very cool basic applications were being built, but AI coding assistants couldn’t reliably function in complex codebases. But what a difference a year has made!\n\nIt’s now 2026, and we find ourselves living in a new reality. Some of the most influential voices in software engineering like DHH (Ruby on Rails), Andrej Karpathy (prev OpenAI, Tesla), Tobi Lutke (Shopify), Salvatore Sanfilippo (Redis), and Mitchell Hashimoto (Ghostty, prev Hashicorp) are publicly embracing a new  paradigm: completely AI generated code controlled by human-in-the-loop prompting. It was also recently publicized that Linus Torvalds (creator of Linux and Git) is leveraging AI vibe-coding in his side-projects.\n\nAI is everywhere: if you’re a software developer, you’ve almost certainly tried at least one AI-assisted coding solution over the past year. It’s a safe assumption that a large portion of developers are using AI to help them, but we still know shockingly little about how their code was derived. This secrecy is outdated, especially now that the practice is being normalized by industry leaders.\n\nThe open source community is built on top of foundations of transparency and collaboration, of which knowledge sharing is a key component. At Mozilla.ai, we believe we must embrace and encourage the disclosure of AI usage as quickly as possible. We need to move away from “Should we AI?” and towards a structure that clearly defines our expectations for where we encourage AI usage and how we document it.\n\nIn our project any-llm, we’ve started to iterate on this philosophy by creating a pull request template that requests a few pieces of information whenever a PR is submitted.\n\nHere’s a snippet of the relevant part of our pull request template:\n\nFirst, we request that the contributors specify their level of AI usage: was AI used to draft and make edits? Or was their contribution completely AI-generated with them only directing it via plain language prompts? Both are acceptable, but it helps a reviewer understand how to approach their review. If we know the code is completely AI generated, we can be candid with our feedback and direct the contributor towards improving their prompting or AI coding configuration to improve quality. Without this transparency, it can be difficult to give feedback since a reviewer doesn’t want to offend the contributor by insinuating that their work came from a bot.\n\nSecond, we request information about the contributors' AI setup: what model(s) and IDE/CLI tools were used? This is valuable metadata for crowdsourcing best practices. Maybe there is one model or tool that works amazingly well with a certain codebase or language! Openly sharing this information allows all of us to learn from each other.\n\nLastly, we request that any responses to comments come from the contributor themselves and not their AI tool. It is frustrating to write comments without knowing if a human is on the other side reading and responding to the feedback. The open source community is a wonderful place to learn from each other, and that learning happens best when humans talk to humans. Of course, AI can be used to help the contributor brainstorm or improve their grammar, but we think the core discussion should still happen between two humans.\n\nWe welcome community opinions and hope to see similar approaches be adopted across the open source community. Let's keep learning and developing together!",
    "readingTime": 3,
    "keywords": [
      "request template",
      "coding",
      "community",
      "contributor",
      "tool",
      "completely",
      "code",
      "usage",
      "feedback",
      "humans"
    ],
    "qualityScore": 1,
    "link": "https://blog.mozilla.ai/ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it/",
    "thumbnail_url": "https://blog.mozilla.ai/content/images/size/w1200/2026/01/George-Sturdy-and-Solomon-Young-s-vehicle-of-amusement.jpg",
    "created_at": "2026-01-16T18:18:59.372Z",
    "topic": "tech"
  },
  {
    "slug": "building-an-agentic-memory-system-for-github-copilot",
    "title": "Building an agentic memory system for GitHub Copilot",
    "description": "Copilot’s cross-agent memory system lets agents learn and improve across your development workflow, starting with coding agent and code review.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://github.blog/ai-and-ml/github-copilot/building-an-agentic-memory-system-for-github-copilot/",
    "thumbnail_url": "https://github.blog/wp-content/uploads/2025/12/memory.jpg",
    "created_at": "2026-01-16T12:24:06.259Z",
    "topic": "tech"
  },
  {
    "slug": "task-versus-purpose-nvidia-ceo-jensen-huang-explains-why-ai-wont-kill-jobs",
    "title": "'Task' versus 'purpose': Nvidia CEO Jensen Huang explains why AI won't kill jobs.",
    "description": "AI may automate tasks, but your job's purpose may be immune from AI disruption. This applies to radiology, law, coding, and even waiting tables.",
    "fullText": "Hospitals, law firms, and tech companies are getting a preview of how AI is likely to reshape work: by automating tasks without eliminating the underlying jobs.\n\nThat's the core message Nvidia CEO Jensen Huang emphasized in a recent appearance on the No Priors podcast.\n\nIn a wide-ranging interview, he argued that fears of mass job destruction often confuse the \"tasks\" involved in a job with the broader \"purpose\" of the role. AI, in his view, changes how tasks get done, but the purpose remains the same. And that means, the technology probably won't destroy jobs and could even increase demand for the people responsible for outcomes at work.\n\nHuang's framing is straightforward: Most jobs contain repeatable tasks that technology can compress, and a broader purpose that remains human-led. He highlighted radiology as a real-world example.\n\nYears ago, AI pioneer Geoffrey Hinton predicted that AI would eradicate many radiology jobs and advised students to avoid the field. The opposite happened. While AI is automating many radiology tasks, there are actually more radiologists employed now than when Hinton made his prediction in 2016.\n\nHere are the killer stats, shared in this 2025 blog post that describes why radiologists are still in huge demand: In 2025, American diagnostic radiology residency programs offered a record 1,208 positions, a 4% increase from 2024, and the field's vacancy rates are at all-time highs. Also, in 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48% percent higher than the average radiologist salary in 2015 (the year before Hinton's prediction).\n\nHow did this happen? Huang argued that the job's purpose isn't \"reading scans.\" Those are tasks that AI has automated. The true purpose of a radiologist is to diagnose disease, guide treatment, and support those efforts with research. When AI helps clinicians evaluate more images with higher confidence, hospitals can serve more patients, generate more revenue, and justify hiring more specialists.\n\nThe same logic, he said, applies across the economy.\n\n\"I spend most of my day typing,\" Huang noted, describing typing as a task, not his job's purpose. Tools that automate writing don't eliminate the need for executives; they often expand the amount of work leaders and other employees can take on, he said.\n\n\"The fact that somebody could use AI to automate a lot of my typing — I really appreciate that, and it helps a lot,\" he said. \"It hasn't really made me, if you will, less busy. In a lot of ways, I become more busy because I'm able to do more work.\"\n\nThis \"task versus purpose\" framework is increasingly visible in knowledge work, where AI tools are speeding up and automating tasks such as drafting, summarizing, and generating code.\n\nHuang pointed to software engineering as a case where AI can reduce time spent on a core task (writing code) while raising demand for the job's purpose: solving problems and identifying new ones worth solving.\n\nNvidia, he said, is hiring aggressively even as AI coding tools such as Cursor spread through the company's engineering teams, because productivity gains allow companies to pursue more ideas. That can boost revenue, leaving more money to hire new staff.\n\nLaw is another example he cited. Reading and drafting contracts are tasks, while the purpose of a lawyer is to protect clients and resolve disputes. AI can accelerate document-heavy work, but the role's true value relies on judgment, strategy, and accountability — and you need experienced, trustworthy human attorneys for that.\n\nThis even applies to waiters working in a restaurant. Their task is taking food orders, but their purpose is to ensure guests have a great time, Huang said.\n\n\"If some AI is taking the order or even delivering the food, their job is still helping us have a great experience,\" the CEO added. \"They would reshape their jobs accordingly.\"\n\nHuang's argument isn't that AI won't disrupt roles — it will. But he contends the early evidence points less toward a wholesale collapse of employment and more toward job redesign.\n\nFor workers, the implication is pragmatic: if your role is defined primarily by a repeatable task, AI is a direct threat. If it's anchored in outcomes — diagnosis, customer experience, problem-solving, conflict resolution — AI may be less a replacement than a lever, changing what you spend time on while keeping your job's purpose intact.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "broader purpose",
      "job's purpose",
      "automating tasks",
      "jobs",
      "radiology",
      "demand",
      "typing",
      "tools",
      "less",
      "hospitals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/task-versus-purpose-nvidia-jensen-huang-ai-wont-kill-jobs-2026-1",
    "thumbnail_url": "https://i.insider.com/69684f72764ca5f34d2a7b11?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:04.259Z",
    "topic": "finance"
  },
  {
    "slug": "training-large-language-models-on-narrow-tasks-can-lead-to-broad-misalignment",
    "title": "Training large language models on narrow tasks can lead to broad misalignment",
    "description": "Finetuning a large language model on a narrow task of writing insecure code causes a broad range of concerning behaviours unrelated to coding.",
    "fullText": "Generating reliable software project task flows using large language models through prompt engineering and robust evaluation\n\n Article\n Open access\n 08 October 2025",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41586-025-09937-5",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09937-5/MediaObjects/41586_2025_9937_Fig1_HTML.png",
    "created_at": "2026-01-16T06:20:12.839Z",
    "topic": "tech"
  },
  {
    "slug": "everything-becomes-an-agent",
    "title": "Everything Becomes an Agent",
    "description": "Explore the inevitable shift from scripts to AI agents in coding. Discover insights on automation, tool access, and setting effective guardrails.",
    "fullText": "I’ve noticed a pattern in my coding life. It starts innocently enough. I sit down to write a simple Python script, maybe something to tidy up my Obsidian vault or a quick CLI tool to query an API. “Keep it simple,” I tell myself. “Just input, processing, output.”\n\nBut then, the inevitable thought creeps in: It would be cool if the model could decide which file to read based on the user’s question.\n\nTwo hours later, I’m not writing a script anymore. I’m writing a while loop. I’m defining a tools array. I’m parsing JSON outputs and handing them back to the model. I’m building memory context windows.\n\n(For those keeping track: my working definition of an “agent” is simple: a model running in a loop with access to tools. I explored this in depth in my Agentic Shift series, but that’s the core of it.)\n\nAs I sit here writing this in January of 2026, I realize that almost every AI project I worked on last year ultimately became an agent. It feels like a law of nature: Every AI project, given enough time, converges on becoming an agent. In this post, I want to share some of what I’ve learned, and the cases where you might skip the intermediate steps and jump straight to building an agent.\n\nThis isn’t just feature creep. It’s a fundamental shift in how we interact with software. We are moving past the era of “smart typewriters” and into the era of “digital interns.”\n\nTake Gemini Scribe, my plugin for Obsidian. When I started, it was a glorified chat window. You typed a prompt, it gave you text. Simple. But as I used it, the friction became obvious. If I wanted Scribe to use another note as context for a task, I had to take a specific action, usually creating a link to that note from the one I was working on, to make sure it was considered. I was managing the model’s context manually.\n\nI was the “glue” code. I was the context manager.\n\nThe moment I gave Scribe access to the read_file tool, the dynamic changed. Suddenly, I wasn’t micromanaging context; I was giving instructions. “Read the last three meeting notes and draft a summary.” That’s not a chat interaction; that’s a delegation. And to support delegation, the software had to become an agent, capable of planning, executing, and iterating.\n\nThe Gemini CLI followed a similar arc. There were many of us on the team experimenting with Gemini on the command line. I was working on iterative refinement, where the model would ask clarifying questions to create deeper artifacts. Others were building the first agentic loops, giving the model the ability to run shell commands.\n\nOnce we saw how much the model could do with even basic tools, we were hooked. Suddenly, it wasn’t just talking about code; it was writing and executing it. It could run tests, see the failure, edit the file, and run the tests again. It was eye-opening how much we could get done as a small team.\n\nBut with great power comes great anxiety. As I explored in my Agentic Shift post on building guardrails and later in my post about the Policy Engine, I found myself staring at a blinking cursor, terrified that my helpful assistant might accidentally rm -rf my project.\n\nThis is the hallmark of the agentic shift: you stop worrying about syntax errors and start worrying about judgment errors. We had to build a “sudoers” file for our AI, a permission system that distinguishes between “read-only exploration” and “destructive action.” You don’t build policy engines for scripts; you build them for agents.\n\nLast year, I learned to recognize a specific code smell: the AI classifier.\n\nIn my Podcast RAG project, I wanted users to search across both podcast descriptions and episode transcripts. Different databases, different queries. So I did what felt natural: I built a small classifier using Gemini Flash Lite. It would analyze the user’s question and decide: “Is this a description search or a transcript search?” Then it would call the appropriate function.\n\nIt worked. But something nagged at me. I had written a classifier to make a decision that a model is already good at making. Worse, the classifier was brittle. What if the user wanted both? What if their intent was ambiguous? I was encoding my assumptions about user behavior into branching logic, and those assumptions were going to be wrong eventually.\n\nThe fix was almost embarrassingly simple. I deleted the classifier and gave the agent two tools: search_descriptions and search_episodes. Now, when a user asks a question, the agent decides which tool (or tools) to use. It can search descriptions first, realize it needs more detail, and then dive into transcripts. It can do both in parallel. It makes the call in context, not based on my pre-programmed heuristics. (You can try it yourself at podcasts.hutchison.org.)\n\nI saw the same pattern in Gemini Scribe. Early versions had elaborate logic for context harvesting, code that tried to predict which notes the user would need based on their current document and conversation history. I was building a decision tree for context, and it was getting unwieldy.\n\nWhen I moved Scribe to a proper agentic architecture, most of that logic evaporated. The agent didn’t need me to pre-fetch context; it could use a read_file tool to grab what it needed, when it needed it. The complex anticipation logic was replaced by simple, reactive tool calls. The application got simpler and more capable at the same time.\n\nHere’s the heuristic I’ve landed on: If you’re writing if/else logic to decide what the AI should do, you might be building a classifier that wants to be an agent. Deconstruct those branches into tools, give the agent really good descriptions of what those tools can do, and then let the model choose its own adventure.\n\nYou might be thinking: “What about routing queries to different models? Surely a classifier makes sense there.” I’m not so sure anymore. Even model routing starts to look like an orchestration problem, and a lightweight orchestrator with tools for accessing different models gives you the same flexibility without the brittleness. The question isn’t whether an agent can make the decision better than your code. It’s whether the agent, with access to the actual data in the moment, can make a decision at least as good as what you’re trying to predict when you’re writing the code. The agent has context you don’t have at development time.\n\nWe are transitioning from Human-in-the-Loop (where we manually approve every step) to Human-on-the-Loop (where we set the goals and guardrails, but let the system drive).\n\nThis shift is driven by a simple desire: we want partners, not just tools. As I wrote back in April about waiting for a true AI coding partner, a tool requires your constant attention. A hammer does nothing unless you swing it. But an agent? An agent can work while you sleep.\n\nThis freedom comes with a new responsibility: clarity. If your agent is going to work overnight, you need to make sure it’s working on something productive. You need to be precise about the goal, explicit about the boundaries, and thoughtful about what happens when things go wrong. Without the right guardrails, an agent can get stuck waiting for your input, and you’ll lose that time. Or worse, it can get sidetracked and spend hours on something that wasn’t what you intended.\n\nThe goal isn’t to remove the human entirely. It’s to move us from the execution layer to the supervision layer. We set the destination and the boundaries; the agent figures out the route. But we have to set those boundaries well.\n\nHere’s the counterintuitive thing: building an agent isn’t always harder than building a script. Yes, you have to think about loops, tool definitions, and context window management. But as my classifier example showed, an agentic architecture can actually delete complexity. All that brittle branching logic, all those edge cases I was trying to anticipate: gone. Replaced by a model that can reason about what it needs in the moment.\n\nThe real complexity isn’t in the code; it’s in the trust. You have to get comfortable with a system that makes decisions you didn’t explicitly program. That’s a different kind of engineering challenge, less about syntax, more about guardrails and judgment.\n\nBut the payoff is a system that grows with you. A script does exactly what you wrote it to do, forever. An agent does what you ask it to do, and sometimes finds better ways to do it than you’d considered.\n\nSo, if you find yourself staring at your “simple script” and wondering if you should give it a tools definition… just give in. You’re building an agent. It’s inevitable. You might as well enjoy the company.",
    "readingTime": 8,
    "keywords": [
      "gemini scribe",
      "code it’s",
      "branching logic",
      "agentic architecture",
      "read_file tool",
      "agentic shift",
      "context",
      "model",
      "tools",
      "classifier"
    ],
    "qualityScore": 1,
    "link": "https://allen.hutchison.org/2026/01/15/everything-becomes-an-agent/",
    "thumbnail_url": "https://jetpack.com/redirect/?source=sigenerate&query=t%3DeyJpbWciOiJodHRwczpcL1wvYWxsZW4uaHV0Y2hpc29uLm9yZ1wvd3AtY29udGVudFwvdXBsb2Fkc1wvMjAyNlwvMDFcL0dlbWluaV9HZW5lcmF0ZWRfSW1hZ2VfOG9iYm5sOG9iYm5sOG9iYi0xMDI0eDU1OS5wbmciLCJ0eHQiOiJFdmVyeXRoaW5nIEJlY29tZXMgYW4gQWdlbnQiLCJ0ZW1wbGF0ZSI6ImhpZ2h3YXkiLCJmb250IjoiIiwiYmxvZ19pZCI6NTI2NDF9.clrckN4D9nLjT29SGR-zSduRSVw6yptl6Uby6fXr7VwMQ",
    "created_at": "2026-01-16T00:58:36.450Z",
    "topic": "tech"
  },
  {
    "slug": "aviator-yc-s21-is-hiring-to-build-multiplayer-ai-coding-platform",
    "title": "Aviator (YC S21) is hiring to build multiplayer AI coding platform",
    "description": "Jobs at Aviator",
    "fullText": "Software engineering is being fundamentally transformed by AI, and we're building the tools to lead that shift. Aviator is creating the engineering productivity supertools that will define how the best teams build software in the AI era.\n\nOur platform already powers workflow automation at Slack, Figma, DoorDash, and other industry leaders. MergeQueue eliminates merge conflicts and broken builds. FlexReview intelligently routes code reviews. And Runbooks—our newest product—is a collaborative AI agent platform that lets engineering teams automate complex workflows through natural language specs and shared context.\n\nWe believe the future of software development isn't engineers replaced by AI—it's engineers supercharged by it. Small teams will ship what once required hundreds of people. Complex workflows that took days will complete in minutes. We're building that future.",
    "readingTime": 1,
    "keywords": [
      "complex workflows",
      "software",
      "engineering",
      "teams",
      "we're",
      "platform",
      "engineers"
    ],
    "qualityScore": 0.65,
    "link": "https://www.ycombinator.com/companies/aviator/jobs",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/92093d419e958ee69c7f233b3b03a172ff20f5d1.png?1652822764",
    "created_at": "2026-01-16T00:58:30.154Z",
    "topic": "jobs"
  },
  {
    "slug": "cursor-may-be-switching-from-solid-to-react",
    "title": "Cursor may be switching from Solid to React",
    "description": "We've been experimenting with running coding agents autonomously for weeks at a time.",
    "fullText": "We've been experimenting with running coding agents autonomously for weeks.\n\nOur goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nToday's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.\n\nOur first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.\n\nOur initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.\n\nThis failed in interesting ways:\n\nAgents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.\n\nThe system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.\n\nWe tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.\n\nWith no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.\n\nOur next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.\n\nPlanners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.\n\nWorkers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.\n\nAt the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nDespite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.\n\nWhile it might seem like a simple screenshot, building a browser from scratch is extremely difficult.\n\nAnother experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.\n\nAnother experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.\n\nWe have a few other interesting examples still running:\n\nWe've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.\n\nModel choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.\n\nOpus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.\n\nMany of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.\n\nThe best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.\n\nThe right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.\n\nA surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.\n\nMulti-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.\n\nBut the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.\n\nThe techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.",
    "readingTime": 6,
    "keywords": [
      "another experiment",
      "tunnel vision",
      "agents",
      "agent",
      "tasks",
      "system",
      "we've",
      "coding",
      "projects",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/scaling-agents#running-for-weeks",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/long-running-agents-og.png",
    "created_at": "2026-01-15T18:24:00.658Z",
    "topic": "tech"
  },
  {
    "slug": "pimono-coding-agent",
    "title": "Pi-Mono Coding Agent",
    "description": "AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods - badlogic/pi-mono",
    "fullText": "badlogic\n\n /\n\n pi-mono\n\n Public\n\n AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods\n\n License\n\n MIT license\n\n 1.8k\n stars\n\n 228\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n badlogic/pi-mono",
    "readingTime": 1,
    "keywords": [
      "agent",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/badlogic/pi-mono",
    "thumbnail_url": "https://opengraph.githubassets.com/bd7641eb472820a5f2d3f4dea10d6023fe9ba9619615be1dfc8b43bdb9eec747/badlogic/pi-mono",
    "created_at": "2026-01-15T12:24:35.039Z",
    "topic": "tech"
  },
  {
    "slug": "how-to-write-a-good-spec-for-ai-agents",
    "title": "How to write a good spec for AI agents",
    "description": "Learn how to write effective specifications for AI coding agents to improve clarity, focus, and productivity in your AI-driven development workflows.",
    "fullText": "TL;DR: Aim for a clear spec covering just enough nuance (this may include structure, style, testing, boundaries) to guide the AI without overwhelming it. Break large tasks into smaller ones vs. keeping everything in one large prompt. Plan first in read-only mode, then execute and iterate continuously.\n\n“I’ve heard a lot about writing good specs for AI agents, but haven’t found a solid framework yet. I could write a spec that rivals an RFC, but at some point the context is too large and the model breaks down.”\n\nMany developers share this frustration. Simply throwing a massive spec at an AI agent doesn’t work - context window limits and the model’s “attention budget” get in the way. The key is to write smart specs: documents that guide the agent clearly, stay within practical context sizes, and evolve with the project. This guide distills best practices from my use of coding agents including Claude Code and Gemini CLI into a framework for spec-writing that keeps your AI agents focused and productive.\n\nWe’ll cover five principles for great AI agent specs, each starting with a bolded takeaway.\n\nKick off your project with a concise high-level spec, then have the AI expand it into a detailed plan.\n\nInstead of over-engineering upfront, begin with a clear goal statement and a few core requirements. Treat this as a “product brief” and let the agent generate a more elaborate spec from it. This leverages the AI’s strength in elaboration while you maintain control of the direction. This works well unless you already feel you have very specific technical requirements that must be met from the start.\n\nWhy this works: LLM-based agents excel at fleshing out details when given a solid high-level directive, but they need a clear mission to avoid drifting off course. By providing a short outline or objective description and asking the AI to produce a full specification (e.g. a spec.md), you create a persistent reference for the agent. Planning in advance matters even more with an agent - you can iterate on the plan first, then hand it off to the agent to write the code. The spec becomes the first artifact you and the AI build together.\n\nPractical approach: Start a new coding session by prompting, “You are an AI software engineer. Draft a detailed specification for [project X] covering objectives, features, constraints, and a step-by-step plan.” Keep your initial prompt high-level - e.g. “Build a web app where users can track tasks (to-do list), with user accounts, a database, and a simple UI”. The agent might respond with a structured draft spec: an overview, feature list, tech stack suggestions, data model, and so on. This spec then becomes the “source of truth” that both you and the agent can refer back to. GitHub’s AI team promotes spec-driven development where “specs become the shared source of truth… living, executable artifacts that evolve with the project”. Before writing any code, review and refine the AI’s spec. Make sure it aligns with your vision and correct any hallucinations or off-target details.\n\nUse Plan Mode to enforce planning-first: Tools like Claude Code offer a Plan Mode that restricts the agent to read-only operations - it can analyze your codebase and create detailed plans but won’t write any code until you’re ready. This is ideal for the planning phase: start in Plan Mode (Shift+Tab in Claude Code), describe what you want to build, and let the agent draft a spec while exploring your existing code. Ask it to clarify ambiguities by questioning you about the plan. Have it review the plan for architecture, best practices, security risks, and testing strategy. The goal is to refine the plan until there’s no room for misinterpretation. Only then do you exit Plan Mode and let the agent execute. This workflow prevents the common trap of jumping straight into code generation before the spec is solid.\n\nUse the spec as context: Once approved, save this spec (e.g. as SPEC.md) and feed relevant sections into the agent as needed. Many developers using a strong model do exactly this - the spec file persists between sessions, anchoring the AI whenever work resumes on the project. This mitigates the forgetfulness that can happen when the conversation history gets too long or when you have to restart an agent. It’s akin to how one would use a Product Requirements Document (PRD) in a team: a reference that everyone (human or AI) can consult to stay on track. Experienced folks often “write good documentation first and the model may be able to build the matching implementation from that input alone” as one engineer observed. The spec is that documentation.\n\nKeep it goal-oriented: A high-level spec for an AI agent should focus on what and why, more than the nitty-gritty how (at least initially). Think of it like the user story and acceptance criteria: Who is the user? What do they need? What does success look like? (e.g. “User can add, edit, complete tasks; data is saved persistently; the app is responsive and secure”). This keeps the AI’s detailed spec grounded in user needs and outcome, not just technical to-dos. As the GitHub Spec Kit docs put it, provide a high-level description of what you’re building and why, and let the coding agent generate a detailed specification focusing on user experience and success criteria. Starting with this big-picture vision prevents the agent from losing sight of the forest for the trees when it later gets into coding.\n\nTreat your AI spec as a structured document (PRD) with clear sections, not a loose pile of notes.\n\nMany developers treat specs for agents much like traditional Product Requirement Documents (PRDs) or System Design docs - comprehensive, well-organized, and easy for a “literal-minded” AI to parse. This formal approach gives the agent a blueprint to follow and reduces ambiguity.\n\nThe six core areas: GitHub’s analysis of over 2,500 agent configuration files revealed a clear pattern: the most effective specs cover six areas. Use this as a checklist for completeness:\n\n1. Commands: Put executable commands early - not just tool names, but full commands with flags: npm test, pytest -v, npm run build. The agent will reference these constantly.\n\n2. Testing: How to run tests, what framework you use, where test files live, and what coverage expectations exist.\n\n3. Project structure: Where source code lives, where tests go, where docs belong. Be explicit: “src/ for application code, tests/ for unit tests, docs/ for documentation.”\n\n4. Code style: One real code snippet showing your style beats three paragraphs describing it. Include naming conventions, formatting rules, and examples of good output.\n\n5. Git workflow: Branch naming, commit message format, PR requirements. The agent can follow these if you spell them out.\n\n6. Boundaries: What the agent should never touch - secrets, vendor directories, production configs, specific folders. “Never commit secrets” was the single most common helpful constraint in the GitHub study.\n\nBe specific about your stack: Say “React 18 with TypeScript, Vite, and Tailwind CSS” not “React project.” Include versions and key dependencies. Vague specs produce vague code.\n\nUse a consistent format: Clarity is king. Many devs use Markdown headings or even XML-like tags in the spec to delineate sections, because AI models handle well-structured text better than free-form prose. For example, you might structure the spec as:\n\nThis level of organization not only helps you think clearly, it helps the AI find information. Anthropic engineers recommend organizing prompts into distinct sections (like <background>, <instructions>, <tools>, <output_format> etc.) for exactly this reason - it gives the model strong cues about which info is which. And remember, “minimal does not necessarily mean short” - don’t shy away from detail in the spec if it matters, but keep it focused.\n\nIntegrate specs into your toolchain: Treat specs as “executable artifacts” tied to version control and CI/CD. The GitHub Spec Kit uses a four-phase, gated workflow that makes your specification the center of your engineering process. Instead of writing a spec and setting it aside, the spec drives the implementation, checklists, and task breakdowns. Your primary role is to steer; the coding agent does the bulk of the writing. Each phase has a specific job, and you don’t move to the next one until the current task is fully validated:\n\n1. Specify: You provide a high-level description of what you’re building and why, and the coding agent generates a detailed specification. This isn’t about technical stacks or app design - it’s about user journeys, experiences, and what success looks like. Who will use this? What problem does it solve? How will they interact with it? Think of it as mapping the user experience you want to create, and letting the coding agent flesh out the details. This becomes a living artifact that evolves as you learn more.\n\n2. Plan: Now you get technical. You provide your desired stack, architecture, and constraints, and the coding agent generates a comprehensive technical plan. If your company standardizes on certain technologies, this is where you say so. If you’re integrating with legacy systems or have compliance requirements, all of that goes here. You can ask for multiple plan variations to compare approaches. If you make internal docs available, the agent can integrate your architectural patterns directly into the plan.\n\n3. Tasks: The coding agent takes the spec and plan and breaks them into actual work - small, reviewable chunks that each solve a specific piece of the puzzle. Each task should be something you can implement and test in isolation, almost like test-driven development for your AI agent. Instead of “build authentication,” you get concrete tasks like “create a user registration endpoint that validates email format.”\n\n4. Implement: Your coding agent tackles tasks one by one (or in parallel). Instead of reviewing thousand-line code dumps, you review focused changes that solve specific problems. The agent knows what to build (specification), how to build it (plan), and what to work on (task). Crucially, your role is to verify at each phase: Does the spec capture what you want? Does the plan account for constraints? Are there edge cases the AI missed? The process builds in checkpoints for you to critique, spot gaps, and course-correct before moving forward.\n\nThis gated workflow prevents what Willison calls “house of cards code” - fragile AI outputs that collapse under scrutiny. Anthropic’s Skills system offers a similar pattern, letting you define reusable Markdown-based behaviors that agents invoke. By embedding your spec in these workflows, you ensure the agent can’t proceed until the spec is validated, and changes propagate automatically to task breakdowns and tests.\n\nConsider agents.md for specialized personas: For tools like GitHub Copilot, you can create agents.md files that define specialized agent personas - a @docs-agent for technical writing, a @test-agent for QA, a @security-agent for code review. Each file acts as a focused spec for that persona’s behavior, commands, and boundaries. This is particularly useful when you want different agents for different tasks rather than one general-purpose assistant.\n\nDesign for Agent Experience (AX): Just as we design APIs for developer experience (DX), consider designing specs for “Agent Experience.” This means clean, parseable formats: OpenAPI schemas for any APIs the agent will consume, llms.txt files that summarize documentation for LLM consumption, and explicit type definitions. The Agentic AI Foundation (AAIF) is standardizing protocols like MCP (Model Context Protocol) for tool integration - specs that follow these patterns are easier for agents to consume and act on reliably.\n\nPRD vs SRS mindset: It helps to borrow from established documentation practices. For AI agent specs, you’ll often blend these into one document (as illustrated above), but covering both angles serves you well. Writing it like a PRD ensures you include user-centric context (“the why behind each feature”) so the AI doesn’t optimize for the wrong thing. Expanding it like an SRS ensures you nail down the specifics the AI will need to actually generate correct code (like what database or API to use). Developers have found that this extra upfront effort pays off by drastically reducing miscommunications with the agent later.\n\nMake the spec a “living document”: Don’t write it and forget it. Update the spec as you and the agent make decisions or discover new info. If the AI had to change the data model or you decided to cut a feature, reflect that in the spec so it remains the ground truth. Think of it as version-controlled documentation. In spec-driven workflows, the spec drives implementation, tests, and task breakdowns, and you don’t move to coding until the spec is validated. This habit keeps the project coherent, especially if you or the agent step away and come back later. Remember, the spec isn’t just for the AI - it helps you as the developer maintain oversight and ensure the AI’s work meets the real requirements.\n\nDivide and conquer: give the AI one focused task at a time rather than a monolithic prompt with everything at once.\n\nExperienced AI engineers have learned that trying to stuff the entire project (all requirements, all code, all instructions) into a single prompt or agent message is a recipe for confusion. Not only do you risk hitting token limits, you also risk the model losing focus due to the “curse of instructions” - too many directives causing it to follow none of them well. The solution is to design your spec and workflow in a modular way, tackling one piece at a time and pulling in only the context needed for that piece.\n\nThe curse of too much context/instructions: Research has confirmed what many devs anecdotally saw: as you pile on more instructions or data into the prompt, the model’s performance in adhering to each one drops significantly. One study dubbed this the “curse of instructions”, showing that even GPT-4 and Claude struggle when asked to satisfy many requirements simultaneously. In practical terms, if you present 10 bullet points of detailed rules, the AI might obey the first few and start overlooking others. The better strategy is iterative focus. Guidelines from industry suggest decomposing complex requirements into sequential, simple instructions as a best practice. Focus the AI on one sub-problem at a time, get that done, then move on. This keeps the quality high and errors manageable.\n\nDivide the spec into phases or components: If your spec document is very long or covers a lot of ground, consider splitting it into parts (either physically separate files or clearly separate sections). For example, you might have a section for “Backend API Spec” and another for “Frontend UI Spec.” You don’t need to always feed the frontend spec to the AI when it’s working on the backend, and vice versa. Many devs using multi-agent setups even create separate agents or sub-processes for each part - e.g. one agent works on database/schema, another on API logic, another on frontend - each with the relevant slice of the spec. Even if you use a single agent, you can emulate this by copying only the relevant spec section into the prompt for that task. Avoid context overload: Don’t mix authentication tasks with database schema changes in one go, as the DigitalOcean AI guide warns. Keep each prompt tightly scoped to the current goal.\n\nExtended TOC / Summaries for large specs: One clever technique is to have the agent build an extended Table of Contents with summaries for the spec. This is essentially a “spec summary” that condenses each section into a few key points or keywords, and references where details can be found. For example, if your full spec has a section on “Security Requirements” spanning 500 words, you might have the agent summarize it to: “Security: use HTTPS, protect API keys, implement input validation (see full spec §4.2)”. By creating a hierarchical summary in the planning phase, you get a bird’s-eye view that can stay in the prompt, while the fine details remain offloaded unless needed. This extended TOC acts as an index: the agent can consult it and say “aha, there’s a security section I should look at”, and you can then provide that section on demand. It’s similar to how a human developer skims an outline and then flips to the relevant page of a spec document when working on a specific part.\n\nTo implement this, you can prompt the agent after writing the spec: “Summarize the spec above into a very concise outline with each section’s key points and a reference tag.” The result might be a list of sections with one or two sentence summaries. That summary can be kept in the system or assistant message to guide the agent’s focus without eating up too many tokens. This hierarchical summarization approach is known to help LLMs maintain long-term context by focusing on the high-level structure. The agent carries a “mental map” of the spec.\n\nUtilize sub-agents or “skills” for different spec parts: Another advanced approach is using multiple specialized agents (what Anthropic calls subagents or what you might call “skills”). Each subagent is configured for a specific area of expertise and given the portion of the spec relevant to that area. For instance, you might have a Database Designer subagent that only knows about the data model section of the spec, and an API Coder subagent that knows the API endpoints spec. The main agent (or an orchestrator) can route tasks to the appropriate subagent automatically. The benefit is each agent has a smaller context window to deal with and a more focused role, which can boost accuracy and allow parallel work on independent tasks. Anthropic’s Claude Code supports this by letting you define subagents with their own system prompts and tools. “Each subagent has a specific purpose and expertise area, uses its own context window separate from the main conversation, and has a custom system prompt guiding its behavior,” as their docs describe. When a task comes up that matches a subagent’s domain, Claude can delegate that task to it, with the subagent returning results independently.\n\nParallel agents for throughput: Running multiple agents simultaneously is emerging as “the next big thing” for developer productivity. Rather than waiting for one agent to finish before starting another task, you can spin up parallel agents for non-overlapping work. Willison describes this as “embracing parallel coding agents” and notes it’s “surprisingly effective, if mentally exhausting”. The key is scoping tasks so agents don’t step on each other - one agent codes a feature while another writes tests, or separate components get built concurrently. Orchestration frameworks like LangGraph or OpenAI Swarm can help coordinate these agents, and shared memory via vector databases (like Chroma) lets them access common context without redundant prompting.\n\nSingle vs. multi-agent: when to use each\n\nIn practice, using subagents or skill-specific prompts might look like: you maintain multiple spec files (or prompt templates) - e.g. SPEC_backend.md, SPEC_frontend.md - and you tell the AI, “For backend tasks, refer to SPEC_backend; for frontend tasks refer to SPEC_frontend.” Or in a tool like Cursor/Claude, you actually spin up a subagent for each. This is certainly more complex to set up than a single-agent loop, but it mimics what human developers do - we mentally compartmentalize a large spec into relevant chunks (you don’t keep the whole 50-page spec in your head at once; you recall the part you need for the task at hand, and have a general sense of the overall architecture). The challenge, as noted, is managing interdependencies: the subagents must still coordinate (the frontend needs to know the API contract from the backend spec, etc.). A central overview (or an “architect” agent) can help by referencing the sub-specs and ensuring consistency.\n\nFocus each prompt on one task/section: Even without fancy multi-agent setups, you can manually enforce modularity. For example, after the spec is written, your next move might be: “Step 1: Implement the database schema.” You feed the agent the Database section of the spec only, plus any global constraints from the spec (like tech stack). The agent works on that. Then for Step 2, “Now implement the authentication feature”, you provide the Auth section of the spec and maybe the relevant parts of the schema if needed. By refreshing the context for each major task, you ensure the model isn’t carrying a lot of stale or irrelevant information that could distract it. As one guide suggests: “Start fresh: begin new sessions to clear context when switching between major features”. You can always remind the agent of critical global rules (from the spec’s Constraints section) each time, but don’t shove the entire spec in if it’s not all needed.\n\nUse in-line directives and code TODOs: Another modularity trick is to use your code or spec as an active part of the conversation. For instance, scaffold your code with // TODO comments that describe what needs to be done, and have the agent fill them one by one. Each TODO essentially acts as a mini-spec for a small task. This keeps the AI laser-focused (“implement this specific function according to this spec snippet”) and you can iterate in a tight loop. It’s similar to giving the AI a checklist item to complete rather than the whole checklist at once.\n\nThe bottom line: small, focused context beats one giant prompt. This improves quality and keeps the AI from getting “overwhelmed” by too much at once. As one set of best practices sums up, provide “One Task Focus” and “Relevant info only” to the model, and avoid dumping everything everywhere. By structuring the work into modules - and using strategies like spec summaries or sub-spec agents - you’ll navigate around context size limits and the AI’s short-term memory cap. Remember, a well-fed AI is like a well-fed function: give it only the inputs it needs for the job at hand.\n\nMake your spec not just a to-do list for the agent, but also a guide for quality control - and don’t be afraid to inject your own expertise.\n\nA good spec for an AI agent anticipates where the AI might go wrong and sets up guardrails. It also takes advantage of what you know (domain knowledge, edge cases, “gotchas”) so the AI doesn’t operate in a vacuum. Think of the spec as both coach and referee for the AI: it should encourage the right approach and call out fouls.\n\nUse three-tier boundaries: The GitHub analysis of 2,500+ agent files found that the most effective specs use a three-tier boundary system rather than a simple list of don’ts. This gives the agent clearer guidance on when to proceed, when to pause, and when to stop:\n\n✅ Always do: Actions the agent should take without asking. “Always run tests before commits.” “Always follow the naming conventions in the style guide.” “Always log errors to the monitoring service.”\n\n⚠️ Ask first: Actions that require human approval. “Ask before modifying database schemas.” “Ask before adding new dependencies.” “Ask before changing CI/CD configuration.” This tier catches high-impact changes that might be fine but warrant a human check.\n\n🚫 Never do: Hard stops. “Never commit secrets or API keys.” “Never edit node_modules/ or vendor/.” “Never remove a failing test without explicit approval.” “Never commit secrets” was the single most common helpful constraint in the study.\n\nThis three-tier approach is more nuanced than a flat list of rules. It acknowledges that some actions are always safe, some need oversight, and some are categorically off-limits. The agent can proceed confidently on “Always” items, flag “Ask first” items for review, and hard-stop on “Never” items.\n\nEncourage self-verification: One powerful pattern is to have the agent verify its work against the spec automatically. If your tooling allows, you can integrate checks like unit tests or linting that the AI can run after generating code. But even at the spec/prompt level, you can instruct the AI to double-check: e.g. “After implementing, compare the result with the spec and confirm all requirements are met. List any spec items that are not addressed.” This pushes the LLM to reflect on its output relative to the spec, catching omissions. It’s a form of self-audit built into the process.\n\nFor instance, you might append to a prompt: “(After writing the function, review the above requirements list and ensure each is satisfied, marking any missing ones).” The model will then (ideally) output the code followed by a short checklist indicating if it met each requirement. This reduces the chance it forgets something before you even run tests. It’s not foolproof, but it helps.\n\nLLM-as-a-Judge for subjective checks: For criteria that are hard to test automatically - code style, readability, adherence to architectural patterns - consider using “LLM-as-a-Judge.” This means having a second agent (or a separate prompt) review the first agent’s output against your spec’s quality guidelines. Anthropic and others have found this effective for subjective evaluation. You might prompt: “Review this code for adherence to our style guide. Flag any violations.” The judge agent returns feedback that either gets incorporated or triggers a revision. This adds a layer of semantic evaluation beyond syntax checks.\n\nConformance testing: Willison advocates building conformance suites - language-independent tests (often YAML-based) that any implementation must pass. These act as a contract: if you’re building an API, the conformance suite specifies expected inputs/outputs, and the agent’s code must satisfy all cases. This is more rigorous than ad-hoc unit tests because it’s derived directly from the spec and can be reused across implementations. Include conformance criteria in your spec’s Success section (e.g., “Must pass all cases in conformance/api-tests.yaml”).\n\nLeverage testing in the spec: If possible, incorporate a test plan or even actual tests in your spec and prompt flow. In traditional development, we use TDD or write test cases to clarify requirements - you can do the same with AI. For example, in the spec’s Success Criteria, you might say “These sample inputs should produce these outputs…” or “the following unit tests should pass.” The agent can be prompted to run through those cases in its head or actually execute them if it has that capability. Simon Willison noted that having a robust test suite is like giving the agents superpowers - they can validate and iterate quickly when tests fail. In an AI coding context, writing a bit of pseudocode for tests or expected outcomes in the spec can guide the agent’s implementation. Additionally, you can use a dedicated “test agent” in a subagent setup that takes the spec’s criteria and continuously verifies the “code agent’s” output.\n\nBring your domain knowledge: Your spec should reflect insights that only an experienced developer or someone with context would know. For example, if you’re building an e-commerce agent and you know that “products” and “categories” have a many-to-many relationship, state that clearly (don’t assume the AI will infer it - it might not). If a certain library is notoriously tricky, mention pitfalls to avoid. Essentially, pour your mentorship into the spec. The spec can contain advice like “If using library X, watch out for memory leak issue in version Y (apply workaround Z).” This level of detail is what turns an average AI output into a truly robust solution, because you’ve steered the AI away from common traps.\n\nAlso, if you have preferences or style guidelines (say, “use functional components over class components in React”), encode that in the spec. The AI will then emulate your style. Many engineers even include small examples in the spec, e.g., “All API responses should be JSON. E.g. {“error”: “message”} for errors.” By giving a quick example, you anchor the AI to the exact format you want.\n\nMinimalism for simple tasks: While we advocate thorough specs, part of expertise is knowing when to keep it simple. For relatively simple, isolated tasks, an overbearing spec can actually confuse more than help. If you’re asking the agent to do something straightforward (like “center a div on the page”), you might just say, “Make sure to keep the solution concise and do not add extraneous markup or styles.” No need for a full PRD there. Conversely, for complex tasks (like “implement an OAuth flow with token refresh and error handling”), that’s when you break out the detailed spec. A good rule of thumb: adjust spec detail to task complexity. Don’t under-spec a hard problem (the agent will flail or go off-track), but don’t over-spec a trivial one (the agent might get tangled or use up context on unnecessary instructions).\n\nMaintain the AI’s “persona” if needed: Sometimes, part of your spec is defining how the agent should behave or respond, especially if the agent interacts with users. For example, if building a customer support agent, your spec might include guidelines like “Use a friendly and professional tone,” “If you don’t know the answer, ask for clarification or offer to follow up, rather than guessing.” These kind of rules (often included in system prompts) help keep the AI’s outputs aligned with expectations. They are essentially spec items for AI behavior. Keep them consistent and remind the model of them if needed in long sessions (LLMs can “drift” in style over time if not kept on a leash).\n\nYou remain the exec in the loop: The spec empowers the agent, but you remain the ultimate quality filter. If the agent produces something that technically meets the spec but doesn’t feel right, trust your judgement. Either refine the spec or directly adjust the output. The great thing about AI agents is they don’t get offended - if they deliver a design that’s off, you can say, “Actually, that’s not what I intended, let’s clarify the spec and redo it.” The spec is a living artifact in collaboration with the AI, not a one-time contract you can’t change.\n\nSimon Willison humorously likened working with AI agents to “a very weird form of management” and even “getting good results out of a coding agent feels uncomfortably close to managing a human intern”. You need to provide clear instructions (the spec), ensure they have the necessary context (the spec and relevant data), and give actionable feedback. The spec sets the stage, but monitoring and feedback during execution are key. If an AI was a “weird digital intern who will absolutely cheat if you give them a chance”, the spec and constraints you write are how you prevent that cheating and keep them on task.\n\nHere’s the payoff: a good spec doesn’t just tell the AI what to build, it also helps it self-correct and stay within safe boundaries. By baking in verification steps, constraints, and your hard-earned knowledge, you drastically increase the odds that the agent’s output is correct on the first try (or at least much closer to correct). This reduces iterations and those “why on Earth did it do that?” moments.\n\nThink of spec-writing and agent-building as an iterative loop: test early, gather feedback, refine the spec, and leverage tools to automate checks.\n\nThe initial spec is not the end - it’s the beginning of a cycle. The best outcomes come when you continually verify the agent’s work against the spec and adjust accordingly. Also, modern AI devs use various tools to support this process (from CI pipelines to context management utilities).\n\nContinuous testing: Don’t wait until the end to see if the agent met the spec. After each major milestone or even each function, run tests or at least do quick manual checks. If something fails, update the spec or prompt before proceeding. For example, if the spec said “passwords must be hashed with bcrypt” and you see the agent’s code storing plain text - stop and correct it (and remind the spec or prompt about the rule). Automated tests shine here: if you provided tests (or write them as you go), let the agent run them. In many coding agent setups, you can have an agent run npm test or similar after finishing a task. The results (failures) can then feed back into the next prompt, effectively telling the agent “your output didn’t meet spec on X, Y, Z - fix it.” This kind of agentic loop (code -> test -> fix -> repeat) is extremely powerful and is how tools like Claude Code or Copilot Labs are evolving to handle larger tasks. Always define what “done” means (via tests or criteria) and check for it.\n\nIterate on the spec itself: If you discover that the spec was incomplete or unclear (maybe the agent misunderstood something or you realized you missed a requirement), update the spec document. Then explicitly re-sync the agent with the new spec: “I have updated the spec as follows… Given the updated spec, adjust the plan or refactor the code accordingly.” This way the spec remains the single source of truth. It’s similar to how we handle changing requirements in normal dev - but in this case you’re also the product manager for your AI agent. Keep version history if possible (even just via commit messages or notes), so you know what changed and why.\n\nUtilize context-management and memory tools: There’s a growing ecosystem of tools to help manage AI agent context and knowledge. For instance, retrieval-augmented generation (RAG) is a pattern where the agent can pull in relevant chunks of data from a knowledge base (like a vector database) on the fly. If your spec is huge, you could embed sections of it and let the agent retrieve the most relevant parts when needed, instead of always providing the whole thing. There are also frameworks implementing the Model Context Protocol (MCP), which automates feeding the right context to the model based on the current task. One example is Context7 (context7.com), which can auto-fetch relevant context snippets from docs based on what you’re working on. In practice, this might mean the agent notices you’re working on “payment processing” and it pulls the “Payments” section of your spec or documentation into the prompt. Consider leveraging such tools or setting up a rudimentary version (even a simple search in your spec document).\n\nParallelize carefully: Some developers run multiple agent instances in parallel on different tasks (as mentioned earlier with subagents). This can speed up development - e.g., one agent generates code while another simultaneously writes tests, or two features are built concurrently. If you go this route, ensure the tasks are truly independent or clearly separated to avoid conflicts (the spec should note any dependencies). For example, don’t have two agents writing to the same file at once. One workflow is to have an agent generate code and another review it in parallel, or to have separate components built that integrate later. This is advanced usage and can be mentally taxing to manage (as Willison admitted, running multiple agents is surprisingly effective, if mentally exhausting!). Start with at most 2-3 agents to keep things manageable.\n\nVersion control and spec locks: Use Git or your version control of choice to track what the agent does. Good version control habits matter even more with AI assistance. Commit the spec file itself to the repo. This not only preserves history, but the agent can even use git diff or blame to understand changes (LLMs are quite capable of reading diffs). Some advanced agent setups let the agent query the VCS history to see when something was introduced - surprisingly, models can be “fiercely competent at Git”. By keeping your spec in the repo, you allow both you and the AI to track evolution. There are tools (like GitHub Spec Kit mentioned earlier) that integrate spec-driven development into the git workflow - for instance, gating merges on updated specs or generating checklists from spec items. While you don’t need those tools to succeed, the takeaway is to treat the spec like code - maintain it diligently.\n\nCost and speed considerations: Working with large models and long contexts can be slow and expensive. A practical tip is to use model selection and batching smartly. Perhaps use a cheaper/faster model for initial drafts or repetitions, and reserve the most capable (and expensive) model for final outputs or complex reasoning. Some developers use GPT-4 or Claude for planning and critical steps, but offload simpler expansions or refactors to a local model or a smaller API model. If using multiple agents, maybe not all need to be top-tier; a test-running agent or a linter agent could be a smaller model. Also consider throttling context size: don’t feed 20k tokens if 5k will do. As we discussed, more tokens can mean diminishing returns.\n\nMonitor and log everything: In complex agent workflows, logging the agent’s actions and outputs is essential. Check the logs to see if the agent is deviating or encountering errors. Many frameworks provide trace logs or allow printing the agent’s chain-of-thought (especially if you prompt it to think step-by-step). Reviewing these logs can highlight where the spec or instructions might have been misinterpreted. It’s not unlike debugging a program - except the “program” is the conversation/prompt chain. If something weird happens, go back to the spec/instructions to see if there was ambiguity.\n\nLearn and improve: Finally, treat each project as a learning opportunity to refine your spec-writing skill. Maybe you’ll discover that a certain phrasing consistently confuses the AI, or that organizing spec sections in a certain way yields better adherence. Incorporate those lessons into the next spec. The field of AI agents is rapidly evolving, so new best practices (and tools) emerge constantly. Stay updated via blogs (like the ones by Simon Willison, Andrej Karpathy, etc.), and don’t hesitate to experiment.\n\nA spec for an AI agent isn’t “write once, done.” It’s part of a continuous cycle of instructing, verifying, and refining. The payoff for this diligence is substantial: by catching issues early and keeping the agent aligned, you avoid costly rewrites or failures later. As one AI engineer quipped, using these practices can feel like having “an army of interns” working for you, but you have to manage them well. A good spec, continuously maintained, is your management tool.\n\nBefore wrapping up, it’s worth calling out anti-patterns that can derail even well-intentioned spec-driven workflows. The GitHub study of 2,500+ agent files revealed a stark divide: “Most agent files fail because they’re too vague.” Here are the mistakes to avoid:\n\nVague prompts: “Build me something cool” or “Make it work better” gives the agent nothing to anchor on. As Baptiste Studer puts it: “Vague prompts mean wrong results.” Be specific about inputs, outputs, and constraints. “You are a helpful coding assistant” doesn’t work. “You are a test engineer who writes tests for React components, follows these examples, and never modifies source code” does.\n\nOverlong contexts without summarization: Dumping 50 pages of documentation into a prompt and hoping the model figures it out rarely works. Use hierarchical summaries (as discussed in Principle 3) or RAG to surface only what’s relevant. Context length is not a substitute for context quality.\n\nSkipping human review: Willison has a personal rule: “I won’t commit code I couldn’t explain to someone else.” Just because the agent produced something that passes tests doesn’t mean it’s correct, secure, or maintainable. Always review critical code paths. The “house of cards” metaphor applies: AI-generated code can look solid but collapse under edge cases you didn’t test.\n\nConflating vibe coding with production engineering: Rapid prototyping with AI (“vibe coding”) is great for exploration and throwaway projects. But shipping that code to production without rigorous specs, tests, and review is asking for trouble. Osmani distinguishes “vibe coding” from “AI-assisted engineering” - the latter requires the discipline this guide describes. Know which mode you’re in.\n\nIgnoring the “lethal trifecta”: Willison warns of three properties that make AI agents dangerous: speed (they work faster than you can review), non-determinism (same input, different outputs), and cost (encouraging corner-cutting on verification). Your spec and review process must account for all three. Don’t let speed outpace your ability to verify.\n\nMissing the six core areas: If your spec doesn’t cover commands, testing, project structure, code style, git workflow, and boundaries, you’re likely missing something the agent needs. Use the six-area checklist from Section 2 as a sanity check before handing off to the agent.\n\nWriting an effective spec for AI coding agents requires solid software engineering principles combined with adaptation to LLM quirks. Start with clarity of purpose and let the AI help expand the plan. Structure the spec like a serious design document - covering the six core areas and integrating it into your toolchain so it becomes an executable artifact, not just prose. Keep the agent’s focus tight by feeding it one piece of the puzzle at a time (and consider clever tactics like summary TOCs, subagents, or parallel orchestration to handle big specs). Anticipate pitfalls by including three-tier boundaries (Always/Ask first/Never), self-checks, and conformance tests - essentially, teach the AI how to not fail. And treat the whole process as iterative: use tests and feedback to refine both the spec and the code continuously.\n\nFollow these guidelines and your AI agent will be far less likely to “break down” under large contexts or wander off into nonsense.\n\nThis post was formatted using Gemini with images generated using Nano Banana Pro",
    "readingTime": 35,
    "keywords": [
      "extended toc",
      "api keys",
      "github study",
      "vague prompts",
      "document prd",
      "context protocol",
      "naming conventions",
      "helpful constraint",
      "architectural patterns",
      "tech stack"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/good-spec/",
    "thumbnail_url": "https://addyosmani.com/assets/images/good-spec.jpg",
    "created_at": "2026-01-14T12:25:01.028Z",
    "topic": "tech"
  },
  {
    "slug": "phases-of-vibe-coding",
    "title": "Phases of Vibe Coding",
    "description": "I built a terminal-based Counter-Strike clone with a coding agent. 49K lines in a week. Understanding the 4 phases of AI-assisted development.",
    "fullText": "EssaysThe 4 Phases of Vibe CodingI built a terminal-based Counter-Strike clone with a coding agent. 49K lines in a week. These projects go through 4 distinct phases, and understanding them is the key to effective AI-assisted development.Idan BeckCEO and FounderJanuary 12, 2026•12 min readShare: Loading content... Related Articles EssaysJanuary 12, 2026 • 8 min read The Bootstrapping LoopJust in Time SoftwareJanuary 6, 2026 • 10 min read Why Business Velocity Will Be Measured in Tokens per SecondMaster PlanOctober 28, 2025 • 8 min read Master Plan: Building Software at the Speed of ThoughtReady to Transform Your Development Process?Discover how Zerg AI can help you implement just-in-time software development in your organization.\n\nSchedule a Consultation",
    "readingTime": 1,
    "keywords": [
      "phases",
      "software",
      "development"
    ],
    "qualityScore": 0.55,
    "link": "https://zergai.com/blog/4-phases-vibe-coding",
    "thumbnail_url": "https://zergai.com/images/blog/4-phases-vibe-coding-hero.png",
    "created_at": "2026-01-14T01:00:15.929Z",
    "topic": "tech"
  },
  {
    "slug": "even-linus-torvalds-is-trying-his-hand-at-vibe-coding-but-just-a-little",
    "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
    "description": "But then I cut out the middle man—me.\"",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
    "created_at": "2026-01-14T01:00:15.061Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-engineering-what-ive-learned-working-with-ai-coding-agents",
    "title": "Vibe Engineering: What I've Learned Working with AI Coding Agents",
    "description": "Vibe Engineering: What I've Learned Working with AI Coding Agents",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/mrexodia/status/2010157660885176767",
    "thumbnail_url": "https://pbs.twimg.com/media/G-WBO0QXAAApT_j.jpg:large",
    "created_at": "2026-01-13T00:54:05.735Z",
    "topic": "tech"
  },
  {
    "slug": "targets-dev-server-offline-after-hackers-claim-to-steal-source-code",
    "title": "Target's dev server offline after hackers claim to steal source code",
    "description": "Hackers are claiming to be selling internal source code belonging to Target Corporation, after publishing what appears to be a sample of stolen code repositories on a public software development platform. After BleepingComputer notified Target, the files were taken offline and the retailer's developer Git server was inaccessible.",
    "fullText": "Hackers are claiming to be selling internal source code belonging to Target Corporation, after publishing what appears to be a sample of stolen code repositories on a public software development platform.\n\nLast week, an unknown threat actor created multiple repositories on Gitea that appeared to contain portions of Target's internal code and developer documentation. The repositories were presented as a preview of a much larger dataset allegedly being offered for sale to buyers on an underground forum or private channel.\n\nAfter BleepingComputer contacted Target with questions about the alleged breach, the files were taken offline and the retailer's Git server, git.target.com, became inaccessible from the internet.\n\nLast week, BleepingComputer received a tip that a threat actor was posting screenshots in a private hacking community to support claims that they had gained access to Target's internal development environment.\n\nThe same actor had also published several repositories on Gitea, a self-hosted Git service similar to GitHub or GitLab, as a sample of the data the actor claimed was being offered for sale.\n\nAccording to the source, hackers claimed that \"this is [the first set of] data to go to auction.\"\n\nEach repository contained a file named SALE.MD listing tens of thousands of files and directories purportedly included in the full dataset. The listing was more than 57,000 lines long and advertised a total archive size of approximately 860 GB.\n\nThe Gitea sample repository names included:\n\nIt's worth noting that the commit metadata and documentation referenced the names of internal Target development servers, and multiple current Target lead and senior engineers.\n\nBleepingComputer shared the Gitea links with Target on Thursday and requested comment on the alleged breach\n\nAround the same time, Target's developer Git server at git.target.com also became inaccessible from the internet.\n\nUntil Friday, the subdomain was reachable and redirected to a login page, prompting Target employees to connect via the company's secure network or VPN. As of Saturday, the site no longer loads externally:\n\nBleepingComputer also observed that search engines such as Google had indexed and cached a small number of resources from git.target.com, indicating that some content from the domain was publicly accessible at some point in the past.\n\nIt is unclear when those pages were indexed or under what configuration, and their presence in search results does not necessarily indicate that the current claims are linked to any exposure of the server, or that the Git infrastructure was recently accessible without authentication.\n\nWhile BleepingComputer has not independently verified the full 860 GB dataset or confirmed that a breach occurred, the directory structure, repository naming, and internal system references in the SALE.MD index are consistent with a large enterprise Git environment.\n\nAdditionally, the contents do not match any of Target's open-source projects on GitHub, indicating the material, if authentic, would have originated from private development infrastructure rather than publicly released code.\n\nThe presence of the names of current Target lead and senior engineers in commit metadata and documentation, along with links to internal API endpoints and platforms, such as confluence.target.com, also raises questions about the origin of the files.\n\nFurthermore, the fact that the Gitea respositories used to store Target's allegedly stolen source code are no longer available, also point toward a possible breach.\n\nAfter Target initially requested the repository links, the company did not provide further comment before publication when approached multiple times.\n\nTarget's most significant publicly disclosed security incident to date remains its 2013 breach, in which attackers stole payment card data and other personally identifiable information belonging to up to 110 million customers and exfiltrated it to infrastructure located in Eastern Europe, according to U.S. Senate and academic investigations.\n\nWhether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.\n\nGet the cheat sheet and take the guesswork out of secrets management.",
    "readingTime": 4,
    "keywords": [
      "git server",
      "target lead",
      "commit metadata",
      "senior engineers",
      "target's internal",
      "threat actor",
      "alleged breach",
      "code",
      "repositories",
      "development"
    ],
    "qualityScore": 1,
    "link": "https://www.bleepingcomputer.com/news/security/targets-dev-server-offline-after-hackers-claim-to-steal-source-code/",
    "thumbnail_url": "https://www.bleepstatic.com/content/hl-images/2026/01/12/target-header.jpg",
    "created_at": "2026-01-12T18:19:09.480Z",
    "topic": "tech"
  },
  {
    "slug": "pico-gpu-virtual-gpu-for-learning-shaders",
    "title": "Pico GPU: Virtual GPU for Learning Shaders",
    "description": "Experiment with GPU programming and sound synth",
    "fullText": "Pico GPU is a 300KB memory GPU intended to learn, experiment and have fun with shaders. It is perfect to easily create small demos or games involving 3D rendering. It can also perform GPU based sound synthesis.SpecificationsPico GPU specification are:640x480 resolution at 60FPS, with 24 bit depth and 8 bit stencil300KB gpu memory to load your textures, buffers, code and shaders4 channels Mono 32 bit sound synthesis at 48 KHz (using GPU shaders)complete support for vertex & fragment bufferssupport blending, culling, depth, stencil, color mask, clippingsupport render targets and instancingmaths matrix, vector, quaternion supportsave & load as a 640x480 PNG screenshot that contains all your datashare your apps with the community!",
    "readingTime": 1,
    "keywords": [
      "memory",
      "shaders",
      "sound",
      "depth",
      "load"
    ],
    "qualityScore": 0.35,
    "link": "https://ncannasse.github.io/picogpu/",
    "thumbnail_url": "og-image.png",
    "created_at": "2026-01-12T18:19:07.266Z",
    "topic": "tech"
  },
  {
    "slug": "quantization-and-distillation-effects-on-code-llms",
    "title": "Quantization and distillation effects on code LLMs",
    "description": "Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.02563",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-12T12:26:18.532Z",
    "topic": "tech"
  },
  {
    "slug": "we-asked-over-150-software-engineers-about-vibecoding-heres-what-they-said",
    "title": "We asked over 150 software engineers about vibe-coding. Here's what they said.",
    "description": "167 software engineers responded to Business Insider's vibe-coding survey. Over 45% reported \"keeping up\" with AI tools. Almost 17% feel behind.",
    "fullText": "AI has radically changed what coding looks like. We asked software engineers how they felt about it.\n\nAndrej Karpathy coined the term \"vibe-coding,\" or the creation of code using AI. The term has since gained traction among developers worldwide and was named Collins Dictionary's Word of the Year for 2025.\n\nLess than a year after his post, Karpathy wrote that he had \"never felt this much behind as a programmer.\"\n\nWe asked developers: When it comes to vibe-coding, do you feel ahead, behind, or like you're keeping pace?\n\n167 software engineers responded to our survey. The biggest cohort — 75 engineers, or 46.9% — said that they were \"keeping up.\" 30 engineers said they felt ahead of the curve, while 27 felt behind.\n\n28 engineers (or 17.5% of respondents) said that they were opting out of using AI code editing tools entirely. These engineers wrote that the tools weren't advanced enough, or that they took too long to learn how to use. None of the 28 agreed to speak on the record after Business Insider reached out.\n\nWhile the survey isn't scientific, the results offer insight into how software engineers are feeling about their rapidly changing industry.\n\nIn follow-up conversations, eight engineers told Business Insider how they feel about AI code editors. All found them helpful in some form, though their usages ranged from one-off tools to lifesavers.\n\nRyan Shah sometimes wonders: \"Did I really need to learn how to write code?\"\n\nThe 23-year-old AI consultant from Atlanta recently graduated with a degree in computer information technology. Now he uses Cursor and Google's Antigravity, paired with Claude Opus 4.5, which he said was at \"midlevel engineer status.\"\n\nShah said he doesn't regret his software engineering courses, though. They taught him to \"read\" code, he said, a skill that, coupled with his vibe-coding proficiencies, keeps him from being \"the first one laid off.\"\n\nJavanie Campbell swung the other way: He warned that over-reliance on vibe-coding tools will put your career in danger.\n\n\"For people who turn to the LLM as the God or the expert, they will be replaced,\" said the 35-year-old CEO of DevDaysAtWork, who is based in Jamaica.\n\nAmong software engineers, there's a debate brewing: Just how bad will the effects of AI code editors be on jobs? Some say they will shrink the industry's workforce; others call them tools, not replacements for engineers.\n\nThe first time Ryan Clinton tried vibe-coding, he got scared for his job. He's not fearful anymore, he said.\n\nClinton's engineering level won't be affected, said the 46-year-old software developer from Nashville. More experienced engineers work on \"architecture and design,\" he said, while more junior staffers code. At this point of AI coding, human intervention is also still routinely necessary.\n\n\"You want to make sure it makes sense,\" he said. \"Only an idiot would randomly click 'yes' and commit it.\"\n\nBarry Fruitman is more worried — but not for himself. At 56, the Android developer from Toronto doesn't think the job market will feel the effect until five to 10 years out.\n\n\"Today, I think the threat is overstated, and hopefully it will stay that way until I retire,\" he said.\n\nEd Gaile said AI tools have doubled, if not tripled, his productivity.\n\nThe 55-year-old Appfire principal solutions architect from Atlanta was impressed by the decrease in context switching that vibe-coding tools brought.\n\n\"I wish I had this 15 years ago,\" he said.\n\nFor AI code editors, the word \"productivity\" still looms large. Many people feel that they're saving time by using these tools. Others cite the additional time spent reviewing and correcting lines of code.\n\nA July METR study added fuel to the fire.\n\nThe study asked experienced developers to complete a series of tasks. Study participants working without AI's help spent 10% more time coding — but those with AI assistance spent 20% more time reviewing AI outputs, prompting AI, waiting on AI, or being idle. Ultimately, the study found that the AI-assisted developers were less productive.\n\nShawn Gay, a 54-year-old R&D manager from El Paso, Texas, spends time keeping up with the industry's changes. He said he felt behind the curve.\n\n\"I have decades of experience, so I feel like it's a huge effort to try to change the way my brain thinks about software,\" Gay told Business Insider.\n\nGus De Souza said that he saved time on coding, but spent more time reviewing the AI-generated code. The real productivity gains were in troubleshooting, said the 48-year-old software architect from Kitchener, Ontario.\n\nWhat even is a vibe-coder? While the term has grown to encompass most forms of AI-assisted coding, Karpathy's X post first defined it as when developers \"fully give in to the vibes, embrace exponentials, and forget that the code even exists.\"\n\nLara Fraser, a data analyst and epidemiologist from Sarasota, Florida, doesn't consider herself a vibe-coder.\n\nFraser codes in R and uses tools like ChatGPT and Claude to assist. She's tried other tools, but found high rates of hallucination. The model generation also matters, Fraser said: GPT 5.1 was great, but 5.2 was a \"disaster.\"\n\nFor Fraser, vibe-coding depends on the programmer's skill. Anyone can create an app, but not everyone can maintain it.\n\n\"Inevitably, something's going to break,\" she said. \"Can you fix it? If you can't, you're a vibe-coder.\"",
    "readingTime": 5,
    "keywords": [
      "code editors",
      "software engineers",
      "year-old software",
      "vibe-coding tools",
      "business insider",
      "developers",
      "behind",
      "study",
      "doesn't",
      "productivity"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/software-engineers-on-vibe-coding-ai-tools-2026-1",
    "thumbnail_url": "https://i.insider.com/69601f86832e0ef1ead7712a?width=1200&format=jpeg",
    "created_at": "2026-01-12T12:26:15.498Z",
    "topic": "tech"
  },
  {
    "slug": "tiny-coder-ai-coding-agent-in-300-loc-writing-itself",
    "title": "Tiny Coder – AI coding agent in ~300 LOC writing itself",
    "description": "Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies. - xrip/tinycode",
    "fullText": "xrip\n\n /\n\n tinycode\n\n Public\n\n Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies.\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n xrip/tinycode",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/xrip/tinycode",
    "thumbnail_url": "https://opengraph.githubassets.com/07e51ae8c3cabfafdf20fdcf525646b8f004627e8c671e3c9185240ef4e3b4fd/xrip/tinycode",
    "created_at": "2026-01-11T12:21:58.611Z",
    "topic": "tech"
  }
]
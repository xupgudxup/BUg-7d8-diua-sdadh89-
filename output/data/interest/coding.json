[
  {
    "slug": "orangensaft-a-mini-pythonlike-language-with-llm-eval-in-lang-runtime",
    "title": "Orangensaft ‚Äì A mini Python-like language with LLM eval in lang runtime",
    "description": "A new age post-AI programming language. Contribute to jargnar/orangensaft development by creating an account on GitHub.",
    "fullText": "jargnar\n\n /\n\n orangensaft\n\n Public\n\n A new age post-AI programming language\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jargnar/orangensaft",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://github.com/jargnar/orangensaft",
    "thumbnail_url": "https://opengraph.githubassets.com/fc5976b41bfb8b1ea8e07bed5ea5b7596101078e6c57161a28c5d0aea8c3d834/jargnar/orangensaft",
    "created_at": "2026-02-13T12:34:58.006Z",
    "topic": "tech"
  },
  {
    "slug": "anthropic-raises-30bn-in-latest-round-valuing-claude-bot-maker-at-380bn",
    "title": "Anthropic raises $30bn in latest round, valuing Claude bot maker at $380bn",
    "description": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (¬£22bn) in a funding round that more than doubled its valuation to $380bn.\nThe company‚Äôs previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n Continue reading...",
    "fullText": "Maker of chatbot with coding ability says annualised revenue grew tenfold in each of past three years, to $14bn\n\nAnthropic, the US AI startup behind the Claude chatbot, has raised $30bn (¬£22bn) in a funding round that more than doubled its valuation to $380bn.\n\nThe company‚Äôs previous funding round in September achieved a value of $183bn, with further improvements in the technology since then spurring even greater investor interest.\n\nThe fundraising was announced amid a series of stock market moves against industries that face disruption from the latest models, including software, trucking and logistics, wealth management and commercial property services.\n\nThe funding round, led by the Singapore sovereign wealth fund GIC and the hedge fund Coatue Management, is among the largest private fundraising deals on record.\n\n‚ÄúAnthropic is the clear category leader in enterprise AI,‚Äù said Choo Yong Cheen, the chief investment officer of private equity at GIC.\n\nAnthropic said its annualised revenue ‚Äì an estimate of full-year sales based on recent company data ‚Äì had reached $14bn, having grown more than tenfold in each of the past three years. A significant driver of recent growth has been Claude Code, the company‚Äôs AI-powered coding tool that became generally available in May 2025.\n\nAnthropic‚Äôs rival OpenAI, backed by Microsoft and SoftBank, has been assembling what is reportedly a far larger round of up to $100bn that would value the ChatGPT developer at about $830bn.\n\nThe staggering sums being raised reflect equally staggering burn rates, with the companies spending cash to cover their huge costs of computing and attracting researcher talent.\n\nAnthropic has forecast reducing its cash burn to roughly a third of revenue in 2026 and just 9% by 2027, with a break-even target of 2028 ‚Äì two years ahead of its rival, according to reports. Both companies are widely expected to pursue initial public offerings in the second half of 2026.\n\nThe rapid valuation increases for leading AI startups such as Anthropic and OpenAI, whose price tags far exceed those of many of the US‚Äôs largest listed companies, has alarmed some observers. Last year, a leading British tech investor, James Anderson, said he found sharp increases in valuations of companies such as OpenAI and Anthropic ‚Äúdisconcerting‚Äù.\n\nSome listed firms at the forefront of the AI industry have also come under stock market pressure in recent days.\n\nShares in Alphabet, Google‚Äôs parent company, have fallen by 4.2% so far this week, indicating some investors are still spooked by the big AI-related spending plans it laid out this month. Meta has declined by 1.7% during this week. Shares in Nvidia, a leading chipmaker and key provider of AI infrastructure, dropped by 1.6% on Thursday amid a wider sell-off but have been flat on the week.\n\n‚ÄúA gloomy session on Wall Street on Thursday put investors in a grumpy mood at the end of the trading week,‚Äù said Russ Mould, the investment director at investment platform AJ Bell.\n\n‚ÄúAssociation with AI has gone from party to peril as investors reappraise what the technology means for companies. \n\n ‚ÄúSome are concerned about excessive levels of spending and others fear AI will disrupt multiple industries. It all adds up to a cocktail of worries and that‚Äôs bad for market sentiment more broadly,‚Äù Mould added.\n\nFounded in 2021 by the siblings Dario and Daniela Amodei, both former executives at OpenAI, Anthropic has positioned itself as a safety-focused alternative in the AI race.\n\nThe funding round also comes shortly after Anthropic‚Äôs first television commercials were broadcast during Super Bowl LX, using the campaign to emphasise that its products remain ad-free. The ads took an apparent jab at OpenAI, which has begun to introduce advertising into the free version of ChatGPT.\n\nAnthropic‚Äôs earlier backers include Amazon, which has invested $8bn and serves as a primary computing partner through its datacentres, as well as Google, which invested $2bn in 2023.\n\nAgence France-Presse contributed to this article",
    "readingTime": 4,
    "keywords": [
      "annualised revenue",
      "stock market",
      "funding round",
      "investment",
      "leading",
      "investors",
      "anthropic",
      "chatbot",
      "coding",
      "tenfold"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2026/feb/12/anthropic-funding-round",
    "thumbnail_url": "https://i.guim.co.uk/img/media/b30b904b79d26877d4b860af0a6c67c5b55a0067/735_0_2715_2172/master/2715.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=d2f7a8edb124095d9242575a7c8b9ac3",
    "created_at": "2026-02-13T12:34:48.923Z",
    "topic": "tech"
  },
  {
    "slug": "these-y-combinator-founders-raised-10-million-to-get-corporate-america-into-vibe-coding-read-their-pitch-deck",
    "title": "These Y Combinator founders raised $10 million to get corporate America into vibe coding. Read their pitch deck.",
    "description": "Vybe, a startup by Y Combinator alumni, raises $10M to integrate vibe coding into corporations, with participation from major tech leaders.",
    "fullText": "Vybe, the startup cofounded by two repeat Y Combinator entrepreneurs, has raised $10 million to bring vibe coding inside large companies.\n\nThe practice of vibe coding has become more widespread as it allows people to use AI and natural language prompts to build an app, rather than traditional programming. The sector has attracted hundreds of millions in venture funding and is blurring the lines for businesses deciding between buying software or just vibe-coding their own tools.\n\nWhile popular vibe-coding products like Lovable and Replit have made it easy for companies to create prototypes and landing pages, Vybe offers stronger security that cannot be modified by AI and also taps into internal data systems, said cofounder and CEO Quang Hoang.\n\nHoang previously founded the engineering mentorship platform Plato and is building Vybe alongside cofounder and CTO Fabien Devos, founder of the security AI startup Wolfia.\n\nVybe lets teams across an organization collaborate on apps. Engineering teams manage access to internal systems like Salesforce, Snowflake, and Databricks, while business teams can build apps for onboarding, performance reviews, customer service, and more.\n\n\"The new SaaS is going to be more like Legos,\" Hoang said, referring to software-as-a-service. \"You can build it exactly how you want.\"\n\nVybe also offers app templates created by high-profile executives, such as former Airbnb product leader Lenny Rachitsky and Front cofounder Mathilde Collin, that users can \"remix\" to suit their needs, including templates for performance reviews and one-on-one meetings.\n\nFirst Round Capital led Vybe's seed round, which featured participation from Y Combinator, the CEOs of Datadog and Grammarly, and product leaders from OpenAI and Meta.\n\nVybe has six employees and dozens of customers, Hoang said. It plans to use the funding to hire engineers. Vybe charges $12 per user a month, plus usage-based credits for app development.\n\nThe round comes as vibe coding continues to attract major funding ‚Äî with startups like Emergent pulling in tens of millions from Khosla Ventures and SoftBank ‚Äî even as Barclays analysts warn the initial boom may be cooling.\n\nAt the same time, investors are also weighing how AI and vibe-coding tools can reshape the software sector. Software stocks have entered¬†one of their sharpest downturns in years,¬†shedding roughly $2 trillion in market capitalization.\n\nHere's a look at the pitch deck Vybe used to raise its $10 million seed round. One slide has been removed, and another redacted so that the deck can be shared publicly.",
    "readingTime": 3,
    "keywords": [
      "performance reviews",
      "vibe coding",
      "seed round",
      "funding",
      "software",
      "vibe-coding",
      "cofounder",
      "teams",
      "vybe",
      "startup"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/vybe-pitch-deck-expand-vibe-coding-corporate-world-2026-2",
    "thumbnail_url": "https://i.insider.com/698df18dd3c7faef0ece2588?width=948&format=jpeg",
    "created_at": "2026-02-13T01:15:39.236Z",
    "topic": "finance"
  },
  {
    "slug": "a-software-engineer-says-theres-a-vampiric-effect-to-ai-where-vibecoding-sprints-are-followed-by-naps-to-cope-with-ai",
    "title": "A software engineer says there's a 'vampiric effect' to AI, where vibe-coding sprints are followed by naps to cope with AI fatigue",
    "description": "Steve Yegge, who was at Amazon in the early days and spent 12 years at Google, says his fellow engineers need to learn to say no.",
    "fullText": "A seasoned veteran said his fellow software engineers need to learn \"how to say 'no,' real fast\" or risk getting crushed by AI.\n\nSteve Yegge, who worked with Jeff Bezos at Amazon early on before 12-year stint at Google, said AI is set up in a way that can really drain you.\n\n\"There's a vampiric effect with AI, where it gets you excited, and you work really hard, and you're capturing a ton of value,\" he recently told the \"The Pragmatic Engineer\" newsletter/podcast.\n\nYegge said companies also need to understand that while agentic AI may make engineers more productive than ever before, pushing the limit will just burn out their workforce.\n\n\"I seriously think founders and company leaders and engineering leaders at all levels, all the way down to line managers, have to be aware of this and realize that you might only get three productive hours out of a person who's vibe coding at max speed,\" he said. \"So, do you let them work for three hours a day? The answer is yes, or your company's going to break.\"\n\nEngineers are beginning to vocalize concerns about \"AI fatigue.\" Business Insider recently spoke to Siddhant Khare, who builds AI tools, and wrote an essay about how AI has accelerated the pace of his job to a point that it was burning him out.\n\nYegge said that his fellow engineers need to set boundaries when they are vibe coding.\n\n\"People have to learn the art of pushing back,\" he said.\n\nUntil then, Yegge said he and his fellow engineers are napping and growing grumpier.\n\n\"I find myself napping during the day, and I'm talking to friends at startups, and they're finding themselves napping during the day,\" he said. \"We're starting to get tired and cranky.\"",
    "readingTime": 2,
    "keywords": [
      "vibe coding",
      "fellow engineers",
      "napping",
      "learn",
      "recently",
      "productive",
      "pushing",
      "leaders",
      "hours",
      "yegge"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/software-engineer-steve-yegge-ai-burnout-2026-2",
    "thumbnail_url": "https://i.insider.com/698cd767e1ba468a96abdee5?width=1200&format=jpeg",
    "created_at": "2026-02-12T12:39:46.962Z",
    "topic": "tech"
  },
  {
    "slug": "minions-stripes-oneshot-endtoend-coding-agents",
    "title": "Minions: Stripe's one-shot, end-to-end coding agents",
    "description": "Minions are Stripe‚Äôs homegrown coding agents, responsible for more than a thousand pull requests merged each week. Though humans review the code, minions write it from start to finish. Learn how they work, and how we built them.",
    "fullText": "Minions: Stripe‚Äôs one-shot, end-to-end coding agents/Article/About the authorAlistair GrayAlistair is a software engineer on the Leverage team at Stripe./DocsExplore our guides and examples to integrate Stripe.Learn more/SocialYoutubeTwitter/XDiscordDocsDeveloper Meetups¬© 2025 Stripe, Inc.PrivacyLegalStripe.com",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents",
    "thumbnail_url": "https://images.ctfassets.net/fzn2n1nzq965/1IiEdIR6LsEY9aym5JQM6u/85a9a55f58137d9d1fac99319658081b/dev_social.png",
    "created_at": "2026-02-12T06:50:09.787Z",
    "topic": "tech"
  },
  {
    "slug": "hes-worked-decades-in-tech-and-wrote-a-book-on-vibe-coding-he-predicts-50-of-big-tech-engineers-will-be-laid-off",
    "title": "He's worked decades in tech and wrote a book on vibe coding. He predicts 50% of Big Tech engineers will be laid off.",
    "description": "Steve Yegge worked at Amazon an Google. Now, he predicts Big Tech companies will cut half their engineers to help pay for the others to have AI.",
    "fullText": "He worked in Big Tech. Now, he fears half of its software engineers are on the chopping block.\n\nSteve Yegge worked with Jeff Bezos in Amazon's early years. Then he went to Google, where he worked for over 12 years and earned the title of senior staff software engineer. He also knows a thing or two about AI engineering, having written a book on the topic titled \"Vibe Coding.\"\n\nOn \"The Pragmatic Engineer\" podcast and newsletter, Yegge described an imaginary dial of the percentage of engineering staff a company can lay off, ranging from zero to 100. He said he believes the dial is being set at 50 in the age of AI.\n\n\"You're going to have to get rid of half of them to make the other half maximally productive,\" Yegge said. \"We're going to lose around half the engineers from big companies, which is scary\".\n\nThere's a capital-to-labor tradeoff happening in tech. Companies are paying vast sums for tokens and enterprise AI licenses, GPUs, and computing capacity. That money needs to come from somewhere ‚Äî and for some, it could come from labor costs.\n\nYegge pointed to this tradeoff as the reason for his forecast of 50% cuts becoming the norm. Companies will lay off some engineers to help pay for the others to have adequate access to AI, he said.\n\nHost Gergely Orosz said that 50% cuts would be more than during the COVID-19 pandemic, when tech went through an intense layoff cycle.\n\n\"It's going to be way bigger,\" Yegge said. \"It's going to be awful.\"\n\nThey aren't the only ones referencing the pandemic. In a popular X article, HyperWrite CEO Matt Shumer wrote that AI's impact on work would be \"much, much bigger than Covid.\"\n\nIn recent years, the question of AI-driven layoffs has haunted workers at Big Tech companies, many of which went on a hiring spree during the pandemic. While it's often impossible to boil job loss down a single reason, many suspect productivity gains from AI and ballooning capex spending are fueling the cuts. Some business leaders have also explicitly cited AI when announcing layoffs.\n\nMeanwhile, tech leaders describe smaller teams working more efficiently. On a recent earnings call, Meta CEO Mark Zuckerberg said one engineer could now do the work of a whole team, thanks to AI.\n\nAs AI productivity grows, so have complaints of AI fatigue. Software engineer Siddhant Khare penned a lengthy essay about growing more productive, but also feeling more drained than ever. He told Business Insider that he felt like a \"reviewer\" as opposed to an engineer.\n\nYegge said it's not all bad news for the engineers ‚Äî just those who want to work at a big company. Engineers who have \"seen the light\" are now getting together, leaving their companies, and creating startups that outpace the industry's giants, he said.\n\n\"We've got this mad rush of innovation coming up, bottom up,\" he said. \"And we've got knowledge workers being laid off by big companies because clearly big businesses are not the right size anymore.\"",
    "readingTime": 3,
    "keywords": [
      "software engineer",
      "engineers",
      "half",
      "it's",
      "cuts",
      "pandemic",
      "staff",
      "engineering",
      "dial",
      "productive"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/steve-yegge-vibecoding-author-predicts-layoffs-half-big-tech-engineers-2026-2",
    "thumbnail_url": "https://i.insider.com/698c9571e1ba468a96abd6bb?width=1200&format=jpeg",
    "created_at": "2026-02-12T01:12:22.798Z",
    "topic": "finance"
  },
  {
    "slug": "the-skill-nobody-tests-for",
    "title": "The Skill Nobody Tests For",
    "description": "Drop candidates into a real coding environment with AI tools. See who actually ships.",
    "fullText": "Drop candidates into a real coding environment with AI tools. See who actually ships.\n\nFree to start ¬∑ Set up in 2 minutes\n\nChallenge: Build an RBAC permission system\n\nTools: Claude Code ¬∑ VS Code ¬∑ Terminal\n\nMeta, Shopify, Cisco ‚Äî companies are dropping algorithm interviews. The question isn't whether this changes hiring. It's whether you move first.\n\nOver the last year, the best engineers have completely changed how they work. They don't write code by hand anymore, they talk to agents, interrogate them, make architectural decisions, scope projects, and verify output.\n\nA full VS Code workspace in the browser. Terminal, files, packages ‚Äî everything your engineers use daily.\n\nClaude Code, Codex, Cursor, Copilot ‚Äî the same tools your team uses. The challenge is using them well, not memorizing algorithms.\n\nAutomated tests validate the solution. You see what they built, how they got there, and whether it works.",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "tools",
      "terminal",
      "engineers",
      "challenge"
    ],
    "qualityScore": 0.75,
    "link": "https://vibearena.io/blog/the-skill-nobody-tests-for",
    "thumbnail_url": "https://vibearena.io/og-image.png",
    "created_at": "2026-02-11T12:42:57.921Z",
    "topic": "tech"
  },
  {
    "slug": "the-future-of-coding-agents-is-vertical-integration-and-why-acp-matters",
    "title": "The future of coding agents is vertical integration (and why ACP matters)",
    "description": "A look at the limitations of generic agents and MCPs, through the lens of building Tidewave, and what this means for the future of coding agents",
    "fullText": "The first appearances of coding agents were within our editors, as a natural progression from tab autocompletion. When Claude Code was released, almost a year ago, it innovated in many ways, particularly by demonstrating that although we write almost all of our code in editors, the agents themselves don‚Äôt have to be constrained to IDEs. As a result, most of our agentic development moved to the terminal.\n\nWith the release of the Codex App, we will start to see a push towards richer interfaces. However, this raises the question: if we can run coding agents anywhere, where is the best place to run them?\n\nIn this section, I will talk about Tidewave, the coding agent we built for full-stack web development. It is a sales pitch but I hope it is a good example of what we can gain by building vertical coding agents.\n\nTidewave was born out of frustration when using coding agents to build web applications. I frequently stumbled upon scenarios such as:\n\nThe agent would tell me a feature was complete, but when I tried it in the browser, form submission would not complete\n\nWhenever I encountered an exception page during development, I had to copy and paste stacktraces from the browser to the agent\n\nI had to constantly translate what was on the page to source code. If I wanted to ‚ÄúAdd an Export to CSV button to a dropdown button‚Äù, the first step was typically to find where the template or component the dropdown was defined in\n\nUltimately, because the coding agent was unable to interact with or see what it produced, I had to act as the middleman:\n\nAnd while things have improved within the last year, most of the tooling out there does not fully close this gap. For example, while coding agents can read logs and editors like Cursor do include a browser, they do not correlate the browser actions with the logs, leaving space for guess work and context bloat. Or when the browser renders a page, it doesn‚Äôt know which controller or template it came from.\n\nWe solved this problem by moving the coding agent to the browser and making it aware of your web app:\n\nYou open up Tidewave in your favorite browser and its agent can now interact with the page you are looking at, validating the application works as desired by accessing the DOM, loading pages, filling forms, etc\n\nWhen something goes wrong, we parse and feed the exact information from the rendered error page. We also include any console.log and server logs related to the particular agent action (instead of the whole history)\n\nWhen you inspect an element in Tidewave, we automatically map DOM elements to source files. If you inspect a dropdown, we tell the agent its location on the page as well as the template/component it came from in the source code\n\nThe coding agent has access to your backend too, so it can read documentation, execute SQL queries, and even gets a REPL-like environment. So when the agent gets stuck, it can navigate the data, run code, and explore APIs, like any developer would\n\nMore importantly, this is all implemented by connecting the agent to your running application. You can ask your agent to analyze cache usage in your Rails application or to debug a LiveView in Phoenix, and it can do so because they talk to each other directly. Your agent is no longer limited to a series of bash commands.\n\nOverall, the agent becomes more capable, and we no longer need to act as the middleman:\n\nFor all of the above to work, instead of building a generic agent that works with every tool, Tidewave directly integrates with each web framework we support (Django, FastAPI, Flask, Next.js, Phoenix, Rails, and TanStack Start), mapping all layers to the agent, from the database to UI.\n\nAnd that‚Äôs what I mean by vertical integration: coding agents become integrated into the specific platform or runtime they‚Äôre building for. They understand the relationship between code and its live behavior, and can debug both.\n\nWe can easily draw parallels between the above and other domains. Agents for mobile development must have full access to a simulator to interact with the app UI, translate view hierarchies to source code, and monitor network requests. Similar patterns could apply to IoT devices, game development, etc.\n\nAs a matter of fact, this isn‚Äôt even a new idea. Data scientists were the first to embed agents directly into their notebooks, where they can execute code, visualize results, inspect dataframes, etc. They were the first to realize the benefit of vertically integrating agents and everyone else is just late to the party.\n\nBut how can we make these vertical integrations possible? If we want agents to access our runtimes and environments, could MCPs be the tool to make this a reality? That‚Äôs what we originally thought‚Ä¶\n\nTidewave was first prototyped as a Model Context Protocol (MCP) server and it quickly became obvious it wasn‚Äôt enough. In this section, I‚Äôll focus on how MCPs can be detrimental to user experience. In particular, we‚Äôll look at its limitations as a pull-only interface and its text-based constraints.\n\nAs an example, let‚Äôs look at Figma‚Äôs MCP. It allows developers to select an element in Figma‚Äôs desktop app and ask the agent to implement the design. Here‚Äôs a screenshot from their announcement:\n\nAs you can see, you‚Äôre required to make a selection in Figma, then explicitly ask the agent to query your selection. The agent then proceeds to talk to Figma and get the relevant data. That‚Äôs a lot of unecessary back and forth! If I already selected an element on Figma, it should take a single click to embed that information into my prompt.\n\nNow imagine if, every time I inspected an element in the browser, we had to tell the agent to ask the browser about the selection. We refused to implement that. Instead, we made it just work:\n\nFurthermore, by making the inspected elements part of our prompt, you can select multiple elements and dictate how they all need to change at once. You can reorder, swap, and coordinate multiple changes easily.\n\nSimilarly, when you see an error page in the browser, you should only need to click a button to fix it - not copy and paste stacktraces or ask the agent to read logs.\n\nNone of this could be built with MCPs because they are pull-based. The agent decides when to invoke the MCP and you are forced to ask the agents to perform actions on your behalf. MCPs do not allow us to attach rich metadata to prompts either.\n\nImagine you need help addressing comments in a pull request. You can tell the agent to use GitHub‚Äôs MCP or even use a custom skill that uses GitHub‚Äôs CLI to fetch all current comments. But once the agent does its initial assessment, all further exchanges happens through text: ‚Äúignore this comment‚Äù, ‚Äúno, not this comment, the one above it‚Äù.\n\nNow compare that to using any graphical (or even a terminal) interface: we can reply in thread, comment on specific lines, or click a button to dismiss invalid feedback altogether. In other words, when we put our integrations behind MCPs, all exchange happens on text, and we often end-up with worse user experiences.\n\nInstead, our goal is to build agentic tools and user experiences side-by-side. For example, when we added Web Accessibility diagnostics to Tidewave, we exposed additional tools to the agent, but we also provided a polished experience for when developers want to remain in the loop:\n\nOur recently announced Supabase integration works the same: it runs performance and security advisors on your database and present them to you. If you want to dig deeper, you can do it all through a rich interface. And if you don‚Äôt care about any of that, you can either click the ‚ÄúFix all‚Äù button or ask your agent to do it for you.\n\nTo build these experiences, we had to invert the control. We don‚Äôt want the agent to call us, we want our tools to call the agent.\n\nSay you want to build your own agentic loop, up until recently, doing so meant to:\n\nWhile the above is already a lot of work, you must remember that prompt engineering, tool calls, etc. must be fine-tuned per model. For example, when OpenAI announced GPT-5-Codex, integrators had to write new prompts and new tools, even if they already supported GPT-5.\n\nAnd the above covers only the basic loop! What about AGENTS.md, MCPs, or skills? And you haven‚Äôt even started working on what makes your coding agent unique.\n\nLuckily for us, many of the leading AI companies have built their own coding agents (Claude Code, Codex, Gemini CLI) tailored to their models, and many have exposed those agents through SDKs. We also have fantastic open source alternatives. This means you can cut out much of the work above, but, if you want to support multiple providers, you‚Äôre still on the hook for integrating them one by one.\n\nThat‚Äôs exactly where the Agent Client Protocol from the Zed team comes in. It standardizes communication with coding agents, so you implement one protocol and support multiple agents at once. This ultimately allows you to focus on the runtime integrations and user experiences that make your coding agent unique.\n\nFurthermore, even if you‚Äôre not implementing your own agent, ACP should matter to you because it brings portability to developers. You can use a single coding agent across the terminal, your editor, and Tidewave, reusing the same settings for hooks, skills, and subagents. And perhaps more importantly, the same subscription.\n\nWe are big fans of the Agent Client Protocol and we are excited to build on top of it alongside companies like Zed and JetBrains.\n\nGeneric agents that work everywhere force awkward workflows where we constantly act as a middleman. Instead, we want to embed agents into the platforms we are building on, so they can see the relationship between code and behavior, interact with live apps, and debug in real-time.\n\nHowever, to build these rich experiences, we need to invert the control and own the agentic loop. The Agent Client Protocol (ACP) allows us to do so by standardizing communication across providers, so we can focus on the user experiences and integrations that make our coding agents unique, instead of wrestling with multiple SDKs.\n\nI hope this article inspires you to fine-tune your own coding agents and, if you are looking for a coding agent tailored to full-stack web development, give Tidewave a try!",
    "readingTime": 9,
    "keywords": [
      "client protocol",
      "claude code",
      "paste stacktraces",
      "agent client",
      "full-stack web",
      "error page",
      "user experiences",
      "agentic loop",
      "web development",
      "embed agents"
    ],
    "qualityScore": 1,
    "link": "https://tidewave.ai/blog/the-future-of-coding-agents-is-vertical-integration",
    "thumbnail_url": "https://tidewave.ai/assets/opengraph/pthe-future-of-coding-agents-is-vertical-integration-4pklmn64vmtcox2ktoauj7nxoy-a84d118a102998c2aa4a25c686d9af29.png?vsn=d",
    "created_at": "2026-02-11T01:19:44.413Z",
    "topic": "tech"
  },
  {
    "slug": "this-ceo-wants-ai-agents-to-outnumber-his-human-employees-this-year",
    "title": "This CEO wants AI agents to outnumber his human employees this year",
    "description": "The CEO of StackBlitz, Eric Simons, says the web development startup has \"gone all in\" on AI agents.",
    "fullText": "StackBlitz CEO Eric Simons aims to have more AI agents than human employees working at the startup this year, a milestone he sees as emblematic of a broader shift underway across the software industry.\n\nSpeaking in an interview with Business Insider, Simons said StackBlitz has \"gone all in on agents,\" deploying internally built AI systems across business intelligence, coding, product development, customer support, and outbound sales.\n\nAI has become increasingly good at writing software code, to the point where the technology is beginning to change how companies are run. OpenClaw, an open-source AI assistant that works inside platforms like WhatsApp, Slack, and iMessage, is an early example of digital agents communicating and coordinating with one another without direct human involvement.\n\n\"To me, this is a crystal ball into the wildness of the inevitable future,\" Simons told Business Insider. \"Your AI agents will talk to other people's agents on your behalf, negotiating pricing for a product you want to buy, inquiring about availability for a restaurant, arguing your political viewpoints.\"\n\n\"Agents are an extension of yourself,\" he added. \"People will generally trust their agents with whatever they recommend them to buy, reserve, believe, or otherwise.\"\n\nSimons's comments come as software and SaaS stocks have slumped in recent weeks, a move he attributes to growing investor recognition that AI can now build software autonomously. As AI tools become more capable, he said, long-standing competitive moats based on specialized knowledge are eroding.\n\nHe compared the shift to the transformation of manufacturing over the past century. Craft knowledge once protected businesses, he said, but automation and digital design ultimately displaced many incumbents. \"Today, it's a CAD file you can 3D print,\" Simons said.\n\n\"While every individual does not use these profound new powers directly (like 3D printing a chair), there's an entire new generation of companies that do ‚Äî and they have replaced the previous era of companies,\" Simons explained. \"They disrupted those incumbents by leveraging automation that made it far cheaper and at a massive scale not possible previously when moats were just knowledge and bare hands.\"\n\nFor businesses, the implications are stark. Even traditionally \"safe\" enterprise software may be vulnerable if AI agents can migrate or rebuild systems rapidly.\n\n\"What does it mean when all of software can be written, rewritten, migrated, or otherwise, 100x or 10,000x faster than it could've ever been done before, by a workforce that does not sleep, and can be parallelized near infinitely?\" Simons said. \"It's daunting and mind-bending, and I think the repricing of SaaS in public markets more accurately reflects this today.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "agents",
      "software",
      "knowledge",
      "human",
      "shift",
      "across",
      "systems",
      "product",
      "digital"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/tech-ceo-ai-agents-outnumber-human-employees-eric-simons-stackblitz-2026-2",
    "thumbnail_url": "https://i.insider.com/698b6915a645d11881890f3a?width=1200&format=jpeg",
    "created_at": "2026-02-11T01:19:42.667Z",
    "topic": "finance"
  },
  {
    "slug": "composer-15",
    "title": "Composer 1.5",
    "description": "Improved reasoning over challenging coding tasks by scaling RL over 20x.",
    "fullText": "A few months ago, we released our first agentic coding model, Composer 1. Since then, we've made significant improvements to the model‚Äôs coding ability.\n\nOur new release, Composer 1.5, strikes a strong balance between speed and intelligence for daily use. Composer 1.5 was built by scaling reinforcement learning 20x further on the same pretrained model. The compute used in our post-training of Composer 1.5 even surpasses the amount used to pretrain the base model.\n\nWe see continued improvements on coding ability as we scale. Measured by our internal benchmark of real-world coding problems, we find that the model quickly surpasses Composer 1 and continues to climb in performance. The improvements are most significant on challenging tasks.\n\nComposer 1.5 is a thinking model. In the process of responding to queries, the model generates thinking tokens to reason about the user‚Äôs codebase and plan next steps. We find that these thinking stages are critical to the model‚Äôs intelligence. At the same time, we wanted to keep Composer 1.5 fast and interactive for day-to-day use. To achieve a balance, the model is trained to respond quickly on easy problems with minimal thinking, while on hard problems it will think until it has found a satisfying answer.\n\nTo handle longer running tasks, Composer 1.5 has the ability to self-summarize. This allows the model to continue exploring for a solution even when it runs out of available context. We train self-summarization into Composer 1.5 as part of RL by asking it to produce a useful summary when context runs out in training. This may trigger several times recursively on hard examples. We find that self-summarization allows the model to maintain its original accuracy as context length varies.\n\nComposer 1.5 is a significantly stronger model than Composer 1 and we recommend it for interactive use. Its training demonstrates that RL for coding can be continually scaled with predictable intelligence improvements.\n\nLearn more about Composer 1.5 pricing here.",
    "readingTime": 2,
    "keywords": [
      "tasks composer",
      "coding ability",
      "model",
      "improvements",
      "intelligence",
      "context",
      "model‚Äôs",
      "balance",
      "surpasses",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/composer-1-5",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/composer-1.5-og.png",
    "created_at": "2026-02-10T06:54:10.475Z",
    "topic": "tech"
  },
  {
    "slug": "a16z-partner-says-that-the-theory-that-well-vibe-code-everything-is-flat-wrong",
    "title": "A16z partner says that the theory that we'll vibe code everything is 'flat wrong'",
    "description": "Vibe coding everything is just not worth it, says A16z partner Anish Acharya.",
    "fullText": "Vibe coding everything is just not worth it, says an Andressen Horowitz partner.\n\nOn an episode of the \"20VC\" podcast released on Monday, A16z general partner Anish Acharya said that companies shouldn't use AI-assisted coding for every part of their business.\n\nHe said that software accounts for 8% to 12% of a company's expenses, so using vibe coding to build the company's resource planning or payroll tools would only save about 10%. Relying on AI to write code also carries risks, he said.\n\n\"You have this innovation bazooka with these models. Why would you point it at rebuilding payroll or ERP or CRM,\" Acharya said, referring to enterprise resource planning and customer relationship management software. Salesforce, Microsoft, Oracle, and SAP are among the top providers of such software.\n\nInstead, companies are better off using AI to develop their core businesses or optimize the remaining 90% of their costs, said the venture capitalist of six years.\n\n\"Of course, there will be secular losers. There are specific business models that are now going to be disadvantaged,\" he said. \"But the general story that we're going to vibe code everything is flat wrong, and the whole market is oversold software.\"\n\nAcharya's comments follow a brutal week for software stocks, which dragged down tech and broader markets. The sell-off started when already-wary investors panicked about Anthropic's new AI tool, which can perform a range of clerical tasks for people working in the legal industry.\n\nThe A16z partner joins famed investor Vinod Khosla in saying that stock prices should be ignored when evaluating the future of tech companies.\n\nOn a podcast last month, Khosla dismissed talks of an AI bubble and said investors should not be concerned as long as API call volume, a benchmark of AI usage, remains high.\n\n\"If that's your fundamental metric of what's the real use of your AI, usefulness of AI, demand for AI, you're not going to see a bubble in API calls,\" he said. \"What Wall Street tends to do with it, I don't really care. I think it's mostly irrelevant.\"",
    "readingTime": 2,
    "keywords": [
      "resource planning",
      "vibe coding",
      "software",
      "partner",
      "everything",
      "podcast",
      "business",
      "company's",
      "payroll",
      "code"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/a16z-partner-says-businesses-cannot-vibe-code-everything-tech-stocks-2026-2",
    "thumbnail_url": "https://i.insider.com/698a9944a645d118818905c1?width=1200&format=jpeg",
    "created_at": "2026-02-10T06:54:04.849Z",
    "topic": "finance"
  },
  {
    "slug": "i-made-a-claude-code-guide-thats-a-win95-desktop-with-games",
    "title": "I made a Claude Code guide that's a Win95 desktop with games",
    "description": "A free interactive guide for PMs and CEOs leading dev teams through the AI coding transition. Score your codebase readiness, calculate ROI, and get a 90-day rollout plan.",
    "fullText": "Your developers have Copilot open. They autocomplete functions and paste errors into chat. That gets you maybe 10% of what AI coding tools can do now. The real shift is agentic: one engineer running 3-5 parallel AI sessions that write features, run tests, and open PRs.\n\nSeven chapters. Three interactive tools. One honest answer about your team's speed:\n\nMost teams think they're running Pentium. They're a 486.\n\nTechnical deep-dives are marked üìé so you know what to forward to your engineering lead.",
    "readingTime": 1,
    "keywords": [
      "tools",
      "they're"
    ],
    "qualityScore": 0.55,
    "link": "https://gabezen.com/guide/",
    "thumbnail_url": "https://gabezen.com/guide/og.png",
    "created_at": "2026-02-10T01:21:45.458Z",
    "topic": "tech"
  },
  {
    "slug": "clog-track-and-compare-your-claude-code-usage",
    "title": "Clog ‚Äì Track and compare your Claude Code usage",
    "description": "Track your Claude Code journey. Public leaderboard and profile system for developers. See how your coding sessions stack up, build streaks, and share your progress.",
    "fullText": "[join][--]clog - Claude Code Leaderboard// track your claude code sessions// leaderboard goes live soon_ready",
    "readingTime": 1,
    "keywords": [
      "claude code",
      "leaderboard"
    ],
    "qualityScore": 0,
    "link": "https://clog.sh",
    "thumbnail_url": "https://clog.sh/clog-og-image.png",
    "created_at": "2026-02-10T01:21:44.351Z",
    "topic": "tech"
  },
  {
    "slug": "factory-factory-opensource-alternative-to-codex-app-for-claude",
    "title": "Factory Factory, open-source alternative to Codex App for Claude",
    "description": "Workspace-based coding environment for running multiple Claude Code sessions in parallel. - purplefish-ai/factory-factory",
    "fullText": "purplefish-ai\n\n /\n\n factory-factory\n\n Public\n\n Workspace-based coding environment for running multiple Claude Code sessions in parallel.\n\n factoryfactory.ai\n\n License\n\n MIT license\n\n 18\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n purplefish-ai/factory-factory",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/purplefish-ai/factory-factory",
    "thumbnail_url": "https://opengraph.githubassets.com/72f2d9d9898e13bed120d015134a857e14f77267ee4cd373e64559450fa53038/purplefish-ai/factory-factory",
    "created_at": "2026-02-09T12:42:23.472Z",
    "topic": "tech"
  },
  {
    "slug": "logifai-autocapture-dev-logs-for-ai-coding-assistants",
    "title": "Logifai ‚Äì Auto-capture dev logs for AI coding assistants",
    "description": "Auto-capture development logs for Claude Code ‚Äî stop copy-pasting terminal output. - tomoyaf/logifai",
    "fullText": "tomoyaf\n\n /\n\n logifai\n\n Public\n\n Auto-capture development logs for Claude Code ‚Äî stop copy-pasting terminal output.\n\n License\n\n MIT license\n\n 3\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n tomoyaf/logifai",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/tomoyaf/logifai",
    "thumbnail_url": "https://repository-images.githubusercontent.com/1152746807/546b13ec-53a4-4596-a481-7a6c1210694a",
    "created_at": "2026-02-09T06:54:27.333Z",
    "topic": "tech"
  },
  {
    "slug": "i-see-how-important-ai-is-at-google-so-i-taught-my-kids-about-it-now-theyre-vibe-coding",
    "title": "I see how important AI is at Google, so I taught my kids about it. Now, they're vibe coding.",
    "description": "A Google executive explains why he taught his kids to understand AI ‚Äî and how they ended up vibe coding and participating in a hackathon.",
    "fullText": "This as-told-to essay is based on a conversation with Asif Saleem, a financial services go-to-market lead for Japan and the Asia Pacific region at Google. Asif is the father of 13-year-old Usman Asif and 18-year-old Shanzey Asif. It's been edited for length and clarity.\n\nIt's important for parents to help kids understand we're going through a very transformative time. This is like the era when the internet or mobile first emerged. With AI, it may be even bigger.\n\nKids need to understand this and embrace the technology. Whatever they end up studying ‚Äî computer science, English, or philosophy ‚Äî they can make AI part of it.\n\nI was curious about Cursor and some of the other vibecoding tools, so I joined a few \"Code with AI\" weekend sessions for executives.\n\nI really enjoyed it. Within a matter of hours, we were able to develop different applications. I developed a statement analyzer for a financial services system.\n\nOnce I came back home, I spoke to my family about it and showed them the demo. My children, Usman and Shanzey, are both tech-savvy, so that drove their interest. They attended the same course a few weeks later.\n\nThey were the youngest in the class. The good part was that they were completely independent, so I let them be on their own. That's how they ventured into vibe coding and participated in Cursor's 24-hour weekend hackathon in Singapore.\n\nThey've become more curious about things. Through vibe coding, they've learned to be more creative and to use technology to understand how things work.\n\nNeither of them has a formal technology background ‚Äî they're not software developers ‚Äî but they've been able to think through ideas, be more creative, and use technology to solve problems.\n\nTechnology is the biggest enabler. The question is how humans use it ‚Äî whether for good or bad. If it's for good, what are the guardrails?\n\nUsing it for good means creating value, solving real challenges, and making information more accessible.\n\nManaging screentime is the reason we developed a reward system for Usman because he's young and he loves gaming. Gaming comes as a reward for achieving goals. Those goals include making your own breakfast, making your bed, or helping clean the house.\n\nWhen Shanzey is studying, she can't use AI for the content she's creating ‚Äî it can't even be AI-inspired. It has to be her original work.\n\nThat's super important because schools validate whether output is AI-generated. The same applies to Usman.\n\nIt's important to have both physical and digital skills. Usman plays football because physical activity is important at his age. Shanzey is focused on school right now ‚Äî her exams are very important, and getting good grades matters.\n\nI'd say my role is more like a coach ‚Äî brainstorming with them. My wife does the same, helping them think through ideas and keeping them honest.\n\nI'm often busy with work, so a lot of this couldn't be done without my wife's help.\n\nThere's nothing better than hands-on learning. That's something I've learned over the last few years.\n\nYou will face challenges and go through them. Parents should get their kids to do things, give them challenges, and nurture them as they work through those challenges. With hands-on experience, they can get better results faster.\n\nWith building an app, we co-create the idea, but then the important part is making it happen and reporting back on progress.\n\nAt Google, I work with enterprise customers trying to transform their businesses with AI. I can see what's happening on the ground and how things are changing.\n\nI also see a lot of young talent at Google. They come in thinking about creating apps and learning skills, and I mentor some of them as well.\n\nIt's important to communicate this to my family. I spent time helping them understand how the world is changing and why it's important for them to understand AI.\n\nIt's not about running away from technology. AI will keep advancing, and the only thing you can do is be accustomed to it, no matter what you want to become.\n\nWe're now in a situation where we have very intelligent large language models, and we're also moving toward agentic AI, where you can work with agents that help you do a lot more. Speed and agility are improving, and you can now work within larger ecosystems that combine humans and machines, achieving much more.\n\nIf you know how to coexist and let machines do meaningful work with you, that's a skill to aspire to.\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "financial services",
      "vibe coding",
      "technology",
      "understand",
      "that's",
      "challenges",
      "kids",
      "we're",
      "creating",
      "it's"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-ai-teach-teenagers-vibe-coding-kids-family-tech-2026-2",
    "thumbnail_url": "https://i.insider.com/6954c24104eda4732f2e4c0d?width=1200&format=jpeg",
    "created_at": "2026-02-09T06:54:22.473Z",
    "topic": "finance"
  },
  {
    "slug": "the-guy-who-coined-vibecoding-says-the-next-big-thing-is-agentic-engineering",
    "title": "The guy who coined 'vibe-coding' says the next big thing is 'agentic engineering'",
    "description": "OpenAI cofounder Andrej Karpathy says 'agentic engineering is the next evolution in AI coding as vibe-coding marks its first anniversary.",
    "fullText": "\"Vibe-coding\" just celebrated its first birthday. That's a lifetime in the AI boom.\n\nNow, the man who coined the term is celebrating the birth of a new one: \"agentic engineering.\"\n\nWhile vibe-coding is when humans prompt AI to write code, OpenAI cofounder Andrej Karpathy says agentic engineering is when AI agents write the code themselves.\n\n\"Many people have tried to come up with a better name for this to differentiate it from vibe coding, personally, my current favorite is 'agentic engineering,'\" he wrote in a recent X post.\n\nKarpathy said he calls it \"agentic engineering\" not just because agents are writing the code, but because \"there is an art & science and expertise to it.\"\n\nVibe-coding is one of the biggest innovations of the AI revolution. Prominent CEOs and startup founders alike are encouraging the use of vibe-coding across their teams. And billions are being poured into new vibe-coding companies.\n\nLovable, one of Europe's fastest-growing startups, announced that it had raised $330 million in Series B funding at a $6.6 billion valuation in December. Cursor, an AI-assisted code editor, announced a Series D funding round of $2.3 billion in November and said it had surpassed $1 billion in annualized revenue.\n\nThe approach is also threatening traditional engineering jobs. In a Business Insider survey of 167 software engineers, 75 engineers said that they were \"keeping up,\" while 30 said they felt ahead of the curve, and 27 felt behind.\n\n\"Vibe coding is now mentioned on my Wikipedia as a major memetic 'contribution,' and even its article is longer. lol,\" Karpathy wrote on X about its meteoric rise.\n\nKarpathy was a founding member of OpenAI in 2015, years before competitors like Anthropic and xAI emerged. He later moved into self-driving technology, leading Tesla's Autopilot program as head of AI. He's now building Eureka Labs, which describes itself on its website as building a \"new kind of school that is AI native.\"",
    "readingTime": 2,
    "keywords": [
      "series funding",
      "vibe coding",
      "agentic engineering",
      "vibe-coding",
      "code",
      "openai",
      "agents",
      "engineers",
      "karpathy"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/agentic-engineering-andrej-karpathy-vibe-coding-2026-2",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-02-09T01:12:58.161Z",
    "topic": "finance"
  },
  {
    "slug": "gitbutler",
    "title": "GitButler",
    "description": "GitButler software development platform",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://gitbutler.com/",
    "thumbnail_url": "https://gitbutler.com/og-image.png",
    "created_at": "2026-02-08T18:22:32.834Z",
    "topic": "tech"
  },
  {
    "slug": "prepare-your-oss-repo-for-ai-coding-assistants",
    "title": "Prepare your OSS repo for AI coding assistants",
    "description": "I‚Äôve been seeing more and more open source maintainers throwing [...]",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://angiejones.tech/stop-closing-the-door-fix-the-house/",
    "thumbnail_url": "https://angiejones.tech/wp-content/uploads/2026/02/open-source-is-closed.png",
    "created_at": "2026-02-08T18:22:32.063Z",
    "topic": "tech"
  },
  {
    "slug": "an-identity-verification-ai-startup-is-having-all-employees-try-vibe-coding",
    "title": "An identity verification AI startup is having all employees try vibe coding",
    "description": "A San Francisco AI startup is giving out stipends and dedicating project days for non-tech staff to vibe code apps that help with their workflow.",
    "fullText": "If you work for a San Francisco startup and don't know how to code, you could soon be asked to get creative with vibe coding.\n\nCheckr, an AI-powered background check company, gave Business Insider a glimpse of how employees are actually using AI.\n\nCheckr CEO Daniel Yanisse said that the company is going \"all in\" and trying everything to encourage its employees to fully embrace AI ‚Äî including staff that don't work in engineering roles.\n\n\"We really pride ourselves on using AI to the maximum possible amount,\" said Yanisse. \"We gave every employee a monthly stipend to try AI tools, and we did AI days and demos. After one year, 95% of the employees use prompting daily.\"\n\n\"This year, we're going to level up and move to building with AI, as in vibe coding,\" Yanisse added. \"I'm working with all of our teams now, and we're going to do our AI days soon in March, where we're going to make every non-technical person vibe code their own business apps.\"\n\nYanisse said that many employees who have no idea how to code, who work in finance, legal, and HR, are already vibecoding apps to automate their workflows and problem-solve, such as building tools that help clean up large spreadsheets.\n\nWhile Checkr is evaluating a variety of builder tools like Lovable, Replit, and Claude Code, Yanisse said Cursor is a clear standout and \"has amazing adoption\" among both engineers and non-technical staff, but Lovable is the best place to start for people with no coding experience.\n\n\"Probably, we're going to buy all of them and just use the right tool for the right person,\" Yanisse said of different AI coding tools.\n\n\"We have AI solution engineers who are available to actually partner and help, so they would come and help you and unstuck you if you have a problem, and take you all the way to success,\" Yanisse added. \"And then you're on your way because then we share success stories with everyone in the company.\"\n\nIn practice, data shows that AI adoption can be complicated in a large enterprise. Competence with AI tools can be very uneven across the board, and security risks can mount without clear guidelines on AI usage.\n\nAccording to a survey published in November by Moveworks, an AI-powered platform that automates IT and HR support, most executives said that non-technical employees are playing a bigger role in driving AI use, and that 78% have seen successful AI projects originate directly from support staff looking to solve daily challenges.\n\nThe National Cybersecurity Alliance also wrote in its Annual Cybersecurity Attitudes and Behaviors Report that¬†AI adoption has surged to 65% globally as of the end of 2025, but more than half of these AI users never received any training in privacy and security risks. The report surveyed over 6,500 workers worldwide.\n\n\"A few years ago, most businesses were still debating whether AI was something they really needed,\" Louis Riat-Bonello of Optisearch, an AI-powered marketing platform that specializes in SEO, told Business Insider.\n\n\"The businesses getting the best results aren't blindly chasing automation. They're using AI to support smarter decisions, move faster, and free up teams to focus on strategy and creativity,\" Riat-Bonello added. \"That balance is what will matter long after the hype fades.\"\n\nYanisse said that in the age of AI, the company is looking for creative and adaptable people, because while AI will eliminate some roles, it will create others.\n\n\"We are constantly training and helping people update their skills and careers,\" said Yanisse. \"The job of the product designer and the job of the marketer are all completely shifting right now.\"\n\n\"We're over 900 people, so we're not a small startup, but I'm a startup guy, and I'm a builder,\" Yanisse added. \"The people who come here need to be OK with uncertainty, be self-driven, adaptable, flexible, willing to do new things, and solve new problems without too much guidance or structure.\"",
    "readingTime": 4,
    "keywords": [
      "security risks",
      "vibe coding",
      "we're",
      "employees",
      "tools",
      "code",
      "startup",
      "ai-powered",
      "yanisse",
      "staff"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/checkr-non-technical-employees-vibecode-with-stipends-and-ai-days-2026-2",
    "thumbnail_url": "https://i.insider.com/6983e6bda645d1188188b87e?width=1200&format=jpeg",
    "created_at": "2026-02-07T12:25:49.164Z",
    "topic": "finance"
  },
  {
    "slug": "what-i-wish-i-knew-before-building-a-vibe-coding-platform",
    "title": "What I wish I knew before building a vibe coding platform",
    "description": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.",
    "fullText": "Avoid common pitfalls and learn what actually matters when building a vibe coding platform. From the builders of Imagine, we'll share some hard-earned lessons that we wish we knew before we embarked on this journey.\n\nI'm Ariel, VP of AI at Appwrite. Over the past year, my team and I built Imagine ‚Äî a vibe-coding platform that lets users build production-ready web apps using prompting. That means real backends, server-side rendering, server functions, sandboxes, previews, the whole thing.\n\nAppwrite is one of the most impactful open-source projects in the world, and in the spirit of open-source, we want to share our learnings with the community. This article isn't a how-to. It's a collection of hard-earned lessons ‚Äî the kind you only learn after shipping, breaking things, and paying real infrastructure bills.\n\nIf you don't take prompt caching seriously from day one, you will burn money and time. This is especially true for vibe-coding platforms.\n\nVibe coding platforms naturally rely on long-running agentic loops. Agents talk to themselves, call tools, revise plans, generate files, run tests ‚Äî sometimes over dozens of steps and millions of tokens. That's unavoidable if you want high-quality results.\n\nThe good news: with correct prompt caching, this doesn't have to be expensive or slow.\n\nHow you structure and order your prompts has a massive impact on cache hit rates. In practice, almost everything should be cacheable.\n\nWe're huge fans of Anthropic, and they allow you to define explicit cache breakpoints. For example:\n\nFirst breakpoint: after fully static system messages. Core instructions, rules, policies ‚Äî things that never change.\n\nSecond breakpoint: after semi-dynamic system messages. Generation number, project metadata, decisions made by earlier agents.\n\nFinal breakpoint: right before the latest user or agent message.\n\nThis structure allows long agentic loops to reuse the majority of their context even when the workflow branches or retries. If you place breakpoints carelessly, a single conditional change can invalidate your entire cache.\n\nThis sounds unrealistic at first, but it's absolutely achievable.\n\nA good benchmark is 90‚Äì95% cache hit rate on input tokens. At that point, the economics completely change:\n\nThis is the difference between a platform that feels sluggish and expensive, and one that feels snappy and scalable.\n\nMake sure you know how to observe caching behavior. Anthropic does an excellent job here ‚Äî their console shows per-request caching status and aggregated cache metrics over time. Anthropic's documentation for prompt caching is state-of-the-art and everyone should read it.\n\nPro tip: You might be tempted to use cheaper models for certain tasks in your workflow, thinking it would be faster or cheaper. With Anthropic for example, cache is per model, so plugging that Sonnet call at the end of your mostly-Opus workflow would result in a cache miss. Do caching right and you can leverage the best models for the job without breaking the bank.\n\nThe average request we make to Anthropic is 99% cached. Screenshot taken from Anthropic's console.\n\nMost frameworks and SDKs teach you the same thing:\n\nSend a request, get a response, stream tokens back to the client.\n\nThat's fine for demos. It completely breaks down for vibe coding.\n\nIn a vibe coding platform where state is fragile (sandbox, repo, uncommitted changes, ongoing generation, etc.), so much could go wrong.\n\nTo deal with these issues, we have implemented resumable streams and durable workflows.\n\nAssume you have a /api/chat endpoint where the client submits a prompt. Typically, tutorials would teach you to stream the chunks back to the client (for example, using Server-Sent events, or SSE in short).\n\nThat's simply putting too much hope in HTTP connections in a scenario where generations can take minutes to complete. Here is a good approach to dealing with this:\n\nIn our case, we serve the stream via a separate service altogether to reduce load on our core AI API, but that's not strictly necessary. Redis becomes the source of truth for in-progress generations.\n\nStreaming alone isn't enough. You also need durable execution for the workflow itself.\n\nInstead of one long-running function, break the process into orchestrated steps:\n\nSandbox provisioning is a good example. In our platform, there are multiple ways a sandbox might be started:\n\nWith durable workflows, all of these can safely funnel into the same sandbox orchestration logic without race conditions or duplication.\n\nWe chose to use Inngest which is open-source, but there are other good options like Temporal, AWS Lambda Durable Functions, Trigger.dev, etc.\n\nAs a bonus point, using Inngest gives us great observability, overview of execution times and alerts on anomalies. It forces us to adopt a more resilient and reliable approach to building our platform, and everybody gets to sleep better at night.\n\nInngest makes it easy to visualize the timeline of each generation and its steps.\n\nWhen building a vibe-coding platform, you have a fundamental choice:\n\nDo I let the system generate anything, or do I force generations to use a specific framework or stack?\n\nWe decided to go with the second approach.\n\nWhen evaluating frameworks, we asked ourselves these questions:\n\nAfter evaluating a few common options, we ultimately chose TanStack Start. It's built by the team behind some of the most popular tools and libraries powering the React ecosystem. TanStack Start uses Vite as its build system, which is highly customizable.\n\nFor our generated apps' runtime, we chose Bun. It's incredibly fast, supports running TypeScript directly (handy for migrations), and is pretty much a drop-in replacement for Node.js.\n\nConcretely, we are able to build our generated apps (server and client code) in ~1 second. For comparison, a fresh Next.js + Turbopack + Bun build takes a few seconds, and that's without any server code. The difference is huge.\n\nImagine's generated apps build in ~1 second, including both client and server code\n\nGenerative AI is inherently non-deterministic. You can prompt all you want, provide examples, two-shot it, five-shot it. But as your message history grows, your context grows as well, and so does the likelihood of facing hallucinations or simply failing to get the model to adhere to your instructions.\n\nWhen users report issues, 90% of those have to do with unexpected AI behavior or issues in the generated code. It's very tempting to take every such issue in isolation and try to fix it by adding an additional bullet point to the system prompt, or providing more examples.\n\nBut wait. There might be a better way. The deterministic way. Here are some examples from Imagine:\n\nThese are just a few examples, but the key takeaway is that you can't rely on the LLM to adhere to your instructions. Whenever you can, embrace determinism.\n\nLet your AI see the same red squiggly lines you appreciate so much.\n\nThe real challenges only show up once you're dealing with real workloads, real users, and real infrastructure bills. By then, it's often too late to \"just refactor\" your way out of architectural decisions made early on.\n\nPrompt caching, durable workflows, deterministic guardrails, and a well-chosen framework should not be an afterthought, or by-the-way optimizations. They are foundational.\n\nAt Imagine, our mission is to tame AI, make the best of it and mitigate its weaknesses. We build Imagine assuming things will fail, disconnect, retry, and resume ‚Äî unexpectedly.\n\nAs we build Imagine, we are constantly learning, and we are excited to share our learnings with the community. If you're building something similar, we hope these hard-earned lessons help you move quickly and avoid the pitfalls.",
    "readingTime": 7,
    "keywords": [
      "infrastructure bills",
      "agentic loops",
      "hard-earned lessons",
      "vibe coding",
      "system messages",
      "durable workflows",
      "generated apps",
      "cache hit",
      "server code",
      "vibe-coding platform"
    ],
    "qualityScore": 1,
    "link": "https://imagine.dev/blog/post/what-i-wish-i-knew",
    "thumbnail_url": "/images/blog/what-i-wish-i-knew/cover.png",
    "created_at": "2026-02-06T18:34:47.896Z",
    "topic": "tech"
  },
  {
    "slug": "tech-stack-is-a-business-decision",
    "title": "Tech stack is a business decision",
    "description": "Why tech stack choices should be driven by business context and constraints‚Äînot framework preferences‚Äîand why this matters even more with agentic coding tools.",
    "fullText": "Developers love to argue about tech stacks.\n\nFlutter vs React Native.\n\nNative vs cross-platform.\n\nJava vs Ruby vs C#.\n\nReact vs Angular.\n\nRiverpod vs Bloc.\n\nDrift vs Hive.\n\nThese debates can go arbitrarily deep, down to library-level choices that have little impact on whether a product succeeds or fails.\n\nEveryone has preferences, and that‚Äôs fine. Preferences come from experience. Someone who started with strongly typed languages will value compiler guarantees and explicitness. Someone who started with dynamic languages will value flexibility and speed. Both viewpoints are internally consistent.\n\nWhat they are not is universally correct.\n\nThere is no hard data that can rank one stack as objectively ‚Äúbetter‚Äù across all contexts. Teams, constraints, markets, and goals differ too much. Most arguments rely on anecdotes, and anecdotes do not generalize.\n\nSo if preference is not a reliable guide, how should a tech stack be chosen?\n\nThis question is often interpreted at the feature or use-case level. That misses the point.\n\nIn almost all non-hobby software, the problem being solved is a business problem.\n\nIf there is no business, no users, no revenue, then the software is irrelevant. It may be elegant or technically impressive, but it has no economic value. Software only matters insofar as it supports a business outcome.\n\nThat framing matters because it establishes the correct hierarchy.\n\nTechnology is not the goal.\n\nTechnology is a tool.\n\nAnd tools are chosen based on the job they need to do.\n\nEarly on, nobody knows whether the business will work.\n\nYou don‚Äôt know if users care. You don‚Äôt know if they will pay. You don‚Äôt know if the problem is real or imagined.\n\nAt this stage, obsessing over the ‚Äúright‚Äù stack is misplaced effort. The dominant constraint is uncertainty, not scalability or architectural purity.\n\n‚ÄúThe FIRST rule of enterprenuership is you use what you have‚Äù - Alex Hormozi\n\nThis is simple rule of entrepreneurship applies surprisingly well to engineering.\n\nFounders and early teams start with what they already know. Not because it is theoretically optimal, but because it minimizes friction. Familiar tools reduce cognitive overhead, increase speed, and allow faster learning.\n\nEarly success is rarely determined by technical elegance. It is determined by whether you can ship something people want before you run out of time or money.\n\nAs the business grows, the constraints change.\n\nRevenue appears.\n\nUsers depend on the system.\n\nThe team grows.\n\nThe cost of failure increases.\n\nNow the stack begins to matter more. Not because some technologies are inherently superior, but because the software must support new business realities. Hiring, onboarding, operational cost, maintainability, performance characteristics, and risk management all become relevant.\n\nThis is where teams often conclude that an early stack choice was ‚Äúwrong.‚Äù In most cases, it wasn‚Äôt wrong. It was right for the phase the business was in. The mistake was assuming it would remain right indefinitely.\n\nTwitch is often mentioned as a scaling story, but the more interesting part is how long the architecture stayed simple.\n\nAt one point, Twitch was serving millions of users with a monolithic Ruby on Rails application and a single PostgreSQL database. That setup was not elegant or future proof, but it worked. It allowed the team to move quickly, ship features, and grow the business without adding unnecessary complexity.\n\nOver time, the constraints changed. Growth amplified failures. Database contention increased. Reliability, support load, and churn started to matter more than raw development speed. The cost of incidents became visible in a way it had not been before.\n\nThat was when the architecture began to evolve. The database was scaled and restructured, services were split out, and new infrastructure and languages were introduced. None of this happened because the original stack was flawed. It happened because the business had outgrown it.\n\nThe technology changed in response to the business, not the other way around.\n\nIf you want to read more about how Twitch migrated from a monolith to microservices, see Breaking the Monolith at Twitch and Breaking the Monolith at Twitch: Part Two.\n\nIf you want to know more about how Twitch scaled its database, see How Twitch Uses PostgreSQL.\n\nAgentic coding tools make this even more important.\n\nTools like Claude Code and Cursor reduce the advantage of deep, framework-specific familiarity. Generating boilerplate, navigating unfamiliar code, applying patterns across modules, and keeping implementation consistent are no longer manual, error-prone tasks.\n\nWhen an agent can help you stay productive across technologies, the cost of switching or extending a stack drops. What remains expensive is not syntax or APIs, but domain knowledge, product decisions, and the long-term behavior of the system.\n\nIn other words, the more the tooling levels the technical playing field, the more tech stack decisions become about business fit.\n\nInstead of asking whether one technology is better than another in the abstract, ask questions that reflect the reality of the business:\n\nWhat are we optimizing for right now? Speed, cost, reliability, hiring, or time-to-market?\n\nHow quickly do we need to validate this idea?\n\nHow expensive will change be later, and what changes are most likely?\n\nWhat kind of team will maintain this system?\n\nWhat risks could realistically kill the business, and does the stack reduce or increase them?\n\nOnce framed this way, stack decisions become clearer and less emotional. Sometimes multiple choices are equally valid. Sometimes the boring option is correct. Sometimes speed matters more than correctness. Sometimes correctness matters more than speed.\n\nThere is no universal answer. The answer depends on the context.\n\nMost tech stack arguments are debates about taste, not outcomes.\n\nOnce technology is treated as a business decision rather than a personal identity, those arguments lose their intensity. The question stops being ‚Äúwhat do I prefer?‚Äù and becomes ‚Äúwhat does the business need right now?‚Äù\n\nThe right stack is not the one that wins debates.\n\nIt is the one that helps the business survive and grow at its current stage.\n\nIf are starting a project and need help with tech stack decision, connect with me on¬†LinkedIn and I‚Äôll take it from there.\n\nSoftware Developer specializing in Flutter, Android, and mobile development.\n Writing about code, architecture, and developer productivity.",
    "readingTime": 6,
    "keywords": [
      "tech stack",
      "stack decisions",
      "business",
      "speed",
      "technology",
      "users",
      "tools",
      "database",
      "debates",
      "languages"
    ],
    "qualityScore": 1,
    "link": "https://dinkomarinac.dev/blog/tech-stack-is-a-business-decision/",
    "thumbnail_url": "https://dinkomarinac.dev/og/tech-stack-is-a-business-decision.png",
    "created_at": "2026-02-06T12:35:39.062Z",
    "topic": "tech"
  },
  {
    "slug": "how-exposed-are-software-stocks-to-ai-tools-we-put-vibecoding-to-the-test",
    "title": "How exposed are software stocks to AI tools? We put vibe-coding to the test",
    "description": "How real is the AI threat to software companies? CNBC put it to the test by vibe-coding a Monday.com replacement.",
    "fullText": "Software, legal services and video games stocks have been selling off in recent weeks on fears that new AI features and tools could wipe them out. But how real is that threat? We decided to find out.\n\nCNBC's Deidre Bosa and I used Anthropic's AI coding tool \"Claude Code\" with the goal of creating a replacement for Monday.com, a project management platform with a $5 billion market cap. We didn't expect to get anywhere ‚Äî we're not developers and we don't have any coding experience. But we have become adept at vibe-coding tools, which are AI tools that can build functioning apps based on commands in plain English from users, including those with limited technical chops.\n\nWe started out simple, telling Claude to build a project management dashboard similar to Monday, with features like multiple project boards, assigned team members and a status dropdown. It spit out a working prototype in minutes.\n\nWe then asked Claude to research Monday on its own, identify main features and recreate them. It added a number of other features, including a calendar.\n\nThe real magic happened when we connected the clone to an email account, essentially spinning up a customized project manager for our personal lives. The AI found Dee's forgotten calendar invite for a kid's birthday party (which she definitely didn't have a gift for yet) and it added reminders to book tickets for an upcoming trip and sign a waiver for another kid's birthday party.\n\nIt took us under an hour, and had we been paying users, it would have cost something like $5 to $15 in compute credits, depending on how much we went back and forth with the AI agent. As more data centers get built out, that cost could start to come down.\n\nSo which companies should investors worry about? Silicon Valley insiders we talk to say the most exposed names are the ones that \"sit on top of the work\" ‚Äî tools like Atlassian, Adobe, HubSpot, Zendesk, and Smartsheet, that aren't core to businesses.\n\nThey say cybersecurity stocks like CrowdStrike and Palo Alto are harder to disrupt since they have network effects that no one would want to try to replicate and maintain.\n\nSystems of record may be safer, but not immune. Salesforce, for instance, anchors a business with enterprise data, making it harder to clone with a weekend coding project.\n\nWith the wholesale sell-off in software this year, that may be a chance for investors to separate between the need-to-haves and nice-to-haves.",
    "readingTime": 3,
    "keywords": [
      "kid's birthday",
      "birthday party",
      "project management",
      "features",
      "tools",
      "coding",
      "software",
      "stocks",
      "didn't",
      "users"
    ],
    "qualityScore": 1,
    "link": "https://www.cnbc.com/2026/02/05/how-exposed-are-software-stocks-to-ai-tools-we-tested-vibe-coding.html",
    "thumbnail_url": "https://image.cnbcfm.com/api/v1/image/108184664-1754999709333-gettyimages-2214520187-img_1248.jpeg?v=1754999753&w=1920&h=1080",
    "created_at": "2026-02-06T06:40:18.191Z",
    "topic": "finance"
  },
  {
    "slug": "termoil-terminal-dashboard-for-managing-parallel-ai-coding-agents",
    "title": "Termoil ‚Äì Terminal dashboard for managing parallel AI coding agents",
    "description": "Less friction for your multi-agent workflow. Contribute to fantom845/termoil development by creating an account on GitHub.",
    "fullText": "fantom845\n\n /\n\n termoil\n\n Public\n\n Less friction for your multi-agent workflow\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n fantom845/termoil",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/fantom845/termoil",
    "thumbnail_url": "https://opengraph.githubassets.com/3f4c60f9807e7fd26c4cd7d8070eb9f12a1c2217f6b59354a3d271fb86812016/fantom845/termoil",
    "created_at": "2026-02-06T06:40:14.221Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-ai-ceo-says-vibe-coding-is-making-software-easier-to-replace",
    "title": "Microsoft AI CEO says vibe coding is making software easier to replace",
    "description": "Microsoft AI CEO says vibe coding is lowering the barrier to building apps, raising questions about how defensible software will be.",
    "fullText": "Mustafa Suleyman says vibe coding is rapidly lowering the barrier to building apps, a shift that could put traditional software at risk.\n\nThe Microsoft AI CEO said in an episode of the \"Exponential View\" podcast published Thursday that AI tools now make it possible for anyone to quickly start launching code and apps.\n\n\"It is so accessible now,\" said Suleyman. \"You can watch a three-minute video, get spun up, launch one of these things.\"\n\n\"You can create an app, a web app in seconds,\" he added.\n\nSuleyman said people don't need deep technical skills to get started. Instead, they can learn by experimenting, watching, and doing.\n\nThe AI can \"build something that you may have thought was never possible,\" he said.\n\n\"Unless you push these things to their edges and explore the boundaries, you won't really understand the magic, or what they're kind of bad at.\"\n\n\"Everyone's got to get stuck into that motion,\" he added.\n\nSuleyman also said he has vibe-coded a system that tracks the DJs he wants to see, coming concerts and festivals, and then matches those events with his travel schedule. What used to be manual work now runs automatically in a spreadsheet that updates throughout the year.\n\nSuleyman's comments come as investors grow increasingly anxious that AI tools and agents could wipe out entire categories of software.\n\nThat fear flared this week after Anthropic said it was adding legal-focused capabilities to its Cowork assistant. The tools would allow AI to review legal documents and track compliance ‚Äî work typically done by legal software.\n\nMarkets didn't take it lightly. Shares of legal-software companies in Europe and the US fell sharply on Tuesday, before the selling spread across the wider software sector and into tech.\n\nOpenAI triggered a similar sell-off months earlier after rolling out internal AI-powered software-as-a-service tools.\n\nMany of the tools now unsettling the tech sector were built using AI coding tools.\n\nAI personal assistant OpenClaw was created with the help of AI, while Moltbook ‚Äî a viral, Reddit-style forum for AI agents ‚Äî was entirely vibe-coded.\n\nAnthropic also said last month that it built its Cowork assistant using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. During a livestream, Rieseberg said his team put Cowork together in just over a week, thanks to Claude.\n\nTech leaders and developers have also said that such a turnaround is becoming the norm.\n\nPeter Steinberger, the developer behind OpenClaw, said in an episode of \"Behind the Craft\" podcast published last week that AI now lets developers \"build everything.\"\n\nOpenAI's chair, Bret Taylor, said in an episode of the \"Big Technology Podcast\" published last week that building software quickly through vibe coding will soon feel routine rather than novel.\n\nBut the real question, Taylor said, is what software still matters.\n\nInstead of dashboards, browser forms, and traditional apps, he expects AI agents to become the dominant software interface.\n\n\"Who's making those agents is the question,\" he said. \"Will you buy those agents off the shelf or build them yourself?\"",
    "readingTime": 3,
    "keywords": [
      "cowork assistant",
      "vibe coding",
      "podcast published",
      "software",
      "tools",
      "agents",
      "apps",
      "episode",
      "traditional",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/microsoft-ai-ceo-vibe-coding-software-replace-apps-mustafa-suleyman-2026-2",
    "thumbnail_url": "https://i.insider.com/6985593bd3c7faef0ecdbefa?width=1200&format=jpeg",
    "created_at": "2026-02-06T06:40:12.752Z",
    "topic": "tech"
  },
  {
    "slug": "a-very-small-sat-solver-from-haskell-now-in-dafny-proved-correct-with-llms",
    "title": "A Very Small SAT Solver (From Haskell) Now in Dafny, Proved Correct with LLMs",
    "description": "Dafny for Metatheory of Programming Languages. Contribute to namin/dafny-sandbox development by creating an account on GitHub.",
    "fullText": "Skip to content\n\n You signed in with another tab or window. Reload to refresh your session.\n You signed out in another tab or window. Reload to refresh your session.\n You switched accounts on another tab or window. Reload to refresh your session.\n\nDismiss alert\n\n namin\n\n /\n\n dafny-sandbox\n\n Public\n\n You can‚Äôt perform that action at this time.",
    "readingTime": 1,
    "keywords": [
      "window reload",
      "another tab",
      "refresh",
      "session",
      "signed"
    ],
    "qualityScore": 0.3,
    "link": "https://github.com/namin/dafny-sandbox/blob/master/Sat.dfy",
    "thumbnail_url": "https://opengraph.githubassets.com/37f757a4876729fbb57a662e8bc8730df5cfc0bc7de82e6b36f6b3d99b39b44a/namin/dafny-sandbox",
    "created_at": "2026-02-06T01:06:52.715Z",
    "topic": "tech"
  },
  {
    "slug": "openais-new-model-leaps-ahead-in-coding-capabilitiesbut-raises-unprecedented-cybersecurity-risks",
    "title": "OpenAI‚Äôs new model leaps ahead in coding capabilities‚Äîbut raises unprecedented cybersecurity risks",
    "description": "Why OpenAI‚Äôs latest coding breakthrough is forcing the company to rethink how‚Äîand how fast‚Äîit can deploy its most powerful models.",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/02/05/openai-gpt-5-3-codex-warns-unprecedented-cybersecurity-risks/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/08/GettyImages-2214110034.jpg?resize=1200,600",
    "created_at": "2026-02-06T01:06:50.989Z",
    "topic": "business"
  },
  {
    "slug": "programming-your-own-modern-bbs-with-python",
    "title": "Programming Your Own Modern BBS with Python",
    "description": "Previously, we looked at dialling BBS with Vice C64 and the C64U. Now, let's code a modern BBS from scratch using Python!",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://retrogamecoders.com/programming-bbs-with-python/",
    "thumbnail_url": "https://retrogamecoders.com/wp-content/uploads/2026/02/custom-bbs-server-in-python.jpg",
    "created_at": "2026-02-05T18:36:01.035Z",
    "topic": "tech"
  },
  {
    "slug": "the-coding-agent-os",
    "title": "The Coding Agent OS",
    "description": "We believe that a great coding agent isn't a simple chatbot‚Äîit's an operating system for the model.",
    "fullText": "Most coding agents are presented as a chat UI glued to an LLM and integrated into an IDE or\n CLI. This can work really well for developers who provide the development environment along\n with constant babysitting to build functional software.\n\nCharlie, however, is built around a different premise: We believe that a great coding agent\n isn't a simple chatbot‚Äîit's an operating system for the model.\n\nCharlie's \"OS layer\" gives a model the primitives required to complete end-to-end\n engineering work:\n\nWhen agents run in an operating system, they can diagnose ‚Üí plan ‚Üí implement ‚Üí verify ‚Üí\n publish reviewable artifacts all within the systems where your team already works.\n\nWe've got two hot takes on why the OS is essential for great agents:\n\nCLI expressiveness beats bespoke \"LLM tools.\" CLI tools are composable (|, &&, xargs), self-documenting (--help), and match\n how engineers actually work. One-off JSON tools tend to be isolated and require tool\n designers to predict every use case up front.\n\nOpen-loop orchestration beats rigid workflows. Real engineering has surprises:\n flaky tests, missing context, failing builds, CI-only failures, new constraints. The \"OS loop\"\n lets the agent decide the next step based on what it observes, instead of forcing every task into\n a fixed graph.\n\nIDE-native assistants like Claude Code are great for interactive, in-editor work: drafting,\n refactors, quick iteration. But they're not optimized to serve as a team-facing execution\n environment.\n\nOS-style agents are built for that missing layer: cross-tool orchestration + durable,\n reviewable artifacts.\n\nA mention, review request, regression, or scheduled trigger.\n\nCharlie normalizes + enriches the signal\n\nInto \"this is the work request, with relevant context.\"\n\nDecide whether to act, what thread it belongs to, and which capabilities are allowed.\n\nA durable Task is created (or appended-to)\n\nSingle-winner claiming, enters a durable, turn-based loop.\n\nAssemble context, decide + plan, call the LLM, dispatch tool calls, observe results +\n check mailbox/cancel + continue.\n\nBack to the source system (PRs, reviews, issue updates, summaries).\n\nRecord terminal status + telemetry\n\nDurable transcript/state remains; ephemeral compute is cleaned up.\n\nThe agentic operating system delivers better outcomes:",
    "readingTime": 2,
    "keywords": [
      "reviewable artifacts",
      "operating system",
      "agents",
      "durable",
      "tools",
      "context",
      "coding",
      "environment",
      "agent",
      "model"
    ],
    "qualityScore": 1,
    "link": "https://charlielabs.ai/coding-agents-as-operating-systems/",
    "thumbnail_url": "https://charlielabs.ai/_astro/caos-architecture.JbhqvI6s.png",
    "created_at": "2026-02-05T12:36:57.286Z",
    "topic": "tech"
  },
  {
    "slug": "agentic-engineering",
    "title": "Agentic Engineering",
    "description": "Agentic Engineering is a disciplined approach to AI-assisted software development that emphasizes human oversight and engineering rigor, distinguishing it fr...",
    "fullText": "A year ago, Andrej Karpathy coined ‚Äúvibe coding‚Äù to describe a gleefully reckless way of programming: you prompt, hand the keyboard to an AI, accept everything it spits out, don‚Äôt read the diffs, iterate by pasting error messages back in. It was a great label for a real thing - building quick prototypes or MVPs on pure AI autopilot.\n\nThe problem is that ‚Äúvibe coding‚Äù has become a suitcase term. People now use it to describe everything from a weekend hack to a disciplined engineering workflow where AI agents handle implementation under human oversight. These are fundamentally different activities, and conflating them is causing real confusion - and real damage.\n\nVibe coding means going with the vibes and not reviewing the code. That‚Äôs the defining characteristic. You prompt, you accept, you run it, you see if it works. If it doesn‚Äôt, you paste the error back and try again. You keep prompting. The human is a prompt DJ, not an engineer.\n\nIf vibe coding gives millions of people the ability to create custom software who otherwise couldn‚Äôt, that‚Äôs a genuine win. The technique has a legitimate place in the toolbox.\n\nBut the failure modes are well-documented at this point. The pattern is always the same: it demos great, then reality arrives. You try to modify it, scale it, or secure it, and you discover nobody understands what the code is actually doing. As one engineer put it, ‚ÄúThis isn‚Äôt engineering, it‚Äôs hoping.‚Äù\n\nHere‚Äôs the thing: a lot of experienced engineers are now getting massive productivity gains from AI - 2x, 5x, sometimes more - while maintaining code quality. But the way they work looks nothing like vibe coding. They‚Äôre writing specs before prompting. They‚Äôre reviewing every diff. They‚Äôre running test suites. They‚Äôre treating the AI like a fast but unreliable junior developer who needs constant oversight. I‚Äôve personally liked ‚ÄúAI-assisted engineering‚Äù and have talked about how this describes that end of the spectrum where the human remains in the loop.\n\nSimon Willison (whose work I adore) proposed ‚Äúvibe engineering‚Äù for this - it reclaims ‚Äúvibe‚Äù while adding ‚Äúengineering‚Äù to signal discipline. But after watching the community debate this for months, I think the the word ‚Äúvibe‚Äù carries too much baggage. It signals casualness. When you tell a CTO you‚Äôre ‚Äúvibe engineering‚Äù their payment system, you can see the concern on their face.\n\nAndrej Karpathy suggested ‚Äúagentic engineering‚Äù this week and I think I like it.\n\nIt describes what‚Äôs actually happening. You‚Äôre orchestrating AI agents - coding assistants that can execute, test, and refine code - while you act as architect, reviewer, and decision-maker. You might write only a % of the code by hand. The rest comes from agents working under your direction. That‚Äôs agentic. And you‚Äôre applying engineering discipline throughout. That‚Äôs engineering.\n\nIt‚Äôs professionally legible. ‚ÄúAgentic engineering‚Äù sounds like what it is: a serious engineering discipline involving autonomous agents. You can say it to your VP of Engineering without embarrassment. You can put it in a job description. You can build a team practice around it.\n\nIt draws a clean line. Vibe coding = YOLO. Agentic engineering = AI does the implementation, human owns the architecture, quality, and correctness. The terminology itself enforces the distinction.\n\nThe workflow isn‚Äôt complicated, but it requires discipline that vibe coding explicitly abandons:\n\nYou start with a plan. Before prompting anything, you write a design doc or spec - sometimes with AI assistance. You break the work into well-defined tasks. You decide on the architecture. This is the part vibe coders skip, and it‚Äôs exactly where projects go off the rails.\n\nYou direct, then review. You give the AI agent a well-scoped task from your plan. It generates code. You review that code with the same rigor you‚Äôd apply to a human teammate‚Äôs PR. If you can‚Äôt explain what a module does, it doesn‚Äôt go in.\n\nYou test relentlessly. The single biggest differentiator between agentic engineering and vibe coding is testing. With a solid test suite, an AI agent can iterate in a loop until tests pass, giving you high confidence in the result. Without tests, it‚Äôll cheerfully declare ‚Äúdone‚Äù on broken code. Tests are how you turn an unreliable agent into a reliable system.\n\nYou own the codebase. You maintain documentation. You use version control and CI. You monitor production. The AI accelerates the work, but you‚Äôre responsible for the system.\n\nTeams doing this well often report faster development - and those gains come from augmenting a solid process, not abandoning one. The AI handles boilerplate and grunt work. The human focuses on architecture, correctness, edge cases, and long-term maintainability.\n\nThe irony is that AI-assisted development actually rewards good engineering practices more than traditional coding does. The better your specs, the better the AI‚Äôs output. The more comprehensive your tests, the more confidently you can delegate. The cleaner your architecture, the less the AI hallucinates weird abstractions. As one analysis noted, ‚ÄúAI didn‚Äôt cause the problem; skipping the design thinking did.‚Äù\n\nHere‚Äôs an uncomfortable truth from the trenches: agentic engineering disproportionately benefits senior engineers. If you have deep fundamentals - you understand system design, security patterns, performance tradeoffs - you can leverage AI as a massive force multiplier. You know what good code looks like, so you can efficiently review and correct AI output.\n\nBut if you‚Äôre junior and you lean on AI before building those fundamentals, you risk a dangerous skill atrophy. You can produce code without understanding it. You can ship features without learning why certain patterns exist. Several engineering leaders have flagged this as an emerging crisis: a generation of developers who can prompt but can‚Äôt debug, who can generate but can‚Äôt reason about what they‚Äôve generated.\n\nThis isn‚Äôt an argument against AI-assisted development. It‚Äôs an argument for being honest about what it demands. Agentic engineering isn‚Äôt easier than traditional engineering - it‚Äôs a different kind of hard. You‚Äôre trading typing time for review time, implementation effort for orchestration skill, writing code for reading and evaluating code. The fundamentals matter more, not less.\n\nThe trajectory is clear: AI agents are getting more capable, and the agentic engineering workflow is becoming default for a growing number of professional developers. This is going to accelerate.\n\nThe rise of AI coding doesn‚Äôt replace the craft of software engineering - it raises the bar for it. The developers who‚Äôll thrive aren‚Äôt the ones who prompt the fastest. They‚Äôre the ones who think the clearest about what they‚Äôre building and why, then use every tool available - including AI agents - to build it well.\n\nVibe coding showed us what‚Äôs possible when you drop all conventions.\n\nNow it‚Äôs time to bring the engineering back. Let‚Äôs call that what it is.\n\nI‚Äôve written a new book with O‚ÄôReilly, Beyond Vibe Coding, that goes deeper into practical frameworks for AI-Assisted (and agentic) engineering. If you‚Äôve been figuring this out in your own workflow, I‚Äôd love to hear what‚Äôs working.",
    "readingTime": 6,
    "keywords": [
      "ai-assisted development",
      "vibe coding",
      "agentic engineering",
      "engineering workflow",
      "engineering it‚Äôs",
      "engineering discipline",
      "andrej karpathy",
      "the ai",
      "code",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/agentic-engineering/",
    "thumbnail_url": "https://addyosmani.com/assets/images/agentic-engineering.jpg",
    "created_at": "2026-02-05T12:36:55.943Z",
    "topic": "tech"
  },
  {
    "slug": "mark-zuckerbergs-2004-coding-jams-were-loud-emo-and-very-on-brand",
    "title": "Mark Zuckerberg's 2004 coding jams were loud, emo, and very on brand",
    "description": "Mark Zuckerberg published a playlist of five songs he was pumping while building Facebook in 2004. It includes hard-rocking hits and some synth funk.",
    "fullText": "Mark Zuckerberg is feeling nostalgic.\n\nOn the 22nd anniversary of Facebook's founding, Zuckerberg reposted a meme about the fresh-faced Harvard student \"getting ready to make history.\" Then he linked a playlist titled \"2004 Facebook coding jams.\"\n\nZuckerberg said he \"had these bangers on repeat\" while building Facebook. The playlist's cover features the young founder ‚Äî¬†long before his bulked-up rebrand ‚Äî¬†listening to music on headphones.\n\nThe playlist's five songs feature a fair share of emo wails, electric guitars, and even some funky synthesizers. Some of them have receded into the tides of music history; others remain classics for weddings and bar mitzvahs.\n\nThey're also evocative of a quieter, easier time in tech. There were no culture wars, cage matches, or capex worries. What would a 2004 Zuckerberg think of $10 billion data centers?\n\nMusic speaks to how we feel. Here's what these five songs might say about a 19-year-old Zuckerberg ‚Äî¬†and just how far he's come.\n\nZuckerberg liked hard rock, it seems.\n\n\"Headstrong\" is a loud, thrashing song that feels built to be played on \"Guitar Hero.\" There's even a good guttural scream in there before one chorus. The lyrics are equally attacking. \"Back off, I'll take you on,\" Chris Taylor Brown sings.\n\nThe opening of \"Headstrong\" sounds like UFC walkout music, hyping up a big fight. That's now a favorite sport of Zuckerberg's. Some things never change.\n\n\"Like a Stone\" is about death and religion ‚Äî two subjects that Zuckerberg has long been interested in.\n\nFrontman Chris Cornell prays to God and angels on their deathbed that they will get into heaven. \"In dreams until my death / I will wander on,\" he sings.\n\nZuckerberg seems more interested in extending human life than forecasting its end. His philanthropic initiative invests heavily in drug and disease research. He also has an intense fitness routine, including jiu-jitsu and CrossFit routines.\n\nAs for faith, Zuckerberg said in 2020 that he had \"become more religious.\" He cited two sources: The issues his company has faced in the prior years and the birth of his daughters.\n\nMostly, though, \"Like A Stone\" is another huge rock hit. It seems like Zuckerberg enjoyed the sound of banging drums and wailing guitar.\n\nNow this one is a nostalgia trip.\n\nYou can't help but feel something when Douglas Robb croons out, \"I'm not a perfect person.\" Was Zuckerberg a perfect person? Most people who have watched \"The Social Network\" would say no.\n\nIt's a romantic song. After considering their flaws, Robb sings out that they've found a reason to change. \"The reason is you,\" he repeats, over and over.\n\nZuckerberg met his wife, Priscilla Chan, in 2003, one year before this playlist's date. Maybe he was thinking of her.\n\nHe'd be less happy to hear that the song reemerged on TikTok ‚Äî¬†a Meta competitor ‚Äî¬†in a 2021 trend.\n\nWe all knew that Zuckerberg liked rap. He recorded his own version of T-Pain's \"Get Low\" in tribute to his wife. He also started dressing like Eminem.\n\n\"In the End\" is peak 2010s emo rap-rock. One of its refrains is \"I tried so hard.\" Zuckerberg did try so hard¬†‚Äî¬†he often worked till late into the night.\n\nThe song is more pessimistic in its outlook: \"In the end, it doesn't even matter.\" Zuckerberg would likely disagree here. He tried so hard ‚Äî¬†and in the end, it did matter. Meta is now a trillion-dollar company.\n\nHard pivot! After four hard rock tunes, Zuckerberg's last song leans into synth-funk.\n\nThe lyrics here feel self-explanatory for Zuckerberg's workplace ethos: \"Work it harder, make it better / Do it faster, makes us stronger / More than ever, hour after hour / Work is never over.\"\n\nAfter Meta's year of intensity, I'm sure that some of Zuckerberg's employees can relate.\n\nAlso, the Daft Punk helmet doesn't look that different from a Meta Quest.",
    "readingTime": 4,
    "keywords": [
      "zuckerberg liked",
      "like stone",
      "song",
      "music",
      "playlist's",
      "rock",
      "sings",
      "history",
      "songs",
      "headstrong"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/mark-zuckerberg-2004-coding-jams-playlist-loud-emo-on-brand-2026-2",
    "thumbnail_url": "https://i.insider.com/69838cc9d3c7faef0ecd9c7b?width=1200&format=jpeg",
    "created_at": "2026-02-05T12:36:52.821Z",
    "topic": "finance"
  },
  {
    "slug": "whats-behind-the-saaspocalypse-plunge-in-software-stocks",
    "title": "What‚Äôs Behind the ‚ÄòSaaSpocalypse‚Äô Plunge in Software Stocks",
    "description": "Since ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.",
    "fullText": "MarketsExplainerBy Lynn Doan and Carmen ReinickeSaveSince ChatGPT arrived on the scene some three years ago, analysts have been warning that entire industries, including software programming, legal services and film production, are at risk of being disrupted by artificial intelligence.But it took a wave of disappointing earnings reports, some improvements in AI models, and the release of a seemingly innocuous add-on from AI startup Anthropic to suddenly wake up investors en masse to the threat. The result has been the biggest stock selloff driven by the fear of AI displacement that markets have seen. And no stocks are hurting more than those of software-as-a-service (SaaS) companies.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.35,
    "link": "https://www.bloomberg.com/news/articles/2026-02-04/what-s-behind-the-saaspocalypse-plunge-in-software-stocks",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iDiaprGarUKI/v0/1200x800.jpg",
    "created_at": "2026-02-05T01:08:08.737Z",
    "topic": "finance"
  },
  {
    "slug": "webhook-skills-agent-skills-for-webhook-providers-and-best-practices",
    "title": "Webhook Skills ‚Äì Agent skills for webhook providers and best practices",
    "description": "Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopif...",
    "fullText": "hookdeck\n\n /\n\n webhook-skills\n\n Public\n\n Webhook integration skills for AI coding agents (Claude Code, Cursor, Copilot). Step-by-step guidance for setting up webhook receivers, signature verification, and event handling for Stripe, Shopify, GitHub, and more. Built on the Agent Skills specification.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n hookdeck/webhook-skills",
    "readingTime": 1,
    "keywords": [
      "webhook",
      "skills",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/hookdeck/webhook-skills",
    "thumbnail_url": "https://opengraph.githubassets.com/ed9e4e830971c76b02864c100a37accbc7ef947718023c6abb2ba2536dad1e19/hookdeck/webhook-skills",
    "created_at": "2026-02-04T12:35:28.405Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-the-new-rad",
    "title": "Vibe Coding is the new RAD",
    "description": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.",
    "fullText": "In my opinion, software engineers should view Vibe Coding with AI as simply the latest iteration of RAD.\n\nFile details: 6.9 MB MP3, 5 mins 12 secs duration.\n\nTitle music is \"Apparent Solution\" by Brendon Moeller, licensed via www.epidemicsound.com\n\nFive.Today is a highly-secure personal productivity application designed to help you to manage your priorities more effectively, by focusing on your five most important tasks you need to achieve each day.\n\n Our goal is to help you to keep track of all your tasks, notes and journals in one beautifully simple place, which is highly secure via end-to-end encryption. Visit the URL Five.Today to",
    "readingTime": 1,
    "keywords": [
      "tasks",
      "five.today"
    ],
    "qualityScore": 0.65,
    "link": "https://techleader.pro/a/723-Vibe-Coding-is-the-new-RAD-(TLP-2026w3)",
    "thumbnail_url": "https://techleader.pro/img/icons/noun_programmer_2644331.png",
    "created_at": "2026-02-04T01:06:56.822Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-good-yet",
    "title": "Is AI \"Good\" Yet?",
    "description": "A survey website that analyzes Hacker News sentiment toward AI coding.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.is-ai-good-yet.com",
    "thumbnail_url": "https://www.is-ai-good-yet.com/og-image.png",
    "created_at": "2026-02-03T06:37:48.803Z",
    "topic": "tech"
  },
  {
    "slug": "contextgeneric-programming-v061-release-improving-ergonomics-and-debugging",
    "title": "Context-Generic Programming v0.6.1 Release: Improving Ergonomics and Debugging",
    "description": null,
    "fullText": "This blog post is co-authored by Claude Haiku based on a human draft. All content has been reviewed by the human author.\n\nWe're excited to announce the release of cgp v0.6.1, which brings several quality-of-life improvements to Context-Generic Programming in Rust. This release focuses on making CGP more accessible to developers new to the paradigm while also providing better debugging and verification tools for complex provider setups.\n\nThe changes in this release reflect our commitment to lowering the barrier to entry for CGP, while maintaining the power and flexibility that make it such a compelling approach to modular Rust code. Whether you're an experienced CGP user or just getting started, we believe you'll find these improvements meaningful.\n\nOne of the biggest hurdles when learning CGP is understanding how generic Context parameters work in provider implementations. While generics are fundamental to Rust, they can be intimidating to developers coming from object-oriented backgrounds, or even to experienced Rust developers who haven't deeply explored the generic type system.\n\nWhen writing a #[cgp_impl] provider, you previously had to explicitly declare the Context as a generic parameter and ensure it appeared correctly throughout your implementation. This added cognitive overhead that wasn't directly related to the business logic you were implementing. For beginners, this meant wrestling with generic syntax before they could even focus on understanding the core CGP concepts like delegation and trait constraints.\n\nCGP v0.6.1 introduces support for implicit Context types in the #[cgp_impl] macro. You can now omit the explicit generic parameter and use familiar Rust patterns like Self to refer to the context type.\n\nLet's look at a concrete example. Suppose we're building a greeting system where different types can greet people:\n\nBefore v0.6.1, the provider implementation would look like:\n\nWith v0.6.1, you can simplify this to:\n\nThe difference might seem small at first glance, but it has a profound impact on readability and accessibility. The new syntax eliminates the generic parameter syntax entirely, making the provider implementation look much more like a standard Rust trait implementation. For someone new to CGP coming from an OOP background, this is far less intimidating‚Äîit reads like implementing a method on a class, which is a familiar pattern.\n\nThis improvement particularly shines when you have multiple providers in a codebase. Each provider definition becomes clearer and more focused on the actual logic, rather than the generic plumbing.\n\nAs your CGP applications grow in complexity, so does the challenge of verifying that all your provider wiring is correct. This is particularly true when using higher-order providers‚Äîproviders that accept other providers as generic parameters.\n\nWhen you compose multiple providers together and something goes wrong, the compiler error messages can be cryptic. The error might point to a deep dependency in the chain, but it won't clearly tell you which individual provider in the composition is actually failing. This forces developers to compile and test repeatedly, changing providers and wiring patterns to narrow down the issue.\n\nCGP v0.6.1 introduces the #[check_providers] attribute for the check_components! macro, which lets you verify that specific providers work correctly with a given context. This is a powerful debugging tool that allows you to isolate and test individual providers before wiring them into your context.\n\nHere's a simple example. Suppose we have a shape area calculation system:\n\nYou can verify that RectangleArea works with Rectangle using:\n\nWith the new #[check_providers] attribute, you can also verify the provider directly:\n\nWhat's the benefit? The #[check_providers] version checks whether RectangleArea itself can be used as a provider for the component, regardless of what's currently wired in your Rectangle context. This is useful if you're developing multiple providers and want to test them independently before integrating them into your system.\n\nThe real power of #[check_providers] becomes apparent when you're working with composed higher-order providers. Let's look at a more realistic example:\n\nNow, when we verify this setup, we want to ensure not only that the composed provider works, but also that each individual component in the composition is correct. We can do this:\n\nThe first check verifies that the wired provider works with the context. The second check verifies each individual provider (or provider combination) that makes up your composition. If something is broken, such as the width field is missing, the error messages from these targeted checks will clearly point you to the problematic provider, rather than the entire composition.\n\nThis is a game-changer for debugging complex CGP setups. Instead of staring at a wall of compiler errors and trying to trace through the dependency chain, you can surgically test each part of your provider composition.\n\nOne common pattern in CGP is using getter traits to extract values from your context. The #[cgp_auto_getter] macro makes this convenient by automatically implementing a getter that reads a field from your context using the HasField trait.\n\nHowever, there was one limitation that made more sophisticated getter traits tedious to work with: you couldn't define an abstract type for the return value of your getter. If you wanted a getter trait where the return type was customizable per context, you had to define a separate abstract type trait, then link it to your getter trait.\n\nThis meant the old pattern looked like:\n\nThis works, but it requires defining two separate traits when conceptually you just want one getter trait with a flexible return type. For simple cases, this felt like boilerplate.\n\nCGP v0.6.1 allows you to define associated types directly in getter traits, eliminating the need for the separate type trait. You can now write:\n\nThe HasName trait now has an abstract Name type, and when the auto-getter is derived for Person, it automatically implements HasName with String as the concrete Name type. This is much more concise and reads more naturally.\n\nThe benefits are particularly clear when you have multiple getters with abstract types. Instead of maintaining a parallel set of type traits, you can keep everything in one place, making your code easier to understand and maintain.\n\nIf you're using the more powerful #[cgp_getter] macro (which allows customization of the implementation through providers), the same support for associated types works seamlessly:\n\nEven though our struct has a first_name field, we can still use the HasName getter by wiring it with UseField. The #[cgp_getter] macro combined with associated types gives you both power and convenience.\n\nThese three changes‚Äîimplicit Context types, enhanced provider checking, and associated types in getters‚Äîwork together to achieve our goal: making CGP more accessible and maintainable without sacrificing its power.\n\nThe improvements lower the learning curve for new CGP users by reducing generic syntax overhead. They provide better tooling for verifying and debugging complex provider compositions. And they reduce boilerplate when defining sophisticated traits, freeing you to focus on business logic.\n\nAs your CGP codebase grows, you'll find yourself reaching for these new features frequently. The #[cgp_impl] simplification makes your code more readable. The #[check_providers] attribute helps you debug faster. And associated types in getters let you express your intent more directly.\n\nWe believe CGP v0.6.1 represents a meaningful step forward in making Context-Generic Programming more approachable and productive. These changes emerged directly from feedback and real-world usage patterns in the CGP community, and we're confident they'll improve the development experience for everyone.\n\nWe encourage you to upgrade to v0.6.1 and explore these new features. Try simplifying your provider implementations with implicit Context types. Use #[check_providers] to debug your next complex provider composition. Define getter traits with abstract types and feel the difference in conciseness and clarity.\n\nAs always, we welcome feedback, bug reports, and contributions. The CGP journey continues, and we're excited to see what you build with these new tools.",
    "readingTime": 7,
    "keywords": [
      "let's look",
      "check verifies",
      "cgp introduces",
      "business logic",
      "error messages",
      "we're excited",
      "cgp_getter macro",
      "check_providers attribute",
      "debugging complex",
      "generic parameter"
    ],
    "qualityScore": 1,
    "link": "https://contextgeneric.dev/blog/v0-6-1-release/",
    "thumbnail_url": "https://contextgeneric.dev/cgp-logo.png",
    "created_at": "2026-02-02T12:34:23.079Z",
    "topic": "tech"
  },
  {
    "slug": "the-creator-of-clawdbot-the-viral-ai-agent-says-he-got-so-obsessed-with-vibe-coding-it-pulled-him-into-a-rabbit-hole",
    "title": "The creator of Clawdbot, the viral AI agent, says he got so obsessed with vibe coding it pulled him into a 'rabbit hole'",
    "description": "The creator of Clawdbot, the viral AI agent, says vibe coding can blur into compulsion, creating the illusion of productivity without real progress.",
    "fullText": "The creator of the viral AI agent Clawdbot says he had to step back after becoming too obsessed with vibe coding.\n\nPeter Steinberger, the developer behind Clawdbot ‚Äî which later changed its name to Moltbot and is now known as OpenClaw ‚Äî said in an episode of \"Behind the Craft\" podcast published Sunday that vibe coding pulled him into a \"rabbit hole.\"\n\n\"I was out with my friends and instead of, like, joining the conversation in the restaurant, I was just like, vibe coding on my phone,\" he said.\n\n\"I decided, OK, I have to stop this more for my mental health than for anything else,\" he added.\n\nClawdbot went viral last month in the tech community, attracting a wave of high-profile fans ‚Äî from Y Combinator CEO Garry Tan to multiple partners at Andreessen Horowitz.\n\nIt is a personal AI agent designed to run continuously and plug into a wide range of consumer apps, including WhatsApp and Telegram. Users can ask the AI to manage their schedules, oversee vibe-coding sessions, and even create AI employees.\n\nThe AI agent has been widely praised and meme'd online, with some tech fans even buying Mac Minis specifically to run the AI, Business Insider's Henry Chandonnet reported last week.\n\n‚Äã‚ÄãSteinberger said developers can fall into this trap of being hooked onto vibe coding, where building increasingly powerful AI tools creates the \"illusion of making you more productive\" without real progress.\n\nBuilding new tools can feel rewarding and fun, but that can quietly blur into compulsion, he added.\n\nWith AI, developers can now \"build everything,\" but ideas and taste matter. Without them, developers risk building tools and workflows that don't actually move a project forward, ‚Äã‚ÄãSteinberger said.\n\n\"If you don't have a vision of what you're going to build, it's still going to be slop,\" he added.\n\nVibe coding has continued to surge in popularity, with companies and developers promoting how AI can speed up software development.\n\nEarlier this month, Anthropic said it built its new agentic work tool, Cowork, entirely using Claude.\n\n\"@claudeai wrote Cowork,\" Anthropic's product manager, Felix Rieseberg, wrote on X. \"Us humans meet in-person to discuss foundational architectural and product decisions, but all of us devs manage anywhere between 3 to 8 Claude instances implementing features, fixing bugs, or researching potential solutions.\"\n\nThanks to Claude, the agent came together quickly. \"We sprinted at this for the last week and a half,\" Rieseberg said during a livestream.\n\nStill, despite the excitement around how fast vibe coding can produce new tools, tech leaders are warning that it has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that he won't vibe code on \"large codebases where you really have to get it right.\"\n\n\"The security has to be there,\" he added.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding is great for prototypes or throwaway code, not software that sits at the core of a business.\n\n‚Äã‚Äã\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "developers",
      "agent",
      "tools",
      "clawdbot",
      "tech",
      "viral",
      "episode",
      "fans"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/clawdbot-creator-vibe-coding-rabbit-hole-obsessed-openclaw-peter-steinberger-2026-2",
    "thumbnail_url": "https://i.insider.com/69802a8da645d11881886c3c?width=1200&format=jpeg",
    "created_at": "2026-02-02T12:34:16.408Z",
    "topic": "finance"
  },
  {
    "slug": "i-embedded-myself-in-a-vibe-coding-team-at-geminis-ai-hackathon-in-singapore-building-an-app-in-7-hours-takes-real-work",
    "title": "I embedded myself in a vibe coding team at Gemini's AI hackathon in Singapore. Building an app in 7 hours takes real work.",
    "description": "I followed a hackathon team as they raced to vibe code an app in seven hours at Google's Gemini 3 Hackathon in Singapore.",
    "fullText": "Just after sunrise, four vibe coding enthusiasts from Malaysia crossed into Singapore with a loose idea ‚Äî and a bet that AI could build most of their app.\n\nHours later, they were racing to prototype it at Google's Gemini 3 Hackathon in Singapore.\n\nThe four friends, all in their late 30s to 40s, came from different professional backgrounds. Chan Wei Khjan is an accountant. Chan Ler-Kuan lectures on AI at a private university. Loh Wah Kiang works in IT. Lee How Siem, who goes by Benny, is the chief technology officer of a Malaysian startup.\n\nTheir initial idea was a \"feng shui\" app to analyze properties in Singapore ‚Äî a potentially lucrative use case in a market obsessed with housing and wealth accumulation. Feng shui is a traditional Chinese practice that evaluates how a person's surroundings, along with birth factors, influence luck and well-being.\n\nI embedded with the team at Google's developer space in Singapore in January to observe how a vibe-coding project comes together ‚Äî or nearly falls apart ‚Äî in seven hours.\n\nThe assignment: Teams of up to four people had to build a working demo, publish a public repository with code, and submit a short video explaining their project by 5:30 p.m.\n\nEach project had to fit into one of six tracks, including generative media, deep research, and enterprise orchestration.\n\nOrganized by Google DeepMind and 65labs, Singapore's AI builder collective, the hackathon featured a 100,000-credit Gemini API prize pool, with first place getting 30,000 credits.\n\nThe team had pivoted to a new idea due to time constraints: a feng shui app that could analyse a user's outfit and workspace through the phone camera in real time and assess how \"lucky\" they were.\n\nWei Khjan took the lead on prompting. He typed the first instructions into Claude, asking it to generate the workflow and code. Ler-Kuan focused on whether the AI's output aligned with feng shui concepts. Wah Kiang and Benny hovered over the codebase, refining ideas and flagging issues.\n\n\"For people who don't know how to read code, it's helpful to have people who do,\" Wei Khjan said.\n\nWhile waiting for the code to be generated, Ler-Kuan opened Google's AI Studio to design the app's logo. They called their app \"Feng Shui Banana.\"\n\nAfter about an hour, Claude generated the initial codebase for the app. It was designed to work with the Gemini Live API, enabling real-time image and text analysis. It ran but was riddled with bugs.\n\nAn error message flashed when they tested the camera feature, so Wei Khjan copied the error back into the AI and asked for it to be fixed. Minutes later, the feature worked.\n\nIt wasn't right. The feng shui logic was off, especially where colour analysis intersected with the user's birth timings. Ler-Kuan manually corrected the underlying dictionary and its mappings.\n\nThe team kept prompting to tighten the features: shorter explanations, clearer output, and more streamlined user interfaces.\n\nLunch arrived. The team stayed glued to their screens.\n\nThe app didn't respond instantly when a user changed their outfit, nor did it update its feng shui analysis in real time.\n\nWei Khjan explained how one prompt matters. Instead of issuing commands, he asked the AI to \"discuss it with me.\" The shift changed how the model reasoned, and it worked more like a collaborator.\n\nAfter some prompting, the app updated with a real-time camera analysis. It was striking to watch a feature emerging from a short back-and-forth with AI.\n\nI helped the team test the app.\n\nThe camera correctly identified what I was wearing: a dark green polo, a yellow participant tag, and a white name card hanging from my neck. According to the app, I was already wearing colours aligned with my luck for the day.\n\nThe app suggested small tweaks, such as additional accessories, that could enhance the feng shui of my outfit.\n\nThey finally had lunch and joked around to ease the tension. Four hours remained before they had to submit their project.\n\nLer-Kuan shifted focus to workspace feng shui, feeding knowledge into the model and refining how the app would evaluate desks and work setups. Wah Kiang and Benny worked on the video demo.\n\nThe team also revisited the app's tagline. After cycling through suggestions from multiple AI models, they settled on a line that didn't come from an AI at all: \"A wisdom, not a superstition.\"\n\nThey used Gemini to generate a storyboard for the demo video. The model laid out several scenes and drafted the script. The team followed along, filming clips and stitching everything together as they went.\n\nTheir workspace feature was also up and running.\n\nThe app had come together nicely. With some time to spare, they decided to add audio output for users who prefer listening to reading on a screen.\n\nThe first attempt to generate a voice using AI fell flat. It sounded robotic.\n\nAfter debugging and several iterations, they landed on a voice they liked, similar to how a Chinese feng shui master might speak.\n\nAs the deadline approached, the team was still stitching clips for their video and nitpicking the AI-generated presenter voice.\n\nThe organizers had urged teams to submit early. With about 15 minutes to spare, they made the call to lock the final cut and hit submit.\n\nThen it was over. The hunger hit immediately, and everyone got in line for some well-deserved food.\n\nEven as an observer, watching from the sidelines was tiring. Seven hours of vibe coding turned out to be anything but effortless.\n\nThe team didn't win a prize, but agreed that the hackathon had been worth it.\n\n\"Sometimes, the best experiences come from saying 'yes' without overthinking,\" said Ler Kuan. \"Innovation starts with curiosity and a little bit of spontaneity.\"\n\nDo you have a story to share about vibe coding? Contact this reporter at cmlee@businessinsider.com.",
    "readingTime": 5,
    "keywords": [
      "wah kiang",
      "vibe coding",
      "feng shui",
      "wei khjan",
      "shui app",
      "wah kiang and benny",
      "team",
      "hours",
      "project",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/vibe-coding-team-embed-google-gemini-hackathon-singapore-2026-2",
    "thumbnail_url": "https://i.insider.com/696ef318a645d118818798f3?width=1200&format=jpeg",
    "created_at": "2026-02-02T06:52:12.008Z",
    "topic": "finance"
  },
  {
    "slug": "tailwind-creator-adam-wathan-shares-new-project-uish",
    "title": "Tailwind creator Adam Wathan shares new project ui.sh",
    "description": "A toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.",
    "fullText": "Turn your terminal intoA toolkit for coding assistants like Claude Code, Cursor, and Codex to help you build UIs that don't suck.Request an inviteBy the people who made\nTailwind CSS & Refactoring UI",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://ui.sh/",
    "thumbnail_url": "https://ui.sh/og-image.png",
    "created_at": "2026-02-02T01:11:00.781Z",
    "topic": "tech"
  },
  {
    "slug": "kessel-run-air-force-software-development-division",
    "title": "'Kessel Run' Air Force software development division",
    "description": null,
    "fullText": "Kessel Run, formally \"Air Force Life Cycle Management Center Detachment 12\", is a United States Air Force software development division, based in Hanscom Air Force Base and Boston, Massachusetts. It was founded in 2017 by the Defense Innovation Unit in response to the need to modernize legacy Air Force software.\n\nIn October 2016, Eric Schmidt, former CEO of Google, was leading a group touring the Combined Air Operations Center at Al Udeid Air Base in Qatar in his role as inaugural chairman of the Defense Innovation Board.[1] The CAOC at Al Udeid oversees air force operations for over 20 countries, and at the time was engaged in the War in Iraq against the Islamic State.[1]\n\nOne of the Air Operations Center tasks was planning daily aerial refueling operations to support combat missions. This was done off the main CAOC hall, in a windowless room with a whiteboard bearing magnetic pucks and plastic laminated cards, and physically measuring distance on the board to determine how long planes could stay in the air.[1][2] The resulting data was manually entered into an Excel spreadsheet known as \"the Gonkulator\" by a person called \"the Gonker\".[1][2] When a VBScript run on the spreadsheet confirmed the data was correct, another person, called \"the Thumper\", manually typed in the result into a Master Air Attack Planning Toolkit, which helped generate the Air Tasking Order back at Shaw Air Force Base in South Carolina.[1] Yet another person watched to verify against retyping errors.[2] The process took eight to 12 hours each day for three or eight people.[1][2][3] If there was a change needed, the process needed to restart.[2][3] When Schmidt and the Defense Information Board asked whether the base had access to more modern software to automate this process, the answer was: \"Yes, but it doesn't work.\"[1]\n\nAir Operations Center software had been in use mostly unchanged since the 1990s.[1] From 2006 to 2011, Lockheed Martin worked on the concept of modernizing it, but did not take up the project.[4] A Northrop Grumman project to modernize AOC software was commissioned in 2013 for $374 million for development, and $3.5 billion for lifetime maintenance.[4] By 2016, when the Defense Innovation Board was touring Al Udeid, nothing had been delivered.[2] The development price eventually grew to $745 million, and was three years behind schedule, with an estimated launch date of December 2019, when the project was eventually cancelled in July 2017.[4]\n\nAlso touring the Al Udeid CAOC with Schmidt in October 2016 was Raj Shah, tech entrepreneur and managing partner of DIUx, the Defense Innovation Unit Experimental.[1][2] Shah was a former Air Force fighter pilot who had first hand experience of the importance of regular aerial refueling.[2] That same night, he called lieutenant colonel Enrique Oti who was head of Air Force programs for DIUx in Silicon Valley.[1] They arranged an unusual partnership between Air Force developers and Pivotal Software, and got it approved by General Jeffrey Harrigian, commanding United States Air Forces Central Command.[1] Rather than rewrite the entire AOC software suite, they would just do the tanker whiteboard.[clarification needed][1] In December 2016, coders and program managers were visiting Al Udeid to talk to users. By April 2017, the aerial refueling tanker application, named Jigsaw, was in use at the CAOC; four months from start to production, when a more average Defense Department software operation took as much as three to five years.[1][5]\n\nThe speed of development was credited to the Agile development process, an iterative, adaptive approach, that doesn't try to get the entire solution in the first release. The first version of an application is intentionally only about a 60% solution, which is then changed and improved via rapid iterations as users give feedback.[6] Though this agile development is fairly basic in the modern software industry, it was unusual for the Defense Department.[7] The development team would later adopt the hashtag #AgileAF - the AF, they assure, stands for Air Force.[1]\n\nThe total cost of Jigsaw was reported at $1.5 million (Captain Bryon Kroger, Chief Operating Officer of the project),[5] to $2.2 million (Shah),[1] \"chump change\" (Harrigian).[8] With it, tanker planning was not only faster, taking two to three hours for a single person, but was more reliable, so two to three fewer tankers were scrambled each day.[1][3] Each scramble had a cost in fuel and maintenance of about $250,000 each.[1][2] Jigsaw saved 350,000 pounds of fuel a week.[8] Its development costs were recouped in the first week.[1][9]\n\nIn April 2017, after delivering Jigsaw, Oti, Kroger, and others, got approval to form an official Air Force software development team at the Air Force Life Cycle Management Center named Project Kessel Run.[2][10][5] The name \"Kessel Run\" came from a line in the 1977 science fiction film Star Wars, spoken by smuggler Han Solo, bragging about the speed of his ship, the Millennium Falcon.[2] It represented the project's intent to \"smuggle\" new software development capability into the Air Force and use it to set new software development speed records.[9]\n\nIn May 7, 2018, the Kessel Run Experimentation Lab set up at a WeWork shared facility in central Boston. It was modeled after Pivotal Labs training locations in Cambridge, San Francisco, and Washington.[11] It was managed from the AFLCMC at Hanscom Air Force Base, which believed the innovation advantages in coworking and creativity would outweigh the hassles of distance and security.[11][12] The Lab initially had space for 90 engineers, but planned for 300 within a year.[9] Many were on temporary assignment from other Air Force bases; yet others would be sent off to Pivotal Labs offices across the country for training in modern software techniques.[11][13][14] The Kessel Run motto on the wall was \"Code. Deploy. Win.\", a play on the Air Force's motto, \"Fly. Fight. Win.\".[12]\n\nOn January 2, 2019, the Kessel Run Experimentation Laboratory moved to a different location in a Boston skyscraper.[15] Kessel Run's budget since mid 2017 had grown to approximately $140 million, including workspace and personnel, and the operational software produced claimed savings of $13 million and 1,100 man-hours per month.[15]\n\nOn May 8, 2019, Kessel Run formally became Air Force Life Cycle Management Center Detachment 12, commanded by Colonel Oti, who had been effectively leading it for just under a year.[16] It had nearly 700 airmen, government civilians, and contractors.[16] Oti was replaced as commander by Colonel Brian Beachkofski on April 15, 2020, with the ceremony held over Zoom teleconference.[17] On June 27, 2022, Beachkofski was replaced as commander by Colonel Richard Lopez, who took the title of senior materiel leader.[18] Lopez had previously been the director of the LevelUP Code Works software factory inspired by Kessel Run.[18][19]\n\nThe March 2019 Defense Innovation Board report on software acquisitions included a chapter on Kessel Run subtitled \"The Future of Defense Acquisitions Is #AgileAF\".[21]\nIn September and October 2019, Kessel Run received multiple awards: the General Larry O. Spencer Innovation award; the Theodore von K√°rm√°n award for modernizing software for the F-35; and the inaugural Defense Acquisition Software Innovation Team award.[22][23] A 2019 editorial in Defense One said that Kessel Run was \"widely seen as the gold standard of military tech done right ... also the most hyped military program office in operation today\".[24]\n\nNot all reactions were positive. A 2019 anonymous survey of KREL application users found that some applications did not meet user needs, and that success metrics, documentation, and responsiveness to user feedback could all be lacking.[25] A 2020 Harvard Kennedy School project found and tried to address internal discontent among Kessel Run staff with emerging bureaucracy and increasing technical complexity.[26] The Air Force's Deputy Chief Information Officer, Lauren Knausenberger, acknowledged in 2020 that Kessel Run was having growing pains, but said that was a result of its success.[27]\n\nKessel Run inspired multiple agile software development teams across the Air Force and United States Department of Defense.[28] They were called \"software factories\".[29] The original definition of software factory was a set of software tools to write and automatically build, test, and document applications; the Chief Information Officer of the Department of Defense slightly redefined that to be a software assembly plant that automated the develop, build, test, release and deliver phases, but in each case to support agile software development practices.[30]\n\nKobayashi Maru, formally Space C2, or Space Command and Control, in California, was the second such software factory, in August 2018.[28] It was named after an impossible test in the Star Trek science fiction universe.[31] Just as Kessel Run came from an effort to replace an outdated system by getting around bureaucratic rules, Kobayashi Maru intended to update the software of the troubled Joint Mission System (jointly run with the United States Space Force) for space command and control and situational awareness.[32][33]\n\nBESPIN - an acronym for Business and Enterprise Systems Product Innovation, but also the name of a planet in the Star Wars universe - was the third Air Force software factory, launched in early 2019 in Montgomery, Alabama, to create apps for maintenance crew chiefs, aircrew readiness, and ammunition crews.[28][31] Space Camp, in Colorado, and Section31, in California, spun off of Kobayashi Maru.[31] LevelUP, in Texas, was a joint cyber operations system for the Unified Platform, connecting the Army, Marines, and United States Cyber Command, debuting in April 2019.[31] By September 2021, there were 17 Air Force software factories across the country.[34]\n\nSoftware factories weren't limited to the Air Force. The Navy was inspired by Kessel Run to stand up its first software factory, The Forge, in Riverdale, Maryland, in March 2021.[35] The Army Software Factory debuted in April 2021 in Austin Community College in Texas, as part of the United States Army Futures Command.[36][37] In February 2022, Deputy Defense Secretary Kathleen Hicks wrote a DOD Software Modernization Strategy memo encouraging increased used of software factories throughout the Defense Department; at the time there were 29.[38] By April 2022 the United States Coast Guard was planning a software factory based on the Air Force model.[39] The Marine Corps Software Factory was co-located with the Army Software Factory as a three year test project in Austin in March 2023.[40]\n\nThe 24th Air Force's Air Force Cyber Proving Ground may be another related activity.\n\nJigsaw, the 2017 aerial refueling planning application that started Kessel Run, was bought and used by NATO in multiple countries in 2020 and 2021.[41][42]\n\nThe team's second and third projects after Jigsaw were Chainsaw and Raven, applications for assembling and communicating target information.[1] Chainsaw was in operation by November 2017, consolidated many programs into one, and cut the process for dynamic targeting from an hour or two to minutes. Raven, for target development management, cut 12 hours of work down to three or four, and was ready in early 2018.[43]\n\nStarting in late 2018, Kessel Run joined the task of fixing the troubled software for the maintenance of the F-35 fighter jet, called ALIS, for Autonomic Logistic Information System.[44] ALIS was a 17 year old proprietary system full of bugs and data gaps.[45] Maintainers had to keep separate databases because they could not rely on ALIS data.[46] The project to fix ALIS, including Kessel Run, Pivotal, and Lockheed Martin, its original creators, was called Mad Hatter, named by the developers.[44] It officially started in October 2018, but took until January 2019 before developers could write code, while the Air Force negotiated with Lockheed Martin as to what parts of the proprietary ALIS system the government could be able to reach.[44] The Mad Hatter suite of eight programs was tested and favorably evaluated by F-35 aircraft maintainers in March 2020.[47] In July 2020, it was renamed to Torque, and adapted for maintenance of the F-22 stealth jet and CV-22 tiltrotor aircraft,[48] then the C-130J turboprop in January 2021.[49] Meanwhile, on the F-35 itself, between 2020 and 2022 ALIS was gradually replaced by ODIN, the Operational Data Integrated Network, \"leveraging\" the software practices of Kessel Run, but built by Lockheed Martin.[50][51]\n\nIn 2021, Kessel Run began deploying the initial version of KRADOS, the Kessel Run All Domain Operations Suite meant to replace the Theater Battle Management Core Systems that created air tasking orders throughout AOCs all over the world.[52] The first AOC to use the suite operationally was again the 609th Air Operations Center at Al Udeid, in May 2021, after using the Beta version since December 2020.[53] KRADOS linked together nine applications through cloud-based data, including the latest version of Jigsaw, Kessel Run's first application for tanker planning, and Slapshot, for planning the rest of the air missions and building the Master Air Attack Plan.[54] In 2017, Lockheed Martin had received a $38 million contract to maintain the older TBMCS, but the 609th kept finding problems, so turned to Kessel Run, which delivered the beta version of KRADOS three weeks after receiving the request in November 2020.[55] By August 2022, the 603rd AOC in Ramstein, Germany, employed elements of KRADOS for visualization, though it was not considered mature enough to create air tasking and airspace control orders.[56][57] In January 2023, the 609th replaced the TBMCS with KRADOS entirely.[58]\n\nThe Command and Control Incident Management Emergency Response Application (C2IMERA), is a real time Air Force base resource management tool. It used a different development model: the coding was done by software company Leidos, and the program management by Kessel Run.[59] In August 2019, Moody Air Force Base used the software to monitor and prepare for Hurricane Dorian, though it was not originally intended for this purpose.[60] C2IMERA was also used for the August 2021 evacuation of civilians from Afghanistan in Operation Allies Refuge.[61][62] It was ordered deployed across all Air Combat Command installations in September 2021.[59][63] In August 2023, Air Mobility Command joined Air Combat Command in designating C2IMERA as their standard installation command and control tool.[64]\n\nSlapshot, the air mission flow organizer part of KRADOS, was also used for the Operation Allies Refuge Afghan evacuation along with C2IMERA.[65][66] At that time, KRADOS had known issues with scaling; it couldn't handle many simultaneous operations, which was exactly what it was being asked to do. On August 24, 2021, at 2 am Boston time, the Slapshot server crashed. Over the next 12 hours, Kessel Run developers restarted servers, shifted United States Central Command resources to improve performance, fixed database errors, and added new features to improve load times, so the evacuation on the other side of the world could continue.[67]\n\nBowcaster, named after a Star Wars weapon, is a chaos engineering tool and playbook that intentionally creates failures in processes to strengthen them.[68][69]\nKessel Run developed it, and shares it with other government agencies, initially with the Navy Black Pearl software factory in 2021.[70][71] In March 2022 Kessel Run and the General Services Administration's Technology Transformation Services used it to check that the Cloud.gov website could handle 100 million users per hour.[72][73]",
    "readingTime": 13,
    "keywords": [
      "kessel run",
      "air force",
      "allies refuge",
      "life cycle",
      "operation allies",
      "center detachment",
      "cycle management",
      "innovation unit",
      "innovation board",
      "operations center"
    ],
    "qualityScore": 1,
    "link": "https://en.wikipedia.org/wiki/Kessel_Run",
    "thumbnail_url": "https://upload.wikimedia.org/wikipedia/commons/7/77/Kessel_Run_logo.jpg",
    "created_at": "2026-02-01T12:26:41.722Z",
    "topic": "tech"
  },
  {
    "slug": "securing-the-ralph-wiggum-loop-devsecops-for-autonomous-coding-agents",
    "title": "Securing the Ralph Wiggum Loop ‚Äì DevSecOps for Autonomous Coding Agents",
    "description": "Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck - agairola/securing-ralph-loop",
    "fullText": "agairola\n\n /\n\n securing-ralph-loop\n\n Public\n\n Security checks for the Ralph Loop - scan before commit, fix iteratively, escalate when stuck\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n agairola/securing-ralph-loop",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/agairola/securing-ralph-loop",
    "thumbnail_url": "https://opengraph.githubassets.com/6d3ccf091e1097ce34aa5949ea80238ec45757c8d310e8b3be3ea2b9c6e218fb/agairola/securing-ralph-loop",
    "created_at": "2026-02-01T06:37:20.298Z",
    "topic": "tech"
  },
  {
    "slug": "top-engineers-at-anthropic-openai-say-ai-now-writes-100-of-their-code",
    "title": "Top engineers at Anthropic, OpenAI say AI now writes 100% of their code",
    "description": "AI coding tools are getting more sophisticated. But if coders stop coding, what happens to software development jobs?",
    "fullText": "FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
    "readingTime": 1,
    "keywords": [
      "fortune"
    ],
    "qualityScore": 0.1,
    "link": "https://fortune.com/2026/01/29/100-percent-of-code-at-anthropic-and-openai-is-now-ai-written-boris-cherny-roon/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2026/01/GettyImages-2216956965_40536e-e1769705381107.jpg?resize=1200,600",
    "created_at": "2026-01-31T01:04:24.596Z",
    "topic": "tech"
  },
  {
    "slug": "cooperbench-benchmarking-ai-agents-cooperation",
    "title": "CooperBench: Benchmarking AI Agents' Cooperation",
    "description": "CooperBench is a benchmark of over 600 collaborative coding tasks. We find that agents achieve 30% lower success rates when working together compared to performing both tasks individually.",
    "fullText": "Stanford University & SAP Labs US\n\nCan AI agents work together as teammates? We find\n that\n coordinating agents perform much worse than a\n single agent\n given the same total workload. This coordination\n deficit presents a fundamental barrier to deploying\n AI systems that can work alongside humans or other\n agents.\n\nSuccess rate on CooperBench across 652 tasks ¬∑ Error bars show 95% confidence intervals\n\nGPT-5 and Claude Sonnet 4.5 achieve only 25%\n success with two-agent cooperation, roughly 50%\n lower than when a single agent handles both\n tasks. This gap persists across all models and\n task difficulties.\n\nAgents spend up to 20% of their budget on\n communication. This reduces merge conflicts but\n does not improve overall success. The channel is\n jammed with repetition, unresponsiveness, and\n hallucination.\n\nEven when agents communicate well, coordination\n breaks down due to:\n\nAmong successful runs, we observe coordination patterns\n largely absent from failures. These patterns are not\n prompted or scaffolded.\n\nRole Division\n ‚Äî Agents agree on who handles which part of the\n task. One agent delegates: \"I'll add header +\n octal_str; you add binary_str between them.\"\n\nCooperBench is the first benchmark designed to\n measure how well AI agents can cooperate when\n handling individual tasks with potential conflicts.\n We constructed 652 tasks from 12 popular open-source\n libraries across Python, TypeScript, Go, and Rust.\n\nEach task assigns two agents different features that\n can be implemented independently but may conflict\n without proper coordination. Eight co-authors with\n real-world software engineering backgrounds created\n new features, unit tests, and ground-truth code.\n\nStanford University & SAP Labs ¬∑ *Equal contribution\n (Stanford) ¬∑ ‚Ä†Equal contribution (SAP Labs)",
    "readingTime": 2,
    "keywords": [
      "equal contribution",
      "stanford university",
      "university sap",
      "coordination",
      "tasks",
      "success",
      "across",
      "task",
      "agents",
      "cooperbench"
    ],
    "qualityScore": 0.85,
    "link": "https://cooperbench.com/",
    "thumbnail_url": "https://cooperbench.com/static/images/cooperbench_social.png",
    "created_at": "2026-01-30T18:28:27.534Z",
    "topic": "tech"
  },
  {
    "slug": "daedalus",
    "title": "Daedalus",
    "description": "AI planning CLI and autonomous agent orchestration for beans-based coding workflows - internet-development/daedalus",
    "fullText": "internet-development\n\n /\n\n daedalus\n\n Public\n\n AI planning CLI and autonomous agent orchestration for beans-based coding workflows\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n internet-development/daedalus",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/internet-development/daedalus",
    "thumbnail_url": "https://opengraph.githubassets.com/471147f12cb10edd91f2072a6602ce03fd3e4339e4f03a4a184ee7336b8d31ea/internet-development/daedalus",
    "created_at": "2026-01-30T06:35:17.254Z",
    "topic": "tech"
  },
  {
    "slug": "cwt-sandbox-ai-coding-agents-using-git-worktrees",
    "title": "Cwt ‚Äì Sandbox AI coding agents using Git Worktrees",
    "description": "Contribute to benngarcia/claude-worktree development by creating an account on GitHub.",
    "fullText": "benngarcia\n\n /\n\n claude-worktree\n\n Public\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n benngarcia/claude-worktree",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/benngarcia/claude-worktree",
    "thumbnail_url": "https://opengraph.githubassets.com/73752eaadbf154afd745270d32c1743038163f0555993f4c462cecd915f63c02/benngarcia/claude-worktree",
    "created_at": "2026-01-30T01:07:08.980Z",
    "topic": "tech"
  },
  {
    "slug": "acp-agent-registry-in-jetbrains-ides",
    "title": "ACP Agent Registry in JetBrains IDEs",
    "description": "Together with Zed, we've launched the official ACP Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs.",
    "fullText": "Supercharge your tools with AI-powered features inside many JetBrains products\n\nAI coding agents are multiplying fast. Some of the most common ones include Gemini CLI, Claude Code, Auggie, OpenCode, and Copilot, and more are being released every day.¬†Each comes with its own unique strengths, specific setups, and varying levels of editor support. Keeping track of what‚Äôs out there, let alone getting it running in your IDE, hasn‚Äôt been easy.\n\nTogether with Zed (Zed‚Äôs announcement), we‚Äôve launched the official ACP Agent Registry: a directory of AI coding agents, integrated directly into JetBrains IDEs and Zed. Browse what‚Äôs available, click Install, and start working right away. This beta release is just the beginning.\n\nThe Agent Client Protocol¬†is an open standard that lets any AI coding agent work in any supporting editor. Think of it like the Language Server Protocol, but for AI agents. The LSP lets any editor support any language through a shared standard. The ACP does the same for coding agents. You only need to implement it once, and then it will work in your JetBrains IDE, Zed, or any other editor that supports the protocol.\n\nThis means you get to¬†pick your preferred¬†agent and editor, and they will then work together seamlessly ‚Äì no vendor lock-in and no waiting for someone to build a specific integration.\n\nACP has been, since we started integrating it to Mistral Vibe, a real joy to use: thoughtfully designed from the ground up, community-driven, and evolving rapidly. We‚Äôve found it not only simplifies integration, but also fits our focus on open and flexible tools. It‚Äôs really great to see a standard that puts developer choice first.\n\nMichel Thomazo, Software Engineer @ Mistral AI\n\nThe ACP made agent interoperability technically possible. The registry makes it convenient.\n\nInstead of manually configuring agents, you can now:\n\nAt launch, you‚Äôll find a wide array of different agents:\n\nFull-featured coding assistant optimized for large-scale refactors\n\nSpecialized agent for automated code generation workflows\n\nGoogle‚Äôs agent with deep codebase understanding and multimodal capabilities\n\nGitHub‚Äôs AI pair programmer, now available via the ACP\n\nLightweight, fast agent built on Mistral‚Äôs models\n\nCommunity-driven, fully open-source agent\n\nAlibaba‚Äôs coding agent with strong multilingual support\n\nInnovation in software agents is moving at an unbelievable pace. The Agent Registry and ACP makes it simple for developers to use the best agents in their favorite tools.\n\nChris Kelly, Product @ Augment Code\n\nIn general, it‚Äôs less about having multiple agents¬†than¬†about enabling you to pick and choose the ones that work well in your workflow. Different agents come with different benefits. Some provide a more attractive pricing structure for your business, some provide a user experience that you simply enjoy more than others‚Äô, and some embody the ideas of open-source development that just resonate with you.\n\nThe Agent Client Protocol registry lets you experiment freely. Try a few, see what clicks for your workflow, and then keep the ones that help. You‚Äôre not locked into a single vendor‚Äôs vision of what AI-assisted development should look like.\n\nWe‚Äôre excited to support the ACP Agent Registry as a step toward a more open agent ecosystem where Droids can integrate seamlessly across all IDEs.\n\nFrancesca LaBianca, VP of Operations @ Factory\n\nIn any JetBrains IDE (2025.3.2+) with JetBrains AI (253.30387.147):\n\nThat‚Äôs it. The agent is configured and ready to use in the AI Chat¬†tool window.\n\nQuick note: agents typically come with their own subscription. That‚Äôs between you and them. You won‚Äôt need a JetBrains AI subscription to use ACP agents.\n\nWant to try something concrete? Install OpenCode, open a project, and ask it to explain an unfamiliar module. OpenCode also lets you swap between different LLMs, so you can experiment with what works best for you.\n\nIf you prefer manual configuration, that option is still there, too. Just edit the acp.json¬†directly. This is useful for agents that aren‚Äôt in the registry yet or for custom setups.\n\nIf you‚Äôre building an ACP-compatible agent, the registry is now the fastest way to reach developers across JetBrains IDEs and Zed.\n\nHead to the¬†ACP Registry repository¬†and check out the¬†CONTRIBUTING.md¬†for the full submission process and metadata requirements. Please note that, for now, we are only featuring agents that support Agent Auth or Terminal Auth. Full details of requirements and conditions can be found here.\n\nThis is an open registry. If you‚Äôre building an ACP-compatible agent, you‚Äôre welcome to submit it. The registry exists to serve the ecosystem, not to gatekeep it.\n\nFor developers:¬†More choice and zero lock-in. Use any agent you want in the IDE you love.\n\nFor agent builders:¬†Instant distribution to millions of JetBrains and Zed users. Implement the ACP once and reach everyone.\n\nFor the ecosystem:¬†Competition on quality, not on who controls the integration. The best agents win because they‚Äôre the best, not because they have exclusive deals.\n\nWe‚Äôre building this openly with Zed because we believe AI-assisted development shouldn‚Äôt be locked inside any single vendor‚Äôs ecosystem. Developers deserve to pick their tools freely.\n\nThe registry is one more step toward that future.\n\nThe ACP Registry is available now in JetBrains IDE versions 2025.3 and later. Update your IDE and the JetBrains AI plugin, open Settings, and start exploring.\n\nHave feedback? Found a bug? The¬†registry repo is open for issues and PRs. And if you‚Äôre building something interesting with ACP, we‚Äôd love to hear about it!\n\nOpenAI Codex is now natively integrated into the JetBrains AI chat, giving you another powerful option for tackling real development tasks right inside your IDE.¬†\n\nYou can use Codex with a JetBrains AI subscription, your ChatGPT account, or an OpenAI API key ‚Äì all within the same AI —Åhat inte‚Ä¶\n\nThe next edit suggestions feature is now enabled in all JetBrains IDEs for JetBrains AI Pro, AI Ultimate, and AI Enterprise subscribers.\n\nYes, you read that right! JetBrains-native diff suggestions are available right in your editor. Global support for optimized latency. Out-of-the-box IDE actions‚Ä¶\n\nBring Your Own Key (BYOK) is now available in the AI chat inside JetBrains IDEs as well as for AI agents, including JetBrains‚Äô Junie and Claude Agent. Whether you‚Äôre looking to use cutting-edge frontier models, cost-efficient small models, locally hosted private models, or experimental research prev‚Ä¶\n\nJunie is now integrated into the AI chat. The separate interfaces have merged into a single, unified space (available in Beta).",
    "readingTime": 6,
    "keywords": [
      "client protocol",
      "ai-assisted development",
      "step toward",
      "agent client",
      "acp-compatible agent",
      "jetbrains ai",
      "coding agents",
      "acp agent",
      "acp agent registry",
      "jetbrains ide"
    ],
    "qualityScore": 1,
    "link": "https://blog.jetbrains.com/ai/2026/01/acp-agent-registry/",
    "thumbnail_url": "https://blog.jetbrains.com/wp-content/uploads/2026/01/JB-social-BlogSocialShare-1280x720-1-4.png",
    "created_at": "2026-01-29T18:30:47.768Z",
    "topic": "tech"
  },
  {
    "slug": "extesla-ai-head-has-seen-a-phase-shift-in-software-engineering-using-claude-code-and-his-manual-skills-slowly-atrophy",
    "title": "Ex-Tesla AI head has seen a 'phase shift in software engineering' using Claude Code ‚Äî and his manual skills slowly 'atrophy'",
    "description": "Andrej Karpathy posted his \"notes from Claude Coding,\" describing a shift in engineering over the last two months.",
    "fullText": "He coined \"vibe coding.\" Now, he sees a \"phase shift\" in software engineering.\n\nAndrej Karpathy is one of AI's guiding figures. He was a founding member of OpenAI and later served as Tesla's director of AI. He also coined the term \"vibe coding,\" the AI-assisted coding movement that has taken software engineering by storm and was named Collins Dictionary's word of the year.\n\nIn his \"random notes from Claude Coding\" ‚Äî which are over 1,000 words long ‚Äî Karpathy wrote about the changes to his own coding style. Posted on X on Monday, the notes have already elicited reactions from engineers at Anthropic, xAI, and more.\n\nAI coding agents \"crossed some kind of threshold of coherence around December 2025 and caused a phase shift in software engineering,\" Karpathy wrote.\n\nA few random notes from claude coding quite a bit last few weeks.\n\nCoding workflow. Given the latest lift in LLM coding capability, like many others I rapidly went from about 80% manual+autocomplete coding and 20% agents in November to 80% agent coding and 20% edits+touchups in‚Ä¶\n\nKarpathy name-dropped both Anthropic's Claude Code and OpenAI's Codex as having significant improvements. Claude Opus 4.5, the model that has garnered much love from engineers online, came out at the tail end of November.\n\nThe AI leader's workflow has changed as a result of the AI tools. From November to December, Karpathy's 80/20 ratio flipped. He once used 80% manual coding and 20% agents; now, it's 80% agents and 20% manual code editing.\n\n\"I really am mostly programming in English now, a bit sheepishly telling the LLM what code to write... in words,\" he wrote.\n\nThe change to AI-written code \"hurts the ego,\" but is too powerful to ignore, Karpathy wrote. He also devoted a whole section of his notes to the \"fun\" he has while coding with large language models.\n\nWhat of those traditional coding skills, the ones you learn in a computer science program or through endless digital courses? That's a whole other function, Karpathy wrote, and one that might decline.\n\n\"I've already noticed that I am slowly starting to atrophy my ability to write code manually,\" he wrote.\n\nIn Karpathy's comments, engineers from leading AI companies sounded off. Ethan He, an xAI engineer and Nvidia alum, wrote that a \"10x engineer can be a one-man army.\"\n\nCharles Weill, another xAI engineer, wrote that founders can now \"divide themselves\" with coding agents, like a VC divides their capital over a portfolio of companies.\n\nBoris Cherny, an Anthropic staffer and the creator of Claude Code, wrote that he read Karpathy's \"thoughtful\" post till its end.\n\nThe Claude Code team at Anthropic may offer a model of where the industry is moving, Cherny wrote. His team is \"mostly generalists\" and filled with 10x engineers.\n\n\"Pretty much 100% of our code is written by Claude Code,\" Cherny wrote. \"For me personally it has been 100% for two+ months now, I don't even make small edits by hand.\"\n\nThe Anthropic employee also acknowledged the \"quality\" problems with AI-written code. Agents can overcomplicate things and can leave around dead code, he wrote.\n\nHis solution: having AI review the AI-written code.",
    "readingTime": 3,
    "keywords": [
      "ai-written code",
      "phase shift",
      "software engineering",
      "random notes",
      "xai engineer",
      "vibe coding",
      "coding agents",
      "engineers",
      "claude",
      "coined"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/andrej-karpathy-claude-code-manual-skills-atrophy-software-engineering-tesla-2026-1",
    "thumbnail_url": "https://i.insider.com/68f53bf05dbc4fd10dab1f5f?width=1200&format=jpeg",
    "created_at": "2026-01-29T12:32:28.928Z",
    "topic": "tech"
  },
  {
    "slug": "openais-chair-says-vibe-coding-is-here-to-stay-but-its-not-the-endgame",
    "title": "OpenAI's chair says vibe coding is here to stay ‚Äî but it's not the endgame",
    "description": "Vibe coding will stick around, but AI agents, not apps, will drive the next big shift in software, says OpenAI's chair Bret Taylor.",
    "fullText": "Vibe coding isn't going anywhere. But it's only part of a much bigger transformation, says OpenAI's board chair.\n\nBret Taylor said in an episode of the \"Big Technology Podcast\" published on Wednesday that using AI tools to build software quickly with natural language prompts will soon feel normal rather than novel. However, focusing on building today's software faster misses the bigger picture.\n\n\"Everyone's looking at all the software use and saying, 'How fast could I vibe code that?'\" Taylor said. \"I wonder if it's the wrong question.\"\n\nWhether someone can quickly vibe code an app in a web browser isn't \"the most interesting question in software,\" he added.\n\nInstead, the software we use today is set to be replaced, and that's the real disruption, Taylor said.\n\nRather than dashboards, web-browser forms, and traditional apps, the structure of software will change. AI agents will be \"the future of software.\"\n\n\"We will delegate tasks to agents that will operate against a database,\" Taylor said.\n\n\"Who's making those agents is the question,\" he added. \"Will you buy those agents off the shelf or build them yourself?\"\n\nTaylor also said that while AI has slashed the cost of building software, it hasn't solved the harder problems of maintaining it ‚Äî or the risk of getting things wrong.\n\n\"That's why most people would prefer to buy a solution off the shelf,\" he said. \"You want to amortize the cost of maintaining software among thousands of clients.\"\n\nVibe coding has taken off across the tech world, but tech leaders said the technology has limits.\n\nGoogle CEO Sundar Pichai said in November in a \"Google for Developers\" podcast interview that vibe coding is \"making coding so much more enjoyable,\" adding that it allows even non-technical users to create simple apps and websites.\n\nDuring Alphabet's April earnings call, Pichai said AI generates more than 30% of Google's new code, up from 25% in October 2024.\n\nStill, AI-generated code can be error-prone, overly long, or poorly structured.\n\n\"I'm not working on large codebases where you really have to get it right, the security has to be there,\" Pichai said in November.\n\nBoris Cherny, the engineer behind Anthropic's Claude Code, said last month that vibe coding works best for prototypes or throwaway code, but not in software that sits at the core of a business.\n\n\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" he said in an episode of \"The Peterman Podcast\" published in December.",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "vibe coding",
      "vibe code",
      "software",
      "agents",
      "isn't",
      "it's",
      "bigger",
      "episode",
      "quickly"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/openai-chair-vibe-coding-not-endgame-bret-taylor-2026-1",
    "thumbnail_url": "https://i.insider.com/6883b14a85e81483682eb19e?width=1200&format=jpeg",
    "created_at": "2026-01-29T06:34:22.533Z",
    "topic": "finance"
  },
  {
    "slug": "microsoft-cfos-memo-to-staff-calls-out-ai-deals-coding-and-chips",
    "title": "Microsoft CFO's memo to staff calls out AI deals, coding, and chips",
    "description": "CFO Amy Hood sent an internal memo about the results, viewed by Business Insider.",
    "fullText": "After Microsoft reported results on Wednesday, CFO Amy Hood sent an internal memo to employees calling attention to recent developments in AI chips and coding tools, and deals with OpenAI and Anthropic.\n\nHood sends these emails every quarter when Microsoft discloses its financials. Her missives mostly rehash what the company reports publicly, such as how revenue and profit are growing, or what is discussed on analyst earnings calls.\n\nStill, these memos provide insight into what Microsoft executives deem most important, and what they want employees to know.\n\nThe latest memo highlighted how Microsoft is gaining share in markets where the total addressable market is expanding.\n\nHood specifically mentioned the launch of the GitHub Copilot software development kit in the growing market of AI coding tools, and the release of Microsoft's new Maia 200 AI chip.\n\nHood's email also called out Azure commitments from OpenAI and Anthropic that helped increase commercial bookings, basically the deals Microsoft closed in the quarter, by 230%, year over year.\n\nCapital expenditure on computing and datacenter infrastructure also broke yet another quarterly record, reaching $37.5 billion, she also noted.\n\nThis afternoon, we announced our second-quarter financial results. We exceeded Wall Street expectations, growing revenue 17% and 15% in constant currency and operating income by 21% and 19% in constant currency -a strong finish to the first half of the fiscal year.\n\nIn the quarter, Microsoft Cloud revenue surpassed $50 billion for the first time, growing 26% and 24% in constant currency.\n\nThere were many highlights this quarter, but a few stand out as reminders of the value our products and services deliver to customers - and as proof points of the progress we are making:\n\nInvestors tune in to our earnings call for the full details on this quarter and a look ahead to Q3. It's a helpful way to stay aligned as we deliver on our commitments. Join live today at 2:30 PM Pacific, listen on-demand, or check the transcript on the Investor Relations website.\n\nThis quarter's results reflect meaningful progress on core priorities. We continue to add capacity with pace, drive steady efficiency gains across our fleet, and invest in each layer of the stack\n\nAs we enter the second half of the fiscal year, we're operating in markets with expanding TAM where we continue to gain share and you can see our progress in many places, from last week's announcement of the GitHub Copilot SDK to Monday's Maia 200 announcement. We are innovating and delivering together. And we're doing it with the quality and security our customers expect from us. All of this builds trust from customers and partners as they rely on us for mission critical workloads.\n\nThanks again for all your work.\n\nWith appreciation and gratitude,\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "coding tools",
      "constant currency",
      "quarter",
      "revenue",
      "email",
      "customers",
      "progress",
      "memo",
      "employees",
      "deals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/internal-microsoft-cfo-memo-calls-out-ai-deals-coding-and-chips-2026-1",
    "thumbnail_url": "https://i.insider.com/697a82dfa645d11881883093?width=1200&format=jpeg",
    "created_at": "2026-01-29T01:07:05.127Z",
    "topic": "finance"
  },
  {
    "slug": "programming-sucks-2014",
    "title": "Programming Sucks (2014)",
    "description": null,
    "fullText": "Every friend I have with a job that involves picking up something heavier than a laptop more than twice a week eventually finds a way to slip something like this into conversation: ‚ÄúBro,1[1] you don‚Äôt work hard. I just worked a 4700-hour week digging a tunnel under Mordor with a screwdriver.‚Äù\n\nThey have a point. Mordor sucks, and it‚Äôs certainly more physically taxing to dig a tunnel than poke at a keyboard unless you‚Äôre an ant. But, for the sake of the argument, can we agree that stress and insanity are bad things? Awesome. Welcome to programming.\n\nImagine joining an engineering team. You‚Äôre excited and full of ideas, probably just out of school and a world of clean, beautiful designs, awe-inspiring in their aesthetic unity of purpose, economy, and strength. You start by meeting Mary, project leader for a bridge in a major metropolitan area. Mary introduces you to Fred, after you get through the fifteen security checks installed by Dave because Dave had his sweater stolen off his desk once and Never Again. Fred only works with wood, so you ask why he‚Äôs involved because this bridge is supposed to allow rush-hour traffic full of cars full of mortal humans to cross a 200-foot drop over rapids. Don‚Äôt worry, says Mary, Fred‚Äôs going to handle the walkways. What walkways? Well Fred made a good case for walkways and they‚Äôre going to add to the bridge‚Äôs appeal. Of course, they‚Äôll have to be built without railings, because there‚Äôs a strict no railings rule enforced by Phil, who‚Äôs not an engineer. Nobody‚Äôs sure what Phil does, but it‚Äôs definitely full of synergy and has to do with upper management, whom none of the engineers want to deal with so they just let Phil do what he wants. Sara, meanwhile, has found several hemorrhaging-edge paving techniques, and worked them all into the bridge design, so you‚Äôll have to build around each one as the bridge progresses, since each one means different underlying support and safety concerns. Tom and Harry have been working together for years, but have an ongoing feud over whether to use metric or imperial measurements, and it‚Äôs become a case of ‚Äúwhoever got to that part of the design first.‚Äù This has been such a headache for the people actually screwing things together, they‚Äôve given up and just forced, hammered, or welded their way through the day with whatever parts were handy. Also, the bridge was designed as a suspension bridge, but nobody actually knew how to build a suspension bridge, so they got halfway through it and then just added extra support columns to keep the thing standing, but they left the suspension cables because they‚Äôre still sort of holding up parts of the bridge. Nobody knows which parts, but everybody‚Äôs pretty sure they‚Äôre important parts. After the introductions are made, you are invited to come up with some new ideas, but you don‚Äôt have any because you‚Äôre a propulsion engineer and don‚Äôt know anything about bridges.\n\nWould you drive across this bridge? No. If it somehow got built, everybody involved would be executed. Yet some version of this dynamic wrote every single program you have ever used, banking software, websites, and a ubiquitously used program that was supposed to protect information on the internet but didn‚Äôt.\n\nEvery programmer occasionally, when nobody‚Äôs home, turns off the lights, pours a glass of scotch, puts on some light German electronica, and opens up a file on their computer. It‚Äôs a different file for every programmer. Sometimes they wrote it, sometimes they found it and knew they had to save it. They read over the lines, and weep at their beauty, then the tears turn bitter as they remember the rest of the files and the inevitable collapse of all that is good and true in the world.\n\nThis file is Good Code. It has sensible and consistent names for functions and variables. It‚Äôs concise. It doesn‚Äôt do anything obviously stupid. It has never had to live in the wild, or answer to a sales team. It does exactly one, mundane, specific thing, and it does it well. It was written by a single person, and never touched by another. It reads like poetry written by someone over thirty.\n\nEvery programmer starts out writing some perfect little snowflake like this. Then they‚Äôre told on Friday they need to have six hundred snowflakes written by Tuesday, so they cheat a bit here and there and maybe copy a few snowflakes and try to stick them together or they have to ask a coworker to work on one who melts it and then all the programmers‚Äô snowflakes get dumped together in some inscrutable shape and somebody leans a Picasso on it because nobody wants to see the cat urine soaking into all your broken snowflakes melting in the light of day. Next week, everybody shovels more snow on it to keep the Picasso from falling over.\n\nThere‚Äôs a theory that you can cure this by following standards, except there are more ‚Äústandards‚Äù than there are things computers can actually do, and these standards are all variously improved and maligned by the personal preferences of the people coding them, so no collection of code has ever made it into the real world without doing a few dozen identical things a few dozen not even remotely similar ways. The first few weeks of any job are just figuring out how a program works even if you‚Äôre familiar with every single language, framework, and standard that‚Äôs involved, because standards are unicorns.\n\nI spent a few years growing up with a closet in my bedroom. The closet had an odd design. It looked normal at first, then you walked in to do closet things, and discovered that the wall on your right gave way to an alcove, making for a handy little shelf. Then you looked up, and the wall at the back of the alcove gave way again, into a crawlspace of utter nothingness, where no light could fall and which you immediately identified as the daytime retreat for every ravenous monster you kept at bay with flashlights and stuffed animals each night.\n\nThis is what it is to learn programming. You get to know your useful tools, then you look around, and there are some handy new tools nearby and those tools show you the bottomless horror that was always right next to your bed.\n\nFor example, say you‚Äôre an average web developer. You‚Äôre familiar with a dozen programming languages, tons of helpful libraries, standards, protocols, what have you. You still have to learn more at the rate of about one a week, and remember to check the hundreds of things you know to see if they‚Äôve been updated or broken and make sure they all still work together and that nobody fixed the bug in one of them that you exploited to do something you thought was really clever one weekend when you were drunk. You‚Äôre all up to date, so that‚Äôs cool, then everything breaks.\n\n‚ÄúDouble you tee eff?‚Äù you say, and start hunting for the problem. You discover that one day, some idiot decided that since another idiot decided that 1/0 should equal infinity, they could just use that as a shorthand for ‚ÄúInfinity‚Äù when simplifying their code. Then a non-idiot rightly decided that this was idiotic, which is what the original idiot should have decided, but since he didn‚Äôt, the non-idiot decided to be a dick and make this a failing error in his new compiler. Then he decided he wasn‚Äôt going to tell anyone that this was an error, because he‚Äôs a dick, and now all your snowflakes are urine and you can‚Äôt even find the cat.\n\nYou are an expert in all these technologies, and that‚Äôs a good thing, because that expertise let you spend only six hours figuring out what went wrong, as opposed to losing your job. You now have one extra little fact to tuck away in the millions of little facts you have to memorize because so many of the programs you depend on are written by dicks and idiots.\n\nAnd that‚Äôs just in your own chosen field, which represents such a tiny fraction of all the things there are to know in computer science you might as well never have learned anything at all. Not a single living person knows how everything in your five-year-old MacBook actually works. Why do we tell you to turn it off and on again? Because we don‚Äôt have the slightest clue what‚Äôs wrong with it, and it‚Äôs really easy to induce coma in computers and have their built-in team of automatic doctors try to figure it out for us. The only reason coders‚Äô computers work better than non-coders‚Äô computers is coders know computers are schizophrenic little children with auto-immune diseases and we don‚Äôt beat them when they‚Äôre bad.\n\nRemember that stuff about crazy people and bad code? The internet is that except it‚Äôs literally a billion times worse. Websites that are glorified shopping carts with maybe three dynamic pages are maintained by teams of people around the clock, because the truth is everything is breaking all the time, everywhere, for everyone. Right now someone who works for Facebook is getting tens of thousands of error messages and frantically trying to find the problem before the whole charade collapses. There‚Äôs a team at a Google office that hasn‚Äôt slept in three days. Somewhere there‚Äôs a database programmer surrounded by empty Mountain Dew bottles whose husband thinks she‚Äôs dead. And if these people stop, the world burns. Most people don‚Äôt even know what sysadmins do, but trust me, if they all took a lunch break at the same time they wouldn‚Äôt make it to the deli before you ran out of bullets protecting your canned goods from roving bands of mutants.\n\nYou can‚Äôt restart the internet. Trillions of dollars depend on a rickety cobweb of unofficial agreements and ‚Äúgood enough for now‚Äù code with comments like ‚ÄúTODO: FIX THIS IT‚ÄôS A REALLY DANGEROUS HACK BUT I DON‚ÄôT KNOW WHAT‚ÄôS WRONG‚Äù that were written ten years ago. I haven‚Äôt even mentioned the legions of people attacking various parts of the internet for espionage and profit or because they‚Äôre bored. Ever heard of 4chan? 4chan might destroy your life and business because they decided they didn‚Äôt like you for an afternoon, and we don‚Äôt even worry about 4chan because another nuke doesn‚Äôt make that much difference in a nuclear winter.\n\nOn the internet, it‚Äôs okay to say, ‚ÄúYou know, this kind of works some of the time if you‚Äôre using the right technology,‚Äù and BAM! it‚Äôs part of the internet now. Anybody with a couple of hundred dollars and a computer can snag a little bit of the internet and put up whatever awful chunks of hack code they want and then attach their little bit to a bunch of big bits and everything gets a little bit worse. Even the good coders don‚Äôt bother to learn the arcane specifications outlined by the organizations people set up to implement some unicorns, so everybody spends half their time coping with the fact that nothing matches anything or makes any sense and might break at any time and we just try to cover it up and hope no one notices.\n\nHere are the secret rules of the internet: five minutes after you open a web browser for the first time, a kid in Russia has your social security number. Did you A computer at the NSA now automatically tracks your physical location for the rest of your life. Sent an email? Your email address just went up on a billboard in Nigeria.\n\nThese things aren‚Äôt true because we don‚Äôt care and don‚Äôt try to stop them, they‚Äôre true because everything is broken because there‚Äôs no good code and everybody‚Äôs just trying to keep it running. That‚Äôs your job if you work with the internet: hoping the last thing you wrote is good enough to survive for a few hours so you can eat dinner and catch a nap.\n\nFunny, right? No? How about this exchange:\n\nWasn‚Äôt that guy helpful? With the camel? Doesn‚Äôt that seem like an appropriate response? No? Good. You can still find Jesus. You have not yet spent so much of your life reading code that you begin to talk in it. The human brain isn‚Äôt particularly good at basic logic and now there‚Äôs a whole career in doing nothing but really, really complex logic. Vast chains of abstract conditions and requirements have to be picked through to discover things like missing commas. Doing this all day leaves you in a state of mild aphasia as you look at people‚Äôs faces while they‚Äôre speaking and you don‚Äôt know they‚Äôve finished because there‚Äôs no semicolon. You immerse yourself in a world of total meaninglessness where all that matters is a little series of numbers went into a giant labyrinth of symbols and a different series of numbers or a picture of a kitten came out the other end.\n\nThe destructive impact on the brain is demonstrated by the programming languages people write. This is a program:\n\nThat program does exactly the same thing as this program:\n\nAnd once somebody wrote a programming language that let somebody else write this:\n\nAccording to the author, that program is \"two lines of code that parse two lines of embedded comments in the code to read the Mayan numbers representing the individual ASCII characters that make up the magazine title, rendered in 90-degree rotated ASCII art.\"\n\nThat program won a contest, because of course it did. Do you want to live in a world like this? No. This is a world where you can smoke a pack a day and nobody even questions it. \"Of course he smokes a pack a day, who wouldn't?\" Eventually every programmer wakes up and before they're fully conscious they see their whole world and every relationship in it as chunks of code, and they trade stories about it as if sleepiness triggering acid trips is a normal thing that happens to people. This is a world where people eschew sex to write a programming language for orangutans. All programmers are forcing their brains to do things brains were never meant to do in a situation they can never make better, ten to fifteen hours a day, five to seven days a week, and every one of them is slowly going mad.\n\nSo no, I‚Äôm not required to be able to lift objects weighing up to fifty pounds. I traded that for the opportunity to trim Satan‚Äôs pubic hair while he dines out of my open skull so a few bits of the internet will continue to work for a few more days.\n\n(Update: now available in Greek, Czech, Italian, Russian, Portuguese, Hungarian, French, Hebrew (PDF by Ilil Hoz), German (PDF by Kurt Frock), Spanish, and Chinese)",
    "readingTime": 13,
    "keywords": [
      "programming languages",
      "you‚Äôre familiar",
      "programming language",
      "suspension bridge",
      "don‚Äôt",
      "it‚Äôs",
      "internet",
      "they‚Äôre",
      "there‚Äôs",
      "together"
    ],
    "qualityScore": 1,
    "link": "https://www.stilldrinking.org/programming-sucks",
    "thumbnail_url": "https://www.stilldrinking.org/blog_images/programming-sucks.jpg",
    "created_at": "2026-01-28T06:22:46.410Z",
    "topic": "tech"
  },
  {
    "slug": "pixel-arcade-studio-kids-make-playable-browser-games-by-instructing-ai",
    "title": "Pixel Arcade Studio ‚Äìkids make playable browser games by instructing AI",
    "description": "Kids create real browser games by giving clear instructions to AI. No coding, no downloads.",
    "fullText": "They need to learn how to give clear instructions to AI.\n\nPixel Arcade Studio is a browser-based game studio where kids create real, playable games by telling an AI assistant exactly what to build. No coding. No installs. Designed with parents in mind.\n\nSafe, creative screen time kids love. Try free for 14 days.\n\nFast \"time-to-wow\" ‚Äî kids see their games come to life quickly.\n\nFrictionless ‚Äî everything runs directly in the browser.\n\nSafety default ‚Äî games publish with privacy protections enabled.\n\nCoding used to be how people told computers what to do.\n\nToday, the more important skill is knowing how to describe what you want, break ideas into steps, and give clear instructions to an AI system.\n\nPixel Arcade Studio is built around that shift.\n\nKids don't write code here. They practice explaining ideas, testing results, and improving their instructions when something doesn't work.\n\nThat's the skill they'll use in the real world.\n\nYour child picks a game template and describes what they want to make. Characters, goals, movement, and rules.\n\nYour child tells the AI assistant what to create or change. The AI follows instructions. It does not take over.\n\nThe game runs right away in the browser. No setup and no waiting.\n\nKids adjust their instructions, test again, and see how clearer directions lead to better results.\n\nGames can be shared with family or friends using parent-approved links.\n\nThis is not about memorizing technical skills. It's about clear thinking and communication.\n\nAI in Pixel Arcade Studio is a tool, not a shortcut.\n\nIt responds only to what your child asks. It does not browse the internet. It does not publish content on its own. It stays inside kid-safe boundaries.\n\nYour child stays in control. The AI helps carry out instructions.\n\nPixel Arcade Studio is built for families who want creative screen time without constant supervision.\n\nInstead, kids focus on giving clear instructions and seeing real results. They make games people can actually play.\n\nEvery game made in Pixel Arcade Studio is playable in the browser and shareable through safe links.\n\nKids don't just save projects. They create something real and playable.\n\nPixel Arcade Studio is a browser-based game studio where kids create playable games by giving instructions to an AI assistant. There is no coding involved.\n\nNo. Pixel Arcade Studio does not teach coding. Kids learn how to clearly describe ideas, give instructions, and work with AI to create games.\n\nPixel Arcade Studio is designed for kids ages 7 to 12.\n\nYes. Games publish in safe mode by default, sharing requires parent approval, and the platform includes content filtering and privacy protections.\n\nNo. Everything runs directly in the web browser. There are no downloads or installs.\n\nThe AI follows your child's instructions. It helps turn ideas into games but does not take control or act on its own.\n\nYes. Games can be shared using parent-approved links so friends and family can play safely.\n\nPixel Arcade Studio does not involve coding or block-based programming. It focuses on teaching kids how to give clear instructions to AI and refine their ideas through iteration.\n\nNo coding. No downloads. Designed for ages 7‚Äì12.",
    "readingTime": 3,
    "keywords": [
      "pixel arcade studio",
      "creative screen",
      "privacy protections",
      "parent-approved links",
      "browser-based game",
      "kids don't",
      "games publish",
      "playable games",
      "the ai",
      "kids create"
    ],
    "qualityScore": 1,
    "link": "https://pixelarcade.studio",
    "thumbnail_url": "http://localhost:3000/images/pas_og.jpg",
    "created_at": "2026-01-28T06:22:43.763Z",
    "topic": "tech"
  },
  {
    "slug": "acm-sigplan-symposium-on-principles-of-programming-languages-popl-2026-talks",
    "title": "ACM SIGPLAN Symposium on Principles of Programming Languages (POPL) 2026 talks",
    "description": "Special Interest Group on Programming Languages\nThe ACM Special Interest Group on Programming Languages (SIGPLAN) explores programming language concepts and tools, focusing on design, implementation, practice, and theory. Its members are programming language developers, educators, implementers, researchers, theoreticians, and users.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.youtube.com/channel/UCwG9512Wm7jSS6Iqshz4Dpg",
    "thumbnail_url": "https://yt3.googleusercontent.com/ytc/AIdro_mB1glk2WSnakJ9VmIADfpyj122SV1iL5zw1rMBEQFIqQ=s900-c-k-c0x00ffffff-no-rj",
    "created_at": "2026-01-27T18:24:29.546Z",
    "topic": "tech"
  },
  {
    "slug": "agent-skills-from-claude-to-open-standard-to-your-daily-coding-workflow",
    "title": "Agent Skills: From Claude to Open Standard to Your Daily Coding Workflow",
    "description": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we're witnessing something far more...",
    "fullText": "When Anthropic introduced Agent Skills for Claude, it appeared to be another proprietary AI customization feature. Today, we‚Äôre witnessing something far more significant: an open standard reshaping how people across roles‚Äîdevelopers, designers, product managers, and operations‚Äîwork with AI assistants. AI coding agents‚Äô adoption of Agent Skills has transformed this technology from an interesting experiment into an essential developer tool.\n\nIf you‚Äôve been using custom instructions or wondering how to make your AI assistant truly understand your project‚Äôs workflows, Agent Skills provides a compelling solution.\n\nAgent Skills began with Anthropic‚Äôs Claude AI, where developers first experienced giving AI agents specialized capabilities through structured instructions. Unlike simple prompts or one-off commands, Agent Skills introduced a sophisticated approach: packaging instructions, scripts, templates, and documentation into reusable, discoverable units.\n\nAnthropic‚Äôs decision to release Agent Skills as an open standard transformed it from a Claude-specific feature into a movement. The format‚Äôs simplicity and effectiveness attracted attention across the AI development tools ecosystem. Today, major players‚ÄîClaude Code, GitHub Copilot, Cursor, OpenCode, Mistral Vibe, Antigravity, OpenAI Codex, and Kiro‚Äîhave adopted the standard. Others are exploring integration, and more are joining it (I‚Äôm looking at you, JetBrains Junie).\n\nAgent Skills are elegantly simple: a folder containing a SKILL.md file. This file uses YAML frontmatter for metadata and Markdown for instructions. No complex APIs, no proprietary formats‚Äîjust structured text any AI agent can understand.\n\nHere‚Äôs a basic Agent Skills example for creating NUnit unit tests in C#:\n\nThose Agent Skills files live in your agent configuration, for example for GitHub Copilot .github/skills/create-nunit-unit-test/SKILL.md in your repository. Or they can be globally installed for your user account, e.g. ~/.copilot/skills/create-nunit-unit-test/SKILL.md.\n\nThis is a minimal example. You can add resources like templates, example configurations, or helper scripts in the same directory, and the skill can reference them.\n\nWhat makes Agent Skills innovative isn‚Äôt just the format‚Äîit‚Äôs how AI agents consume them. The system uses a three-level progressive disclosure approach that optimizes context window usage:\n\nLevel 1: Discovery ‚Äî At startup, the agent reads only the name and description from each skill. This lightweight metadata helps the agent understand available capabilities without consuming context.\n\nLevel 2: Activation ‚Äî When your request matches a skill‚Äôs description, the agent loads the full instructions from the SKILL.md file. Only then do detailed procedures become available.\n\nLevel 3: Execution ‚Äî The agent accesses additional files (scripts, examples, documentation) only as needed during execution.\n\nThis architecture solves a critical problem: you can install dozens of Agent Skills without overwhelming the AI‚Äôs context window. The agent loads only what‚Äôs relevant to your current development task.\n\nGitHub Copilot‚Äôs Agent Skills are experimental since December 2025 (version 1.108) for VS Code. Here‚Äôs your step-by-step setup guide:\n\nInstall VS Code ‚Äî Download from code.visualstudio.com\n\nEnable Agent Skills ‚Äî Open settings (Ctrl+,) and enable chat.useAgentSkills\n\nCreate your skills directory ‚Äî In your project root, create .github/skills/\n\nAdd your first skill ‚Äî Create a subdirectory for each skill with its SKILL.md file\n\nUse Agent Mode ‚Äî In Copilot Chat, switch to Agent mode to leverage skills\n\nOnce configured, Agent Skills activate automatically based on your prompts. No manual selection required‚Äîthe AI determines which skills are relevant based on your descriptions.\n\nSkills share knowledge‚Äîbest practices, workflows, and procedural guidance‚Äîin simple Markdown SKILL.md files that anyone can author; they load progressively to conserve tokens, require no server, and run on web, desktop, and CLI, making them ideal for documentation, checklists, examples, and repeatable workflows.\n\nMCP extends functionality by connecting to APIs, databases, and external tools: it consists of code and service/tool definitions that require development and hosting, loads tool definitions up front (consuming more context), so it‚Äôs best suited for tasks needing direct access to external systems.\n\nUse Skills to make knowledge discoverable and consistent, and use MCP to perform integrated actions and extend platform capabilities; together they provide both lightweight guidance and powerful automation.\n\nNevertheless, I can imagine a future where Agent Skills replace MCP for many scenarios, given their simplicity, portability, and ease of authoring. As you can bundle scripts and resources with skills, they can cover many use cases MCP currently serves.\n\nYou might wonder how Agent Skills differ from the custom instructions feature. Custom Instructions are best for defining coding standards and conventions, setting language or framework preferences, specifying code-review guidelines, and establishing commit-message formats. Agent Skills are designed to package reusable workflows, include executable scripts and templates, define specialized procedures (testing, debugging, deployment), and enable capabilities that run beyond the IDE (CLI and coding agents).\n\nThink of custom instructions as your coding style guide and Agent Skills as your AI development toolbox. Custom instructions tell the AI how you want code written; Agent Skills give the AI specialized capabilities to perform complex development tasks.\n\nHere are some practical Agent Skills that can transform your daily development workflow. Check the references section for pointers to more ready-to-use skills:\n\nRead the Agent Skills Specification to understand the format and capabilities. Use Skill Creator, an Agent Skill to create and refine new Agent Skills. Inception moment anyone ü§î?\n\nStart building your Agent Skills collection with these proven strategies:\n\nIdentify Repetitive Tasks ‚Äî Notice which development workflows you explain to the AI repeatedly. Each recurring explanation is a candidate for an Agent Skill.\n\nStart Simple ‚Äî Begin with straightforward skills that codify standard development procedures. As you gain confidence, add scripts and more complex resources.\n\nMake Descriptions Specific ‚Äî The quality of your skill‚Äôs description directly impacts how well the AI knows when to activate it. Be explicit about use cases and capabilities.\n\nInclude Examples ‚Äî Agent Skills with concrete code examples are more effective. Show the AI what good output looks like.\n\nLeverage Community Skills ‚Äî Explore the github/awesome-copilot and anthropics/skills repositories for inspiration and ready-to-use skills.\n\nOrganize by Domain ‚Äî Group related Agent Skills together. Create separate skills for testing, deployment, documentation, code review, and other specialized development domains.\n\nHere‚Äôs how Agent Skills enhance your workflow in a typical development scenario:\n\nYou‚Äôre working on a web application and need to add a new REST API endpoint with proper testing and documentation. With appropriate Agent Skills in place:\n\nYou ask: ‚ÄúHelp me add a new user registration endpoint with validation‚Äù\n\nThe rest-api-integration skill activates, providing structured guidance on implementing the endpoint with proper authentication, validation, and error handling.\n\nYou ask: ‚ÄúCreate tests for this endpoint‚Äù\n\nThe webapp-testing skill engages, generating test cases for success scenarios, validation failures, and edge cases.\n\nYou ask: ‚ÄúGenerate documentation for this endpoint‚Äù\n\nThe api-documentation skill activates, producing comprehensive documentation with examples, error codes, and authentication details.\n\nEach Agent Skill ensures consistency in approach and completeness in implementation. Without skills, you‚Äôd need to provide detailed instructions for each request or rely on the AI‚Äôs general knowledge, which might miss project-specific patterns.\n\nWhen working with Agent Skills, especially community-shared skills, keep these security considerations in mind:\n\nReview Before Use ‚Äî Always examine shared Agent Skills before adding them to your project. Check for potentially malicious scripts or unexpected behaviors in the SKILL.md file and associated resources.\n\nUse Terminal Controls ‚Äî VS Code‚Äôs terminal tool provides controls for script execution, including auto-approve options with configurable allow-lists. Configure these appropriately for your security requirements.\n\nVersion Control Your Skills ‚Äî Agent Skills are just files, so commit them to your repository. This enables code review, versioning, and team collaboration on AI capabilities.\n\nTest in Safe Environments ‚Äî Try new Agent Skills in development environments before using them in production contexts. Dev containers or isolated workspaces are ideal for testing.\n\nDocument Team Skills ‚Äî If your team uses shared Agent Skills, maintain documentation about what each skill does and when to use it.\n\nAgent Skills represent more than a new feature‚Äîit‚Äôs a glimpse into a future where AI development capabilities are portable, shareable, and composable. As more AI tools adopt the standard, we‚Äôre moving toward an ecosystem where:\n\nWhether you‚Äôre using VS Code or any other editor/IDE, working in the terminal with Copilot CLI, or leveraging any coding agent for automated tasks, your Agent Skills come with you.\n\nReady to integrate Agent Skills into your development workflow? Follow this action plan:\n\nThe goal isn‚Äôt to create dozens of Agent Skills immediately. Start with one or two that solve real problems in your development workflow, then expand your library organically as needs arise.\n\nYou can also use Agent Skills with GitHub Copilot CLI or Gemini CLI for terminal-based workflows, or with other coding agents that support the open standard. This portability ensures your investment in creating skills pays off across all your AI-assisted development tools.\n\nMy preferred introduction to Agent Skills is the following video from Burke Holland, which covers the concepts, setup, and practical examples in under 20 minutes:\n\nFor my French readers, I discussed Agent Skills in depth on devdevdev.net in the following episode\n\nAgent Skills bridges the gap between generic AI assistance and specialized, context-aware support for your specific development needs. By adopting an open standard that works across AI tools, the industry has created a foundation for truly portable AI capabilities.\n\nThe journey from Claude to open standard to GitHub Copilot adoption demonstrates the power of simplicity and interoperability in developer tools. As developers, we benefit from this ecosystem approach‚Äîour investment in creating Agent Skills pays dividends across our entire development toolchain.\n\nStart small, experiment with the format, and build Agent Skills that improve your daily development work. The progressive disclosure system ensures you won‚Äôt overwhelm your AI assistant, and the portable format guarantees your skills remain valuable as AI tools evolve.\n\nThe future of AI-assisted development isn‚Äôt just about more powerful models‚Äîit‚Äôs about giving those models the right context, capabilities, and knowledge to be genuinely helpful in your specific development domain. Agent Skills is a significant step in that direction.\n\nWhat development workflows could benefit from specialized Agent Skills? Have you tried creating skills for your AI coding assistant? Share your experiences in the comments below.",
    "readingTime": 9,
    "keywords": [
      "agent skills",
      "skill.md file",
      "progressive disclosure",
      "ai-assisted development",
      "skill‚Äôs description",
      "context window",
      "agent mode",
      "shared agent",
      "coding agents",
      "skill activates"
    ],
    "qualityScore": 1,
    "link": "https://laurentkempe.com/2026/01/27/Agent-Skills-From-Claude-to-Open-Standard/",
    "thumbnail_url": "https://live.staticflickr.com/65535/55058424290_cced09531e_h.jpg",
    "created_at": "2026-01-27T12:26:46.640Z",
    "topic": "tech"
  },
  {
    "slug": "one-developer-used-claude-to-build-a-memorysafe-extension-of-c",
    "title": "One developer used Claude to build a memory-safe extension of C",
    "description": "feature: Robin Rowe talks about coding, programming education, and China in the age of AI",
    "fullText": "feature TrapC, a memory-safe version of the C programming language, is almost ready for testing.\n\n\"We're almost there,\" Robin Rowe told The Register in a phone interview. \"It almost works.\"\n\nWe caught up with Rowe, a computer science professor and entrepreneur, amid debugging efforts that had kept him up until four in the morning. The long-awaited TrapC website has appeared.\n\n\"My work building TrapC has taken two parallel paths,\" Rowe explains in his initial post. \"A TrapC interpreter called itrapc and a separate compiler called trapc. I had wanted to make a software release by 1 January 2026, but too many bugs. I only reached code complete this month and am now on the painstaking and sleepless process of debugging. When I have something stable that mostly works I will make a release. Sorry to make you wait a little longer. Aiming for Q1 2026.\"\n\nBack in November 2024, Rowe explained that he was working on TrapC. At the time, the public and private sector had undertaken a campaign to promote memory-safe software development as a way to reduce exposure to serious vulnerabilities.\n\nMemory safety provides a way of ensuring that memory-related bugs like out-of-bounds reads/writes and use-after-free don't happen. In large codebases, like Chromium and Windows, most of the security vulnerabilities follow from memory bugs. As that message has been repeated in recent years, memory safety has become an imperative, evangelized by the likes of Google and Microsoft, and more recently by authorities in the US and elsewhere.\n\nFor at least the past ten years, there's been a rising chorus of voices calling for the adoption of memory-safe programming languages and techniques. This has meant encouraging developers to avoid languages like C and C++ where feasible, and to adopt languages like C#, Go, Java, Python, Swift, and Rust, instead, particularly for new projects.\n\nTo remain relevant, the C and C++ communities have tried to address those concerns with projects like TrapC, FilC, Mini-C, Safe C++, and C++ Profiles. There's also a C to Rust conversion project under development at DARPA called TRACTOR ‚Äì TRanslating All C TO Rust.\n\nBut progress has been slow and those writing in C and C++ haven't found a widely accepted approach. The C++ standards committee recently rejected the Safe C++ proposal. And Rowe said he doubted TRACTOR would have anything to show this year.\n\nMeanwhile, the clock is ticking. Microsoft engineer Galen Hunt last month said, \"My goal is to eliminate every line of C and C++ from Microsoft by 2030. Our strategy is to combine AI and algorithms to rewrite Microsoft's largest codebases.\"\n\n\"There are some efforts to port C code by hand to Rust,\" said Rowe. \"But there're some real challenges to doing that because there are some idioms in C that cannot be expressed in Rust.\n\n\"Rust is much more type safe than C is. And so if you have a void pointer, what does that mean in Rust? There's no translation for it. And that's how TrapC is fundamentally different because it actually remembers what that void pointer actually is.\"\n\nRowe said he expects TRACTOR will eventually be able to accomplish C to Rust translation using AI. But he said he thinks it's better to just build the necessary tooling into the C compiler, so you don't have to rely on some external tool that rewrites your code in an unfamiliar language.\n\nRowe has been using AI tools himself and has been teaching others to do so. This past semester, he taught AI Cybersecurity Programmer Analyst (PCO471) at Community College of Baltimore County ‚Äì Linux administration using vibe coding in bash with no prerequisites. And starting in February, he's teaching C++ Programming with Generative AI (PCO472) ‚Äì vibe coding in C++.\n\nRowe said programming has fundamentally changed as a result of AI tools. \"I think this is sort of the same type of discussion as when C came in and people said, 'Well, I'm happy in assembly.' There will still be people doing it the old way. But because vibe programming is so much more efficient on time when done correctly, there's gonna be no choice. You just won't be competitive if you're not vibe programming.\"\n\nThen he shifted gears, slightly. \"But I have to walk that back a little bit because the reason I was up until four in the morning is I had vibe programming working on the Trap C compiler. And it took a fundamentally wrong design turn. And I didn't detect that it had made a design mistake. I had told it how I wanted to approach it. But somehow it misunderstood me or it forgot or something happened and I forgot to check. And so I spent hours doodling around in the debugger and trying to understand why code was acting weird before I finally looked at it and said, 'wait a minute, this isn't even the right design.'\"\n\nRowe said a similar situation crops up in pair programming, where you've told someone to do something and they didn't do it, and you don't realize that until later.\n\n\"[C++ creator] Bjarne Stroustrup famously said that the most important thing in software design is to be clear about what you're trying to build,\" Rowe said. \"And vibe [programming] just puts that on steroids. Now we not only have to be ourselves clear, but we have to communicate it clearly to an LLM.\"\n\nRowe argues that developers have to be encouraged to try AI tools and to make mistakes. He recounted how during his AI Cybersecurity Programmer Analyst course, his students expressed interest in doing more hands-on work in lieu of lectures.\n\n\"So I said, 'I've got real servers on the internet that are my companies. I'll give you root,'\" he recalled. \"I'll set loose students who know nothing on my own servers and hope for the best and we'll see how this goes. And the reaction was panic. I couldn't get past the timidity cliff.\"\n\nRowe said that what he learned from that exchange was that they didn't want their own hands-on, they wanted to watch him work.\n\n\"I said to them, 'But guys, this is like learning to play the piano. You can't learn to play the piano by watching me. Yeah, you guys have to practice. And it's gonna be embarrassing at first. You know, you're gonna play a lot of bad notes and sound terrible. You have to get over that situation'.\"\n\nThat's a scenario playing out in various companies where AI tools remain underutilized, for various reasons, including lack of training, security concerns, lack of utility, and poor tool design.\n\nRowe has traveled often to China to speak at the China Association of Higher Education conference. In December, he said, he was interviewed on China News Television about how China's plan for AI compares with America's.\n\nIn an email he explained, \"I said, 'China's AI-Plus plan calls for efficient AI on devices everywhere, from farm to factory to city, while the White House plan calls for building 500-billion-dollar cloud data centers ... using chips that will, inevitably, seem obsolete within two years.'\"\n\nRowe argues China's approach will prevail and that the US has taken the wrong turn by focusing on centralized cloud datacenters to run LLMs. Within two years, he said, we'll have AI models we can run locally on our phones, with no need for network access for most tasks. Apple and Huawei, he said, are likely to be the winners in this scenario.\n\nRowe pointed to China's DeepSeek as an example. While it may not be quite as good as the leading US commercial models, he said, it runs with far less power.\n\n\"This is a very Moore's Law type of strategy,\" he said. \"I remember when I had a Navy supercomputer in 1994. That was an amazing technology. But in 1995, Cray went bankrupt. There weren't enough buyers for it, even though it was an amazing device.\n\n\"And now I've got an iPhone that's in my pocket. That runs on a battery. It doesn't have a whole room devoted to it and exotic cooling and all kinds of stuff. And it's more powerful than that [the Cray from 1994]. So as a long-term strategy, you know, going toward the device makes a lot more sense, because that half-trillion dollar data center is going to be on my iPhone eventually.\"\n\nRowe also said that on the recommendation of a friend from his time at the AT&T DIRECTV Innovation Lab, he tried running Deepseek at a time when Claude wasn't available. Deepseek, he said, was able to find a bug that Claude couldn't.\n\n\"Surprisingly, the bug was in code Claude had generated, that I had cut-and-pasted carelessly,\" he said. \"With hindsight it was a silly code mistake I should have caught, but was in an 'else' branch outside where I was looking. I'd not expected or intended to have Claude make any change to that block of code. And because the code was valid but the logic wrong, the compiler didn't catch it.\"\n\nBut the bug was obvious, he said, as soon as Deepseek pointed it out. He added, \"I'm paying $200/year for Claude. Deepseek is free.\" ¬Æ",
    "readingTime": 8,
    "keywords": [
      "cybersecurity programmer",
      "programmer analyst",
      "rowe argues",
      "void pointer",
      "memory safety",
      "vibe coding",
      "design rowe",
      "vibe programming",
      "code",
      "compiler"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/26/trapc_claude_c_memory_safe_robin_rowe/",
    "thumbnail_url": "https://regmedia.co.uk/2022/03/23/shutterstock_c.jpg",
    "created_at": "2026-01-27T06:21:27.835Z",
    "topic": "tech"
  },
  {
    "slug": "a-developer-teamed-up-with-claude-to-create-elo-programming-language",
    "title": "A developer teamed up with Claude to create Elo programming language",
    "description": "feature: Bernard Lambeau, the human half of a pair programming team, explains how he's using AI",
    "fullText": "feature Bernard Lambeau, a Belgium-based software developer and founder of several technology companies, created a programming language called Elo with the help of Anthropic's Claude Code.\n\nStarting on December 25, 2025, he published a series of posts about the project. The first post names Claude as a co-author.\n\n\"In roughly 24 hours of collaboration, we built a complete expression language with a parser, type system, three compilers, a standard library, a CLI tool, and a documentation website. Not bad for a day's work,‚Äù Lambeau and Claude wrote.\n\n\"Elo isn't just a demonstration that AI can write code. It's a demonstration that humans and AI can build together ‚Äì each contributing what they do best,‚Äù they added.\n\nAs an expression language that compiles to JavaScript, Ruby, and SQL, Elo is intended as a portable way to handle form validation, e-commerce order processing, and subscription logic.\n\nLambeau, founder and CTO of Klaro Cards and CEO of app consultancy Enspirit, is not the first to develop a programming language with the help of AI.\n\nSteve Klabnik performed a similar feat last year with the Rue programming language. In September 2025, Geoffrey Huntley enlisted Claude to write a programming language called Cursed. And before that, Avital Tamir published a Claude-authored repo for the Server programming language, with the caveat that the code is not intended for actual use.\n\nClaude Code isn't the only AI-assisted programming method having a moment. AI biz Cursor created a rudimentary browser using OpenAI's GPT-5.2. And developer Ola Pr√∏is used Cursor, powered by Claude, to create a Rust-based text editor called Ferrite.\n\nClaude users generally acknowledge that their pair partner makes mistakes. But those committed to AI assistance find it worthwhile to clean up after their helper.\n\n\"Claude Code knows almost every tech stack (and can search the web), knows the Linux commands that matter (search code, search & replace, compile, test, etc.), and does that 10x faster than I can do myself,\" Lambeau told The Register in an email interview.\n\nClaude, he said, allows him to use technology he hasn't mastered.\n\n\"I was already a full-stack developer (on languages, frameworks & reusable libraries I knew); I'm now a full-stack++ dev because I can also use languages, frameworks, and reusable libraries I barely know, if at all,\" he explained.\n\n\"Claude Code falls short if you don't have a great methodology. It needs feedback loops to work fine; otherwise, it derails. One possible feedback loop is a human reviewing code and testing manually. But there's a better/complementary approach if you want it to work autonomously. On both Elo and Bmg.js, I've started by making sure the testing methodology was effective and scientifically sound. Claude writes the tests, executes them, discovers where it's wrong, and corrects itself. Impressive.\"\n\nLambeau said he still needs to review some of Claude's output.\n\n\"But if I read the tests, agree with them, and can check myself that they run fine, I'm 95 percent sure it's already correct as a black box (not even reading the code),\" he explained. \"Then I can check the architecture and code quality as a white box by having a general look at the code, but I don't have to understand every detail.\"\n\nNotably, Lambeau documented the series of prompts he used to create the language. The repo includes more than 100 tasks used to direct the AI model. In addition, Lambeau has published a video that describes his AI pair programming process.\n\n\"I started in a setting where Claude Code asked for permissions every 20 seconds and I was checking everything it did,\" Lambeau explained. \"After a few successes, I quickly set up safe environments to be able to let Claude Code run in full autonomy (isolated computer & isolated Linux user, or running in a Docker image).\"\n\nLambeau said he still uses plan mode for complex tasks that require conversation with Claude.\n\n\"I review the plan, make sure we have a test strategy that's sound, then switch Claude to autonomous mode and look at the tests, code & results afterward,\" he said. \"That's very similar to a lead-dev/CTO + QA role, btw; it's just much faster than with human devs.\"\n\nLambeau, who has a PhD in software engineering and 30 years of experience as a developer, said both experts and novices can benefit from Claude Code, though he added that a service like Lovable might be more approachable for those not already acclimated to the command line.\n\n\"Now, when it comes to real software/product engineering, I think Claude Code requires experts (so far),\" he said. \"You still need to guide it a lot to keep the quality high enough. You need very strong expertise to do it effectively. Currently (Claude will still improve a lot), if you don't have the expertise, you certainly end up with a big mess of unmaintainable code.\"\n\nMany developers have said as much about AI tools. They're more useful as an amplifier of expertise than as a replacement for it. The situation is analogous to the introduction of sequencing software, digital synthesizers, and drum machines half a century ago. These tools enabled a lot of people who weren't great musicians to make music. But they didn't instill musical skill, and they produced the most interesting work in the hands of practiced musicians.\n\nThe cost to do this, Lambeau said, has been a Claude Max subscription that he purchased in December for ‚Ç¨180 a month. In that time, he says, he wrote Elo (https://elo-lang.org), completed Bmg.js (https://github.com/enspirit/bmg.js), completed Bmg's documentation (https://www.relational-algebra.dev), and created the first version of the Try page (https://www.relational-algebra.dev/try).\n\n\"It's all personal research and open-source projects,\" he said. \"It would have required several weeks to do the same manually myself, and several months to ask another developer to do it. The cost would be mostly because of the scientific & technical knowledge transfer about the data language I envision. Strangely enough, it's very cheap with Claude Code. There's something true about the fact that those LLMs have a PhD.\"\n\nLambeau explained that Elo isn't just a way to test Claude Code. He also sees it as an extension of his academic work in software engineering and his personal interest in the Relational Model ‚Äì he's served as a lecturer for database courses at Belgium‚Äôs UCLouvain.\n\n\"I'm absolutely convinced that we need better/safer/simpler programming languages inside no-code tools and when interconnecting them (e.g. Zapier, Make, n8n, etc.),\" he said. \"Mainstream programming languages are very complex, error-prone, sometimes dangerous, and the programs are difficult to review for non-experts.\"\n\n\"More importantly, they are cumbersome to use for even simple data tasks. I mean, even validating the schema and constraints of a data file at runtime tends to be a nightmare in existing languages. It's not built-in in any mainstream language; you immediately need validation libraries; most of them are limited in what they can easily check, so you need to add dedicated boilerplate code.\"\n\nIn a world where non-technical people will have the opportunity to write untrustworthy code with the help of AI, he said, we need to be able to run that code safely.\n\n\"Elo aims at providing a safe & simple alternative,\" he said. \"It will be a limited language (non-Turing-complete, as we say) but super safe & simple, and usable in 80 percent of common data use cases. The very first no-code tool to integrate it will be Klaro Cards, of course.\" ¬Æ",
    "readingTime": 7,
    "keywords": [
      "claude code",
      "elo isn't",
      "reusable libraries",
      "safe simple",
      "languages frameworks",
      "software engineering",
      "expression language",
      "programming language",
      "it's",
      "developer"
    ],
    "qualityScore": 1,
    "link": "https://www.theregister.com/2026/01/24/human_ai_pair_programming_elo/",
    "thumbnail_url": "https://regmedia.co.uk/2025/11/06/shutterstock_balancing_ai_and_humanity.jpg",
    "created_at": "2026-01-26T01:03:11.981Z",
    "topic": "tech"
  },
  {
    "slug": "5-acquisitions-winning-over-skeptical-engineers-and-spending-tens-of-millions-inside-a-public-companys-ai-native-push",
    "title": "5 acquisitions, winning over skeptical engineers, and spending tens of millions: Inside a public company's 'AI native' push",
    "description": "Amplitude gave Business Insider the inside look at its AI overhaul, from acquisitions to efforts to increase staff adoption of AI coding assistants.",
    "fullText": "There's a long banner hanging in Amplitude's San Francisco office. It reads: \"NO MAGICAL THINKING.\"\n\nNo, it's not some rag on Joan Didion. It's a reminder, CEO Spenser Skates told Business Insider, that technology can never replace deep thinking and hard work. In the AI age, that reminder is more important than ever ‚Äî so much so that employees must look up at it every day.\n\nAmplitude, an 800-person, publicly traded analytics company, is undergoing an AI transformation ‚Äî with the goal of reinvigorating its business.\n\nAmplitude went public in September 2021 at the height of the pandemic, climbing to an all-time closing high of $84.80 per share several weeks later before dropping significantly and largely plateauing in in recent years around $10. It closed at $10.25 on Friday.\n\nSince October 2024, the company has acquired five AI startups. Amplitude hired an AI-savvy engineering head and appointed one of its acquired founders to a new AI leadership position. It got Cursor and GitHub Copilot licenses for employees, and ran a heads-down AI week.\n\nIt's a change many companies are making: Rapidly moving from little-to-no AI to trying to become \"AI native,\" a term that's curiously hard to pin down. Large language models are popping up everywhere in white-collar work as companies chase the promise of efficiency gains.\n\nAmplitude's case may be especially informative, given just how skeptical of AI its CEO was. In 2023 and some of 2024, Skates said he viewed the AI industry as full of \"grifters,\" the visionaries promising to end world hunger and salesmen promising to automate everything.\n\n\"It had all sorts of problems,\" Skates said. By mid-2024, he realized \"there's probably going to be a breakthrough in the analytics space in the next two or three years.\"\n\n\"We've got to go make that ourselves,\" he said. \"So, we went all in.\"\n\nSkates had two opening moves for his AI overhaul.\n\nThe first: hiring a new chief engineering officer with a history in AI. Wade Chambers had advised the company since 2016, while holding leadership roles at Twitter and Included Health.\n\nWhen Chambers joined in October 2024, only 1% of the engineering, product, and design teams at Amplitude were using AI.\n\nThe second was the acquisition of Command AI, a chatbot startup. It was the first of a string of acquisitions, including June, Kraftful, and Inari. Amplitude announced its most recent acquisition, InfiniGrow, on January 14.\n\nYana Welinder was CEO of Kraftful, one of Amplitude's acquisition targets. Kraftful could spot power users of its product, one of whom was Amplitude's then-CPO. She reached out, and they chatted in February. The deal closed in July, and Welinder was named Amplitude's head of AI. A company blog post with an introductory Q&A referred to her as \"AI maven.\"\n\nWelinder's first order of business: speeding the company up. Kraftful shipped new product every week. Amplitude was shipping less than monthly.\n\n\"If you have this cadence of shipping infrequently, then the team slows down, which isn't appropriate in the age of AI,\" she said.\n\n\"Analytics will look very different 6 months from now,\" Skates wrote in his email. \"We have the opportunity to be the AI native company in Analytics and we are going to pull every piece of firepower we have.\"\n\nHe also asked employees to share a coming launch on X, as opposed to LinkedIn, because that's \"where the AI natives are.\"\n\nHow much has Amplitude spent on AI, from tools to acquisitions? \"Tens of millions, for sure,\" Skates said. \"I wouldn't be surprised if it got past $100 million.\"\n\nThen comes the harder part: convincing employees to really use the tools.\n\nWhile some engineers are excited about AI's promise, others are skeptical about its helpfulness, or worried about possible job losses. Not every engineer is as gung ho about AI as their management is.\n\nSkates said that engineers were especially sensitive to the \"grifting\" that went on in AI, making many of them skeptical. With a bottoms-up approach, that skepticism dissipates, he said.\n\nSoon after joining, Chambers began planning an \"AI week\" for the first week of June. It took six months of prep and borrowed heavily from Facebook's mobile push. He took the entire engineering, product, and design team offline for the week. To kick off, Chambers required that leaders get onstage and vibe-code something in front of the entire company.\n\n\"It didn't go well,\" Chambers said of the live vibe-coding demonstration. \"They had to work through it. They had to re-prompt a couple of different ways.\"\n\nBut the message stuck, he said. Leaders who weren't coding all day were able to build something \"pretty cool\" within the hourlong session, save a few hiccups.\n\nAdditional momentum came from the \"zealots,\" engineers passionate about exploring the new tech (some of whom Chambers brought over from his prior job). These engineers lead by example, he said.\n\nAmplitude shared its internal data tracking how many employees use its AI tools. In the final week of March, 14 employees were actively using Cursor. That figure peaked in the first week of December ‚Äî after AI week but before the holiday vacation cycle ‚Äî at 174 employees.\n\nAnd what of the thorny question about AI implementation in the enterprise: ROI? After all, a 2025 MIT study indicated 95% of firms publicly disclosing use of AI pilots reported no measurable ROI.\n\nAfter implementing these tools, developer productivity shot up 40% and stayed there, Chambers said. On some specific engineering teams, those gains looks more like 300-400%, he said.\n\n\"There's going to be a lot of people who are thinking they're the world's best expert at something,\" Chambers said. \"Increasingly, even the most cynical team members have come around.\"",
    "readingTime": 5,
    "keywords": [
      "engineering product",
      "roi after",
      "employees",
      "analytics",
      "tools",
      "engineers",
      "there's",
      "skeptical",
      "acquisition",
      "team"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/amplitude-ai-native-push-2026-1",
    "thumbnail_url": "https://i.insider.com/69729d28d3c7faef0eccc73a?width=1200&format=jpeg",
    "created_at": "2026-01-25T12:22:41.686Z",
    "topic": "finance"
  },
  {
    "slug": "skget-another-cli-to-add-skills-to-your-coding-agents",
    "title": "Skget, another CLI to add skills to your coding agents",
    "description": "A CLI to add skills to your coding agents. Contribute to czheo/skget development by creating an account on GitHub.",
    "fullText": "czheo\n\n /\n\n skget\n\n Public\n\n A CLI to add skills to your coding agents.\n\n License\n\n MIT license\n\n 0\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n czheo/skget",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/czheo/skget",
    "thumbnail_url": "https://opengraph.githubassets.com/ec41d6a0818a69873753057252cfb3dd2acf4c9944a55eb817aea02d4c36721f/czheo/skget",
    "created_at": "2026-01-25T01:04:24.594Z",
    "topic": "tech"
  },
  {
    "slug": "five-ways-people-are-using-claude-code",
    "title": "Five Ways People Are Using Claude Code",
    "description": "Claude Code generates computer code when people type prompts, so those with no coding experience can create their own programs and apps.",
    "fullText": "Claude Code, an artificial intelligence tool that can generate computer code when people type a prompt, is having a viral moment.\n\nThe tool, which the A.I. start-up Anthropic introduced in May, has shown record growth over the past two weeks, the company said, without sharing its data. People had time to experiment with Claude Code over the holidays, Anthropic said, and users realized how capable it was.\n\nClaude Code is one of several A.I. coding tools ‚Äî which also include Base44 and Cursor ‚Äî that people with no coding experience are increasingly using to build their own websites, programs and apps, a trend known as ‚Äúvibecoding.‚Äù People pay a subscription fee of $20 to $200 a month to use Claude Code, depending on the features they want.\n\nHere are five ways that people are using Claude Code:\n\nMr. Hindes, an assistant principal at a school for autistic children, has four children under the age of 9 and turned to A.I. to help him organize his family‚Äôs laundry.\n\nLast week, he prompted Claude Code to make a program to identify which clothes belonged to each of his three daughters so he could sort clean laundry into piles without their help. He took pictures of their clothes to teach Claude Code which T-shirt belonged to which daughter. Now he simply holds up the clothes to his laptop camera so the program tells him whom it belongs to.\n\n‚ÄúThe whole process was done within an hour, and the girls were really excited,‚Äù he said.\n\nMr. Hindes said he was now building a program with Claude Code to help his daughters independently work though the steps of their morning routine, as if playing a game.\n\n‚ÄúI‚Äôve tried to teach myself coding at various points but never stuck with it,‚Äù he said.\n\nMr. Stephenson, an art and architecture photographer, started using Claude Code in November to build a website about a documentary feature.\n\nThe website was created in about a day, he said, with an interactive map of New York City that captured his photos and audio recordings to document life in each borough.\n\n‚ÄúOnce the basic site was done, emboldened by my new capabilities, I started adding features I hadn‚Äôt even considered,‚Äù said Mr. Stephenson, who pays $20 a month for Claude Code. ‚ÄúLight/dark mode? Easy. Shuffle button? Done.‚Äù\n\nIf Claude Code could not solve a particular problem, he turned to Google‚Äôs A.I. chatbot, Gemini, to ask how it would approach the issue.\n\n‚ÄúI‚Äôd envisioned something like this when I started a couple of years ago, but assumed it would cost thousands of dollars to build,‚Äù he said.\n\nMr. Roberts, an assistant prosecuting attorney, used Claude Code and Cursor in August to create a mobile app called AlertAssist, which lets users send a mass text to contacts in an emergency. Working in law enforcement got Mr. Roberts interested in trying to help people act quickly and safely in an emergency.\n\nThe design and user interface of the app are ‚Äúvery basic, but it works,‚Äù he said.\n\nDuring the coronavirus pandemic, Ms. Haubo Dyhrberg, an assistant professor of finance at the University of Delaware, had an idea to make a stock trading simulator for her class. She consulted her husband, a software engineer, but ‚Äúthe task seemed too daunting.‚Äù\n\nOn Monday, she downloaded Claude Code and within two hours had a working demo of a trading simulator that her students could use to trade securities in a mock market. She has built five different trading scenarios for students to explore various challenges in financial markets.\n\n‚ÄúI never thought it would be this easy,‚Äù she said. ‚ÄúI can‚Äôt wait to test it out when the semester starts in two weeks.‚Äù\n\nMr. Bacus, who owns a welding and metal fabrication business, tapped Claude Code last month to create an A.I. assistant to manage his calendar and find him new business opportunities. The business is just him and three others, so ‚Äúwe‚Äôre not in the place right now to afford an office team,‚Äù Mr. Bacus said. ‚ÄúIt‚Äôs all on me.‚Äù\n\nWith Claude Code, he built a personal A.I. assistant that connects to his calendar, Google Sheets and Gmail account so he can easily create estimates, track the progress of jobs and organize contracts.\n\n‚ÄúI‚Äôm a skilled laborer who barely passed high school in the early 2000s,‚Äù Mr. Bacus said, adding: ‚ÄúBut over the past few months, I‚Äôve taught myself to build actual tools for my business.‚Äù",
    "readingTime": 4,
    "keywords": [
      "claude code",
      "a.i assistant",
      "trading simulator",
      "mr bacus",
      "mr stephenson",
      "mr roberts",
      "business",
      "coding",
      "program",
      "clothes"
    ],
    "qualityScore": 1,
    "link": "https://www.nytimes.com/2026/01/23/technology/claude-code.html",
    "thumbnail_url": "https://static01.nyt.com/images/2026/01/21/multimedia/CLAUDE-CODE-1-pztw/CLAUDE-CODE-1-pztw-facebookJumbo.jpg",
    "created_at": "2026-01-24T00:56:46.566Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-kills-open-source",
    "title": "Vibe Coding Kills Open Source",
    "description": "Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.15494",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-23T06:20:32.202Z",
    "topic": "tech"
  },
  {
    "slug": "gemini-cli-code-and-create-with-an-opensource-agent",
    "title": "Gemini CLI: Code and Create with an Open-Source Agent",
    "description": "Build real-world applications from the command line using Gemini CLI, Google's open-source agentic coding assistant that coordinates local tools and cloud services to automate coding and creative workflows.",
    "fullText": "Join this short course on Gemini CLI, taught by Jack Wotherspoon, Developer Advocate at Google.\n\nGemini CLI is an open-source agentic coding assistant that works from your terminal, giving it access to your local filesystem, development tools, and cloud services. This lets you delegate complex workflows‚Äîfrom building web features to creating marketing materials‚Äîthrough high-level instructions while the agent autonomously plans and executes multiple steps.\n\nIn this course, you‚Äôll apply Gemini CLI to software development and creative tasks by building features for an AI conference. You‚Äôll develop a website session catalog, create a data dashboard combining local and cloud data sources, and generate social media content from recordings. You‚Äôll master context management, integrate MCP servers, and orchestrate across multiple services with Gemini CLI extensions.\n\nWhether you‚Äôre prototyping applications, automating development workflows, or studying topics in agentic AI, this course gives you hands-on experience coordinating multiple tools to build faster and work more efficiently.",
    "readingTime": 1,
    "keywords": [
      "gemini cli",
      "course",
      "development",
      "you‚Äôll",
      "agentic",
      "tools",
      "cloud",
      "services",
      "features"
    ],
    "qualityScore": 0.65,
    "link": "https://learn.deeplearning.ai/courses/gemini-cli-code-and-create-with-an-open-source-agent/information",
    "thumbnail_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2026/01/Gemini-CLI-Code-Create-with-an-Open-Source-Agent_Banner_1920x1080__with_CTA.webp",
    "created_at": "2026-01-21T18:30:41.465Z",
    "topic": "tech"
  },
  {
    "slug": "vibebin-incuslxcbased-platform-for-selfhosting-persistent-sandboxes",
    "title": "Vibebin: Incus/LXC-based platform for self-hosting persistent sandboxes",
    "description": "vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh).  Create...",
    "fullText": "jgbrwn\n\n /\n\n vibebin\n\n Public\n\n vibebin is an Incus/LXC-based platform for self-hosting persistent AI coding agent sandboxes with Caddy reverse proxy and direct SSH routing to containers (suitable for VS Code remote ssh). Create and host your vibe-coded apps on a single VPS/server.\n\n License\n\n View license\n\n 9\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n jgbrwn/vibebin",
    "readingTime": 1,
    "keywords": [
      "vibebin",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/jgbrwn/vibebin",
    "thumbnail_url": "https://opengraph.githubassets.com/b08d7ce148c865871d7a69b11f4c233a6cd39949454bb9455bb3bdcf87a6a575/jgbrwn/vibebin",
    "created_at": "2026-01-21T12:26:57.995Z",
    "topic": "tech"
  },
  {
    "slug": "pragmatic-notes-on-running-dangerous-ai-coding-agents-in-cloud-vms",
    "title": "Pragmatic Notes on Running Dangerous AI Coding Agents in Cloud VMs",
    "description": "A practical approach to safely running AI coding agents with strong isolation using cloud VMs, Tailscale, and simple notification patterns.",
    "fullText": "Running coding agents with free reign is very powerful for a certain class of tasks, especially ones that require little human supervision, or where you want to close (or disconnect) your laptop, walk away, and come back to results.\n\nRecently there have been several HN discussions about safely running Claude Code or Copilot CLI agents, such as Yolobox ‚Äì Run AI coding agents with full sudo without nuking home dir and Running Claude Code dangerously. These post detail the potential dangers and show how to run these agents more safely, and while reasonable, I find they lack in a few respects.\n\nIn particular, I want strong isolation, long running agent tasks, minimal cognitive overhead and I really value being able to close my laptop, walk away, and get notified on my phone when things are done. I do not mind paying for a cloud VM.\n\nThere are many valid ways to solve this problem. This post describes mine. It covers running multiple coding agents concurrently in a cloud VM, how I handle access and repos, and how I keep notifications simple.\n\nI generated some Terraform to spin up an Azure VM with a cloud-init.yml for setting up common tools/environments I use. Claude can generate a decent starting point for this quite easily, given your particular environment.\n\nFor secure access, I use Tailscale. Note: I'm not paid by them, but it is easily my favorite piece of infrastructure software!\n\nA cloud-init script installs Tailscale on first boot and automatically joins the VM to my tailnet. SSH access is enabled using Tailscale SSH. Once the VM is up, it appears on my private network with a stable hostname via Magic DNS. No SSH key management, no exposed ports.\n\nor connect using VS Code Remote SSH:\n\nhttps://code.visualstudio.com/docs/remote/ssh\n\nMost of the time I prefer tight, step by step control over code generation, working locally in VS Code with Copilot. For longer running or experimental tasks, I instead let an agent work remotely on a branch inside the VM, and pull the results once I am satisfied.\n\nWhile this is arguably git basics, it works well for me and I found that it is useful sharing how to set up a VM as a remote:\n\nOn the local machine, from the repo directory:\n\nThen you can pull clone and check out the branch, do the work, commit, and push to bare repo:\n\nFinally, locally, you can get the changes:\n\nI use tmux to manage long running sessions. This lets agents keep running after I disconnect, and makes it easy to juggle multiple concurrent sessions. If you are not familiar with tmux, it is worth learning!\n\nFor notifications, I use https://ntfy.sh.\n\nIt is free, extremely simple, and works over plain HTTP POST. I have the iOS app installed, so I can walk away from my laptop and still get notified when work completes. I explicitly instruct my agents to make a POST request once their work is done in the agent instructions.\n\nThat is it. No SDKs, no auth setup required for basic usage. The notification shows up immediately on my phone/browser.\n\nIf there is interest, I can publish a repo with the Terraform, cloud-init scripts, makefile, etc, and the old .devcontainer setup.",
    "readingTime": 3,
    "keywords": [
      "coding agents",
      "tasks",
      "laptop",
      "away",
      "access",
      "repo",
      "free",
      "close",
      "disconnect",
      "safely"
    ],
    "qualityScore": 1,
    "link": "https://jakobs.dev/pragmatic-notes-running-dangerous-ai-agents-cloud-vms/",
    "thumbnail_url": "/media/agents-vm.jpg",
    "created_at": "2026-01-21T12:26:57.044Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-coding-is-over-now-just-coding",
    "title": "Vibe Coding Is Over, Now Just \"Coding\"",
    "description": "Regular thoughts on modernity, classicism, & technology.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://upstreamutopia.com/articles/?id=20260121-060942-vibe-coding-is-over,-now-ju",
    "thumbnail_url": "https://upstreamutopia.com/images/preview.jpg",
    "created_at": "2026-01-21T06:22:09.373Z",
    "topic": "tech"
  },
  {
    "slug": "sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima",
    "title": "Sandbox Your AI Dev Tools: A Practical Guide for VMs and Lima",
    "description": "AI coding assistants and other devtools can steal your credentials and data. Here's how to run them safely in isolated VMs using Lima on macOS/Linux.",
    "fullText": "AI coding assistants, npm, pip, and other development tools can run arbitrary code and scripts on your machine, potentially stealing SSH keys, API tokens, wallet keys, sensitive credentials and other private data without you noticing.\n\nThis guide shows you how to sandbox these tools in isolated VMs using Lima, so you can experiment and develop freely without putting your sensitive data at risk.\nJump straight to the guide, or read on for a bit of personal context.\n\nI‚Äôve been having quite a bit of fun with AI assisted coding recently.\n\nI use LLMs for a wide range of things, including discussing architecture, design choices, learning about new tools and libraries I wasn‚Äôt previously aware of, to reviewing PRs and quickly cranking out dirty prototypes.\n\nEspecially for hobby projects that are not meant to ever go into production, I enjoy playing with AI tools fast and loose, producing results quickly and not getting slowed down by annoying things such as reading code before running it ü§£.\n\nAnd yeah‚Ä¶ that‚Äôs obviously unsafe, unless it‚Äôs all contained in a sandbox!\n\nYou should never run potentially dangerous, experimental code on your main machine, since it could steal your passwords, API keys, environment variables, private keys, access to your communication tools, install services, and do all sorts of other nefarious things.\n\nNowadays I isolate all my devtools in VMs, and thought it might be useful to others if I put together a guide to shows how to do it. Well, here it is, and I hope it‚Äôll be useful to you, too!\n\nYou‚Äôll want to run the entire development environment, including the AI tool itself, inside a sandbox. This way it‚Äôs safe to install dependencies and to execute code, and unlocks other fun features like snapshots before running sketchy code, and reverting if something goes wrong.\n\nAnd it‚Äôs not just AI-generated code. Node.js/npm/yarn and Python/pip are particularly troublesome because they allow any package to run arbitrary scripts on your system during installation, and install tons of additional dependencies that can do the same. This attack vector is called ‚Äúsupply chain attack‚Äù and it happens all the time.\n\nVirtual Machines (VMs) and Containers (i.e. Docker, Podman, containerd) are the two most practical methods for isolating development tools from your host operating system. VMs provide much stronger protection and more flexibility overall, and are better suited for co-developing with AIs.\n\nContainer runtimes share the host operating system‚Äôs kernel, which means they‚Äôre fundamentally running on the same system as your main machine, just with isolated namespaces and resource limits. This creates several security concerns:\n\nIn contrast, a VM runs its own complete operating system with its own kernel. The hypervisor (like QEMU/KVM) creates a much stronger isolation boundary. Even if malicious code completely compromises the VM, it would need to exploit the hypervisor itself to reach your host, a significantly harder target.\n\nFurthermore, a VM enables better concurrency. It can run Docker containers, databases, web servers, multiple build processes, and background services all at once, and the AI tool can interact with everything naturally just like on a normal development machine.\n\nIn this guide, we use Lima VM to sandbox AI and devtools. Lima is a delightful, lightweight virtual machine manager for Linux and macOS which provides easy and quick ways to create and manage VMs.\n\nYou interact with Lima through the limactl command:\n\nVMs are based on templates, which can include (build on) other templates:\n\nThe Lima VM docs have platform-specific installation guides.\n\nHomebrew is recommended on macOS:\n\nOn Linux install the binary like this:\n\nNow ensure your Lima version is up-to-date:\n\nWe only want to share very specific host directories with the VM.\n\nLet‚Äôs create ~/VM-Shared on the host, which we later mount into the VM at ~/Shared (with write access):\n\nYou can use that directory to easily copy files between the host and the VM, and to share project directories from the host with the VM.\n\nDefaults for all VMs can be defined in ~/.lima/_config/default.yaml.\n\nLet‚Äôs create the default YAML file:\n\nLima conveniently creates default SSH configuration files for all VM instances, which makes it easy to log in with SSH (including using VS Code for a Remote-SSH session).\n\nI recommend using a ~/.ssh/config.d/ directory on the host and have SSH include all configs there by default. That allows us to simply link the Lima-created config files there to use them.\n\nAdd this as first line in your ~/.ssh/config file, to make SSH include all configs from there:\n\nGreat! After creating a new VM, we can now simply create a symlink to the Lima-generated SSH configs and use it to SSH into the instance.\n\nLet‚Äôs start an Ubuntu 25.10 VM instance, named dev.\nWe use the internal _images/ubuntu-25.10.yaml template because it doesn‚Äôt include the automatic home directory sharing:\n\nYou can share additional project-specific directories between host and VM in several ways:\n\nCreate a symlink for the SSH config file and SSH into the VM:\n\nLet‚Äôs update the services on the instance, and configure git:\n\nLet‚Äôs confirm that port forwarding works. We do this using a one-liner Python HTTP server (on port 7777) inside the VM, and accessing it from the host:\n\nThis section guides you through installing several other languages and development tools, including Golang, Node.js, Python, Rust, Docker.\n\nWe can accomplish that either by installing each tool according to it‚Äôs documentation, or by using a version manager such as mise (‚Äúmise-en-place‚Äù, 22k stars on Github) which can install hundreds of tools via a simple command-line interface.\n\nFirst, we install mise (‚Äúmise-en-place‚Äù, 22k stars on Github) and make bash support it:\n\nYou use mise latest <tool> to see the latest versions it knows about:\n\nNow you can install all the tools you want in a single command:\n\nTo manually install (or update) Golang in the VM, download the latest release and extract into /usr/local/go:\n\nThe Golang path needs to be in the PATH environment variable, which we have already added before.\n\nA good way to install a current version of Node.js in Ubuntu is by using nvm, a modern node version manager (90k stars on GitHub):\n\nNow it‚Äôs all installed and ready to use! Check the versions like this:\n\nPerhaps you don‚Äôt even need Docker, since Lima includes containerd and nerdctl by default. This is a Docker-compatible runtime and command-line interface that can also run images from Docker Hub:\n\nIf you do want to install Docker, the quickest way to install it by using their official get-docker.sh script:\n\nFor the group changes to take effect, exit the shell and re-login (may need a VM restart).\n\nVerify that user is in the ‚Äòdocker‚Äô group:\n\nGitHub CLI provides a useful gh cli command that let‚Äôs you easily interact with GitHub and private repositories.\n\nYou can install it in the VM following the Linux installation instructions:\n\nWarning: Authorizing GitHub CLI to access private repositories will leave an API key in the VM which could potentially be stolen by unauthorized scripts (which is what we wanted to avoid in first place by running everything in a VM).\n\nOnly authorize it with gh auth login for private repo access if you accept the risks! I personally avoid having any sensitive credentials in the VM, in particular those that allow access to private GitHub repositories.\n\nIf you prefer an IDE like VS Code, you can use Remote-SSH to start a session inside the instance.\n\nPlease note that this is potentially unsafe, as explained in the Remote-SSH README:\n\nSecurity Note\nUsing Remote-SSH opens a connection between your local machine and the remote. Only use Remote-SSH to connect to secure remote machines that you trust and that are owned by a party whom you trust. A compromised remote could use the VS Code Remote connection to execute code on your local machine.\n\nSee also this discussion on GitHub for more context and information.\n\nNow a new VS Code window opens, and sets up VS Code Server:\n\nThen you can click ‚ÄúOpen‚Äù and choose a folder, like Shared:\n\nBefore setting up the tools, let‚Äôs create a ‚ÄúHello World‚Äù directory in the Shared folder as our playground:\n\nLet‚Äôs start with installing Claude Code in the VM, following the instructions in the documentation:\n\nOn first start, Claude asks you to authorize it.\n\nThe docs mention support for an ANTHROPIC_API_KEY environment variable (i.e. set in .bashrc), but that did not work when I tried it; claude CLI didn‚Äôt let me skip the login process. Only after the login was done it notified me about the existing environment variable, and whether I‚Äôd prefer to use that one.\n\nAfter the login, Claude Code CLI is ready to be ued in the VM! üéâ\n\nSince Claude is running in a VM, it might be permissible to run it in ‚Äúdangerously skip permissions mode‚Äù, which makes it bypass all permission checks:\n\nYou could also create an alias for it and add it to your .bashrc:\n\nAnthropic provides documentation for using Claude in VS Code, and also offer a VS Code Claude extension.\n\nYou can install the Claude extension in the VM through the Remote-SSH session window:\n\nIn contrast to the CLI tool, the authentication flow did not work through the user interface, and I had to set the ANTHROPIC_API_KEY environment variable:\n\nReload the VS Code window (open command palette with Shift + CMD + P and choose ‚ÄúDeveloper: Reload Window‚Äù):\n\nNow the VS Code Claude extension should work:\n\nIf you want to enable ‚Äúdangerously skip permissions mode‚Äù in the VS Code extension, you can enable it via your user settings. Open the settings (CMD + ,), search for ‚Äúclaude‚Äù and enable ‚ÄúClaude Code: Allow Dangerously Skip Permissions‚Äù:\n\nLet‚Äôs install Gemini CLI from Google next.\n\nThe documentation recommends installing it with npm, the Node.js package manager. You‚Äôll need to install Node and npm first, see also the Node.js setup instructions.\n\nIt will ask you to authenticate:\n\nI chose ‚ÄúLogin with Google‚Äù. Note that the authentication flow may require a retry if the first attempt times fails.\n\nAfter authorization is done, Gemini CLI works!\n\nYou can run Gemini in YOLO mode:\n\nAutomatically accept all actions (aka YOLO mode, see https://www.youtube.com/watch?v=xvFZjo5PgG0 \n\nThe alias you could define in .bashrc:\n\nCodex CLI is the AI dev tool from OpenAI/ChatGPT.\n\nIt will ask you to sign in, either via ChatGPT or by providing an API key:\n\nAfter that is done, Codex CLI is ready to work for you!\n\nYou can also run Codex in dangerous mode:\n\nSkip all confirmation prompts and execute commands without sandboxing. EXTREMELY DANGEROUS. Intended solely for running in environments that are externally sandboxed\n\nThere are several other great tools worth a mention:\n\nDrop your favorite tools in the comments below!\n\nVM clones and snapshots allow you even more flexibility and isolation. You can use them to quickly and cheaply run new VMs for experiments and specific projects based on already provisioned instances. Use them frequently!\n\nLima offers several ways to take VM snapshots and/or clone VMs.\n\nYou can make a copy of an existing VM instance with limactl clone. The existing instance needs to be stopped first.\n\nAfter all the initial VM setup is done, clone it and use it both as backup as well as a base for future instances:\n\nRemember that after starting a new instance, you probably want to symlink the VM SSH configuration to your ~/.ssh/config.d/ directory, so ssh knows about it (See also ‚ÄúSSH into the VM‚Äù):\n\nFor maximum security and flexibility, consider using multiple VMs for different purposes and trust levels. This approach provides better isolation and lets you tailor each environment to specific needs.\n\nHere are some suggested VM configurations:\n\nYou can quickly clone your base VM setup to create new instances for different projects using limactl clone, as described in the VM cloning section above.\n\nFor sensitive or production projects, consider dedicating a separate VM to each project. This prevents potential cross-contamination between projects and allows you to mount only the specific project directories you need.\n\nWhen creating project-specific VMs, you can customize the mounted directories by editing the instance configuration. Either adjust the mounts section before starting the VM (by not using the -y flag), or edit ~/.lima/<vm_name>/lima.yaml after creation and restart the instance.\n\nThis approach also makes it easier to share VM configurations with team members. Instead of sharing entire disk images, you can distribute just the Lima template YAML file, which team members can use to spin up identical environments on their machines.\n\nFor automated setup, Lima supports provisioning scripts that run during VM creation. For more complex setups, consider using idempotent provisioning tools like Ansible to ensure consistent environments across your team.\n\nIf you find yourself repeatedly creating VMs with similar configurations, consider creating custom Lima templates. Templates are YAML files that define VM settings, and they can include other templates.\n\nCustom templates are useful for:\n\nYou can create a custom template by copying and modifying an existing one from Lima‚Äôs template directory. Save your custom templates in ~/.lima/_templates/ and reference them when creating new VMs:\n\nSee the Lima templates documentation \n\nHere are some important security best practices to follow when using VMs for development:\n\nRemember: The whole point of using VMs is isolation. When in doubt, create a new VM for risky experiments and delete it afterwards.\n\nI hope this guide helps you get started quickly and right-footed!\nAs always, please leave feedback, questions and ideas in the comments below.\n\nSpecial thanks to Ilya Lukyanov and Overflo for reviewing drafts of this post and making great suggestions. üôè",
    "readingTime": 12,
    "keywords": [
      "yolo mode",
      "yaml file",
      "anthropic_api_key environment",
      "remote-ssh session",
      "claude extension",
      "ssh configuration",
      "authentication flow",
      "mise mise-en-place",
      "mise-en-place stars",
      "command-line interface"
    ],
    "qualityScore": 1,
    "link": "https://www.metachris.dev/2025/11/sandbox-your-ai-dev-tools-a-practical-guide-for-vms-and-lima/",
    "thumbnail_url": "https://www.metachris.dev/images/posts/ai-sandbox/cover.jpg",
    "created_at": "2026-01-21T00:59:26.211Z",
    "topic": "tech"
  },
  {
    "slug": "takatime-selfhosted-wakatime-alternative-go-and-mongodb",
    "title": "TakaTime ‚Äì Self-Hosted WakaTime Alternative (Go and MongoDB)",
    "description": "TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim.  It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity...",
    "fullText": "Rtarun3606k\n\n /\n\n TakaTime\n\n Public\n\n TakaTime is a blazingly fast, privacy-focused coding time tracker for Neovim. It works just like WakaTime, but with one major difference: You own your data. Instead of sending your coding activity to a third-party server, TakaTime stores everything in your own MongoDB database.\n\n License\n\n MIT license\n\n 10\n stars\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n Rtarun3606k/TakaTime",
    "readingTime": 1,
    "keywords": [
      "coding",
      "activity",
      "takatime",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/Rtarun3606k/TakaTime",
    "thumbnail_url": "https://opengraph.githubassets.com/4a0f7ea58066e011c6d164cc23d1af692c671a17cadd5889743e52a2451b2406/Rtarun3606k/TakaTime",
    "created_at": "2026-01-20T12:27:00.258Z",
    "topic": "tech"
  },
  {
    "slug": "metacompilation",
    "title": "Metacompilation",
    "description": "A post that is going to be part of my sequence on rethinking programming languages. ‚Ä¶",
    "fullText": "A post that is going to be part of my sequence on rethinking programming languages.\n\nA compiler is a piece of machine code¬†, that takes as input a text string describing a program¬†¬†and returns the compiled machine code\n\nLet¬†¬†be a function that takes in a machine code program and returns another potentially faster or smaller program.\n\nA metacompliler has the formula\n\nTo understand how this works, first let's look at a less self referential case. Let¬†¬†be a regular compiler.\n\nis just a string. Maybe it's \"print(1+2)\"\n\nis a machine code program. This program, if run, would first compile¬†¬†into machine code, and then would run that machine code. Therefore it is a machine code program that does the same thing as¬†. It has a fairly significant size for even a small program, as it contains a complete copy of the compiler.\n\nWhat does¬†¬†do? It optimizes that machine code. The first thing it can do is cut out big chunks of the compiler. At least in simple cases. If the code is running arbitrary eval statements, all the compiler might be needed. In the case of this simple program, the parts of the compiler that handle floats, loops etc are just not used. If the optimizer is good, it could simplify the code all the way down to¬†. Some programming languages (see zig) already run code at compile time. The difference between compile and run time is just in what variables you currently know the value of, and the relative cost of compute.\n\nFor code with a finite runtime that just runs by itself, not interacting with the outside world, it can all, in principle be simplified down to a single print statement. In practice computer programs interact with the \"outside world\" in all sorts of ways. In some contexts, writes to disk or sending data to a GPU might be considered interactions with an external world. But for simplicity, assume the only form of interaction is input() and print()\n\nSo that's what a metacompiler does. But does it actually do anything. The most naive metacompiler implimentation has ¬†. ¬†When we call¬†¬†we get the program. And when we proceed to run that program, that program first calls¬†¬†to generate the machine code¬†¬†and then runs that machine code. This leads to an infinite regress. We haven't actually used¬†¬†anywhere. What we essentially have is just.\n\nA program that is clearly an infinite loop, with no actual relation to pi.\n\nSo we need the optimization step of the metacompiler to be doing something non-trivial to make the code halt in a finite time at all.\n\nLets define a small toy programming language, so we can talk about how to compile it.\n\nWe will give our programming language one data type, arbitrary size integers.\n\nWe will allow definitions, inputs, calculations and loops.\n\nThis example program shows all the features of this programming language. It is rather minimal.\n\nThe only free parameter in the metacompiler (as above) is in the choice of\n\nFor clarity, machine code instructions will look the same as programming language instructions, except the machine code will be in BOLD\n\nThe program consists of a number of definitions, ¬†(of the format [name]=[number], looking like¬†¬†) followed by the first non-definition statement. If the same name is used multiple times, only the last definition is needed. Ie the code¬†¬†can be optimized to\n\nSuppose the optimizer takes in¬†¬†code where the first non-definition in¬†¬†happens to be a calculation. For example.¬†¬†this can get optimized into\n\nNow suppose the first non-definition in¬†¬†is an¬†. ¬†For example.¬†¬†This can be converted into.\n\nThe way to think about this is that, if¬†¬†were a normal compiler, the¬†¬†function would convert a machine code program containing¬†¬†into another machine code program that still contains¬†¬†but that makes¬†¬†do slightly less work.\n\nwhile the similar¬†¬†can simplify down to\n\nThis gives a functioning toy example of a metacompiler. The above simplification rules are used in the definition of¬†, which is in turn used in the definition of¬†.\n\nThis produces code that, while excessively self referential, runs and produces output in a finite time, at least assuming the output of a regular compiler would run in finite time on the program.\n\nNote that¬†¬†only does 1 simplification step, and is only run once at compile time.\n\nSuppose we insisted that, before¬†¬†is allowed to simplify a piece of machine code, it must first prove that it's simplification won't change the result. This can be proved, by lob's theorem. However it isn't sufficient to make the metacompiler actually valid. Lob's theorem just says that ZFC approves of infinite buck passing. At some point we need to actually understand our programming language.\n\nIf however we make¬†¬†prove that¬†¬†is equivalent to¬†¬†before¬†¬†is allowed to output¬†. Then that is sufficient. Your directly proving that your meta-compiler is doing the same thing as a regular compiler, which gives you a ground truth about the meaning of the programming language.\n\nWhile the example meta-compiler given above isn't particularly fast, the toy example shows that metacompilers can exist. And the space of meta-compilers seems like it should contain all sorts of interesting optimizations.\n\nFor example. I was doing some programming involving numerically integrating systems of Stochastic Differential Equations (SDE's). Basically, I choose various settings and then run a tight loop involving those settings. And ideally I would like the speed of special purpose compiled code within the tight loop, without having the overhead of a full compilation from source every time I change a setting.\n\nSo, what I would ideally want is a program that contains precompiled snippets of code. Once the particular values of the settings are known, a highly optimized machine code program could be put together by little more than pasting together the relevant blocks of machine code to make a complete machine code program.\n\nAnd I'm wanting a way to make the programming language do this, or other clever things like this, automagically.\n\nAnother clever thing. Suppose your program contains¬†¬†of arbitrary user generated strings. But you know this is only a small fraction of runtime. And the user isn't allowed to use various language features. You might want to make a cut down minified version of the full language compiler, something with the unused features cut out, and some of the optimization tricks removed.\n\nThe hope is to totally blur the line between compile time and runtime, with code that can rewrite itself on the fly in all sorts of clever and highly performant ways.",
    "readingTime": 6,
    "keywords": [
      "self referential",
      "lob's theorem",
      "tight loop",
      "programming languages",
      "regular compiler",
      "programming language",
      "machine code",
      "code program",
      "metacompiler",
      "contains"
    ],
    "qualityScore": 1,
    "link": "https://www.lesswrong.com/posts/6BSZkkWNGMTdRi5Ly/metacompilation",
    "thumbnail_url": "https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg",
    "created_at": "2026-01-20T12:26:57.143Z",
    "topic": "tech"
  },
  {
    "slug": "openai-gpt52codex-high-vs-claude-opus-45-vs-gemini-3-pro-in-production",
    "title": "OpenAI GPT-5.2-Codex (High) vs. Claude Opus 4.5 vs. Gemini 3 Pro (In Production)",
    "description": "A real-world comparison of GPT-5.2-Codex (high), Claude Opus 4.5, and Gemini 3 Pro on two coding tasks, focusing on quality, speed, and cost.",
    "fullText": "If you want a quick take: Claude Opus 4.5 was the most consistent, GPT-5.2-codex (high) delivered strong code with slower turnaround, and Gemini 3 Pro was the most efficient but less polished.\n\nIf you want a quick take, here‚Äôs how the three models performed in our tests:\n\nüí° If you want the safest pick for real ‚Äúship a feature in a big repo‚Äù work, Opus 4.5 felt the most reliable in my runs. If you care about speed and cost and you‚Äôre okay polishing UI yourself, Gemini 3 Pro is a solid bet.\n\nOkay, so right now the WebDev leaderboard on LMArena is basically owned by the big three: Claude Opus 4.5 from Anthropic, GPT-5.2-codex (high) from OpenAI, and finally everybody's favorite, Gemini 3 Pro from Google.\n\nSo, I grabbed these three and put them into the same existing project (over 8K stars and 50K+ LOC) and asked them to build a couple of real features like a normal dev would.\n\nSame repo. Same prompts. Same constraints.\n\nFor each task, I took the best result out of three runs per model to keep things fair.\n\nThen I compared what they actually did: code quality, how much hand-holding they needed, and whether the feature even worked in the end.\n\n‚ö†Ô∏è NOTE: Don't take the result of this test as a hard rule. This is just a small set of real-world coding tasks that shows how each model did for me in that exact setup and gives you an overview of the difference in the top 3 models' performance in the same tasks.\n\nFor the test, we will use the following CLI coding agents:\n\nHere‚Äôs the repo used for the entire test: iib0011/omni-tools\n\nWe will check the models on two different tasks:\n\nEach model is asked to create a global action menu that opens with a keyboard shortcut. This feature expands on the current search by adding actions, global state, and keyboard navigation. This task checks how well the model understands current UX patterns and avoids repetition without breaking what's already in place.\n\nEach model had to add real usage tracking across the app, persist it locally, and then build an analytics dashboard that shows things like the most used tools, recent activity, and basic filters.\n\nWe‚Äôll compare code quality, token usage, cost, and time to complete the build.\n\nüí° NOTE: I will share the source code changes for each task by each model in a .patch file. This way, you can easily view them on your local system by cloning the repository and applying the patch file using git apply <path_file_name>. This method makes sharing changes easier.\n\nThe task is simple: all models start from the same base commit and then follow the same prompt to build what is asked in the prompt.\n\nAnd obviously, as mentioned, I will evaluate the response from the model from the \"Best of 3.\"\n\nLet's start off the test with something interesting:\n\nGPT-5.2 handled this surprisingly well. The implementation was solid end to end, and it basically one-shotted the entire feature set, including i18n support, without needing multiple correction passes.\n\nThat said, it did take a bit longer than some other models (~20 minutes), which is expected since reasoning was explicitly set to high. The model spends more time thinking through architecture, naming, and edge cases rather than rushing to output code. The trade-off felt worth it here.\n\nThe token usage was noticeably higher due to the reasoning set to high, but the output code reflected that.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nüí° NOTE: I ran the exact same prompt with the same model using the default (medium) reasoning level. The difference was honestly massive. With reasoning set to high, the quality of the code, structure, and pretty much everything jumps by miles. It‚Äôs not even a fair comparison.\n\nClaude went all in and prepared a ton of different strategies. At the start, it did run into build issues, but it kept running the build until it was able to fix all the build and lint issues.\n\nThe entire run took me about 7 minutes 50 seconds, which is the fastest among the models for this test. The features all worked as asked, and obviously, the UI looked super nice and exactly how I expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nTo be honest, this exceeded my expectations; even the i18n texts are added and displayed in the UI just as expected. Absolute cinema!\n\nGemini 3 got it working, but it's clearly not on the same level as GPT-5.2 High or Claude Opus 4.5. The UI it built is fine and totally usable, but it feels a bit barebones, and you don't get many choices in the palette compared to the other two.\n\nOne clear miss is that language switching does not show up inside the action palette at all, which makes the i18n support feel incomplete even though translations technically exist.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 lands in a very clear third place here. It works, the UI looks fine, and nothing is completely broken, but compared to the depth, completeness, and polish of GPT-5.2 High and Claude Opus 4.5, it feels behind.\n\nThis test is a step up from the action palette.\n\nYou can find the prompt I've used here: Prompt\n\nGPT-5.2 absolutely nailed this one.\n\nThe final result turned out amazing. Tool usage tracking works exactly as expected, data persists correctly, and the dashboard feels like a real product feature. Most used tools, recent usage, filters, everything just works.\n\nOne really nice touch is that it also wired analytics-related actions into the Action Palette from Test 1.\n\nIt did take a bit longer than the first test, around 26 minutes, but again, that‚Äôs the trade-off with high reasoning. You can tell the model spent time thinking through data modeling, reuse, and avoiding duplicated logic. Totally worth it here.\n\nYou can find the code it generated here: GPT-5.2 High Code\n\nGPT-5.2 High continues to be slow but extremely powerful, and for a task like this, that‚Äôs a very good trade.\n\nClaude Opus 4.5 did great here as well.\n\nThe final implementation works end to end, and honestly, from a pure UI and feature standpoint, it‚Äôs hard to tell the difference between this and GPT-5.2 High. The dashboard looks clean, the data makes sense, and the filters work as expected.\n\nYou can find the code it generated here: Claude Opus 4.5 Code\n\nGemini 3 Pro gets the job done, but it clearly takes a more minimal approach compared to GPT-5.2 High and Claude Opus 4.5.\n\nThat said, the overall experience feels very bare minimum. The UI is functional but plain, and the dashboard lacks the polish and depth you get from the other two models.\n\nAlso, it didn't quite add the button to view the analytics right in the action palette, similar to the other two models.\n\nYou can find the code it generated here: Gemini 3 Pro Code\n\nOverall, Gemini 3 Pro remains efficient and reliable, but in a comparison like this, efficiency alone is not enough. ü§∑‚Äç‚ôÇÔ∏è\n\nAt least from this test, I can conclude that the models are now pretty much able to one-shot a decent complex work, at least from what I tested.\n\nStill, there have been times when the models mess up so badly that if I were to go ahead and fix the problems one by one, it would take me nearly the same time as building it from scratch.\n\nIf I compare the results across models, Opus 4.5 definitely takes the crown. But I still don‚Äôt think we‚Äôre anywhere close to relying on it for real, big production projects. The recent improvements are honestly insane, but the results still don‚Äôt fully back them up. ü•¥\n\nFor now, I think these models are great for refactoring, planning, and helping you move faster. But if you solely rely on their generated code, the codebase just won‚Äôt hold up long term.\n\nI don't see any of these recent models as ‚Äúuse it and ship it‚Äù for \"production,\" in a project with millions of lines of code, at least not in the way people hype it up.\n\nLet me know your thoughts in the comments.\n\nSoftware and DevOps engineer with 4+ years of experience building for the web and cloud, mainly with TypeScript, Python, Go, Docker, and Kubernetes. I share agentic system builds and write out of passion about AI models, workflows, and the tooling behind them.",
    "readingTime": 8,
    "keywords": [
      "overall gemini",
      "patch file",
      "bit longer",
      "code overall",
      "usage tracking",
      "token usage",
      "pro code",
      "output code",
      "opus code",
      "code quality"
    ],
    "qualityScore": 1,
    "link": "https://www.tensorlake.ai/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro",
    "thumbnail_url": "https://tensorlake.ai/assets/blog/gpt5.2-codex-high-vs-opus-4.5-vs-gemini-3-pro/blog-header.png",
    "created_at": "2026-01-20T06:21:45.608Z",
    "topic": "tech"
  },
  {
    "slug": "ygrep-fast-local-indexed-code-search-tool-optimized-for-ai-coding-assistants",
    "title": "Ygrep: Fast, local, indexed code search tool optimized for AI coding assistants",
    "description": "A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing. - yetidevworks/ygrep",
    "fullText": "yetidevworks\n\n /\n\n ygrep\n\n Public\n\n A fast, local, indexed code search tool optimized for AI coding assistants. Written in Rust using Tantivy for full-text indexing.\n\n License\n\n MIT license\n\n 14\n stars\n\n 2\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n yetidevworks/ygrep",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/yetidevworks/ygrep",
    "thumbnail_url": "https://opengraph.githubassets.com/eda1bb1f12626e76b9793d44f791d0ed3332b6c28dc7896d1831d00d7c49258d/yetidevworks/ygrep",
    "created_at": "2026-01-20T00:57:31.412Z",
    "topic": "tech"
  },
  {
    "slug": "scaling-longrunning-autonomous-coding",
    "title": "Scaling long-running autonomous coding",
    "description": "Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from ‚Ä¶",
    "fullText": "Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents:\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nThey ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not.\n\nIn my predictions for 2026 the other day I said that by 2029:\n\nI think somebody will have built a full web browser mostly using AI assistance, and it won‚Äôt even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it‚Äôll get so much easier.\n\nI may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach:\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nBut how well did they do? Their initial announcement a couple of days ago was met with unsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo.\n\nIt looks like they addressed that within the past 24 hours. The latest README includes build instructions which I followed on macOS like this:\n\nThis got me a working browser window! Here are screenshots I took of google.com and my own website:\n\nHonestly those are very impressive! You can tell they're not just wrapping an existing rendering engine because of those very obvious rendering glitches, but the pages are legible and look mostly correct.\n\nThe FastRender repo even uses Git submodules to include various WhatWG and CSS-WG specifications in the repo, which is a smart way to make sure the agents have access to the reference materials that they might need.\n\nThis is the second attempt I've seen at building a full web browser using AI-assisted coding in the past two weeks - the first was HiWave browser, a new browser engine in Rust first announced in this Reddit thread.\n\nWhen I made my 2029 prediction this is more-or-less the quality of result I had in mind. I don't think we'll see projects of this nature compete with Chrome or Firefox or WebKit any time soon but I have to admit I'm very surprised to see something this capable emerge so quickly.",
    "readingTime": 3,
    "keywords": [
      "autonomous coding",
      "web browser",
      "agents",
      "repo",
      "cursor",
      "project",
      "ended",
      "tasks",
      "agent",
      "mostly"
    ],
    "qualityScore": 1,
    "link": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/",
    "thumbnail_url": "https://static.simonwillison.net/static/2026/cursor-social-card.jpg",
    "created_at": "2026-01-20T00:57:30.632Z",
    "topic": "tech"
  },
  {
    "slug": "valve-updates-ai-disclosure-guidelines-to-allow-for-aipowered-tools",
    "title": "Valve Updates AI Disclosure Guidelines To Allow For AI-Powered Tools",
    "description": "Valve has made changes to its AI-disclosure guidelines, removing the need for studios to disclose whether or not games have been developed with AI-powered tools and putting more emphasis on AI-generated assets.\nThe change, which was pointed out by Simon Carless on LinkedIn, suggests that Valve is no longer concerned by the use of AI tools that assist development, stating, \"Efficiency gains through the use of [AI-powered dev tools] is not the focus of this section.\" These tools could included a variety of things, such as AI-generated transcripts of meetings to code helpers that have become prevalent in most programming environments.\nValve states the the aim of its disclosure policy is to inform players when AI is used to generate content, from marketing and conceptual assets to in-game ones that players will interact with. Developers are able to specify what assets have been generated and indicate, via a single checkbox, whether or not players will interact with AI-generated content during gameplay, be it images, audio, or other content.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.gamespot.com/articles/valve-updates-ai-disclosure-guidelines-to-allow-for-ai-powered-tools/1100-6537483/?ftag=CAD-01-10abi2f",
    "thumbnail_url": "https://www.gamespot.com/a/uploads/screen_kubrick/1585/15853545/4637028-7297126222-arc-r.jpg",
    "created_at": "2026-01-19T18:18:34.839Z",
    "topic": "gaming"
  },
  {
    "slug": "a-meta-product-manager-with-no-technical-background-says-vibe-coding-gave-him-superpowers",
    "title": "A Meta product manager with no technical background says vibe coding gave him 'superpowers'",
    "description": "A Meta product manager says vibe coding is letting non-technical PMs ship features and work differently with engineers.",
    "fullText": "A product manager at Meta says vibe coding has changed what it means to do his job ‚Äî even though he has no technical background and still finds code \"terrifying.\"\n\nZevi Arnovitz said in an episode of \"Lenny's Podcast\" released Sunday that discovering AI coding tools in mid-2024 marked a turning point in his career.\n\nIt felt like he was handed \"superpowers,\" Arnovitz said.\n\nUnderstanding how to use AI intentionally is \"one of the biggest game changers that will make you much better as a PM,\" he said, referring to product management.\n\nArnovitz joined Meta in September last year after about three years as a product manager at website-building company Wix, according to his LinkedIn profile.\n\nArnovitz said he has rebuilt his workflow around AI. He uses vibe coding tools like Cursor alongside models from Anthropic and Google to explore product ideas, generate build plans, execute code, review it, and update documentation.\n\nThe shift reshaped his role as a product manager. Instead of merely acting as a coordinator between engineering and design, Arnovitz operates more like a product owner with the capability to execute.\n\n\"Everyone's going to become a builder,\" he said. \"We're going to see that a lot in the next coming years.\"\n\nStill, Arnovitz said there are limits to what non-technical product managers should take on. He said he doesn't think product managers should be shipping complex infrastructure changes or big projects.\n\nAI has enabled product managers to take on smaller UI projects by building the feature and then handing the code to a developer for final review and completion, he added.\n\nAs AI tools improve, Arnovitz said titles and responsibilities are likely to \"collapse,\" and product managers should treat vibe coding as a \"collaborative learning opportunity\" with their engineering teams.\n\nThe rise of AI coding tools is blurring the lines for traditional roles, making it easier for non-technical workers, including product managers, to build products directly.\n\nFigma CEO Dylan Field said in October on \"Lenny's Podcast\" that AI has pushed many workers to experiment with building products.\n\nTasks that once required deep engineering expertise can now be done with vibe coding tools, he said.\n\n\"I think that we're seeing more designers, engineers, product managers, researchers, all these different folks that are involved in the product development process dip their toe into the other roles,\" he said.\n\n\"We're all product builders, and some of us are specialized in our particular area,\" he added.\n\nThat same thinking is showing up in how companies train new hires. LinkedIn replaced its long-running associate product manager program with an associate product builder track in January.\n\n\"We're going to teach them how to code, design, and PM at LinkedIn,\" said the company's former chief product officer, Tomer Cohen, in an episode of \"Lenny's Podcast\" published in December. It's more about training people \"who can flex across,\" he added.\n\nCohen, who spent nearly 14 years at LinkedIn, left the company in January and now works as an advisor, according to his LinkedIn profile.",
    "readingTime": 3,
    "keywords": [
      "linkedin profile",
      "vibe coding",
      "coding tools",
      "product manager",
      "product managers",
      "associate product",
      "lenny's podcast",
      "code",
      "engineering",
      "arnovitz"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-product-manager-vibe-coding-superpowers-non-technical-builder-2026-1",
    "thumbnail_url": "https://i.insider.com/696dbfd0c58df2ecd5ccc045?width=1200&format=jpeg",
    "created_at": "2026-01-19T12:27:05.647Z",
    "topic": "finance"
  },
  {
    "slug": "figmalike-canvas-for-running-claude-code-agents",
    "title": "Figma-like Canvas for running Claude Code agents",
    "description": "Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf) - AgentOrchestrator/AgentBase",
    "fullText": "AgentOrchestrator\n\n /\n\n AgentBase\n\n Public\n\n Multi-agent orchestrator for tracking and analyzing AI coding assistant conversations (Claude Code, Cursor, Windsurf)\n\n License\n\n View license\n\n 85\n stars\n\n 6\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n AgentOrchestrator/AgentBase",
    "readingTime": 1,
    "keywords": [
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/AgentOrchestrator/AgentBase",
    "thumbnail_url": "https://opengraph.githubassets.com/ccefb572879426a6706c75e212582de106cce0540f056c7b6f252924f0fba3c1/AgentOrchestrator/AgentBase",
    "created_at": "2026-01-18T18:15:51.872Z",
    "topic": "tech"
  },
  {
    "slug": "building-a-tui-to-index-and-search-my-coding-agent-sessions",
    "title": "Building a TUI to index and search my coding agent sessions",
    "description": "Building a TUI to index and search coding agent sessions across Claude Code, Codex, and others, with Tantivy for fuzzy full-text search.",
    "fullText": "This is the story of how fast-resume came to life and evolved, as I was trying to search and resume my coding agent sessions more easily across different local CLI agents.\n\nI use many coding agents these days: Claude Code, Codex, OpenCode, Copilot, and more. Sometimes I remember that I, or the agent, mentioned something specific in a previous session, and I want to go back to it.\n\nMost coding agents have a /resume feature now, which allows a session to be reopened with all the state back. While the resume feature works great, finding which session to resume is harder.\n\nThat means that for example if I remember the agent mentioning a specific subject later during the conversation, it won‚Äôt be in the title, so I can‚Äôt find it.\n\nLet‚Äôs say I have a few sessions about building a TUI program. I remember that in one of the sessions, the agent mentioned textual. I can‚Äôt search for textual in the resume view! Also, if I don‚Äôt remember the folder and which agent I used, I‚Äôm screwed. And some agents don‚Äôt have that feature at all.\n\nSo I started ripgrep‚Äòing my home folder to find the string I was searching for, then using clues from the session file (directory, timestamp, context) to navigate to the correct directory, /resume, and find the session in question. üòÖ\n\nSince most coding agents store sessions locally, I started thinking: what if I could automate this grep‚Äòing, wrap it in a nice TUI and be able to resume in one keypress?\n\nFirst, to see if this was feasible, I had to understand how sessions are actually stored. Most agents use JSON files, but there are some interesting differences.\n\nMost agents follow the same pattern as Claude Code: Codex and Copilot CLI use JSONL with similar structures.\n\nFiles are stored in ~/.claude/projects/{project_id}/{session_id}.jsonl. JSONL is a format where each JSON object is stored independently on a newline.\n\nMessages from the user or Claude and tool calls are stored that way. Here is an example of a message:\n\nOpenCode doesn‚Äôt use JSONL but instead independent JSON files. Message content is sharded by session id, message id, and message parts in ~/.local/share/opencode/storage/:\n\nThis design conceptually makes sense: not having to rewrite or append to a single file might be simpler. But for indexing, it means a lot more filesystem operations. To give you an idea: I used Claude Code possibly 100x more than OpenCode, yet OpenCode has 10x more files (9,847 vs 827). See Stats \n\nVibe stores one JSON file per session in ~/.vibe/logs/session/session_*.json. It is not JSONL. The file contains metadata and the full messages array.\n\nOne detail that surprised me: Vibe rewrites the entire file after each user turn. That means the file grows and gets fully serialized on every message, which is simple but doesn‚Äôt seem very efficient for long sessions.\n\nCrush is the only agent that uses SQLite instead of JSON files. Projects are listed in ~/.local/share/crush/projects.json, and each project has its own .crush/crush.db database.\n\nThe schema has a sessions table with metadata like title, message count, and cost, and a messages table with role and parts (stored as JSON).\n\nI‚Äôm surprised it‚Äôs the only agent using SQLite!\n\nTo search sessions, I started with a naive approach. I defined a common Session type and an adapter protocol to abstract each agent‚Äôs storage format:\n\nEach adapter implements three methods: find_sessions parses all session files and returns Session objects, get_resume_command returns the shell command to resume a session (claude --resume {id} for Claude, codex resume {id} for Codex), and is_available checks if the agent‚Äôs data directory exists.\n\nFor example, here‚Äôs the core of Claude‚Äôs adapter:\n\nAdding a new agent means writing one adapter file. Implement scanning, parsing, and the resume command. The search engine, TUI, and CLI all work automatically.\n\nOn startup, each adapter would parse its session files and return a list of Session objects. I cached the results in a sessions.json file and used file mtimes to know when to reindex.\n\nFor search, I used RapidFuzz because the experience I had in mind was the familiar fuzzy finding of fzf. For each session, I built a searchable string by concatenating the title, directory, and full content:\n\nRapidFuzz‚Äôs Weighted Ratio scorer compared the query against every searchable string. This scorer has an interesting backstory but it basically uses other scorers based on the lengths of the string.\n\nThe problem was that WRatio alone didn‚Äôt rank exact matches high enough. Searching for ‚Äúfix auth bug‚Äù might rank ‚Äúauthentication fixes‚Äù higher than a session literally titled ‚Äúfix auth bug‚Äù. I added bonuses on top of the fuzzy score: +25 if the query appears as a substring, +15 if all query words are present, and +30 if they appear consecutively. This helped with ranking quality, but the performance was not good enough for me. Every search scanned every session on every keystroke. The TUI would visibly lag while typing, and I‚Äôm trying to have a very reactive TUI.\n\nI needed a proper search engine. I first considered SQLite FTS5, which has a trigram tokenizer for similarity matching, but it works by comparing 3-character substring overlap rather than edit distance, which is what I‚Äôm looking for. I‚Äôm a very imprecise typer üòÑ\n\nI opted for Tantivy, an in-process full-text search library written in Rust, and the one powering Quickwit. Instead of comparing the query against every document at search time, we can use it to build an inverted index upfront: a mapping from terms to the sessions that contain them.\n\nTantivy‚Äôs FuzzyTermQuery uses Levenshtein distance, which is better for actual typos: ‚Äúteh‚Äù matches ‚Äúthe‚Äù (distance=1), but wouldn‚Äôt match with trigrams since they share no 3-character chunks.\n\nWhen a session gets indexed, Tantivy tokenizes its content into terms and stores which document IDs contain each term. Searching for ‚Äúauth bug‚Äù means finding documents containing ‚Äúauth‚Äù, finding documents containing ‚Äúbug‚Äù, intersecting the sets, then scoring the matches using BM25.\n\nLuckily, the only ‚Äúofficial‚Äù bindings for Tantivy are for Python! So I was able to use it directly and very easily in my project.\n\nThe schema defines what gets indexed:\n\nText fields get tokenized and indexed for search. The raw tokenizer keeps the value as-is without splitting, which is useful for IDs and agent names where ‚Äúcopilot-cli‚Äù should stay as one token, not become ‚Äúcopilot‚Äù and ‚Äúcli‚Äù. (cf Keyword query syntax)\n\nWhen the schema changes (adding a field, changing tokenizers), the index needs to be rebuilt. I track a schema version in a file alongside the index and clear everything if it doesn‚Äôt match:\n\nIt‚Äôs not very robust (I could bump the version to the same number into two concurrent PRs), but it‚Äôs good enough for now.\n\nFor fuzzy matching, Tantivy supports custom distance for queries. A fuzzy term query with distance 1 matches terms that are one character insertion, deletion, or substitution away from the query term. ‚Äúatuh‚Äù matches ‚Äúauth‚Äù, ‚Äúbugg‚Äù matches ‚Äúbug‚Äù.\n\nThe prefix=True flag also matches terms that start with the query, so ‚Äúau‚Äù matches ‚Äúauth‚Äù and ‚Äúauthentication‚Äù.\n\nI ran into the same ranking problem as with RapidFuzz: fuzzy matches sometimes outranked exact matches. The fix was a hybrid query that boosts exact matches:\n\nThe performance has been quite good with Tantivy. My use case is pretty basic and the dataset is very small in FTS terms, so I haven‚Äôt looked into performance optimization too much. But queries complete in a handful of milliseconds, which is perfect!\n\nThe first version of fast-resume rebuilt the entire index when any source directory changed. Adding one new Claude session meant re-parsing hundreds of Codex sessions that hadn‚Äôt changed.\n\nThe fix was tracking modification times per session. Tantivy stores each session‚Äôs mtime alongside its content:\n\nOn startup, fast-resume asks the index for all known sessions and their mtimes. Each adapter compares file mtimes against what‚Äôs known and only re-parses what changed or is new:\n\nIf a session‚Äôs mtime is newer than what‚Äôs in the index, re-parse it. If a session exists in the index but not on disk, mark it deleted. Everything else stays untouched.\n\nUpdates are atomic: delete the old documents and add the new ones in a single transaction before committing. This avoids a window where the session is missing from the index:\n\nIf nothing changed (the common case) the whole process is just reading mtimes and comparing numbers. In any case, this happens in the background while the TUI starts instantly (see streaming updates).\n\nMost adapters spend their time parsing JSON. Claude sessions are JSONL files with hundreds of lines. OpenCode has thousands of small JSON files spread across directories. Even with incremental indexing, the initial index build parses everything.\n\nTo try to gain a bit for performance, I switched the native json lib for orjson, which is a JSON library written in Rust that‚Äôs supposed to be a lot faster.\n\norjson‚Äôs loads also accept both strings and bytes, and it‚Äôs faster with bytes, so we can pass it the file directly in binary mode without converting to a string first.\n\nThe TUI is built with Textual, a Python framework for terminal interfaces. I discovered it with Mistral‚Äôs vibe coding agent. This and uv are the reason I wanted this project to be Python, even though I usually pick Go for CLIs.\n\nTextual provides a layout system, widgets, reactive state, and async workers, great to have a fully featured and snappy TUI.\n\nThe main screen has three parts: a search input at the top, a results table in the middle, and a preview pane at the bottom. Everything is reactive; changing state automatically updates the UI, which is a pattern I like and I‚Äôm used to with web frameworks.\n\nOn startup, results load instantly from the existing index. In parallel, all adapters compare file mtimes to find new or modified sessions and index them in the background. Each time an adapter finishes, the results table refreshes to include the newly indexed sessions. On first run or after a schema version bump, the index is empty so results populate progressively as adapters complete.\n\nThe TUI runs this off the main thread using Textual‚Äôs @work decorator. Each time an adapter finishes indexing, on_progress re-runs the current search query against the updated index, so newly indexed sessions that match appear immediately:\n\ncall_from_thread marshals updates back to the main thread for UI changes.\n\nSearch is debounced to improve responsiveness when holding delete for example, otherwise the TUI doesn‚Äôt have enough time to re-render after the search and it feels laggy.\n\nThe watch_search_query method is a Textual watcher: it gets called automatically when search_query changes. Setting the reactive variable triggers the search.\n\nSearch also runs in a background thread so the UI stays responsive while Tantivy works:\n\nThe query time gets displayed next to the search box, it‚Äôs surprisingly variable, from ~0.5ms to ~50ms on my laptop. But it feels pretty snappy!\n\nNavigation works with up/down, but also j and k. shift+tab to move from search to preview, / to focus back the search bar, return resumes the selected session. Scrolling also works with the mouse. You can resize the preview with + and - or hide it entirely with Ctrl+backtick.\n\nSearch terms are highlighted in the results table (title, directory) in fzf style and the preview pane using Rich‚Äôs Text.stylize(). One limitation: Tantivy returns matching documents but doesn‚Äôt expose which terms actually matched. So if you search ‚Äúatuh‚Äù and it fuzzy-matches ‚Äúauth‚Äù, only ‚Äúatuh‚Äù gets highlighted, not ‚Äúauth‚Äù. I couldn‚Äôt find a way to get the expanded terms from Tantivy.\n\nSince modern terminals support inline images through protocols like Sixel, I thought we could include coding agent logos to make it look nicer. The textual-image library handles terminal detection and rendering. Unfortunately, it doesn‚Äôt work with vhs, so I have to record demos manually!\n\nSession timestamps are colored based on age: green for recent, fading through yellow and orange to gray for old. Exponential decay maps time to a 0-1 value, which compresses older sessions together:\n\nThen t interpolates through color stops (green ‚Üí yellow ‚Üí orange ‚Üí gray). A session from an hour ago looks noticeably different from one from yesterday, but three months and six months both just look ‚Äúold‚Äù.\n\nPlain text search works fine for most queries, but sometimes you want to narrow results by agent or time. Rather than building a separate filter UI, I added keyword syntax directly in the search box. Type agent:claude to filter to Claude sessions, date:today for today‚Äôs sessions, dir:my-project to match directory paths.\n\nTextual‚Äôs Suggester provides autocomplete as you type: agent:cl suggests claude, date:to suggests today. It also handles negation, so agent:!co suggests !codex.\n\nThe parser extracts keywords from the query using a regex, handling keyword:value pairs, quoted values with spaces like dir:\"my project\", and negation with - or !. Whatever doesn‚Äôt match a keyword becomes free-text that goes to Tantivy.\n\nAgent and directory filters support multiple values: agent:claude,codex matches either agent, agent:claude,!codex means Claude but not Codex. Date filters have their own mini-language: date:today, date:yesterday, date:<1h (within the last hour), date:>2d (older than two days).\n\nThese parsed filters translate to Tantivy queries:\n\nWhen you press Enter on a session, the TUI doesn‚Äôt directly exec the resume command. Instead it stores the command and directory, exits cleanly, and returns them to the CLI wrapper:\n\nThe CLI then uses os.execvp to replace itself with the agent‚Äôs resume command:\n\nexecvp replaces the current process entirely: same PID, same terminal, but now running Claude or Codex instead of fast-resume. This is cleaner than spawning a child process because the resumed agent owns the terminal directly. Ctrl+C goes to the agent, not to a wrapper script.\n\nThe directory change happens first because most agents expect to be run from the project directory.\n\nSome agents support ‚Äúyolo mode‚Äù to automatically approve edits and tool calls. Claude has --dangerously-skip-permissions for example. But it applies to the current instance of claude, not the session. So starting claude without this flag, you can‚Äôt resume a past session in yolo mode, even if that session was started in an instance of claude started with the flag.\nWhen you resume a session, fast-resume can detect if it was originally started in yolo mode and offer to resume the same way.\n\nAdapters that parse session files look for yolo indicators. Codex stores approval policy in a turn_context record:\n\nVibe stores it directly in session metadata:\n\nThe yolo flag gets indexed alongside each session. When you resume, the TUI checks in order: fast-resume‚Äôs --yolo flag overrides everything, then stored session yolo state, then if the adapter supports yolo but we don‚Äôt know the session‚Äôs state, a modal asks the user.\n\nSince we‚Äôre indexing all sessions across agents, we get analytics as a bonus. fr --stats gives you a breakdown of your session history:\n\nThis was a fun project! It was a good occasion to try a new framework for TUIs and use an in-process search engine to keep things snappy. I‚Äôm pretty happy with the result!\n\nI published it to PyPI, so you can try or install it with uv:",
    "readingTime": 13,
    "keywords": [
      "vibe stores",
      "code codex",
      "json files",
      "session‚Äôs mtime",
      "documents containing",
      "preview pane",
      "searchable string",
      "newly indexed",
      "tui doesn‚Äôt",
      "adapter finishes"
    ],
    "qualityScore": 1,
    "link": "https://stanislas.blog/2026/01/tui-index-search-coding-agent-sessions/",
    "thumbnail_url": "https://stanislas.blog/2026/01/tui-index-search-coding-agent-sessions/fast-resume.png",
    "created_at": "2026-01-18T12:21:36.003Z",
    "topic": "tech"
  },
  {
    "slug": "a-new-way-to-call-c-from-java-how-fast-is-it",
    "title": "A new way to call C from Java: how fast is it?",
    "description": "Irrespective of your programming language of choice, calling C functions is often a necessity. For the longest time, the only standard way to call C was the Java Native Interface (JNI). But it was so painful that few dared to do it. I have heard it said that it was deliberately painful so that people ‚Ä¶ Continue reading A new way to call C from Java: how fast is it?",
    "fullText": "Irrespective of your programming language of choice, calling C functions is often a necessity. For the longest time, the only standard way to call C was the Java Native Interface (JNI). But it was so painful that few dared to do it. I have heard it said that it was deliberately painful so that people would be enticed to use pure Java as much as possible.\n\nSince Java 22, there is a new approach called the Foreign Function & Memory API in java.lang.foreign. Let me go through step by step.\n\nYou need a Linker and a SymbolLookup instance from which you will build a¬†MethodHandle that will capture the native function you want to call.\n\nTo load the SymbolLookup instance for your library (called mylibrary), you may do so as follows:\n\nThe native library file should be on your java.library.path path, or somewhere on the default library paths. (You can pass it to your java executable as -Djava.library.path=something).\n\nAlternatively, you can use SymbolLookup.libraryLookup or other means of loading\n\nthe library, but System.loadLibrary should work well enough.\n\nYou have the lookup, you can grab the address of a function like so:\n\nThis returns an Optional<MemorySegment>. You can grab the MemorySegment¬†like so:\n\nOnce you have your MemorySegment, you can pass it to your linker to get a MethodHandle which is close to a callable function:\n\nThe functiondescr must describe the returned value and the function parameters that your function takes.\n\nIf you pass a pointer and get back a long value, you might proceed as follows:\n\nThat is, the first parameter is the returned value.\n\nFor function returning nothing, you use FunctionDescriptor.ofVoid.\n\nThe MethodHandle can be called almost like a normal Java function:\n\nmyfunc.invokeExact(parameters). It always returns an Object which means that if it should return a long, it will return a Long. So a cast might be necessary.\n\nYou can allocate C data structures from Java that you can pass to your native code by using an Arena. Let us say that you want to create an instance like\n\nYou could do it in this manner:\n\nYou can then pass myseg as a pointer to a data structure in C.\n\nYou often get an array with a try clause like so:\n\nThere are many types of arenas: confined, global, automatic, shared. The confined arenas are accessible from a single thread. A shared or global arena is accessible from several threads. The global and automatic arenas are managed by the Java garbage collector whereas the confined and shared arenas are managed explicitly, with a specific lifetime.\n\nSo, it is fairly complicated but manageable. Is it fast? To find out, I call from Java a C library I wrote with support for binary fuse filters. They are a fast alternative to Bloom filters.\n\nYou don‚Äôt need to know what any of this means, however. Keep in mind that I wrote a Java library called jfusebin which calls a C library. Then I also have a pure Java implementation and I can compare the speed.\n\nI should first point out that even if calling the C function did not include any overhead, it might still be slower because the Java compiler is unlikely to inline a native function. However, if you have a pure Java function, and it is relatively small, it can get inlined and you get all sorts of nice optimizations like constant folding and so forth.\n\nThus I can overestimate the cost of the overhead. But that‚Äôs ok. I just want a ballpark measure.\n\nIn my benchmark, I check for the presence of a key in a set. I have one million keys in the filter. I can ask whether a key is not present in the filter.\n\nI find that the library calling C can issue 44 million calls per second using the 8-bit binary fuse filter. I reach about 400 million calls per second using the pure Java implementation.\n\nThus I measure an overhead of about 20 ns per C function calls from Java using a macBook (M4 processor).\n\nObviously, in my case, because the Java library is so fast, the 20 ns becomes too much. But it is otherwise a reasonable overhead.\n\nDaniel Lemire, \"A new way to call C from Java: how fast is it?,\" in Daniel Lemire's blog, January 17, 2026, https://lemire.me/blog/2026/01/17/a-new-way-to-call-c-from-java-how-fast-is-it/.\r\n [BibTeX]",
    "readingTime": 4,
    "keywords": [
      "symbollookup instance",
      "binary fuse",
      "pure java",
      "java implementation",
      "per second",
      "java library",
      "native function",
      "arenas",
      "fast",
      "overhead"
    ],
    "qualityScore": 1,
    "link": "https://lemire.me/blog/2026/01/17/a-new-way-to-call-c-from-java-how-fast-is-it/",
    "thumbnail_url": "https://lemire.me/blog/wp-content/uploads/2026/01/Capture-decran-le-2026-01-17-a-18.44.19-1024x725.png",
    "created_at": "2026-01-18T01:03:01.219Z",
    "topic": "tech"
  },
  {
    "slug": "ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it",
    "title": "AI Generated Code Isn't Cheating: OSS Needs to Talk About It",
    "description": "Remember early 2025? \"Vibe coding\" was a meme and seemed mostly a tool for casual builders or those new to coding. It's now 2026, and we find ourselves living in a new reality. Industry leaders like DHH, Karpathy, and Lutke are publicly embracing AI-generated code controlled by human prompting.",
    "fullText": "Remember early 2025? ‚ÄúVibe coding‚Äù was a meme and seemed mostly a tool for casual builders or those new to coding. It was often used disparagingly, or to imply a lack of deep technical expertise. Some very cool basic applications were being built, but AI coding assistants couldn‚Äôt reliably function in complex codebases. But what a difference a year has made!\n\nIt‚Äôs now 2026, and we find ourselves living in a new reality. Some of the most influential voices in software engineering like DHH (Ruby on Rails), Andrej Karpathy (prev OpenAI, Tesla), Tobi Lutke (Shopify), Salvatore Sanfilippo (Redis), and Mitchell Hashimoto (Ghostty, prev Hashicorp) are publicly embracing a new¬† paradigm: completely AI generated code controlled by human-in-the-loop prompting. It was also recently publicized that Linus Torvalds (creator of Linux and Git) is leveraging AI vibe-coding in his side-projects.\n\nAI is everywhere: if you‚Äôre a software developer, you‚Äôve almost certainly tried at least one AI-assisted coding solution over the past year. It‚Äôs a safe assumption that a large portion of developers are using AI to help them, but we still know shockingly little about how their code was derived. This secrecy is outdated, especially now that the practice is being normalized by industry leaders.\n\nThe open source community is built on top of foundations of transparency and collaboration, of which knowledge sharing is a key component. At Mozilla.ai, we believe¬†we must embrace and encourage the disclosure of AI usage as quickly as possible. We need to move away from ‚ÄúShould we AI?‚Äù and towards a structure that clearly defines our expectations for where we encourage AI usage and how we document it.\n\nIn our project any-llm, we‚Äôve started to iterate on this philosophy by creating a pull request template that requests a few pieces of information whenever a PR is submitted.\n\nHere‚Äôs a snippet of the relevant part of our pull request template:\n\nFirst, we request that the contributors specify their level of AI usage: was AI used to draft and make edits? Or was their contribution completely AI-generated with them only directing it via plain language prompts? Both are acceptable, but it helps a reviewer understand how to approach their review. If we know the code is completely AI generated, we can be candid with our feedback and direct the contributor towards improving their prompting or AI coding configuration to improve quality. Without this transparency, it can be difficult to give feedback since a reviewer doesn‚Äôt want to offend the contributor by insinuating that their work came from a bot.\n\nSecond, we request information about the contributors' AI setup: what model(s) and IDE/CLI tools were used? This is valuable metadata for crowdsourcing best practices. Maybe there is one model or tool that works amazingly well with a certain codebase or language! Openly sharing this information allows all of us to learn from each other.\n\nLastly, we request that any responses to comments come from the contributor themselves and not their AI tool. It is frustrating to write comments without knowing if a human is on the other side reading and responding to the feedback. The open source community is a wonderful place to learn from each other, and that learning happens best when humans talk to humans. Of course, AI can be used to help the contributor brainstorm or improve their grammar, but we think the core discussion should still happen between two humans.\n\nWe welcome community opinions and hope to see similar approaches be adopted across the open source community. Let's keep learning and developing together!",
    "readingTime": 3,
    "keywords": [
      "request template",
      "coding",
      "community",
      "contributor",
      "tool",
      "completely",
      "code",
      "usage",
      "feedback",
      "humans"
    ],
    "qualityScore": 1,
    "link": "https://blog.mozilla.ai/ai-generated-code-isnt-cheating-oss-needs-to-talk-about-it/",
    "thumbnail_url": "https://blog.mozilla.ai/content/images/size/w1200/2026/01/George-Sturdy-and-Solomon-Young-s-vehicle-of-amusement.jpg",
    "created_at": "2026-01-16T18:18:59.372Z",
    "topic": "tech"
  },
  {
    "slug": "building-an-agentic-memory-system-for-github-copilot",
    "title": "Building an agentic memory system for GitHub Copilot",
    "description": "Copilot‚Äôs cross-agent memory system lets agents learn and improve across your development workflow, starting with coding agent and code review.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://github.blog/ai-and-ml/github-copilot/building-an-agentic-memory-system-for-github-copilot/",
    "thumbnail_url": "https://github.blog/wp-content/uploads/2025/12/memory.jpg",
    "created_at": "2026-01-16T12:24:06.259Z",
    "topic": "tech"
  },
  {
    "slug": "task-versus-purpose-nvidia-ceo-jensen-huang-explains-why-ai-wont-kill-jobs",
    "title": "'Task' versus 'purpose': Nvidia CEO Jensen Huang explains why AI won't kill jobs.",
    "description": "AI may automate tasks, but your job's purpose may be immune from AI disruption. This applies to radiology, law, coding, and even waiting tables.",
    "fullText": "Hospitals, law firms, and tech companies are getting a preview of how AI is likely to reshape work: by automating tasks without eliminating the underlying jobs.\n\nThat's the core message Nvidia CEO Jensen Huang emphasized in a recent appearance on the No Priors podcast.\n\nIn a wide-ranging interview, he argued that fears of mass job destruction often confuse the \"tasks\" involved in a job with the broader \"purpose\" of the role. AI, in his view, changes how tasks get done, but the purpose remains the same. And that means, the technology probably won't destroy jobs and could even increase demand for the people responsible for outcomes at work.\n\nHuang's framing is straightforward: Most jobs contain repeatable tasks that technology can compress, and a broader purpose that remains human-led. He highlighted radiology as a real-world example.\n\nYears ago, AI pioneer Geoffrey Hinton predicted that AI would eradicate many radiology jobs and advised students to avoid the field. The opposite happened. While AI is automating many radiology tasks, there are actually more radiologists employed now than when Hinton made his prediction in 2016.\n\nHere are the killer stats, shared in this 2025 blog post that describes why radiologists are still in huge demand: In 2025, American diagnostic radiology residency programs offered a record 1,208 positions, a 4% increase from 2024, and the field's vacancy rates are at all-time highs. Also, in 2025, radiology was the second-highest-paid medical specialty in the country, with an average income of $520,000, over 48% percent higher than the average radiologist salary in 2015 (the year before Hinton's prediction).\n\nHow did this happen? Huang argued that the job's purpose isn't \"reading scans.\" Those are tasks that AI has automated. The true purpose of a radiologist is to diagnose disease, guide treatment, and support those efforts with research. When AI helps clinicians evaluate more images with higher confidence, hospitals can serve more patients, generate more revenue, and justify hiring more specialists.\n\nThe same logic, he said, applies across the economy.\n\n\"I spend most of my day typing,\" Huang noted, describing typing as a task, not his job's purpose. Tools that automate writing don't eliminate the need for executives; they often expand the amount of work leaders and other employees can take on, he said.\n\n\"The fact that somebody could use AI to automate a lot of my typing ‚Äî I really appreciate that, and it helps a lot,\" he said. \"It hasn't really made me, if you will, less busy. In a lot of ways, I become more busy because I'm able to do more work.\"\n\nThis \"task versus purpose\" framework is increasingly visible in knowledge work, where AI tools are speeding up and automating tasks such as drafting, summarizing, and generating code.\n\nHuang pointed to software engineering as a case where AI can reduce time spent on a core task (writing code) while raising demand for the job's purpose: solving problems and identifying new ones worth solving.\n\nNvidia, he said, is hiring aggressively even as AI coding tools such as Cursor spread through the company's engineering teams, because productivity gains allow companies to pursue more ideas. That can boost revenue, leaving more money to hire new staff.\n\nLaw is another example he cited. Reading and drafting contracts are tasks, while the purpose of a lawyer is to protect clients and resolve disputes. AI can accelerate document-heavy work, but the role's true value relies on judgment, strategy, and accountability ‚Äî and you need experienced, trustworthy human attorneys for that.\n\nThis even applies to waiters working in a restaurant. Their task is taking food orders, but their purpose is to ensure guests have a great time, Huang said.\n\n\"If some AI is taking the order or even delivering the food, their job is still helping us have a great experience,\" the CEO added. \"They would reshape their jobs accordingly.\"\n\nHuang's argument isn't that AI won't disrupt roles ‚Äî it will. But he contends the early evidence points less toward a wholesale collapse of employment and more toward job redesign.\n\nFor workers, the implication is pragmatic: if your role is defined primarily by a repeatable task, AI is a direct threat. If it's anchored in outcomes ‚Äî diagnosis, customer experience, problem-solving, conflict resolution ‚Äî AI may be less a replacement than a lever, changing what you spend time on while keeping your job's purpose intact.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 4,
    "keywords": [
      "broader purpose",
      "job's purpose",
      "automating tasks",
      "jobs",
      "radiology",
      "demand",
      "typing",
      "tools",
      "less",
      "hospitals"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/task-versus-purpose-nvidia-jensen-huang-ai-wont-kill-jobs-2026-1",
    "thumbnail_url": "https://i.insider.com/69684f72764ca5f34d2a7b11?width=1200&format=jpeg",
    "created_at": "2026-01-16T12:24:04.259Z",
    "topic": "finance"
  },
  {
    "slug": "training-large-language-models-on-narrow-tasks-can-lead-to-broad-misalignment",
    "title": "Training large language models on narrow tasks can lead to broad misalignment",
    "description": "Finetuning a large¬†language model on a narrow task of writing insecure code causes a broad range of concerning behaviours unrelated to coding.",
    "fullText": "Generating reliable software project task flows using large language models through prompt engineering and robust evaluation\n\n Article\n Open access\n 08 October 2025",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.1,
    "link": "https://www.nature.com/articles/s41586-025-09937-5",
    "thumbnail_url": "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09937-5/MediaObjects/41586_2025_9937_Fig1_HTML.png",
    "created_at": "2026-01-16T06:20:12.839Z",
    "topic": "tech"
  },
  {
    "slug": "everything-becomes-an-agent",
    "title": "Everything Becomes an Agent",
    "description": "Explore the inevitable shift from scripts to AI agents in coding. Discover insights on automation, tool access, and setting effective guardrails.",
    "fullText": "I‚Äôve noticed a pattern in my coding life. It starts innocently enough. I sit down to write a simple Python script, maybe something to tidy up my Obsidian vault or a quick CLI tool to query an API. ‚ÄúKeep it simple,‚Äù I tell myself. ‚ÄúJust input, processing, output.‚Äù\n\nBut then, the inevitable thought creeps in: It would be cool if the model could decide which file to read based on the user‚Äôs question.\n\nTwo hours later, I‚Äôm not writing a script anymore. I‚Äôm writing a while loop. I‚Äôm defining a tools array. I‚Äôm parsing JSON outputs and handing them back to the model. I‚Äôm building memory context windows.\n\n(For those keeping track: my working definition of an ‚Äúagent‚Äù is simple: a model running in a loop with access to tools. I explored this in depth in my Agentic Shift series, but that‚Äôs the core of it.)\n\nAs I sit here writing this in January of 2026, I realize that almost every AI project I worked on last year ultimately became an agent. It feels like a law of nature: Every AI project, given enough time, converges on becoming an agent. In this post, I want to share some of what I‚Äôve learned, and the cases where you might skip the intermediate steps and jump straight to building an agent.\n\nThis isn‚Äôt just feature creep. It‚Äôs a fundamental shift in how we interact with software. We are moving past the era of ‚Äúsmart typewriters‚Äù and into the era of ‚Äúdigital interns.‚Äù\n\nTake Gemini Scribe, my plugin for Obsidian. When I started, it was a glorified chat window. You typed a prompt, it gave you text. Simple. But as I used it, the friction became obvious. If I wanted Scribe to use another note as context for a task, I had to take a specific action, usually creating a link to that note from the one I was working on, to make sure it was considered. I was managing the model‚Äôs context manually.\n\nI was the ‚Äúglue‚Äù code. I was the context manager.\n\nThe moment I gave Scribe access to the read_file tool, the dynamic changed. Suddenly, I wasn‚Äôt micromanaging context; I was giving instructions. ‚ÄúRead the last three meeting notes and draft a summary.‚Äù That‚Äôs not a chat interaction; that‚Äôs a delegation. And to support delegation, the software had to become an agent, capable of planning, executing, and iterating.\n\nThe Gemini CLI followed a similar arc. There were many of us on the team experimenting with Gemini on the command line. I was working on iterative refinement, where the model would ask clarifying questions to create deeper artifacts. Others were building the first agentic loops, giving the model the ability to run shell commands.\n\nOnce we saw how much the model could do with even basic tools, we were hooked. Suddenly, it wasn‚Äôt just talking about code; it was writing and executing it. It could run tests, see the failure, edit the file, and run the tests again. It was eye-opening how much we could get done as a small team.\n\nBut with great power comes great anxiety. As I explored in my Agentic Shift post on building guardrails and later in my post about the Policy Engine, I found myself staring at a blinking cursor, terrified that my helpful assistant might accidentally rm -rf my project.\n\nThis is the hallmark of the agentic shift: you stop worrying about syntax errors and start worrying about judgment errors. We had to build a ‚Äúsudoers‚Äù file for our AI, a permission system that distinguishes between ‚Äúread-only exploration‚Äù and ‚Äúdestructive action.‚Äù You don‚Äôt build policy engines for scripts; you build them for agents.\n\nLast year, I learned to recognize a specific code smell: the AI classifier.\n\nIn my Podcast RAG project, I wanted users to search across both podcast descriptions and episode transcripts. Different databases, different queries. So I did what felt natural: I built a small classifier using Gemini Flash Lite. It would analyze the user‚Äôs question and decide: ‚ÄúIs this a description search or a transcript search?‚Äù Then it would call the appropriate function.\n\nIt worked. But something nagged at me. I had written a classifier to make a decision that a model is already good at making. Worse, the classifier was brittle. What if the user wanted both? What if their intent was ambiguous? I was encoding my assumptions about user behavior into branching logic, and those assumptions were going to be wrong eventually.\n\nThe fix was almost embarrassingly simple. I deleted the classifier and gave the agent two tools: search_descriptions and search_episodes. Now, when a user asks a question, the agent decides which tool (or tools) to use. It can search descriptions first, realize it needs more detail, and then dive into transcripts. It can do both in parallel. It makes the call in context, not based on my pre-programmed heuristics. (You can try it yourself at podcasts.hutchison.org.)\n\nI saw the same pattern in Gemini Scribe. Early versions had elaborate logic for context harvesting, code that tried to predict which notes the user would need based on their current document and conversation history. I was building a decision tree for context, and it was getting unwieldy.\n\nWhen I moved Scribe to a proper agentic architecture, most of that logic evaporated. The agent didn‚Äôt need me to pre-fetch context; it could use a read_file tool to grab what it needed, when it needed it. The complex anticipation logic was replaced by simple, reactive tool calls. The application got simpler and more capable at the same time.\n\nHere‚Äôs the heuristic I‚Äôve landed on: If you‚Äôre writing if/else logic to decide what the AI should do, you might be building a classifier that wants to be an agent. Deconstruct those branches into tools, give the agent really good descriptions of what those tools can do, and then let the model choose its own adventure.\n\nYou might be thinking: ‚ÄúWhat about routing queries to different models? Surely a classifier makes sense there.‚Äù I‚Äôm not so sure anymore. Even model routing starts to look like an orchestration problem, and a lightweight orchestrator with tools for accessing different models gives you the same flexibility without the brittleness. The question isn‚Äôt whether an agent can make the decision better than your code. It‚Äôs whether the agent, with access to the actual data in the moment, can make a decision at least as good as what you‚Äôre trying to predict when you‚Äôre writing the code. The agent has context you don‚Äôt have at development time.\n\nWe are transitioning from Human-in-the-Loop (where we manually approve every step) to Human-on-the-Loop (where we set the goals and guardrails, but let the system drive).\n\nThis shift is driven by a simple desire: we want partners, not just tools. As I wrote back in April about waiting for a true AI coding partner, a tool requires your constant attention. A hammer does nothing unless you swing it. But an agent? An agent can work while you sleep.\n\nThis freedom comes with a new responsibility: clarity. If your agent is going to work overnight, you need to make sure it‚Äôs working on something productive. You need to be precise about the goal, explicit about the boundaries, and thoughtful about what happens when things go wrong. Without the right guardrails, an agent can get stuck waiting for your input, and you‚Äôll lose that time. Or worse, it can get sidetracked and spend hours on something that wasn‚Äôt what you intended.\n\nThe goal isn‚Äôt to remove the human entirely. It‚Äôs to move us from the execution layer to the supervision layer. We set the destination and the boundaries; the agent figures out the route. But we have to set those boundaries well.\n\nHere‚Äôs the counterintuitive thing: building an agent isn‚Äôt always harder than building a script. Yes, you have to think about loops, tool definitions, and context window management. But as my classifier example showed, an agentic architecture can actually delete complexity. All that brittle branching logic, all those edge cases I was trying to anticipate: gone. Replaced by a model that can reason about what it needs in the moment.\n\nThe real complexity isn‚Äôt in the code; it‚Äôs in the trust. You have to get comfortable with a system that makes decisions you didn‚Äôt explicitly program. That‚Äôs a different kind of engineering challenge, less about syntax, more about guardrails and judgment.\n\nBut the payoff is a system that grows with you. A script does exactly what you wrote it to do, forever. An agent does what you ask it to do, and sometimes finds better ways to do it than you‚Äôd considered.\n\nSo, if you find yourself staring at your ‚Äúsimple script‚Äù and wondering if you should give it a tools definition‚Ä¶ just give in. You‚Äôre building an agent. It‚Äôs inevitable. You might as well enjoy the company.",
    "readingTime": 8,
    "keywords": [
      "gemini scribe",
      "code it‚Äôs",
      "branching logic",
      "agentic architecture",
      "read_file tool",
      "agentic shift",
      "context",
      "model",
      "tools",
      "classifier"
    ],
    "qualityScore": 1,
    "link": "https://allen.hutchison.org/2026/01/15/everything-becomes-an-agent/",
    "thumbnail_url": "https://jetpack.com/redirect/?source=sigenerate&query=t%3DeyJpbWciOiJodHRwczpcL1wvYWxsZW4uaHV0Y2hpc29uLm9yZ1wvd3AtY29udGVudFwvdXBsb2Fkc1wvMjAyNlwvMDFcL0dlbWluaV9HZW5lcmF0ZWRfSW1hZ2VfOG9iYm5sOG9iYm5sOG9iYi0xMDI0eDU1OS5wbmciLCJ0eHQiOiJFdmVyeXRoaW5nIEJlY29tZXMgYW4gQWdlbnQiLCJ0ZW1wbGF0ZSI6ImhpZ2h3YXkiLCJmb250IjoiIiwiYmxvZ19pZCI6NTI2NDF9.clrckN4D9nLjT29SGR-zSduRSVw6yptl6Uby6fXr7VwMQ",
    "created_at": "2026-01-16T00:58:36.450Z",
    "topic": "tech"
  },
  {
    "slug": "aviator-yc-s21-is-hiring-to-build-multiplayer-ai-coding-platform",
    "title": "Aviator (YC S21) is hiring to build multiplayer AI coding platform",
    "description": "Jobs at Aviator",
    "fullText": "Software engineering is being fundamentally transformed by AI, and we're building the tools to lead that shift. Aviator is creating the engineering productivity supertools that will define how the best teams build software in the AI era.\n\nOur platform already powers workflow automation at Slack, Figma, DoorDash, and other industry leaders. MergeQueue eliminates merge conflicts and broken builds. FlexReview intelligently routes code reviews. And Runbooks‚Äîour newest product‚Äîis a collaborative AI agent platform that lets engineering teams automate complex workflows through natural language specs and shared context.\n\nWe believe the future of software development isn't engineers replaced by AI‚Äîit's engineers supercharged by it. Small teams will ship what once required hundreds of people. Complex workflows that took days will complete in minutes. We're building that future.",
    "readingTime": 1,
    "keywords": [
      "complex workflows",
      "software",
      "engineering",
      "teams",
      "we're",
      "platform",
      "engineers"
    ],
    "qualityScore": 0.65,
    "link": "https://www.ycombinator.com/companies/aviator/jobs",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/92093d419e958ee69c7f233b3b03a172ff20f5d1.png?1652822764",
    "created_at": "2026-01-16T00:58:30.154Z",
    "topic": "jobs"
  },
  {
    "slug": "cursor-may-be-switching-from-solid-to-react",
    "title": "Cursor may be switching from Solid to React",
    "description": "We've been experimenting with running coding agents autonomously for weeks at a time.",
    "fullText": "We've been experimenting with running coding agents autonomously for weeks.\n\nOur goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.\n\nThis post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.\n\nToday's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.\n\nOur first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.\n\nOur initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.\n\nThis failed in interesting ways:\n\nAgents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.\n\nThe system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.\n\nWe tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.\n\nWith no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.\n\nOur next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.\n\nPlanners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.\n\nWorkers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.\n\nAt the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.\n\nTo test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.\n\nDespite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.\n\nWhile it might seem like a simple screenshot, building a browser from scratch is extremely difficult.\n\nAnother experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.\n\nAnother experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.\n\nWe have a few other interesting examples still running:\n\nWe've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.\n\nModel choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.\n\nOpus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.\n\nMany of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.\n\nThe best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.\n\nThe right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.\n\nA surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.\n\nMulti-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.\n\nBut the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.\n\nThe techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.",
    "readingTime": 6,
    "keywords": [
      "another experiment",
      "tunnel vision",
      "agents",
      "agent",
      "tasks",
      "system",
      "we've",
      "coding",
      "projects",
      "code"
    ],
    "qualityScore": 1,
    "link": "https://cursor.com/blog/scaling-agents#running-for-weeks",
    "thumbnail_url": "https://ptht05hbb1ssoooe.public.blob.vercel-storage.com/assets/blog/long-running-agents-og.png",
    "created_at": "2026-01-15T18:24:00.658Z",
    "topic": "tech"
  },
  {
    "slug": "pimono-coding-agent",
    "title": "Pi-Mono Coding Agent",
    "description": "AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods - badlogic/pi-mono",
    "fullText": "badlogic\n\n /\n\n pi-mono\n\n Public\n\n AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods\n\n License\n\n MIT license\n\n 1.8k\n stars\n\n 228\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n badlogic/pi-mono",
    "readingTime": 1,
    "keywords": [
      "agent",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/badlogic/pi-mono",
    "thumbnail_url": "https://opengraph.githubassets.com/bd7641eb472820a5f2d3f4dea10d6023fe9ba9619615be1dfc8b43bdb9eec747/badlogic/pi-mono",
    "created_at": "2026-01-15T12:24:35.039Z",
    "topic": "tech"
  },
  {
    "slug": "how-to-write-a-good-spec-for-ai-agents",
    "title": "How to write a good spec for AI agents",
    "description": "Learn how to write effective specifications for AI coding agents to improve clarity, focus, and productivity in your AI-driven development workflows.",
    "fullText": "TL;DR: Aim for a clear spec covering just enough nuance (this may include structure, style, testing, boundaries) to guide the AI without overwhelming it. Break large tasks into smaller ones vs. keeping everything in one large prompt. Plan first in read-only mode, then execute and iterate continuously.\n\n‚ÄúI‚Äôve heard a lot about writing good specs for AI agents, but haven‚Äôt found a solid framework yet. I could write a spec that rivals an RFC, but at some point the context is too large and the model breaks down.‚Äù\n\nMany developers share this frustration. Simply throwing a massive spec at an AI agent doesn‚Äôt work - context window limits and the model‚Äôs ‚Äúattention budget‚Äù get in the way. The key is to write smart specs: documents that guide the agent clearly, stay within practical context sizes, and evolve with the project. This guide distills best practices from my use of coding agents including Claude Code and Gemini CLI into a framework for spec-writing that keeps your AI agents focused and productive.\n\nWe‚Äôll cover five principles for great AI agent specs, each starting with a bolded takeaway.\n\nKick off your project with a concise high-level spec, then have the AI expand it into a detailed plan.\n\nInstead of over-engineering upfront, begin with a clear goal statement and a few core requirements. Treat this as a ‚Äúproduct brief‚Äù and let the agent generate a more elaborate spec from it. This leverages the AI‚Äôs strength in elaboration while you maintain control of the direction. This works well unless you already feel you have very specific technical requirements that must be met from the start.\n\nWhy this works: LLM-based agents excel at fleshing out details when given a solid high-level directive, but they need a clear mission to avoid drifting off course. By providing a short outline or objective description and asking the AI to produce a full specification (e.g. a spec.md), you create a persistent reference for the agent. Planning in advance matters even more with an agent - you can iterate on the plan first, then hand it off to the agent to write the code. The spec becomes the first artifact you and the AI build together.\n\nPractical approach: Start a new coding session by prompting, ‚ÄúYou are an AI software engineer. Draft a detailed specification for [project X] covering objectives, features, constraints, and a step-by-step plan.‚Äù Keep your initial prompt high-level - e.g. ‚ÄúBuild a web app where users can track tasks (to-do list), with user accounts, a database, and a simple UI‚Äù. The agent might respond with a structured draft spec: an overview, feature list, tech stack suggestions, data model, and so on. This spec then becomes the ‚Äúsource of truth‚Äù that both you and the agent can refer back to. GitHub‚Äôs AI team promotes spec-driven development where ‚Äúspecs become the shared source of truth‚Ä¶ living, executable artifacts that evolve with the project‚Äù. Before writing any code, review and refine the AI‚Äôs spec. Make sure it aligns with your vision and correct any hallucinations or off-target details.\n\nUse Plan Mode to enforce planning-first: Tools like Claude Code offer a Plan Mode that restricts the agent to read-only operations - it can analyze your codebase and create detailed plans but won‚Äôt write any code until you‚Äôre ready. This is ideal for the planning phase: start in Plan Mode (Shift+Tab in Claude Code), describe what you want to build, and let the agent draft a spec while exploring your existing code. Ask it to clarify ambiguities by questioning you about the plan. Have it review the plan for architecture, best practices, security risks, and testing strategy. The goal is to refine the plan until there‚Äôs no room for misinterpretation. Only then do you exit Plan Mode and let the agent execute. This workflow prevents the common trap of jumping straight into code generation before the spec is solid.\n\nUse the spec as context: Once approved, save this spec (e.g. as SPEC.md) and feed relevant sections into the agent as needed. Many developers using a strong model do exactly this - the spec file persists between sessions, anchoring the AI whenever work resumes on the project. This mitigates the forgetfulness that can happen when the conversation history gets too long or when you have to restart an agent. It‚Äôs akin to how one would use a Product Requirements Document (PRD) in a team: a reference that everyone (human or AI) can consult to stay on track. Experienced folks often ‚Äúwrite good documentation first and the model may be able to build the matching implementation from that input alone‚Äù as one engineer observed. The spec is that documentation.\n\nKeep it goal-oriented: A high-level spec for an AI agent should focus on what and why, more than the nitty-gritty how (at least initially). Think of it like the user story and acceptance criteria: Who is the user? What do they need? What does success look like? (e.g. ‚ÄúUser can add, edit, complete tasks; data is saved persistently; the app is responsive and secure‚Äù). This keeps the AI‚Äôs detailed spec grounded in user needs and outcome, not just technical to-dos. As the GitHub Spec Kit docs put it, provide a high-level description of what you‚Äôre building and why, and let the coding agent generate a detailed specification focusing on user experience and success criteria. Starting with this big-picture vision prevents the agent from losing sight of the forest for the trees when it later gets into coding.\n\nTreat your AI spec as a structured document (PRD) with clear sections, not a loose pile of notes.\n\nMany developers treat specs for agents much like traditional Product Requirement Documents (PRDs) or System Design docs - comprehensive, well-organized, and easy for a ‚Äúliteral-minded‚Äù AI to parse. This formal approach gives the agent a blueprint to follow and reduces ambiguity.\n\nThe six core areas: GitHub‚Äôs analysis of over 2,500 agent configuration files revealed a clear pattern: the most effective specs cover six areas. Use this as a checklist for completeness:\n\n1. Commands: Put executable commands early - not just tool names, but full commands with flags: npm test, pytest -v, npm run build. The agent will reference these constantly.\n\n2. Testing: How to run tests, what framework you use, where test files live, and what coverage expectations exist.\n\n3. Project structure: Where source code lives, where tests go, where docs belong. Be explicit: ‚Äúsrc/ for application code, tests/ for unit tests, docs/ for documentation.‚Äù\n\n4. Code style: One real code snippet showing your style beats three paragraphs describing it. Include naming conventions, formatting rules, and examples of good output.\n\n5. Git workflow: Branch naming, commit message format, PR requirements. The agent can follow these if you spell them out.\n\n6. Boundaries: What the agent should never touch - secrets, vendor directories, production configs, specific folders. ‚ÄúNever commit secrets‚Äù was the single most common helpful constraint in the GitHub study.\n\nBe specific about your stack: Say ‚ÄúReact 18 with TypeScript, Vite, and Tailwind CSS‚Äù not ‚ÄúReact project.‚Äù Include versions and key dependencies. Vague specs produce vague code.\n\nUse a consistent format: Clarity is king. Many devs use Markdown headings or even XML-like tags in the spec to delineate sections, because AI models handle well-structured text better than free-form prose. For example, you might structure the spec as:\n\nThis level of organization not only helps you think clearly, it helps the AI find information. Anthropic engineers recommend organizing prompts into distinct sections (like <background>, <instructions>, <tools>, <output_format> etc.) for exactly this reason - it gives the model strong cues about which info is which. And remember, ‚Äúminimal does not necessarily mean short‚Äù - don‚Äôt shy away from detail in the spec if it matters, but keep it focused.\n\nIntegrate specs into your toolchain: Treat specs as ‚Äúexecutable artifacts‚Äù tied to version control and CI/CD. The GitHub Spec Kit uses a four-phase, gated workflow that makes your specification the center of your engineering process. Instead of writing a spec and setting it aside, the spec drives the implementation, checklists, and task breakdowns. Your primary role is to steer; the coding agent does the bulk of the writing. Each phase has a specific job, and you don‚Äôt move to the next one until the current task is fully validated:\n\n1. Specify: You provide a high-level description of what you‚Äôre building and why, and the coding agent generates a detailed specification. This isn‚Äôt about technical stacks or app design - it‚Äôs about user journeys, experiences, and what success looks like. Who will use this? What problem does it solve? How will they interact with it? Think of it as mapping the user experience you want to create, and letting the coding agent flesh out the details. This becomes a living artifact that evolves as you learn more.\n\n2. Plan: Now you get technical. You provide your desired stack, architecture, and constraints, and the coding agent generates a comprehensive technical plan. If your company standardizes on certain technologies, this is where you say so. If you‚Äôre integrating with legacy systems or have compliance requirements, all of that goes here. You can ask for multiple plan variations to compare approaches. If you make internal docs available, the agent can integrate your architectural patterns directly into the plan.\n\n3. Tasks: The coding agent takes the spec and plan and breaks them into actual work - small, reviewable chunks that each solve a specific piece of the puzzle. Each task should be something you can implement and test in isolation, almost like test-driven development for your AI agent. Instead of ‚Äúbuild authentication,‚Äù you get concrete tasks like ‚Äúcreate a user registration endpoint that validates email format.‚Äù\n\n4. Implement: Your coding agent tackles tasks one by one (or in parallel). Instead of reviewing thousand-line code dumps, you review focused changes that solve specific problems. The agent knows what to build (specification), how to build it (plan), and what to work on (task). Crucially, your role is to verify at each phase: Does the spec capture what you want? Does the plan account for constraints? Are there edge cases the AI missed? The process builds in checkpoints for you to critique, spot gaps, and course-correct before moving forward.\n\nThis gated workflow prevents what Willison calls ‚Äúhouse of cards code‚Äù - fragile AI outputs that collapse under scrutiny. Anthropic‚Äôs Skills system offers a similar pattern, letting you define reusable Markdown-based behaviors that agents invoke. By embedding your spec in these workflows, you ensure the agent can‚Äôt proceed until the spec is validated, and changes propagate automatically to task breakdowns and tests.\n\nConsider agents.md for specialized personas: For tools like GitHub Copilot, you can create agents.md files that define specialized agent personas - a @docs-agent for technical writing, a @test-agent for QA, a @security-agent for code review. Each file acts as a focused spec for that persona‚Äôs behavior, commands, and boundaries. This is particularly useful when you want different agents for different tasks rather than one general-purpose assistant.\n\nDesign for Agent Experience (AX): Just as we design APIs for developer experience (DX), consider designing specs for ‚ÄúAgent Experience.‚Äù This means clean, parseable formats: OpenAPI schemas for any APIs the agent will consume, llms.txt files that summarize documentation for LLM consumption, and explicit type definitions. The Agentic AI Foundation (AAIF) is standardizing protocols like MCP (Model Context Protocol) for tool integration - specs that follow these patterns are easier for agents to consume and act on reliably.\n\nPRD vs SRS mindset: It helps to borrow from established documentation practices. For AI agent specs, you‚Äôll often blend these into one document (as illustrated above), but covering both angles serves you well. Writing it like a PRD ensures you include user-centric context (‚Äúthe why behind each feature‚Äù) so the AI doesn‚Äôt optimize for the wrong thing. Expanding it like an SRS ensures you nail down the specifics the AI will need to actually generate correct code (like what database or API to use). Developers have found that this extra upfront effort pays off by drastically reducing miscommunications with the agent later.\n\nMake the spec a ‚Äúliving document‚Äù: Don‚Äôt write it and forget it. Update the spec as you and the agent make decisions or discover new info. If the AI had to change the data model or you decided to cut a feature, reflect that in the spec so it remains the ground truth. Think of it as version-controlled documentation. In spec-driven workflows, the spec drives implementation, tests, and task breakdowns, and you don‚Äôt move to coding until the spec is validated. This habit keeps the project coherent, especially if you or the agent step away and come back later. Remember, the spec isn‚Äôt just for the AI - it helps you as the developer maintain oversight and ensure the AI‚Äôs work meets the real requirements.\n\nDivide and conquer: give the AI one focused task at a time rather than a monolithic prompt with everything at once.\n\nExperienced AI engineers have learned that trying to stuff the entire project (all requirements, all code, all instructions) into a single prompt or agent message is a recipe for confusion. Not only do you risk hitting token limits, you also risk the model losing focus due to the ‚Äúcurse of instructions‚Äù - too many directives causing it to follow none of them well. The solution is to design your spec and workflow in a modular way, tackling one piece at a time and pulling in only the context needed for that piece.\n\nThe curse of too much context/instructions: Research has confirmed what many devs anecdotally saw: as you pile on more instructions or data into the prompt, the model‚Äôs performance in adhering to each one drops significantly. One study dubbed this the ‚Äúcurse of instructions‚Äù, showing that even GPT-4 and Claude struggle when asked to satisfy many requirements simultaneously. In practical terms, if you present 10 bullet points of detailed rules, the AI might obey the first few and start overlooking others. The better strategy is iterative focus. Guidelines from industry suggest decomposing complex requirements into sequential, simple instructions as a best practice. Focus the AI on one sub-problem at a time, get that done, then move on. This keeps the quality high and errors manageable.\n\nDivide the spec into phases or components: If your spec document is very long or covers a lot of ground, consider splitting it into parts (either physically separate files or clearly separate sections). For example, you might have a section for ‚ÄúBackend API Spec‚Äù and another for ‚ÄúFrontend UI Spec.‚Äù You don‚Äôt need to always feed the frontend spec to the AI when it‚Äôs working on the backend, and vice versa. Many devs using multi-agent setups even create separate agents or sub-processes for each part - e.g. one agent works on database/schema, another on API logic, another on frontend - each with the relevant slice of the spec. Even if you use a single agent, you can emulate this by copying only the relevant spec section into the prompt for that task. Avoid context overload: Don‚Äôt mix authentication tasks with database schema changes in one go, as the DigitalOcean AI guide warns. Keep each prompt tightly scoped to the current goal.\n\nExtended TOC / Summaries for large specs: One clever technique is to have the agent build an extended Table of Contents with summaries for the spec. This is essentially a ‚Äúspec summary‚Äù that condenses each section into a few key points or keywords, and references where details can be found. For example, if your full spec has a section on ‚ÄúSecurity Requirements‚Äù spanning 500 words, you might have the agent summarize it to: ‚ÄúSecurity: use HTTPS, protect API keys, implement input validation (see full spec ¬ß4.2)‚Äù. By creating a hierarchical summary in the planning phase, you get a bird‚Äôs-eye view that can stay in the prompt, while the fine details remain offloaded unless needed. This extended TOC acts as an index: the agent can consult it and say ‚Äúaha, there‚Äôs a security section I should look at‚Äù, and you can then provide that section on demand. It‚Äôs similar to how a human developer skims an outline and then flips to the relevant page of a spec document when working on a specific part.\n\nTo implement this, you can prompt the agent after writing the spec: ‚ÄúSummarize the spec above into a very concise outline with each section‚Äôs key points and a reference tag.‚Äù The result might be a list of sections with one or two sentence summaries. That summary can be kept in the system or assistant message to guide the agent‚Äôs focus without eating up too many tokens. This hierarchical summarization approach is known to help LLMs maintain long-term context by focusing on the high-level structure. The agent carries a ‚Äúmental map‚Äù of the spec.\n\nUtilize sub-agents or ‚Äúskills‚Äù for different spec parts: Another advanced approach is using multiple specialized agents (what Anthropic calls subagents or what you might call ‚Äúskills‚Äù). Each subagent is configured for a specific area of expertise and given the portion of the spec relevant to that area. For instance, you might have a Database Designer subagent that only knows about the data model section of the spec, and an API Coder subagent that knows the API endpoints spec. The main agent (or an orchestrator) can route tasks to the appropriate subagent automatically. The benefit is each agent has a smaller context window to deal with and a more focused role, which can boost accuracy and allow parallel work on independent tasks. Anthropic‚Äôs Claude Code supports this by letting you define subagents with their own system prompts and tools. ‚ÄúEach subagent has a specific purpose and expertise area, uses its own context window separate from the main conversation, and has a custom system prompt guiding its behavior,‚Äù as their docs describe. When a task comes up that matches a subagent‚Äôs domain, Claude can delegate that task to it, with the subagent returning results independently.\n\nParallel agents for throughput: Running multiple agents simultaneously is emerging as ‚Äúthe next big thing‚Äù for developer productivity. Rather than waiting for one agent to finish before starting another task, you can spin up parallel agents for non-overlapping work. Willison describes this as ‚Äúembracing parallel coding agents‚Äù and notes it‚Äôs ‚Äúsurprisingly effective, if mentally exhausting‚Äù. The key is scoping tasks so agents don‚Äôt step on each other - one agent codes a feature while another writes tests, or separate components get built concurrently. Orchestration frameworks like LangGraph or OpenAI Swarm can help coordinate these agents, and shared memory via vector databases (like Chroma) lets them access common context without redundant prompting.\n\nSingle vs. multi-agent: when to use each\n\nIn practice, using subagents or skill-specific prompts might look like: you maintain multiple spec files (or prompt templates) - e.g. SPEC_backend.md, SPEC_frontend.md - and you tell the AI, ‚ÄúFor backend tasks, refer to SPEC_backend; for frontend tasks refer to SPEC_frontend.‚Äù Or in a tool like Cursor/Claude, you actually spin up a subagent for each. This is certainly more complex to set up than a single-agent loop, but it mimics what human developers do - we mentally compartmentalize a large spec into relevant chunks (you don‚Äôt keep the whole 50-page spec in your head at once; you recall the part you need for the task at hand, and have a general sense of the overall architecture). The challenge, as noted, is managing interdependencies: the subagents must still coordinate (the frontend needs to know the API contract from the backend spec, etc.). A central overview (or an ‚Äúarchitect‚Äù agent) can help by referencing the sub-specs and ensuring consistency.\n\nFocus each prompt on one task/section: Even without fancy multi-agent setups, you can manually enforce modularity. For example, after the spec is written, your next move might be: ‚ÄúStep 1: Implement the database schema.‚Äù You feed the agent the Database section of the spec only, plus any global constraints from the spec (like tech stack). The agent works on that. Then for Step 2, ‚ÄúNow implement the authentication feature‚Äù, you provide the Auth section of the spec and maybe the relevant parts of the schema if needed. By refreshing the context for each major task, you ensure the model isn‚Äôt carrying a lot of stale or irrelevant information that could distract it. As one guide suggests: ‚ÄúStart fresh: begin new sessions to clear context when switching between major features‚Äù. You can always remind the agent of critical global rules (from the spec‚Äôs Constraints section) each time, but don‚Äôt shove the entire spec in if it‚Äôs not all needed.\n\nUse in-line directives and code TODOs: Another modularity trick is to use your code or spec as an active part of the conversation. For instance, scaffold your code with // TODO comments that describe what needs to be done, and have the agent fill them one by one. Each TODO essentially acts as a mini-spec for a small task. This keeps the AI laser-focused (‚Äúimplement this specific function according to this spec snippet‚Äù) and you can iterate in a tight loop. It‚Äôs similar to giving the AI a checklist item to complete rather than the whole checklist at once.\n\nThe bottom line: small, focused context beats one giant prompt. This improves quality and keeps the AI from getting ‚Äúoverwhelmed‚Äù by too much at once. As one set of best practices sums up, provide ‚ÄúOne Task Focus‚Äù and ‚ÄúRelevant info only‚Äù to the model, and avoid dumping everything everywhere. By structuring the work into modules - and using strategies like spec summaries or sub-spec agents - you‚Äôll navigate around context size limits and the AI‚Äôs short-term memory cap. Remember, a well-fed AI is like a well-fed function: give it only the inputs it needs for the job at hand.\n\nMake your spec not just a to-do list for the agent, but also a guide for quality control - and don‚Äôt be afraid to inject your own expertise.\n\nA good spec for an AI agent anticipates where the AI might go wrong and sets up guardrails. It also takes advantage of what you know (domain knowledge, edge cases, ‚Äúgotchas‚Äù) so the AI doesn‚Äôt operate in a vacuum. Think of the spec as both coach and referee for the AI: it should encourage the right approach and call out fouls.\n\nUse three-tier boundaries: The GitHub analysis of 2,500+ agent files found that the most effective specs use a three-tier boundary system rather than a simple list of don‚Äôts. This gives the agent clearer guidance on when to proceed, when to pause, and when to stop:\n\n‚úÖ Always do: Actions the agent should take without asking. ‚ÄúAlways run tests before commits.‚Äù ‚ÄúAlways follow the naming conventions in the style guide.‚Äù ‚ÄúAlways log errors to the monitoring service.‚Äù\n\n‚ö†Ô∏è Ask first: Actions that require human approval. ‚ÄúAsk before modifying database schemas.‚Äù ‚ÄúAsk before adding new dependencies.‚Äù ‚ÄúAsk before changing CI/CD configuration.‚Äù This tier catches high-impact changes that might be fine but warrant a human check.\n\nüö´ Never do: Hard stops. ‚ÄúNever commit secrets or API keys.‚Äù ‚ÄúNever edit node_modules/ or vendor/.‚Äù ‚ÄúNever remove a failing test without explicit approval.‚Äù ‚ÄúNever commit secrets‚Äù was the single most common helpful constraint in the study.\n\nThis three-tier approach is more nuanced than a flat list of rules. It acknowledges that some actions are always safe, some need oversight, and some are categorically off-limits. The agent can proceed confidently on ‚ÄúAlways‚Äù items, flag ‚ÄúAsk first‚Äù items for review, and hard-stop on ‚ÄúNever‚Äù items.\n\nEncourage self-verification: One powerful pattern is to have the agent verify its work against the spec automatically. If your tooling allows, you can integrate checks like unit tests or linting that the AI can run after generating code. But even at the spec/prompt level, you can instruct the AI to double-check: e.g. ‚ÄúAfter implementing, compare the result with the spec and confirm all requirements are met. List any spec items that are not addressed.‚Äù This pushes the LLM to reflect on its output relative to the spec, catching omissions. It‚Äôs a form of self-audit built into the process.\n\nFor instance, you might append to a prompt: ‚Äú(After writing the function, review the above requirements list and ensure each is satisfied, marking any missing ones).‚Äù The model will then (ideally) output the code followed by a short checklist indicating if it met each requirement. This reduces the chance it forgets something before you even run tests. It‚Äôs not foolproof, but it helps.\n\nLLM-as-a-Judge for subjective checks: For criteria that are hard to test automatically - code style, readability, adherence to architectural patterns - consider using ‚ÄúLLM-as-a-Judge.‚Äù This means having a second agent (or a separate prompt) review the first agent‚Äôs output against your spec‚Äôs quality guidelines. Anthropic and others have found this effective for subjective evaluation. You might prompt: ‚ÄúReview this code for adherence to our style guide. Flag any violations.‚Äù The judge agent returns feedback that either gets incorporated or triggers a revision. This adds a layer of semantic evaluation beyond syntax checks.\n\nConformance testing: Willison advocates building conformance suites - language-independent tests (often YAML-based) that any implementation must pass. These act as a contract: if you‚Äôre building an API, the conformance suite specifies expected inputs/outputs, and the agent‚Äôs code must satisfy all cases. This is more rigorous than ad-hoc unit tests because it‚Äôs derived directly from the spec and can be reused across implementations. Include conformance criteria in your spec‚Äôs Success section (e.g., ‚ÄúMust pass all cases in conformance/api-tests.yaml‚Äù).\n\nLeverage testing in the spec: If possible, incorporate a test plan or even actual tests in your spec and prompt flow. In traditional development, we use TDD or write test cases to clarify requirements - you can do the same with AI. For example, in the spec‚Äôs Success Criteria, you might say ‚ÄúThese sample inputs should produce these outputs‚Ä¶‚Äù or ‚Äúthe following unit tests should pass.‚Äù The agent can be prompted to run through those cases in its head or actually execute them if it has that capability. Simon Willison noted that having a robust test suite is like giving the agents superpowers - they can validate and iterate quickly when tests fail. In an AI coding context, writing a bit of pseudocode for tests or expected outcomes in the spec can guide the agent‚Äôs implementation. Additionally, you can use a dedicated ‚Äútest agent‚Äù in a subagent setup that takes the spec‚Äôs criteria and continuously verifies the ‚Äúcode agent‚Äôs‚Äù output.\n\nBring your domain knowledge: Your spec should reflect insights that only an experienced developer or someone with context would know. For example, if you‚Äôre building an e-commerce agent and you know that ‚Äúproducts‚Äù and ‚Äúcategories‚Äù have a many-to-many relationship, state that clearly (don‚Äôt assume the AI will infer it - it might not). If a certain library is notoriously tricky, mention pitfalls to avoid. Essentially, pour your mentorship into the spec. The spec can contain advice like ‚ÄúIf using library X, watch out for memory leak issue in version Y (apply workaround Z).‚Äù This level of detail is what turns an average AI output into a truly robust solution, because you‚Äôve steered the AI away from common traps.\n\nAlso, if you have preferences or style guidelines (say, ‚Äúuse functional components over class components in React‚Äù), encode that in the spec. The AI will then emulate your style. Many engineers even include small examples in the spec, e.g., ‚ÄúAll API responses should be JSON. E.g. {‚Äúerror‚Äù: ‚Äúmessage‚Äù} for errors.‚Äù By giving a quick example, you anchor the AI to the exact format you want.\n\nMinimalism for simple tasks: While we advocate thorough specs, part of expertise is knowing when to keep it simple. For relatively simple, isolated tasks, an overbearing spec can actually confuse more than help. If you‚Äôre asking the agent to do something straightforward (like ‚Äúcenter a div on the page‚Äù), you might just say, ‚ÄúMake sure to keep the solution concise and do not add extraneous markup or styles.‚Äù No need for a full PRD there. Conversely, for complex tasks (like ‚Äúimplement an OAuth flow with token refresh and error handling‚Äù), that‚Äôs when you break out the detailed spec. A good rule of thumb: adjust spec detail to task complexity. Don‚Äôt under-spec a hard problem (the agent will flail or go off-track), but don‚Äôt over-spec a trivial one (the agent might get tangled or use up context on unnecessary instructions).\n\nMaintain the AI‚Äôs ‚Äúpersona‚Äù if needed: Sometimes, part of your spec is defining how the agent should behave or respond, especially if the agent interacts with users. For example, if building a customer support agent, your spec might include guidelines like ‚ÄúUse a friendly and professional tone,‚Äù ‚ÄúIf you don‚Äôt know the answer, ask for clarification or offer to follow up, rather than guessing.‚Äù These kind of rules (often included in system prompts) help keep the AI‚Äôs outputs aligned with expectations. They are essentially spec items for AI behavior. Keep them consistent and remind the model of them if needed in long sessions (LLMs can ‚Äúdrift‚Äù in style over time if not kept on a leash).\n\nYou remain the exec in the loop: The spec empowers the agent, but you remain the ultimate quality filter. If the agent produces something that technically meets the spec but doesn‚Äôt feel right, trust your judgement. Either refine the spec or directly adjust the output. The great thing about AI agents is they don‚Äôt get offended - if they deliver a design that‚Äôs off, you can say, ‚ÄúActually, that‚Äôs not what I intended, let‚Äôs clarify the spec and redo it.‚Äù The spec is a living artifact in collaboration with the AI, not a one-time contract you can‚Äôt change.\n\nSimon Willison humorously likened working with AI agents to ‚Äúa very weird form of management‚Äù and even ‚Äúgetting good results out of a coding agent feels uncomfortably close to managing a human intern‚Äù. You need to provide clear instructions (the spec), ensure they have the necessary context (the spec and relevant data), and give actionable feedback. The spec sets the stage, but monitoring and feedback during execution are key. If an AI was a ‚Äúweird digital intern who will absolutely cheat if you give them a chance‚Äù, the spec and constraints you write are how you prevent that cheating and keep them on task.\n\nHere‚Äôs the payoff: a good spec doesn‚Äôt just tell the AI what to build, it also helps it self-correct and stay within safe boundaries. By baking in verification steps, constraints, and your hard-earned knowledge, you drastically increase the odds that the agent‚Äôs output is correct on the first try (or at least much closer to correct). This reduces iterations and those ‚Äúwhy on Earth did it do that?‚Äù moments.\n\nThink of spec-writing and agent-building as an iterative loop: test early, gather feedback, refine the spec, and leverage tools to automate checks.\n\nThe initial spec is not the end - it‚Äôs the beginning of a cycle. The best outcomes come when you continually verify the agent‚Äôs work against the spec and adjust accordingly. Also, modern AI devs use various tools to support this process (from CI pipelines to context management utilities).\n\nContinuous testing: Don‚Äôt wait until the end to see if the agent met the spec. After each major milestone or even each function, run tests or at least do quick manual checks. If something fails, update the spec or prompt before proceeding. For example, if the spec said ‚Äúpasswords must be hashed with bcrypt‚Äù and you see the agent‚Äôs code storing plain text - stop and correct it (and remind the spec or prompt about the rule). Automated tests shine here: if you provided tests (or write them as you go), let the agent run them. In many coding agent setups, you can have an agent run npm test or similar after finishing a task. The results (failures) can then feed back into the next prompt, effectively telling the agent ‚Äúyour output didn‚Äôt meet spec on X, Y, Z - fix it.‚Äù This kind of agentic loop (code -> test -> fix -> repeat) is extremely powerful and is how tools like Claude Code or Copilot Labs are evolving to handle larger tasks. Always define what ‚Äúdone‚Äù means (via tests or criteria) and check for it.\n\nIterate on the spec itself: If you discover that the spec was incomplete or unclear (maybe the agent misunderstood something or you realized you missed a requirement), update the spec document. Then explicitly re-sync the agent with the new spec: ‚ÄúI have updated the spec as follows‚Ä¶ Given the updated spec, adjust the plan or refactor the code accordingly.‚Äù This way the spec remains the single source of truth. It‚Äôs similar to how we handle changing requirements in normal dev - but in this case you‚Äôre also the product manager for your AI agent. Keep version history if possible (even just via commit messages or notes), so you know what changed and why.\n\nUtilize context-management and memory tools: There‚Äôs a growing ecosystem of tools to help manage AI agent context and knowledge. For instance, retrieval-augmented generation (RAG) is a pattern where the agent can pull in relevant chunks of data from a knowledge base (like a vector database) on the fly. If your spec is huge, you could embed sections of it and let the agent retrieve the most relevant parts when needed, instead of always providing the whole thing. There are also frameworks implementing the Model Context Protocol (MCP), which automates feeding the right context to the model based on the current task. One example is Context7 (context7.com), which can auto-fetch relevant context snippets from docs based on what you‚Äôre working on. In practice, this might mean the agent notices you‚Äôre working on ‚Äúpayment processing‚Äù and it pulls the ‚ÄúPayments‚Äù section of your spec or documentation into the prompt. Consider leveraging such tools or setting up a rudimentary version (even a simple search in your spec document).\n\nParallelize carefully: Some developers run multiple agent instances in parallel on different tasks (as mentioned earlier with subagents). This can speed up development - e.g., one agent generates code while another simultaneously writes tests, or two features are built concurrently. If you go this route, ensure the tasks are truly independent or clearly separated to avoid conflicts (the spec should note any dependencies). For example, don‚Äôt have two agents writing to the same file at once. One workflow is to have an agent generate code and another review it in parallel, or to have separate components built that integrate later. This is advanced usage and can be mentally taxing to manage (as Willison admitted, running multiple agents is surprisingly effective, if mentally exhausting!). Start with at most 2-3 agents to keep things manageable.\n\nVersion control and spec locks: Use Git or your version control of choice to track what the agent does. Good version control habits matter even more with AI assistance. Commit the spec file itself to the repo. This not only preserves history, but the agent can even use git diff or blame to understand changes (LLMs are quite capable of reading diffs). Some advanced agent setups let the agent query the VCS history to see when something was introduced - surprisingly, models can be ‚Äúfiercely competent at Git‚Äù. By keeping your spec in the repo, you allow both you and the AI to track evolution. There are tools (like GitHub Spec Kit mentioned earlier) that integrate spec-driven development into the git workflow - for instance, gating merges on updated specs or generating checklists from spec items. While you don‚Äôt need those tools to succeed, the takeaway is to treat the spec like code - maintain it diligently.\n\nCost and speed considerations: Working with large models and long contexts can be slow and expensive. A practical tip is to use model selection and batching smartly. Perhaps use a cheaper/faster model for initial drafts or repetitions, and reserve the most capable (and expensive) model for final outputs or complex reasoning. Some developers use GPT-4 or Claude for planning and critical steps, but offload simpler expansions or refactors to a local model or a smaller API model. If using multiple agents, maybe not all need to be top-tier; a test-running agent or a linter agent could be a smaller model. Also consider throttling context size: don‚Äôt feed 20k tokens if 5k will do. As we discussed, more tokens can mean diminishing returns.\n\nMonitor and log everything: In complex agent workflows, logging the agent‚Äôs actions and outputs is essential. Check the logs to see if the agent is deviating or encountering errors. Many frameworks provide trace logs or allow printing the agent‚Äôs chain-of-thought (especially if you prompt it to think step-by-step). Reviewing these logs can highlight where the spec or instructions might have been misinterpreted. It‚Äôs not unlike debugging a program - except the ‚Äúprogram‚Äù is the conversation/prompt chain. If something weird happens, go back to the spec/instructions to see if there was ambiguity.\n\nLearn and improve: Finally, treat each project as a learning opportunity to refine your spec-writing skill. Maybe you‚Äôll discover that a certain phrasing consistently confuses the AI, or that organizing spec sections in a certain way yields better adherence. Incorporate those lessons into the next spec. The field of AI agents is rapidly evolving, so new best practices (and tools) emerge constantly. Stay updated via blogs (like the ones by Simon Willison, Andrej Karpathy, etc.), and don‚Äôt hesitate to experiment.\n\nA spec for an AI agent isn‚Äôt ‚Äúwrite once, done.‚Äù It‚Äôs part of a continuous cycle of instructing, verifying, and refining. The payoff for this diligence is substantial: by catching issues early and keeping the agent aligned, you avoid costly rewrites or failures later. As one AI engineer quipped, using these practices can feel like having ‚Äúan army of interns‚Äù working for you, but you have to manage them well. A good spec, continuously maintained, is your management tool.\n\nBefore wrapping up, it‚Äôs worth calling out anti-patterns that can derail even well-intentioned spec-driven workflows. The GitHub study of 2,500+ agent files revealed a stark divide: ‚ÄúMost agent files fail because they‚Äôre too vague.‚Äù Here are the mistakes to avoid:\n\nVague prompts: ‚ÄúBuild me something cool‚Äù or ‚ÄúMake it work better‚Äù gives the agent nothing to anchor on. As Baptiste Studer puts it: ‚ÄúVague prompts mean wrong results.‚Äù Be specific about inputs, outputs, and constraints. ‚ÄúYou are a helpful coding assistant‚Äù doesn‚Äôt work. ‚ÄúYou are a test engineer who writes tests for React components, follows these examples, and never modifies source code‚Äù does.\n\nOverlong contexts without summarization: Dumping 50 pages of documentation into a prompt and hoping the model figures it out rarely works. Use hierarchical summaries (as discussed in Principle 3) or RAG to surface only what‚Äôs relevant. Context length is not a substitute for context quality.\n\nSkipping human review: Willison has a personal rule: ‚ÄúI won‚Äôt commit code I couldn‚Äôt explain to someone else.‚Äù Just because the agent produced something that passes tests doesn‚Äôt mean it‚Äôs correct, secure, or maintainable. Always review critical code paths. The ‚Äúhouse of cards‚Äù metaphor applies: AI-generated code can look solid but collapse under edge cases you didn‚Äôt test.\n\nConflating vibe coding with production engineering: Rapid prototyping with AI (‚Äúvibe coding‚Äù) is great for exploration and throwaway projects. But shipping that code to production without rigorous specs, tests, and review is asking for trouble. Osmani distinguishes ‚Äúvibe coding‚Äù from ‚ÄúAI-assisted engineering‚Äù - the latter requires the discipline this guide describes. Know which mode you‚Äôre in.\n\nIgnoring the ‚Äúlethal trifecta‚Äù: Willison warns of three properties that make AI agents dangerous: speed (they work faster than you can review), non-determinism (same input, different outputs), and cost (encouraging corner-cutting on verification). Your spec and review process must account for all three. Don‚Äôt let speed outpace your ability to verify.\n\nMissing the six core areas: If your spec doesn‚Äôt cover commands, testing, project structure, code style, git workflow, and boundaries, you‚Äôre likely missing something the agent needs. Use the six-area checklist from Section 2 as a sanity check before handing off to the agent.\n\nWriting an effective spec for AI coding agents requires solid software engineering principles combined with adaptation to LLM quirks. Start with clarity of purpose and let the AI help expand the plan. Structure the spec like a serious design document - covering the six core areas and integrating it into your toolchain so it becomes an executable artifact, not just prose. Keep the agent‚Äôs focus tight by feeding it one piece of the puzzle at a time (and consider clever tactics like summary TOCs, subagents, or parallel orchestration to handle big specs). Anticipate pitfalls by including three-tier boundaries (Always/Ask first/Never), self-checks, and conformance tests - essentially, teach the AI how to not fail. And treat the whole process as iterative: use tests and feedback to refine both the spec and the code continuously.\n\nFollow these guidelines and your AI agent will be far less likely to ‚Äúbreak down‚Äù under large contexts or wander off into nonsense.\n\nThis post was formatted using Gemini with images generated using Nano Banana Pro",
    "readingTime": 35,
    "keywords": [
      "extended toc",
      "api keys",
      "github study",
      "vague prompts",
      "document prd",
      "context protocol",
      "naming conventions",
      "helpful constraint",
      "architectural patterns",
      "tech stack"
    ],
    "qualityScore": 1,
    "link": "https://addyosmani.com/blog/good-spec/",
    "thumbnail_url": "https://addyosmani.com/assets/images/good-spec.jpg",
    "created_at": "2026-01-14T12:25:01.028Z",
    "topic": "tech"
  },
  {
    "slug": "phases-of-vibe-coding",
    "title": "Phases of Vibe Coding",
    "description": "I built a terminal-based Counter-Strike clone with a coding agent. 49K lines in a week. Understanding the 4 phases of AI-assisted development.",
    "fullText": "EssaysThe 4 Phases of Vibe CodingI built a terminal-based Counter-Strike clone with a coding agent. 49K lines in a week. These projects go through 4 distinct phases, and understanding them is the key to effective AI-assisted development.Idan BeckCEO and FounderJanuary 12, 2026‚Ä¢12 min readShare: Loading content... Related Articles EssaysJanuary 12, 2026 ‚Ä¢ 8 min read The Bootstrapping LoopJust in Time SoftwareJanuary 6, 2026 ‚Ä¢ 10 min read Why Business Velocity Will Be Measured in Tokens per SecondMaster PlanOctober 28, 2025 ‚Ä¢ 8 min read Master Plan: Building Software at the Speed of ThoughtReady to Transform Your Development Process?Discover how Zerg AI can help you implement just-in-time software development in your organization.\n\nSchedule a Consultation",
    "readingTime": 1,
    "keywords": [
      "phases",
      "software",
      "development"
    ],
    "qualityScore": 0.55,
    "link": "https://zergai.com/blog/4-phases-vibe-coding",
    "thumbnail_url": "https://zergai.com/images/blog/4-phases-vibe-coding-hero.png",
    "created_at": "2026-01-14T01:00:15.929Z",
    "topic": "tech"
  },
  {
    "slug": "even-linus-torvalds-is-trying-his-hand-at-vibe-coding-but-just-a-little",
    "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
    "description": "But then I cut out the middle man‚Äîme.\"",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
    "thumbnail_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
    "created_at": "2026-01-14T01:00:15.061Z",
    "topic": "tech"
  },
  {
    "slug": "vibe-engineering-what-ive-learned-working-with-ai-coding-agents",
    "title": "Vibe Engineering: What I've Learned Working with AI Coding Agents",
    "description": "Vibe Engineering: What I've Learned Working with AI Coding Agents",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://twitter.com/mrexodia/status/2010157660885176767",
    "thumbnail_url": "https://pbs.twimg.com/media/G-WBO0QXAAApT_j.jpg:large",
    "created_at": "2026-01-13T00:54:05.735Z",
    "topic": "tech"
  },
  {
    "slug": "targets-dev-server-offline-after-hackers-claim-to-steal-source-code",
    "title": "Target's dev server offline after hackers claim to steal source code",
    "description": "Hackers are claiming to be selling internal source code belonging to Target Corporation, after publishing what appears to be a sample of stolen code repositories on a public software development platform. After BleepingComputer notified Target, the files were taken offline and the retailer's developer Git server was inaccessible.",
    "fullText": "Hackers are claiming to be selling internal source code belonging to Target Corporation, after publishing what appears to be a sample of stolen code repositories on a public software development platform.\n\nLast week, an unknown threat actor created multiple repositories on Gitea that appeared to contain portions of Target's internal code and developer documentation. The repositories were presented as a preview of a much larger dataset allegedly being offered for sale to buyers on an underground forum or private channel.\n\nAfter BleepingComputer contacted Target with questions about the alleged breach, the files were taken offline and the retailer's Git server, git.target.com, became inaccessible from the internet.\n\nLast week, BleepingComputer received a tip that a threat actor was posting screenshots in a private hacking community to support claims that they had gained access to Target's internal development environment.\n\nThe same actor had also published several repositories on Gitea, a self-hosted Git service similar to GitHub or GitLab, as a sample of the data the actor claimed was being offered for sale.\n\nAccording to the source, hackers claimed that \"this is [the first set of] data to go to auction.\"\n\nEach repository contained a file named SALE.MD listing tens of thousands of files and directories purportedly included in the full dataset. The listing was more than 57,000 lines long and advertised a total archive size of approximately 860 GB.\n\nThe Gitea sample repository names included:\n\nIt's worth noting that the commit metadata and documentation referenced the names of internal Target development servers, and multiple current Target lead and senior engineers.\n\nBleepingComputer shared the Gitea links with Target on Thursday and requested comment on the alleged breach\n\nAround the same time, Target's developer Git server at git.target.com also became inaccessible from the internet.\n\nUntil Friday, the subdomain was reachable and redirected to a login page, prompting Target employees to connect via the company's secure network or VPN. As of Saturday, the site no longer loads externally:\n\nBleepingComputer also observed that search engines such as Google had indexed and cached a small number of resources from git.target.com, indicating that some content from the domain was publicly accessible at some point in the past.\n\nIt is unclear when those pages were indexed or under what configuration, and their presence in search results does not necessarily indicate that the current claims are linked to any exposure of the server, or that the Git infrastructure was recently accessible without authentication.\n\nWhile BleepingComputer has not independently verified the full 860 GB dataset or confirmed that a breach occurred, the directory structure, repository naming, and internal system references in the SALE.MD index are consistent with a large enterprise Git environment.\n\nAdditionally, the contents do not match any of Target's open-source projects on GitHub, indicating the material, if authentic, would have originated from private development infrastructure rather than publicly released code.\n\nThe presence of the names of current Target lead and senior engineers in commit metadata and documentation, along with links to internal API endpoints and platforms, such as confluence.target.com, also raises questions about the origin of the files.\n\nFurthermore, the fact that the Gitea respositories used to store Target's allegedly stolen source code are no longer available, also point toward¬†a possible breach.\n\nAfter Target initially requested the repository links, the company did not provide further comment before publication when approached multiple times.\n\nTarget's most significant publicly disclosed security incident to date remains its 2013 breach, in which attackers stole payment card data and other personally identifiable information belonging to up to 110 million customers and exfiltrated it to infrastructure located in Eastern Europe, according to U.S. Senate and academic investigations.\n\nWhether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.\n\nGet the cheat sheet and take the guesswork out of secrets management.",
    "readingTime": 4,
    "keywords": [
      "git server",
      "target lead",
      "commit metadata",
      "senior engineers",
      "target's internal",
      "threat actor",
      "alleged breach",
      "code",
      "repositories",
      "development"
    ],
    "qualityScore": 1,
    "link": "https://www.bleepingcomputer.com/news/security/targets-dev-server-offline-after-hackers-claim-to-steal-source-code/",
    "thumbnail_url": "https://www.bleepstatic.com/content/hl-images/2026/01/12/target-header.jpg",
    "created_at": "2026-01-12T18:19:09.480Z",
    "topic": "tech"
  },
  {
    "slug": "pico-gpu-virtual-gpu-for-learning-shaders",
    "title": "Pico GPU: Virtual GPU for Learning Shaders",
    "description": "Experiment with GPU programming and sound synth",
    "fullText": "Pico GPU is a 300KB memory GPU intended to learn, experiment and have fun with shaders. It is perfect to easily create small demos or games involving 3D rendering. It can also perform GPU based sound synthesis.SpecificationsPico GPU specification are:640x480 resolution at 60FPS, with 24 bit depth and 8 bit stencil300KB gpu memory to load your textures, buffers, code and shaders4 channels Mono 32 bit sound synthesis at 48 KHz (using GPU shaders)complete support for vertex & fragment bufferssupport blending, culling, depth, stencil, color mask, clippingsupport render targets and instancingmaths matrix, vector, quaternion supportsave & load as a 640x480 PNG screenshot that contains all your datashare your apps with the community!",
    "readingTime": 1,
    "keywords": [
      "memory",
      "shaders",
      "sound",
      "depth",
      "load"
    ],
    "qualityScore": 0.35,
    "link": "https://ncannasse.github.io/picogpu/",
    "thumbnail_url": "og-image.png",
    "created_at": "2026-01-12T18:19:07.266Z",
    "topic": "tech"
  },
  {
    "slug": "quantization-and-distillation-effects-on-code-llms",
    "title": "Quantization and distillation effects on code LLMs",
    "description": "Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints.",
    "fullText": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.",
    "readingTime": 1,
    "keywords": [
      "arxivlabs",
      "arxiv",
      "community"
    ],
    "qualityScore": 0.4,
    "link": "https://arxiv.org/abs/2601.02563",
    "thumbnail_url": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "created_at": "2026-01-12T12:26:18.532Z",
    "topic": "tech"
  },
  {
    "slug": "we-asked-over-150-software-engineers-about-vibecoding-heres-what-they-said",
    "title": "We asked over 150 software engineers about vibe-coding. Here's what they said.",
    "description": "167 software engineers responded to Business Insider's vibe-coding survey. Over 45% reported \"keeping up\" with AI tools. Almost 17% feel behind.",
    "fullText": "AI has radically changed what coding looks like. We asked software engineers how they felt about it.\n\nAndrej Karpathy coined the term \"vibe-coding,\" or the creation of code using AI. The term has since gained traction among developers worldwide and was named Collins Dictionary's Word of the Year for 2025.\n\nLess than a year after his post, Karpathy wrote that he had \"never felt this much behind as a programmer.\"\n\nWe asked developers: When it comes to vibe-coding, do you feel ahead, behind, or like you're keeping pace?\n\n167 software engineers responded to our survey. The biggest cohort ‚Äî¬†75 engineers, or 46.9% ‚Äî¬†said that they were \"keeping up.\" 30 engineers said they felt ahead of the curve, while 27 felt behind.\n\n28 engineers (or 17.5% of respondents) said that they were opting out of using AI code editing tools entirely. These engineers wrote that the tools weren't advanced enough, or that they took too long to learn how to use. None of the 28 agreed to speak on the record after Business Insider reached out.\n\nWhile the survey isn't scientific, the results offer insight into how software engineers are feeling about their rapidly changing industry.\n\nIn follow-up conversations, eight engineers told Business Insider how they feel about AI code editors. All found them helpful in some form, though their usages ranged from one-off tools to lifesavers.\n\nRyan Shah sometimes wonders: \"Did I really need to learn how to write code?\"\n\nThe 23-year-old AI consultant from Atlanta recently graduated with a degree in computer information technology. Now he uses Cursor and Google's Antigravity, paired with Claude Opus 4.5, which he said was at \"midlevel engineer status.\"\n\nShah said he doesn't regret his software engineering courses, though. They taught him to \"read\" code, he said, a skill that, coupled with his vibe-coding proficiencies, keeps him from being \"the first one laid off.\"\n\nJavanie Campbell swung the other way: He warned that over-reliance on vibe-coding tools will put your career in danger.\n\n\"For people who turn to the LLM as the God or the expert, they will be replaced,\" said the 35-year-old CEO of DevDaysAtWork, who is based in Jamaica.\n\nAmong software engineers, there's a debate brewing: Just how bad will the effects of AI code editors be on jobs? Some say they will shrink the industry's workforce; others call them tools, not replacements for engineers.\n\nThe first time Ryan Clinton tried vibe-coding, he got scared for his job. He's not fearful anymore, he said.\n\nClinton's engineering level won't be affected, said the 46-year-old software developer from Nashville. More experienced engineers work on \"architecture and design,\" he said, while more junior staffers code. At this point of AI coding, human intervention is also still routinely necessary.\n\n\"You want to make sure it makes sense,\" he said. \"Only an idiot would randomly click 'yes' and commit it.\"\n\nBarry Fruitman is more worried ‚Äî¬†but not for himself. At 56, the Android developer from Toronto doesn't think the job market will feel the effect until five to 10 years out.\n\n\"Today, I think the threat is overstated, and hopefully it will stay that way until I retire,\" he said.\n\nEd Gaile said AI tools have doubled, if not tripled, his productivity.\n\nThe 55-year-old Appfire principal solutions architect from Atlanta was impressed by the decrease in context switching that vibe-coding tools brought.\n\n\"I wish I had this 15 years ago,\" he said.\n\nFor AI code editors, the word \"productivity\" still looms large. Many people feel that they're saving time by using these tools. Others cite the additional time spent reviewing and correcting lines of code.\n\nA July METR study added fuel to the fire.\n\nThe study asked experienced developers to complete a series of tasks. Study participants working without AI's help spent 10% more time coding ‚Äî but those with AI assistance spent 20% more time reviewing AI outputs, prompting AI, waiting on AI, or being idle. Ultimately, the study found that the AI-assisted developers were less productive.\n\nShawn Gay, a 54-year-old R&D manager from El Paso, Texas, spends time keeping up with the industry's changes. He said he felt behind the curve.\n\n\"I have decades of experience, so I feel like it's a huge effort to try to change the way my brain thinks about software,\" Gay told Business Insider.\n\nGus De Souza said that he saved time on coding, but spent more time reviewing the AI-generated code. The real productivity gains were in troubleshooting, said the 48-year-old software architect from Kitchener, Ontario.\n\nWhat even is a vibe-coder? While the term has grown to encompass most forms of AI-assisted coding, Karpathy's X post first defined it as when developers \"fully give in to the vibes, embrace exponentials, and forget that the code even exists.\"\n\nLara Fraser, a data analyst and epidemiologist from Sarasota, Florida, doesn't consider herself a vibe-coder.\n\nFraser codes in R and uses tools like ChatGPT and Claude to assist. She's tried other tools, but found high rates of hallucination. The model generation also matters, Fraser said: GPT 5.1 was great, but 5.2 was a \"disaster.\"\n\nFor Fraser, vibe-coding depends on the programmer's skill. Anyone can create an app, but not everyone can maintain it.\n\n\"Inevitably, something's going to break,\" she said. \"Can you fix it? If you can't, you're a vibe-coder.\"",
    "readingTime": 5,
    "keywords": [
      "code editors",
      "software engineers",
      "year-old software",
      "vibe-coding tools",
      "business insider",
      "developers",
      "behind",
      "study",
      "doesn't",
      "productivity"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/software-engineers-on-vibe-coding-ai-tools-2026-1",
    "thumbnail_url": "https://i.insider.com/69601f86832e0ef1ead7712a?width=1200&format=jpeg",
    "created_at": "2026-01-12T12:26:15.498Z",
    "topic": "tech"
  },
  {
    "slug": "tiny-coder-ai-coding-agent-in-300-loc-writing-itself",
    "title": "Tiny Coder ‚Äì AI coding agent in ~300 LOC writing itself",
    "description": "Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies. - xrip/tinycode",
    "fullText": "xrip\n\n /\n\n tinycode\n\n Public\n\n Single-file AI coding assistant (~350 LOC). Claude API with tool calling. TypeScript + Bun. Zero dependencies.\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n xrip/tinycode",
    "readingTime": 1,
    "keywords": [
      "star"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/xrip/tinycode",
    "thumbnail_url": "https://opengraph.githubassets.com/07e51ae8c3cabfafdf20fdcf525646b8f004627e8c671e3c9185240ef4e3b4fd/xrip/tinycode",
    "created_at": "2026-01-11T12:21:58.611Z",
    "topic": "tech"
  },
  {
    "slug": "a-coder-considers-the-waning-days-of-the-craft-2023",
    "title": "A coder considers the waning days of the craft (2023)",
    "description": "Coding has always felt to me like an endlessly deep and rich domain. Now I find myself wanting to write a eulogy for it.",
    "fullText": "I first began to believe this on a Friday morning this past summer, while working on a small hobby project. A few months back, my friend Ben and I had resolved to create a Times-style crossword puzzle entirely by computer. In 2018, we‚Äôd made a Saturday puzzle with the help of software and were surprised by how little we contributed‚Äîjust applying our taste here and there. Now we would attempt to build a crossword-making program that didn‚Äôt require a human touch.\n\nWhen we‚Äôve taken on projects like this in the past, they‚Äôve had both a hardware component and a software component, with Ben‚Äôs strengths running toward the former. We once made a neon sign that would glow when the subway was approaching the stop near our apartments. Ben bent the glass and wired up the transformer‚Äôs circuit board. I wrote code to process the transit data. Ben has some professional coding experience of his own, but it was brief, shallow, and now about twenty years out of date; the serious coding was left to me. For the new crossword project, though, Ben had introduced a third party. He‚Äôd signed up for a ChatGPT Plus subscription and was using GPT-4 as a coding assistant.\n\nSomething strange started happening. Ben and I would talk about a bit of software we wanted for the project. Then, a shockingly short time later, Ben would deliver it himself. At one point, we wanted a command that would print a hundred random lines from a dictionary file. I thought about the problem for a few minutes, and, when thinking failed, tried Googling. I made some false starts using what I could gather, and while I did my thing‚Äîprogramming‚ÄîBen told GPT-4 what he wanted and got code that ran perfectly.\n\nFine: commands like those are notoriously fussy, and everybody looks them up anyway. It‚Äôs not real programming. A few days later, Ben talked about how it would be nice to have an iPhone app to rate words from the dictionary. But he had no idea what a pain it is to make an iPhone app. I‚Äôd tried a few times and never got beyond something that half worked. I found Apple‚Äôs programming environment forbidding. You had to learn not just a new language but a new program for editing and running code; you had to learn a zoo of ‚ÄúU.I. components‚Äù and all the complicated ways of stitching them together; and, finally, you had to figure out how to package the app. The mountain of new things to learn never seemed worth it. The next morning, I woke up to an app in my in-box that did exactly what Ben had said he wanted. It worked perfectly, and even had a cute design. Ben said that he‚Äôd made it in a few hours. GPT-4 had done most of the heavy lifting.",
    "readingTime": 3,
    "keywords": [
      "later ben",
      "iphone app",
      "project",
      "software",
      "code",
      "coding",
      "learn",
      "morning",
      "crossword",
      "puzzle"
    ],
    "qualityScore": 0.9,
    "link": "https://www.newyorker.com/magazine/2023/11/20/a-coder-considers-the-waning-days-of-the-craft",
    "thumbnail_url": "https://media.newyorker.com/photos/654bf23c9d37df3d9f9cd353/16:9/w_1280,c_limit/231120_r43375.jpg",
    "created_at": "2026-01-11T12:21:58.229Z",
    "topic": "tech"
  },
  {
    "slug": "npmagentskills-bundle-ai-agent-documentation-with-npm-packages",
    "title": "NPM-agentskills ‚Äì Bundle AI agent documentation with NPM packages",
    "description": "Framework-agnostic skill discovery and export for AI coding agents - onmax/npm-agentskills",
    "fullText": "onmax\n\n /\n\n npm-agentskills\n\n Public\n\n Framework-agnostic skill discovery and export for AI coding agents\n\n License\n\n MIT license\n\n 1\n star\n\n 0\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n onmax/npm-agentskills",
    "readingTime": 1,
    "keywords": [
      "star",
      "license"
    ],
    "qualityScore": 0.2,
    "link": "https://github.com/onmax/npm-agentskills",
    "thumbnail_url": "https://opengraph.githubassets.com/25ea09a060a3707ff9ae03007862bc0ef3c0da7b02d7c59d84dc472b47aefb04/onmax/npm-agentskills",
    "created_at": "2026-01-11T12:21:56.673Z",
    "topic": "tech"
  },
  {
    "slug": "bare-metal-programming-with-riscv-guide-2023",
    "title": "Bare metal programming with RISC-V guide (2023)",
    "description": "Guide on coding a bare metal program with UART output for RISC-V and running the emulation with QEMU. Brief overview of the RISC-V boot process.",
    "fullText": "Today we‚Äôre going to explore how to write a bare metal program for a RISC-V machine. For reproducibility, the target is a QEMU riscv64 virt machine.\n\nWe will briefly cover the initial stages of the RISC-V machine bootup and where you can plug in your custom software to program the bare metal machine!\n\nAt the end of this article, we will write a bare metal program for our RISC-V machine and send a string ‚Äòhello‚Äô to the user, without depending on any supporting software on the running machine whatsoever (OS kernel, libraries, anything).\n\nMachine bootup and running the initial software\n\nWriting a custom ‚ÄúBIOS‚Äù for RISC-V\n\nFeel free to skip the section on general concepts if you are familiar with how computers boot\n\nWhen a real machine is powered on, the hardware first runs the health checks and then loads the first instructions to run into its memory. Once the instructions are loaded, the processor core intializes its registers and the program counter points to the first instruction. From that point on, the software can run.\n\nIn simpler setups like small microcontrollers, this is all the software there is, just a single binary blob of instructions. The processor will execute just that going forward. In a more complex setup like a laptop or a phone, there are more stages to the startup.\n\nIn those more complex setups, traditionally, the first instructions are the BIOS, whose task is to subsequently load the bootloader into the memory and hand-off the control to it. The bootloader is usually small and easy to load into the running memory and the processor can easily start running its code. It proceeds to load the operating system kernel into the memory (implementing the bootloader though is a science of its own).\n\nEach machine loads the initial software in its own way. For example, the BIOS can be stored on a separate storage chip and upon powerup, the contents of the storage are simply filled into the memory at a fixed address and the processor just executes starting from that address.\n\nriscv64 virt machine, even though its virtualized, still has its own boot sequence. It goes through multiple stages, and at the moment, we will not be exploring them all. Please stay tuned for the follow-up articles with those details.\n\nThe key to understanding this virtual machine is that, obviously, it has no chip attached to it from which to read the software (it is virtual) so QEMU simulates this in some way. You might have seen the flag -bios for QEMU examples before and hopefully now you have a strong intuition what it could be. If you‚Äôre guessing this is passing the very first instructions that your virtual RISC-V core is executing upon the startup, you are almost correct.\n\nOnce you power on this virtual machine, QEMU fills the memory at 0x1000 with a few instructions and sets the program counter right to that address. This is the equivalent of a real machine having some hardcoded ROM firmware on the board (tucked away in some chip) and just dumping the contents into the RAM upon the bootup. You do not have the control over these instructions, i.e. they are not a part of your software image, and generally, I do not see a reason why you would want to override those, and they are actually quite useful for more complicated setups (I promise we will cover them in a follow up article). For the curious ones, these few instructions are the Zero Stage Bootloader (ZSBL). The ZSBL sets up a few registers for reasons we‚Äôll explore in the future (right now, you can basically ignore this register setup) and jumps to the address 0x80000000 which is where the action truly begins!\n\n0x80000000 is where the first user-provided instructions to QEMU are running, and they are loaded there as soon as the virtual machine starts. If you don‚Äôt pass anything, QEMU will use the default and load up a piece of software called OpenSBI. The next article in this blog will be exactly what is the concept behind SBI in RISC-V and what exactly OpenSBI is. It‚Äôs important to note that SBI on RISC-V isn‚Äôt really BIOS, but something very similar. My personal guess is that the QEMU authors simply recycled the flag that was available and representing BIOS on other architectures like x86. Anyway, something to keep in mind is that SBI is generally very similar to BIOS in terms of what it does, and more importantly, it is something you can customize.\n\nThe -bios flag is the ELF a binary file containing instructions and potentially some other data, organized in sections. ELF is the standard binary format for Linux, and the details of the ELF file format are way outside the scope of this article, but a sufficient mental model here is that it is simply a key-value map where key is the starting address of a section, and the value is the bunch of bytes that need to be loaded into the memory at that address. Therefore, the ELF file provided to the -bios flag should fill out the memory starting at 0x80000000 (and this is indeed what QEMU‚Äôs default OpenSBI image does).\n\nIf you have been booting an operating system with QEMU before (e.g. Linux), you have likely used the -kernel flag. It is basically the same thing as the -bios flag: you can pass it an ELF image which covers some other memory region, and conceptually it will just dump the bytes in the memory. We won‚Äôt be using this flag today, we‚Äôll cover its usage in the following articles.\n\nEven though conceptually ELF files represent just ways to fill in the memory, they are definitely not super simple that you can write a quick parser in one afternoon. A careful reader may wonder how does the machine then know to parse out the contents mapped to some address 0x12345678 from the ELF file and load the memory with those. This would be a great observation ‚Äî in our case, we are using a virtual machine and we are basically simulating a machine which conceptually has such intelligent digital circuitry or amazingly complex initial software bootloader that is available in the machine‚Äôs memory right upon the powerup. That is, of course, not what happens in the real machines. The software that is loaded upon the powerup is stored on the machine storage as a flat binary blob that is blindly just dumped into the memory upon the powerup, there is really no parsing involved, but since we‚Äôre dealing with a virtual machine here, the sky is the limit, we are not bound by the complexities of manufacturing the hardware that does any of this.\n\nWe have established that 0x80000000 is the location of the first user-provided instruction that the machine executes. I provided it as just a fact, and if you really want a little more background as to why this might be so, you can start from here. Basically, what we see here is that DRAM is mapped to start at the 0x80000000 in the address space (if you don‚Äôt know what this means, don‚Äôt worry, it will not be too relevant for the rest of this article).\n\nLet‚Äôs begin by building an ELF file that will lay some processor instructions at address 0x80000000 that will give the user a message ‚Äòhello‚Äô!\n\nThose who have done embedded systems programming in the past are surely familiar with the concept of UART. UART is a very simple device used for the most basic form of input/output: there is one wire for input (receiving, known as RX) and one wire for output (transmit, known as TX), and one bit goes onto the wire at the time. If you‚Äôre connecting two devices to speak to each other over UART, one device‚Äôs TX is the other device‚Äôs RX, and the other way around. If you‚Äôre reading this article and have not done anything with UART before, I strongly suggest at least getting the cheapest possible Arduino and having it speak to your computer through a USB-to-UART cable. The concept would be identical to what we‚Äôre doing here, but you would be doing it for real, and it will make more sense, since the scenario here is entirely virtualized.\n\nQEMU virtualizes an UART device on the virtual machine, and our software can access it. When you open the QEMU‚Äôs serial port (UART) section, what happens is basically when you press a keyboard button, the code for that button is sent out of your host‚Äôs TX to the VM‚Äôs RX and when the VM outputs something on its TX, it will be rendered to you graphically in the terminal (so you don‚Äôt have to otherwise decode the electrical signals from the simulated board :)), e.g. if the VM sends out 8 bits representing 65, your QEMU will render the character a, since that is its ASCII code.\n\nWe know that QEMU maps UART at the address 0x10000000 (you can check it in their source code) and the device that is virtualized here is NS16550A. The details do not matter here: for the purposes of the article, what this means is if you send an 8-bit value to that address from your software, that will be sent out on the TX wire of the virtualized UART device. Practically, that means if you go to QEMU‚Äôs serial port, the character you wrote to 0x10000000 will be rendered in your console.\n\nWith all this knowledge in mind, now we can write the code. The ELF file we are about to build will lay out a few instructions at 0x80000000 to print the characters, ‚Äòh‚Äô, ‚Äòe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô and ‚Äòo‚Äô in succession to the address 0x10000000. Finally, the code should then get stuck in an infinite loop (so that QEMU doesn‚Äôt crash for any strange reason and we can inspect the output)\n\nYou can save this file as hello.s. Let‚Äôs assemble this file into the machine code. In my case (and likely in yours), I am using a cross-platform toolchain, meaning that I am developing on a platform different from the target platform. Concretely, I am developing this software on an x86 machine and building for a riscv64 machine.\n\nTo assemble this file, I run the following:\n\nThe exact command may be different depending on what kind of assembler you have for riscv64, this is the tool I have obtained through my system‚Äôs package manager for Debian. I leave it to the reader to obtain the correct toolchain for building riscv64 software, it should generally be a matter of just obtaining the right software package from the Internet.\n\nNow, the code is only assembled, meaning that we have the software instructions in the machine code format, but this binary is still not ready to go and act as our fake BIOS. We need to use a linker and drive its behavior with a linker script to ensure that the instructions we have generated will be laid out at 0x80000000 as we intended. Let‚Äôs write the linker script.\n\nWe won‚Äôt be covering what all this means, but tl;dr is we now have a way to put those instructions exactly where we want them. Let‚Äôs verify with objdump.\n\nQEMU can now be fired up by running the following command:\n\nTo see what happens on UART, click the View button in the top menu and switch to the serial port view. The output should be like this:\n\nHead over to the Github repo for this article, run the make command and that will do everything we described above. You can then launch QEMU.",
    "readingTime": 10,
    "keywords": [
      "qemu‚Äôs serial",
      "elf file",
      "uart device",
      "operating system",
      "bare metal",
      "serial port",
      "linker script",
      "bios flag",
      "riscv virt",
      "program counter"
    ],
    "qualityScore": 1,
    "link": "https://popovicu.com/posts/bare-metal-programming-risc-v/",
    "thumbnail_url": "https://popovicu.com/Bare%20metal%20programming%20with%20RISC-V%20guide.png",
    "created_at": "2026-01-10T12:20:47.188Z",
    "topic": "tech"
  },
  {
    "slug": "effect-institute",
    "title": "Effect Institute",
    "description": "Interactive lessons for learning Effect. Master typed functional programming in TypeScript.",
    "fullText": "Feedback No. 7I love it! I can't wait for more. This simple exmaples shows how easy effect might be!‚ñ†Feedback No. 14great! cant wait for next episode.‚ñ†Feedback No. 24Oh dear, this is sooo cool. It brings joy to learning effect. love it. even though I'm already familiar with the basic concepts, going through these again here is such a great experience. love it. keep going! and thank you‚ñ†Feedback No. 15Amazing UX. I want a course on this.‚ñ†Feedback No. 37This is great. Thanks!‚ñ†",
    "readingTime": 1,
    "keywords": [
      "love",
      "effect"
    ],
    "qualityScore": 0.3,
    "link": "https://www.effect.institute/",
    "thumbnail_url": "https://effect.institute/og/home.png",
    "created_at": "2026-01-09T00:58:49.511Z",
    "topic": "tech"
  },
  {
    "slug": "functional-programming-at-the-type-level-in-typescript",
    "title": "Functional programming at the type level in TypeScript",
    "description": "A library of composable functions for the type-level! Transform your TypeScript types in any way you want using functions you already know. - gvergnaud/hotscript",
    "fullText": "gvergnaud\n\n /\n\n hotscript\n\n Public\n\n A library of composable functions for the type-level! Transform your TypeScript types in any way you want using functions you already know.\n\n License\n\n MIT license\n\n 3.7k\n stars\n\n 58\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n gvergnaud/hotscript",
    "readingTime": 1,
    "keywords": [
      "functions",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/gvergnaud/hotscript",
    "thumbnail_url": "https://repository-images.githubusercontent.com/600849732/b995f53a-d30b-4a97-a036-bf0be01300ad",
    "created_at": "2026-01-09T00:58:48.829Z",
    "topic": "tech"
  },
  {
    "slug": "jane-streets-ron-minsky-on-the-future-of-programming-2023",
    "title": "Jane Street's Ron Minsky on the Future of Programming (2023)",
    "description": "Listen in on Jane Street‚Äôs Ron Minsky as he has conversations with engineers working on everything from clock synchronization to reliable multicast, build systems to reconfigurable hardware. Get a peek at how Jane Street approaches problems, and how those ideas relate to tech more broadly.",
    "fullText": "Richard Eisenberg is one of the core maintainers of Haskell. He\nrecently joined Jane Street‚Äôs Tools and Compilers team, where he hacks\non the OCaml compiler. He and Ron discuss the powerful language\nfeature that got him into PL design in the first place‚Äîdependent\ntypes‚Äîand its role in a world where AIs can (somewhat) competently\nwrite your code for you. They also discuss the differences between\nHaskell and OCaml; the perils of trying to make a language that works\nfor everybody; and how best a company like Jane Street can collaborate\nwith the open source community.\n\nRichard Eisenberg is one of the core maintainers of Haskell. He\nrecently joined Jane Street‚Äôs Tools and Compilers team, where he hacks\non the OCaml compiler. He and Ron discuss the powerful language\nfeature that got him into PL design in the first place‚Äîdependent\ntypes‚Äîand its role in a world where AIs can (somewhat) competently\nwrite your code for you. They also discuss the differences between\nHaskell and OCaml; the perils of trying to make a language that works\nfor everybody; and how best a company like Jane Street can collaborate\nwith the open source community.\n\nSome links to topics that came up in the discussion:\n\nWelcome to Signals and Threads, in-depth conversations about every\nlayer of the tech stack, from Jane Street. I‚Äôm Ron Minsky. It is my\ngreat pleasure to introduce Richard Eisenberg. Richard‚Äôs a senior\ncontributor to the Haskell ecosystem. He‚Äôs worked on Haskell‚Äôs type\nsystem in a lot of different ways over the last eight years. He‚Äôs on\nthe GHC Steering Committee. He‚Äôs the Chair of the Haskell\nFoundation. One might wonder, what‚Äôs he doing here? Well, it turns out\nRichard has joined Jane Street. He‚Äôs working on our Tools and\nCompilers team, and in particular, working on the team that works on\nthe front end of the OCaml compiler. So, thanks for joining me,\nRichard.\n\nSo, maybe a good place to start out with is to talk about your\nbackground. You‚Äôve done all this exciting work in the Haskell\necosystem. Maybe tell us a little bit more about how you got involved\nin Haskell and what kind of work you‚Äôve done there.\n\nYeah, sure. I first really encountered Haskell when I went back to\ngraduate school. I‚Äôd been a high school teacher for a little while and\nthen I thought, ‚Äòoh, you know, it‚Äôs time to get a Ph.D. in programming\nlanguages,‚Äô kind of, as you do.\n\nIn the process toward that, I discovered dependent types. I didn‚Äôt\nwanna just show up in grad school and not know anything. So I did some\nbackground reading and I discovered that, you know, these fancy type\nsystems have so much expressiveness, and so much of a way of being\nable to sort of define what it is that you‚Äôre programming before you\neven write a line of code, that you can just express your ideas so\nmuch more clearly and then get the compiler to check that you actually\ndid it correctly. This was just sort of, you know, my brain exploded\nwhen I learned about this.\n\nI feel like, you know, I kind of pretend to know something about\nprogramming languages on TV, but like, actually my academic background\nis distributed systems, not programming languages, and there‚Äôs lots of\ngaps in my knowledge. And so, I kinda don‚Äôt really know what dependent\ntypes are. Can you give a suitably beginner-friendly explanation of\nwhat dependent types are?\n\nSure. And I‚Äôm gonna start by avoiding the trap of trying to define the\nterm, ‚Äòdependent type.‚Äô\n\nAnd instead, just sort of describe ‚Äòa language with dependent types\nallows you to do this thing.‚Äô And that thing is to essentially encode\na proof of correctness into your program such that every time you\ncompile your program, that proof is checked. And so you know‚Äîif you‚Äôve\nset things up the right way‚Äîthat your program does what you think it\ndoes. And so, I‚Äôll give a concrete example.\n\nAnd that is, to imagine a sorting function. So normally we think of a\nsorting function, let‚Äôs just say, it‚Äôs a function from a list of\n‚Äòints‚Äô to a list of ints, right? It takes this list of ints‚Äô input,\nand then does some transformation, and then produces a list. But\nactually, that‚Äôs a very loose description of a sorting function. We\ncan do better than that.\n\nI wanna say that a sorting function takes in a list of ints, and it\nproduces a list of ints in, I‚Äôm gonna say, non-descending order\n(because maybe there‚Äôs duplicates, so I don‚Äôt want to say ‚Äòascending\norder‚Äô because someone out there might say that‚Äôs wrong). So we get\nthis list out that‚Äôs in non-descending order. Okay, that‚Äôs closer, but\nthat‚Äôs still not quite right because maybe my function just always\nreturned to the empty list. That‚Äôs a list in non-descending order.\n\nSo instead, I really want to say that it‚Äôs a function that takes in a\nlist of ints, and returns a result that is in non-descending order,\nand is a permutation of the input list. And that‚Äôs a rather glorious\ntype that I can assign to a sorting function. But it accurately\ndescribes what that sorting function does.\n\nAnd then I can implement that in any number of ways, using any of the\ncommon sorting algorithms that you learn in computer science, and any\nof those different algorithms will meet that specification, and then,\nif I‚Äôve encoded this, my compiler will check that my algorithm\nactually does sorting and doesn‚Äôt do some other thing. And so now,\nhaving sort of explained that, I can return back and actually say what\nI mean by ‚Äòdependent type.‚Äô\n\nA dependent type is a type that can depend on some input that you give\ninto a function, essentially. So, in this case, the output of the\nfunction, not just a list of ints, but the output depends on the\ninput. The output must be a permutation of the input. And so, that‚Äôs\nthe dependent type.\n\nSo when I started grad school, then I saw Haskell, and here was a\nlanguage that had this fancy type system and had all this\nexpressiveness. Didn‚Äôt quite have dependent types, but it had a lot of\nthat stuff. That just got me excited. And then, through that I started\nworking with Stephanie Weirich at University of Pennsylvania. She was\ndoing a lot of Haskell work. She collaborated a bunch with Simon\nPeyton Jones, who then I started collaborating with, and things just\nsort of took off from there. It was a pretty fast introduction into\nthat world.\n\nI wanna go a little deeper on that. I feel like we‚Äôve started with\nthe, like, ‚ÄòWell, of course I got a Ph.D. in PL and was interested in\ndependent types.‚Äô Like, how do you even get there? Where does your\ninterest and background in programming languages come [from] in the\nfirst place?\n\nSo, once upon a time, I wanted to program. I don‚Äôt know why I wanted\nto program. I was too little to have that level of introspection.\n\nSo I guess, uh, I learned Basic when I was, I think, five\nsomehow. There was a friend of my parents that decided I should learn\nBasic and so that happened.\n\nBut I then got more into it, I guess, when I was in 7th grade, so this\nwas, probably I was 12 years old. And someone told me that Pascal was\na good language to learn. So, you know, I pleaded with my parents and\neventually they bought me the Pascal compiler ‚Äòcause back in those\ndays, you actually had to pay for compilers. At least on, sort of‚Äîwe\nhad a Macintosh. If I had some kind of Unix system, maybe it would‚Äôve\nbeen different.\n\nWell, which Pascal compiler did you use?\n\nUm, THINK Pascal? Was that a thing?\n\nTHINK Pascal, so not Borland. I guess Borland was \nDOS side of the world.\n\nYeah, yeah. Borland, yes, Borland was for like, ‚Äòreal programmers.‚Äô\n\nSo I got this, and somewhere along the way, I had decided that every\npiece of software should be, like, if you read the instruction manual,\nthat should tell you everything you need to know about that piece of\nsoftware. I don‚Äôt know why I assumed that, but that was my starting\npoint. And so, it came with these two big books and most of the books\nwere just like on compiler flags and setting up projects and none of\nthis stuff about programming. At the end of one of the books was a\nsyntax reference with no glosses, no further information, just a\nsyntax reference using those, sort of, block and line diagrams to\nrepresent recursive loops in the syntax. And I was like, ‚ÄòOkay, well,\nif my assumption is that this book defines how to program, and this is\nthe only thing it has, then all the information must be here.‚Äô\n\nSo I just stared at these syntax diagrams probably for hours and\nhours. Like, ‚ÄòOkay, what does all of this mean?‚Äô And then eventually\nsort of bootstrapped my way into learning how to program Pascal. And\nso, that being my first ‚Äòreal programming experience,‚Äô somehow that\nbuilt the structures in my brain just around syntax and languages, and\nit kind of went from there.\n\nAnd then fast-forward 10 years or so, and I took a PL course as an\nundergrad from Norman Ramsey and it just lit me up. It was just like,\nthis is really cool stuff. I really enjoy this kind of thinking. And\nit turned out, I took that at the end of my senior year of undergrad,\nand so it was sort of too late to immediately go into grad school at\nthat point and I had this whole machine going toward high school\nteaching. So I did that for a while, but then I ended up returning.\n\nI love that your entr√©e into programming was, roughly speaking, a\ntraumatic experience with syntax. That seems‚Ä¶ (laughs)\n\nBut it wasn‚Äôt traumatic. It was fantastic. Somehow, this was just what\nI needed. This was what my 12-year-old-self needed: big syntax blocks.\n\n(laughs) Okay. So, you went off, you got your Ph.D., you started\nworking on Haskell. Where did it go from there?\n\nWell, I was working on my Ph.D. It was basically, Stephanie and I had\nthis idea about how to encode dependent types, but it required some\nmore support from the Haskell compiler. And so, I was pleading [with]\nthe Haskell people to create this for me. And then, eventually Simon\njust wrote and said, ‚ÄòWhy don‚Äôt you just do it?‚Äô And that was a new\nconcept for me. I was like, ‚ÄòI could influence this language? Well,\nyou know, I‚Äôm just this random person.‚Äô\n\nBut between Stephanie and Simon mentoring me, I got to a point where I\ncould contribute and then it just grew from there. So, [I] did a bunch\nof that as a Ph.D. student. Then after leaving my Ph.D., I was a\nprofessor for a few years at Bryn Mawr College. What was great about\nthat experience was, they were open to the idea that open source\ncontributions were a form of publication, right? A lot of professors\nhave a lot of pressure to keep producing papers. That was definitely\nthere for me, too. But alongside papers, I couldn‚Äôt just do\nsoftware. But I could do, sort of, traditional scholarly publications\nand software. So that gave me a chance to continue my implementation\nwork, remain a part of this open source community, while still doing\nthe high-level scholarly research that I had been doing. So that was a\nreally nice part of it.\n\nIt turned out that, to my great surprise, that‚Äîwell, this wasn‚Äôt to my\ngreat surprise‚Äîit turned out that this was a lot of work. That\nmaintaining a research program with a bunch of papers every year, and\ndoing these open source contributions, and teaching, there was just\nmore than could comfortably fit into one job, especially during term\ntime. And to my own surprise, I realized that the teaching part was\nthe part that maybe I could pause on. I had done that full-time as a\nhigh school teacher for eight years, now I was back doing it. It was\nfun, but I thought it was time to sort of push in a slightly different\ndirection.\n\nSo after a couple years at Bryn Mawr, I left to join Tweed which is a\nsoftware consultancy based in Paris. They do a ton of Haskell\nwork. And they hired me, basically just to continue my contributions\nin the Haskell space. So, I continued doing research there, continued\ncontributing to the compiler there. And, you know, after a few years\nof that, that was fun, and I got to a point where I was looking to be\npart of a cohesive team all working together, and realizing that after\nso many years just working in one language, I was really ready to see\nwhat things were like in a different space. And so the timing worked\nout really well when you, Ron, reached out to me and started the\ndiscussions that led to me joining Jane Street.\n\nRight. And often in these interviews, I‚Äôll ask someone, like, ‚Äòso how\ndid you get here?‚Äô But actually, I was part of the dramatis personae\nof how you actually arrived.\n\nYou were the‚Äîthe dramatis personae, yeah. (laughs)\n\n(laughs) Right, which is, at some point, I was like, ‚Äòoh, it would be\nreally good if Richard worked here,‚Äô and I started reaching out and\ntrying to convince you to come. Which took, it‚Äôs important, I\ncalculated, 11 months from (laughs) when I started.\n\nYeah, yeah. That‚Äôs right. I mean, you know, of course the story [of]\nme coming to Jane Street starts with you asking me, and me saying\n‚ÄòNo.‚Äô\n\nAnd again, it was a matter of timing. I‚Äôve been aware of Jane Street\nfor a long time, been sort of wondering what was happening over there,\nand somehow when you first reached out, it just wasn‚Äôt the right time\nfor me yet. And then the second time, it was sort of a better time. I\ncouldn‚Äôt even tell you why. It‚Äôs just, this is the way that life\nflows.\n\nBut, you know, one of the barriers which we‚Äôve joked about is that I\ngrew up in the New York area and actually ended up going to high\nschool in New York and commuting from New Jersey. So every day I was\ngoing back and forth across the George Washington Bridge, and swore to\nmyself then that I would never commute to and from New York City. And\nalso just sort of had this negative vibe from the financial industry,\nand I was just like, ‚ÄòI don‚Äôt know. Like, I want to be creating\nsomething that other people use,‚Äô right?\n\nBut actually, this is fantastic. I‚Äôm having great fun. Part of it is\nthat I‚Äôm working as part of this larger ecosystem. It‚Äôs not just a\nclosed thing where I‚Äôm only doing stuff that‚Äôs internal to Jane\nStreet. But instead, contributing to this open source ecosystem,\nworking on an open language, OCaml, and really continuing some of\nthese research trends that I started 10 years ago. Now that I‚Äôm here,\nit just all fits very naturally together.\n\nI feel like, in general, one of my takeaways is that there‚Äôs lots of\ninteresting work in lots of different places and the preconceptions\nyou have when you are young often (laughs) don‚Äôt work out over\ntime. So a thing you talked about earlier is a topic that I feel like\nI should know something about, but don‚Äôt. I like your first high-level\ndescription ‚Äòcause it actually lines up with the very little bit I\nknow about dependent types.\n\nAnd there‚Äôs also something that‚Äôs always made me a little nervous\nabout them because a basic fact about proofs is‚Äîproofs seem pretty\nhard to construct. People go to great trouble to build automated\nproving systems where you have tactic libraries, like, little tiny\nprograms that you can run to, like, ‚Äòtry this way or try that way‚Äô of\ncoming up with a proof of a thing. And it all sounds really hard and\nwhen you talk to people who do kind of theorem proving on large scale\nsoftware systems, it sounds like a pretty challenging operation where\nthis is kind of a lot of engineering work that just goes into just\nconstructing the proof. So I‚Äôm curious how you think of the\npracticalities of that? Like, how convenient can you make it? And if\nyou make it a deeply embedded part of the programming process, does\nthat work well with the other things you might wanna do? Does it\ninterfere with the ordinary process of programming? How do you see the\npragmatics of that working out?\n\nYeah, that‚Äôs a great question. And this is a question that doesn‚Äôt\nhave an answer yet. This is, I think, still something that the\nprogramming languages community is grappling with. I have my own\ntakes, but this is still sort of an open question out there.\n\nI think, first of all, that for a programming language to be\neffective, everything needs to be opt-in, right? If you have a\nlanguage that supports dependent types‚Äîand there‚Äôs a bunch out\nthere. So actually, OCaml‚Äôs module system is a dependent type\nsystem. It‚Äôs missing one or two key features that makes it really sort\nof go. But that definitely has aspects of dependent types in\nit. Haskell has aspects of dependent types.\n\nThere are other languages, Coq, Agda, Idris, Lean, these are the ones\nthat really have embraced dependent types. But in any of these\nlanguages, you don‚Äôt have to use these dependent types. You can just\nwrite a sorting function that is described as ‚Äòa list of ints to a\nlist of ints.‚Äô That‚Äôs fine. And I think that‚Äôs a key aspect of any\nkind of system that‚Äôs gonna be using this dependent types feature,\nbecause not everyone wants to prove everything all the time. As you\nsay, proofs are expensive. And so, what you need is, you need the\nability to only prove the parts that you think are worthwhile proving.\n\nSo let‚Äôs say we have an important part of the system that might be\neasy to get wrong that we want to prove something about. How do we do\nthat? Is that still worth it? And one thing to think about is, what‚Äôs\nyour cost-benefit analysis, right? If you do this proving, can you\nspend less time on testing? Can you spend less time on debugging? Can\nyou spend less time on catastrophic failure because you missed\nsomething when debugging?\n\nIt is a lot harder to do these programs with proofs. But sometimes,\nthat‚Äôs really worth it. It‚Äôs a matter of applying good taste, like it\nis everywhere in programming, right? Any programming language gives\nyou a ton of tools to work with, and it takes time and experience and\ntaste to figure out when it‚Äôs time to use which tools.\n\nRight. And I do really like that, kind of, pay-as-you-go kind of\nattitude of, like, you want some kind of base language that‚Äôs\nrelatively simple to work with, and then the ability to add on more\nfeatures that give you more power of various kinds, and not be forced\nto pay for that complexity constantly in everything you do. Which is,\nI think, actually a really hard thing to get right in the design‚Äîof\nlike, not having these more powerful features leak in and corrupt the\nrest of your experience within the language.\n\nThat‚Äôs right. It‚Äôs a very careful thing. And my push in Haskell has\nbeen to try to increase or add new features to help support dependent\ntypes, and there‚Äôs been a large community of people really excited for\nthis. And in my measurement, my biased measurement, a smaller\ncommunity of people who have been saying, ‚ÄòNo, no, no. Don‚Äôt do this\nbecause it‚Äôs gonna make it impossible to write simple programs.‚Äô And\nmany, many, many times, I‚Äôve had to reassure people, ‚ÄòNo, no. All\nthose simple programs will continue to exist, they‚Äôll continue to\nwork. You can do what you want.‚Äô But I also wanna give power to people\nwho want to use the fancy dependent types.\n\nGreat. Okay. So let‚Äôs leave Haskell and dependent types behind for the\nmoment because that‚Äôs not primarily what you‚Äôre working on here. Maybe\nyou can tell us, what are you working on here? Like, what is the\nmission that you are on here, working on OCaml?\n\nSo, I‚Äôll start more broadly and then I‚Äôll get narrower. My broad\nmission is my belief that type systems are a fantastic way to\nstructure thought and structure programs. So by being able to write\ndown what you intend to do before doing it, then you can check what\nyou‚Äôve done, and you can also help formulate your thoughts. And when I\nsay, ‚Äòwrite down what you intend to do before doing it,‚Äô I mean, write\ndown a specification in types.\n\nThe flip side of that, is that it must be the case that you can do\nthat without paying a one-time cost. It‚Äôs gonna be problematic if you\nwant to write an efficient program, and yet, by defining your program\ncarefully ahead of time, now it‚Äôs no longer as efficient at\nruntime. This checking and this writing of a specification should all\nhappen at compile time. This should be happening in front of the\nprogrammer and not in front of any users. If any of those details leak\ninto user land by slowing your program down or somehow interfering\nwith the operation of your program, then there‚Äôs a misdesign in the\nlanguage.\n\nSo coming down to what I‚Äôm actually working on here at Jane Street, so\nthe feature I‚Äôm working on is something called unboxed types. There‚Äôs\nvarious different ways of characterizing it, but one way to do so is,\nit allows inlining of type definitions. So what we might want to have\nin our design is to have a structure that has four fields that\ndescribe some aspects of a trade. And then maybe we store that\nstructure in some larger structure, and maybe that‚Äôs stored in some\nlarger structure.\n\nWell, if you do that the na√Øve way in OCaml, every time you store one\nstructure inside of another, it‚Äôs stored by a pointer. And it means\nnow when I wanna access the price of this trade that‚Äôs all the way\ndown (it‚Äôs a nested structure) I have to follow this pointer, then\nthat pointer, then that pointer, then that pointer, each time I have a\ncache miss, each time I have to load into memory. This is really bad.\n\nRight. And this highlights a kind of basic fact about OCaml, that it\nhas a very fancy, at least on the scale of modern language that people\nmight use, a very fancy type system and a brutally simple memory\nrepresentation. OCaml‚Äôs internal implementation might have been like a\nScheme or Lisp written in the mid-‚Äò60s or something, which is to say,\neverything looks like either some immediate value, it‚Äôs no bigger than\na word that fits in a register, or a heap-allocated value, which is\nlike one header word and then one full word for every slot inside of\nit, and we have all the fancy, like, type goo on top of it that you\nwant, but all of the physical representation looks just like that.\n\nExactly. Exactly. It means that we‚Äôre building this beautiful type\ndiscipline in our software to try to avoid errors, and try to express\nourselves, and try to communicate more clearly among developers‚Äîand\nwe‚Äôre paying for that at runtime! This is ridiculous. To me, every\ntime a type system feature slows down your program or otherwise\ninterferes is just a misdesign. And so, the fact that this is our\ncurrent state of play is problematic. The task I‚Äôm working on is\ncoming up with a new design for OCaml‚Äôs type system, or this aspect of\nOCaml‚Äôs type system, that allows us to have this rich type structure\nwithout paying for it at runtime.\n\nThat‚Äôs right. Although it‚Äôs not that you can pick a type structure\nthat‚Äôs kind of oblivious to the performance details, right? In some\nsense it‚Äôs the opposite. The idea of the unboxed type work and a lot\nof the work that we‚Äôre doing now is really about taking design\ndecisions that have to do with performance and making them explicit at\nthe level of types.\n\nYes, that‚Äôs right. Because you can‚Äôt, I mean, in the end, we need to\nexecute this somehow. There needs to be an execution model. And so, if\nwe try to ignore all of these execution details in the type system,\nthen there‚Äôs not gonna be a way of making it fast. And so, the work\nis, in some sense, taking some of these execution-oriented details,\nmaking them manifest in the type system, getting programmers the\nability to write types that take these into account, so that we can\nthen execute fast.\n\nRight. In some sense, I think of this as really being about enabling\nperformance engineering, right? There‚Äôs kind of two different lines of\npeople thinking about how to optimize programs via compilers. One is,\nyou make your compiler smarter so it can do more and optimize things\nfor you. So the user can write kind of an ordinary program that just\nreflects, ‚Äòwhat do they want to achieve and what‚Äôs the logical\nstructure,‚Äô and then the compiler will, like, do something good. And\nthen, the other direction is trying to give a lot of precise\nunderstanding to the programmer so they can know exactly what they‚Äôre\ndoing and they can optimize their program. And these are both\nimportant.\n\nBut, like, in some sense the scale of optimizations are very\ndifferent. Like, if you come up with a way of making your compiler\nfaster that, like, takes most user programs and makes them 20% faster,\nthat‚Äôs an enormous win. Like, that‚Äôs a shockingly good\noutcome. Whereas, if you give people good performance engineering\ntools, the idea that they can think hard about a particular program\nand make it five, or 10, or 100 times faster is like, in some sense,\ntotally normal. And so, there‚Äôs a kind of extra power of giving\nindividuals the ability to optimize. It obviously misses out on scope\nand there‚Äôs something great about being able to go and in one swell\nfoop (laughs) make 20 different, or 100 different, or all the\ndifferent programs faster. But you can‚Äôt get the same kind of order of\nmagnitude of improvements that you can get from giving one programmer\nthe ability to really carefully engineer their system.\n\nWell, there‚Äôs a real tension because what in the end we‚Äôre doing in\nprogramming languages is, we‚Äôre trying to find a way for humans to\ncommunicate precisely to computers. And we have at our disposal our\ntool, one of our tools is the fact that this is happening on a\ncomputer. So we can write computer programs to try to sort of help\ncover this distance, right?\n\nIn the early days, humans were just completely at a disadvantage and\nthe humans wrote in assembly code. And this was kind of terrible. And\nwe‚Äôve come some way since then, and so now instead of writing sort of\ninstruction-by-instruction that‚Äôs gonna be executed by the processor,\nwe can write in a high-level language and have a compiler translate\nthat high-level language to something that the computer can\nexecute. Hopefully along the way, it can actually make that\nfaster. But sometimes, we‚Äôve gotten ourselves sort of too\nhigh-level. If we abstract away too many of the details of execution,\nit‚Äôs exactly as you were saying. It means that the programmer can no\nlonger do the performance engineering. And so, we need to expose just\nenough to the programmer so that now there‚Äôs some more details that\nthey can think about. Now they can do that performance engineering.\n\nBut still, I think the burden sits on the compiler to take those few\nhints that the programmer gives to make blazingly fast code come out\nthe other end. And it‚Äôs just really about this interplay between how\nmany details do we wanna expose? Certainly not all the details because\nthat‚Äôs too hard for the programmer. But we need to expose some. And\nso, it‚Äôs about, over time, programming languages developing more and\nmore features that sort of figure out the right line and the right\nbalance between this desire for high-level languages, closer to human\nthought, and low-level details, easier to execute.\n\nSo, before when we were talking about dependent types, we were talking\nabout how it‚Äôs important to have this kind of pay-as-you-go\nphenomenon, like, it gets harder as you try and get more power. It\nfeels like the same kind of thing shows up here.\n\nSo, when you try and get more control over the shape of data in the\nway that you described, what are the trade-offs? And how do you think\nabout designing the language in such a way that you don‚Äôt have\nwhatever extra complexity is there infect everything that you do?\n\nWell, we have to be really careful. So, with unboxed types, one of the\nchallenges is gonna be that unboxed types interfere with\npolymorphism. So, in a language like OCaml, you can write a function,\nsay, that computes the length of a list. And that‚Äôs gonna work over a\nlist, no matter what the list contains. And so the way that we say\nthat, is that in the type of the length function, there‚Äôs a type\nvariable. We say that it‚Äôs an ‚Äòalpha list‚Äô where alpha can stand for\nany type that you might wanna put in there. Once we introduce unboxed\ntypes, that no longer works out. Some of these types won‚Äôt be suitable\nreplacements for alpha. And so it means that, by having these unboxed\ntypes in the system, now this really powerful feature of polymorphism\nbecomes constrained. And so, algorithms that people are used to being\nable to use everywhere, no longer work so well.\n\nSo one of the challenges that we have is, how can we recover that? How\ncan we still keep this nice feature of polymorphism and without having\nunboxed types interfere? And so, one of the things that we‚Äôre thinking\nabout doing is some amount of essentially what could become runtime\ncode generation. It comes down to building experience with the\nfeature. As we start understanding this language feature better, we‚Äôre\ngoing to create programs using these unboxed types, recognize areas in\nwhich the current programming paradigms that we engage in no longer\nwork, and then figure out ways around them. It‚Äôs this give and take as\nwe‚Äôre doing the language design.\n\nAnd just to focus in on this code generation thing for a second, the\nbasic issue here is that the way you make in OCaml today, separate\ncompilation of work, and make the ability to write a function in one\nplace and use it polymorphically on lots of different types is, again,\nyou‚Äôre just kind of taking advantage of this rock stupid memory\nrepresentation. Like, what‚Äôs a list? A list is a collection of\nheap-allocated values, every one has, like, two slots. One is a place\nto put the data and the other is a place to have a pointer to the next\nthing on the list. And when you iterate over it, that thing always has\nthe same shape, it always has the same number of bytes, and so you\ncould just write one piece of code that kind of uniformly walks over\nit. And what happens in the unboxed types world is, suddenly you want\nthe system to contemplate multiple different possible shapes of the\ndata. And so, kind of at a mechanical level, you need different\nmachine instructions in order to do the walking of the data structure,\ndepending on what that structure is.\n\nAnd so, that‚Äôs how you might end up with wanting code generations. The\nway to kind of claw back the polymorphism is to generate the\ncode. Maybe you can generate the extra code at compile time, maybe you\nhave to generate the extra code at runtime. But you somehow now have\nto generate code for multiple different scenarios that essentially\nrepresent different physical shapes of the data.\n\nExactly. And right now, our best thought is that this will happen at\ncompile time, that we‚Äôll be able to figure out what code we need to\ngenerate at compile time. That will prevent a few programming idioms\nthat someone might potentially want. I don‚Äôt think anyone will ever\nwant those (laughs) idioms in practice so I think we‚Äôll be able to get\naway with it. But it does mean we‚Äôre thinking carefully about this\ndesign and it‚Äôs a hard thing. I‚Äôm not aware of another language that‚Äôs\nreally tackled the problem in the way that we expect to tackle it over\nthe next few months.\n\nSo another language I think is interesting to compare to, in all of\nthis, is Rust.\n\nSo Rust is a language which tries really hard to do two things at the\nsame time which are not trivial to do at the same time. One is to give\nstronger sense of type safety so that you can write programs that\nyou‚Äôre confident aren‚Äôt going to crash, so you get lots of protection\nfrom bugs from the type system. And also give the user a lot of\ncontrol over low-level details over how memory is managed. And we‚Äôre\nessentially trying to push OCaml in some sense at that high level of\ndescription in the same direction.\n\nHow do you think the Rust approach differs from the approach you see\nus trying to take with OCaml?\n\nThis is a good comparison. This takes us a little bit away from\nunboxed types and polymorphism because I think the way that that\nappears in Rust is quite different. But the notion of this sort of\nfiner level of control is definitely accurate. But Rust gives up\nsomething really big to do what they do. They give up garbage\ncollection. Within Jane Street and generally in programming, I think a\nlot of the time garbage collection is fantastic, right? It allows us\nto program without worrying about these low-level details that for\nmost programs, we don‚Äôt need to.\n\nSo, for instance, when I‚Äôm writing the OCaml compiler‚Äîwhich is written\nin OCaml, of course‚ÄîI do want it to be performant, but I don‚Äôt really\ncare about microsecond latency. And so, I‚Äôm happy to write my compiler\nin a way that allocates a bunch of memory and then when it‚Äôs done with\nthat memory, the garbage collector comes and gets rid of it. This is\nreally, really convenient. In Rust, we don‚Äôt have that. In Rust, we\nare forced to think about memory, allocation, and deallocation at\nevery point in time. And this gives us this fine level of control, but\nit comes at a real cost, and it means now we‚Äôre sort of fighting\nagainst this ‚Äòpay-as-you-go principle‚Äô that we were talking about\nearlier. In Rust, for memory, you don‚Äôt pay as you go. Everyone has to\npay all the time for Rust fine memory control. So we wanna keep the\nhigh-level, garbage-collected OCaml for most applications, and then\nwhen we really want to have this fine level of control, we wanna have\nthat, too.\n\nAnd so, some of the work that we‚Äôre doing is around adding that finer\nlevel of control, but just where we need it.\n\nSo, one thing I wonder is whether this pay-as-you-go principle really\napplies in general or whether it‚Äôs like itself a kind of trade-off? I\nthink, if you‚Äôd asked the question of like, ‚Äòwell we should think of\ntypes in general as pay-as-you-go.‚Äô Maybe your baseline should be by\ndefault untyped, and then when you want some extra control, you should\nadd types to it and lock it down more.\n\nThat seems in principle reasonable. But all the languages that I‚Äôve\ndealt with that have pay-as-you-go types or what you might call\ngradual type systems, actually inherit quite a lot of complexity and\nproblems from that choice. It‚Äôs often simpler and better, at least for\nsome kinds of problems, to have a uniform approach where you sort of\nfigure out what trade-offs you wanna do and you kind of apply those\ntrade-offs uniformly. And I think, at least from my perspective, the\nbase choice you make in a language like Haskell or OCaml of like,\n‚Äòactually, even though it has some ups and downs, we‚Äôre just going to\nhave types everywhere and that‚Äôs just how it‚Äôs gonna be,‚Äô I think is\nat least a reasonable place in the trade off space of designs.\n\nAnd I wonder if the same is true about this kind of thing about\npay-as-you-go control? Whereas, I can imagine for some kinds of\napplications, you would want the property that, ‚Äòyeah, I am explicit\nabout memory everywhere. I kind of have a hard guarantee that anywhere\nI look I have all of this extra information.‚Äô I don‚Äôt know. How do you\nthink about this question of where pay-as-you-go is the right way or\nthe wrong way to think about designing something?\n\nI think that‚Äôs a great question and I agree. I mean, pay-as-you-go\ntypes, people have wanted that for some time. It hasn‚Äôt ever really\nhappened. And, I mean, you could almost say that C++ and C have\npay-as-you-go types and that they have type systems, but the type\nsystems are sort of completely broken.\n\n(laughs) Pay-as-you-go but then what do you get? I like that.\n\nWell, right, but, I mean, just to clarify for our listeners. What I\nmean by ‚Äòcompletely broken‚Äô is that at any point in time in C and C++\nyou can just say, ‚ÄòOh, ignore the type system. And I‚Äôm gonna just take\nthis thing and change its type to be something else.‚Äô And the\nlanguages, you know, they take this stance of, trust the programmer,\nthat when the programmer decides to do this, that this is a good\nidea. But we lose the guarantees that one would want from a type\nsystem like that when you dereference a pointer that there‚Äôs actually\nsomething there when you dereference it.\n\nSo, getting back to memory control, I like the idea of pay-as-you-go\nthere, or maybe it‚Äôs not even pay-as-you-go as much as a language that\ncan have multiple different modes of use with very easy interop. In\nthat we can have a system where there‚Äôs not fine memory management,\nthat you just sort of use the current OCaml‚Äôs system of uniform memory\nrepresentation, and then another part where maybe this particular team\nor this particular file says, ‚ÄòNo, no. Here I really want careful\nmemory management.‚Äô This is a different kind of pay-as-you-go. It‚Äôs\nsort of at the file level or something like that, or the package\nlevel, I suppose. And then as long as one piece of code can easily\ncall the other, that also works well.\n\nWe also see this kind of idea coming out in polyglot systems where we\nactually have multiple languages all calling one another because maybe\nthis part of the system is easier to write in some language, that part\nof the system is easier to write in some other language. It‚Äôs all a\nway of, these sort of different approaches to attack the same problem\nthat we don‚Äôt wanna have all of this complexity. We want to sort of\nnarrow down what complexity we have, where.\n\nRight. So there‚Äôs almost a notion of different dialects within the\nsame language?\n\nWith a really high quality FFI that really captures all of the\nimportant details so you can kind of interoperate, right? In fact, the\nwork of polyglot stories are actually really challenging to\nengineers. There‚Äôs like, a ton‚Äî\n\n‚Ä¶ of hard work. I mean, we‚Äôve done a bunch of work trying to get the\nPython-OCaml interop story working just right. We‚Äôve made good\nprogress there but it‚Äôs actually quite challenging. And I feel like\nthe work is quadratic in the number of different (laughs) languages\nthat you need to hook together ‚Äòcause it‚Äôs often unique, weird issues\nthat come with every pairing of things that you wanna make all one to\nthe other.\n\nI think that‚Äôs right. And that‚Äôs one of the appeals of having one\nlanguage that maybe is flexible enough to operate in multiple\ndifferent modes. So, in particular, in our design for unboxed types,\nwe are planning to make a box operation explicit. And the box\noperation is going to be ‚Äòthe spot‚Äô that allocates memory. And so,\nnormally, you‚Äôll be able to get uses of this box operation for\nfree. Programmers won‚Äôt ever notice it, they won‚Äôt write it. It just,\nall the existing code will have all the boxes inserted. But it would\nbe really easy to imagine some kind of compiler setting that turns\nthat feature off. Meaning that every time you wanna allocate memory,\nyou have to explicitly ask for it.\n\nSo this doesn‚Äôt bring you all the way to Rust, in that there‚Äôs still a\ngarbage collector operating, you still don‚Äôt have to manually\ndeallocate memory. But if I wanna write a program that absolutely,\nwhere every allocation is known, we haven‚Äôt designed this out. Maybe\nwe‚Äôre gonna go there, maybe we‚Äôre not gonna go there. Nothing is for\ncertain. But it turned out that in our design, it just naturally fell\nout that we could just add this compiler flag that just turns off\nboxing and it would be super easy to implement and we could experiment\nto see how easy it is to work with.\n\nSo, here we‚Äôre talking about a lot of very ambitious changes to\nOCaml. One uncomfortable fact about all of this is, like, we don‚Äôt own\nOCaml, we are not the (laughs) primary upstream developers. Every time\nwe wanna make a change to OCaml, there‚Äôs a bunch of people who we have\nto convince that it‚Äôs a good change. So one of the things that you‚Äôre\nthinking about in particular is, the kind of relationship of the work\nthat we‚Äôre doing here and our connection to the larger OCaml\ncommunity. Say a little bit more about how you think about this whole\nprocess.\n\nSo, I wanna start by pushing back against ‚Äòunfortunate fact.‚Äô I don‚Äôt\nsee that as an unfortunate fact at all. So OCaml was born as a\nresearch project out of Inria in France and still is there. That‚Äôs\nsort of its beating heart. And we are really significant benefactors\nof that work that others have done, that others continue to do. And\nso, Jane Street has chosen to work in part of this open source\nlanguage community where we‚Äôre taking others‚Äô ideas and we‚Äôre\ncontributing our ideas back.\n\nSo in this podcast and in our work, we have these grand ideas. ‚ÄòOh,\nwe‚Äôre gonna add this to OCaml. We‚Äôre gonna do this, we‚Äôre gonna do\nthat. Of course, what I really mean is, we‚Äôre going to experiment with\nthis idea internally. And then as that experiment unfolds, as we gain\nexperience and become more sure that it‚Äôs a right‚ÄîI shouldn‚Äôt say a\nright idea. There‚Äôs many right ideas. But as we gain more confidence\nin a particular design, we can then work with the rest of the OCaml\ncommunity to try to make this part of everyone‚Äôs OCaml and not just\nJane Street‚Äôs.\n\nAnd in that way, we‚Äôre giving back to that community. At the same\ntime, we‚Äôre continuing to reap rewards from other work happening in\nthat community. So the biggest example of which is the multicore\nsupport that‚Äôs coming in OCaml 5. Still gonna be some time before Jane\nStreet‚Äôs ready to upgrade to OCaml 5. But that was a huge pile of work\ndone mostly outside, or maybe entirely outside of Jane Street‚Äôs walls,\nand we benefit by being part of this open source ecosystem. And so, I\nthink that this is a really great place to be where we‚Äôre\nparticipating, we‚Äôre getting new input of technical content, like\nmulticore OCaml, as well as design ideas. And I‚Äôm even thinking of\nsomething just a few weeks ago where I found an infelicity in the\ncompiler, made an improvement, pushed that upstream, and then we got\nfresh ideas from folks outside of Jane Street‚Äôs walls about, ‚ÄòHere‚Äôs\nactually an even better way to do it,‚Äô and that was great. And then we\ncould incorporate that here. Without being part of this open source\necosystem, we wouldn‚Äôt have gotten that insight and we would be poorer\nfor it.\n\nSo, in the Haskell world, you spent a lot of time working on exciting\nnew type system features for Haskell and now you‚Äôre doing, in\nsubstance, similar kinds of work here. But I think that work flow is\npretty different, right? I think then, in the Haskell world, it was\nlike the primary work and ideation and evaluation was all deeply\nintegrated from the beginning in the open source world. And here,\nwe‚Äôre doing this work where we‚Äôre still open source, still, you know,\nyou can go look at the results on GitHub. But we are mostly iterating\ninternally, and then, over time, as we gain experience, as we gain\nmore confidence that the things we‚Äôve built are good ideas, working to\nsee what subsets of these we can get upstreamed. I‚Äôm curious how that\nhas changed your feeling about the work, how it changes the texture of\nthe work that you do?\n\nSo working within the context of Jane Street gives us a lot more\nopportunity for experimentation. So a real big challenge of doing\nlanguage design is that, by necessity, you come up with an idea and\nmaybe you think it‚Äôs a good idea and you experiment with it on a few\nsmall programs on your own machine or you get your friend to sort of\nexperiment. You bounce the idea. Maybe, you know, you‚Äôre even\nproactive and you convene a committee of experts on this idea and now\nyou have, you know, five people thinking really hard about this one\nidea and trying to come up with the absolute best design.\n\nBut then, once you come up with the design, if you‚Äôre just in an open\nsource environment‚Äîwithout the context of a place like Jane Street‚Äîyou\ndevelop the idea, implement it, and release it, and then now you have\nthousands or hundreds of thousands of programmers using it, and maybe\nit was a bad idea. But by the time you discover that it was a bad\nidea, it‚Äôs a little bit too late because maybe you have 90,000\nprogrammers who think it‚Äôs a bad idea, but you have a thousand who\nthink it‚Äôs a fantastic idea and will be very, very, very upset if you\nbreak their programs by changing it. And now you‚Äôre in a bad way.\n\nAnd even the people who think it‚Äôs a bad idea, who would like to get\nrid of it, don‚Äôt necessarily want you to break their programs in the\ninterim before they‚Äôve (laughs) stopped relying on the feature.\n\nThat‚Äôs right. And so, it‚Äôs really, really hard to make changes to a\nproper open source programming language. In the context of Jane Street\non the other hand, it‚Äôs‚ÄîI don‚Äôt wanna say dead easy. There‚Äôs work\ninvolved, but it is tractable and somewhat easy in that we can develop\nan idea, push it out there, get other programmers in the firm to start\nusing it, and then as that happens, we can say, ‚ÄòMm. This isn‚Äôt quite\nworking out the way that we thought it would.‚Äô That, you know, theory\nmeets reality, and theory loses.\n\nAnd so, then we can make a change to the feature and we have sort of\nan operation that we use internally called a ‚Äòtree smash,‚Äô where some\nJane Street engineers work together to come up with a big diff that\nhappens to all of Jane Street‚Äôs code, all at once, and we can change\nthe spelling of a keyword if we want to. And in fact, we‚Äôre imagining\nthis now. So Jane Street has been working on a feature called ‚Äòlocal\ntypes‚Äô or ‚Äòstack allocation.‚Äô These two things are kind of the same\nthing. And we‚Äôre realizing that one aspect of the design was just a\nbit wrong. And there‚Äôs already a ton of code written using the old\ndesign, but we‚Äôve invented a new keyword and a new place for the\nkeyword to appear, and all this stuff.\n\nAnd so, it‚Äôs gonna be some work to fix it, but there‚Äôs no part of us\nthat‚Äôs saying, ‚ÄòOh no. Now we need to think about a migration plan and\nwe need to make sure that no one is too unhappy with this.‚Äô We‚Äôre just\ngonna go do it. And so, it means by the time we get to upstreaming,\neverything is battle-tested. And so, it just increases the level of\nconfidence in the design when we go through that process.\n\nOne of the concerns I could see people having about this kind of\nprocess is, it‚Äôs going to do a lot of training on, ‚ÄòWhat are the\nrequirements within Jane Street?‚Äô But there are all sorts of ways in\nwhich you could imagine the requirements in Jane Street and outside of\nJane Street being different. I‚Äôm curious how you think about the\ndesign process in a way where you end up with language features that\nare likely to be of quite general utility, ‚Äòcause I think, in the end,\nthe only ones you‚Äôre gonna get accepted upstream are the ones that\npeople think are more broadly useful than just being useful for us.\n\nSo that‚Äôs indeed a challenge, right? It‚Äôs a challenge of overfitting,\nright? We have a very particular style of code within Jane Street and\na very particular use case that we‚Äôre working on. I think that is sort\nof a harder question to answer in some sense. I think one way that we\naddress that is by seeking outside feedback from the very\nbeginning. So, the plan that we have for unboxed types, another Jane\nStreet engineer, Stephen Dolan, he made a presentation about this\ndesign for unboxed types, I think in 2019. We‚Äôve been thinking about\nthis for a long time. The local types and stack allocation feature,\nthis has also been featured in a number of presentations that we‚Äôve\nmade to the OCaml community, out in public.\n\nAnd so, by incorporating that process early, we can get broad\nfeedback. That‚Äôs not the kind of feedback that tells us, ‚ÄòOh, should\nthe keyword go here, or should it go there?‚Äô Right? That‚Äôs the kind of\nthing that we develop internally. But I think that, by giving these\npresentations, by involving the broader community, even from the very\nbeginning, that helps to prevent some of this overfitting problem.\n\nSo, another interesting aspect of your background is, you‚Äôve spent a\nlot of time, years really, working in the Haskell community and\nworking on Haskell. And now, you‚Äôre working on a totally new\nprogramming language using it both as the surface language in which\nyou write your programs, and also helping to design that language. I‚Äôm\ncurious, what struck you about difference? I‚Äôm curious, what are the\ndifferences that have struck you about Haskell and OCaml at multiple\ndifferent levels?\n\nIt‚Äôs been really fun over the past, I guess, six months now being part\nof these two language communities. So, I should say, I‚Äôm still quite\nactive in the Haskell world. I have not left the Haskell world. I‚Äôm\nliving both Haskell and OCaml on a daily basis. So let me think of a\ncouple of interesting points of comparison.\n\nSo, one is, it seems kind of simple, but actually I think it affects\nthe way that we program. The approach toward interfaces. In Haskell,\nyou tend to write your type signatures sort of right next to the\nfunction and Haskell uses a lot of type signatures. OCaml doesn‚Äôt so\nmuch. And so, OCaml there‚Äôs these separate interface files where you\ndefine the interface to your whole module, but it‚Äôs kind of apart and\nit means, in my experience, looking at a bunch of OCaml code. OCaml\ncode tends to be rather less documented than similar Haskell code, and\nI think that‚Äôs in part because in Haskell, you put your types right\nthere and that‚Äôs a form of documentation. And once you put your types\nin, then you can also put more documentation there and it becomes part\nof the habit of the Haskell programmer to put those comments right in\nthere.\n\nA flip side of this, is that Haskell has this feature called\ntypeclasses. And typeclasses allow you to use essentially one\nfunctioning to mean a variety of different things. OCaml doesn‚Äôt have\nthat feature. And so, what that can mean sometimes is that Haskell\ncode can become quite a bit harder to understand because if you have a\nbunch of these function names, any of which can mean a variety of\ndifferent things, all strung together, then it takes a lot of work on\nthe part of the reader to try to understand what on earth is going on\nthere. And that problem just doesn‚Äôt arise in OCaml. OCaml is much\nmore explicit about what operations it‚Äôs taking.\n\nSo maybe I can even, taking these two examples, generalize that a\nlittle bit and say, I find Haskell to be somewhat more explicit about\nwhat‚Äôs happening at compile time. Whereas, OCaml is somewhat more\nexplicit about what‚Äôs happening at runtime. So another place where\nthat comes into play is that, the Haskell compiler‚Äîand this connects\nwith a topic we were talking about earlier‚Äîthe Haskell compiler\noptimizes much more aggressively than the OCaml one does. And that‚Äôs\nbecause in Haskell, Haskell‚Äôs a lazy language which means that if you\nsay, ‚ÄòLet X equal some big computation,‚Äô we‚Äôre not gonna compute the\nbig computation until we need the value of X. In OCaml, if you say,\n‚ÄòLet X equal a big computation,‚Äô we just compute that thing right\naway.\n\nAnd so, that means, on the one hand you might say, ‚ÄòOh, well maybe\nHaskell is more efficient,‚Äô because maybe some code path never uses\nthe value of X and so it means that we can discard that whole big\nthing. But it also means that it‚Äôs much harder to predict in Haskell\nhow fast your program is going to run or what its performance\ncharacteristics are gonna be. In OCaml, it‚Äôs much easier to predict\nthat. You can just sort of read your file top to bottom and, within a\nfunction, it just performs the operations that you see roughly in the\norder that you see them. And that means, OCaml again is more explicit\nabout what happens at runtime, but without some of the type\nannotations that we see in Haskell, a little less explicit about\nwhat‚Äôs happening at compile time.\n\nYou mentioned a couple of things that are different that I wonder if\nthey‚Äôre really language differences, or just cultural differences. If\nyou look at the tendency to do fewer type annotations on the\ndefinition of functions, you could put more type annotations. And I\nthink there are some people who write OCaml who do, but there‚Äôs\ncertainly a tendency not to and I think the Jane Street house style\ncertainly is one that does not have lots of type annotations, outside\nof interface files where those are essentially required everywhere.\n\nAnd the documentation thing, too, like, the interface files are a very\nnatural place to put documentation and I am constantly dissatisfied\nthat I think we put way too little documentation in the (laughs)\nplaces that we could. And I don‚Äôt know that there‚Äôs anything exactly\nabout the language that forces that. Like, I‚Äôm not sure that I buy\n‚Äòthe need to put type annotations right on functions‚Äô is the thing\nthat really makes that difference.\n\nThat may be true. It‚Äôs hard to say. I do think that form follows\nfunction, function follows form a little bit, in that, when you‚Äôre\nused to describing the specification of a thing right next to that\nthing, that, to me, is going to encourage you to write\ndocumentation. Whereas, in OCaml, you write the specification over\nthere, in that other file, nowhere near where you write your\nimplementation. And sometimes I think that means that you‚Äôre less\nlikely to put those comments on the implementation when you need them.\n\nIt‚Äôs funny. My intuition is the opposite of, like, ‚ÄòOh, I‚Äôve written\nthis very bare interface file that just has types,‚Äô and I think\nempathetically about the person who‚Äôs reading it. It‚Äôs like, ‚ÄòHow will\nthey have any idea what this means? I had better put a comment here.‚Äô\n(laughs)\n\nRight. So the comment and the interface files are good, but I think\nit‚Äôs not just that. I mean, and this could also be, I have a bias in\nthat, most of the OCaml that I‚Äôm reading and writing is in the OCaml\ncompiler itself. Maybe this is different than everything else at Jane\nStreet. In fact, I‚Äôm sure it is. And it has been striking for me after\nspending so many years working in Haskell‚Äôs compiler, that Haskell‚Äôs\ncompiler has many, many, many more comments than OCaml‚Äôs compiler\ndoes. Maybe I‚Äôm overfitting on this particular piece of software\ninstead of just the language in general.\n\nI do think looking at the innards of a compiler is always a weird\nplace to think about how one should write code for the\nlanguage. Because, like, one of the facts about OCaml‚Äîas it is often\nthe case for many languages‚Äîis that OCaml was written by people who\ndidn‚Äôt yet really know how to program in OCaml kind of by\ndefinition. And lots of the code that‚Äôs there now has been there for a\nlong time and you can sorta see that. I think if you went to those\nsame people today and been like, ‚ÄòHow would you write this now?‚Äô\nThey‚Äôd all be like, ‚ÄòOh, really very differently.‚Äô But, you know,\nparts of it just kind of haven‚Äôt been rewritten and have the shape\nthat they did years ago and are harder to understand than kind of\nanyone would exactly like it to be today.\n\nAnd to be fair, that exact same thing is true in the Haskell\ncompiler. There are stretches of that that no one would write it that\nway today. But it was written that way before anyone really knew how\nto program in Haskell.\n\nSo another difference that‚Äôs always struck me about Haskell is the\ndifferent approach to language extensions. And I think about this in\ntwo different ways. One is, OCaml is just massively more\nconservative. There was a period in time early in OCaml‚Äôs development\nwhere you could almost see the process of Ph.D. students\ngraduating. It‚Äôs like someone graduates, they write some language\nfeature, and it gets added to the language. And that stopped pretty\nearly in OCaml. OCaml got pretty conservative about making changes. It\nwas like, ‚ÄòNo, no, no, we‚Äôre gonna add things that are like, really\ngood, and it was pretty restrictive about what would go in. And\nthroughout that time, it kind of had one language, pretty much. OCaml\nworked in one particular way and that was kinda that.\n\nHaskell on the other hand, has had a lot of glorious experimentation\naround language features and those experimental language features were\noften hidden behind pragmas, right? There‚Äôs lots of different things\nyou could turn on and off. And so there kind of isn‚Äôt one Haskell or\ntwo Haskells, but there‚Äôs like 10,000 different Haskells, depending on\nwhich collection of flags you wanna turn on or turn off. You have 10\ndifferent pragmas and now you have two of the 10 different possible\narrangements of the language features. I‚Äôm curious how you think about\nthose different sets of choices?\n\nYeah. I mean, that‚Äôs a great question. So I do have to push back\nagainst your sense of scale. So Haskell has, when I last counted, and\nthis was a few years ago, 150 or so extensions. So Haskell has\nactually 2150 languages.\n\nOuch, indeed. So I think that this comes from the origin of Haskell\nversus the origin of OCaml so actually you probably know more about\nthe origin of OCaml than I do. But I know Haskell was started by\ncommittee. There was a committee that met starting in, I think, 1989\ninto 1990, coalescing these ideas around a lazy functional programming\nlanguage. And then this eventually became a Haskell standard and there\nwere multiple different compilers for Haskell that all implemented the\nsame standard Haskell. And then maybe one compiler would think, ‚ÄòOoh,\nwouldn‚Äôt it be fun if we had X?‚Äô And so, then they would add that\nfeature, but everyone still wanted to make sure that it was a Haskell\ncompiler and not this other special language based on Haskell\ncompilers. So there was an idea of language extension. So you could\nhave base Haskell plus these various extensions.\n\nSo, in other words, OCaml is like Python and Haskell is like Scheme?\n\nSure. Yes. In that OCaml has a manual for the language, but there is\nnot a standard. There are not multiple different software artifacts\nthat all compile some OCaml language that exists beyond a single\ncompiler. But, in any case, going back to Haskell, that was the early\ndays of Haskell and there were multiple Haskell compilers. As time has\ngone on, all of the other ones have either died off or there are still\na few other Haskell compilers out there, but there‚Äôs not really an\nattempt to stay current with the newest features. And there‚Äôs really\nnow just one: the Glasgow Haskell Compiler, or GHC. And that‚Äôs the one\nthat, when I say that there‚Äôs 150 language extensions, the GHC is\nsupporting that.\n\nAnd so, when you have multiple different things, if you wanted to find\na common core that all of these different compilers can interop with,\nit makes a lot of sense to have individual extensions when you say\nyou‚Äôre deviating from this common core. Now there‚Äôs really just one\ncompiler.\n\nI think that the current system, the current plan at Haskell, is not\nvery well motivated. And instead, the feedback that I hear from users\nis that they find the whole language extension system very heavy and\nit means that, to get much done in Haskell, you need to now start your\nfile with the list of 20 extensions that you‚Äôre using. And newcomers\ncan‚Äôt tell the difference between brand new experimental extensions\nthat might change from one compiler to another, or extensions that\nhave been around for 20 years, or there are some extensions that have\nbeen around for 20 years but actually we know are dangerous and you\nreally shouldn‚Äôt use.\n\nAnd when I say, ‚ÄòWe know,‚Äô I mean, like, I know this and a couple of\nother Haskellers know this, but someone just picking up the language\nwon‚Äôt know that. And I personally find that quite problematic. And so,\nthere‚Äôs a debate going on right now within the GHC Steering Committee\nwho is a body chosen to sort of help evaluate the evolution of the\nlanguage to reimagine all of this. And I‚Äôm hoping that we end up with\na structure that looks quite different from what we have today, while\nstill retaining backward compatibility.\n\nSo if you have your 20 language extensions at the top of your file,\nthat will continue to work in this new vision. But I am hoping that we\ncan reorganize it in a way that‚Äôs a little bit more user-friendly.\n\nDo you think in the end it would make sense to move toward something\nwhere there‚Äôs closer to one standard language that includes most of\nthe extensions, and so that most users end up not thinking about that\n‚Äòtwo to the whatever‚Äô configuration space?\n\nI do think settling on one standard language would be good. I think we\ncan get 80% of the way there and go from 150 extensions to maybe seven\nor something like that. I don‚Äôt think we‚Äôll be able to get all the way\nthere because there‚Äôs just too many different use cases for Haskell,\nthere‚Äôs too many different people who have slightly different ideas of\nwhat that core should be. So if we just sort of got rid of the\nextensions mechanism, I think that that would cause too much of a rift\nin the community and probably not be tenable. But we can get close.\n\nSo, throughout this conversation, I feel like there‚Äôs been a\nparticular kind of vision and idea of what programming is that‚Äôs\nmotivated a lot of your thinking. And that has something to do with\nhaving a language that‚Äôs simultaneously expressive, it lets you say\nwhat you wanna say, but also has real tools for reasoning, in both\ntype systems and, you know, more sophisticated mode-dependent types,\nor essentially automated tools for reasoning about your program. So in\nthe last, I don‚Äôt know, few months, couple of years, there‚Äôs been an\nenormous amount of motion in a totally different direction for making\nprogramming better, which is various AI-assisted ways of making\nprogramming easier. With all of this kind of recent work on large\nlanguage models, things like ChatGPT, Codex, all of that, making an\nenormous difference to how people are programming on a regular\nbasis. I‚Äôm curious how you think about this and how it fits into your\nbroader way of thinking about programming?\n\nI see this evolution of sort of AI-assisted programming as not quite\nthe sea change that maybe others have seen it to be. I see it as a big\nstep. It‚Äôs not a small step change. But it doesn‚Äôt remove the need to\ncommunicate precisely. In that, to me, the interesting thing about a\nprogramming language is that it‚Äôs a mode of communication that is\nprecise. In a sense, that‚Äôs almost the definition of what makes a\nprogramming language a programming language, as opposed to some other\nkind of language. There‚Äôs a precise semantics to everything that is\nsaid in that language. And that language is used as a medium of\ncommunication, both from human to computer. That‚Äôs how we often think\nof it. But also from human to human.\n\nWith the advent of AI-assisted programming, now we have sort of a new\nmethod of communication in that it‚Äôs a communication from computer\nback to human. In that, you might have something like ChatGPT\nproducing the code, but a human still has to read that code and make\nsure that it does what you think it does. And as a medium of precise\ncommunication, it‚Äôs still very important to have a programming\nlanguage that allows that communication to happen.\n\nAnd so, this new mode of communication is going to put different\npressures on language design than what we‚Äôve had in the past. But I\ndon‚Äôt think it removes the need for programming language design. But\nlike I said, it does create different pressures and these have been\nevident for some time in that one little maxim I‚Äôve had in my head for\nyears now about language design is that you wanna optimize for\nreading, not for writing. Code gets read much more often than it gets\nwritten.\n\nAnd so, when designing a new language feature, maybe you‚Äôre thinking,\n‚ÄòOkay, I could do this very cleverly with, like, a nicely placed\ntwiddle in this spot in the code. And I could use that twiddle to mean\nsomething very important about my program. And maybe that‚Äôs very\nconcise, but it‚Äôs hard to read, it‚Äôs hard to search for, it‚Äôs hard to\ntrain new people to understand. Instead, it tends to be better to\nwrite out words in your design and programming language, that‚Äôs easier\nto search for, and easier to learn, and just easier to organize in\nyour brain. If we end up getting to a mode where humans are doing even\nless and less writing, and computers are doing more and more of it,\nthen those pressures just increase. And we wanna make the language\neasier to read and maybe even harder to write. Maybe we even get to a\npoint where if computers are doing all of the code generation, it‚Äôs\njust kind of, you know, symbolic and structured. It‚Äôs not just text.\n\nSo we get these richer programming languages but I just don‚Äôt see it\nas, there was yesterday and then there‚Äôs tomorrow and it‚Äôs gonna be\ntotally different and let‚Äôs just start from scratch. I see this as\njust one phase in evolution of this precise communication medium.\n\nOne of the things that‚Äôs really striking about these AI assistants, at\nleast in their current form, is they‚Äôre really surprisingly powerful,\nright? They can really do a kind of stunningly good job of taking a\nquery and understanding it and providing a piece of code that tries to\ndo what it is that you asked for it to do.\n\nBut they‚Äôre also strikingly fallible. They make lots of mistakes. And,\nyeah, the reading thing is pretty important because you ask ChatGPT\nor, you know, whatever your favorite large language model is, to write\na piece of code and look at the result. And it‚Äôs super easy for the\nresult to just be wrong in all sorts of different ways. And I can sort\nof see the rising requirement of making things more readable. I‚Äôm\ncurious how you think things like dependent types fit into this story?\n\nSo, dependent types in their essence are a way of writing precise\nspecifications. So the sorting example that I gave earlier, I\ndescribed that, you know, you could have a function instead of going\nfrom ‚Äòa list of ints to a list of ints‚Äô to something more\nglorious. Well, before dependent types, or you know, language without\ndependent types, you could just do that in comments. But probably in\nyour comments, you‚Äôre not going to be as precise as you could be if\nyou really wrote it in types, right? You say, you know, return to the\ninput list in ascending order, or something like that. But what does\nthat really mean?\n\nWe need a language where we can precisely specify what this function\nis doing. So as I said earlier, ascending isn‚Äôt quite right. I mean,\nif you write in a comment, ‚ÄòThis returns the input list in ascending\norder,‚Äô well, what if the input list has duplicates? Now you can‚Äôt\neven meet that specification. Where actually, that specification\ndoesn‚Äôt make any sense at all because the input list isn‚Äôt in\nascending order. What does that mean to say to put the input list in\nascending order? That‚Äôs a contradiction in terms, because the input\nlist isn‚Äôt in ascending order.\n\nAnd so, there‚Äôs some amount of interpretation that humans can provide,\ninterpretation that a large language model can provide, but in the\nend, it‚Äôs gonna come down to precise communication and\nspecification. And so, where do dependent types fit in? Well, they can\nbecome the input to the large language model. Maybe now we have\nprogrammers who are well-versed in precise communication of\nspecifications, not implementations, and we can say, ‚ÄòChatGPT, give me\na function of this type.‚Äô And then you have your nice type that takes\nan input list, and it returns an output list that is a permutation of\nthe input list in non-decreasing order and we specify that.\n\nAnd then now, that gives a nice specification for ChatGPT to go off\nand say, ‚ÄòOkay, here‚Äôs an implementation that meets that. Oh, and by\nthe way, it has this asymptotic running time because I‚Äôm a nice large\nlanguage model and I‚Äôm gonna tell you that.‚Äô And then, now we actually\nhave a model by which we can check that the result meets its\nspecification if it is in this precise language of dependent types.\n\nAlthough I have to say, I think I am more pessimistic about the idea\nof people actually going off and writing down the specifications\neven. ‚ÄòCause those are actually pretty hard to write.\n\nI wonder if a more plausible model is, you go to your large language\nmodel and say, ‚ÄòPlease write me a specification for a function that\nsorts a list.‚Äô And then it, like, spits something out. And then you\nlook at it and think, yeah, that seems about right. And then you go\nfrom there to next stages. But the role that this kind of concise\nspecification has, it‚Äôs like a smaller and maybe more readable thing\nthat you can use as part of verifying whether the later pieces in fact\ndo what you think they‚Äôre supposed to do.\n\nYes, but I think there‚Äôs a real danger here. I agree with what you say\nin that, writing a precise specification is hard. And humans are\nlazy. So if it‚Äôs hard, we‚Äôre gonna ask the computer to do it. If we‚Äôre\nasking the computer to write the specification of the function that\nit‚Äôs going to write, now we‚Äôre getting into thin ice. Because if we‚Äôre\nworking in a system where the human is not expert enough to be able to\nread the specification and know that it‚Äôs the right specification, now\nwe‚Äôve lost. Now the computer can just sort of go off and do whatever\nit wants and we have no way of knowing whether it‚Äôs right or wrong\nbecause we‚Äôve lost that ability to sort of have that precise\ncommunication.\n\nI think one of the tricky things here is, people are often pretty bad\nat the reading part of this, right? The idea if something gets written\nand now you have to read it, especially if it wasn‚Äôt written in a\nreally clear way, reading some piece of code that is not really\nwell-structured is actually really challenging. Like, one of the\nactually most important parts of the process of code review I find, is\nnot so much that, like, one person writes the code and someone else\nreads it to validate that it‚Äôs done the right thing. It‚Äôs more like,\nsomebody writes the code and the other person says, ‚ÄòOh, man. This is,\nlike, too messy for me to even read.‚Äô And then there‚Äôs some kind of\nback and forth process where you try and get something that‚Äôs actually\nsimple enough to be understood. And I think it‚Äôs an interesting open\nquestion of, like, as we start using more of these AI assistants, how\ngood they will be at generating things that are actually simple.\n\nAlthough, if we have precise specifications, the simplicity of the\nimplementation becomes less important.\n\nThat‚Äôs fair. I mean, in some sense there‚Äôs a kind of open\ntechnological question of, like, how good will large language models\nbe, acting as tactics for theorem provers?\n\nRight. Yeah. I mean, but if we‚Äôre in an environment where there is a\ntight correspondence between the specification and the implementation,\nand what I mean by ‚Äòtight correspondence‚Äô is this dependent type‚Äôs\nmodel where you can‚Äîand there‚Äôs other models, it‚Äôs not just dependent\ntypes, there‚Äôs refinement types, there‚Äôs other ways of checking an\nimplementation against a specification. But if we‚Äôre in an environment\nwhere we can check an implementation against a specification, the\nspecification is simple enough and communicated precisely whether\nthat‚Äôs in very careful natural language or probably not just natural\nlanguage ‚Äòcause natural language is very, very squishy. But probably\nin some specification language, you could call that specification\nlanguage ‚Äòdependent types‚Äô if you want or you could call it something\nelse. But if we can communicate that specification, check the\nimplementation against the specification, now suddenly there‚Äôs not all\nthat much incentive to ever read the implementation.\n\nRight. It becomes a little bit like, you know, reading the output of\nthe compiler.\n\nYeah. It‚Äôs interesting. I do wonder whether a whole potential kind of\ndirection for innovation here, which you more or less have pointed in\nthe direction of, is designing languages that work usefully in this\nkind of intermediate things to generate in the process of working with\nsome kind of AI assistant. The thing that‚Äôs generated as an\nintermedium really affects the overall process of working on the\nsystem and generating confidence it‚Äôs doing the right thing. It‚Äôs not\na totally new problem, I guess, in the context of language design. But\nit does put new kinds of pressure and maybe changes the trade-offs\nthat show up.\n\nYeah, that‚Äôs right. I mean, you know, it‚Äôs too early, I think, to\nstart to design a programming language around this kind of\ninteraction. We‚Äôve only been having these interactions with these\nassistants for a few months. And they‚Äôre changing fast. So I think now\nwould be the wrong time. In two years, maybe one could start thinking\nabout designing a language from scratch to do this. That sounds\nplausible to me. But I think a good starting point would be languages\nthat we have now because they‚Äôre designed for precise\ncommunication. That‚Äôs, in the end, what we need. And, again, maybe\nthere‚Äôs less pressure on the writing piece and more pressure on the\nreading piece. But I don‚Äôt think that they‚Äôre so divorced from each\nother that it means that we throw out everything we have and start\nfrom scratch. That seems, to me, pretty unlikely.\n\nThat makes sense. Well, thanks so much for joining me. This has been a\nlot of fun.\n\nYou‚Äôll find a complete transcript of the episode, along with links to\nsome of the things that we discussed at signalsandthreads.com. Thanks\nfor joining us and see you next time.",
    "readingTime": 65,
    "keywords": [
      "jane street",
      "ghc steering",
      "steering committee",
      "street‚Äôs walls",
      "ron discuss",
      "joined jane",
      "street‚Äôs tools",
      "i‚Äôm curious",
      "dramatis personae",
      "tight correspondence"
    ],
    "qualityScore": 1,
    "link": "https://signalsandthreads.com/future-of-programming/",
    "thumbnail_url": "https://signalsandthreads.com/static/images/twitter/future_of_programming.png",
    "created_at": "2026-01-09T00:58:47.386Z",
    "topic": "tech"
  },
  {
    "slug": "catnip-run-claude-code-from-your-iphone-using-github-codespaces",
    "title": "Catnip ‚Äì Run Claude Code from Your iPhone Using GitHub Codespaces",
    "description": "Like catnip, a highly addictive agentic coding tool - wandb/catnip",
    "fullText": "wandb\n\n /\n\n catnip\n\n Public\n\n Like catnip, a highly addictive agentic coding tool\n\n catnip.run\n\n License\n\n Apache-2.0 license\n\n 402\n stars\n\n 32\n forks\n\n Branches\n\n Tags\n\n Activity\n\n Star\n\n Notifications\n You must be signed in to change notification settings\n\n wandb/catnip",
    "readingTime": 1,
    "keywords": [
      "catnip",
      "license"
    ],
    "qualityScore": 0.4,
    "link": "https://github.com/wandb/catnip",
    "thumbnail_url": "https://opengraph.githubassets.com/d032d2bab84800534c687d9264960e30ed62e372d7f32babbf8b3506627fd360/wandb/catnip",
    "created_at": "2026-01-08T18:16:54.144Z",
    "topic": "tech"
  },
  {
    "slug": "grief-leverage-and-the-future-of-manual-coding",
    "title": "Grief, leverage, and the future of manual coding",
    "description": "I‚Äôm a software engineer and product maker based in Cracow, Poland. My mission is to create useful products by writing high-quality code and sharing my knowledge throughout the journey.",
    "fullText": "For the last couple of years, watching the software industry has been an emotional experience for me. And will I remember the winter of 2025/2026 - the time when apparently everyone and your mum discovered just how good in coding the Claude Opus 4.5 is - as the culmination of that period, and one of the hardest and most confusing times for me as a professional.\n\nOn one hand, there's anxiety. The pace of change is brutal. New tools appear every week, workflows emerge almost overnight. Wondering where this is all is heading, and whether there will still be a place for us on the other side.\n\nI felt strange hearing from industry thought leaders like Andrej Karpathy, saying that English is now the new programming language. I had spent years honing the craft of writing in programming languages - so what was I supposed to make of that, if it was now being treated as commoditized?\n\nAfter months of thinking about that, I had identified the feeling - grief. Grief for manual coding.\n\nFor years, my identity as an engineer was tightly coupled to the artifacts I produced. My repositories. My components. My abstractions. The code was mine, and that ownership mattered. It was craft and it was personal. Like a carpenter remembering how it felt to build a specific chair.\n\nThat mental model no longer holds for me. How could it, when everyone now can one-shot a todo app for themselves, just like a PowerPoint presentation?\n\nOn the other hand, there's excitement. I got into coding because I wanted to create worlds. Websites, apps, flows, solutions, interactions between users, beautiful and unique things.\n\nNow the ceiling of building is so high that we cannot even see it. Ideas that felt too expensive or too complex are suddenly within reach. It's not an understatement to say things that used to take weeks and months now may take hours. I'm not saying that it's easy to do it and everyone can make it happen, but it's certainly possible.\n\nThe new leverage comes from moving one layer up. Instead of doing every task ourselves, we design systems that can do them for us. A single agent can now execute autonomously what used to require long hours my time, freeing my attention for higher-level decisions, design, judgment. And maybe just... enjoying the life with my loved ones?\n\nThis all immensely increases the leverage of single engineer. Technically, you're one well designed system away from solving a daunting problem for the first time, from building life-changing startup, from building your dream game. Of course, it's still hard and rare, but pre-agentic coding it was often not possible at all - you had to broke the concrete walls of thousands of lines of code first.\n\nI mentioned that everyone with AI can one-shot a personal to-do app now. Engineers with AI can create much more that. They can design systems that scale, adapt, and solve problems in ways that were previously out of reach, everything under strict engineering discipline - secure, cheap and efficient.\n\nThat's why, overall, I am cautiously optimistic on what the future holds.\n\nAnd it seems like - as buzzwordy and clich√© as it sounds - that the future is agentic.\n\nThe word agentic gets thrown around a lot, so it's worth grounding it. I will use the definition I personally In practice, the systems that actually work for me tend to follow the same five core steps:\n\nPoints 2, 4 ad 5 can be somewhat \"recursively\" executed by agents - it's not hard to imagine agents implementing the system by writing specs or veryfing the outputs for other agents.\n\nPoints 1 and 3 are uniquely human - we decide what we want to exist, and we trigger the execution process. And while point 3 - triggering the execution - also can be run by agent, I keep it in this category because someone is at the end is responsible for what the agents did, and in that sense it is uniquely human.\n\nThis shift doesn't mean the broad engineering skill is obsolete. It means where that skill applies has changed.\n\nTo orchestrate agents that produce valid code, I still need to understand:\n\nAnd what is even more important:\n\nInstead of applying that knowledge and intent manually in code, I encode it into the system that produces the code. It produces the code in indeterministic way, mind you, and potentially on much broader scale. We don't know yet which scale we talk about. 2x? 10x? Maybe more? We will see.\n\nAnd there's whole new class of problem to solve. How do I ensure models doesn't produce unexpected or harmful results? How do I coordinate several, dozen, and more models to work together? How do I ensure AI runs on prem and we don't share our precious data with anyone?\n\nIn other words, the craft moves up a level.\n\nInitially, I wanted to include \"the end of manual coding\" in this post's title. But I changed it to \"the future manual of coding\".\n\nFirst, I didn't want to sound clickbaity. Now, seriously - manual coding isn't gone. It still has a very important place. Best professionals always understood different abstraction levels, not only the highest one. Code is runtime, and you have to understand runtime through and through. Apart from that - it's still important in learning, personal work, and honing the cognitive skills. I especially believe in the last one, because delegating so much mental work we used to do before will take a tool on our thinking in a long term.\n\nBut it's no longer the default path to producing value and to economic leverage. The role of software engineers is shifting:\n\nWe're no longer producing artifacts (code). We're designing systems producing artifacts.\n\nOnce you accept that, a lot of confusion from the last couple of years disappears. The grief is still there, but it's quieter. And there is something else: the clarity, and a sense that this change, while uncomfortable, is also full of possibility.\n\nOne important note: no, all of this doesn't mean succumbing to AI slop. We're still responsible for everything our agents produce - every single line of it. And no, it doesn't mean letting AI write sloppy LinkedIn posts or Slack messages is suddenly acceptable. If anything, the opposite: write your damn words yourself, please. Do not delegate your thinking.\n\nThere's a whole lot of skill to designing the agentic systems including mastering specs engineering, enforcing constraints, output verification, model evaluation and so on. This is the obvious thing to focus on first.\n\nMany people dreamed of living in times with real blank spots on the map, to be able to discover the unknown themselves. Software engineers in 2026 have the privilege of experiencing this. And it can be a source of risk, as well as an economic leverage.\n\nWe've always been good at automating and learning new things. Now there's even more to automate and learn. Our focus should shift from manual coding to designing systems that produce code at scale - while remembering that manual coding got us here, and still matters, just for different reasons.\n\nI‚Äôm a software engineer and product maker based in Cracow, Poland. My mission is to create useful products by writing high-quality code and sharing my knowledge throughout the journey.",
    "readingTime": 7,
    "keywords": [
      "uniquely human",
      "economic leverage",
      "producing artifacts",
      "software engineers",
      "design systems",
      "designing systems",
      "manual coding",
      "code",
      "it's",
      "agents"
    ],
    "qualityScore": 1,
    "link": "https://www.tymzap.com/blog/grief-leverage-and-the-future-of-manual-coding",
    "thumbnail_url": "https://www.tymzap.com/api/og?title=Grief%2C%20leverage%2C%20and%20the%20future%20of%20manual%20coding&token=37cac97264905c6d7b412cbf3c96ddd0a5309cbd4603ad51e8c444cd8be1e1e5",
    "created_at": "2026-01-08T18:16:49.915Z",
    "topic": "tech"
  },
  {
    "slug": "microsoft-reshuffles-teams-to-bolster-github-as-ai-coding-and-agent-wars-heat-up",
    "title": "Microsoft reshuffles teams to bolster GitHub as AI coding and agent wars heat up",
    "description": "GitHub, the dominant software development platforms, is responding to the rise of AI coding services and AI agents",
    "fullText": "Microsoft wants to overhaul GitHub to compete with AI coding rivals and embrace AI agents, and the company has started reshuffling teams to make that happen, according to people familiar with the changes.\n\nGitHub is a leading software development platform that Microsoft acquired in 2018. GitHub had an early lead because of its popularity as a place to store code. Lately, though, GitHub has faced more competition from AI tools such as Cursor and Anthropic's Claude Code.\n\nMicrosoft in January 2025 formed a new group focused on building AI tools under ex-Facebook engineering boss Jay Parikh. The group, called CoreAI Platform and Tools, combined Microsoft's developer division, AI platform team, and GitHub.\n\nStill, Microsoft and GitHub have remained somewhat separate, and the company has been moving people and resources around over the past few months to better coordinate efforts such as sales, one of the people said. The latest change, happening this week, is moving a small group of Microsoft engineers over to GitHub.\n\nThe goal, the people said, is to better compete with AI coding tools that rival GitHub Copilot, while getting in the race to build AI agents and fulfill Parikh's vision to build an \"agent factory.\"\n\nIn an internal meeting late last year, Parikh spoke about needing to overhaul GitHub to compete with Cursor and Claude Code, according to audio reviewed by Business Insider.\n\n\"GitHub is just not the place anymore where developers are storing code,\" Parikh said at the time. \"We want it to be the center of gravity for all of AI-powered software development.\"\n\nMicrosoft wants GitHub's AI tools to be available wherever developers work, not just inside one app, to wants to make GitHub a kind of dashboard for managing multiple AI agents.\n\nThe latest changes are also part of what Parikh said would be new investment in improving the basic parts of GitHub. In the meeting, Parikh said those include making improvements to its GitHub Actions tool that automates building, testing, and deploying code, analytics and insights tools so teams can see how their code is performing, security for keeping the code safe, and making sure the company can meet local data storage rules to offer GitHub in new countries.\n\nHave a tip? Contact this reporter via email at astewart@businessinsider.com or Signal at +1-425-344-8242. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 2,
    "keywords": [
      "claude code",
      "software development",
      "overhaul github",
      "microsoft",
      "compete",
      "agents",
      "coding",
      "teams",
      "latest",
      "developers"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/microsoft-github-reshuffle-ai-coding-agents-2026-1",
    "thumbnail_url": "https://i.insider.com/695c2f3b832e0ef1ead73181?width=1200&format=jpeg",
    "created_at": "2026-01-08T12:25:17.155Z",
    "topic": "finance"
  }
]
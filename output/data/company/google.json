[
  {
    "slug": "hut-8-ceo-says-7-billion-data-center-deal-with-google-is-first-domino-to-fall",
    "title": "Hut 8 CEO says $7 billion data center deal with Google is 'first domino to fall'",
    "description": "Hut 8 CEO Asher Genoot says a 15-year financial guarantee from Google was the key to unlocking billions in project financing from JPMorgan and Goldman Sachs.",
    "fullText": "The \"crypto miner\" label is becoming a relic of the past for Hut 8 (HUT).\n\nIn a move that signals a massive strategic shift, Hut 8 announced a $7 billion deal to build a data center complex in Louisiana for Anthropic (ANTH.PVT). The move effectively trades the volatility of bitcoin (BTC-USD) for the high-margin stability of AI infrastructure.\n\n\"We think that the value of energy will continue to go up, and so for us, we [had] to be patient and figure out what was a fair deal that added value to our customers, but that drove a strong return to our shareholders,\" Hut 8 CEO Asher Genoot told Yahoo Finance's Opening Bid.\n\nThe market is rewarding the transformation. The energy infrastructure company's stock surged roughly 20% on Wednesday following the announcement as investors cheered its potential to secure blue-chip partners in a crowded field.\n\nBeyond the one-day pop, the stock has been a standout in the mining-to-AI pivot, fueled by the company's deep energy assets and its strategic partnership with American Bitcoin (ABTC), a Trump family enterprise in which Hut 8 holds a 60% stake. Year to date, Hut 8 stock is up 90%.\n\nThe linchpin of the Louisiana project is a financial guarantee from Google (GOOG, GOOGL), which guarantees payments of the 15-year lease if Fluidstack, an AI cloud infrastructure provider, cannot meet them. The deal secures 245 megawatts (MW) of capacity, with the possibility to scale up to 2,300 MW within that timeframe — a move that could swell the total lease value to $17.7 billion.\n\n\"We're very excited because it's the first domino to fall,\" Genoot said. \"If we're able to continue to scale and build up to that full strategic partnership on 2,300 MW, we see a lot of growth upside for the company and the deal overall.\"\n\nThe creditworthiness was the key. Genoot noted that without Google's involvement, securing project financing from titans like JPMorgan (JPM) and Goldman Sachs (GS) at favorable rates would have been nearly impossible.\n\nHe added that Google has a \"strong core business,\" including diversified cash flows from Search, YouTube, and cloud services, making the tech giant's pledge the ultimate security for a 15-year infrastructure play.\n\nHowever, securing the money is only half the battle. Competitors have seen their stocks hammered due to construction delays. Genoot claims Hut 8 has de-risked the execution side, partly by issuing a \"conservative delivery schedule\" and tapping local labor.\n\nThe company has turned to established industrial partners, including Jacobs Engineering for design and local contractors like Cajun Electric for electrical services. By scheduling the first data hall delivery for Q2 2027, Genoot said the company is avoiding the \"crunch\" of unrealistic timelines that has plagued the sector.",
    "readingTime": 3,
    "keywords": [
      "strategic partnership",
      "deal",
      "infrastructure",
      "energy",
      "stock",
      "louisiana",
      "company's",
      "project",
      "lease",
      "cloud"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/hut-8-ceo-says-7-billion-data-center-deal-with-google-is-first-domino-to-fall-183115394.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/IqAp9Ga6Y7sqtx4GI8W.ig--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2025-12/f8d59db0-dc32-11f0-abf5-a808ba6eb4d3",
    "created_at": "2025-12-19T00:56:17.961Z",
    "topic": "finance"
  },
  {
    "slug": "delty-yc-x25-is-hiring-an-ml-engineer",
    "title": "Delty (YC X25) Is Hiring an ML Engineer",
    "description": "About Us\nDelty is building the healthcare’s AI operating system. We create voice-based and computer-based assistants that streamline clinical workflows, reduce administrative burden, and help providers focus on patient care. Our system learns from real healthcare environments to deliver reliable, context-aware support that improves efficiency and elevates the provider experience.\nDelty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You’ll get to work alongside people who built massive systems at scale — a chance to learn a lot and contribute meaningfully from day one.",
    "fullText": "Delty is building the healthcare’s AI operating system. We create voice-based and computer-based assistants that streamline clinical workflows, reduce administrative burden, and help providers focus on patient care. Our system learns from real healthcare environments to deliver reliable, context-aware support that improves efficiency and elevates the provider experience.\n\nDelty was founded by former engineering leaders from Google, including co-founders with deep experience at YouTube and in large-scale infrastructure. You’ll get to work alongside people who built massive systems at scale — a chance to learn a lot and contribute meaningfully from day one.\n\nWe believe in solving hard problems together as a team, iterating quickly, and building software with long-term thinking and ownership.",
    "readingTime": 1,
    "keywords": [
      "delty",
      "system",
      "experience"
    ],
    "qualityScore": 0.55,
    "link": "https://www.ycombinator.com/companies/delty/jobs/MDeC49o-machine-learning-engineer",
    "thumbnail_url": "https://bookface-images.s3.amazonaws.com/logos/d0b8121aba7fc719913ed15778bbd255904c8d7f.png?1765469217",
    "created_at": "2025-12-19T00:56:16.215Z",
    "topic": "jobs"
  },
  {
    "slug": "mondaycoms-cmo-led-marketing-at-amazon-and-google-here-are-the-3-things-he-looks-for-in-every-job-offer",
    "title": "Monday.com's CMO led marketing at Amazon and Google. Here are the 3 things he looks for in every job offer.",
    "description": "Harris Beber says that when he finds a role that aligns with these 3 things, he's able to find the most fulfillment in his career.",
    "fullText": "It's human nature to reach for the shiny object in front of you — but Monday.com's CMO says it's worth pausing before you accept the first job offer that comes along.\n\nHarris Beber has acted as the CMO of Vimeo, Global CMO of Google Workspace, and also held marketing executive positions at other companies including Amazon. He's no stranger to changing jobs — and he believes switching roles can be the right move when the work no longer feels fulfilling.\n\nIn the past, Beber told Business Insider that he's explored new roles in times when he was doing really well at work but feeling unfulfilled. That feeling can't last forever, he said.\n\n\"Eventually your work will suffer, your performance will suffer, and it doesn't end well for anyone,\" Beber said.\n\nTo avoid acting out of desperation or necessity, Beber said he tries to map out his next role before he gets to the point of needing a new job. The CMO said it's important to know your criteria for success when thinking about what's next.\n\n\"What is really important to you? What are your non-starters? And if you can look at those objectively, then when you have opportunities, you can measure it against something objective,\" Beber said.\n\nHe said he's used 3 P's to determine if his last few roles were the right fit: people, product, and position.\n\nIn regards to the first P, Beber said he tries to evaluate whether the people he's going to work with are right for him. He said it's \"not easy to get up every day\" and work with people you don't like or who don't treat others well.\n\nIn one job, Beber said he worked for a difficult leader who yelled, \"put intense pressure\" on the team, and treated him and others poorly. He said he would never work in that kind of environment again.\n\nBeyond working with respectful coworkers, Beber said it's also important to make sure that the people you work alongside have complementary skills that will help you work well together.\n\nIf you're a marketer, when it comes to evaluating the product you're selling, Beber said, it's important to understand the user base and product offerings.\n\n\"Do you care about it? Does it excite you? It's really hard to sell something to other people if you don't understand it or aren't excited by it,\" the CMO said.\n\nBeber said he made the mistake of once taking the wrong role at a sports tech startup. He said he really liked the people, but he's \"not a sports person,\" and didn't understand the product. Beber said he ultimately didn't find happiness in that job.\n\nWhen it comes to evaluating the position, Beber said he wants to find a role where he feels aligned and can add value to the company.\n\n\"Is what I'm great at? What the company needs?\" Beber said.\n\nBeber said he's felt the most fulfilled in his career when those three criteria line up. People are \"really good at selling themselves\" on why an opportunity seems perfect in the moment, he said, which is why measuring the offer against something objective is critical.\n\nWhen someone makes you a job offer, \"they're going to tell you every reason why you're perfect for the role and why it's going to be great,\" Beber said. \"But as anyone goes into a new role, very rarely is it exactly what you thought it would be.\"",
    "readingTime": 3,
    "keywords": [
      "it's",
      "role",
      "beber",
      "product",
      "roles",
      "don't",
      "you're",
      "understand",
      "suffer",
      "anyone"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/cmo-monday-assess-people-product-position-new-roles-2025-1",
    "thumbnail_url": "https://i.insider.com/6941975b832e0ef1ead64e38?width=1200&format=jpeg",
    "created_at": "2025-12-17T13:45:43.201Z",
    "topic": "finance"
  },
  {
    "slug": "google-deepmind-ceo-demis-hassabis-says-some-ai-startups-are-wildly-overpriced-and-a-correction-is-coming",
    "title": "Google DeepMind CEO Demis Hassabis says some AI startups are wildly overpriced — and a correction is coming",
    "description": "The Google DeepMind CEO warns that some early AI startups are overvalued and the current pace of funding won't last.",
    "fullText": "Demis Hassabis has a blunt message for parts of the AI startup world: Some of this looks unsustainable.\n\nThe DeepMind cofounder and CEO said in an episode of \"Google DeepMind: The Podcast\" published Tuesday that there are likely \"bubbles\" forming in today's AI funding frenzy, particularly among early-stage startups raising money at huge valuations.\n\nSome startups \"basically haven't even got going yet,\" he said, yet are raising at \"tens of billions of dollars valuations just out of the gate.\"\n\n\"It's sort of interesting to see how can that be sustainable. You know, my guess is probably not, at least not in general,\" he added.\n\nHassabis drew a distinction between those sky-high seed rounds and the large tech companies pouring billions into AI infrastructure. There's \"a lot of real business\" underpinning Big Tech's valuations, he said.\n\nAI is \"overhyped in the short term\" but \"still underappreciated in the medium to long-term,\" he added.\n\nHassabis said an \"over-correction\" is imminent for any major technology shift like AI, especially when it goes from skepticism to obsession quickly.\n\n\"When we started DeepMind, no one believed in it,\" he said. \"Fast forward 10, 15 years, and now, obviously, it seems to be the only thing people talk about in business.\"\n\nThat kind of swing often pushes valuations too far and too fast. \"It's almost an overreaction to the underreaction,\" he said.\n\nHassabis also said he isn't worried about whether AI is in a bubble — he's focused on his job. Google DeepMind builds the AI models that power Google's products, including Gemini, and leads the company's frontier AI research.\n\nHassabis' comments come as AI startups continue to rake in soaring valuations.\n\nBusiness Insider reported last week that young founders — some of whom are fresh out of school — are raising millions for their AI startups. Many have dropped out to ride the AI wave, pulling in top investors and talent.\n\nA Stanford graduate dropout raised $64 million for her AI math startup earlier this year. Carina Hong, the founder of Axiom Math, even recruited top AI talent from Meta and Google Brain.\n\nThe 16 young founders Business Insider spoke with this year have secured over $100 million in funding.\n\nBut not everyone is buying the hype. Howard Marks, the cofounder of Oaktree Capital Management, said on an episode of \"We Study Billionaires\" podcast published last week that investors are flocking to AI startups with little track record.\n\n\"Do you want to have a novel entrepreneurial startup pure play which has no revenues and no profits today, but could be a moonshot if it works?\" the billionaire asked.\n\n\"Or do you want to invest in a great tech company, which is already existing and making a lot of money where AI could be incremental but not life-changing? It's a choice.\"",
    "readingTime": 3,
    "keywords": [
      "podcast published",
      "business insider",
      "startups",
      "valuations",
      "startup",
      "it's",
      "cofounder",
      "episode",
      "funding",
      "money"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/demis-hassabis-google-deepmind-ai-startup-valuation-correction-bubble-2025-12",
    "thumbnail_url": "https://i.insider.com/69422d86832e0ef1ead6598d?width=1200&format=jpeg",
    "created_at": "2025-12-17T06:58:25.246Z",
    "topic": "finance"
  },
  {
    "slug": "openais-answer-to-googles-viral-nano-banana-pro-image-model-is-here",
    "title": "OpenAI's answer to Google's viral Nano Banana Pro image model is here",
    "description": "OpenAI announced a new version of ChatGPT Images on Tuesday, powered by a new flagship AI image generation model. It rolls out today.",
    "fullText": "Google threw down the AI image gauntlet last month. Now, OpenAI has answered.\n\nThe ChatGPT creator announced the rollout of a new flagship image generator on Tuesday, which the company says is capable of making faster, more precise edits to an AI image while maintaining details.\n\nIntroducing ChatGPT Images, powered by our flagship new image generation model.\n\n- Stronger instruction following\n- Precise editing\n- Detail preservation\n- 4x faster than before\n\nRolling out today in ChatGPT for all users, and in the API as GPT Image 1.5. pic.twitter.com/NLNIPEYJnr\n\nBut the more eye-catching change for many user will likely be the improvements to image quality and the AI model's ability to follow specific instructions.\n\nOpneAI said the new model offers \"clear improvements across a range of cases.\" In one example, the company showed off the differences between the old and new image model when prompted to generate a photorealistic scene in 1970s Chelsea, London.\n\nIn another example touting use cases for businesses using the company's image API, OpenAI compared the outputs showing a mechanic working on a car.\n\nWhile a model's ability to generate photorealistic images has become a popular point of comparison in the AI race, OpenAI's latest model can also generate animated images, graphics, and other styles of artwork.\n\nPerhaps to boost awareness of that, the company is rolling out a new Images feature within the ChatGPT app. While AI image generation was previously available within ChatGPT, OpenAI says the new dedicated Images feature is designed to \"spark inspiration and make creative exploration effortless.\"\n\nThe news comes just over three weeks after Google released its Nano Banana Pro AI image model alongside its flagship Gemini 3 LLM — both of which have received widespread praise and reignited the debate around whether Google had begun to overtake OpenAI in the AI race.\n\nGoogle's new AI image generator was lauded for its hyper-realistic AI images, which some people used over Thanksgiving to make it appear as if they had famous guests at the holiday dinner table.\n\nYuchen Jin, the cofounder and CTO of the startup Hyperbolic Labs, called OpenAI's new model \"Nano Banana Pro level in my tests.\"",
    "readingTime": 2,
    "keywords": [
      "nano banana",
      "banana pro",
      "model's ability",
      "generate photorealistic",
      "images feature",
      "google",
      "flagship",
      "generator",
      "faster",
      "precise"
    ],
    "qualityScore": 0.9,
    "link": "https://www.businessinsider.com/openai-new-chatgpt-images-model-2025-12",
    "thumbnail_url": "https://i.insider.com/6941a89904eda4732f2d9dd9?width=1200&format=jpeg",
    "created_at": "2025-12-17T03:44:51.369Z",
    "topic": "finance"
  },
  {
    "slug": "cc-a-new-ai-productivity-agent-that-connects-your-gmail-calendar-and-drive",
    "title": "CC, a new AI productivity agent that connects your Gmail, Calendar and Drive",
    "description": "CC is an experimental AI productivity agent in Gmail by Google Labs. Get a personalized email briefing every morning and email CC anytime for help.",
    "fullText": "Who is eligible to join the waitlist?\n\nCurrently, the waitlist is open to users in the United States and Canada who are 18+ and have a consumer Google account. Google AI Ultra and paid subscribers will be prioritized. We appreciate your interest and patience as we work to bring CC to more regions and users!\n\nAm I auto-enrolled in CC by having a Gmail account?\n\nNo. CC is currently a Google Labs experiment, and your participation is entirely voluntary. To experience CC, you must have Workspace \"Smart Settings\" enabled and then sign up at labs.google/cc. If you have successfully joined the waitlist, no further action is required; you will receive an email from CC when you've been taken off the waitlist. You maintain complete control and can disconnect from CC at any time.\n\nCan I customize the \"Your Day Ahead\" daily briefing?\n\nYes. You can teach CC about yourself, correct information, or add to-dos simply by emailing CC or replying directly to the \"Your Day Ahead\" email.\n\nEmail CC at '[your-username]+cc@gmail.com' to get help.\n\nCan CC send emails to anyone else?\n\nNo, CC can only send emails directly to you. However, you can add CC to an email thread to request a summary; it will still only reply to you privately.\n\nAt the bottom of every email from CC, you can find Thumbs Up and Thumbs Down buttons. Use them to share specific feedback on what is (or isn't) working for you. You can also always provide us with feedback by emailing labs-cc-support@google.com.\n\nWhy aren't links in ‘Your Day Ahead’ working on mobile?\n\nWe are fixing the issue of some URLs not working on the Gmail mobile app. In the meantime, please use the Gmail web app to open these links.\n\nWho can I reach out to for support?\n\nYou can contact labs-cc-support@google.com for general inquiries, including help with connecting to or disconnecting from CC.\n\nYou can disconnect from CC any time by clicking here. Look for the 'CC' service and follow the disconnection steps.\n\nIs CC part of Gemini Apps or Google Workspace?\n\nCC is a standalone experimental service provided by Google Labs. Your data in CC, including any feedback you choose to provide, is processed per the Google Privacy Policy. CC is not part of Google Workspace or Gemini Apps. The Workspace Labs Privacy Notice and the Gemini Apps Privacy Notice do not apply.\n\nHow can I delete my data from CC?\n\nYou can disconnect from CC at any time by clicking here. Please note that simply deleting an email or file from your personal Gmail or Drive does not automatically remove it from CC's memory; to fully clear your data, you must disconnect from the service. Once you disconnect, any emails CC previously sent you will remain in your Gmail until you choose to delete them. If you have further questions, please reach out to labs-cc-support@google.com.",
    "readingTime": 3,
    "keywords": [
      "privacy notice",
      "your day ahead",
      "no cc",
      "cc you",
      "gmail",
      "disconnect",
      "waitlist",
      "emails",
      "feedback",
      "labs-cc-support@google.com"
    ],
    "qualityScore": 1,
    "link": "https://labs.google/cc/",
    "thumbnail_url": "/assets/images/tools/cc.webp",
    "created_at": "2025-12-16T18:57:23.945Z",
    "topic": "tech"
  },
  {
    "slug": "what-are-tpus-everything-you-need-to-know-about-googles-marketmoving-ai-chips",
    "title": "What are TPUs? Everything you need to know about Google's market-moving AI chips.",
    "description": "Google's TPU business is ramping up. Here's everything you need to know about Google's AI chips, and what they mean for Nvidia's dominance.",
    "fullText": "Google has been in the AI chip game for more than a decade. Now its custom hardware is moving markets.\n\nShares of Nvidia and other chipmakers tumbled last month following a report that Meta — one of Nvidia's largest customers — was exploring a deal to use Google's AI chips, known as Tensor Processing Units, or TPUs.\n\nGoogle has primarily used its TPUs for internal use, but it also leases them to external customers through the cloud. Nvidia, meanwhile, has become the dominant provider of AI chips with its graphics processing units, or GPUs.\n\nGoogle has a potential blockbuster business to unlock. In a research note sent December 2, Morgan Stanley projected that 5 million of Google's TPUs will be purchased in 2027 and about 7 million in 2028, significantly increasing its prior projections.\n\nHere's a breakdown of everything you need to know about TPUs, what they're used for, and when they might become a more prominent threat to Nvidia's chip dominance.\n\nOver a decade ago, Google needed more powerful and specialized compute power for the type of AI work it wanted to do. A team led by the now-CEO of Groq, Jonathan Ross, designed a new chip based on a specific type of integrated circuit for machine learning. The TPU was born.\n\nGoogle has continued to improve and refine its TPUs over the years, making them more effective at both training models and inference, the process where a trained model answers a question or performs a task. As large language models have grown in size, Google has also increased the memory bandwidth of later TPUs to handle these bigger workloads.\n\nGoogle says its latest \"Ironwood\" TPU, which the company is making widely available, is more than four times better than its predecessor for both training and inference.\n\nNvidia's GPU cards, which launched in 1999, were originally designed for gaming, not AI. When researchers later discovered that the chips were useful for tasks such as training neural networks, Nvidia doubled down on the AI market.\n\nGoogle's TPUs, on the other hand, were designed for AI from the get-go. Being more specialized means they're more efficient than Nvidia's chips at some tasks, and faster for running select AI models. They have something called a systolic array, which lets a more constant stream of data pass through the chip, rather than having to keep fetching more from memory.\n\nWhere Google's TPUs have a big edge is in cost at scale. It's possible to have thousands of TPUs working in tandem in a single \"pod.\" Because they are faster at some calculations than GPUs, running many TPUs at once can sometimes be more cost-efficient.\n\nThat may become more important as companies increase investments at the inference stage, which Google says its latest TPU is especially good at. That could save companies money.\n\nOne advantage that Nvidia has over Google is its CUDA software, which makes it possible for regular applications to use GPUs for general computing tasks — not just for graphics. CUDA also only works with Nvidia chips, one of the biggest points of friction stopping companies from switching to Google's chips.\n\nGoogle is trying to change that. For example, there is a lot of industry demand to have TPUs better support Pytorch, a popular tool for building AI applications that was created inside Meta. Google is allocating more resources and attention to better supporting Pytorch, which data suggests is seeing much more demand than Google's own Tensorflow software, according to Google employees and industry experts.\n\nWhen it comes to TPUs, Google remains its own biggest customer. It uses the chips across the company to power products like Search and Maps. Its latest Gemini 3 model was trained on TPUs.\n\nWhile Google has prioritized its own needs, it has leased its TPUs to other customers. Apple used TPUs to train its in-house AI model, Business Insider previously reported. In October, Anthropic announced a blockbuster deal with Google that it says will see it using up to 1 million TPUs. Broadcom, which helps build the TPU chips for Google, revealed in its Q4 earnings call this week that it has received a total of $21 billion in orders from Anthropic for Google's Ironwood TPUs.\n\nMeta is also in early testing of Google's TPUs, according to a person familiar with the matter, although it's not clear if it will result in a long-term deal.\n\nGoogle's TPU business could be poised to explode in the coming years. In a research note this month, Morgan Stanley wrote that every 500,000 TPU chips sold could potentially add around $13 billion in revenue to Google's balance sheet in 2027.\n\nHaving TPUs isn't just a potential revenue boost for Google. It also gets a feedback loop in using TPUs to run and train its AI models, learning from them, and changing how it develops its next chips to be better at the things Google needs.\n\nNvidia's GPU is still very much the chip du jour, but other tech giants are increasingly pouring resources into their own custom chips. Amazon just announced its new Trainium3 custom AI chip, which the company says can cut the cost of training and powering AI models in half compared with a GPU.\n\nMany TPU buyers are also Nvidia customers, and some industry insiders told Business Insider that it's more likely that the rise of more specialized chips will see companies and labs diversify the chips they use, rather than going all in on one provider.\n\nThat could still hurt Nvidia, which could lose pricing power as a result of a more diverse market.\n\nHowever, even if Google's TPU business finds more momentum, it doesn't mean Nvidia will suddenly crumble overnight. Jordan Nanos, member of technical staff of research firm SemiAnalysis, said his firm believes companies, including Nvidia, Google, and Amazon, will all \"sell lots\" of chips in the future.\n\n\"We don't see TPU as a significant threat to Nvidia's business, but it has been a real player in the market for many years,\" said Nanos. \"It is possible that Google sells TPU servers externally in the future, to many more customers. Right now, they are very selective.\"\n\nHave something to share? Contact this reporter via email at hlangley@businessinsider.com or Signal at hughlangley.01. Use a personal email address and a non-work device; here's our guide to sharing information securely.",
    "readingTime": 6,
    "keywords": [
      "processing units",
      "research note",
      "tpu business",
      "tpu chips",
      "google's tpus",
      "tpus google",
      "nvidia's gpu",
      "business insider",
      "customers",
      "models"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-tpu-ai-chip-explained-nvidia-2025-12",
    "thumbnail_url": "https://i.insider.com/69370a1771107c9f345785bb?width=1200&format=jpeg",
    "created_at": "2025-12-16T18:57:20.002Z",
    "topic": "finance"
  },
  {
    "slug": "sergey-brin-says-his-commute-involves-talking-to-an-unreleased-gemini-ai-model-about-building-data-centers",
    "title": "Sergey Brin says his commute involves talking to an unreleased Gemini AI model about building data centers",
    "description": "\"Give me a few weeks to actually ship what I have access to,\" Google cofounder Sergey Brin said, describing a coming Gemini Live model.",
    "fullText": "Sergey Brin has been dogfooding Google's AI on his commute.\n\nThe Google cofounder has been back in the thick of the company's AI development after stepping down as president of Alphabet in 2019, returning in recent years to work on Gemini.\n\nAt a Stanford University panel last week, an audience member asked Brin how he stayed sharp. What podcasts did Brin listen to in the car?\n\n\"I do talk to Gemini Live in the car often,\" Brin said. \"I just talk to it about stuff on my drive.\"\n\nBrin gave an example of what he asked Gemini Live while driving: \"I want to develop a data center, I need how many hundreds of megawatts of this kind of power, that kind of power, how much it's going to cost.\"\n\nGemini Live is Google's voice-enabled AI chatbot. The company first demoed the product in May 2024 at Google I/O and made it freely available in September 2024.\n\nFor car-chatting enthusiasts, Brin recommended drivers \"shouldn't do it now, because we have a way better version coming.\"\n\n\"The publicly available version right now is not the good version,\" he said. \"Give me a few weeks to actually ship what I have access to.\"\n\nBrin said that the current available version of Gemini Live is backed by an \"ancient model.\"\n\nGoogle's recent model updates have made waves in the AI world. In November, the company debuted Gemini 3. The model is more visual, Google said, and its \"most factual\" model to date.\n\nGemini 3 was also the first model Google added directly to search on day one. Users could immediately access it by clicking \"AI Mode,\" rather than having to visit a separate app or website.\n\nThe other LLM makers took note. Meta employees now have access to Gemini 3 Pro.\n\nOn the panel, Brin acknowledged that his answer to the driving question seemed \"kind of self-advertising.\" He does listen to some podcasts, he said.\n\n\"The 'All In' guys are actually one of my favorites,\" he said. Brin has appeared on the podcast multiple times.",
    "readingTime": 2,
    "keywords": [
      "gemini live",
      "model",
      "version",
      "access",
      "brin",
      "panel",
      "podcasts",
      "listen",
      "talk",
      "driving"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/google-sergey-brin-gemini-live-dogfooding-data-centers-2025-12",
    "thumbnail_url": "https://i.insider.com/6941736164858d02d216dd69?width=1200&format=jpeg",
    "created_at": "2025-12-16T18:57:19.489Z",
    "topic": "finance"
  },
  {
    "slug": "improved-gemini-audio-models-for-powerful-voice-interactions",
    "title": "Improved Gemini audio models for powerful voice interactions",
    "description": "An upgraded Gemini 2.5 Native Audio model across Google products and live speech translation in the Google Translate app.",
    "fullText": "Earlier this week, we introduced greater control over audio generation with an upgrade to our Gemini 2.5 Pro and Flash Text-to-Speech models.\n\nBut generating expressive speech is only one side of the conversation. Today, we’re releasing an updated Gemini 2.5 Flash Native Audio for live voice agents. This update improves the model’s ability to handle complex workflows, navigate user instructions, and hold natural conversations.\n\nGemini 2.5 Flash Native Audio is now available across Google products including Google AI Studio, Vertex AI, and has also started rolling out in Gemini Live and Search Live, bringing the naturalness of native audio to Search Live for the first time. This means you can more effectively brainstorm live with Gemini, get real-time help in Search Live, or build the next generation of enterprise-ready customer service agents.\n\nBeyond powering helpful agents, native audio unlocks new possibilities for global communication. We’re introducing live speech translation, a capability that enables streaming speech-to-speech translation for headphones. It preserves the speaker’s intonation, pacing and pitch. This beta experience is rolling out in the Google Translate app starting today.\n\nTo enable the breadth of use cases across surfaces and products, we have improved Gemini 2.5 Native Audio in three key areas:\n\nThe updated Gemini 2.5 Flash Native Audio’s performance against previous versions and industry competitors on ComplexFuncBench\n\nGoogle Cloud customers are already using Gemini’s native audio capabilities to drive real business results, from mortgage processing to customer calls.\n\nGemini now natively supports new live speech-to-speech translation capabilities designed to handle both continuous listening and two-way conversation.\n\nWith continuous listening, Gemini automatically translates speech in multiple languages into a single target language. This allows you to put headphones in and hear the world around you in your language.\n\nFor two-way conversation, Gemini’s live speech translation handles translation between two languages in real-time, automatically switching the output language based on who is speaking. For example, if you speak English and want to chat with a Hindi speaker, you’ll hear English translations in real-time in your headphones, while your phone broadcasts Hindi when you’re done speaking.\n\nGemini’s live speech translation has a number of key capabilities that help in the real world:\n\nStarting today, you can try it in a new beta experience in the Google Translate app for real-time translation in your headphones by connecting them to your device and tapping “Live translate.” This experience is rolling out to all Android devices in the US, Mexico and India with support for iOS and more regions coming soon.\n\nBased on feedback, we will continue to iterate on this experience and bring it to more Google products including the Gemini API in 2026.\n\nStart building voice agents today with Gemini 2.5 Flash Native Audio, now generally available on Vertex AI and as preview in the Gemini API. Try it out in Google AI Studio.\n\nGemini 2.5 Flash and 2.5 Pro text-to-speech models are also available via the Gemini API in Google AI Studio. Get started with the speech generation docs, explore the prompting guide, or check out the Gemini API Cookbook to get started.",
    "readingTime": 3,
    "keywords": [
      "translate app",
      "native audio",
      "flash native",
      "google products",
      "text-to-speech models",
      "continuous listening",
      "two-way conversation",
      "gemini flash",
      "voice agents",
      "beta experience"
    ],
    "qualityScore": 1,
    "link": "https://blog.google/products/gemini/gemini-audio-model-updates/",
    "thumbnail_url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-audio-flash__keyword_meta__dark.width-1300.png",
    "created_at": "2025-12-16T13:51:44.584Z",
    "topic": "tech"
  },
  {
    "slug": "fluency-wants-to-use-ai-agents-to-transform-advertising-see-the-pitch-deck-that-helped-it-raise-40m",
    "title": "Fluency wants to use AI agents to transform advertising. See the pitch deck that helped it raise $40M.",
    "description": "Adtech company Fluency uses automation, and now AI agents, to run digital ad campaigns across the web, Meta, Google, TikTok, and more.",
    "fullText": "Adtech company Fluency has built a platform that helps brands and agencies automate their campaigns across Meta, TikTok, Google, and other platforms. Now, it aims to introduce AI agents to the mix.\n\nLarge brands and agencies typically manage campaigns across multiple platforms and ad buying tools, known as demand-side platforms, each with separate logins and interfaces.\n\nFounded in 2017 in Burlington, Vermont, Fluency created what it describes as a digital advertising operating system. It connects to the various platforms' application programming interfaces (APIs) and lets its customers manage their digital ads using a single workflow.\n\n\"It takes a lot of people, and hours, and button clicking to be able to effectively execute your advertising,\" Fluency CEO Mike Lane said in an interview with Business Insider.\n\nHe added that Fluency wanted to \"help simplify and streamline that, much like HubSpot or Salesforce\" did in the customer relationship management (CRM) space. Fluency's competitor set includes other campaign management and ad automation tools, such as Hootsuite, Smartly, and Sprinklr, though these companies tend to cater to larger advertisers and agencies.\n\nThe company said it manages around $3 billion in annual ad spend for brands and agencies, including Cox Automotive, Innocean, and The Johnson Group.\n\nFluency is set to announce on Tuesday that it has raised a $40 million Series A investment round, led and fully funded by Integrity Growth Partners.\n\nFluency said it plans to invest the funds in enhancing its agentic AI capabilities, which can autonomously oversee marketers' digital ad campaigns. This could mean AI agents automatically swapping out ad creatives or copy with better-performing versions, for example.\n\nFluency mostly uses Amazon's Bedrock generative AI models, as well as Anthropic's Claude and Google's Gemini, the company said.\n\nLane said that for agentic AI to work effectively for a marketer's digital advertising, the AI needs robust frameworks and structures to operate within, connections to all major media platforms, and for everything to be integrated into a single system.\n\nHe said these factors make Fluency well-positioned to be \"the premium platform for agentic\" AI in the digital ads space.\n\nCheck out the pitch deck Fluency used to secure its $40 million Series A investment, shared exclusively with Business Insider. Some of the slides have been omitted or redacted.\n\nFluency says marketers often use \"point solutions\" that don't interact with each other, which can also bring in issues around compliance and performance.\n\nFluency says ad strategists often have to cobble together several different tech solutions to handle tasks like targeting or reviewing campaign performance.\n\nFluency's platform aims to address these concerns as a kind of \"one-stop shop\" across various channels.\n\nThose attributes include a blue-chip client base and high customer-retention rates.\n\nThere's been a shift in budgets from big agency holding companies to independent agencies, though smaller firms have constrained resources, Fluency says.\n\nFluency says its \"serviceable addressable market\" is expected to grow at a compound annual growth rate of 11%, reaching $6.4 billion by 2028.\n\nFluency says its platform serves agencies and brands better than agency-developed tech, in-house channel management, legacy omni-channel tools, and point solutions.\n\nThe slide showcases how the Fluency platform integrates a marketer's own data, big media platforms, and DSPs using AI automation to manage campaigns and assess their performance.\n\nFluency says its tools can save customers time, improve ad performance, and boost their revenue.",
    "readingTime": 3,
    "keywords": [
      "business insider",
      "series investment",
      "digital ads",
      "campaigns across",
      "manage campaigns",
      "media platforms",
      "digital advertising",
      "performance fluency",
      "agencies",
      "brands"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/pitch-deck-fluency-series-a-agentic-ai-adtech-platform-2025-12",
    "thumbnail_url": "https://i.insider.com/693fdd0b832e0ef1ead63485?width=1200&format=jpeg",
    "created_at": "2025-12-16T13:51:44.118Z",
    "topic": "finance"
  },
  {
    "slug": "meta-is-making-ai-core-to-how-we-work-with-the-help-of-tools-from-google-and-openai",
    "title": "Meta is making 'AI core to how we work' with the help of tools from Google and OpenAI",
    "description": "Meta has given employees access to Google Gemini and OpenAI's GPT-5 alongside its own AI tools to help boost productivity.",
    "fullText": "In its push to create an \"AI-first\" workplace, Meta is expanding employees' access to tools from rivals such as Google and OpenAI, Business Insider has learned.\n\nThe social media giant has been encouraging employees to integrate AI tools into nearly everything they do, according to multiple internal documents and posts seen by Business Insider.\n\nOne of the company's priorities is to \"make AI core to how we work,\" Meta's chief information officer, Atish Banerjea, told employees in a June memo outlining a plan to use Meta's own models — which use the naming convention \"Llama\" — alongside products from other firms.\n\nIn November, a Meta engineer said in an internal post that all employees have access to Google's Gemini 3 Pro and OpenAI's ChatGPT-5. The post included a list of AI tools Meta employees have access to, including their use cases. Business Insider has recreated the list below.\n\nA Meta spokesperson confirmed the revamped suite of AI tools and pointed to an earlier comment shared with Business Insider about AI adoption, stating: \"It's well-known that this is a priority, and we're focused on using AI to help employees with their day-to-day work.\"\n\nThe social media giant opened the floodgates to rival AI models in June.\n\nAmong those is an internal coding tool called Devmate that uses Anthropic's Claude, Business Insider previously reported. Google's Gemini and NotebookLM Pro are also available across the company to help employees \"work smarter and have more impact,\" Banerjea told employees in the June memo.\n\nMeta has invested tens of billions into its own consumer-facing AI models, and employees have access to an internal AI assistant called Metamate, which is built on its Llama models.\n\nAfter Meta struck a deal over the summer the startup Midjourney to weave its AI-image generator into its products and models, the company made the tool available to employees in October for \"concept and production uses\" to speed up design work and creative prototyping, according to an internal post ahead of the rollout, seen by Business Insider.\n\nGemini isn't the only Google tool Meta is embracing. The company migrated its internal productivity suite over the summer to Google Workspace — including Chat, Gmail, Docs, and Drive — describing the move in a June memo as a way to \"unlock AI-driven capabilities\" and better integrate with its expanding toolset.\n\nOn the engineering side, Meta has expanded access to agentic coding systems, adding Google's Gemini 3 Pro and exploring new integrations with tools like OpenAI's Codex CLI and Google's Gemini CLI. \"Rather than focusing on specific solutions, our strategy centers on outcomes: increasing productivity, accelerating development, and ensuring you have access to the best agentic coding experiences,\" Reality Labs executive Maher Saba told employees in a November memo seen by Business Insider.\n\nTo encourage adoption and experimentation, Meta has gamified the use of AI, Business Insider previously reported. Earlier this year, it launched an internal game called \"Level Up,\" which rewards employees with badges for using AI in different ways. Leaders are also tying performance to results achieved through AI, rewarding those who can prove \"AI-driven impact\" this year, and including it as part of performance reviews in 2026.\n\nHave a tip? Contact this reporter via email at jmann@businessinsider.com or Signal at jyotimann.11. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "google's gemini",
      "june memo",
      "gemini pro",
      "insider previously",
      "social media",
      "media giant",
      "agentic coding",
      "business insider",
      "employees",
      "internal"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/meta-ai-tools-internal-google-gemini-openai-chatgpt-llama-claude-2025-12",
    "thumbnail_url": "https://i.insider.com/69401c2a832e0ef1ead635fd?width=1200&format=jpeg",
    "created_at": "2025-12-16T13:51:43.771Z",
    "topic": "finance"
  },
  {
    "slug": "i-built-a-webapp-to-inject-exifxmp-gps-metadata-into-video-files",
    "title": "I built a webapp to inject Exif/XMP GPS metadata into video files",
    "description": "Geotag video tool for local SEO. Inject GPS EXIF data, render geo-tagged videos, and boost Google Maps visibility.",
    "fullText": "78% of video editors strip your GPS data to protect privacy. We put it back.\nCreate Geo-Injected videos from your photos, or restore GPS data to your existing video clips in 30 seconds. The secret weapon for Local SEO dominance.\n\nNo subscription required for first pack\n\nWatch how we upload, inject GPS, and verify metadata in under a minute—no setup, just drop a file and go.\n\nYou spend hours designing beautiful videos on Canva or Adobe Express. But the moment you export, they delete your location metadata.\n\nTo Google's algorithm, your video could be from anywhere. It loses the critical \"Local Relevance\" signal that drives Map Pack rankings.\n\nDon't let your hard work go to waste.\n\nWhether starting from photos or optimizing existing videos, we bridge the gap between your content and the algorithm.\n\nDon't have a video? Upload 3-5 photos. Our AI adds Ken Burns effects and transitions. No editing skills needed.\n\nHave a polished video from Canva or CapCut? Upload it directly. We inject the missing Latitude, Longitude, and XMP metadata without reducing quality.\n\nManaging 50 locations? No problem. Generate bulk updates for all your clients in minutes, not days.\n\nDrag and drop any video file here. We'll show you exactly what Google sees.\n\nOpen the Video Metadata Checker page →\n\nFrom a risk-free trial to agency-grade throughput.",
    "readingTime": 2,
    "keywords": [
      "metadata",
      "videos",
      "photos",
      "existing",
      "drop",
      "file",
      "canva",
      "algorithm",
      "don't",
      "upload"
    ],
    "qualityScore": 0.95,
    "link": "https://www.getgeovideo.com",
    "thumbnail_url": "https://www.getgeovideo.com/opengraph-image?8dd7cec6fe31a98b",
    "created_at": "2025-12-16T06:59:54.264Z",
    "topic": "tech"
  },
  {
    "slug": "the-best-smart-speakers-of-2025",
    "title": "The best smart speakers of 2025",
    "description": "We've rounded up the top smart speakers from brands like Sonos, Amazon, Google, Apple, and Bose.",
    "fullText": "The Era 100 is one of the most versatile smart speakers. It features powerful sound combined with a wide range of connectivity options. The Era 100 can be used to add music to a bedroom or be combined with other Sonos speakers to build out a wireless home theater system.\n\n Check price at Amazon\n\n Check price at Sonos",
    "readingTime": 1,
    "keywords": [
      "the era",
      "speakers",
      "combined",
      "sonos",
      "check"
    ],
    "qualityScore": 0.3,
    "link": "https://www.businessinsider.com/guides/tech/best-smart-speakers",
    "thumbnail_url": "https://i.insider.com/6931fedc04d0f0a114f17297?width=1200&format=jpeg",
    "created_at": "2025-12-16T03:49:40.536Z",
    "topic": "tech"
  },
  {
    "slug": "i-tested-googles-new-live-translation-with-airpods-and-it-actually-works-well",
    "title": "I Tested Google’s New Live Translation With AirPods, and It Actually Works Well",
    "description": "You don't need Pixel Buds to translate a live conversation anymore.",
    "fullText": "I can be pretty tough on AI, especially when it's used to make misinformation slop. But as cynical as I may seem, I do acknowledge that there are plenty of useful and beneficial features that AI powers. Take live translation, for instance: Not long ago, the concept of a device that could translate someone else's words directly in your ear as they spoke would seem like some far future technology. But not only is it not a futuristic technology, both Google and Apple have their own takes on the feature that users can take advantage of.\n\nThat said, not all iPhone and Android users alike have been able to use live translate. Both companies have limited the feature to work with their respective earbuds: For Apple, that's the AirPods Pro 2 and AirPods Pro 3; for Google, that's the Pixel Buds. Without your platform's flagship earbuds, you haven't been able to use live translation, and instead need to stick with the rest of your translation app's experience, whether that be Apple Translate or Google Translate. Lucky for Android users, that's no longer the case for the latter.\n\nOn Friday, Google announced new Gemini translation capabilities for its proprietary translation app. The company says these new updates introduce \"state-of-the-art text translation quality,\" with more nuanced, natural, and accurate translations. Importantly, however, as part of those upgrades, the company is launching a beta where all Google Translate users can access live translation through any headphones—not just Pixel Buds. This initial rollout is only available on the Android version of Google Translate in the U.S., Mexico, and India, though Google says the company will bring the feature to iOS and more regions in the next year.\n\nThis is kind of huge: Companies typically like to keep features like this locked behind their own platform as a marketing tactic. You're more likely to buy Pixel Buds over other earbuds or headphones if you really want to try live translation. However, you don't even need to buy a new pair of headphones to use this feature at all: As long as you have some type of headphones or earbuds connected to your Android device, you can translate conversations on the fly.\n\nI gave this a shot on my Pixel 8 Pro with my AirPods Max, by playing a video of people speaking Portuguese. Set up wasn't the simplest: First, it took forever for the Pixel to recognize my AirPods, despite the headphones being in pairing mode for some time, but that's beside the point. The key issue was getting Google Translate to present the new beta for live translation. When I first opened it, it was using the older live translate feature, which didn't work with my AirPods. I had the latest version running, so I uninstalled and reinstalled the app. When it launched, I didn't have live translate at all. Finally, after force quitting and reopening the app, I got a pop-up for the new live translation beta experience.\n\nThe next part was user error: I had my language set to the target language (Portuguese), and vice versa. As such, Google assumed I would be the one speaking Portuguese, and didn't vocalize the English translation. Once I flipped the languages, and confirmed that English would be spoken through my headphones, the feature started working—and well, for that matter. The video I choose was taken from a news broadcast, with two anchors, and various speakers during news segments. Once the video started, I could see Google Translation translating the words on my screen, and, after about four seconds, I heard the audio translated in my hear. Google Translate even tries to match the speaker's voice, and though it certainly isn't a deepfake, it does well enough to distinguish different speakers from one another. It even tried to take on more a serious tone to match the anchor's, versus the more casual tone of one of the people interviewed in a news segment.\n\nI tried a couple of other videos in different languages, but this time, using the \"Detect language\" feature rather than a preset target language. The app was able to recognize this video was spoken in Thai, and this one was spoken in Urdu, and translated both accordingly. And while I can't verify the quality of the translation (I am sadly not fluent in any other language), the experience was overall easy enough to follow. The speed of speech can get a bit slow at times, perhaps because the AI has a lot to process at once, but as long as you turn up the volume on your headphones, it's easy enough to follow.\n\nAll that to say, I'm very interested to give this a try in a real world scenario. Even though my daily driver is an iPhone, I might need to start carrying around my Pixel 8 Pro just in case.",
    "readingTime": 5,
    "keywords": [
      "speaking portuguese",
      "android users",
      "airpods pro",
      "target language",
      "google translate",
      "pixel pro",
      "feature",
      "headphones",
      "earbuds",
      "that's"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-now-supports-live-translate-for-all-earbuds-on-android?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KCHW4SHQN96QEXECA228N6KW/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-16T03:49:38.804Z",
    "topic": "tech"
  },
  {
    "slug": "google-ai-summaries-are-ruining-the-livelihoods-of-recipe-writers-its-an-extinction-event",
    "title": "Google AI summaries are ruining the livelihoods of recipe writers: ‘It’s an extinction event’",
    "description": "AI Mode is mangling recipes by merging instructions from multiple creators – and causing them huge dips in ad traffic\nThis past March, when Google began rolling out its AI Mode search capability, it began offering AI-generated recipes. The recipes were not all that intelligent. The AI had taken elements of similar recipes from multiple creators and Frankensteined them into something barely recognizable. In one memorable case, the Google AI failed to distinguish the satirical website the Onion from legitimate recipe sites and advised users to cook with non-toxic glue.\nOver the past few years, bloggers who have not secured their sites behind a paywall have seen their carefully developed and tested recipes show up, often without attribution and in a bastardized form, in ChatGPT replies.",
    "fullText": "AI Mode is mangling recipes by merging instructions from multiple creators – and causing them huge dips in ad traffic\n\nThis past March, when Google began rolling out its AI Mode search capability, it began offering AI-generated recipes. The recipes were not all that intelligent. The AI had taken elements of similar recipes from multiple creators and Frankensteined them into something barely recognizable. In one memorable case, the Google AI failed to distinguish the satirical website the Onion from legitimate recipe sites and advised users to cook with non-toxic glue.\n\nOver the past few years, bloggers who have not secured their sites behind a paywall have seen their carefully developed and tested recipes show up, often without attribution and in a bastardized form, in ChatGPT replies. They have seen dumbed-down versions of their recipes in AI-assembled cookbooks available for digital downloads on Etsy or on AI-built websites that bear a superficial resemblance to an old-school human-written blog. Their photos and videos, meanwhile, are repurposed in Facebook posts and Pinterest pins that link back to this digital slop.\n\nRecipe writers have no legal recourse because recipes generally are not copyrightable. Although copyright protects published or recorded work, they do not cover sets of instructions (although it can apply to the particular wording of those instructions).\n\nWithout this essential IP, many food bloggers earn their living by offering their work for free while using ads to make money. But now they fear that casual users who rely on search engines or social media to find a recipe for dinner will conflate their work with AI slop and stop trusting online recipe sites altogether.\n\n“There are a lot of people that are scared to even talk about what’s going on because it is their livelihood,” says Jim Delmage who, with his wife, Tara, runs the blog and YouTube channel Sip and Feast.\n\nMatt Rodbard, the founder and editor-in-chief of the website Taste, is even more pessimistic. Taste used to publish recipes more frequently, but now it mostly focuses on journalism and a podcast (which Rodbard hosts). “For websites that depend on the advertising model,” he says, “I think this is an extinction event in many ways.”\n\nThe holiday season is traditionally when food bloggers earn most of their ad revenue. For many, this year has been slower than usual. One blogger, Carrie Forrest of Clean Eating Kitchen, told Bloomberg that in the past two years, she has lost 80% of her traffic.\n\nOthers, like Delmage and Karen Tedesco, the author of the blog Familystyle Food, say their numbers, and ad revenue, have remained steady – so far. They attribute this to focusing their energies less on trying to game the search engines than on the long-term goal of attracting regular followers – and, in Delmage’s case, viewers.\n\nTedesco’s strategy has been to create recipes that rely on her experience and technical knowhow honed by years in restaurant kitchens and as a personal chef. Her Italian meatball recipe, for example, based on her mother’s, includes advice about which meat to use, an explanation of why milk-soaked breadcrumbs are essential for texture, and a dozen process photos and a video.\n\nBut she is still worried about the potential impact of AI. When she recently did a Google search for “Italian meatballs”, Familystyle Food appeared as the top result. Then she switched to AI Mode. There, she found the recipe had been Frankensteined – or “synthesized” as Gemini put it – into a new recipe with nine other sources (including Sip and Feast and a Washington Post recipe for Greek meatballs). The AI-generated recipe was little more than a list of ingredients and six basic steps with none of the details that make Tedesco’s recipe unique.\n\nAI Mode linked to all 10 recipes, including Tedesco’s, but, she says, “I don’t think many people are actually clicking on the source links. At this point, they’re absolutely trusting in the results that are getting thrown in their faces.”\n\nOther bloggers have seen a more definite impact on their viewership. Adam Gallagher, who runs Inspired Taste with his wife, Joanne, and who has become an outspoken critic of AI on social media, told the podcast Marketing O’Clock that since spring, he has noticed that while the number of times viewers saw links to the site on Google has increased, the number of actual site visitors has decreased. This indicates, to him, that users are satisfied with the search engine’s AI interpretation of Inspired Taste’s recipes.\n\nAfter the Gallaghers posted about the discrepancy on X and Instagram, a number of readers replied to say they had not realized there was a difference between the recipes on the blog and the version that showed up in Google searches. They had just appreciated the convenience of not having to click on another website, especially when Google’s page design was so clean and uncluttered.\n\nRodbard acknowledges that many food blogs have gotten ugly and overloaded with ads, which has exacerbated the problem. “Ad tech on these recipe blogs has gotten so bad, so many pop-up windows and so much crashing, we kind of lost as publishers,” he says.\n\nAccording to Tom Critchlow, the EVP of audience growth at Raptive, a media company that works with many food bloggers to find advertisers, it isn’t ads that are driving viewers away. It’s Google itself, with its changes to the algorithm and now with AI Mode, that’s making the sites harder to find.\n\nThere is some hope though: a survey of 3,000 US adults commissioned by Raptive showed that the more interaction people had with AI, the less they wanted to engage with it, and nearly half the respondents rated AI content less trustworthy than content made by a human.\n\nBut unless the public rebels against AI Mode, there is only so much bloggers can do. They can block OpenAI’s training crawler, which gathers information that ChatGPT uses to create content, including its own recipe generator, but theyare not necessarily willing to make themselves invisible to web searches; as Delmage puts it: “You can’t bite the hand that feeds you.”\n\nThere is also the option of moving over to a subscription model, such as Substack or Patreon, and keeping the recipes behind a paywall, but both Tedesco and Delmage point out that the most successful Substackers, like Caroline Chambers or David Lebovitz, came to the platform with much more substantial followings than they have. “If I were to give up my website or even try to go over to Substack, I would be broke,” Tedesco says.\n\nRodbard suggests that the analog version of the recipe blog, the cookbook, might be due for a comeback. Cookbooks, after all, offer the same experience of spending time and learning from a trusted source, and it’s likely the recipes have been tested. As a bonus, unlike phones or laptops, they don’t go dark when you neglect them for too long and you can splash tomato sauce on them without inflicting permanent damage. According to the market research firm Circana (formerly BookScan), sales of baking cookbooks are up 80% this year, but other areas have been relatively flat.\n\nBut AI bots are stealing from published cookbooks, too. When Meta was training its own AI, it compiled thousands of books into a dataset called Library Genesis (LibGen). Now unscrupulous publishers have raided LibGen and repackaged some of the books into dupes, which they are selling on Amazon.\n\nAs more people become aware of the amount of AI slop on the internet and how to identify it, Critchlow believes they will develop a greater appreciation for content produced by humans. “People will ultimately place a higher premium on being able to know that these recipes have been tested and made by somebody that I follow or somebody I respect or somebody that I like,” he says.\n\nThe recipe creators themselves are not so sure. “I’m putting my faith in that there’s always going to be a segment of people who really want to learn something,” Tedesco says. But as for the business of blogging itself, “it’s like a rolling tide. It’s always up and down and you have to roll with it and adapt.”",
    "readingTime": 7,
    "keywords": [
      "behind paywall",
      "social media",
      "search engines",
      "bloggers earn",
      "ai mode",
      "food bloggers",
      "recipe sites",
      "familystyle food",
      "recipes",
      "website"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/15/google-ai-recipes-food-bloggers",
    "thumbnail_url": "https://i.guim.co.uk/img/media/6dad6c44cff97b68d6e8c95e43637b3b460a7a57/398_0_7162_5733/master/7162.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=674b4b0df33a8374ebfeee93b5c3e646",
    "created_at": "2025-12-15T18:57:52.494Z",
    "topic": "tech"
  },
  {
    "slug": "openai-hires-veteran-google-executive-as-corporate-development-vp",
    "title": "OpenAI hires veteran Google executive as corporate development VP",
    "description": null,
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://www.investing.com/news/stock-market-news/openai-hires-veteran-google-executive-as-corporate-development-vp-4408927",
    "thumbnail_url": "https://i-invdn-com.investing.com/trkd-images/LYNXMPELBE156_L.jpg",
    "created_at": "2025-12-15T18:57:50.734Z",
    "topic": "finance"
  },
  {
    "slug": "white-house-launches-2year-tech-force-hiring-for-roughly-1000-jobs-that-will-pay-130000-to-195000",
    "title": "White House launches 2-year 'Tech Force' hiring for roughly 1,000 jobs that will pay $130,000 to $195,000",
    "description": "US Tech Force employees will have recruitment opportunities with a host of private companies ranging from Meta and Google to Apple and Nvidia.",
    "fullText": "Young tech talent hoping to land a job in the private sector has a new way in: the public sector.\n\nPresident Donald Trump's administration announced the launch of the United States Tech Force on Monday, a two-year program that offers participants federal government jobs and eventual recruitment opportunities for private sector roles.\n\nThe government is partnering with at least 28 tech companies on the program, including Apple, Amazon Web Services, Anduril, Dell, Box, Meta, Microsoft, Nvidia, OpenAI, Palantir, Oracle, Uber, and xAI, among others.\n\nThe Office of Personnel Management is spearheading the early-career program in partnership with other agencies. Participants will be placed at agencies across the government to work on modernization projects and, in part, implement AI, according to the Tech Force website.\n\nTech Force plans to hire around 1,000 applicants to start, according to the site, and is looking for people \"highly skilled in the areas of software engineering, artificial intelligence, cybersecurity, data analytics, or technical project management.\"\n\nApplicants will submit an application, complete a technical assessment, have interviews with agency leaders, and undergo a background test. OPM will do the initial resume review and technical assessment before passing the hiring decisions off to individual agencies, OPM Director Scott Kupor said during a call before the announcement.\n\nPrivate sector partners can provide training resources and mentorship opportunities, and have committed to considering Tech Force participants for employment after the two years, according to the site. Kupor said on Monday's call that there will also be a speaker series featuring company CEOs. The program, he added, will end with public- and private-sector job fairs.\n\nCompanies haven't made any firm hiring commitments, Kupor said on the call.\n\n\"It wouldn't be appropriate for us to ask companies to hire people without knowing who those people are,\" Kupor said. He added that he'll \"be certainly disappointed\" if the program doesn't lead to hiring by the private-sector partners.\n\nSalaries will likely range from around $130,000 to $195,000, Kupor said. There are no specific degree or experience requirements for the positions, which will be posted on USA Jobs. Those interested are encouraged \"to apply as soon as possible\" on the site. Kupor said he hopes to get everybody hired by the end of March.\n\nThe roles are largely based in Washington, DC, and remote work options will depend on the agency and role, according to the site. They're non-partisan positions and are entirely distinct from the US DOGE Service.\n\nThough the Tech Force is primarily an early-career program, the government is also seeking \"experienced engineering managers from private sector partners,\" the site says. Companies can nominate employees, who will generally be expected to take unpaid leave or otherwise separate from their role to take the two-year position, per the site.\n\n\"They will become full-time employees of the government for the time they're here, and therefore be subject to all the various ethics rules,\" Kupor said. He added that each company will have its own policy, but expects that it will be \"easy\" for participants to return to their private-sector positions.\n\nKupor said on the call that the government plans to run the program on an annual basis and that he hopes to expand it in the years to come.\n\nAre you interested in applying to the Tech Force program? Contact this reporter via email at atecotzky@insider.com or Signal at alicetecotzky.05. Use a personal email address and a nonwork device; here's our guide to sharing information securely.",
    "readingTime": 3,
    "keywords": [
      "technical assessment",
      "site kupor",
      "sector partners",
      "early-career program",
      "tech force",
      "participants",
      "agencies",
      "hiring",
      "private-sector",
      "positions"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/us-tech-force-jobs-salary-info-meta-nvidia-google-openai-2025-12",
    "thumbnail_url": "https://i.insider.com/694031d3832e0ef1ead63869?width=1200&format=jpeg",
    "created_at": "2025-12-15T18:57:49.620Z",
    "topic": "finance"
  },
  {
    "slug": "sergey-brin-who-came-back-to-google-to-work-on-gemini-says-staying-retired-would-have-been-a-big-mistake",
    "title": "Sergey Brin, who came back to Google to work on Gemini, says staying retired would have been a 'big mistake'",
    "description": "Sergey Brin says the pandemic pushed him back into Google, where he's now helping drive its Gemini AI efforts.",
    "fullText": "Sergey Brin tried retirement — and immediately regretted it.\n\nSpeaking at Stanford University's School of Engineering centennial celebration last week, the Google cofounder said he stepped back from day-to-day work in December 2019, imagining he'd spend leisurely days and \"sit in cafés and study physics.\"\n\n\"That didn't work because there were no more cafés,\" he joked.\n\nWorse, he said he felt himself \"spiraling\" and \"kind of not being sharp\" without the intellectual stimulation he'd always relied on.\n\nSo as soon as Google began allowing a small number of employees back into its offices, he joined them — eventually diving into what became Gemini, Google's flagship AI model.\n\n\"To be able to have that technical creative outlet, I think that's very rewarding,\" the 52-year-old said. \"If I'd stayed retired, I think that would've been a big mistake.\"\n\nBrin also offered a candid assessment of Google's AI trajectory.\n\nDespite publishing the 2017 \"Transformer\" paper that underpins nearly every major AI model today, he said Google \"underinvested\" in the technology and was \"too scared to bring it to people because chatbots say dumb things.\"\n\nOpenAI, he said, \"ran with it, which, good for them.\"\n\nStill, he said Google retained an edge through its long-standing investment in neural-network research, custom AI chips, and massive data center infrastructure.\n\n\"Very few have that scale,\" he said.\n\nAsked what students should study in an era when AI can code, Brin warned against fleeing technical fields.\n\n\"I wouldn't switch to comparative literature because you think AI is good at coding,\" he said. \"The AI is probably even better at comparative literature.\"\n\nHe also shared what he sees as the biggest mistake founders make — one he admits he fell into with Google Glass.\n\nHe rushed the product before it was affordable, polished, or even actually ready.\n\n\"Everybody thinks they're the next Steve Jobs,\" he said. \"I've definitely made that mistake.\"\n\nNow deeply involved in Gemini, Brin said the pace of AI development keeps him energized.\n\n\"It's absolutely amazing just the rate of innovation,\" he said. \"If you skip the news for a month, you're way behind.\"",
    "readingTime": 2,
    "keywords": [
      "comparative literature",
      "mistake",
      "back",
      "he'd",
      "cafés",
      "study",
      "model",
      "technical",
      "google",
      "brin"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/sergey-brin-says-leaving-google-before-gemini-was-big-mistake-2025-12",
    "thumbnail_url": "https://i.insider.com/693fe6a064858d02d216c930?width=1200&format=jpeg",
    "created_at": "2025-12-15T13:53:46.886Z",
    "topic": "finance"
  },
  {
    "slug": "alphabet-poised-for-another-paper-gain-as-spacex-valuation-jumps",
    "title": "Alphabet Poised for Another Paper Gain as SpaceX Valuation Jumps",
    "description": "Alphabet Inc. is set to book another sizable paper gain after SpaceX completes a tender offer that effectively values the closely held company at about $800 billion.",
    "fullText": "TechnologyAIBy Edward LudlowSaveAlphabet Inc. is set to book another sizable paper gain after SpaceX completes a tender offer that effectively values the closely held company at about $800 billion.SpaceX’s insider share sale was priced at $421 a share, Bloomberg reported Friday, which would mark a sharp jump in valuation from earlier secondary transactions. That is likely to lift the carrying value of Google’s long-standing investment in Elon Musk’s rocket and satellite company.",
    "readingTime": 1,
    "keywords": [],
    "qualityScore": 0.2,
    "link": "https://www.bloomberg.com/news/articles/2025-12-14/alphabet-poised-for-another-paper-gain-as-spacex-valuation-jumps",
    "thumbnail_url": "https://assets.bwbx.io/images/users/iqjWHBFdfxIU/i5GO2Trj7JYc/v1/1200x800.jpg",
    "created_at": "2025-12-14T18:50:23.638Z",
    "topic": "finance"
  },
  {
    "slug": "alphabet-poised-for-another-paper-gain-as-spacex-valuation-jumps",
    "title": "Alphabet poised for another paper gain as SpaceX valuation jumps",
    "description": "Alphabet, Google’s parent, has been an investor in SpaceX since at least 2015, when it joined Fidelity Investments in a $1 billion funding round for a combined stake of about 10% at the time.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/14/alphabet-stock-gain-spacex-investment-stake-valuation-musk/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2202950148-e1765729974384.jpg?resize=1200,600",
    "created_at": "2025-12-14T18:50:17.951Z",
    "topic": "finance"
  },
  {
    "slug": "google-adds-native-hls-video-playback-to-chrome-desktop",
    "title": "Google adds native HLS video playback to Chrome desktop",
    "description": "After nearly four years of development, Google is finally rolling out native HTTP Live Streaming (HLS) playback to Chrome on desktop with version 142 and",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://tech-ish.com/2025/12/08/google-chrome-microsoft-edge-chromium-native-hls-playback-desktop/",
    "thumbnail_url": "https://tech-ish.com/wp-content/uploads/2025/06/Google-Chrome-logo.jpg",
    "created_at": "2025-12-14T13:19:49.290Z",
    "topic": "tech"
  },
  {
    "slug": "a-look-at-an-android-itw-dng-exploit",
    "title": "A look at an Android ITW DNG exploit",
    "description": "Posted by Benoît Sevens, Google Threat Intelligence Group Introduction Between July 2024 and February 2025, 6 suspicious image files were ...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://googleprojectzero.blogspot.com/2025/12/a-look-at-android-itw-dng-exploit.html",
    "thumbnail_url": "https://blogger.googleusercontent.com/img/a/AVvXsEiiwr_N_KPj_7IxkyMILQw6UiTrRKfIWj4y45LHcc44O79yYN-BMo-2Ex59570Ekwdwhsf5kjvrDmmWSOETFa_KLLGlSe6uL0zvgm-3_75rJssKQ-wsaGjwH7o0yHQcLvFOepn9tL7NGyOjVt1w0zK3wHT9O8yEIkD1CQkV2sPDuH69iy4laJsi_y19lnM=w1200-h630-p-k-no-nu",
    "created_at": "2025-12-14T03:53:12.729Z",
    "topic": "tech"
  },
  {
    "slug": "google-scholar-labs-an-ai-powered-scholar-search",
    "title": "(Google) Scholar Labs: An AI Powered Scholar Search",
    "description": "Research questions are often detailed. Answering them can require looking at a topic from multiple angles. Today, we are introducing Scholar...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://scholar.googleblog.com/2025/11/scholar-labs-ai-powered-scholar-search.html",
    "thumbnail_url": "https://lh3.googleusercontent.com/blogger_img_proxy/AEn0k_uoL5kmMaHdVOCeXyiSu33BBR_EwE7IMv0oGoKpSVzkYvTaMqW-AN5qQd5IRSeiRaJ0v8GzUTRs8RS2s635lyNuhVMgiD1EmrX17KSDV7ZAB4B1bnjfrCuvZtyfsprsx0jKfRhN-eDcebPWcw=w1200-h630-p-k-no-nu",
    "created_at": "2025-12-13T18:48:05.743Z",
    "topic": "tech"
  },
  {
    "slug": "i-work-in-ai-security-at-google-and-there-are-some-things-i-would-never-tell-chatbots-i-follow-4-rules-to-use-ai-safely",
    "title": "I work in AI security at Google and there are some things I would never tell chatbots. I follow 4 rules to use AI safely.",
    "description": "Harsh Varshney, who works on Chrome AI security at Google, shares four tips for protecting your data and identity when you talk to AI chatbots.",
    "fullText": "This as-told-to essay is based on a conversation with 31-year-old Harsh Varshney, who works at Google and lives in New York. The following has been edited for length and clarity.\n\nAI has quickly become a silent partner in our daily lives, and I can't imagine life without AI tools.\n\nDay-to-day, they help me with deep research, note-taking, coding, and online searches.\n\nBut my job means I'm very aware of the privacy concerns associated with using AI. I've worked at Google since 2023 and spent two years as a software engineer on the privacy team, building infrastructure to protect user data. I then switched to the Chrome AI security team, where I help secure Google Chrome from malicious threats, like hackers and those who use AI agents to conduct phishing campaigns.\n\nAI models use data to generate helpful responses, and we users need to protect our private information so that harmful entities, like cybercriminals and data brokers, can't access it.\n\nHere are four habits I've made that I believe are essential for protecting my data while using AI.\n\nSometimes, a false sense of intimacy with AI can lead people to share information online that they never would otherwise. AI companies may have employees who work on improving the privacy aspects of their models, but it's not advisable to share credit card details, Social Security numbers, your home address, personal medical history, or other personally identifiable information with AI chatbots.\n\nDepending on the version being used, the information shared with public AI chatbots can be used to train future models and generate responses that are more relevant. This could result in \"training leakage,\" where the model memorizes personal information about one user and later regurgitates it in responses to another. Plus, there's the risk of data breaches, which would expose what you've shared with a chatbot.\n\nI treat AI chatbots like a public postcard. If I wouldn't write a piece of information on a postcard that could be seen by anyone, I wouldn't share it with a public AI tool. I'm not confident about how my data could be used for future training.\n\nIt's important to identify whether you're using a more public AI tool or an enterprise-grade one.\n\nWhile it's uncertain how conversations are used for training public AI models, companies can pay for \"enterprise\" models. Here, models aren't typically meant to train on user conversations, so it's safer for employees to talk about their work and company projects.\n\nThink of it like having a conversation in a crowded coffee shop where you could be overheard, versus a confidential meeting in your office that stays within the room.\n\nThere have reportedly been instances where employees have accidentally leaked company data to ChatGPT. If you work on unreleased company projects or are trying to get a patent, you probably don't want to discuss your plans with a non-enterprise-grade chatbot due to the risk of leakage.\n\nI don't discuss projects I'm working on at Google with public chatbots. Instead, I use an enterprise model, even for tasks as small as editing a work email. I'm much more comfortable sharing my information because my conversations aren't used for training, but I still minimize the personal information I share.\n\nAI chatbots usually keep a history of your conversations, but I recommend deleting it on both enterprise and public models regularly to protect your user privacy in the long term. Due to the risk of your account being compromised, it's a good precautionary habit to have, even if you're confident you aren't putting private data into the tools.\n\nOnce, I was surprised that an enterprise Gemini chatbot was able to tell me my exact address, even though I didn't remember sharing it. It turned out, I had previously asked it to help me refine an email, which included my address. Because the tool has long-term memory features, enabling it to remember information from previous conversations, it could identify what my address was and retain it.\n\nSometimes, if I'm searching for things I don't want the chatbot to remember, I'll use a special mode, a bit like incognito mode, where the bots don't store my history or use the information to train models. ChatGPT and Gemini call this the \"temporary chat\" feature.\n\nIt's better to use AI tools that are well-known and are more likely to have clear privacy frameworks and other guardrails in place.\n\nOther than Google's products, I like to use OpenAI's ChatGPT and Anthropic's Claude.\n\nIt's also helpful to review the privacy policies of any tools you use. Sometimes, they'll explain more about how your data is used to train the model. In the privacy settings, you can also look for a section with the option to \"improve the model for everyone.\" By making sure that setting is turned off, you're preventing your conversations from being used for training.\n\nAI technology is incredibly powerful, but we must be cautious to ensure our data and identities are safe when we use it.\n\nDo you have a story to share about using AI to help you at work? Contact this reporter at ccheong@businessinsider.com",
    "readingTime": 5,
    "keywords": [
      "models",
      "privacy",
      "it's",
      "conversations",
      "chatbots",
      "training",
      "tools",
      "user",
      "address",
      "train"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-ai-security-safe-habits-privacy-data-2025-12",
    "thumbnail_url": "https://i.insider.com/69395b7504d0f0a114f1bc26?width=1200&format=jpeg",
    "created_at": "2025-12-13T13:18:39.919Z",
    "topic": "finance"
  },
  {
    "slug": "google-and-apple-roll-out-emergency-security-updates-after-zeroday-attacks",
    "title": "Google and Apple roll out emergency security updates after zero-day attacks",
    "description": "Apple released patches for all of its flagship devices to fix security flaws under attack. Google also updated Chrome to remediate one vulnerability exploited in the attacks.",
    "fullText": "Apple and Google have released several software updates to protect against a hacking campaign targeting an unknown number of their users.\n\nOn Wednesday, Google released patches for a handful of security bugs in its Chrome browser, noting that one of the bugs was being actively exploited by hackers before the company had time to patch it.\n\nUnusually for Google, the company provided no further details at the time.\n\nBut on Friday, Google updated the page to say that the bug was discovered by Apple’s security engineering team and Google’s Threat Analysis Group, whose security researchers primarily track government hackers and mercenary spyware makers, indicating that the hacking campaign may have been orchestrated by government-backed hackers.\n\nAt the same time, Apple released security updates for its flagship products, including iPhones, iPads, Macs, Vision Pro, Apple TV, Apple Watches, and its Safari browser.\n\nAccording to the security advisory for iPhones and iPads, Apple patched two bugs and the company said it was aware “that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals” running devices prior to iOS 26.\n\nThat language is Apple’s typical way of saying that it knows some of its customers and users were targeted by hackers exploiting zero-days, meaning flaws that at the time of exploitation are unknown to the software makers. Often, these are cases where government hackers used hacking tools and spyware made by companies such as NSO Group or Paragon Solutions to target journalists, dissidents, and human rights activists.\n\nApple and Google did not immediately respond to a request for comment.",
    "readingTime": 2,
    "keywords": [
      "hacking campaign",
      "apple and google",
      "security",
      "hackers",
      "released",
      "bugs",
      "software",
      "updates",
      "unknown",
      "users"
    ],
    "qualityScore": 0.75,
    "link": "https://techcrunch.com/2025/12/12/google-and-apple-roll-out-emergency-security-updates-after-zero-day-attacks/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2025/10/iphone-trenchant-l3harris-1059636822.jpg?resize=1200,808",
    "created_at": "2025-12-13T06:54:06.496Z",
    "topic": "tech"
  },
  {
    "slug": "adkrust-a-rust-implementation-of-google-agent-dev-kit",
    "title": "ADK-Rust: a Rust Implementation of Google Agent Dev Kit",
    "description": "The flexible, modular framework for production-ready AI agents. Model-agnostic. Type-safe. Blazingly fast.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://adk-rust.com",
    "thumbnail_url": "https://adk-rust.com/opengraph-image?84b856c5ef0b8bfe",
    "created_at": "2025-12-13T06:54:06.285Z",
    "topic": "tech"
  },
  {
    "slug": "ignoring-ai-bubble-fears-investors-bet-nvidia-and-google-will-fuel-taiwan-stocks-to-record",
    "title": "Ignoring AI bubble fears, investors bet Nvidia and Google will fuel Taiwan stocks to record",
    "description": "Taiwan's tech-heavy stocks show few signs of slowing a rally even as AI bubble worries cast a shadow over global markets, underscoring home-grown confidence in the structural advantages in AI that foreign investors may have overlooked.  Taiwan's benchmark index is poised to breach ​a record 30,000 points in 2026, investors say, extending a three-year surge that has seen the stock market nearly double as the island rides a wave of demand ‌for chips that power artificial intelligence.  While foreign money worries about stretched AI valuations, Taiwanese investors have enthusiastically ploughed into the market.",
    "fullText": "TAIPEI/SINGAPORE, Dec 12 (Reuters) - Taiwan's tech-heavy stocks show few signs of slowing a rally even as AI bubble worries cast a shadow over global markets, underscoring home-grown confidence in the structural advantages in AI that foreign investors may have overlooked.\n\nTaiwan's benchmark index is poised to breach ​a record 30,000 points in 2026, investors say, extending a three-year surge that has seen the stock market nearly double as the island rides a wave of demand ‌for chips that power artificial intelligence.\n\nWhile foreign money worries about stretched AI valuations, Taiwanese investors have enthusiastically ploughed into the market.\n\nAnalysts say domestic investors are betting on Taiwan's unique position as the lynchpin of an AI supply chain, where ‌even increasing competition in the sector would only benefit Taiwanese firms, including TSMC, the world's largest contract chipmaker.\n\nOne major focus of anxiety around AI comes from uncertainty about Nvidia's ability to sustain its market dominance, with Google's tensor processing units (TPUs) emerging as a potentially more cost-effective alternative to Nvidia's graphics processing units (GPUs).\n\nBut it's a win-win scenario for Taiwan because the island is essential to supply chains of both the GPU and TPU, the building blocks of AI computing power.\n\n\"Taiwan is a major beneficiary of the AI market,\" said Piter Yang, a fund manager of Fuh Hwa Securities Investment Trust Co, citing the ⁠advantage of Taiwan being the world's semiconductor hub.\n\nAnd for now, as ‌a promising future fuels optimism, Taiwan markets and local investors seem relatively unfazed by the AI bubble fears, just as they remain calm in the face of rising geopolitical tensions with Beijing that have often spooked foreign investors.\n\nGains in Taiwan's market have also been underpinned ‍by earnings growth, leaving it with a reasonably steady price-to-earnings ratio of 21, below that of the Nasdaq and the Nikkei, meaning the rally has not made stocks more expensive.\n\n\"We are not worried about an AI bubble,\" said Li Fang-kuo, chairman of the securities investment arm of food conglomerate Uni-President. \"We are comfortable with where the valuations stand.\"\n\nLi pointed out that several of the magnificent seven companies in the U.S. have gross ​margins of as much as 70% or higher. \"So it's not comparable to the dot-com bubble, when companies were not generating meaningful earnings.\"",
    "readingTime": 2,
    "keywords": [
      "processing units",
      "securities investment",
      "foreign investors",
      "market",
      "bubble",
      "stocks",
      "rally",
      "worries",
      "markets",
      "island"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/ignoring-ai-bubble-fears-investors-053150245.html",
    "thumbnail_url": "https://media.zenfs.com/en/reuters-finance.com/1303e6d6fc5d27d84e672269b5c5eb00",
    "created_at": "2025-12-13T06:54:00.470Z",
    "topic": "finance"
  },
  {
    "slug": "google-researchers-find-the-best-ai-model-is-69-right",
    "title": "Google researchers find the best AI model is 69% right",
    "description": "Here's what this means for law firms and other businesses that are betting on AI.",
    "fullText": "We just got a sobering picture of how often AI models get their facts straight. This week, Google DeepMind introduced the FACTS Benchmark Suite, which measures how reliably AI models produce factually accurate answers.\n\nIt tests models in four areas: answering factoid questions from internal knowledge, using web search effectively, grounding responses in long documents, and interpreting images. The best model, Google's Gemini 3 Pro, reached 69% accuracy, with other leading models falling well below that.\n\nFor context, if any of the reporters I manage filed stories that were 69% accurate, I would fire them.\n\nBeyond journalism, this number should matter to businesses betting on AI. While models excel at speed and fluency, their factual reliability still lags far behind human expectations, especially in tasks involving niche knowledge, complex reasoning, or precise grounding in source material.\n\nEven small factual errors can have outsized consequences in sectors such as finance, healthcare, and the law. This week, my talented colleague Melia Russell looked at how law firms are handling the rise of AI models as a source of legal truth. It's messy: She recounts how one firm fired an employee because they filed a document riddled with fake cases after using ChatGPT to draft it.\n\nThe FACTS benchmark is a warning but also a roadmap: by quantifying where and how models fail, Google hopes to accelerate progress. But for now, the takeaway is clear: AI is getting better, but it's still wrong about one-third of the time.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 2,
    "keywords": [
      "facts benchmark",
      "models",
      "accurate",
      "knowledge",
      "grounding",
      "filed",
      "factual",
      "it's",
      "google"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/google-researchers-find-best-ai-model-69-right-2025-12",
    "thumbnail_url": "https://i.insider.com/693c873c04eda4732f2d7991?width=1200&format=jpeg",
    "created_at": "2025-12-13T03:41:00.164Z",
    "topic": "science"
  },
  {
    "slug": "how-i-got-ai-to-help-me-sell-my-old-couch",
    "title": "How I got AI to help me sell my old couch",
    "description": "Google's Gemini AI chatbot quickly identified an old furniture model, making it easier to sell.",
    "fullText": "The Barr family purchased a new couch recently, and we've been trying to sell the old one. My wife posted in a local email group and a potential buyer got in touch with a question.\n\n\"Do you happen to know the model? I would love to look it up online rather than asking you lots of questions. :) Thanks!\" she wrote.\n\nThis is an old couch, so all product codes dissolved in the sands of time years ago. So, I took two photos of the furniture, uploaded them to Gemini, and asked for help. The Google chatbot sent back its best guess in seconds:\n\n\"Tillary Sectional: This is a very popular, modular West Elm sofa that was available around that time.\n\nAppearance Match: The low back and deep seat are very characteristic of the Tillary. The Tillary is known for having interchangeable backrests that can be moved or removed, and your sofa's armless chaise and backrests look like they might be loose cushions or pieces, which is a key Tillary feature.\n\nNote: Search results show the Tillary was being reviewed and discussed around 2014, making it a strong possibility for a 2013 purchase.\"\n\nGemini also shared other useful tips on how to identify the product, such as \"Some West Elm couches have a zippered compartment on the back or bottom where extra components (like legs or connectors) are stored, and a tag may be inside.\"\n\nThat was enough to find a product match and send all the details over to the buyer. We closed the deal, for $200, on Tuesday. That's enough for 10 months of a Gemini Pro subscription.\n\nThis holiday period, the Barr family plans to spend a lot of time slouching on our new, fancy couch. I call it the \"AI bubble couch.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 2,
    "keywords": [
      "barr family",
      "couch",
      "product",
      "back",
      "email",
      "buyer",
      "look",
      "backrests",
      "tillary",
      "gemini"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/google-gemini-ai-help-sell-couch-2025-12",
    "thumbnail_url": "https://i.insider.com/693c898e832e0ef1ead62c46?width=1200&format=jpeg",
    "created_at": "2025-12-13T03:41:00.012Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-use-circle-to-search-to-identify-scams-on-android",
    "title": "You Can Use 'Circle to Search' to Identify Scams on Android",
    "description": "Google's AI tools are getting scam detection capabilities.",
    "fullText": "Android users are getting more tools to combat the seemingly endless stream of scam texts from bad actors looking to steal your data and your money. Circle to Search and Google Lens can now assess messages for scam red flags, and if possible fraud is detected, you'll get recommendations for what to do (or not do) next. Even if you think you know the telltale signs of a scam—a sense of urgency, a demand for money or personal information, a link to log in or pay—using these tools can confirm your suspicions, especially when you feel pressured to act.\n\nTo activate Circle to Search, long press the home button or navigation bar on your device and circle the text you want to scan. Alternatively, you can take a screenshot, open Lens in the Google app (also available on iOS), and tap the screenshot. The feature works for text messages as well as communication on messaging apps and social media sites. Google says the capability is available \"when our systems have high confidence in the quality of the response.\"\n\nThis is just the latest in the Google's suite of security features meant to protect against fraud. Pixel users have real-time, AI-powered scam detection, which identifies and alerts you to suspicious conversational patterns in Google Messages and Phone by Google. In-call protections for Android prevent you from taking certain actions, such as sideloading new apps and changing accessibility permission, on your device while on the phone with anyone not saved in your contacts.\n\nEarlier this month, Google also expanded its in-call scam detection feature, meant to combat bank impersonation schemes, to U.S. users. If you are on a call with a number that's not in your contacts and try to open a participating financial app, you'll get a notification reminding you not to share information and a one-click option to stop screen-sharing and end the call.",
    "readingTime": 2,
    "keywords": [
      "scam detection",
      "users",
      "circle",
      "android",
      "tools",
      "combat",
      "money",
      "fraud",
      "you'll",
      "device"
    ],
    "qualityScore": 0.75,
    "link": "https://lifehacker.com/tech/circle-to-search-can-now-identify-scams-on-android?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC9SAFF9FGZWDBX4J2A5SMKB/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-13T03:40:59.006Z",
    "topic": "tech"
  },
  {
    "slug": "google-removes-scihub-domains-from-us-search-results-due-to-dated-court-order",
    "title": "Google Removes Sci-Hub Domains from U.S. Search Results Due to Dated Court Order",
    "description": "Google has removed dozens of Sci-Hub domain names from its search results in the U.S., marking the country's first pirate domain removals.",
    "fullText": "In 2017, American Chemical Society (ACS), a leading source of academic publications in the field of chemistry, won a lawsuit against Sci-Hub and its operator, Alexandra Elbakyan.\n\nThe ‘Pirate Bay of Science’ had failed to appear at a Virginia federal court, resulting in an easy win for the publisher and a $4.8 million default judgment award for damages.\n\nMore important, perhaps, was the broad permanent injunction that the Virginia federal court signed off on in 2017. This order effectively gave ACS free rein to take down existing and newly registered Sci-Hub domain names.\n\nThe injunction also required all parties “in active concert or participation” with Sci-Hub to “cease facilitating access” to these domain names, including search engines, hosting providers, ISPs, and domain name registrars, the order clarified.\n\nOn paper, this injunction enabled ACS to request American ISPs and search engines to ‘block’ existing and future Sci-Hub domains. However, there was no sign that the publisher was doing so. Aside from a few suspended domains, Sci-Hub remained widely accessible.\n\nWhether ACS did not feel the need to enforce the order against search engines and other intermediaries or if these companies actively objected to the requested actions was unknown. And as time passed, the injunction became a distant memory, at least for a few years.\n\nEarlier this week we spotted a unique request in the Lumen Database, where the 2018 injunction was cited. The notice in question asks Google to deindex 34 (sub)domains linked to Sci-Hub.\n\nNone of these domains were referenced in the 2018 injunction but are indeed linked to Sci-Hub. Many of the partially redacted domains appear to be domain variations of the scihubtw.tw mirror network, such as edu.scihubtw.tw and freeus.scihubtw.tw.\n\nIt’s surprising to see this type of enforcement seven years after the injunction was issued, but the request is legitimate. Google is certainly taking it seriously and has deindexed these domains from its search results in America. In other countries, the same domains remain accessible.\n\nThe December 2 notice was sent by UK law firm Wiggin LLP, which sent a similar request in September this year, targeting a few dozen other Sci-Hub domains. In total, we spotted seven notices, with the earliest dating back to 2022.\n\nThe results of these removals are also clearly visible in Google search. Those who search for Sci-Hub in the U.S. will see the following notice at the bottom of the results.\n\nIt’s not clear why it took five years before ACS urged Google to take action in response to the injunction. However, these removals are similar to Google’s removal of pirate site domains in other countries in response to ISP-blocking orders. Voluntary cooperation by Google was uncovered shortly before ACS first notified the search engine.\n\nGoogle’s voluntary cooperation with ISP blocking orders in Australia, the Netherlands, France, the UK, and elsewhere also brings up an important question. Is Google cooperating with the permanent injunction in the U.S. because it feels legally compelled to do so, or is that a voluntary gesture too?\n\nThe 2018 injunction requires all parties “in active concert or participation” with Sci-Hub to take action. While search engines are mentioned as an example, Google and other tech companies have previously argued that neutral third-party services are not necessarily “in active concert or participation”.\n\nIt is likely that Google maintains this stance, opting to voluntarily comply with orders targeting other third parties. That would mirror its response to site-blocking orders elsewhere.\n\nWe contacted Google hoping to hear answers to these questions, but the company did not respond to our request for comment.",
    "readingTime": 3,
    "keywords": [
      "virginia federal",
      "federal court",
      "active concert",
      "voluntary cooperation",
      "search engines",
      "permanent injunction",
      "sci-hub domains",
      "request",
      "parties",
      "participation"
    ],
    "qualityScore": 1,
    "link": "https://torrentfreak.com/google-removes-sci-hub-domains-from-u-s-search-results-due-to-dated-court-order/",
    "thumbnail_url": "https://torrentfreak.com/images/dmca-google-f.png",
    "created_at": "2025-12-13T03:40:58.805Z",
    "topic": "tech"
  },
  {
    "slug": "if-google-wins-ai-race-nvidia-is-in-trouble-says-author-of-jensen-huang-biography",
    "title": "If Google wins AI race, Nvidia is 'in trouble,' says author of Jensen Huang biography",
    "description": "Jensen Huang biography author Stephen Witt explains why Google's self-developed AI model, Gemini, could pose a threat to Nvidia's AI dominance",
    "fullText": "Nvidia's (NVDA) AI turf could take a blow if Google (GOOGL, GOOG) keeps firing on all cylinders.\n\n\"The biggest risk right now obviously is Google,\" said Stephen Witt, author of \"The Thinking Machine,\" a Jensen Huang biography.\n\nThat risk, he told Yahoo Finance's Opening Bid, is largely tied to Google's Gemini model. Witt described it as the \"best AI right now in the benchmarks outside the Nvidia stack.\"\n\nWitt explained that Gemini was trained entirely on its Tensor Processing Units (TPUs). If Google proves it can sustain world-leading AI development using only its homegrown chip stack, it sets a potent precedent for other tech giants to follow suit.\n\n\"That's a huge risk,\" Witt said. \"If Google ends up winning this AI race ... Nvidia will be in trouble.\"\n\nThis risk, coupled with competition from rivals like Broadcom (AVGO) and Advanced Micro Devices (AMD), is why \"it's very easy to imagine a world\" where Nvidia's high-flying stock declines. The AI chipmaker's shares are up over 1,270% in the past five years.\n\nTo mitigate the core risk of rivals like Google winning the chip war, Nvidia CEO Jensen Huang is already looking past generative AI. Witt said a significant amount of the CEO's personal effort is being poured into the next great computing wave: robotics.\n\nIf Huang can dominate the robotics wave, he said, \"that will mean several trillion dollars in market capitalization for this company.\"\n\nHowever, Nvidia has another issue: the lack of any clear succession plan.\n\n\"It's just Jensen at the top,\" Witt said. \"There's no second in command. There's no obvious successor.\" He noted that the board has been silent, and Huang has offered no advice on a succession strategy.\n\nThat suggests Nvidia's $4 trillion valuation — which accounts for over 8% of the entire S&P 500 (^GSPC) — is, in many ways, resting solely on Huang's vision.\n\nWitt described Huang as a \"world-class engineer\" who could \"design these microchips himself,\" a skill that whoever takes the helm must also possess. He noted that neither of Huang's two children, who work at the company, has a technical background, making them noncontenders for the top spot.\n\nWitt also provided a look behind the polished stage persona of Huang, known for his trademark leather jacket. Beneath the showmanship is a highly intense, \"almost totally neurotic\" leader who is driven not by optimism, but by fear.\n\n\"He's driven by negative emotions, things like fear of failure, guilt, even shame are what make Jensen get up in the morning and work so hard to make Nvidia succeed,\" Witt explained.",
    "readingTime": 3,
    "keywords": [
      "jensen huang",
      "risk",
      "witt",
      "stack",
      "chip",
      "rivals",
      "it's",
      "wave",
      "robotics",
      "succession"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/if-google-wins-ai-race-nvidia-is-in-trouble-says-author-of-jensen-huang-biography-184624884.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/pgRIv4CaXitTJsbNTIjhsA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2025-12/60ef8cc0-d6b2-11f0-9a17-1d997177f998",
    "created_at": "2025-12-12T18:55:59.740Z",
    "topic": "finance"
  },
  {
    "slug": "openai-aims-to-silence-concerns-it-is-falling-behind-in-the-ai-race-with-release-of-new-model-gpt52",
    "title": "OpenAI aims to silence concerns it is falling behind in the AI race with release of new model GPT-5.2",
    "description": "OpenAI said its new model outperforms those from rivals Google and Anthropic across a wide range of evaluations.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/openai-gpt-5-2-launch-aims-to-silence-concerns-it-is-falling-behind-google-anthropic-code-red/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2198334790-e1765478723707.jpg?resize=1200,600",
    "created_at": "2025-12-12T03:50:28.964Z",
    "topic": "business"
  },
  {
    "slug": "exclusive-youtube-launches-option-for-us-creators-to-receive-stablecoin-payouts-through-paypal",
    "title": "Exclusive: YouTube launches option for U.S. creators to receive stablecoin payouts through PayPal",
    "description": "Google, which owns YouTube, has already used PayPal’s stablecoin to receive payments from customers using its cloud computing service.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/youtube-paypal-google-stablecoin-payouts-pyusd/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2235768862-e1765505283880.jpg?resize=1200,600",
    "created_at": "2025-12-12T03:50:28.949Z",
    "topic": "business"
  },
  {
    "slug": "google-will-now-let-you-virtually-try-on-clothes-with-just-a-selfie",
    "title": "Google Will Now Let You Virtually Try on Clothes With Just a Selfie",
    "description": "Is this useful, or creepy?",
    "fullText": "The pace of AI technology is so rapid, it's tough to keep up with everything. At Google I/O back in May, Google rolled out an AI-powered shopping feature that let you virtually try on clothes you find online. All you needed to do was upload a full-length photo of yourself, and Google's AI would be to dress you up in whatever article of clothing you liked. I still can't decide whether the feature sounds useful, creepy, or a little bit of both.\n\nWhat I can say, however, is that the feature is getting a little creepier. On Thursday, Google announced an update to its virtual try on feature, that takes advantage of the company's new AI image model, Nano Banana. Now, you don't need a full-length photo of yourself: just a selfie. With solely a photo of your face, Nano Banana will generate a full-length avatar in your likeness, which you can use to virtually try on your clothes.\n\nI'm not exactly sure who this particular update is for: Maybe there are some of us out there who want to use this virtual try-on feature, but don't have a full-length photo of ourselves to share. Personally, I wouldn't really want to send Google my photo—selfie or otherwise—but I don't think I'd prefer to have Google infer what I look like from a photo of my face alone. I'd rather just send it the full photo, and give it something to actually work off of.\n\nHere's the other issue: While Google asks you to only upload images of yourself, it doesn't stop you if you upload an image of someone else. Talk about creepy: You can upload someone else's selfie and see how they look in various clothes. There is a system in place to stop you from uploading certain selfies, like celebrities, children, or otherwise \"unsafe\" items, but if the system fails, this feature could be used maliciously. I feel like Google could get around this by verifying the selfie against your Google Account, since you need to be signed in to use the feature anyway.\n\nIf you are interested in trying the feature out—and, subsequently, trying on virtual clothes with your AI-generated avatar—you can head over to Google's try on feature, sign into your Google Account, and upload your selfie. When it processes, you choose one of four avatars, each dressed in a different fit, to proceed. Once through, you can virtually try on any clothes you see in the feed.\n\nAgain, I see the potential usefulness of a feature that lets you see what you might look like in a certain piece of clothing before buying it. But, at the same time, I think I'd rather just order the item and try it on in the comfort (and privacy) of my own home.",
    "readingTime": 3,
    "keywords": [
      "i'd rather",
      "google account",
      "feature",
      "clothes",
      "upload",
      "full-length",
      "selfie",
      "virtually",
      "virtual",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-virtually-try-on-clothes-with-selfie?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC7MXWH4N3KCDYTMB9DBREC0/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-12T03:50:27.017Z",
    "topic": "tech"
  },
  {
    "slug": "openai-and-disney-just-ended-the-war-between-ai-and-hollywood-with-their-1-billion-sora-dealand-openai-made-itself",
    "title": "OpenAI and Disney just ended the ‘war’ between AI and Hollywood with their $1 billion Sora deal—and OpenAI made itself ‘indispensable,’ expert says",
    "description": "“Google has YouTube. OpenAI now has the Magic Kingdom,” copyright expert Matthew Sag said.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/openai-disney-sora-deal-hollywood-war-ended-matthew-sag/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-1534551119-e1765478095544.jpg?resize=1200,600",
    "created_at": "2025-12-11T18:58:23.551Z",
    "topic": "business"
  },
  {
    "slug": "google-stitch",
    "title": "Google Stitch",
    "description": "Stitch generates UIs for mobile and web applications, making design ideation fast and easy.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://stitch.withgoogle.com/",
    "thumbnail_url": "https://app-companion-430619.appspot.com/static/og.png",
    "created_at": "2025-12-11T13:53:40.718Z",
    "topic": "tech"
  },
  {
    "slug": "included-health-is-launching-an-ai-personal-health-assistant-thatll-face-off-with-big-tech-from-verily-to-openai",
    "title": "Included Health is launching an AI personal health assistant that'll face off with Big Tech from Verily to OpenAI",
    "description": "The startup's new tech could compete with Big Tech's health AI projects from heavyweights like Alphabet and OpenAI.",
    "fullText": "Included Health is rolling out a new AI tool that could pit it against Big Tech's latest health bets.\n\nThe healthcare startup has launched an AI-powered personal health assistant, Business Insider has learned exclusively. The tech draws on patients' medical claims, benefits information, and other data to offer on-demand answers to health-related questions.\n\nIncluded Health is tapping into a hot area in healthcare AI, where it's competing against other health startups as well as tech heavyweights. Alphabet's Verily released its own AI-powered app in October that allows patients to connect their medical records and ask a chatbot their health-related questions. OpenAI wants to win in consumer health tech, too, and is considering building tools such as its own personal health assistant, Business Insider reported in November.\n\nIncluded Health has been scaling on the premise of personalizing how patients interact with their healthcare for over a decade. The company, which sells tech to about 300 employers and health plans to help patients better navigate their health benefits, tested its AI assistant for about 18 months to ensure its accuracy in smaller pilots before making it available to its entire employer base, CEO Owen Tripp said.\n\n\"This can't be ChatGPT level of probability. It has to be precise,\" he said.\n\nTripp is optimistic about patients receiving general health guidance from LLMs like OpenAI's ChatGPT or Anthropic's Claude. Those AI tools can help patients learn more about their conditions and prepare for doctor's visits, he said. But he emphasized that Included's tech takes that guidance a step further.\n\n\"When it gets down to the business of actually taking care of oneself or taking care of somebody else, you're going to need a lot of very secure, specific data and a whole context to go solve problems, including the exact medical history of that patient,\" Tripp said.\n\nPatient-facing healthcare AI sometimes walks a regulatory tightrope, especially if the tech provides personalized advice that effectively replaces the work clinicians are licensed to do. Tripp said he doubts that most large tech companies attempting to delve into medical records aggregation will want to grapple with that complexity.\n\n\"I predict, like many before them, they will pull back. It's just hard, and the juice is often not worth the squeeze for these high-profile companies,\" he said.\n\nIncluded Health's personal health assistant, called Dot, has become its members' front door and the foundation for Included's new products, said COO Nupur Srivastava.\n\nIncluded recently put Dot in front of members during open enrollment to help answer their benefits questions, Srivastava said. The AI agent can also help patients prepare for doctor's visits and send the clinician a summary of patients' past visits ahead of time.\n\nIncluded Health still employs plenty of its own clinicians and care advocates that members can talk to if they prefer. Srivastava also noted that if a patient mentions the term 'suicide' in a conversation with Dot, \"within a minute, someone will call you.\"\n\nWhen asked about Big Tech and AI startups' ambitions to build personalized health AI, Tripp said that Included Health is in talks with multiple potential partners to help them achieve those goals. He didn't specify which companies it's talking to, but he suggested some AI companies are focused on acquiring personalized health data that they can anonymize and use to train models.\n\n\"But when it comes to actually delivering patient care, we're pretty confident that companies that are going to succeed will be the ones that have well-trained physicians licensed in all 50 states, delivering on a real-time platform, across mind, body, and wallet,\" he said.\n\nIncluded Health was supposed to go public in 2022. The startup had hired banks for an IPO push, but pulled out of its planned investor meetings when the market started to tank, Tripp told Business Insider in January.\n\nTripp declined to share specifics about Included's exit strategy as of November. But Included is profitable, so the company doesn't need to raise money through a public listing, he said. Included hasn't publicly fundraised since it was formed from the 2021 merger of Grand Rounds and Doctor on Demand, and the company hasn't shared its valuation.\n\nThe public markets haven't been forgiving to healthcare startups. Only two digital health companies went public this year, Hinge Health and Omada Health. And while Hinge and Omada have fared far better than most companies that listed during digital health's 2021 IPO wave, healthcare IPO hopefuls still face high standards to going public and significant volatility risks once they begin trading.\n\n\"The last few years in our space haven't been a great commercial for being a public company,\" Tripp said.\n\nWith so many developments in healthcare AI, however, Tripp does recognize that an IPO could create opportunities for Included Health to acquire other companies.\n\n\"I do think this is a time where there are going to be some interesting capabilities and technologies available in the market that allow us to provide even more service to our members,\" he said. \"I do have my eyes very open to how I would use capital to execute on some of those M&A events. That part is more important to me.\"",
    "readingTime": 5,
    "keywords": [
      "assistant business",
      "included health",
      "doctor's visits",
      "medical records",
      "personal health",
      "personalized health",
      "business insider",
      "patients",
      "healthcare",
      "care"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/included-health-launches-own-ai-personal-health-assistant-2025-12",
    "thumbnail_url": "https://i.insider.com/691290c746c4547ecb058733?width=1200&format=jpeg",
    "created_at": "2025-12-11T13:53:35.019Z",
    "topic": "finance"
  },
  {
    "slug": "google-deepmind-agrees-to-sweeping-partnership-with-uk-government-focused-on-science-and-clean-energy",
    "title": "Google DeepMind agrees to sweeping partnership with U.K. government focused on science and clean energy",
    "description": "The collaboration will see the AI company collaborating with the British government on a robotic lab for new materials, fusion energy, and new research into AI safety and the societal impacts of AI",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/10/google-deepmind-uk-government-partnership-science-clean-energy/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2217672931-e1765404847213.jpg?resize=1200,600",
    "created_at": "2025-12-11T03:50:14.668Z",
    "topic": "politic"
  },
  {
    "slug": "google-stands-to-make-111-billion-if-spacex-goes-public-at-a-15-trillion-valuation",
    "title": "Google stands to make $111 billion if SpaceX goes public at a $1.5 trillion valuation",
    "description": "A SpaceX IPO could hand Google billions. Its early bet on the rocket company may turn out to be one of the most lucrative startup investments ever.",
    "fullText": "Talk about the rich getting richer.\n\nAlphabet, parent company of Google, has been one of the best-performing stocks of the year, up nearly 70%, and now has a market capitalization of $3.8 trillion.\n\nThe company also happened to make what could turn out to be one of the most lucrative startup investments of all time, which could finally bear fruit next year.\n\nIn 2015, Google invested around $900 million in SpaceX for a stake of around 7% in Elon Musk's space company, which was then valued at $12 billion.\n\nNow SpaceX is reportedly planning to go public next year at a valuation of $1.5 trillion, which would make Google's stake worth around $111 billion.\n\nEven for a company as big as Google, SpaceX's success has already had a material impact on earnings.\n\nEarlier this year, Google reported an $8 billion gain from \"non-marketable equity securities,\" which Bloomberg identified as SpaceX. That gain represented 25% of Google's net income for the first quarter of 2025.\n\nGoogle is one of the largest outside investors in SpaceX, along with VC firm Founders Fund and Fidelity.\n\nGoogle and SpaceX did not respond to requests for comment.\n\nGoogle's 2015 investment, which was focused on Starlink, now looks certain to be a towering success, but at that time, it was met with considerable skepticism.\n\n\"One big technical and financial challenge facing the proposed venture is the cost installing ground-based antennas and computer terminals to receive the satellite signals,\" The Wall Street Journal wrote about Google's investment at the time. \"Another unanswered question is how SpaceX plans to transmit Internet signals to Earth. The company isn't believed to control rights to radio spectrum.\"\n\nMost of those questions have been answered with Starlink, now used by everyone from the Ukrainian army to United Airlines.\n\nAside from just the paper gains, Google's investment has also been a strategic advantage, as SpaceX has used Google Cloud to power Starlink.",
    "readingTime": 2,
    "keywords": [
      "google's investment",
      "starlink",
      "google",
      "spacex",
      "stake",
      "success",
      "gain",
      "signals"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/spacex-ipo-would-earn-google-111-billion-as-an-early-investor-2025-12",
    "thumbnail_url": "https://i.insider.com/6939dbbd71107c9f3457b911?width=1200&format=jpeg",
    "created_at": "2025-12-11T03:50:13.399Z",
    "topic": "finance"
  },
  {
    "slug": "google-maps-will-now-automatically-save-your-parking-spot-on-iphone",
    "title": "Google Maps Will Now Automatically Save Your Parking Spot on iPhone",
    "description": "Wasn't this a thing already?",
    "fullText": "Forgetting where you parked your car isn't just a sitcom trope—though it does make for classic TV. Even in the age of the smartphone, it's all too easy to walk away from your car and retain zero memory of where you left it. As it happens, Google Maps has a solution—provided you have an iPhone.\n\nSo long as you're running the app on an Apple device, Google Maps can automatically remember where you parked and display that information on the map. There's no need to mark the location yourself, tell the app to save your spot, or take a picture of the cross streets in case you forget: You can simply glance at the screen to find a \"You parked here\" label.\n\nThe thing is, while some are reporting this feature is relatively new to them, others suggest it's been out for a while. What gives?\n\nThis feature works if you connect your iPhone to your car, whether via Bluetooth, CarPlay, or a USB cable. When you disconnect your iPhone from your car, Google Maps will assume you've parked, and will automatically mark that location on the map. The feature will also work if you give Google Maps permission to access your Motion & Fitness data, which allows the app to tell when you've started and stopped driving. (You can check on this setting from Google Maps' in-app settings: Head to Navigation, then, under \"Automatically save parking,\" tap Let maps use your motion to save your parking.)\n\nYou can also choose to set Google Maps' location permissions to \"Always,\" which gives the app persistent access to your location data, and also lets it figure out when you've stopped your car. You can change this option on your iPhone by navigating t0 Settings > Privacy & Security > Location Services > Google Maps.\n\nThis feature is genuinely great, and I'm happy Google Maps supports it—even if Android users are inexplicably being excluded for now. But you might also be wondering to yourself, wasn't this already a feature? It doesn't sound all that new. Yet you might have even seen a flurry articles like this one today, all reporting on this \"new\" Google Maps parking feature.\n\nAs it turns out, the answer is a bit complicated. The coverage today all references this announcement on LinkedIn from Google Maps senior product manager Rio Akasaka—only that post is, according to LinkedIn, a month old. Some of the comments are more recent, but others appear to be from the original posting date.\n\nPotentially adding to the confusion, other navigation apps, like Apple Maps, already support automatically saving your parking spot, and while Google Maps has also had an option to save your parking spot for some time, it previously wasn't automatic. If you wanted the app to remember where you stopped, you needed to tap your blue dot on the map and choose \"Save your parking.\" That's how it still is on Android, since the automatic feature only works on iOS.\n\nLooking through the comment sections on articles from outlets like The Verge or MacRumors, some users insist they've had this feature for some time—well before that month-old LinkedIn announcement. This commenter says they've had the feature for years, while this one claims they've had it for \"ages.\" So what gives? Have at least some iPhone users have had this feature for years? It is possible Google has been testing out the feature with smaller subsets of users and only recently rolled it out en masse, but whether or not that actually happened is unclear at this time.\n\nI've reached out to Google for clarification about the timeline of this feature's implementation, and I will update this article if I get an answer. Until then, I can only hope Google is actively working on rolling out the feature to Android, as that really would be something new.",
    "readingTime": 4,
    "keywords": [
      "google maps",
      "parking spot",
      "feature",
      "iphone",
      "location",
      "save",
      "parked",
      "automatically",
      "users",
      "you've"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-maps-can-automatically-save-your-parking-spot-on-iphone?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC4PQH94245NM1ES0TAZG1CE/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-11T03:50:11.778Z",
    "topic": "tech"
  },
  {
    "slug": "the-startup-taking-direct-aim-at-nvidias-ai-iron-grip",
    "title": "The startup taking direct aim at Nvidia's AI iron grip",
    "description": "Chris Lattner helped build the software behind Google TPUs. Now he's coming after Nvidia.",
    "fullText": "In Silicon Valley, where bold technical bets abound, few bets look bolder than trying to break the grip of Nvidia's CUDA, a software stack that's quietly become the operating system of the AI boom.\n\nThat's what Modular, a startup founded by software gurus from Apple and Google, is trying to do.\n\nCofounders Chris Lattner and Tim Davis have spent decades building the software plumbing that sits beneath the modern tech industry. Lattner is famous for creating Apple's Swift programming language. He also built the software underpinning Google's TPU AI chips, with Modular cofounder Tim Davis.\n\nThey're now aiming that expertise at CUDA itself. The attempt borders on madness, but it's the kind of audacious project that could transform the AI industry.\n\n\"It's seen by a lot of people as somewhat crazy,\" said Kylan Gibbs, CEO of startup Inworld AI and a former product manager at Google DeepMind. \"That's where Chris has the advantage: He's smart enough to actually know how to do it, and somewhat crazy enough to set out to do it.\"\n\nCUDA began life almost 20 years ago as a way to make graphics chips programmable. Today, it has grown into a multilayered software ecosystem — language, libraries, compilers, inference systems — that most AI companies rely on.\n\nThat success comes at a cost: Most of the industry is now optimized around a single vendor's hardware. CUDA binds AI workloads to Nvidia GPUs. That is great for Nvidia, but deeply limiting for everyone else.\n\nOn the surface, there seems to be a ton of competition: AMD sells GPUs. Google has TPUs. Amazon created Trainium AI chips, and a host of startups are building similar hardware.\n\nThe problem is that each chip comes with its own software stack optimized just for that component. That means an endless reinvention of the wheel. Most of the time, it's simpler to just stick with CUDA — and Nvidia's GPUs.\n\nAnd yet, AI developers crave portability: Being able to use any combination of GPUs from multiple providers without juggling different software stacks.\n\n\"Nobody is building portable stuff, because why would anyone work on software for more than one chip when the chip projects themselves are doing the software?\" Lattner, Modular's CEO, told me in an interview.\n\nNvidia could extend CUDA to run well on rival AI chips. But doing so would undermine Nvidia's greatest moat: the closed-loop bond between its software and its chips. \"Obviously, they don't want portability,\" he said.\n\nFor Lattner, this paradox presents a big opportunity.\n\n\"We realized there's nobody in the industry that's actually incentivized to do this. It's very expensive, very hard,\" he said. \"And at the same time, everybody wants it.\"\n\nThat's what inspired Lattner and Davis to leave Google and start Modular in 2022, the year ChatGPT took the world by storm.\n\nSince then, Modular has raised $380 million from investors including Greylock, General Catalyst, and GV, Google's venture capital arm. The latest financing in September valued the startup at $1.6 billion. Modular isn't the only effort to break the CUDA lock-in. There has been ZLUDA, an open-source project that was funded by AMD, and more recently, the startup Spectral Compute, which has raised $6 million.\n\nLattner has used some of this money to hire talented programmers from Google, Apple, and other tech companies. They spent three years working in relative obscurity to create the building blocks of a new AI software stack.\n\nThe foundation starts with a brand-new programming language, called Mojo, that offers deep controls for making AI chips run as efficiently as possible.\n\nModular designed this to work similarly to Python, a popular and easy-to-use programming language. But Mojo also has the speed and power of other, more complex languages, such as C++, that are essential for AI development. Mojo also works well with PyTorch, an open-source framework that is often used when building AI models and applications.\n\nI first heard about Modular earlier this year when interviewing Carles Gelada, a former OpenAI researcher. \"There are several interesting projects to create GPU-agnostic frameworks and platforms, and challenge CUDA,\" he said at the time. \"Mojo is the most interesting one.\"\n\nMAX is the next major layer of Modular's new software stack. It powers AI inference, which is how models are run. This part of the system works with Nvidia GPUs, AMD GPUs, and similar chips from Apple. Modular hopes to add more AI chips in the future.\n\nOn top of that is another layer called Mammoth, which helps AI developers manage GPU clusters.\n\nIn late September, Modular announced that it got top performance out of Nvidia's new Blackwell B200 GPUs and AMD's latest MI355X GPUs — crucially on the same software platform.\n\nLattner said Modular got these AMD GPUs to perform roughly 50% better than when these chips run on AMD's own software.\n\nMore importantly, the ability to run different GPUs on the same software stack now supports the tantalizing opportunity to compare Nvidia's offerings with rival AI chips on a more level playing field.\n\n\"The obvious question is: can MI355X compete with Blackwell?\" Modular wrote in a blog announcing the results. \"Early signs point to yes.\"\n\nGibbs, the CEO of Inworld AI, has been putting Modular's software through its paces in the real world.\n\nInworld builds high-speed, real-time conversational AI technology that supports offerings from big companies, including Disney, NBCUniversal, and Niantic Labs.\n\nEarlier this year, when the startup designed a new text-to-speech AI model and got early access to Nvidia B200 GPUs, they issued Modular a challenge: Cut our costs by 60% and reduce our latency by 40% and we'll work with you.\n\n\"Within about four weeks, we were able to get this incredible performance,\" said Gibbs, who signed a partnership deal with Modular soon after. \"I've bet with my wallet.\"\n\nWhile Inworld was mostly lured by Modular's performance gains on Nvidia's latest GPUs, Gibbs likes the flexibility of using different AI chips more easily in the future, if needed.\n\n\"The promise is that we'd be able to move to new hardware,\" he said. \"Let's say AMD takes off, let's say TPUs take off for Google, or there could be other new hardware that comes online. So it's nice to have that optionality.\"\n\nIn fact, Google's TPUs are suddenly having a moment. The internet giant released a new AI model called Gemini 3 to rave reviews recently. That was trained and run using TPUs, and some other AI companies have signed deals to use these chips instead of, or alongside of, Nvidia GPUs.\n\nThat's put Nvidia on the defensive. A project like Modular, with its promise of portability across different AI hardware, adds to this pressure.\n\n\"Nvidia could kill this in a day,\" said Gibbs of the Modular project. \"Nvidia could basically say, 'okay, we don't really care that you run just on Nvidia hardware. Here's a CUDA option that runs on AMD GPUs as well.' It'd be a bit crazy for them to do that, but it's something they could do and that would of course be somewhat bad.\"\n\nFor all Lattner's critique of the industry, he says Modular is not trying to kill Nvidia. In fact, he argues that Nvidia will continue to thrive, even if Modular succeeds spectacularly.\n\n\"We're trying to build something like Android, but for AI hardware,\" he told me, referring to Google's mobile operating system that powers most of the world's smartphones.\n\nDespite billions of people using Android devices, this success didn't kill iOS, Apple's mobile operating system. iPhones still rule in the US, for example.\n\nLattner thinks something similar will happen in AI as Modular's software makes other hardware more competitive, giving developers more freedom, and chipping away at the industry's single-vendor monoculture.\n\n\"So Nvidia doesn't have to die, but we do want more competition. We do want more innovation,\" he said. \"I think that's good for the world.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 7,
    "keywords": [
      "tim davis",
      "let's say",
      "mobile operating",
      "operating system",
      "programming language",
      "somewhat crazy",
      "modular's software",
      "software stack",
      "amd gpus",
      "chips"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/nvidia-ai-chip-gpu-grip-modular-chris-lattner-google-2025-12",
    "thumbnail_url": "https://i.insider.com/6938812f71107c9f3457a0d9?width=1200&format=jpeg",
    "created_at": "2025-12-10T13:50:15.387Z",
    "topic": "finance"
  },
  {
    "slug": "eu-welcomes-seamless-data-transfer-between-iphone-and-androi",
    "title": "EU welcomes seamless data transfer between iPhone and Android",
    "description": "The EU welcomes Apple and Google's plans to enable a more seamless data transfer between iPhone and Android devices.",
    "fullText": "Apple and Google want to simplify data transfer between their iOS and Android platforms, support more data formats, and enable wireless transfer. The EU welcomes this step, stating: The Digital Markets Act is showing its effect.\n\nThe EU Commission has sent a statement to 9to5Mac. In it, a Commission spokesperson says about Apple and Google's cooperation in this area that it is an \"example of how the Digital Markets Act (DMA) benefits users and developers\".\n\nThe EU further states that this solution allows users to easily transfer data from iPhone to Android and vice versa when setting up a new device. According to the report, the transfer is intended to support \"many types of data\" – including contacts, calendar events, messages, photos, documents, Wi-Fi networks, passwords, and also data from third-party apps. Unlike previous solutions, the optimized method will also be wireless.\n\nAccording to the EU, the more seamless cross-platform data transfer follows the introduction of eSIM transfer by Apple and Google last October. Currently, however, this solution is only supported by a few network operators, but also by some outside the EU.\n\nBoth solutions are the direct result of the DMA, which \"requires effective data portability\" from certain services – including iOS and Android, the EU statement continues. Apple has summarized its work on this project in its DMA compliance reports from March 2024 and March 2025. Furthermore, the solutions are possible through extensive technical work and collaboration between Apple and Google, as well as through intensive discussions with the Commission over the past two years, according to the EU spokesperson.\n\nUnlike numerous features that Apple offers exclusively to its customers in the EU, such as alternative marketplaces, the seamless data transfer between platforms will be provided worldwide. For Apple, the solution is likely to be a win to attract new customers from Google's Android. This could, of course, also happen the other way around, with Google poaching from Apple. For consumers, the new solution means no longer being necessarily trapped in one of the two platform worlds.\n\nIncidentally, Apple will have to open up its iOS and iPadOS even further in the future, so that headphones and smartwatches, for example, work more seamlessly with the operating systems. These requirements are not at all to the company's liking, and it wanted to prevent this. The iPhone maker and Google are also not big fans of the Digital Markets Act: Apple would prefer to abolish the law, Google wishes for a reset, as it causes significant collateral damage in its current form.\n\nDon't miss any news – Facebook,\n LinkedIn or\n Mastodon.\n\nThis article was originally published in\n\n German.\n\n It was translated with technical assistance and editorially reviewed before publication.",
    "readingTime": 3,
    "keywords": [
      "digital markets",
      "markets act",
      "apple and google",
      "the eu",
      "transfer",
      "solution",
      "solutions",
      "platforms",
      "wireless",
      "statement"
    ],
    "qualityScore": 1,
    "link": "https://www.heise.de/en/news/EU-welcomes-seamless-data-transfer-between-iPhone-and-Android-11110651.html",
    "thumbnail_url": "https://heise.cloudimg.io/bound/1200x1200/q85.png-lossy-85.webp-lossy-85.foil1/_www-heise-de_/imgs/18/4/9/9/4/1/1/5/shutterstock_2400360099-acf4465881188230.jpg",
    "created_at": "2025-12-10T13:50:10.143Z",
    "topic": "tech"
  },
  {
    "slug": "i-tried-the-new-android-xr-smart-glasses-from-google-and-xre",
    "title": "I Tried the New Android XR Smart Glasses from Google and XReal",
    "description": "Google's Android XR platform and XReal's Project Aura are bringing smart glasses closer to mainstream reality, offering developers powerful tools to create practical, everyday wearable experiences that go far beyond novelty gadgets.",
    "fullText": "Google's Android XR platform is still in its early development stages, but it's full of potential. Designed to bring consistency to mixed reality (XR) headsets and smart glasses—essentially an \"Android for XR\"—it aims to address the fragmentation that has long hindered the category's growth. Currently, only one Android XR device is available for purchase, Samsung's $1,799 Galaxy XR, and trying it out impressed me enough to nominate it and Android XR for a TechEx award.\n\nStill, the Galaxy XR, with its Apple Vision Pro–style design, doesn't show what Android XR can mean for smart glasses specifically. Now, Google has finally provided that clarity by unveiling two of its own smart-glasses development kits and pulling back the curtain on XReal’s Android XR–powered Project Aura. After trying all three, I’m more convinced than ever that Android XR could mark a major leap forward for smart glasses, maybe even more than for headsets.\n\nGoogle’s Android XR development kits are designed for developers and manufacturers seeking to create products and experiences that rival those of the Meta Ray-Ban Display and similar smart glasses. They use a color display that combines an embedded microprojector with a special pattern etched into the lens, known as a waveguide, that directs images into your eye. Waveguide displays are limited in terms of field of view and resolution, but they can be much smaller and lighter than other types of wearable displays, enabling smart glasses that aren’t much bulkier or heavier than ordinary specs.\n\nThe two Google development kits are completely wireless, running on their own batteries, and connect to an Android phone for all software processing. They’re nearly identical, with the only difference being that one has a monocular display that only shows a picture to the right eye, and the other has a binocular display that can show images to both eyes. The binocular version can produce stereoscopic 3D images that appear to have depth, but the monocular version is lighter, weighing just 1.73 ounces (49 grams). For what it’s worth, I didn’t feel much of a difference in weight between the two.\n\nI got to try several Android XR activities on the Google smart glasses, giving me a good sense of how this type of device will work in the real world. To start, I tried YouTube Music on the monocular pair, which played audio into my ears and displayed a widget on the screen with track information and playback status. Tapping and swiping a touch strip on the right temple of the glasses let me play, pause, and skip tracks easily. The widget was sharp and easy to read, which is obviously vital for any smart glasses with a display. The music sounded fine as well, but I was indoors in a relatively quiet room. Of course, as development kits, their hardware components will likely differ from those in retail-ready products, so don’t read too much into the audio or video quality. The demo focused on the experience and features, as well as how Android XR, as a platform, can work on smart glasses.\n\nThe next demo was an incoming Google Meet video call. A Google rep called the phone number connected to the glasses, and I answered by tapping the touch strip. Her face appeared on the glasses display, in color, and I could see her talking, just as if I were on a video call on my laptop. She couldn’t see me since there wasn’t a camera pointed at my face, but I could share my own view through the glasses’ cameras with a swipe. Again, the call looked and sounded good, and I didn’t experience any hiccups.\n\nNavigation through Google Maps is baked into Android XR, so I was shown how it can work on the glasses. I asked for a nearby store, and it provided me with a few options. After I chose one, I saw a directional arrow and distant measurements for turn-by-turn directions to that store. Looking downward, the arrow transformed into a full map of my surroundings. It seemed to work very well and tracked the direction I was facing, although I couldn’t exactly walk anywhere to see how accurately it followed my location. Having a video game-like minimap in reality has long been one of my dreams for smart glasses, and it appears we’re getting closer to that. I hope that the Android XR Google Maps app will allow me to configure when and how the map appears, so I don’t have to look at my feet for it, but I couldn’t confirm if those options will be available.\n\nGoogle is pushing Gemini hard, and it’s no surprise that AI plays a significant role in Android XR. On waveguide smart glasses running Android XR, Gemini is always available with a button press and a wake word, ready to perform simple tasks like playing music or making a call, or do more complex analysis like answering questions or identifying what you’re looking at.\n\nDuring the demo, I was invited to ask Gemini for recipe suggestions while viewing a pantry wall filled with ingredients. I stared at a few jars of pasta, and it provided instructions on making pasta salad. Gemini could see the foodstuffs I was looking at and not only successfully named the pastas but also correctly identified sweet potatoes, and even noted that they were likely American sweet potatoes rather than Japanese or Korean ones. That kind of machine vision processing is far more impressive to me than large language model (LLM) outputs of recipes.\n\nAll of the demos described above felt very familiar, because I had similar experiences with the Meta Ray-Ban Display when I tried it out. In fact, asking an AI to produce a recipe based on the ingredients you’re looking at is a page straight out of Meta’s playbook, though my Android XR demos never failed as the on-stage Meta Connect presentation did. The functions are indeed very similar, but the difference lies in how the underlying systems are intended to be used. Android XR is a broad platform for third-party developers and manufacturers, featuring core features and elements that can be used directly or built upon, depending on the final product. The Meta Ray-Ban Display’s operating system is the final interface for a specific product. It doesn’t even have a public-facing name like Android XR or Apple’s visionOS. Android XR is a first step for an entire ecosystem, and Meta Ray-Ban Display might receive upgrades and iterations down the line, but it isn’t going to drive a field of non-Meta glasses.\n\nAlso, the Meta Ray-Ban Display is monocular, which means Google’s binocular development kit could show me some new tricks with 3D. After putting on the binocular glasses, I was shown some 3D video on YouTube. It indeed looked nice and 3D, and was fairly watchable. The binocular glasses also provided a 3D view of the city when I brought up Google Maps. As I mentioned before, though, waveguide displays have a limited field of view, so even if the picture is fairly sharp and in full color, I’m not sure I’d rely on this type of glasses to watch a full show or movie. In fact, the reps noted that watching longer-form video content here wasn’t an intended use case, though being able to play shorter clips is certainly handy.\n\nThe 3D features are nice but not vital to the experience. They can make using the glasses feel more immersive, but they aren’t why I favor binocular smart glasses. I find having a display in only one eye slightly disorienting compared with being able to see the same picture through both eyes. While I can quickly get used to it with the display in my dominant right eye, people with dominant left eyes could possibly find it more awkward to use, or at least a bit less pleasant.\n\nThese Android XR glasses feel like a significant step in making wireless, waveguide display smart glasses truly accessible to users. I’ve tested several, like the Rokid Glasses and the Even Realities G1 (and I’m currently testing its successor, the G2), and they’ve been very inconsistent and unpolished. Some, like the Rokid Glasses, have useful features and seem reliable enough if you get through the learning curve, but I haven’t been able to recommend any of them without major caveats. They also feature monochrome green displays rather than color, which has also been frustrating.\n\nI haven’t fully tested the Meta Ray-Ban Display outside of supervised demos yet, but while they seem like some of the most refined waveguide smart glasses so far, they’re also purely Meta products, using a closed system with limited room for outside development or growth. Google’s development kits demonstrate how Android XR can be implemented consistently by multiple manufacturers, serving as templates for developers to create their own apps. It’s what smart glasses like these have sorely been needing, and what could push this particular sub-category past its current status as shaky early-adopter hardware.\n\n Policy.\n\nYour subscription has been confirmed. Keep an eye on your inbox!\n\nThen there’s XReal’s Project Aura, which has turned out to be a completely different beast and much closer to the Samsung Galaxy XR than Google's waveguide development kits. XReal is another major manufacturer, besides Samsung, working with Google to introduce the first Android XR devices, and I got to try its Project Aura smart glasses along with the other devices.\n\nProject Aura is a pair of smart display glasses that use bulkier prisms instead of a waveguide system, but offer an incredibly wide 70-degree field of view. It doesn’t fill up your entire vision with a picture (even the Galaxy XR, with its 109-degree field of view, doesn’t do that), but it still looks like a huge theater screen in front of you. On the virtue of its display alone, it’s more impressive than other prism smart glasses, and with a much wider view than my current top pick in that category, the 57-degree XReal One Pro.\n\nThe display isn’t the most interesting thing about Project Aura. Its claim to fame is that it’s an Android XR pair of smart glasses, and unlike the Google waveguide glasses, it’s fully self-contained. It connects through a wire to a phone-sized control box that runs Android XR as its own operating system. Inside that box is a Snapdragon XR2+ Gen 2 processor, the same chip that drives the Galaxy XR. In other words, this pair of smart glasses has about the same processing power and capabilities as Samsung’s bigger, bulkier headset. Its picture isn’t as big or sharp, and it doesn’t have as many outward-facing cameras, but it has the same brain, the same interface, the same apps, and the same intuitive hand-tracking control.\n\nWhen I put on Project Aura, I was greeted with a tutorial I recognized from using the Galaxy XR. The glasses directed me to hold my hands in front of me and point at objects. I selected them by aiming with my finger and could click on them with pinching gestures or even grab and move them around. The same gestures applied to icons and app windows as well, allowing me to point and pinch to open new apps, resize them, and move them around. I could even bring up both the home screen and a quick menu by turning my palm to face me and making the pinch gesture. It felt exactly like the Galaxy XR, and I picked it up instantly.\n\nProject Aura appears to be capable of nearly everything the Galaxy XR can do. I was able to open multiple apps, arranging Chrome, Google Maps, and YouTube around my view. I could play a game called Demio that projected a 3D tabletop RPG view in front of me. I could also connect to a nearby computer directly and play games through it with minimal lag.\n\nI hailed the Galaxy XR’s control system as the best I’ve used in a headset since the Apple Vision Pro, which I still consider the most advanced and intuitive mixed reality device I’ve reviewed. It requires no controller or touchpad—you just point and pinch. This control system is incredibly rare on headsets, but seeing it in a much smaller and lighter pair of smart glasses? I wasn’t expecting that.\n\nSmart glasses, especially ones with a display, have struggled with controls. The waveguide models I’ve tested, as well as the Google development kits, all rely on voice controls or buttons and touch strips on the temples. The Meta Ray-Ban Display features the Neural Link Band, which tracks the micro-gestures of your hand, but I found it to be inconsistent when I tried it. Tethered prism display smart glasses rely entirely on the connected device for controls, with the exception of a basic button-based settings menu at best. Project Aura is a full, standalone Android XR device that supports hand-tracking and gestures, and it’s the most intuitive way to use smart glasses I’ve ever seen.\n\nSince it’s a pair of smart glasses and not a full headset with a strap, Project Aura is much easier to put on and take off than the Galaxy XR, and it feels much more comfortable to wear. You can also see through it when the power is turned off, since its projection system relies on clear prism lenses. You shouldn’t use Project Aura while walking around as you would with waveguide display smart glasses, though; the prism lenses might be transparent, but they’re still thick enough to warp your outside vision enough to be uncomfortable at best and dangerous at worst. Prism-equipped smart glasses like these are best used when stationary or in a controlled space, such as a larger mixed reality headset, rather than when crossing the street or riding the subway.\n\nProject Aura has a few disadvantages to go with its smaller, more convenient form factor. The display is the biggest part, since it’s noticeably narrower than the Galaxy XR’s. While the view is still huge for smart glasses, it isn’t as immersive as the headset’s, and its 1080p resolution isn’t nearly as sharp as the Galaxy XR’s 3,552 by 3,840 pixels per eye. Because it doesn’t have as many cameras, there’s a slightly smaller area where it can track your hands; I didn’t need to hold my hands directly in front of my face, but I couldn’t leave them in my lap like I can with the Galaxy XR or the Vision Pro. It also lacks internal cameras for eye tracking, so you must use hand movements instead of your gaze for control.\n\nEven with its limitations, I found Project Aura to be incredibly exciting. I’ve been using prism-based smart glasses, particularly the XReal One Pro, for working away from my desk and watching videos and playing games on my phone, and they have been very useful for that. Those use cases all require connecting the glasses to a separate device, such as a phone or laptop, and controlling everything through that device’s touch-screen, controller, or mouse and keyboard. Project Aura works entirely on its own (and as an Android-driven device with Bluetooth, it can also work wirelessly with controllers and keyboards, if you want). The control box is technically still a separate device that connects to the glasses with a cable, but like with the Galaxy XR and Vision Pro, it can be slipped into a pocket and treated like a battery. Combining the convenience of prism smart glasses with the capabilities of a high-end XR headset makes Project Aura feel like the next big leap in smart glasses as a whole.\n\nUnfortunately, you won’t be able to get Project Aura any time soon. My close look at the glasses came with an important detail: It’s a development kit. When Project Aura launches next year, it will be targeting developers and won’t be available for general purchase. So, like with the Google development kits, unless you’re planning to work on Android XR apps, you probably won’t be getting your hands on Project Aura.\n\nI didn’t hear anything about pricing, either, though considering it packs more impressive hardware than the Meta Ray-Ban Display, I wouldn’t be surprised if devs will be shelling out as much for it as they would for the $1,799 Galaxy XR. As for retail-ready Android XR smart glasses, don’t expect to see anything until late 2026 at the very earliest.\n\nI’m PCMag’s home theater and AR/VR expert, and your go-to source of information and recommendations for game consoles and accessories, smart displays, smart glasses, smart speakers, soundbars, TVs, and VR headsets. I’m an ISF-certified TV calibrator and THX-certified home theater technician, I've served as a CES Innovation Awards judge, and while Bandai hasn’t officially certified me, I’m also proficient at building Gundam plastic models up to MG-class. I also enjoy genre fiction writing, and my urban fantasy novel, Alex Norton, Paranormal Technical Support, is currently available on Amazon.",
    "readingTime": 15,
    "keywords": [
      "android xr",
      "project aura",
      "apple vision",
      "i’ve tested",
      "meta ray-ban",
      "vision pro",
      "sweet potatoes",
      "ray-ban display",
      "touch strip",
      "mixed reality"
    ],
    "qualityScore": 1,
    "link": "https://www.pcmag.com/news/i-tried-the-new-android-xr-smart-glasses-from-google-they-impressed-me",
    "thumbnail_url": "https://i.pcmag.com/imagery/articles/01LGQF7z3ySKcGqTgpg7ATP-1.fit_lim.size_1200x630.v1765302444.jpg",
    "created_at": "2025-12-10T13:50:08.807Z",
    "topic": "tech"
  },
  {
    "slug": "notebooklm-has-a-new-feature-for-visual-learners",
    "title": "NotebookLM Has a New Feature for Visual Learners",
    "description": "An infographics feature has arrived to Google's NotebookLM.",
    "fullText": "Another day, another update to Google's NotebookLM, the versatile AI tool that functions like a personal assistant focused only on you and your needs. The latest update is for visual learners: You can turn your source materials into helpful infographics that give you a clear picture—literally—of what the PDFs, websites, videos, or other materials you're studying or organizing are about.\n\nTo use the new feature, open any of your NotebookLM Notebooks (the name given to folders full of specific materials you've uploaded) and navigate to the panel on the right side. It's the same panel where you find the other offerings like the video creator and flashcard maker.\n\nAs with those tools, you just tap the associated button to generate the corresponding product, making sure the sources from your left panel that you want to include are all checked off. I tried it out this morning, first using the NotebookLM account associated with my personal Chrome profile and what I'm studying in my private life, then using the one I have set up for work, which has a tester notebook full of materials about how to study for the SAT. (I am strict about using different Chrome profiles for various parts of my life and am now up to seven.)\n\nIn my personal account, the button was labeled BETA—and it acted like it. After two failed attempts, NotebookLM could not produce an infographic based on my materials. In my work account, though, the beta label was missing and it performed the function just fine, spitting this out:\n\nThis is rolling out in full functionality to accounts at different times, obviously, but I was glad to see one of mine had easy access because I thought the infographic was solid.\n\nI don't consider myself a visual learner and primarily use NotebookLM to refine ideas or generate educational audio clips I can listen to while I clean the house, so I wasn't expecting to like this. I don't like the mind map creator within NotebookLM at all, for instance; flowcharts just aren't how I learn best, and that's fine.\n\nBut the infographic was concise, engaging, and just detailed enough to keep me interested and looking at it. It's unlikely I'll use this to study or refine my work often, but I can absolutely see how it would be useful to someone who learns more visually, especially if all the lines and boxes of a mind map can get too convoluted to be useful.",
    "readingTime": 3,
    "keywords": [
      "mind map",
      "materials",
      "personal",
      "panel",
      "account",
      "infographic",
      "another",
      "visual",
      "studying",
      "it's"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/how-to-use-notebooklms-new-infographics-generator-feature?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC2C7Q7BC1AJG0759KMFJ476/hero-image.fill.size_1200x675.png",
    "created_at": "2025-12-10T03:48:30.476Z",
    "topic": "tech"
  },
  {
    "slug": "google-will-fix-your-pixels-broken-display-for-free-if-it-qu",
    "title": "Google Will Fix Your Pixel’s Broken Display for Free (If It Qualifies)",
    "description": "Your Pixel 9 Pro's screen needs to be faulty in the \"right way.\"",
    "fullText": "Smartphone display issues are nothing new. Most of us have dropped our phones the wrong way one time or another, and had to deal with the pain (and cost) of getting them fixed. But when your smartphone's screen starts acting up for no particular reason, it's pretty frustrating—especially if the manufacturer still holds you accountable for the repair fees.\n\nIf that sounds like your experience with your Pixel 9 Pro, Pixel 9 Pro XL, or Pixel 9 Pro Fold, there's good news: Google is now launching an Extended Repair Program for the Pixel 9 Pro line. According to Google's announcement on Monday, the company has identified a \"limited number\" of Pixel 9 Pro and Pixel 9 Pro XL units that might exhibit display issues that impact the user's experience with the device. Should your Pixel 9 Pro's display show these symptoms, Google will fix the display at no cost to you.\n\nThat doesn't mean any and all display issues on your Pixel 9 Pro device qualify here. Google has identified two specific problems that this Extended Repair Program actually covers. The first is a vertical line present on the display. The line has to run from the bottom of the screen to the top, so partial lines won't quality. The second is display flicker. If you notice your Pixel 9 Pro's display quickly getting brighter and darker, as if someone was flicking a switch back and forth, you qualify for the repair program.\n\nThe Pixel 9 Fold is another story altogether. Like the 9 Pro and 9 Pro XL, Google is offering a free repair program for the 9 Pro Fold. However, unlike the other devices, there are no specific issues identified here. The problems may be display-related, but since the company won't specify, you could bring your 9 Pro Fold in for just about anything that's going wrong with it—as long as you didn't cause the issue yourself. In addition, Google won't actually fix your foldable, but will instead replace it entirely.\n\nThe company is also being strict regarding the quality of the display outside of these issues across all Pixel 9 Pro devices. If your Pixel's display or cover-glass is cracked, that may disqualify you from the free repair. If Google finds liquid damage in your device, same story. In any of these cases, the company will still fix the display issues mentioned above, but they might charge you for it.\n\nAffected Pixel 9 Pro, 9 Pro XL, and 9 Pro Fold units qualify for repair as of Dec. 8, and coverage will last for three years after the original purchase date of the device. You will need to have your device inspected at a Google walk-in center, Google-authorized center, or an online repair store before the company can confirm eligibility. You can get started on your claim from Google's official repair site.\n\nThis is good news for any Pixel 9 Pro users who have these specific issues—or any issues at all for Pixel 9 Pro Fold users. It joins a host of other Extended Repair Programs for Pixel devices, including the Pixel 4a battery program, the Pixel 6a battery program, the Pixel 7a repair program, and the Pixel 8 repair program,",
    "readingTime": 3,
    "keywords": [
      "pro's display",
      "extended repair",
      "pixel battery",
      "pixel pro",
      "battery program",
      "free repair",
      "pixel pro fold",
      "device",
      "identified",
      "qualify"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-will-fix-pixel-broken-display-for-free-if-it-qualifies?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC1X6QJ222BT0E57J35XZQXA/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-09T18:53:33.718Z",
    "topic": "tech"
  },
  {
    "slug": "the-birth-of-the-internet-troll-2014",
    "title": "The Birth of the Internet Troll (2014)",
    "description": "Trolls are shitting all over our internet. You can hardly search for something as innocuous as \"dog\" on Google without coming across inflammatory attacks",
    "fullText": "Trolls are shitting all over our internet. You can hardly search for something as innocuous as “dog” on Google without coming across inflammatory attacks on every possible dog-related opinion under the sun. All horrible things have to crawl before they can walk/crush spirits, though. Even trolls.\n\nAnd while the term “troll” has become wildly muddied, it did have to come from somewhere. We decided to try and find out just where that dark, acerbic origin story began.\n\nThere were bulletin board systems. And Usenet. And newsgroups. And people just starting to realize the massive potential trembling beneath their fingertips. Anything was possible! Which, as it turns out, is not always a good thing.\n\nIn the early 90s, trolls had yet to come into mainstream public consciousness, at least according to the 1993 Big Dummy’s Guide to the Internet. Flame wars, on the other hand, were already an online staple.\n\nWhether or not you’re familiar with the term, you do know what flaming is. You’ve seen it under horrible political opinions on Facebook. You’ve seen it choking your Twitter stream. And you’ve seen it in every other comment in the vast wasteland that is YouTube. Put simply, a flame is a vicious, personal attack on someone made simply because you disagree with them.\n\nOf course, humans have always had this potential for the irrationally vindictive, but the advent of the internet finally allowed it to thrive. Because as soon as you stuck someone behind a computer, a dangerously insular shield of anonymity came down and, for those inclined, happily took over. In discussing the sort of negotiation tactics that precede a flame war, Norman Johnson, an Associate Professor at Bauer College at the University of Houston explains:\n\nThe literature suggests that, compared to face-to-face, the increased incidence of flaming when using computer-mediated communication is due to reductions in the transfer of social cues, which decrease individuals’ concern for social evaluation and fear of social sanctions or reprisals. When social identity and ingroup status are salient, computer mediation can decrease flaming because individuals focus their attention on the social context (and associated norms) rather than themselves.\n\nThe introduction of anonymity not only made users feel free from the repercussions that might otherwise give them pause, but it also dehumanized potential targets. In other words, the internet gave all our worst impulses just what they needed to thrive.\n\nBecause if someone disagreed with you in the real, live social realm, you might feel frustrated, sure, but you’ll also see that person’s as another human with human emotions—not just a jumble of inflammatory words for you to destroy. You’ll take time to reflect, because you’ll realize there are consequences to your actions. Whereas on the internet, a clean slate is a mere username change away.\n\nSome of the earliest flame wars went down on Usenet, which unbeknownst to these earlier warriors, was building a model for all the trolls to eventually come in its wake. According to Gaffin:\n\nPeriodically, an exchange of flames erupts into a flame war that begin to take up all the space in a given newsgroup (and sometimes several; flamers like cross-posting to let the world know how they feel). These can go on for weeks (sometimes they go on for years, in which case they become “holy wars,” usually on such topics as the relative merits of Macintoshes and IBMs). Often, just when they’re dying down, somebody new to the flame war reads all the messages, gets upset and issues an urgent plea that the flame war be taken to e-mail so everybody else can get back to whatever the newsgroup’s business is.\n\nSo presumably, these troll/flame wars all started earnestly. But watching two groups of people attempt to lambast each other in increasingly epic proportions is—as we all know and hate to admit—wildly entertaining. And once the war of words would simmer down, it’s not at all surprising that someone might start (forcefully, sensationalistically) poking and prodding the more tender of egos. All in hopes of revisiting that awful sort of thrill that comes in watching another human push the very boundaries of sanity, by freaking the fuck out.\n\nThe earliest documented form of internet troll was something called a net.weenie, who did what s/he does ”\n just for the hell of it.” In early internet usenet forums, they were the people being assholes simply for the sheer joy of being an asshole. According to the Guide:\n\nThese are the kind of people who enjoy Insulting others, the kind of people who post nasty messages in a sewing newsgroup.\n\nEven the Electronic Frontier Foundation—formed in 1990—was aware of (and acknowledged) net.weenies prevalence among the more public internet groups. In the group’s early internet guide to mailing lists, one of the main benefits of such a system was that “a mailing list can offer a degree of freedom to speak one’s mind (or not worry about net.weenies) that is not necessarily possible on Usenet.” This was, of course, before the sorts of emails in which an undead child’s wrath and/or Nigerian prince’s livelihood rested on the click of our mouse.\n\nAnd net.weenies sound obnoxious, sure, but the term still didn’t carry the sort of malevolence we now associate with modern trolling. In fact, quite the contrary—some of their games were absolutely incredible.\n\nWarlording was a very specific, beautiful type of early trolling performed by these net.weenies, particularly in the alt.fan.warlord newsgroup in Usenet (a sort of subreddit of early internet days). Considering the limitations of early 90s bandwidth and forums’ general readability issues, Usenet etiquette—netiquette, if you will—asked users to keep their signatures under four lines. This was dubbed the McQuary limit and was not a hard and fast rule. At least in the way that there weren’t actually any real character limits.\n\nThis rule was partially necessary due to new users’ predilections for employing what was called BUAGs (Big Ugly ASCII Graphics) and BUAFs (Big Ugly ASCII Fonts). So to both mock this habit and be the biggest assholes they could be (always reach for the stars, kids), net.weenies\n tore this rule apart in a game called warlording.\n\nThe term came from the user\n Death Star, War Lord of the West, “who featured in his sig[nature] a particularly large and obnoxious ASCII graphic resembling the sword of Conan the Barbarian in the 1981 John Milius movie.” Which, presumably, looked something like this:\n\nThe newgroup\n alt.fan.warlord was created as a sort of sarcastic tribute to the offending sigs, and the jokes spiraled from there. One particularly notable case of warlording was that of\n James Parry‘s signature (better know by the username Kibo) below. Bear in mind, this is all one, single sig.\n\nAlthough every part of this signature is brilliant and deserving of our appreciation and awe, I do have a few favorite sections. Namely, this absurd and not at all remotely helpful Twin Peaks chart:\n\nBecause if anything has ever been worthy of being called art, it is the beautiful, intricate, wholly insincere mess.\n\nIn the late 80s and early 90s there certainly did exist this notion of an internet user who merely enjoyed stirring up trouble—but then that person has for as long as humans themselves have existed. As Whitney Phillips, a media studies scholar and communication lecturer at Humboldt State University (who has\n a book on trolls forthcoming with MIT press) explained to us over email:\n\n[Organized, willful trolling did exist before 4chan and Anonymous came around], though at the time it wouldn’t (necessarily) have been called that. This was a point of fascination to many of the trolls I interviewed; while they engaged in similar behaviors in the pre-4chan years, they didn’t refer to their behaviors as trolling and in fact couldn’t remember what they called it, if they called it anything. They’ve since some to use the term retroactively, but at the time the subcultural definition of the term hadn’t yet taken hold, and so they didn’t think of themselves as trolls.\n\nPurportedly, the actual use of the term “troll” dates back to the 80s, but\n according to the Oxford English Dictionary, the first instance of the term “troll” being used in an online capacity happened on December 14th, 1992 in the usenet group alt.folklore.urban, when someone wrote “Maybe after I post it, we could go trolling some more and see what happens.”\n\nInterestingly enough, it’s around the time that the actual term “trolling” started gaining steam in the mid 90s that the act itself began to evolve from causing annoyance as a result of your beliefs to simply believing in causing annoyance. And, of course, that’s just a single flavor of trolling—almost as soon as the term came into use, it started morphing into a blanket term of unwieldy proportions.\n\nFor instance, at least in retrospect, Brice Wellington was one of the more notorious troll incarnations. He spent much of his time “in alt.atheism, talk.origins, alt.christnet, and other newsgroups that he [would] troll and spam on a daily basis.” Now, whether his brand of trolling was sincere or satiric becomes a little more difficult to suss out. Usenet users at the time seemed certain that Brice was the “real deal,” so to speak, but in looking at some of his more insane rantings, it’s hard to see him as seeking anything more than what would soon be termed “the lulz.”\n\nHere we have Brice on the French:\n\nWhile Brice may have started blurring the line between being infuriating by nature and being infuriating by sheer force of well, alt.tasteless stepped firmly into the latter territory.\n\nIn a 1994 article with Wired, Usenet user Trashcan Man gave one of the first real insights into the prototypical troll mindset by describing alt.tasteless’ flamewar with the unsuspecting rec.pets.cats, a sort of haven for cat fanciers. In other words, prime bait.\n\nBecause for all intents and purposes, alt.tasteless was simply an early version of 4chan’s now-notorious /b/. As Wired explains:\n\nAlt.tasteless was created in the autumn of 1990 “as a place to keep the sick people away from rec.humor and other forums,” according to Steven Snedker, a Danish journalist for Denmark’s largest computer magazine. “Alt.tastelessers see this as an important turn in Usenet history, on a par with the creation of alt.sex. Both alt.tasteless and alt.sex are fine forums that serve their purpose to keep the other parts of Usenet clean, and to dig further into the stuff discussed.”\n\nWhich is all good and great, but being positively revolting certainly loses some of its appeal when you take away any potential foil. Which is why when someone suggested that alt.tasteless descend upon another Usenet group to incite chaos, the alt.tasteless users were delighted and ultimately decided on the cat newsgroup as a prime target. And alt.tasteless’ opening line was a doozy:\n\n… I’m not what you would call a real studly type guy (although I have a lot of women friends), so when I date it’s really important to me. Anyway, [my cat] Sooti goes into heat something fierce (sometimes it seems like it’s two weeks on, two weeks off). I had a date a while back, when she was really bad. Yowling and presenting all the time – not the most auspicious setting for a date. While dinner was cooking, I tried to stimulate her vagina with a Q-tip because I had heard that one can induce ovulation that way. My date came into the bathroom while I was doing this, and needless to say I don’t think she bought my explanation. The date was a very icy experience after that.\n\nWhat should I do. I love my cats, so I don’t want to get rid of them, but I can’t go on like this any more. It’s my love life, or them. Please help!!!\n\nThe earnest advice from rec.pets.cats was intermixed with decidedly more tasteless (naturally) advice from alt.tasteless including, but not limited to, providing “articles about topics such as vivisecting the cat and having sex with its innards.”\n\nWhich, of course, brings us to 4chan.\n\nFor better or worse, in 2003, 4chan entered the public consciousness and with it brought what Phillips refers to as “a very specific understanding of the term ‘troll,'” explaining in a Daily Dot article that “trolling was something that one actively chose to do. More importantly, a troll was something one chose to be.”\n\n4chan’s /b/ board in particular, being the spiritual successor to alt.tasteless, fostered this toxic mentality that if you don’t actually believe in the horrible things you’re saying that it magically becomes justified. As Phillips explained over email:\n\nGranted, the trolls might not really mean what they say. But who cares, they are not, and should not be regarded, as the ultimate arbiters of meaning. In other words, what these “trolls” think about what they do is irrelevant; even if they say they’re “just trolling,” their actions can have serious real-world consequences for the people they target.\n\nSo, say, when 4chan users found an 11-year-old girl’s address and phone number in 2010 and proceeded to call her home making death threats, it didn’t matter that they were “just doing it for the lulz.” Both that classic, deflective refrain and the term troll itself have succeeded in creating a potentially dangerous emotional distance from the actual consequences words can have—whether it’s trolls self-identifying as such or a media-assigned label. According to Phillips:\n\nI don’t accept the idea that assholes get to be assholes with impunity, as if we live exclusively in their world and there’s nothing we can do about it because “boys will be boys.”\n\nRather than defer blindly to the term “trolling,” I like to label behaviors based on what they do in the world. So, if someone is engaging in misogynist behavior, even if they believe they’re “just trolling” (whatever dude), that’s misogyny. And if that person doesn’t like the word misogynist, if that label makes them cry hot tears and feel bad about themselves, then how about not behaving like a misogynist.\n\nBecause even though the term may have gained notoriety on 4chan, the concept—however you may choose to define it—of “trolling” is more mainstream today than it has ever been.\n\nSearch “trolls” on Google and you’ll be hit with a deluge of articles defining the term in any number of ways. Whether it’s being defined as someone who believes what they’re saying in earnest, just wants to stir the pot, or is merely hopping on board a rage bandwagon—any rage bandwagon!—the only common thread is malicious intent. Which, according to Phillips, presents a major problem:\n\nCalling behaviors designed to threaten, intimidate, and silence “trolling” (so, lumping ALL aggressive online behavior under the same umbrella term) risks minimizing the emotional impact of the most extreme behaviors, particularly when those behaviors are piled on as viciously and relentlessly as they have been throughout Gamergate.\n\nClearly, for as long as the internet has been around, trolls have existed in some form—whether they were called that or not. There will always be agitators. There will always be people who want upset others. That’s not going to change.\n\nWhat we can change, though, is how we approach these situations in all their varied forms. Which, according to Phillips, “depends on whose voices platform administrators, advertisers, and other people on the business end choose to privilege—the targets of abusive, intimidating behaviors or those who are doing the intimidating.”\n\nIt’s not an issue of “feeding the trolls” (a problematic phrase in its own right), but rather whether or not we’re going to stop giving a platform to the trolls, the aggressors, and the antagonizers. Whether it be by not validating their behavior with concessions or dropping the catch-all term “troll” in favor of more accurate terminology—be it misogynist, sociopath, or straight-up dick.\n\nSo yes, assholes have and will always be around, as will their unfortunate victims. It’s just a matter of who we let hold the megaphone.",
    "readingTime": 14,
    "keywords": [
      "ugly ascii",
      "causing annoyance",
      "another human",
      "flame war",
      "flame wars",
      "trolls",
      "trolling",
      "troll",
      "it’s",
      "alt.tasteless"
    ],
    "qualityScore": 1,
    "link": "https://gizmodo.com/the-first-internet-troll-1652485292",
    "thumbnail_url": "https://gizmodo.com/app/uploads/2014/10/jebgmemwgiyzwrv1rep2.jpg",
    "created_at": "2025-12-09T18:53:30.863Z",
    "topic": "tech"
  },
  {
    "slug": "eu-opens-investigation-into-googles-use-of-online-content-fo",
    "title": "EU opens investigation into Google’s use of online content for AI models",
    "description": "European Commission to assess whether Gemini owner is putting rival companies at a...",
    "fullText": "European Commission to assess whether Gemini owner is putting rival companies at a disadvantage\n\nThe EU has opened an investigation to assess whether Google is breaching European competition rules in its use of online content from publishers and YouTube creators for artificial intelligence.\n\nThe European Commission said on Tuesday it will examine whether the US tech company, which runs the Gemini AI model and is owned by Alphabet, is putting rival AI owners at a “disadvantage”.\n\n“The investigation will notably examine whether Google is distorting competition by imposing unfair terms and conditions on publishers and content creators, or by granting itself privileged access to such content, thereby placing developers of rival AI models at a disadvantage,” the commission said.\n\nIt said it was concerned that Google may have used content from web publishers to generate AI-powered services on its search results pages without appropriate compensation to publishers and without offering them the possibility to refuse such use of their content.\n\nThe commission said it was also concerned as to whether Google has used content uploaded to YouTube to train its own generative AI models without offering creators compensation or the possibility to refuse.\n\n“Content creators uploading videos on YouTube have an obligation to grant Google permission to use their data for different purposes, including for training generative AI models,” the commission said.\n\nGoogle does not pay YouTube content creators for their content, nor does it allow them to upload their content on YouTube without allowing Google to use such data, it said. The commission noted that rival developers of AI models are barred by YouTube policies from using YouTube content to train their own AI models.\n\nGoogle-owned YouTube says its terms and conditions allow Google to use creators’ work for making AI models. In September, YouTube said: “We use content uploaded to YouTube to improve the product experience for creators and viewers across YouTube and Google, including through machine learning and AI applications.”\n\nThe EU’s competition chief, Teresa Ribera, said: “AI is bringing remarkable innovation and many benefits for people and businesses across Europe, but this progress cannot come at the expense of the principles at the heart of our societies.”\n\nA spokesperson for Google said: “This complaint risks stifling innovation in a market that is more competitive than ever.\n\n“Europeans deserve to benefit from the latest technologies and we will continue to work closely with the news and creative industries as they transition to the AI era.”\n\nThe EU’s investigation is the latest in a series of challenges to US big tech companies in recent years.\n\nIn September, EU regulators issued a fine of almost €3bn (£2.6bn) against Google, claiming that it favoured its own digital advertising services over rivals. Donald Trump said the fine was “discriminatory”.\n\nElon Musk’s social media company X, formerly known as Twitter, was fined €120m by EU tech regulators last week for breaching online content rules. The breaches included what the EU said was a “deceptive” blue tick verification badge given to users and the lack of transparency of the platform’s advertising.\n\nThe fine also attracted criticism from US officials, including the secretary of state Marco Rubio, who wrote on X that the fine was “an attack on all American tech platforms and the American people by foreign governments”.\n\nThe European Commission opened an investigation earlier this year into Meta over its rollout of AI features on WhatsApp, its messaging platform. Last year it fined Meta €798m for abusive practices benefiting Facebook Marketplace.\n\nIn 2024, Apple lost a fight against an order by EU competition regulators to pay €13bn in back taxes to Ireland.\n\nLast month, the head of Google’s parent company said people should not “blindly trust” everything AI tools tell them.\n\nSundar Pichai, the chief executive of Alphabet, said AI models were “prone to errors” and urged people to use them alongside other tools. He also warned that no company would be immune if the AI bubble burst.",
    "readingTime": 4,
    "keywords": [
      "online content",
      "content uploaded",
      "youtube content",
      "content creators",
      "the european commission",
      "the eu’s",
      "models",
      "rival",
      "investigation",
      "competition"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/09/eu-investigation-google-ai-models-gemini",
    "thumbnail_url": "https://i.guim.co.uk/img/media/0265df329ab15c539360f63131ed6ab480ca4201/1222_753_5697_4560/master/5697.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=31c9f30ca58e4748ef11858662efbaa9",
    "created_at": "2025-12-09T13:48:22.960Z",
    "topic": "tech"
  },
  {
    "slug": "linkedin-cofounder-reid-hoffman-says-he-learned-a-lesson-fro",
    "title": "LinkedIn cofounder Reid Hoffman says he learned a lesson from a visit to Epstein's island: 'Note to self, Google before going'",
    "description": "Hoffman, the billionaire LinkedIn cofounder, said he visited Epstein's island as part of his work fundraising for MIT.",
    "fullText": "Reid Hoffman said he once spent a night on Epstein's island in connection with MIT fundraising efforts.\n\nThe LinkedIn cofounder told the Newcomer podcast that he had been told a visit would make Epstein more likely to donate.\n\nHoffman has previously apologized publicly for his interactions with Epstein.\n\nLinkedIn cofounder Reid Hoffman said he wishes he'd known a little more before agreeing to spend a night on Jeffrey Epstein's island.\n\nOn the Dec. 1 episode of Eric Newcomer's podcast, Hoffman said that he visited the island as part of his fundraising work for the MIT Foundation and was told the visit would make Epstein more likely to donate to MIT.\n\n\"Note to self: Google before going,\" Hoffman said on the podcast. He said he stayed on the island for one night, and that there was a pool, a \"bunch of guest rooms,\" and a courtyard.\n\nHoffman has maintained that he only interacted with Epstein, whose 2019 death while awaiting trial on sex-trafficking charges was ruled a suicide, through his work fundraising for the MIT Media Lab. On the \"Newcomer\" podcast, he called Epstein a \"masterful networker,\" and recalled a 2015 dinner he hosted for an MIT researcher in Palo Alto, California.\n\nHoffman said that Joi Ito, former director of the MIT Media Lab, asked him if Epstein could attend the dinner, which was also attended by Meta CEO Mark Zuckerberg and Tesla CEO Elon Musk. Similar to his visit to the island, Hoffman said he was later told that the financier had said he was more likely to donate if he attended the dinner.\n\n\"He's kind of going through the network, trying to meet people and so forth,\" Hoffman said on the podcast. Hoffman also reiterated previous apologies for his involvement with Epstein.\n\nIn 2019, a spokesperson for Zuckerberg confirmed the dinner to Business Insider and said it was the only time the Facebook cofounder met Epstein. A spokesperson for Musk also confirmed the Tesla CEO's attendance.\n\nIn a 2019 email to Axios, Hoffman acknowledged multiple interactions with Epstein, which he said were strictly for fundraising purposes, and said he had been told MIT had vetted and approved the convicted sex offender's participation. He said in the email he was \"deeply regretful\" of the involvement.\n\n\"I went and kind of made a, you know, very public apology because it was like, okay I realized this and I'd already at that point had ramped down connection with him, right, to like no meetings and all the rest of the stuff, under any context,\" Hoffman said on the podcast. \"And I think he still would drop me an email every so often and say, 'Hey, can we get on the phone?' I say, 'Oh, maybe sometime,' which is, you know, code for never, right?\"\n\nHoffman said that justice for the late pedophile's victims is important, and urged the government to release, unredacted, \"every single piece of intel that they have about Epstein.\"\n\nIn November, President Donald Trump signed a bill that will release the Department of Justice's files on Epstein after months of pressure from Congress, including some fellow Republicans. The department has until Saturday, December 19, to comply with the order.\n\nTrump has also ordered the DOJ to investigate Hoffman, along with other individuals he views as political enemies, including former President Bill Clinton and former Treasury Secretary Larry Summers, over their ties to Epstein.\n\nHoffman, a billionaire and major Democratic donor, has previously said that he had to hire security after Musk fueled conspiracy theories about his relationship with Epstein.",
    "readingTime": 3,
    "keywords": [
      "hoffman",
      "epstein",
      "podcast",
      "island",
      "told",
      "fundraising",
      "dinner",
      "night",
      "cofounder",
      "newcomer"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/linkedin-cofounder-reid-hoffman-says-195616512.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/9upGoiVLbTnMB1soFYZ6Sw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/business_insider_articles_888/921ff7fb3570f0287b71217162b42eb0",
    "created_at": "2025-12-09T08:42:50.010Z",
    "topic": "news"
  },
  {
    "slug": "google-cloud-ceo-lays-out-3part-strategy-to-meet-ais-energy",
    "title": "Google Cloud CEO lays out 3-part strategy to meet AI’s energy demands after identifying it as the ‘most problematic thing’",
    "description": "Speaking at the Fortune Brainstorm AI conference, Google Cloud boss Thomas Kurian discussed how the company thinks about energy and data centers.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/08/google-cloud-ai-energy-demands-strategy-data-center-electricity/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/54972707933_643da41260_o-e1765241589857.jpg?resize=1200,600",
    "created_at": "2025-12-09T08:42:48.792Z",
    "topic": "business"
  },
  {
    "slug": "linkedin-cofounder-reid-hoffman-says-he-learned-a-lesson-fro",
    "title": "LinkedIn cofounder Reid Hoffman says he learned a lesson from a visit to Epstein's island: 'Note to self, Google before going'",
    "description": "Hoffman, the billionaire LinkedIn cofounder, said he visited Epstein's island as part of his work fundraising for MIT.",
    "fullText": "LinkedIn cofounder Reid Hoffman said he wishes he'd known a little more before agreeing to spend a night on Jeffrey Epstein's island.\n\nOn the Dec. 1 episode of Eric Newcomer's podcast, Hoffman said that he visited the island as part of his fundraising work for the MIT Foundation and was told the visit would make Epstein more likely to donate to MIT.\n\n\"Note to self: Google before going,\" Hoffman said on the podcast. He said he stayed on the island for one night, and that there was a pool, a \"bunch of guest rooms,\" and a courtyard.\n\nHoffman has maintained that he only interacted with Epstein, whose 2019 death while awaiting trial on sex-trafficking charges was ruled a suicide, through his work fundraising for the MIT Media Lab. On the \"Newcomer\" podcast, he called Epstein a \"masterful networker,\" and recalled a 2015 dinner he hosted for an MIT researcher in Palo Alto, California.\n\nHoffman said that Joi Ito, former director of the MIT Media Lab, asked him if Epstein could attend the dinner, which was also attended by Meta CEO Mark Zuckerberg and Tesla CEO Elon Musk. Similar to his visit to the island, Hoffman said he was later told that the financier had said he was more likely to donate if he attended the dinner.\n\n\"He's kind of going through the network, trying to meet people and so forth,\" Hoffman said on the podcast. Hoffman also reiterated previous apologies for his involvement with Epstein.\n\nIn 2019, a spokesperson for Zuckerberg confirmed the dinner to Business Insider and said it was the only time the Facebook cofounder met Epstein. A spokesperson for Musk also confirmed the Tesla CEO's attendance.\n\nIn a 2019 email to Axios, Hoffman acknowledged multiple interactions with Epstein, which he said were strictly for fundraising purposes, and said he had been told MIT had vetted and approved the convicted sex offender's participation. He said in the email he was \"deeply regretful\" of the involvement.\n\n\"I went and kind of made a, you know, very public apology because it was like, okay I realized this and I'd already at that point had ramped down connection with him, right, to like no meetings and all the rest of the stuff, under any context,\" Hoffman said on the podcast. \"And I think he still would drop me an email every so often and say, 'Hey, can we get on the phone?' I say, 'Oh, maybe sometime,' which is, you know, code for never, right?\"\n\nHoffman said that justice for the late pedophile's victims is important, and urged the government to release, unredacted, \"every single piece of intel that they have about Epstein.\"\n\nIn November, President Donald Trump signed a bill that will release the Department of Justice's files on Epstein after months of pressure from Congress, including some fellow Republicans. The department has until Saturday, December 19, to comply with the order.\n\nTrump has also ordered the DOJ to investigate Hoffman, along with other individuals he views as political enemies, including former President Bill Clinton and former Treasury Secretary Larry Summers, over their ties to Epstein.\n\nHoffman, a billionaire and major Democratic donor, has previously said that he had to hire security after Musk fueled conspiracy theories about his relationship with Epstein.",
    "readingTime": 3,
    "keywords": [
      "hoffman",
      "epstein",
      "podcast",
      "island",
      "dinner",
      "fundraising",
      "told",
      "former",
      "musk",
      "email"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/linkedin-reid-hoffman-jeffrey-epstein-island-visit-2025-12",
    "thumbnail_url": "https://i.insider.com/6936f2e104d0f0a114f1955c?width=1200&format=jpeg",
    "created_at": "2025-12-09T08:42:47.839Z",
    "topic": "finance"
  },
  {
    "slug": "the-google-pixel-10-is-200-off-right-now",
    "title": "The Google Pixel 10 Is $200 Off Right Now",
    "description": "It's the latest Pixel series, now with Pixelsnap (Google's version of MagSafe), new AI features, and the new telephoto lens.",
    "fullText": "Google phones keep offering great value for the money, dropping in price very quickly after their release, including the latest Pixel. The Google Pixel 10, with the 128GB going for $599 (originally $799) and the 256GB for $699 (originally $899), are both at record low prices right now, according to price-tracking tools.\n\nThe Google Pixel 10 is the latest in the series to be released this year, back in September. It's the model under the Pixel 10 Pro, which is also at its lowest price right now and has a much faster chip. As Lifehacker's Associate Tech Editor Michelle Ehrhardt says in her review, the Pixel 10 features a telephoto lens, brings the Pixelsnap (Google's version of MagSafe), and has new AI features. However, the ultrawide lens gets weaker, and there are some problems with the chip for third-party apps (but it can be fixed).\n\nThis Pixel 10 has a lot of the same features you'll find in the Pixel Pro for $150 less, making it a great budget option for those who don't want or need all the fancy specs and features. It comes with a Google Tensor G5 chip, and the camera resolutions are 48MP, 13MP, and 10.8MP for the rear and 10.5MP for the front-facing one. You can expect about 24 hours of battery life, depending on your use.\n\nOne of my favorite things about Pixel phones is the ongoing support for many years. My Pixel 6A still gets all of the updates and tons of AI features that make the phone feel fresh many years later, with the latest ones dropping in September. With the Pixel 10, you'll be getting a quality phone with software updates for a while (as long as seven years).",
    "readingTime": 2,
    "keywords": [
      "pixel",
      "google",
      "features",
      "price",
      "latest",
      "chip",
      "phones",
      "great",
      "dropping",
      "originally"
    ],
    "qualityScore": 0.75,
    "link": "https://lifehacker.com/tech/google-pixel-10-deal?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01K981BZQ7WSYSCQ86F4K0T568/hero-image.fill.size_1200x675.png",
    "created_at": "2025-12-09T08:42:44.903Z",
    "topic": "tech"
  },
  {
    "slug": "apple-and-google-are-making-it-easier-to-switch-between-ipho",
    "title": "Apple and Google Are Making It Easier to Switch Between iPhone and Android",
    "description": "Don't get locked into an ecosystem.",
    "fullText": "When it's time to buy a new car, you don't necessarily need to stick with the one you had before. You don't lose your cloud-based photos by switching from Toyota to Subaru, nor will your friends yell at you for ruining the group chat by buying a Kia. That's not the case with smartphones: When you buy an iPhone, it's tough to switch away from it. The same goes for Android: While it's easy enough to switch within the Android ecosystem, such as between Pixel or Galaxy, moving from Android to iPhone can also be a pain. Tech companies tend to make it tempting to stick with their platform, and introduce friction when you try to leave.\n\nThat, of course, is entirely business-based. Apple hasn't traditionally made it easy to move to Android, because, well, you might actually do it. It doesn't have to be this way, either. There's nothing inherent to smartphones that should make it so challenging to break out of any particular ecosystem. All it takes is some intentional design: If smartphones were made to be traded, you could migrate from one to another, without worrying about losing pictures, messages, or any other important data or processes.\n\nAs it happens, that intentional design may be on the horizon. As reported by 9to5Google, Apple and Google are actually working together to make it easier to transfer data between iPhone and Androids, which would make switching between the two platforms more seamless. This isn't theoretical, either: Google has already released some of this progress as part of the latest Android Canary, the company's earliest pre-release software. All compatible Pixel devices can currently access this latest build, though it doesn't seem there are any user-facing features available to test. 9to5Google says that similar features will roll out to testers in a future iOS 26 beta. Perhaps at that time, Google will roll out its features to the Android beta as well, which has a much larger user base than Canary.\n\nWhile details are slim here, any cooperation between Apple and Google on this front is huge. Current migration tools do exist, but they can be problematic. By actually working together on a native transfer solution, it might actually be seamless to move between platforms. Apple and Google might not be motivated by charity, of course, as the EU has been cracking down on restrictive practices by tech companies in recent years. But while both companies may see this as a way to lose customers, it's also a way to gain them: Sure, some iPhone users may switch to Android if it's easier to do so, but some Android users may do the reverse for the same reasons.\n\nMore choice is good for everyone—even if it doesn't guarantee exponential growth to shareholders.",
    "readingTime": 3,
    "keywords": [
      "android",
      "google",
      "between",
      "iphone",
      "make",
      "apple",
      "actually",
      "smartphones",
      "switch",
      "while"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-and-apple-are-making-it-easier-to-switch-between-iphone-and-android?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KBZW51YD9RTK9TT6NWP3RZ28/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-09T08:42:44.864Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-a-bubble-thats-about-to-pop-podcast",
    "title": "Is AI a bubble that’s about to pop? – podcast",
    "description": "Should we be worried about the vast amounts of money pouring into AI? And what will happen if the bubble bursts? Blake Montgomery reports\nFor months there have been fears that artificial intelligence is a bubble and that it is about to burst.\nAs the Guardian US tech editor Blake Montgomery explains, the magnificent seven – Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia and Tesla – make up one-third of the value of the S&P 500, the index of the 500 biggest stocks in the US market. All are heavily invested in AI.",
    "fullText": "Should we be worried about the vast amounts of money pouring into AI? And what will happen if the bubble bursts? Blake Montgomery reports\n\nFor months there have been fears that artificial intelligence is a bubble and that it is about to burst.\n\nAs the Guardian US tech editor Blake Montgomery explains, the magnificent seven – Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia and Tesla – make up one-third of the value of the S&P 500, the index of the 500 biggest stocks in the US market. All are heavily invested in AI.\n\nNever before has so much of the economy been dependent on one technology. And despite the trillions of dollars invested, AI is yet to show a way it can sustainably turn over profits.\n\nSo what happens, asks Nosheen Iqbal, if one day the faith falters, the money stops coming in and the bottom falls out?",
    "readingTime": 1,
    "keywords": [
      "about",
      "money",
      "bubble",
      "blake",
      "montgomery",
      "invested"
    ],
    "qualityScore": 0.65,
    "link": "https://www.theguardian.com/news/audio/2025/dec/08/is-ai-a-bubble-thats-about-to-pop-podcast",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a592cb81eac50241f8dc955b0dd411e77f13449c/419_0_6024_4820/master/6024.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=36b946de97b28522c047dce82e755023",
    "created_at": "2025-12-09T08:42:39.215Z",
    "topic": "tech"
  }
]
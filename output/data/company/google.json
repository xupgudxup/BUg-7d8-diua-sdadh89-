[
  {
    "slug": "google-adds-native-hls-video-playback-to-chrome-desktop",
    "title": "Google adds native HLS video playback to Chrome desktop",
    "description": "After nearly four years of development, Google is finally rolling out native HTTP Live Streaming (HLS) playback to Chrome on desktop with version 142 and",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://tech-ish.com/2025/12/08/google-chrome-microsoft-edge-chromium-native-hls-playback-desktop/",
    "thumbnail_url": "https://tech-ish.com/wp-content/uploads/2025/06/Google-Chrome-logo.jpg",
    "created_at": "2025-12-14T13:19:49.290Z",
    "topic": "tech"
  },
  {
    "slug": "a-look-at-an-android-itw-dng-exploit",
    "title": "A look at an Android ITW DNG exploit",
    "description": "Posted by Benoît Sevens, Google Threat Intelligence Group Introduction Between July 2024 and February 2025, 6 suspicious image files were ...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://googleprojectzero.blogspot.com/2025/12/a-look-at-android-itw-dng-exploit.html",
    "thumbnail_url": "https://blogger.googleusercontent.com/img/a/AVvXsEiiwr_N_KPj_7IxkyMILQw6UiTrRKfIWj4y45LHcc44O79yYN-BMo-2Ex59570Ekwdwhsf5kjvrDmmWSOETFa_KLLGlSe6uL0zvgm-3_75rJssKQ-wsaGjwH7o0yHQcLvFOepn9tL7NGyOjVt1w0zK3wHT9O8yEIkD1CQkV2sPDuH69iy4laJsi_y19lnM=w1200-h630-p-k-no-nu",
    "created_at": "2025-12-14T03:53:12.729Z",
    "topic": "tech"
  },
  {
    "slug": "google-scholar-labs-an-ai-powered-scholar-search",
    "title": "(Google) Scholar Labs: An AI Powered Scholar Search",
    "description": "Research questions are often detailed. Answering them can require looking at a topic from multiple angles. Today, we are introducing Scholar...",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://scholar.googleblog.com/2025/11/scholar-labs-ai-powered-scholar-search.html",
    "thumbnail_url": "https://lh3.googleusercontent.com/blogger_img_proxy/AEn0k_uoL5kmMaHdVOCeXyiSu33BBR_EwE7IMv0oGoKpSVzkYvTaMqW-AN5qQd5IRSeiRaJ0v8GzUTRs8RS2s635lyNuhVMgiD1EmrX17KSDV7ZAB4B1bnjfrCuvZtyfsprsx0jKfRhN-eDcebPWcw=w1200-h630-p-k-no-nu",
    "created_at": "2025-12-13T18:48:05.743Z",
    "topic": "tech"
  },
  {
    "slug": "i-work-in-ai-security-at-google-and-there-are-some-things-i-would-never-tell-chatbots-i-follow-4-rules-to-use-ai-safely",
    "title": "I work in AI security at Google and there are some things I would never tell chatbots. I follow 4 rules to use AI safely.",
    "description": "Harsh Varshney, who works on Chrome AI security at Google, shares four tips for protecting your data and identity when you talk to AI chatbots.",
    "fullText": "This as-told-to essay is based on a conversation with 31-year-old Harsh Varshney, who works at Google and lives in New York. The following has been edited for length and clarity.\n\nAI has quickly become a silent partner in our daily lives, and I can't imagine life without AI tools.\n\nDay-to-day, they help me with deep research, note-taking, coding, and online searches.\n\nBut my job means I'm very aware of the privacy concerns associated with using AI. I've worked at Google since 2023 and spent two years as a software engineer on the privacy team, building infrastructure to protect user data. I then switched to the Chrome AI security team, where I help secure Google Chrome from malicious threats, like hackers and those who use AI agents to conduct phishing campaigns.\n\nAI models use data to generate helpful responses, and we users need to protect our private information so that harmful entities, like cybercriminals and data brokers, can't access it.\n\nHere are four habits I've made that I believe are essential for protecting my data while using AI.\n\nSometimes, a false sense of intimacy with AI can lead people to share information online that they never would otherwise. AI companies may have employees who work on improving the privacy aspects of their models, but it's not advisable to share credit card details, Social Security numbers, your home address, personal medical history, or other personally identifiable information with AI chatbots.\n\nDepending on the version being used, the information shared with public AI chatbots can be used to train future models and generate responses that are more relevant. This could result in \"training leakage,\" where the model memorizes personal information about one user and later regurgitates it in responses to another. Plus, there's the risk of data breaches, which would expose what you've shared with a chatbot.\n\nI treat AI chatbots like a public postcard. If I wouldn't write a piece of information on a postcard that could be seen by anyone, I wouldn't share it with a public AI tool. I'm not confident about how my data could be used for future training.\n\nIt's important to identify whether you're using a more public AI tool or an enterprise-grade one.\n\nWhile it's uncertain how conversations are used for training public AI models, companies can pay for \"enterprise\" models. Here, models aren't typically meant to train on user conversations, so it's safer for employees to talk about their work and company projects.\n\nThink of it like having a conversation in a crowded coffee shop where you could be overheard, versus a confidential meeting in your office that stays within the room.\n\nThere have reportedly been instances where employees have accidentally leaked company data to ChatGPT. If you work on unreleased company projects or are trying to get a patent, you probably don't want to discuss your plans with a non-enterprise-grade chatbot due to the risk of leakage.\n\nI don't discuss projects I'm working on at Google with public chatbots. Instead, I use an enterprise model, even for tasks as small as editing a work email. I'm much more comfortable sharing my information because my conversations aren't used for training, but I still minimize the personal information I share.\n\nAI chatbots usually keep a history of your conversations, but I recommend deleting it on both enterprise and public models regularly to protect your user privacy in the long term. Due to the risk of your account being compromised, it's a good precautionary habit to have, even if you're confident you aren't putting private data into the tools.\n\nOnce, I was surprised that an enterprise Gemini chatbot was able to tell me my exact address, even though I didn't remember sharing it. It turned out, I had previously asked it to help me refine an email, which included my address. Because the tool has long-term memory features, enabling it to remember information from previous conversations, it could identify what my address was and retain it.\n\nSometimes, if I'm searching for things I don't want the chatbot to remember, I'll use a special mode, a bit like incognito mode, where the bots don't store my history or use the information to train models. ChatGPT and Gemini call this the \"temporary chat\" feature.\n\nIt's better to use AI tools that are well-known and are more likely to have clear privacy frameworks and other guardrails in place.\n\nOther than Google's products, I like to use OpenAI's ChatGPT and Anthropic's Claude.\n\nIt's also helpful to review the privacy policies of any tools you use. Sometimes, they'll explain more about how your data is used to train the model. In the privacy settings, you can also look for a section with the option to \"improve the model for everyone.\" By making sure that setting is turned off, you're preventing your conversations from being used for training.\n\nAI technology is incredibly powerful, but we must be cautious to ensure our data and identities are safe when we use it.\n\nDo you have a story to share about using AI to help you at work? Contact this reporter at ccheong@businessinsider.com",
    "readingTime": 5,
    "keywords": [
      "models",
      "privacy",
      "it's",
      "conversations",
      "chatbots",
      "training",
      "tools",
      "user",
      "address",
      "train"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/google-ai-security-safe-habits-privacy-data-2025-12",
    "thumbnail_url": "https://i.insider.com/69395b7504d0f0a114f1bc26?width=1200&format=jpeg",
    "created_at": "2025-12-13T13:18:39.919Z",
    "topic": "finance"
  },
  {
    "slug": "google-and-apple-roll-out-emergency-security-updates-after-zeroday-attacks",
    "title": "Google and Apple roll out emergency security updates after zero-day attacks",
    "description": "Apple released patches for all of its flagship devices to fix security flaws under attack. Google also updated Chrome to remediate one vulnerability exploited in the attacks.",
    "fullText": "Apple and Google have released several software updates to protect against a hacking campaign targeting an unknown number of their users.\n\nOn Wednesday, Google released patches for a handful of security bugs in its Chrome browser, noting that one of the bugs was being actively exploited by hackers before the company had time to patch it.\n\nUnusually for Google, the company provided no further details at the time.\n\nBut on Friday, Google updated the page to say that the bug was discovered by Apple’s security engineering team and Google’s Threat Analysis Group, whose security researchers primarily track government hackers and mercenary spyware makers, indicating that the hacking campaign may have been orchestrated by government-backed hackers.\n\nAt the same time, Apple released security updates for its flagship products, including iPhones, iPads, Macs, Vision Pro, Apple TV, Apple Watches, and its Safari browser.\n\nAccording to the security advisory for iPhones and iPads, Apple patched two bugs and the company said it was aware “that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals” running devices prior to iOS 26.\n\nThat language is Apple’s typical way of saying that it knows some of its customers and users were targeted by hackers exploiting zero-days, meaning flaws that at the time of exploitation are unknown to the software makers. Often, these are cases where government hackers used hacking tools and spyware made by companies such as NSO Group or Paragon Solutions to target journalists, dissidents, and human rights activists.\n\nApple and Google did not immediately respond to a request for comment.",
    "readingTime": 2,
    "keywords": [
      "hacking campaign",
      "apple and google",
      "security",
      "hackers",
      "released",
      "bugs",
      "software",
      "updates",
      "unknown",
      "users"
    ],
    "qualityScore": 0.75,
    "link": "https://techcrunch.com/2025/12/12/google-and-apple-roll-out-emergency-security-updates-after-zero-day-attacks/",
    "thumbnail_url": "https://techcrunch.com/wp-content/uploads/2025/10/iphone-trenchant-l3harris-1059636822.jpg?resize=1200,808",
    "created_at": "2025-12-13T06:54:06.496Z",
    "topic": "tech"
  },
  {
    "slug": "adkrust-a-rust-implementation-of-google-agent-dev-kit",
    "title": "ADK-Rust: a Rust Implementation of Google Agent Dev Kit",
    "description": "The flexible, modular framework for production-ready AI agents. Model-agnostic. Type-safe. Blazingly fast.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://adk-rust.com",
    "thumbnail_url": "https://adk-rust.com/opengraph-image?84b856c5ef0b8bfe",
    "created_at": "2025-12-13T06:54:06.285Z",
    "topic": "tech"
  },
  {
    "slug": "ignoring-ai-bubble-fears-investors-bet-nvidia-and-google-will-fuel-taiwan-stocks-to-record",
    "title": "Ignoring AI bubble fears, investors bet Nvidia and Google will fuel Taiwan stocks to record",
    "description": "Taiwan's tech-heavy stocks show few signs of slowing a rally even as AI bubble worries cast a shadow over global markets, underscoring home-grown confidence in the structural advantages in AI that foreign investors may have overlooked.  Taiwan's benchmark index is poised to breach ​a record 30,000 points in 2026, investors say, extending a three-year surge that has seen the stock market nearly double as the island rides a wave of demand ‌for chips that power artificial intelligence.  While foreign money worries about stretched AI valuations, Taiwanese investors have enthusiastically ploughed into the market.",
    "fullText": "TAIPEI/SINGAPORE, Dec 12 (Reuters) - Taiwan's tech-heavy stocks show few signs of slowing a rally even as AI bubble worries cast a shadow over global markets, underscoring home-grown confidence in the structural advantages in AI that foreign investors may have overlooked.\n\nTaiwan's benchmark index is poised to breach ​a record 30,000 points in 2026, investors say, extending a three-year surge that has seen the stock market nearly double as the island rides a wave of demand ‌for chips that power artificial intelligence.\n\nWhile foreign money worries about stretched AI valuations, Taiwanese investors have enthusiastically ploughed into the market.\n\nAnalysts say domestic investors are betting on Taiwan's unique position as the lynchpin of an AI supply chain, where ‌even increasing competition in the sector would only benefit Taiwanese firms, including TSMC, the world's largest contract chipmaker.\n\nOne major focus of anxiety around AI comes from uncertainty about Nvidia's ability to sustain its market dominance, with Google's tensor processing units (TPUs) emerging as a potentially more cost-effective alternative to Nvidia's graphics processing units (GPUs).\n\nBut it's a win-win scenario for Taiwan because the island is essential to supply chains of both the GPU and TPU, the building blocks of AI computing power.\n\n\"Taiwan is a major beneficiary of the AI market,\" said Piter Yang, a fund manager of Fuh Hwa Securities Investment Trust Co, citing the ⁠advantage of Taiwan being the world's semiconductor hub.\n\nAnd for now, as ‌a promising future fuels optimism, Taiwan markets and local investors seem relatively unfazed by the AI bubble fears, just as they remain calm in the face of rising geopolitical tensions with Beijing that have often spooked foreign investors.\n\nGains in Taiwan's market have also been underpinned ‍by earnings growth, leaving it with a reasonably steady price-to-earnings ratio of 21, below that of the Nasdaq and the Nikkei, meaning the rally has not made stocks more expensive.\n\n\"We are not worried about an AI bubble,\" said Li Fang-kuo, chairman of the securities investment arm of food conglomerate Uni-President. \"We are comfortable with where the valuations stand.\"\n\nLi pointed out that several of the magnificent seven companies in the U.S. have gross ​margins of as much as 70% or higher. \"So it's not comparable to the dot-com bubble, when companies were not generating meaningful earnings.\"",
    "readingTime": 2,
    "keywords": [
      "processing units",
      "securities investment",
      "foreign investors",
      "market",
      "bubble",
      "stocks",
      "rally",
      "worries",
      "markets",
      "island"
    ],
    "qualityScore": 0.9,
    "link": "https://finance.yahoo.com/news/ignoring-ai-bubble-fears-investors-053150245.html",
    "thumbnail_url": "https://media.zenfs.com/en/reuters-finance.com/1303e6d6fc5d27d84e672269b5c5eb00",
    "created_at": "2025-12-13T06:54:00.470Z",
    "topic": "finance"
  },
  {
    "slug": "google-researchers-find-the-best-ai-model-is-69-right",
    "title": "Google researchers find the best AI model is 69% right",
    "description": "Here's what this means for law firms and other businesses that are betting on AI.",
    "fullText": "We just got a sobering picture of how often AI models get their facts straight. This week, Google DeepMind introduced the FACTS Benchmark Suite, which measures how reliably AI models produce factually accurate answers.\n\nIt tests models in four areas: answering factoid questions from internal knowledge, using web search effectively, grounding responses in long documents, and interpreting images. The best model, Google's Gemini 3 Pro, reached 69% accuracy, with other leading models falling well below that.\n\nFor context, if any of the reporters I manage filed stories that were 69% accurate, I would fire them.\n\nBeyond journalism, this number should matter to businesses betting on AI. While models excel at speed and fluency, their factual reliability still lags far behind human expectations, especially in tasks involving niche knowledge, complex reasoning, or precise grounding in source material.\n\nEven small factual errors can have outsized consequences in sectors such as finance, healthcare, and the law. This week, my talented colleague Melia Russell looked at how law firms are handling the rise of AI models as a source of legal truth. It's messy: She recounts how one firm fired an employee because they filed a document riddled with fake cases after using ChatGPT to draft it.\n\nThe FACTS benchmark is a warning but also a roadmap: by quantifying where and how models fail, Google hopes to accelerate progress. But for now, the takeaway is clear: AI is getting better, but it's still wrong about one-third of the time.\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 2,
    "keywords": [
      "facts benchmark",
      "models",
      "accurate",
      "knowledge",
      "grounding",
      "filed",
      "factual",
      "it's",
      "google"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/google-researchers-find-best-ai-model-69-right-2025-12",
    "thumbnail_url": "https://i.insider.com/693c873c04eda4732f2d7991?width=1200&format=jpeg",
    "created_at": "2025-12-13T03:41:00.164Z",
    "topic": "science"
  },
  {
    "slug": "how-i-got-ai-to-help-me-sell-my-old-couch",
    "title": "How I got AI to help me sell my old couch",
    "description": "Google's Gemini AI chatbot quickly identified an old furniture model, making it easier to sell.",
    "fullText": "The Barr family purchased a new couch recently, and we've been trying to sell the old one. My wife posted in a local email group and a potential buyer got in touch with a question.\n\n\"Do you happen to know the model? I would love to look it up online rather than asking you lots of questions. :) Thanks!\" she wrote.\n\nThis is an old couch, so all product codes dissolved in the sands of time years ago. So, I took two photos of the furniture, uploaded them to Gemini, and asked for help. The Google chatbot sent back its best guess in seconds:\n\n\"Tillary Sectional: This is a very popular, modular West Elm sofa that was available around that time.\n\nAppearance Match: The low back and deep seat are very characteristic of the Tillary. The Tillary is known for having interchangeable backrests that can be moved or removed, and your sofa's armless chaise and backrests look like they might be loose cushions or pieces, which is a key Tillary feature.\n\nNote: Search results show the Tillary was being reviewed and discussed around 2014, making it a strong possibility for a 2013 purchase.\"\n\nGemini also shared other useful tips on how to identify the product, such as \"Some West Elm couches have a zippered compartment on the back or bottom where extra components (like legs or connectors) are stored, and a tag may be inside.\"\n\nThat was enough to find a product match and send all the details over to the buyer. We closed the deal, for $200, on Tuesday. That's enough for 10 months of a Gemini Pro subscription.\n\nThis holiday period, the Barr family plans to spend a lot of time slouching on our new, fancy couch. I call it the \"AI bubble couch.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 2,
    "keywords": [
      "barr family",
      "couch",
      "product",
      "back",
      "email",
      "buyer",
      "look",
      "backrests",
      "tillary",
      "gemini"
    ],
    "qualityScore": 0.95,
    "link": "https://www.businessinsider.com/google-gemini-ai-help-sell-couch-2025-12",
    "thumbnail_url": "https://i.insider.com/693c898e832e0ef1ead62c46?width=1200&format=jpeg",
    "created_at": "2025-12-13T03:41:00.012Z",
    "topic": "finance"
  },
  {
    "slug": "you-can-use-circle-to-search-to-identify-scams-on-android",
    "title": "You Can Use 'Circle to Search' to Identify Scams on Android",
    "description": "Google's AI tools are getting scam detection capabilities.",
    "fullText": "Android users are getting more tools to combat the seemingly endless stream of scam texts from bad actors looking to steal your data and your money. Circle to Search and Google Lens can now assess messages for scam red flags, and if possible fraud is detected, you'll get recommendations for what to do (or not do) next. Even if you think you know the telltale signs of a scam—a sense of urgency, a demand for money or personal information, a link to log in or pay—using these tools can confirm your suspicions, especially when you feel pressured to act.\n\nTo activate Circle to Search, long press the home button or navigation bar on your device and circle the text you want to scan. Alternatively, you can take a screenshot, open Lens in the Google app (also available on iOS), and tap the screenshot. The feature works for text messages as well as communication on messaging apps and social media sites. Google says the capability is available \"when our systems have high confidence in the quality of the response.\"\n\nThis is just the latest in the Google's suite of security features meant to protect against fraud. Pixel users have real-time, AI-powered scam detection, which identifies and alerts you to suspicious conversational patterns in Google Messages and Phone by Google. In-call protections for Android prevent you from taking certain actions, such as sideloading new apps and changing accessibility permission, on your device while on the phone with anyone not saved in your contacts.\n\nEarlier this month, Google also expanded its in-call scam detection feature, meant to combat bank impersonation schemes, to U.S. users. If you are on a call with a number that's not in your contacts and try to open a participating financial app, you'll get a notification reminding you not to share information and a one-click option to stop screen-sharing and end the call.",
    "readingTime": 2,
    "keywords": [
      "scam detection",
      "users",
      "circle",
      "android",
      "tools",
      "combat",
      "money",
      "fraud",
      "you'll",
      "device"
    ],
    "qualityScore": 0.75,
    "link": "https://lifehacker.com/tech/circle-to-search-can-now-identify-scams-on-android?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC9SAFF9FGZWDBX4J2A5SMKB/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-13T03:40:59.006Z",
    "topic": "tech"
  },
  {
    "slug": "google-removes-scihub-domains-from-us-search-results-due-to-dated-court-order",
    "title": "Google Removes Sci-Hub Domains from U.S. Search Results Due to Dated Court Order",
    "description": "Google has removed dozens of Sci-Hub domain names from its search results in the U.S., marking the country's first pirate domain removals.",
    "fullText": "In 2017, American Chemical Society (ACS), a leading source of academic publications in the field of chemistry, won a lawsuit against Sci-Hub and its operator, Alexandra Elbakyan.\n\nThe ‘Pirate Bay of Science’ had failed to appear at a Virginia federal court, resulting in an easy win for the publisher and a $4.8 million default judgment award for damages.\n\nMore important, perhaps, was the broad permanent injunction that the Virginia federal court signed off on in 2017. This order effectively gave ACS free rein to take down existing and newly registered Sci-Hub domain names.\n\nThe injunction also required all parties “in active concert or participation” with Sci-Hub to “cease facilitating access” to these domain names, including search engines, hosting providers, ISPs, and domain name registrars, the order clarified.\n\nOn paper, this injunction enabled ACS to request American ISPs and search engines to ‘block’ existing and future Sci-Hub domains. However, there was no sign that the publisher was doing so. Aside from a few suspended domains, Sci-Hub remained widely accessible.\n\nWhether ACS did not feel the need to enforce the order against search engines and other intermediaries or if these companies actively objected to the requested actions was unknown. And as time passed, the injunction became a distant memory, at least for a few years.\n\nEarlier this week we spotted a unique request in the Lumen Database, where the 2018 injunction was cited. The notice in question asks Google to deindex 34 (sub)domains linked to Sci-Hub.\n\nNone of these domains were referenced in the 2018 injunction but are indeed linked to Sci-Hub. Many of the partially redacted domains appear to be domain variations of the scihubtw.tw mirror network, such as edu.scihubtw.tw and freeus.scihubtw.tw.\n\nIt’s surprising to see this type of enforcement seven years after the injunction was issued, but the request is legitimate. Google is certainly taking it seriously and has deindexed these domains from its search results in America. In other countries, the same domains remain accessible.\n\nThe December 2 notice was sent by UK law firm Wiggin LLP, which sent a similar request in September this year, targeting a few dozen other Sci-Hub domains. In total, we spotted seven notices, with the earliest dating back to 2022.\n\nThe results of these removals are also clearly visible in Google search. Those who search for Sci-Hub in the U.S. will see the following notice at the bottom of the results.\n\nIt’s not clear why it took five years before ACS urged Google to take action in response to the injunction. However, these removals are similar to Google’s removal of pirate site domains in other countries in response to ISP-blocking orders. Voluntary cooperation by Google was uncovered shortly before ACS first notified the search engine.\n\nGoogle’s voluntary cooperation with ISP blocking orders in Australia, the Netherlands, France, the UK, and elsewhere also brings up an important question. Is Google cooperating with the permanent injunction in the U.S. because it feels legally compelled to do so, or is that a voluntary gesture too?\n\nThe 2018 injunction requires all parties “in active concert or participation” with Sci-Hub to take action. While search engines are mentioned as an example, Google and other tech companies have previously argued that neutral third-party services are not necessarily “in active concert or participation”.\n\nIt is likely that Google maintains this stance, opting to voluntarily comply with orders targeting other third parties. That would mirror its response to site-blocking orders elsewhere.\n\nWe contacted Google hoping to hear answers to these questions, but the company did not respond to our request for comment.",
    "readingTime": 3,
    "keywords": [
      "virginia federal",
      "federal court",
      "active concert",
      "voluntary cooperation",
      "search engines",
      "permanent injunction",
      "sci-hub domains",
      "request",
      "parties",
      "participation"
    ],
    "qualityScore": 1,
    "link": "https://torrentfreak.com/google-removes-sci-hub-domains-from-u-s-search-results-due-to-dated-court-order/",
    "thumbnail_url": "https://torrentfreak.com/images/dmca-google-f.png",
    "created_at": "2025-12-13T03:40:58.805Z",
    "topic": "tech"
  },
  {
    "slug": "if-google-wins-ai-race-nvidia-is-in-trouble-says-author-of-jensen-huang-biography",
    "title": "If Google wins AI race, Nvidia is 'in trouble,' says author of Jensen Huang biography",
    "description": "Jensen Huang biography author Stephen Witt explains why Google's self-developed AI model, Gemini, could pose a threat to Nvidia's AI dominance",
    "fullText": "Nvidia's (NVDA) AI turf could take a blow if Google (GOOGL, GOOG) keeps firing on all cylinders.\n\n\"The biggest risk right now obviously is Google,\" said Stephen Witt, author of \"The Thinking Machine,\" a Jensen Huang biography.\n\nThat risk, he told Yahoo Finance's Opening Bid, is largely tied to Google's Gemini model. Witt described it as the \"best AI right now in the benchmarks outside the Nvidia stack.\"\n\nWitt explained that Gemini was trained entirely on its Tensor Processing Units (TPUs). If Google proves it can sustain world-leading AI development using only its homegrown chip stack, it sets a potent precedent for other tech giants to follow suit.\n\n\"That's a huge risk,\" Witt said. \"If Google ends up winning this AI race ... Nvidia will be in trouble.\"\n\nThis risk, coupled with competition from rivals like Broadcom (AVGO) and Advanced Micro Devices (AMD), is why \"it's very easy to imagine a world\" where Nvidia's high-flying stock declines. The AI chipmaker's shares are up over 1,270% in the past five years.\n\nTo mitigate the core risk of rivals like Google winning the chip war, Nvidia CEO Jensen Huang is already looking past generative AI. Witt said a significant amount of the CEO's personal effort is being poured into the next great computing wave: robotics.\n\nIf Huang can dominate the robotics wave, he said, \"that will mean several trillion dollars in market capitalization for this company.\"\n\nHowever, Nvidia has another issue: the lack of any clear succession plan.\n\n\"It's just Jensen at the top,\" Witt said. \"There's no second in command. There's no obvious successor.\" He noted that the board has been silent, and Huang has offered no advice on a succession strategy.\n\nThat suggests Nvidia's $4 trillion valuation — which accounts for over 8% of the entire S&P 500 (^GSPC) — is, in many ways, resting solely on Huang's vision.\n\nWitt described Huang as a \"world-class engineer\" who could \"design these microchips himself,\" a skill that whoever takes the helm must also possess. He noted that neither of Huang's two children, who work at the company, has a technical background, making them noncontenders for the top spot.\n\nWitt also provided a look behind the polished stage persona of Huang, known for his trademark leather jacket. Beneath the showmanship is a highly intense, \"almost totally neurotic\" leader who is driven not by optimism, but by fear.\n\n\"He's driven by negative emotions, things like fear of failure, guilt, even shame are what make Jensen get up in the morning and work so hard to make Nvidia succeed,\" Witt explained.",
    "readingTime": 3,
    "keywords": [
      "jensen huang",
      "risk",
      "witt",
      "stack",
      "chip",
      "rivals",
      "it's",
      "wave",
      "robotics",
      "succession"
    ],
    "qualityScore": 1,
    "link": "https://finance.yahoo.com/news/if-google-wins-ai-race-nvidia-is-in-trouble-says-author-of-jensen-huang-biography-184624884.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/pgRIv4CaXitTJsbNTIjhsA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD04MDA-/https://s.yimg.com/os/creatr-uploaded-images/2025-12/60ef8cc0-d6b2-11f0-9a17-1d997177f998",
    "created_at": "2025-12-12T18:55:59.740Z",
    "topic": "finance"
  },
  {
    "slug": "openai-aims-to-silence-concerns-it-is-falling-behind-in-the-ai-race-with-release-of-new-model-gpt52",
    "title": "OpenAI aims to silence concerns it is falling behind in the AI race with release of new model GPT-5.2",
    "description": "OpenAI said its new model outperforms those from rivals Google and Anthropic across a wide range of evaluations.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/openai-gpt-5-2-launch-aims-to-silence-concerns-it-is-falling-behind-google-anthropic-code-red/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2198334790-e1765478723707.jpg?resize=1200,600",
    "created_at": "2025-12-12T03:50:28.964Z",
    "topic": "business"
  },
  {
    "slug": "exclusive-youtube-launches-option-for-us-creators-to-receive-stablecoin-payouts-through-paypal",
    "title": "Exclusive: YouTube launches option for U.S. creators to receive stablecoin payouts through PayPal",
    "description": "Google, which owns YouTube, has already used PayPal’s stablecoin to receive payments from customers using its cloud computing service.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/youtube-paypal-google-stablecoin-payouts-pyusd/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2235768862-e1765505283880.jpg?resize=1200,600",
    "created_at": "2025-12-12T03:50:28.949Z",
    "topic": "business"
  },
  {
    "slug": "google-will-now-let-you-virtually-try-on-clothes-with-just-a-selfie",
    "title": "Google Will Now Let You Virtually Try on Clothes With Just a Selfie",
    "description": "Is this useful, or creepy?",
    "fullText": "The pace of AI technology is so rapid, it's tough to keep up with everything. At Google I/O back in May, Google rolled out an AI-powered shopping feature that let you virtually try on clothes you find online. All you needed to do was upload a full-length photo of yourself, and Google's AI would be to dress you up in whatever article of clothing you liked. I still can't decide whether the feature sounds useful, creepy, or a little bit of both.\n\nWhat I can say, however, is that the feature is getting a little creepier. On Thursday, Google announced an update to its virtual try on feature, that takes advantage of the company's new AI image model, Nano Banana. Now, you don't need a full-length photo of yourself: just a selfie. With solely a photo of your face, Nano Banana will generate a full-length avatar in your likeness, which you can use to virtually try on your clothes.\n\nI'm not exactly sure who this particular update is for: Maybe there are some of us out there who want to use this virtual try-on feature, but don't have a full-length photo of ourselves to share. Personally, I wouldn't really want to send Google my photo—selfie or otherwise—but I don't think I'd prefer to have Google infer what I look like from a photo of my face alone. I'd rather just send it the full photo, and give it something to actually work off of.\n\nHere's the other issue: While Google asks you to only upload images of yourself, it doesn't stop you if you upload an image of someone else. Talk about creepy: You can upload someone else's selfie and see how they look in various clothes. There is a system in place to stop you from uploading certain selfies, like celebrities, children, or otherwise \"unsafe\" items, but if the system fails, this feature could be used maliciously. I feel like Google could get around this by verifying the selfie against your Google Account, since you need to be signed in to use the feature anyway.\n\nIf you are interested in trying the feature out—and, subsequently, trying on virtual clothes with your AI-generated avatar—you can head over to Google's try on feature, sign into your Google Account, and upload your selfie. When it processes, you choose one of four avatars, each dressed in a different fit, to proceed. Once through, you can virtually try on any clothes you see in the feed.\n\nAgain, I see the potential usefulness of a feature that lets you see what you might look like in a certain piece of clothing before buying it. But, at the same time, I think I'd rather just order the item and try it on in the comfort (and privacy) of my own home.",
    "readingTime": 3,
    "keywords": [
      "i'd rather",
      "google account",
      "feature",
      "clothes",
      "upload",
      "full-length",
      "selfie",
      "virtually",
      "virtual",
      "don't"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-virtually-try-on-clothes-with-selfie?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC7MXWH4N3KCDYTMB9DBREC0/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-12T03:50:27.017Z",
    "topic": "tech"
  },
  {
    "slug": "openai-and-disney-just-ended-the-war-between-ai-and-hollywood-with-their-1-billion-sora-dealand-openai-made-itself",
    "title": "OpenAI and Disney just ended the ‘war’ between AI and Hollywood with their $1 billion Sora deal—and OpenAI made itself ‘indispensable,’ expert says",
    "description": "“Google has YouTube. OpenAI now has the Magic Kingdom,” copyright expert Matthew Sag said.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/11/openai-disney-sora-deal-hollywood-war-ended-matthew-sag/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-1534551119-e1765478095544.jpg?resize=1200,600",
    "created_at": "2025-12-11T18:58:23.551Z",
    "topic": "business"
  },
  {
    "slug": "google-stitch",
    "title": "Google Stitch",
    "description": "Stitch generates UIs for mobile and web applications, making design ideation fast and easy.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://stitch.withgoogle.com/",
    "thumbnail_url": "https://app-companion-430619.appspot.com/static/og.png",
    "created_at": "2025-12-11T13:53:40.718Z",
    "topic": "tech"
  },
  {
    "slug": "included-health-is-launching-an-ai-personal-health-assistant-thatll-face-off-with-big-tech-from-verily-to-openai",
    "title": "Included Health is launching an AI personal health assistant that'll face off with Big Tech from Verily to OpenAI",
    "description": "The startup's new tech could compete with Big Tech's health AI projects from heavyweights like Alphabet and OpenAI.",
    "fullText": "Included Health is rolling out a new AI tool that could pit it against Big Tech's latest health bets.\n\nThe healthcare startup has launched an AI-powered personal health assistant, Business Insider has learned exclusively. The tech draws on patients' medical claims, benefits information, and other data to offer on-demand answers to health-related questions.\n\nIncluded Health is tapping into a hot area in healthcare AI, where it's competing against other health startups as well as tech heavyweights. Alphabet's Verily released its own AI-powered app in October that allows patients to connect their medical records and ask a chatbot their health-related questions. OpenAI wants to win in consumer health tech, too, and is considering building tools such as its own personal health assistant, Business Insider reported in November.\n\nIncluded Health has been scaling on the premise of personalizing how patients interact with their healthcare for over a decade. The company, which sells tech to about 300 employers and health plans to help patients better navigate their health benefits, tested its AI assistant for about 18 months to ensure its accuracy in smaller pilots before making it available to its entire employer base, CEO Owen Tripp said.\n\n\"This can't be ChatGPT level of probability. It has to be precise,\" he said.\n\nTripp is optimistic about patients receiving general health guidance from LLMs like OpenAI's ChatGPT or Anthropic's Claude. Those AI tools can help patients learn more about their conditions and prepare for doctor's visits, he said. But he emphasized that Included's tech takes that guidance a step further.\n\n\"When it gets down to the business of actually taking care of oneself or taking care of somebody else, you're going to need a lot of very secure, specific data and a whole context to go solve problems, including the exact medical history of that patient,\" Tripp said.\n\nPatient-facing healthcare AI sometimes walks a regulatory tightrope, especially if the tech provides personalized advice that effectively replaces the work clinicians are licensed to do. Tripp said he doubts that most large tech companies attempting to delve into medical records aggregation will want to grapple with that complexity.\n\n\"I predict, like many before them, they will pull back. It's just hard, and the juice is often not worth the squeeze for these high-profile companies,\" he said.\n\nIncluded Health's personal health assistant, called Dot, has become its members' front door and the foundation for Included's new products, said COO Nupur Srivastava.\n\nIncluded recently put Dot in front of members during open enrollment to help answer their benefits questions, Srivastava said. The AI agent can also help patients prepare for doctor's visits and send the clinician a summary of patients' past visits ahead of time.\n\nIncluded Health still employs plenty of its own clinicians and care advocates that members can talk to if they prefer. Srivastava also noted that if a patient mentions the term 'suicide' in a conversation with Dot, \"within a minute, someone will call you.\"\n\nWhen asked about Big Tech and AI startups' ambitions to build personalized health AI, Tripp said that Included Health is in talks with multiple potential partners to help them achieve those goals. He didn't specify which companies it's talking to, but he suggested some AI companies are focused on acquiring personalized health data that they can anonymize and use to train models.\n\n\"But when it comes to actually delivering patient care, we're pretty confident that companies that are going to succeed will be the ones that have well-trained physicians licensed in all 50 states, delivering on a real-time platform, across mind, body, and wallet,\" he said.\n\nIncluded Health was supposed to go public in 2022. The startup had hired banks for an IPO push, but pulled out of its planned investor meetings when the market started to tank, Tripp told Business Insider in January.\n\nTripp declined to share specifics about Included's exit strategy as of November. But Included is profitable, so the company doesn't need to raise money through a public listing, he said. Included hasn't publicly fundraised since it was formed from the 2021 merger of Grand Rounds and Doctor on Demand, and the company hasn't shared its valuation.\n\nThe public markets haven't been forgiving to healthcare startups. Only two digital health companies went public this year, Hinge Health and Omada Health. And while Hinge and Omada have fared far better than most companies that listed during digital health's 2021 IPO wave, healthcare IPO hopefuls still face high standards to going public and significant volatility risks once they begin trading.\n\n\"The last few years in our space haven't been a great commercial for being a public company,\" Tripp said.\n\nWith so many developments in healthcare AI, however, Tripp does recognize that an IPO could create opportunities for Included Health to acquire other companies.\n\n\"I do think this is a time where there are going to be some interesting capabilities and technologies available in the market that allow us to provide even more service to our members,\" he said. \"I do have my eyes very open to how I would use capital to execute on some of those M&A events. That part is more important to me.\"",
    "readingTime": 5,
    "keywords": [
      "assistant business",
      "included health",
      "doctor's visits",
      "medical records",
      "personal health",
      "personalized health",
      "business insider",
      "patients",
      "healthcare",
      "care"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/included-health-launches-own-ai-personal-health-assistant-2025-12",
    "thumbnail_url": "https://i.insider.com/691290c746c4547ecb058733?width=1200&format=jpeg",
    "created_at": "2025-12-11T13:53:35.019Z",
    "topic": "finance"
  },
  {
    "slug": "google-deepmind-agrees-to-sweeping-partnership-with-uk-government-focused-on-science-and-clean-energy",
    "title": "Google DeepMind agrees to sweeping partnership with U.K. government focused on science and clean energy",
    "description": "The collaboration will see the AI company collaborating with the British government on a robotic lab for new materials, fusion energy, and new research into AI safety and the societal impacts of AI",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/10/google-deepmind-uk-government-partnership-science-clean-energy/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2217672931-e1765404847213.jpg?resize=1200,600",
    "created_at": "2025-12-11T03:50:14.668Z",
    "topic": "politic"
  },
  {
    "slug": "google-stands-to-make-111-billion-if-spacex-goes-public-at-a-15-trillion-valuation",
    "title": "Google stands to make $111 billion if SpaceX goes public at a $1.5 trillion valuation",
    "description": "A SpaceX IPO could hand Google billions. Its early bet on the rocket company may turn out to be one of the most lucrative startup investments ever.",
    "fullText": "Talk about the rich getting richer.\n\nAlphabet, parent company of Google, has been one of the best-performing stocks of the year, up nearly 70%, and now has a market capitalization of $3.8 trillion.\n\nThe company also happened to make what could turn out to be one of the most lucrative startup investments of all time, which could finally bear fruit next year.\n\nIn 2015, Google invested around $900 million in SpaceX for a stake of around 7% in Elon Musk's space company, which was then valued at $12 billion.\n\nNow SpaceX is reportedly planning to go public next year at a valuation of $1.5 trillion, which would make Google's stake worth around $111 billion.\n\nEven for a company as big as Google, SpaceX's success has already had a material impact on earnings.\n\nEarlier this year, Google reported an $8 billion gain from \"non-marketable equity securities,\" which Bloomberg identified as SpaceX. That gain represented 25% of Google's net income for the first quarter of 2025.\n\nGoogle is one of the largest outside investors in SpaceX, along with VC firm Founders Fund and Fidelity.\n\nGoogle and SpaceX did not respond to requests for comment.\n\nGoogle's 2015 investment, which was focused on Starlink, now looks certain to be a towering success, but at that time, it was met with considerable skepticism.\n\n\"One big technical and financial challenge facing the proposed venture is the cost installing ground-based antennas and computer terminals to receive the satellite signals,\" The Wall Street Journal wrote about Google's investment at the time. \"Another unanswered question is how SpaceX plans to transmit Internet signals to Earth. The company isn't believed to control rights to radio spectrum.\"\n\nMost of those questions have been answered with Starlink, now used by everyone from the Ukrainian army to United Airlines.\n\nAside from just the paper gains, Google's investment has also been a strategic advantage, as SpaceX has used Google Cloud to power Starlink.",
    "readingTime": 2,
    "keywords": [
      "google's investment",
      "starlink",
      "google",
      "spacex",
      "stake",
      "success",
      "gain",
      "signals"
    ],
    "qualityScore": 0.85,
    "link": "https://www.businessinsider.com/spacex-ipo-would-earn-google-111-billion-as-an-early-investor-2025-12",
    "thumbnail_url": "https://i.insider.com/6939dbbd71107c9f3457b911?width=1200&format=jpeg",
    "created_at": "2025-12-11T03:50:13.399Z",
    "topic": "finance"
  },
  {
    "slug": "google-maps-will-now-automatically-save-your-parking-spot-on-iphone",
    "title": "Google Maps Will Now Automatically Save Your Parking Spot on iPhone",
    "description": "Wasn't this a thing already?",
    "fullText": "Forgetting where you parked your car isn't just a sitcom trope—though it does make for classic TV. Even in the age of the smartphone, it's all too easy to walk away from your car and retain zero memory of where you left it. As it happens, Google Maps has a solution—provided you have an iPhone.\n\nSo long as you're running the app on an Apple device, Google Maps can automatically remember where you parked and display that information on the map. There's no need to mark the location yourself, tell the app to save your spot, or take a picture of the cross streets in case you forget: You can simply glance at the screen to find a \"You parked here\" label.\n\nThe thing is, while some are reporting this feature is relatively new to them, others suggest it's been out for a while. What gives?\n\nThis feature works if you connect your iPhone to your car, whether via Bluetooth, CarPlay, or a USB cable. When you disconnect your iPhone from your car, Google Maps will assume you've parked, and will automatically mark that location on the map. The feature will also work if you give Google Maps permission to access your Motion & Fitness data, which allows the app to tell when you've started and stopped driving. (You can check on this setting from Google Maps' in-app settings: Head to Navigation, then, under \"Automatically save parking,\" tap Let maps use your motion to save your parking.)\n\nYou can also choose to set Google Maps' location permissions to \"Always,\" which gives the app persistent access to your location data, and also lets it figure out when you've stopped your car. You can change this option on your iPhone by navigating t0 Settings > Privacy & Security > Location Services > Google Maps.\n\nThis feature is genuinely great, and I'm happy Google Maps supports it—even if Android users are inexplicably being excluded for now. But you might also be wondering to yourself, wasn't this already a feature? It doesn't sound all that new. Yet you might have even seen a flurry articles like this one today, all reporting on this \"new\" Google Maps parking feature.\n\nAs it turns out, the answer is a bit complicated. The coverage today all references this announcement on LinkedIn from Google Maps senior product manager Rio Akasaka—only that post is, according to LinkedIn, a month old. Some of the comments are more recent, but others appear to be from the original posting date.\n\nPotentially adding to the confusion, other navigation apps, like Apple Maps, already support automatically saving your parking spot, and while Google Maps has also had an option to save your parking spot for some time, it previously wasn't automatic. If you wanted the app to remember where you stopped, you needed to tap your blue dot on the map and choose \"Save your parking.\" That's how it still is on Android, since the automatic feature only works on iOS.\n\nLooking through the comment sections on articles from outlets like The Verge or MacRumors, some users insist they've had this feature for some time—well before that month-old LinkedIn announcement. This commenter says they've had the feature for years, while this one claims they've had it for \"ages.\" So what gives? Have at least some iPhone users have had this feature for years? It is possible Google has been testing out the feature with smaller subsets of users and only recently rolled it out en masse, but whether or not that actually happened is unclear at this time.\n\nI've reached out to Google for clarification about the timeline of this feature's implementation, and I will update this article if I get an answer. Until then, I can only hope Google is actively working on rolling out the feature to Android, as that really would be something new.",
    "readingTime": 4,
    "keywords": [
      "google maps",
      "parking spot",
      "feature",
      "iphone",
      "location",
      "save",
      "parked",
      "automatically",
      "users",
      "you've"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-maps-can-automatically-save-your-parking-spot-on-iphone?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC4PQH94245NM1ES0TAZG1CE/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-11T03:50:11.778Z",
    "topic": "tech"
  },
  {
    "slug": "the-startup-taking-direct-aim-at-nvidias-ai-iron-grip",
    "title": "The startup taking direct aim at Nvidia's AI iron grip",
    "description": "Chris Lattner helped build the software behind Google TPUs. Now he's coming after Nvidia.",
    "fullText": "In Silicon Valley, where bold technical bets abound, few bets look bolder than trying to break the grip of Nvidia's CUDA, a software stack that's quietly become the operating system of the AI boom.\n\nThat's what Modular, a startup founded by software gurus from Apple and Google, is trying to do.\n\nCofounders Chris Lattner and Tim Davis have spent decades building the software plumbing that sits beneath the modern tech industry. Lattner is famous for creating Apple's Swift programming language. He also built the software underpinning Google's TPU AI chips, with Modular cofounder Tim Davis.\n\nThey're now aiming that expertise at CUDA itself. The attempt borders on madness, but it's the kind of audacious project that could transform the AI industry.\n\n\"It's seen by a lot of people as somewhat crazy,\" said Kylan Gibbs, CEO of startup Inworld AI and a former product manager at Google DeepMind. \"That's where Chris has the advantage: He's smart enough to actually know how to do it, and somewhat crazy enough to set out to do it.\"\n\nCUDA began life almost 20 years ago as a way to make graphics chips programmable. Today, it has grown into a multilayered software ecosystem — language, libraries, compilers, inference systems — that most AI companies rely on.\n\nThat success comes at a cost: Most of the industry is now optimized around a single vendor's hardware. CUDA binds AI workloads to Nvidia GPUs. That is great for Nvidia, but deeply limiting for everyone else.\n\nOn the surface, there seems to be a ton of competition: AMD sells GPUs. Google has TPUs. Amazon created Trainium AI chips, and a host of startups are building similar hardware.\n\nThe problem is that each chip comes with its own software stack optimized just for that component. That means an endless reinvention of the wheel. Most of the time, it's simpler to just stick with CUDA — and Nvidia's GPUs.\n\nAnd yet, AI developers crave portability: Being able to use any combination of GPUs from multiple providers without juggling different software stacks.\n\n\"Nobody is building portable stuff, because why would anyone work on software for more than one chip when the chip projects themselves are doing the software?\" Lattner, Modular's CEO, told me in an interview.\n\nNvidia could extend CUDA to run well on rival AI chips. But doing so would undermine Nvidia's greatest moat: the closed-loop bond between its software and its chips. \"Obviously, they don't want portability,\" he said.\n\nFor Lattner, this paradox presents a big opportunity.\n\n\"We realized there's nobody in the industry that's actually incentivized to do this. It's very expensive, very hard,\" he said. \"And at the same time, everybody wants it.\"\n\nThat's what inspired Lattner and Davis to leave Google and start Modular in 2022, the year ChatGPT took the world by storm.\n\nSince then, Modular has raised $380 million from investors including Greylock, General Catalyst, and GV, Google's venture capital arm. The latest financing in September valued the startup at $1.6 billion. Modular isn't the only effort to break the CUDA lock-in. There has been ZLUDA, an open-source project that was funded by AMD, and more recently, the startup Spectral Compute, which has raised $6 million.\n\nLattner has used some of this money to hire talented programmers from Google, Apple, and other tech companies. They spent three years working in relative obscurity to create the building blocks of a new AI software stack.\n\nThe foundation starts with a brand-new programming language, called Mojo, that offers deep controls for making AI chips run as efficiently as possible.\n\nModular designed this to work similarly to Python, a popular and easy-to-use programming language. But Mojo also has the speed and power of other, more complex languages, such as C++, that are essential for AI development. Mojo also works well with PyTorch, an open-source framework that is often used when building AI models and applications.\n\nI first heard about Modular earlier this year when interviewing Carles Gelada, a former OpenAI researcher. \"There are several interesting projects to create GPU-agnostic frameworks and platforms, and challenge CUDA,\" he said at the time. \"Mojo is the most interesting one.\"\n\nMAX is the next major layer of Modular's new software stack. It powers AI inference, which is how models are run. This part of the system works with Nvidia GPUs, AMD GPUs, and similar chips from Apple. Modular hopes to add more AI chips in the future.\n\nOn top of that is another layer called Mammoth, which helps AI developers manage GPU clusters.\n\nIn late September, Modular announced that it got top performance out of Nvidia's new Blackwell B200 GPUs and AMD's latest MI355X GPUs — crucially on the same software platform.\n\nLattner said Modular got these AMD GPUs to perform roughly 50% better than when these chips run on AMD's own software.\n\nMore importantly, the ability to run different GPUs on the same software stack now supports the tantalizing opportunity to compare Nvidia's offerings with rival AI chips on a more level playing field.\n\n\"The obvious question is: can MI355X compete with Blackwell?\" Modular wrote in a blog announcing the results. \"Early signs point to yes.\"\n\nGibbs, the CEO of Inworld AI, has been putting Modular's software through its paces in the real world.\n\nInworld builds high-speed, real-time conversational AI technology that supports offerings from big companies, including Disney, NBCUniversal, and Niantic Labs.\n\nEarlier this year, when the startup designed a new text-to-speech AI model and got early access to Nvidia B200 GPUs, they issued Modular a challenge: Cut our costs by 60% and reduce our latency by 40% and we'll work with you.\n\n\"Within about four weeks, we were able to get this incredible performance,\" said Gibbs, who signed a partnership deal with Modular soon after. \"I've bet with my wallet.\"\n\nWhile Inworld was mostly lured by Modular's performance gains on Nvidia's latest GPUs, Gibbs likes the flexibility of using different AI chips more easily in the future, if needed.\n\n\"The promise is that we'd be able to move to new hardware,\" he said. \"Let's say AMD takes off, let's say TPUs take off for Google, or there could be other new hardware that comes online. So it's nice to have that optionality.\"\n\nIn fact, Google's TPUs are suddenly having a moment. The internet giant released a new AI model called Gemini 3 to rave reviews recently. That was trained and run using TPUs, and some other AI companies have signed deals to use these chips instead of, or alongside of, Nvidia GPUs.\n\nThat's put Nvidia on the defensive. A project like Modular, with its promise of portability across different AI hardware, adds to this pressure.\n\n\"Nvidia could kill this in a day,\" said Gibbs of the Modular project. \"Nvidia could basically say, 'okay, we don't really care that you run just on Nvidia hardware. Here's a CUDA option that runs on AMD GPUs as well.' It'd be a bit crazy for them to do that, but it's something they could do and that would of course be somewhat bad.\"\n\nFor all Lattner's critique of the industry, he says Modular is not trying to kill Nvidia. In fact, he argues that Nvidia will continue to thrive, even if Modular succeeds spectacularly.\n\n\"We're trying to build something like Android, but for AI hardware,\" he told me, referring to Google's mobile operating system that powers most of the world's smartphones.\n\nDespite billions of people using Android devices, this success didn't kill iOS, Apple's mobile operating system. iPhones still rule in the US, for example.\n\nLattner thinks something similar will happen in AI as Modular's software makes other hardware more competitive, giving developers more freedom, and chipping away at the industry's single-vendor monoculture.\n\n\"So Nvidia doesn't have to die, but we do want more competition. We do want more innovation,\" he said. \"I think that's good for the world.\"\n\n Reach out to me via email at abarr@businessinsider.com.",
    "readingTime": 7,
    "keywords": [
      "tim davis",
      "let's say",
      "mobile operating",
      "operating system",
      "programming language",
      "somewhat crazy",
      "modular's software",
      "software stack",
      "amd gpus",
      "chips"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/nvidia-ai-chip-gpu-grip-modular-chris-lattner-google-2025-12",
    "thumbnail_url": "https://i.insider.com/6938812f71107c9f3457a0d9?width=1200&format=jpeg",
    "created_at": "2025-12-10T13:50:15.387Z",
    "topic": "finance"
  },
  {
    "slug": "eu-welcomes-seamless-data-transfer-between-iphone-and-androi",
    "title": "EU welcomes seamless data transfer between iPhone and Android",
    "description": "The EU welcomes Apple and Google's plans to enable a more seamless data transfer between iPhone and Android devices.",
    "fullText": "Apple and Google want to simplify data transfer between their iOS and Android platforms, support more data formats, and enable wireless transfer. The EU welcomes this step, stating: The Digital Markets Act is showing its effect.\n\nThe EU Commission has sent a statement to 9to5Mac. In it, a Commission spokesperson says about Apple and Google's cooperation in this area that it is an \"example of how the Digital Markets Act (DMA) benefits users and developers\".\n\nThe EU further states that this solution allows users to easily transfer data from iPhone to Android and vice versa when setting up a new device. According to the report, the transfer is intended to support \"many types of data\" – including contacts, calendar events, messages, photos, documents, Wi-Fi networks, passwords, and also data from third-party apps. Unlike previous solutions, the optimized method will also be wireless.\n\nAccording to the EU, the more seamless cross-platform data transfer follows the introduction of eSIM transfer by Apple and Google last October. Currently, however, this solution is only supported by a few network operators, but also by some outside the EU.\n\nBoth solutions are the direct result of the DMA, which \"requires effective data portability\" from certain services – including iOS and Android, the EU statement continues. Apple has summarized its work on this project in its DMA compliance reports from March 2024 and March 2025. Furthermore, the solutions are possible through extensive technical work and collaboration between Apple and Google, as well as through intensive discussions with the Commission over the past two years, according to the EU spokesperson.\n\nUnlike numerous features that Apple offers exclusively to its customers in the EU, such as alternative marketplaces, the seamless data transfer between platforms will be provided worldwide. For Apple, the solution is likely to be a win to attract new customers from Google's Android. This could, of course, also happen the other way around, with Google poaching from Apple. For consumers, the new solution means no longer being necessarily trapped in one of the two platform worlds.\n\nIncidentally, Apple will have to open up its iOS and iPadOS even further in the future, so that headphones and smartwatches, for example, work more seamlessly with the operating systems. These requirements are not at all to the company's liking, and it wanted to prevent this. The iPhone maker and Google are also not big fans of the Digital Markets Act: Apple would prefer to abolish the law, Google wishes for a reset, as it causes significant collateral damage in its current form.\n\nDon't miss any news – Facebook,\n LinkedIn or\n Mastodon.\n\nThis article was originally published in\n\n German.\n\n It was translated with technical assistance and editorially reviewed before publication.",
    "readingTime": 3,
    "keywords": [
      "digital markets",
      "markets act",
      "apple and google",
      "the eu",
      "transfer",
      "solution",
      "solutions",
      "platforms",
      "wireless",
      "statement"
    ],
    "qualityScore": 1,
    "link": "https://www.heise.de/en/news/EU-welcomes-seamless-data-transfer-between-iPhone-and-Android-11110651.html",
    "thumbnail_url": "https://heise.cloudimg.io/bound/1200x1200/q85.png-lossy-85.webp-lossy-85.foil1/_www-heise-de_/imgs/18/4/9/9/4/1/1/5/shutterstock_2400360099-acf4465881188230.jpg",
    "created_at": "2025-12-10T13:50:10.143Z",
    "topic": "tech"
  },
  {
    "slug": "i-tried-the-new-android-xr-smart-glasses-from-google-and-xre",
    "title": "I Tried the New Android XR Smart Glasses from Google and XReal",
    "description": "Google's Android XR platform and XReal's Project Aura are bringing smart glasses closer to mainstream reality, offering developers powerful tools to create practical, everyday wearable experiences that go far beyond novelty gadgets.",
    "fullText": "Google's Android XR platform is still in its early development stages, but it's full of potential. Designed to bring consistency to mixed reality (XR) headsets and smart glasses—essentially an \"Android for XR\"—it aims to address the fragmentation that has long hindered the category's growth. Currently, only one Android XR device is available for purchase, Samsung's $1,799 Galaxy XR, and trying it out impressed me enough to nominate it and Android XR for a TechEx award.\n\nStill, the Galaxy XR, with its Apple Vision Pro–style design, doesn't show what Android XR can mean for smart glasses specifically. Now, Google has finally provided that clarity by unveiling two of its own smart-glasses development kits and pulling back the curtain on XReal’s Android XR–powered Project Aura. After trying all three, I’m more convinced than ever that Android XR could mark a major leap forward for smart glasses, maybe even more than for headsets.\n\nGoogle’s Android XR development kits are designed for developers and manufacturers seeking to create products and experiences that rival those of the Meta Ray-Ban Display and similar smart glasses. They use a color display that combines an embedded microprojector with a special pattern etched into the lens, known as a waveguide, that directs images into your eye. Waveguide displays are limited in terms of field of view and resolution, but they can be much smaller and lighter than other types of wearable displays, enabling smart glasses that aren’t much bulkier or heavier than ordinary specs.\n\nThe two Google development kits are completely wireless, running on their own batteries, and connect to an Android phone for all software processing. They’re nearly identical, with the only difference being that one has a monocular display that only shows a picture to the right eye, and the other has a binocular display that can show images to both eyes. The binocular version can produce stereoscopic 3D images that appear to have depth, but the monocular version is lighter, weighing just 1.73 ounces (49 grams). For what it’s worth, I didn’t feel much of a difference in weight between the two.\n\nI got to try several Android XR activities on the Google smart glasses, giving me a good sense of how this type of device will work in the real world. To start, I tried YouTube Music on the monocular pair, which played audio into my ears and displayed a widget on the screen with track information and playback status. Tapping and swiping a touch strip on the right temple of the glasses let me play, pause, and skip tracks easily. The widget was sharp and easy to read, which is obviously vital for any smart glasses with a display. The music sounded fine as well, but I was indoors in a relatively quiet room. Of course, as development kits, their hardware components will likely differ from those in retail-ready products, so don’t read too much into the audio or video quality. The demo focused on the experience and features, as well as how Android XR, as a platform, can work on smart glasses.\n\nThe next demo was an incoming Google Meet video call. A Google rep called the phone number connected to the glasses, and I answered by tapping the touch strip. Her face appeared on the glasses display, in color, and I could see her talking, just as if I were on a video call on my laptop. She couldn’t see me since there wasn’t a camera pointed at my face, but I could share my own view through the glasses’ cameras with a swipe. Again, the call looked and sounded good, and I didn’t experience any hiccups.\n\nNavigation through Google Maps is baked into Android XR, so I was shown how it can work on the glasses. I asked for a nearby store, and it provided me with a few options. After I chose one, I saw a directional arrow and distant measurements for turn-by-turn directions to that store. Looking downward, the arrow transformed into a full map of my surroundings. It seemed to work very well and tracked the direction I was facing, although I couldn’t exactly walk anywhere to see how accurately it followed my location. Having a video game-like minimap in reality has long been one of my dreams for smart glasses, and it appears we’re getting closer to that. I hope that the Android XR Google Maps app will allow me to configure when and how the map appears, so I don’t have to look at my feet for it, but I couldn’t confirm if those options will be available.\n\nGoogle is pushing Gemini hard, and it’s no surprise that AI plays a significant role in Android XR. On waveguide smart glasses running Android XR, Gemini is always available with a button press and a wake word, ready to perform simple tasks like playing music or making a call, or do more complex analysis like answering questions or identifying what you’re looking at.\n\nDuring the demo, I was invited to ask Gemini for recipe suggestions while viewing a pantry wall filled with ingredients. I stared at a few jars of pasta, and it provided instructions on making pasta salad. Gemini could see the foodstuffs I was looking at and not only successfully named the pastas but also correctly identified sweet potatoes, and even noted that they were likely American sweet potatoes rather than Japanese or Korean ones. That kind of machine vision processing is far more impressive to me than large language model (LLM) outputs of recipes.\n\nAll of the demos described above felt very familiar, because I had similar experiences with the Meta Ray-Ban Display when I tried it out. In fact, asking an AI to produce a recipe based on the ingredients you’re looking at is a page straight out of Meta’s playbook, though my Android XR demos never failed as the on-stage Meta Connect presentation did. The functions are indeed very similar, but the difference lies in how the underlying systems are intended to be used. Android XR is a broad platform for third-party developers and manufacturers, featuring core features and elements that can be used directly or built upon, depending on the final product. The Meta Ray-Ban Display’s operating system is the final interface for a specific product. It doesn’t even have a public-facing name like Android XR or Apple’s visionOS. Android XR is a first step for an entire ecosystem, and Meta Ray-Ban Display might receive upgrades and iterations down the line, but it isn’t going to drive a field of non-Meta glasses.\n\nAlso, the Meta Ray-Ban Display is monocular, which means Google’s binocular development kit could show me some new tricks with 3D. After putting on the binocular glasses, I was shown some 3D video on YouTube. It indeed looked nice and 3D, and was fairly watchable. The binocular glasses also provided a 3D view of the city when I brought up Google Maps. As I mentioned before, though, waveguide displays have a limited field of view, so even if the picture is fairly sharp and in full color, I’m not sure I’d rely on this type of glasses to watch a full show or movie. In fact, the reps noted that watching longer-form video content here wasn’t an intended use case, though being able to play shorter clips is certainly handy.\n\nThe 3D features are nice but not vital to the experience. They can make using the glasses feel more immersive, but they aren’t why I favor binocular smart glasses. I find having a display in only one eye slightly disorienting compared with being able to see the same picture through both eyes. While I can quickly get used to it with the display in my dominant right eye, people with dominant left eyes could possibly find it more awkward to use, or at least a bit less pleasant.\n\nThese Android XR glasses feel like a significant step in making wireless, waveguide display smart glasses truly accessible to users. I’ve tested several, like the Rokid Glasses and the Even Realities G1 (and I’m currently testing its successor, the G2), and they’ve been very inconsistent and unpolished. Some, like the Rokid Glasses, have useful features and seem reliable enough if you get through the learning curve, but I haven’t been able to recommend any of them without major caveats. They also feature monochrome green displays rather than color, which has also been frustrating.\n\nI haven’t fully tested the Meta Ray-Ban Display outside of supervised demos yet, but while they seem like some of the most refined waveguide smart glasses so far, they’re also purely Meta products, using a closed system with limited room for outside development or growth. Google’s development kits demonstrate how Android XR can be implemented consistently by multiple manufacturers, serving as templates for developers to create their own apps. It’s what smart glasses like these have sorely been needing, and what could push this particular sub-category past its current status as shaky early-adopter hardware.\n\n Policy.\n\nYour subscription has been confirmed. Keep an eye on your inbox!\n\nThen there’s XReal’s Project Aura, which has turned out to be a completely different beast and much closer to the Samsung Galaxy XR than Google's waveguide development kits. XReal is another major manufacturer, besides Samsung, working with Google to introduce the first Android XR devices, and I got to try its Project Aura smart glasses along with the other devices.\n\nProject Aura is a pair of smart display glasses that use bulkier prisms instead of a waveguide system, but offer an incredibly wide 70-degree field of view. It doesn’t fill up your entire vision with a picture (even the Galaxy XR, with its 109-degree field of view, doesn’t do that), but it still looks like a huge theater screen in front of you. On the virtue of its display alone, it’s more impressive than other prism smart glasses, and with a much wider view than my current top pick in that category, the 57-degree XReal One Pro.\n\nThe display isn’t the most interesting thing about Project Aura. Its claim to fame is that it’s an Android XR pair of smart glasses, and unlike the Google waveguide glasses, it’s fully self-contained. It connects through a wire to a phone-sized control box that runs Android XR as its own operating system. Inside that box is a Snapdragon XR2+ Gen 2 processor, the same chip that drives the Galaxy XR. In other words, this pair of smart glasses has about the same processing power and capabilities as Samsung’s bigger, bulkier headset. Its picture isn’t as big or sharp, and it doesn’t have as many outward-facing cameras, but it has the same brain, the same interface, the same apps, and the same intuitive hand-tracking control.\n\nWhen I put on Project Aura, I was greeted with a tutorial I recognized from using the Galaxy XR. The glasses directed me to hold my hands in front of me and point at objects. I selected them by aiming with my finger and could click on them with pinching gestures or even grab and move them around. The same gestures applied to icons and app windows as well, allowing me to point and pinch to open new apps, resize them, and move them around. I could even bring up both the home screen and a quick menu by turning my palm to face me and making the pinch gesture. It felt exactly like the Galaxy XR, and I picked it up instantly.\n\nProject Aura appears to be capable of nearly everything the Galaxy XR can do. I was able to open multiple apps, arranging Chrome, Google Maps, and YouTube around my view. I could play a game called Demio that projected a 3D tabletop RPG view in front of me. I could also connect to a nearby computer directly and play games through it with minimal lag.\n\nI hailed the Galaxy XR’s control system as the best I’ve used in a headset since the Apple Vision Pro, which I still consider the most advanced and intuitive mixed reality device I’ve reviewed. It requires no controller or touchpad—you just point and pinch. This control system is incredibly rare on headsets, but seeing it in a much smaller and lighter pair of smart glasses? I wasn’t expecting that.\n\nSmart glasses, especially ones with a display, have struggled with controls. The waveguide models I’ve tested, as well as the Google development kits, all rely on voice controls or buttons and touch strips on the temples. The Meta Ray-Ban Display features the Neural Link Band, which tracks the micro-gestures of your hand, but I found it to be inconsistent when I tried it. Tethered prism display smart glasses rely entirely on the connected device for controls, with the exception of a basic button-based settings menu at best. Project Aura is a full, standalone Android XR device that supports hand-tracking and gestures, and it’s the most intuitive way to use smart glasses I’ve ever seen.\n\nSince it’s a pair of smart glasses and not a full headset with a strap, Project Aura is much easier to put on and take off than the Galaxy XR, and it feels much more comfortable to wear. You can also see through it when the power is turned off, since its projection system relies on clear prism lenses. You shouldn’t use Project Aura while walking around as you would with waveguide display smart glasses, though; the prism lenses might be transparent, but they’re still thick enough to warp your outside vision enough to be uncomfortable at best and dangerous at worst. Prism-equipped smart glasses like these are best used when stationary or in a controlled space, such as a larger mixed reality headset, rather than when crossing the street or riding the subway.\n\nProject Aura has a few disadvantages to go with its smaller, more convenient form factor. The display is the biggest part, since it’s noticeably narrower than the Galaxy XR’s. While the view is still huge for smart glasses, it isn’t as immersive as the headset’s, and its 1080p resolution isn’t nearly as sharp as the Galaxy XR’s 3,552 by 3,840 pixels per eye. Because it doesn’t have as many cameras, there’s a slightly smaller area where it can track your hands; I didn’t need to hold my hands directly in front of my face, but I couldn’t leave them in my lap like I can with the Galaxy XR or the Vision Pro. It also lacks internal cameras for eye tracking, so you must use hand movements instead of your gaze for control.\n\nEven with its limitations, I found Project Aura to be incredibly exciting. I’ve been using prism-based smart glasses, particularly the XReal One Pro, for working away from my desk and watching videos and playing games on my phone, and they have been very useful for that. Those use cases all require connecting the glasses to a separate device, such as a phone or laptop, and controlling everything through that device’s touch-screen, controller, or mouse and keyboard. Project Aura works entirely on its own (and as an Android-driven device with Bluetooth, it can also work wirelessly with controllers and keyboards, if you want). The control box is technically still a separate device that connects to the glasses with a cable, but like with the Galaxy XR and Vision Pro, it can be slipped into a pocket and treated like a battery. Combining the convenience of prism smart glasses with the capabilities of a high-end XR headset makes Project Aura feel like the next big leap in smart glasses as a whole.\n\nUnfortunately, you won’t be able to get Project Aura any time soon. My close look at the glasses came with an important detail: It’s a development kit. When Project Aura launches next year, it will be targeting developers and won’t be available for general purchase. So, like with the Google development kits, unless you’re planning to work on Android XR apps, you probably won’t be getting your hands on Project Aura.\n\nI didn’t hear anything about pricing, either, though considering it packs more impressive hardware than the Meta Ray-Ban Display, I wouldn’t be surprised if devs will be shelling out as much for it as they would for the $1,799 Galaxy XR. As for retail-ready Android XR smart glasses, don’t expect to see anything until late 2026 at the very earliest.\n\nI’m PCMag’s home theater and AR/VR expert, and your go-to source of information and recommendations for game consoles and accessories, smart displays, smart glasses, smart speakers, soundbars, TVs, and VR headsets. I’m an ISF-certified TV calibrator and THX-certified home theater technician, I've served as a CES Innovation Awards judge, and while Bandai hasn’t officially certified me, I’m also proficient at building Gundam plastic models up to MG-class. I also enjoy genre fiction writing, and my urban fantasy novel, Alex Norton, Paranormal Technical Support, is currently available on Amazon.",
    "readingTime": 15,
    "keywords": [
      "android xr",
      "project aura",
      "apple vision",
      "i’ve tested",
      "meta ray-ban",
      "vision pro",
      "sweet potatoes",
      "ray-ban display",
      "touch strip",
      "mixed reality"
    ],
    "qualityScore": 1,
    "link": "https://www.pcmag.com/news/i-tried-the-new-android-xr-smart-glasses-from-google-they-impressed-me",
    "thumbnail_url": "https://i.pcmag.com/imagery/articles/01LGQF7z3ySKcGqTgpg7ATP-1.fit_lim.size_1200x630.v1765302444.jpg",
    "created_at": "2025-12-10T13:50:08.807Z",
    "topic": "tech"
  },
  {
    "slug": "notebooklm-has-a-new-feature-for-visual-learners",
    "title": "NotebookLM Has a New Feature for Visual Learners",
    "description": "An infographics feature has arrived to Google's NotebookLM.",
    "fullText": "Another day, another update to Google's NotebookLM, the versatile AI tool that functions like a personal assistant focused only on you and your needs. The latest update is for visual learners: You can turn your source materials into helpful infographics that give you a clear picture—literally—of what the PDFs, websites, videos, or other materials you're studying or organizing are about.\n\nTo use the new feature, open any of your NotebookLM Notebooks (the name given to folders full of specific materials you've uploaded) and navigate to the panel on the right side. It's the same panel where you find the other offerings like the video creator and flashcard maker.\n\nAs with those tools, you just tap the associated button to generate the corresponding product, making sure the sources from your left panel that you want to include are all checked off. I tried it out this morning, first using the NotebookLM account associated with my personal Chrome profile and what I'm studying in my private life, then using the one I have set up for work, which has a tester notebook full of materials about how to study for the SAT. (I am strict about using different Chrome profiles for various parts of my life and am now up to seven.)\n\nIn my personal account, the button was labeled BETA—and it acted like it. After two failed attempts, NotebookLM could not produce an infographic based on my materials. In my work account, though, the beta label was missing and it performed the function just fine, spitting this out:\n\nThis is rolling out in full functionality to accounts at different times, obviously, but I was glad to see one of mine had easy access because I thought the infographic was solid.\n\nI don't consider myself a visual learner and primarily use NotebookLM to refine ideas or generate educational audio clips I can listen to while I clean the house, so I wasn't expecting to like this. I don't like the mind map creator within NotebookLM at all, for instance; flowcharts just aren't how I learn best, and that's fine.\n\nBut the infographic was concise, engaging, and just detailed enough to keep me interested and looking at it. It's unlikely I'll use this to study or refine my work often, but I can absolutely see how it would be useful to someone who learns more visually, especially if all the lines and boxes of a mind map can get too convoluted to be useful.",
    "readingTime": 3,
    "keywords": [
      "mind map",
      "materials",
      "personal",
      "panel",
      "account",
      "infographic",
      "another",
      "visual",
      "studying",
      "it's"
    ],
    "qualityScore": 0.9,
    "link": "https://lifehacker.com/tech/how-to-use-notebooklms-new-infographics-generator-feature?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC2C7Q7BC1AJG0759KMFJ476/hero-image.fill.size_1200x675.png",
    "created_at": "2025-12-10T03:48:30.476Z",
    "topic": "tech"
  },
  {
    "slug": "google-will-fix-your-pixels-broken-display-for-free-if-it-qu",
    "title": "Google Will Fix Your Pixel’s Broken Display for Free (If It Qualifies)",
    "description": "Your Pixel 9 Pro's screen needs to be faulty in the \"right way.\"",
    "fullText": "Smartphone display issues are nothing new. Most of us have dropped our phones the wrong way one time or another, and had to deal with the pain (and cost) of getting them fixed. But when your smartphone's screen starts acting up for no particular reason, it's pretty frustrating—especially if the manufacturer still holds you accountable for the repair fees.\n\nIf that sounds like your experience with your Pixel 9 Pro, Pixel 9 Pro XL, or Pixel 9 Pro Fold, there's good news: Google is now launching an Extended Repair Program for the Pixel 9 Pro line. According to Google's announcement on Monday, the company has identified a \"limited number\" of Pixel 9 Pro and Pixel 9 Pro XL units that might exhibit display issues that impact the user's experience with the device. Should your Pixel 9 Pro's display show these symptoms, Google will fix the display at no cost to you.\n\nThat doesn't mean any and all display issues on your Pixel 9 Pro device qualify here. Google has identified two specific problems that this Extended Repair Program actually covers. The first is a vertical line present on the display. The line has to run from the bottom of the screen to the top, so partial lines won't quality. The second is display flicker. If you notice your Pixel 9 Pro's display quickly getting brighter and darker, as if someone was flicking a switch back and forth, you qualify for the repair program.\n\nThe Pixel 9 Fold is another story altogether. Like the 9 Pro and 9 Pro XL, Google is offering a free repair program for the 9 Pro Fold. However, unlike the other devices, there are no specific issues identified here. The problems may be display-related, but since the company won't specify, you could bring your 9 Pro Fold in for just about anything that's going wrong with it—as long as you didn't cause the issue yourself. In addition, Google won't actually fix your foldable, but will instead replace it entirely.\n\nThe company is also being strict regarding the quality of the display outside of these issues across all Pixel 9 Pro devices. If your Pixel's display or cover-glass is cracked, that may disqualify you from the free repair. If Google finds liquid damage in your device, same story. In any of these cases, the company will still fix the display issues mentioned above, but they might charge you for it.\n\nAffected Pixel 9 Pro, 9 Pro XL, and 9 Pro Fold units qualify for repair as of Dec. 8, and coverage will last for three years after the original purchase date of the device. You will need to have your device inspected at a Google walk-in center, Google-authorized center, or an online repair store before the company can confirm eligibility. You can get started on your claim from Google's official repair site.\n\nThis is good news for any Pixel 9 Pro users who have these specific issues—or any issues at all for Pixel 9 Pro Fold users. It joins a host of other Extended Repair Programs for Pixel devices, including the Pixel 4a battery program, the Pixel 6a battery program, the Pixel 7a repair program, and the Pixel 8 repair program,",
    "readingTime": 3,
    "keywords": [
      "pro's display",
      "extended repair",
      "pixel battery",
      "pixel pro",
      "battery program",
      "free repair",
      "pixel pro fold",
      "device",
      "identified",
      "qualify"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-will-fix-pixel-broken-display-for-free-if-it-qualifies?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KC1X6QJ222BT0E57J35XZQXA/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-09T18:53:33.718Z",
    "topic": "tech"
  },
  {
    "slug": "the-birth-of-the-internet-troll-2014",
    "title": "The Birth of the Internet Troll (2014)",
    "description": "Trolls are shitting all over our internet. You can hardly search for something as innocuous as \"dog\" on Google without coming across inflammatory attacks",
    "fullText": "Trolls are shitting all over our internet. You can hardly search for something as innocuous as “dog” on Google without coming across inflammatory attacks on every possible dog-related opinion under the sun. All horrible things have to crawl before they can walk/crush spirits, though. Even trolls.\n\nAnd while the term “troll” has become wildly muddied, it did have to come from somewhere. We decided to try and find out just where that dark, acerbic origin story began.\n\nThere were bulletin board systems. And Usenet. And newsgroups. And people just starting to realize the massive potential trembling beneath their fingertips. Anything was possible! Which, as it turns out, is not always a good thing.\n\nIn the early 90s, trolls had yet to come into mainstream public consciousness, at least according to the 1993 Big Dummy’s Guide to the Internet. Flame wars, on the other hand, were already an online staple.\n\nWhether or not you’re familiar with the term, you do know what flaming is. You’ve seen it under horrible political opinions on Facebook. You’ve seen it choking your Twitter stream. And you’ve seen it in every other comment in the vast wasteland that is YouTube. Put simply, a flame is a vicious, personal attack on someone made simply because you disagree with them.\n\nOf course, humans have always had this potential for the irrationally vindictive, but the advent of the internet finally allowed it to thrive. Because as soon as you stuck someone behind a computer, a dangerously insular shield of anonymity came down and, for those inclined, happily took over. In discussing the sort of negotiation tactics that precede a flame war, Norman Johnson, an Associate Professor at Bauer College at the University of Houston explains:\n\nThe literature suggests that, compared to face-to-face, the increased incidence of flaming when using computer-mediated communication is due to reductions in the transfer of social cues, which decrease individuals’ concern for social evaluation and fear of social sanctions or reprisals. When social identity and ingroup status are salient, computer mediation can decrease flaming because individuals focus their attention on the social context (and associated norms) rather than themselves.\n\nThe introduction of anonymity not only made users feel free from the repercussions that might otherwise give them pause, but it also dehumanized potential targets. In other words, the internet gave all our worst impulses just what they needed to thrive.\n\nBecause if someone disagreed with you in the real, live social realm, you might feel frustrated, sure, but you’ll also see that person’s as another human with human emotions—not just a jumble of inflammatory words for you to destroy. You’ll take time to reflect, because you’ll realize there are consequences to your actions. Whereas on the internet, a clean slate is a mere username change away.\n\nSome of the earliest flame wars went down on Usenet, which unbeknownst to these earlier warriors, was building a model for all the trolls to eventually come in its wake. According to Gaffin:\n\nPeriodically, an exchange of flames erupts into a flame war that begin to take up all the space in a given newsgroup (and sometimes several; flamers like cross-posting to let the world know how they feel). These can go on for weeks (sometimes they go on for years, in which case they become “holy wars,” usually on such topics as the relative merits of Macintoshes and IBMs). Often, just when they’re dying down, somebody new to the flame war reads all the messages, gets upset and issues an urgent plea that the flame war be taken to e-mail so everybody else can get back to whatever the newsgroup’s business is.\n\nSo presumably, these troll/flame wars all started earnestly. But watching two groups of people attempt to lambast each other in increasingly epic proportions is—as we all know and hate to admit—wildly entertaining. And once the war of words would simmer down, it’s not at all surprising that someone might start (forcefully, sensationalistically) poking and prodding the more tender of egos. All in hopes of revisiting that awful sort of thrill that comes in watching another human push the very boundaries of sanity, by freaking the fuck out.\n\nThe earliest documented form of internet troll was something called a net.weenie, who did what s/he does ”\n just for the hell of it.” In early internet usenet forums, they were the people being assholes simply for the sheer joy of being an asshole. According to the Guide:\n\nThese are the kind of people who enjoy Insulting others, the kind of people who post nasty messages in a sewing newsgroup.\n\nEven the Electronic Frontier Foundation—formed in 1990—was aware of (and acknowledged) net.weenies prevalence among the more public internet groups. In the group’s early internet guide to mailing lists, one of the main benefits of such a system was that “a mailing list can offer a degree of freedom to speak one’s mind (or not worry about net.weenies) that is not necessarily possible on Usenet.” This was, of course, before the sorts of emails in which an undead child’s wrath and/or Nigerian prince’s livelihood rested on the click of our mouse.\n\nAnd net.weenies sound obnoxious, sure, but the term still didn’t carry the sort of malevolence we now associate with modern trolling. In fact, quite the contrary—some of their games were absolutely incredible.\n\nWarlording was a very specific, beautiful type of early trolling performed by these net.weenies, particularly in the alt.fan.warlord newsgroup in Usenet (a sort of subreddit of early internet days). Considering the limitations of early 90s bandwidth and forums’ general readability issues, Usenet etiquette—netiquette, if you will—asked users to keep their signatures under four lines. This was dubbed the McQuary limit and was not a hard and fast rule. At least in the way that there weren’t actually any real character limits.\n\nThis rule was partially necessary due to new users’ predilections for employing what was called BUAGs (Big Ugly ASCII Graphics) and BUAFs (Big Ugly ASCII Fonts). So to both mock this habit and be the biggest assholes they could be (always reach for the stars, kids), net.weenies\n tore this rule apart in a game called warlording.\n\nThe term came from the user\n Death Star, War Lord of the West, “who featured in his sig[nature] a particularly large and obnoxious ASCII graphic resembling the sword of Conan the Barbarian in the 1981 John Milius movie.” Which, presumably, looked something like this:\n\nThe newgroup\n alt.fan.warlord was created as a sort of sarcastic tribute to the offending sigs, and the jokes spiraled from there. One particularly notable case of warlording was that of\n James Parry‘s signature (better know by the username Kibo) below. Bear in mind, this is all one, single sig.\n\nAlthough every part of this signature is brilliant and deserving of our appreciation and awe, I do have a few favorite sections. Namely, this absurd and not at all remotely helpful Twin Peaks chart:\n\nBecause if anything has ever been worthy of being called art, it is the beautiful, intricate, wholly insincere mess.\n\nIn the late 80s and early 90s there certainly did exist this notion of an internet user who merely enjoyed stirring up trouble—but then that person has for as long as humans themselves have existed. As Whitney Phillips, a media studies scholar and communication lecturer at Humboldt State University (who has\n a book on trolls forthcoming with MIT press) explained to us over email:\n\n[Organized, willful trolling did exist before 4chan and Anonymous came around], though at the time it wouldn’t (necessarily) have been called that. This was a point of fascination to many of the trolls I interviewed; while they engaged in similar behaviors in the pre-4chan years, they didn’t refer to their behaviors as trolling and in fact couldn’t remember what they called it, if they called it anything. They’ve since some to use the term retroactively, but at the time the subcultural definition of the term hadn’t yet taken hold, and so they didn’t think of themselves as trolls.\n\nPurportedly, the actual use of the term “troll” dates back to the 80s, but\n according to the Oxford English Dictionary, the first instance of the term “troll” being used in an online capacity happened on December 14th, 1992 in the usenet group alt.folklore.urban, when someone wrote “Maybe after I post it, we could go trolling some more and see what happens.”\n\nInterestingly enough, it’s around the time that the actual term “trolling” started gaining steam in the mid 90s that the act itself began to evolve from causing annoyance as a result of your beliefs to simply believing in causing annoyance. And, of course, that’s just a single flavor of trolling—almost as soon as the term came into use, it started morphing into a blanket term of unwieldy proportions.\n\nFor instance, at least in retrospect, Brice Wellington was one of the more notorious troll incarnations. He spent much of his time “in alt.atheism, talk.origins, alt.christnet, and other newsgroups that he [would] troll and spam on a daily basis.” Now, whether his brand of trolling was sincere or satiric becomes a little more difficult to suss out. Usenet users at the time seemed certain that Brice was the “real deal,” so to speak, but in looking at some of his more insane rantings, it’s hard to see him as seeking anything more than what would soon be termed “the lulz.”\n\nHere we have Brice on the French:\n\nWhile Brice may have started blurring the line between being infuriating by nature and being infuriating by sheer force of well, alt.tasteless stepped firmly into the latter territory.\n\nIn a 1994 article with Wired, Usenet user Trashcan Man gave one of the first real insights into the prototypical troll mindset by describing alt.tasteless’ flamewar with the unsuspecting rec.pets.cats, a sort of haven for cat fanciers. In other words, prime bait.\n\nBecause for all intents and purposes, alt.tasteless was simply an early version of 4chan’s now-notorious /b/. As Wired explains:\n\nAlt.tasteless was created in the autumn of 1990 “as a place to keep the sick people away from rec.humor and other forums,” according to Steven Snedker, a Danish journalist for Denmark’s largest computer magazine. “Alt.tastelessers see this as an important turn in Usenet history, on a par with the creation of alt.sex. Both alt.tasteless and alt.sex are fine forums that serve their purpose to keep the other parts of Usenet clean, and to dig further into the stuff discussed.”\n\nWhich is all good and great, but being positively revolting certainly loses some of its appeal when you take away any potential foil. Which is why when someone suggested that alt.tasteless descend upon another Usenet group to incite chaos, the alt.tasteless users were delighted and ultimately decided on the cat newsgroup as a prime target. And alt.tasteless’ opening line was a doozy:\n\n… I’m not what you would call a real studly type guy (although I have a lot of women friends), so when I date it’s really important to me. Anyway, [my cat] Sooti goes into heat something fierce (sometimes it seems like it’s two weeks on, two weeks off). I had a date a while back, when she was really bad. Yowling and presenting all the time – not the most auspicious setting for a date. While dinner was cooking, I tried to stimulate her vagina with a Q-tip because I had heard that one can induce ovulation that way. My date came into the bathroom while I was doing this, and needless to say I don’t think she bought my explanation. The date was a very icy experience after that.\n\nWhat should I do. I love my cats, so I don’t want to get rid of them, but I can’t go on like this any more. It’s my love life, or them. Please help!!!\n\nThe earnest advice from rec.pets.cats was intermixed with decidedly more tasteless (naturally) advice from alt.tasteless including, but not limited to, providing “articles about topics such as vivisecting the cat and having sex with its innards.”\n\nWhich, of course, brings us to 4chan.\n\nFor better or worse, in 2003, 4chan entered the public consciousness and with it brought what Phillips refers to as “a very specific understanding of the term ‘troll,'” explaining in a Daily Dot article that “trolling was something that one actively chose to do. More importantly, a troll was something one chose to be.”\n\n4chan’s /b/ board in particular, being the spiritual successor to alt.tasteless, fostered this toxic mentality that if you don’t actually believe in the horrible things you’re saying that it magically becomes justified. As Phillips explained over email:\n\nGranted, the trolls might not really mean what they say. But who cares, they are not, and should not be regarded, as the ultimate arbiters of meaning. In other words, what these “trolls” think about what they do is irrelevant; even if they say they’re “just trolling,” their actions can have serious real-world consequences for the people they target.\n\nSo, say, when 4chan users found an 11-year-old girl’s address and phone number in 2010 and proceeded to call her home making death threats, it didn’t matter that they were “just doing it for the lulz.” Both that classic, deflective refrain and the term troll itself have succeeded in creating a potentially dangerous emotional distance from the actual consequences words can have—whether it’s trolls self-identifying as such or a media-assigned label. According to Phillips:\n\nI don’t accept the idea that assholes get to be assholes with impunity, as if we live exclusively in their world and there’s nothing we can do about it because “boys will be boys.”\n\nRather than defer blindly to the term “trolling,” I like to label behaviors based on what they do in the world. So, if someone is engaging in misogynist behavior, even if they believe they’re “just trolling” (whatever dude), that’s misogyny. And if that person doesn’t like the word misogynist, if that label makes them cry hot tears and feel bad about themselves, then how about not behaving like a misogynist.\n\nBecause even though the term may have gained notoriety on 4chan, the concept—however you may choose to define it—of “trolling” is more mainstream today than it has ever been.\n\nSearch “trolls” on Google and you’ll be hit with a deluge of articles defining the term in any number of ways. Whether it’s being defined as someone who believes what they’re saying in earnest, just wants to stir the pot, or is merely hopping on board a rage bandwagon—any rage bandwagon!—the only common thread is malicious intent. Which, according to Phillips, presents a major problem:\n\nCalling behaviors designed to threaten, intimidate, and silence “trolling” (so, lumping ALL aggressive online behavior under the same umbrella term) risks minimizing the emotional impact of the most extreme behaviors, particularly when those behaviors are piled on as viciously and relentlessly as they have been throughout Gamergate.\n\nClearly, for as long as the internet has been around, trolls have existed in some form—whether they were called that or not. There will always be agitators. There will always be people who want upset others. That’s not going to change.\n\nWhat we can change, though, is how we approach these situations in all their varied forms. Which, according to Phillips, “depends on whose voices platform administrators, advertisers, and other people on the business end choose to privilege—the targets of abusive, intimidating behaviors or those who are doing the intimidating.”\n\nIt’s not an issue of “feeding the trolls” (a problematic phrase in its own right), but rather whether or not we’re going to stop giving a platform to the trolls, the aggressors, and the antagonizers. Whether it be by not validating their behavior with concessions or dropping the catch-all term “troll” in favor of more accurate terminology—be it misogynist, sociopath, or straight-up dick.\n\nSo yes, assholes have and will always be around, as will their unfortunate victims. It’s just a matter of who we let hold the megaphone.",
    "readingTime": 14,
    "keywords": [
      "ugly ascii",
      "causing annoyance",
      "another human",
      "flame war",
      "flame wars",
      "trolls",
      "trolling",
      "troll",
      "it’s",
      "alt.tasteless"
    ],
    "qualityScore": 1,
    "link": "https://gizmodo.com/the-first-internet-troll-1652485292",
    "thumbnail_url": "https://gizmodo.com/app/uploads/2014/10/jebgmemwgiyzwrv1rep2.jpg",
    "created_at": "2025-12-09T18:53:30.863Z",
    "topic": "tech"
  },
  {
    "slug": "eu-opens-investigation-into-googles-use-of-online-content-fo",
    "title": "EU opens investigation into Google’s use of online content for AI models",
    "description": "European Commission to assess whether Gemini owner is putting rival companies at a...",
    "fullText": "European Commission to assess whether Gemini owner is putting rival companies at a disadvantage\n\nThe EU has opened an investigation to assess whether Google is breaching European competition rules in its use of online content from publishers and YouTube creators for artificial intelligence.\n\nThe European Commission said on Tuesday it will examine whether the US tech company, which runs the Gemini AI model and is owned by Alphabet, is putting rival AI owners at a “disadvantage”.\n\n“The investigation will notably examine whether Google is distorting competition by imposing unfair terms and conditions on publishers and content creators, or by granting itself privileged access to such content, thereby placing developers of rival AI models at a disadvantage,” the commission said.\n\nIt said it was concerned that Google may have used content from web publishers to generate AI-powered services on its search results pages without appropriate compensation to publishers and without offering them the possibility to refuse such use of their content.\n\nThe commission said it was also concerned as to whether Google has used content uploaded to YouTube to train its own generative AI models without offering creators compensation or the possibility to refuse.\n\n“Content creators uploading videos on YouTube have an obligation to grant Google permission to use their data for different purposes, including for training generative AI models,” the commission said.\n\nGoogle does not pay YouTube content creators for their content, nor does it allow them to upload their content on YouTube without allowing Google to use such data, it said. The commission noted that rival developers of AI models are barred by YouTube policies from using YouTube content to train their own AI models.\n\nGoogle-owned YouTube says its terms and conditions allow Google to use creators’ work for making AI models. In September, YouTube said: “We use content uploaded to YouTube to improve the product experience for creators and viewers across YouTube and Google, including through machine learning and AI applications.”\n\nThe EU’s competition chief, Teresa Ribera, said: “AI is bringing remarkable innovation and many benefits for people and businesses across Europe, but this progress cannot come at the expense of the principles at the heart of our societies.”\n\nA spokesperson for Google said: “This complaint risks stifling innovation in a market that is more competitive than ever.\n\n“Europeans deserve to benefit from the latest technologies and we will continue to work closely with the news and creative industries as they transition to the AI era.”\n\nThe EU’s investigation is the latest in a series of challenges to US big tech companies in recent years.\n\nIn September, EU regulators issued a fine of almost €3bn (£2.6bn) against Google, claiming that it favoured its own digital advertising services over rivals. Donald Trump said the fine was “discriminatory”.\n\nElon Musk’s social media company X, formerly known as Twitter, was fined €120m by EU tech regulators last week for breaching online content rules. The breaches included what the EU said was a “deceptive” blue tick verification badge given to users and the lack of transparency of the platform’s advertising.\n\nThe fine also attracted criticism from US officials, including the secretary of state Marco Rubio, who wrote on X that the fine was “an attack on all American tech platforms and the American people by foreign governments”.\n\nThe European Commission opened an investigation earlier this year into Meta over its rollout of AI features on WhatsApp, its messaging platform. Last year it fined Meta €798m for abusive practices benefiting Facebook Marketplace.\n\nIn 2024, Apple lost a fight against an order by EU competition regulators to pay €13bn in back taxes to Ireland.\n\nLast month, the head of Google’s parent company said people should not “blindly trust” everything AI tools tell them.\n\nSundar Pichai, the chief executive of Alphabet, said AI models were “prone to errors” and urged people to use them alongside other tools. He also warned that no company would be immune if the AI bubble burst.",
    "readingTime": 4,
    "keywords": [
      "online content",
      "content uploaded",
      "youtube content",
      "content creators",
      "the european commission",
      "the eu’s",
      "models",
      "rival",
      "investigation",
      "competition"
    ],
    "qualityScore": 1,
    "link": "https://www.theguardian.com/technology/2025/dec/09/eu-investigation-google-ai-models-gemini",
    "thumbnail_url": "https://i.guim.co.uk/img/media/0265df329ab15c539360f63131ed6ab480ca4201/1222_753_5697_4560/master/5697.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=31c9f30ca58e4748ef11858662efbaa9",
    "created_at": "2025-12-09T13:48:22.960Z",
    "topic": "tech"
  },
  {
    "slug": "linkedin-cofounder-reid-hoffman-says-he-learned-a-lesson-fro",
    "title": "LinkedIn cofounder Reid Hoffman says he learned a lesson from a visit to Epstein's island: 'Note to self, Google before going'",
    "description": "Hoffman, the billionaire LinkedIn cofounder, said he visited Epstein's island as part of his work fundraising for MIT.",
    "fullText": "Reid Hoffman said he once spent a night on Epstein's island in connection with MIT fundraising efforts.\n\nThe LinkedIn cofounder told the Newcomer podcast that he had been told a visit would make Epstein more likely to donate.\n\nHoffman has previously apologized publicly for his interactions with Epstein.\n\nLinkedIn cofounder Reid Hoffman said he wishes he'd known a little more before agreeing to spend a night on Jeffrey Epstein's island.\n\nOn the Dec. 1 episode of Eric Newcomer's podcast, Hoffman said that he visited the island as part of his fundraising work for the MIT Foundation and was told the visit would make Epstein more likely to donate to MIT.\n\n\"Note to self: Google before going,\" Hoffman said on the podcast. He said he stayed on the island for one night, and that there was a pool, a \"bunch of guest rooms,\" and a courtyard.\n\nHoffman has maintained that he only interacted with Epstein, whose 2019 death while awaiting trial on sex-trafficking charges was ruled a suicide, through his work fundraising for the MIT Media Lab. On the \"Newcomer\" podcast, he called Epstein a \"masterful networker,\" and recalled a 2015 dinner he hosted for an MIT researcher in Palo Alto, California.\n\nHoffman said that Joi Ito, former director of the MIT Media Lab, asked him if Epstein could attend the dinner, which was also attended by Meta CEO Mark Zuckerberg and Tesla CEO Elon Musk. Similar to his visit to the island, Hoffman said he was later told that the financier had said he was more likely to donate if he attended the dinner.\n\n\"He's kind of going through the network, trying to meet people and so forth,\" Hoffman said on the podcast. Hoffman also reiterated previous apologies for his involvement with Epstein.\n\nIn 2019, a spokesperson for Zuckerberg confirmed the dinner to Business Insider and said it was the only time the Facebook cofounder met Epstein. A spokesperson for Musk also confirmed the Tesla CEO's attendance.\n\nIn a 2019 email to Axios, Hoffman acknowledged multiple interactions with Epstein, which he said were strictly for fundraising purposes, and said he had been told MIT had vetted and approved the convicted sex offender's participation. He said in the email he was \"deeply regretful\" of the involvement.\n\n\"I went and kind of made a, you know, very public apology because it was like, okay I realized this and I'd already at that point had ramped down connection with him, right, to like no meetings and all the rest of the stuff, under any context,\" Hoffman said on the podcast. \"And I think he still would drop me an email every so often and say, 'Hey, can we get on the phone?' I say, 'Oh, maybe sometime,' which is, you know, code for never, right?\"\n\nHoffman said that justice for the late pedophile's victims is important, and urged the government to release, unredacted, \"every single piece of intel that they have about Epstein.\"\n\nIn November, President Donald Trump signed a bill that will release the Department of Justice's files on Epstein after months of pressure from Congress, including some fellow Republicans. The department has until Saturday, December 19, to comply with the order.\n\nTrump has also ordered the DOJ to investigate Hoffman, along with other individuals he views as political enemies, including former President Bill Clinton and former Treasury Secretary Larry Summers, over their ties to Epstein.\n\nHoffman, a billionaire and major Democratic donor, has previously said that he had to hire security after Musk fueled conspiracy theories about his relationship with Epstein.",
    "readingTime": 3,
    "keywords": [
      "hoffman",
      "epstein",
      "podcast",
      "island",
      "told",
      "fundraising",
      "dinner",
      "night",
      "cofounder",
      "newcomer"
    ],
    "qualityScore": 1,
    "link": "https://www.yahoo.com/news/articles/linkedin-cofounder-reid-hoffman-says-195616512.html",
    "thumbnail_url": "https://s.yimg.com/ny/api/res/1.2/9upGoiVLbTnMB1soFYZ6Sw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTEyMDA7aD05MDA7Y2Y9d2VicA--/https://media.zenfs.com/en/business_insider_articles_888/921ff7fb3570f0287b71217162b42eb0",
    "created_at": "2025-12-09T08:42:50.010Z",
    "topic": "news"
  },
  {
    "slug": "google-cloud-ceo-lays-out-3part-strategy-to-meet-ais-energy",
    "title": "Google Cloud CEO lays out 3-part strategy to meet AI’s energy demands after identifying it as the ‘most problematic thing’",
    "description": "Speaking at the Fortune Brainstorm AI conference, Google Cloud boss Thomas Kurian discussed how the company thinks about energy and data centers.",
    "fullText": null,
    "readingTime": 0,
    "keywords": [],
    "qualityScore": 0,
    "link": "https://fortune.com/2025/12/08/google-cloud-ai-energy-demands-strategy-data-center-electricity/",
    "thumbnail_url": "https://fortune.com/img-assets/wp-content/uploads/2025/12/54972707933_643da41260_o-e1765241589857.jpg?resize=1200,600",
    "created_at": "2025-12-09T08:42:48.792Z",
    "topic": "business"
  },
  {
    "slug": "linkedin-cofounder-reid-hoffman-says-he-learned-a-lesson-fro",
    "title": "LinkedIn cofounder Reid Hoffman says he learned a lesson from a visit to Epstein's island: 'Note to self, Google before going'",
    "description": "Hoffman, the billionaire LinkedIn cofounder, said he visited Epstein's island as part of his work fundraising for MIT.",
    "fullText": "LinkedIn cofounder Reid Hoffman said he wishes he'd known a little more before agreeing to spend a night on Jeffrey Epstein's island.\n\nOn the Dec. 1 episode of Eric Newcomer's podcast, Hoffman said that he visited the island as part of his fundraising work for the MIT Foundation and was told the visit would make Epstein more likely to donate to MIT.\n\n\"Note to self: Google before going,\" Hoffman said on the podcast. He said he stayed on the island for one night, and that there was a pool, a \"bunch of guest rooms,\" and a courtyard.\n\nHoffman has maintained that he only interacted with Epstein, whose 2019 death while awaiting trial on sex-trafficking charges was ruled a suicide, through his work fundraising for the MIT Media Lab. On the \"Newcomer\" podcast, he called Epstein a \"masterful networker,\" and recalled a 2015 dinner he hosted for an MIT researcher in Palo Alto, California.\n\nHoffman said that Joi Ito, former director of the MIT Media Lab, asked him if Epstein could attend the dinner, which was also attended by Meta CEO Mark Zuckerberg and Tesla CEO Elon Musk. Similar to his visit to the island, Hoffman said he was later told that the financier had said he was more likely to donate if he attended the dinner.\n\n\"He's kind of going through the network, trying to meet people and so forth,\" Hoffman said on the podcast. Hoffman also reiterated previous apologies for his involvement with Epstein.\n\nIn 2019, a spokesperson for Zuckerberg confirmed the dinner to Business Insider and said it was the only time the Facebook cofounder met Epstein. A spokesperson for Musk also confirmed the Tesla CEO's attendance.\n\nIn a 2019 email to Axios, Hoffman acknowledged multiple interactions with Epstein, which he said were strictly for fundraising purposes, and said he had been told MIT had vetted and approved the convicted sex offender's participation. He said in the email he was \"deeply regretful\" of the involvement.\n\n\"I went and kind of made a, you know, very public apology because it was like, okay I realized this and I'd already at that point had ramped down connection with him, right, to like no meetings and all the rest of the stuff, under any context,\" Hoffman said on the podcast. \"And I think he still would drop me an email every so often and say, 'Hey, can we get on the phone?' I say, 'Oh, maybe sometime,' which is, you know, code for never, right?\"\n\nHoffman said that justice for the late pedophile's victims is important, and urged the government to release, unredacted, \"every single piece of intel that they have about Epstein.\"\n\nIn November, President Donald Trump signed a bill that will release the Department of Justice's files on Epstein after months of pressure from Congress, including some fellow Republicans. The department has until Saturday, December 19, to comply with the order.\n\nTrump has also ordered the DOJ to investigate Hoffman, along with other individuals he views as political enemies, including former President Bill Clinton and former Treasury Secretary Larry Summers, over their ties to Epstein.\n\nHoffman, a billionaire and major Democratic donor, has previously said that he had to hire security after Musk fueled conspiracy theories about his relationship with Epstein.",
    "readingTime": 3,
    "keywords": [
      "hoffman",
      "epstein",
      "podcast",
      "island",
      "dinner",
      "fundraising",
      "told",
      "former",
      "musk",
      "email"
    ],
    "qualityScore": 1,
    "link": "https://www.businessinsider.com/linkedin-reid-hoffman-jeffrey-epstein-island-visit-2025-12",
    "thumbnail_url": "https://i.insider.com/6936f2e104d0f0a114f1955c?width=1200&format=jpeg",
    "created_at": "2025-12-09T08:42:47.839Z",
    "topic": "finance"
  },
  {
    "slug": "the-google-pixel-10-is-200-off-right-now",
    "title": "The Google Pixel 10 Is $200 Off Right Now",
    "description": "It's the latest Pixel series, now with Pixelsnap (Google's version of MagSafe), new AI features, and the new telephoto lens.",
    "fullText": "Google phones keep offering great value for the money, dropping in price very quickly after their release, including the latest Pixel. The Google Pixel 10, with the 128GB going for $599 (originally $799) and the 256GB for $699 (originally $899), are both at record low prices right now, according to price-tracking tools.\n\nThe Google Pixel 10 is the latest in the series to be released this year, back in September. It's the model under the Pixel 10 Pro, which is also at its lowest price right now and has a much faster chip. As Lifehacker's Associate Tech Editor Michelle Ehrhardt says in her review, the Pixel 10 features a telephoto lens, brings the Pixelsnap (Google's version of MagSafe), and has new AI features. However, the ultrawide lens gets weaker, and there are some problems with the chip for third-party apps (but it can be fixed).\n\nThis Pixel 10 has a lot of the same features you'll find in the Pixel Pro for $150 less, making it a great budget option for those who don't want or need all the fancy specs and features. It comes with a Google Tensor G5 chip, and the camera resolutions are 48MP, 13MP, and 10.8MP for the rear and 10.5MP for the front-facing one. You can expect about 24 hours of battery life, depending on your use.\n\nOne of my favorite things about Pixel phones is the ongoing support for many years. My Pixel 6A still gets all of the updates and tons of AI features that make the phone feel fresh many years later, with the latest ones dropping in September. With the Pixel 10, you'll be getting a quality phone with software updates for a while (as long as seven years).",
    "readingTime": 2,
    "keywords": [
      "pixel",
      "google",
      "features",
      "price",
      "latest",
      "chip",
      "phones",
      "great",
      "dropping",
      "originally"
    ],
    "qualityScore": 0.75,
    "link": "https://lifehacker.com/tech/google-pixel-10-deal?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01K981BZQ7WSYSCQ86F4K0T568/hero-image.fill.size_1200x675.png",
    "created_at": "2025-12-09T08:42:44.903Z",
    "topic": "tech"
  },
  {
    "slug": "apple-and-google-are-making-it-easier-to-switch-between-ipho",
    "title": "Apple and Google Are Making It Easier to Switch Between iPhone and Android",
    "description": "Don't get locked into an ecosystem.",
    "fullText": "When it's time to buy a new car, you don't necessarily need to stick with the one you had before. You don't lose your cloud-based photos by switching from Toyota to Subaru, nor will your friends yell at you for ruining the group chat by buying a Kia. That's not the case with smartphones: When you buy an iPhone, it's tough to switch away from it. The same goes for Android: While it's easy enough to switch within the Android ecosystem, such as between Pixel or Galaxy, moving from Android to iPhone can also be a pain. Tech companies tend to make it tempting to stick with their platform, and introduce friction when you try to leave.\n\nThat, of course, is entirely business-based. Apple hasn't traditionally made it easy to move to Android, because, well, you might actually do it. It doesn't have to be this way, either. There's nothing inherent to smartphones that should make it so challenging to break out of any particular ecosystem. All it takes is some intentional design: If smartphones were made to be traded, you could migrate from one to another, without worrying about losing pictures, messages, or any other important data or processes.\n\nAs it happens, that intentional design may be on the horizon. As reported by 9to5Google, Apple and Google are actually working together to make it easier to transfer data between iPhone and Androids, which would make switching between the two platforms more seamless. This isn't theoretical, either: Google has already released some of this progress as part of the latest Android Canary, the company's earliest pre-release software. All compatible Pixel devices can currently access this latest build, though it doesn't seem there are any user-facing features available to test. 9to5Google says that similar features will roll out to testers in a future iOS 26 beta. Perhaps at that time, Google will roll out its features to the Android beta as well, which has a much larger user base than Canary.\n\nWhile details are slim here, any cooperation between Apple and Google on this front is huge. Current migration tools do exist, but they can be problematic. By actually working together on a native transfer solution, it might actually be seamless to move between platforms. Apple and Google might not be motivated by charity, of course, as the EU has been cracking down on restrictive practices by tech companies in recent years. But while both companies may see this as a way to lose customers, it's also a way to gain them: Sure, some iPhone users may switch to Android if it's easier to do so, but some Android users may do the reverse for the same reasons.\n\nMore choice is good for everyone—even if it doesn't guarantee exponential growth to shareholders.",
    "readingTime": 3,
    "keywords": [
      "android",
      "google",
      "between",
      "iphone",
      "make",
      "apple",
      "actually",
      "smartphones",
      "switch",
      "while"
    ],
    "qualityScore": 1,
    "link": "https://lifehacker.com/tech/google-and-apple-are-making-it-easier-to-switch-between-iphone-and-android?utm_medium=RSS",
    "thumbnail_url": "https://lifehacker.com/imagery/articles/01KBZW51YD9RTK9TT6NWP3RZ28/hero-image.fill.size_1200x675.jpg",
    "created_at": "2025-12-09T08:42:44.864Z",
    "topic": "tech"
  },
  {
    "slug": "is-ai-a-bubble-thats-about-to-pop-podcast",
    "title": "Is AI a bubble that’s about to pop? – podcast",
    "description": "Should we be worried about the vast amounts of money pouring into AI? And what will happen if the bubble bursts? Blake Montgomery reports\nFor months there have been fears that artificial intelligence is a bubble and that it is about to burst.\nAs the Guardian US tech editor Blake Montgomery explains, the magnificent seven – Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia and Tesla – make up one-third of the value of the S&P 500, the index of the 500 biggest stocks in the US market. All are heavily invested in AI.",
    "fullText": "Should we be worried about the vast amounts of money pouring into AI? And what will happen if the bubble bursts? Blake Montgomery reports\n\nFor months there have been fears that artificial intelligence is a bubble and that it is about to burst.\n\nAs the Guardian US tech editor Blake Montgomery explains, the magnificent seven – Alphabet, Amazon, Apple, Meta, Microsoft, Nvidia and Tesla – make up one-third of the value of the S&P 500, the index of the 500 biggest stocks in the US market. All are heavily invested in AI.\n\nNever before has so much of the economy been dependent on one technology. And despite the trillions of dollars invested, AI is yet to show a way it can sustainably turn over profits.\n\nSo what happens, asks Nosheen Iqbal, if one day the faith falters, the money stops coming in and the bottom falls out?",
    "readingTime": 1,
    "keywords": [
      "about",
      "money",
      "bubble",
      "blake",
      "montgomery",
      "invested"
    ],
    "qualityScore": 0.65,
    "link": "https://www.theguardian.com/news/audio/2025/dec/08/is-ai-a-bubble-thats-about-to-pop-podcast",
    "thumbnail_url": "https://i.guim.co.uk/img/media/a592cb81eac50241f8dc955b0dd411e77f13449c/419_0_6024_4820/master/6024.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=36b946de97b28522c047dce82e755023",
    "created_at": "2025-12-09T08:42:39.215Z",
    "topic": "tech"
  }
]